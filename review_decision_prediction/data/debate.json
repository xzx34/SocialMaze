[
    {
        "id": "B1gNKxrYPB",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Attributed Graph Learning with 2-D Graph Convolution\nKeywords: 2-D Graph Convolution, Attributed Graph, Representation learning\nAbstract: Graph convolutional neural networks have demonstrated promising performance in attributed graph learning, thanks to the use of graph convolution that effectively combines graph structures and node features for learning node representations. However, one intrinsic limitation of the commonly adopted 1-D graph convolution is that it only exploits graph connectivity for feature smoothing, which may lead to inferior performance on sparse and noisy real-world attributed networks. To address this problem, we propose to explore relational information among node attributes to complement node relations for representation learning. In particular, we propose to use 2-D graph convolution to jointly model the two kinds of relations and develop a computationally efficient dimensionwise separable 2-D graph convolution (DSGC). Theoretically, we show that DSGC can reduce intra-class variance of node features on both the node dimension and the attribute dimension to facilitate learning. Empirically, we demonstrate that by incorporating attribute relations, DSGC achieves significant performance gain over state-of-the-art methods on node classification and clustering on several real-world attributed networks. \n"
            },
            {
                "round2": "Reviewer 1:\nThe paper proposes a new 2-D graph convolution method to aggregate information using both the node relation graph and the attribute graph generated using, e.g., PMI and KNN. The motivation does make sense that using 1-D convolution along the node dimension might not enough for learning representation for those low-degree nodes. Then, the attribute relation might be used to further smooth the node representation. The assumption could be that documents in a class are likely to consist of similar (related) words. To achieve this, the information aggregation along the node and the attribute dimension is implemented via a product of three matrices, the node graph convolutional filter computed from node affinity matrix, the attribute graph convolutional filter computed the attribute affinity matrix given by PMI or KNN, and the node-attribute matrix, which can be one main contribution. \nBesides, the paper also includes a detailed discussion of intra-class variance reduction. They evaluated the proposed method on both the mode classification and the node clustering against several existing methods, demonstrated that the proposed method almost always outperforms those methods on the two datasets. Overall, it is an interesting paper. \n\nFor aggregating the information along the node dimension and the attribute dimension, as mentioned in the paper, is it possible to firstly do the propagation over the attribute graph using a 1-layer GCN, followed by another 1-layer GCN over the node relation graph, similar to dense graph propagation module in “Rethinking knowledge graph propagation for zero-shot learning”?\n\nThere are two ways to build an attribute graph. The experiments seem to show that the performance is quite different. It would be good to have some discussion on this. \n\nOne motivation is about the low-degree nodes, where the attribute graph might help. It would be good to have a study on the performance of the methods on those low-degree nodes. \n\nReviewer 2:\nThis work proposes a 2D graph convolution to combine the relational information encoded both in the nodes and in the edges.\n\nThe basic idea is to reduce the intra-class variance. The authors provide theorems and proofs to support this claim even though it is quite intuitive that smoothing with similar neighbours preserves higher variance with respect to dissimilar neighbours.\n\nIt is not straightforward to understand the limitations on the size of graphs.\n\nThe experimental analysis provides the empirical evidence of the properties of the proposed method. It is worthwhile to remark that connectivity is not necessarily helpful, like in the wiki dataset where connected nodes are not similar.\n\n\nReviewer 3:\nThe authors propose a method that incorporates two types of graphs into a graph convolutional networks:\n\n(1) the given graph, which the authors refer to as node affinity graph, and \n(2) the graph that models an affinity between node attribute values.\n\nThe main contribution of their work is a type of graph convolution that combines convolutions operating on these two graphs. \n\nThe paper is densely written and provides several mathematical derivations that are unnecessary to convey the proposed method. Personally, I don't see any benefits in having sections 3.1-3.4 in the main paper. The method actually proposed and evaluated in the paper is described in section 3.5. Sections 3.1-3.4 could be moved to an appendix. They confuse the reader more than they help. (They demonstrate knowledge of graph signal processing on the parts of the authors but little more.)\n\nTrying to provide some theoretical analysis of the proposed method (and standard graph convolutions) by showing that the intra-class variance is reduced is laudable. The theorems, however, only hold under strong assumptions and could, in my opinion, also be moved to an appendix. In the end, they don't have any bearing on the performance of the methods using real-world datasets. Adding some experiments to analyse to what extent the assumptions made by the theorems are met in the given datasets would be an interesting addition to the paper. \n\nThe authors discuss related work sufficiently with one exception: there has been recent work on learning the structure of graph neural networks. See for example [1]. The structure is derived/bootstrapped using node attribute similarities and it is shown that augmenting the graph with these new edges improves accuracy significantly. I would like to point the authors specifically to Figure 2 and Table 1 in said paper, where the authors show that adding edges (e.g., based on some node attribute affinity before or during training) is beneficial and improves accuracy. It would therefore be interesting to see how the authors proposed 2-D convolution would compare to a baseline where the edges based on attribute affinity are added to the original (node affinity) graph. It is a (somewhat simpler) alternative way to combine node affinity and node attribute graphs. \n\n[1] https://arxiv.org/pdf/1903.11960.pdf\n\nThe empirical results are mixed. Due to the numerous different variations of DSGC for which experiments were conducted, the difference between DSGC and existing methods is probably not statistically significant (a bonferroni correction was not performed to counteract the multiple comparisons). \n\nOverall this an interesting paper that introduces a way to incorporate node attribute affinity graphs. It is too densely written and could benefit from moving the theoretical parts to an appendix. They don't really add much to the core of the paper. Moreover, the authors do not consider approaches that also add edges to the graph (based, e.g., on attribute value similarity or during learning, see e.g. [1]) showing that that improves performance even when using a vanilla GCN. A comparison to a baseline that simply adds edges based on attribute affinity to the graph and applied a vanilla GCN should be part of the evaluation. The empirical results are mixed and don't show a clear advantage of the proposed method. \n"
            },
            {
                "round3": "Thank you for your positive and constructive feedback.\n\nQ1. Comparison with works on learning graph structures such as LDS (https://arxiv.org/pdf/1903.11960.pdf).\n\n>> First of all, we would like to thank the reviewer for pointing out this interesting work. In the revised manuscript, we have included some discussion on this line of research in the 3rd paragraph of section 2.\n\nWe have also conducted experiments on LDS. Since LDS cannot scale to the size of the 20 Newsgroup dataset (out of GPU Memory) used in our experiments, we follow the authors to test on a 10-category subset of the 20 NG. We then test LDS on this subset of 20 NG, L-Cora, and Wiki. For classification on each dataset, LDS uses 20 labels per class for training and extra 20 labels per class for validation (the algorithm requires validation). Note that we do not use any validation data for the proposed DSGC method for classification. Due to the differences in datasets and experimental setup, we do not include the results of LDS in Table 1.\n\nInstead, we report the results of LDS in Table 2 to see whether the proposed DSGC can be used to improve LDS. We incorporate DSGC into LDS as described in section 5.2 by applying attribute graph convolution on the node features before training. The results in Table 2 show that DSGC significantly improves LDS on Newsgroup and Wiki and slightly improves LDS on L-Cora. We have also tested another case of LDS without using the given node affinity graphs of the three datasets and observed similar results. \n\nThe experiments show that DSGC can complement and improve LDS, just as it can complement and improve other SOTA methods based on the regular 1-D graph convolution such as GCN/GAT/GraphSAGE as shown in Table 2.\n\n\nQ2. The paper is densely written.\n\n>> As the reviewer suggested, we have reorganized sections 3 and 4 to make them more compact in the revised manuscript. In section 3, we intend to show how the proposed 2-D graph convolution DSGC is derived, which follows a similar path of the development of 1-D GCN (from “spectral networks” to “ChebyNet” to “GCN”). In section 4, we want to provide some insights into why DSGC works by analyzing the variance reduction effect of node graph convolution and attribute graph convolution respectively. \n\nQ3. The empirical results are mixed.\n\n>> We have improved the presentation of the experiments in the revised manuscript. We kindly ask the reviewer to read section 7 about the experiments again. Our results are statistically significant. For datasets with good node affinity graphs such as 20 Newsgroup and L-Cora, the proposed 2-D graph convolution DSGC (GXF) significantly outperforms most SOTA methods. For datasets with bad node affinity graphs such as Wiki, the proposed 2-D graph convolution DSGC (GXF) still outperforms most SOTA methods by a large margin but is less effective than DSGC (XF) (since the node affinity graph G is bad). DSGC can also be used to significantly improve SOTA methods including GCN, GAT, LDS and GraphSAGE. Please refer to section 7 in the manuscript for more detailed explanation. \n\n\n\nThank you for your positive and helpful feedback. As you suggested, we have further emphasized in section 7.2 of the revised manuscript that connectivity such as the hyperlinks in Wiki is not necessarily helpful.\n\nThank you for your positive and helpful comments.\n\nQ1. “Is it possible to firstly do the propagation over the attribute graph using a 1-layer GCN, followed by another 1-layer GCN over the node relation graph, similar to dense graph propagation module in “Rethinking knowledge graph propagation for zero-shot learning”?\n\n>> Yes, it is possible to do that, and the performance is expected to be similar as the proposed DSGC.\n\nQ2. Discussion on the two ways for constructing the attribute affinity graph.\n\n>>Thank you for the suggestion.  For both classiﬁcation and clustering, we observe that in most cases DSGC with PPMI can achieve better performance than with Emb. This shows the effectiveness of PPMI in capturing meaningful word relations based on information theory and statistics (Church & Hanks, 1989), whereas Emb only relies on a distance metric for measuring word similarity. We have also revised the manuscript to include discussion on this.\n\n\nQ3. “One motivation is about the low-degree nodes, where the attribute graph might help. It would be good to have a study on the performance of the methods on those low-degree nodes.\n\n>>This is a good point. Actually, we already did that. In our experiments, we compared the proposed attributed graph convolution DSGC (XF) with MLP. The former outperforms the latter by a very large margin on all the three datasets. Note that this is an extreme case where each node has 0 degree (GCN reduces to MLP in this case), which shows that attribute graph convolution works well even when there are no links between nodes. We have revised the manuscript to emphasize this point in section 7.2 as you suggested. \n\n\n\nWe would like to thank all the reviewers for their valuable time and feedback. \n\nWe have incorporated their suggestion and revised the manuscript accordingly. Major changes include: 1) In section 7, we improve the presentation of experimental results and include comparison with a suggested baseline LDS; 2) We reorganize the content of section 3 and make section 4 more compact; 3) In section 2, we add some discussion of recent related work on learning graph structures for graph neural networks.\n\nWe will release source code and datasets to ensure the reproducibility of our results.\n"
            }
        ]
    },
    {
        "id": "S1xRnxSYwS",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Goten: GPU-Outsourcing Trusted Execution of Neural Network Training and Prediction\nKeywords: machine learning, security, privacy, TEE, trusted processors, Intel SGX, GPU, high-performance\nAbstract: Before we saw worldwide collaborative efforts in training machine-learning models or widespread deployments of prediction-as-a-service, we need to devise an efﬁcient privacy-preserving mechanism which guarantees the privacy of all stakeholders (data contributors, model owner, and queriers). Slaom (ICLR ’19) preserves privacy only for prediction by leveraging both trusted environment (e.g., Intel SGX) and untrusted GPU. The challenges for enabling private training are explicitly left open – its pre-computation technique does not hide the model weights and fails to support dynamic quantization corresponding to the large changes in weight magnitudes during training. Moreover, it is not a truly outsourcing solution since (ofﬂine) pre-computation for a job takes as much time as computing the job locally by SGX, i.e., it only works before all pre-computations are exhausted.\n\nWe propose Goten, a privacy-preserving framework supporting both training and prediction. We tackle all the above challenges by proposing a secure outsourcing protocol which 1) supports dynamic quantization, 2) hides the model weight from GPU, and 3) performs better than a pure-SGX solution even if we perform the precomputation online. Our solution leverages a non-colluding assumption which is often employed by cryptographic solutions aiming for practical efﬁciency (IEEE SP ’13, Usenix Security ’17, PoPETs ’19). We use three servers, which can be reduced to two if the pre-computation is done ofﬂine. Furthermore, we implement our tailor-made memory-aware measures for minimizing the overhead when the SGX memory limit is exceeded (cf., EuroSys ’17, Usenix ATC ’19). Compared to a pure-SGX solution, our experiments show that Goten can speed up linear-layer computations in VGG up to 40×, and overall speed up by 8.64× on VGG11."
            },
            {
                "round2": "Reviewer 1:\nI thank the authors for the detailed rebuttal.\nNevertheless, I think that many misconceptions remain, which I describe below:\n\n* On the LAN vs WAN setting:\nWhile one issue in a WAN is bandwidth, the much bigger problem is *latency*. After every layer, the server hosting the SGX enclave has to send data to the server hosting the second GPU, and wait for the GPU's reply before moving on to the next layer. In a WAN, the roundtrip latency will be in the 5-150 millisecond range depending on how geographically close the servers are. Now, multiply this by the number of layers. Running the entire computation inside SGX will always be faster.\n\nYour rebuttal mentions the need of a good network line. Paying more money will indeed get you more bandwidth, but latency is fundamentally limited by the speed of light.\n    \n* On non-collusion in a LAN:\nThe real-world example you give with Facebook's password protections doesn't really relate to your case. Here, Facebook is trying to protect against an outsider adversary. But Facebook obviously has access to the contents of both servers, so this doesn't protect the data from Facebook.\nThe challenge in Goten is to make sure that no single entity can see the data processed by the two GPUs. If these two GPUs both belong to the same company (e.g., Facebook), then you need to trust that company for privacy. But if you do trust this company for privacy, you can just have them run the entire computation in the clear.\n    \n* On access control:\nYou mention that one way to ensure non-collusion within an organization is to enforce access-control of the servers. Again, this doesn't actually provide any security from the organization itself, which is in charge of the access control. But even so, at this point you could also just run the whole computation in the clear in a single GPU (without SGX) and restrict access to this GPU using the same access control strategies. Using two servers and SGX doesn't buy you anything in this scenario.\n    \n* On optimizing CaffeScone:\nEven if optimizing this baseline is not the main goal of your paper, it is still necessary to make the point that Goten is practical. The question is basically: if a company wants to spend resources to deploy private ML, should they implement an optimized Goten, or an optimized CaffeScone? If the optimized CaffeScone achieves similar performance, they'll be more likely to use that as it is much simpler to deploy and doesn't require non-colluding servers.\n\nReviewer 2:\nThe paper proposes a method for privacy-preserving training and evaluation of DNNs. The method is based on a combination of hardware support from a trusted execution enclave (Intel SGX) and an algorithm for offloading intensive computation to unsecure GPU devices and communicating with the trusted environment without losing security guarantees during communication.  Compared to related work on a similar system (Slalom), the proposed system enables secure training in addition to inference.  The approach is based on the use of additive secret sharing to relegate chunks of computation to independent GPU servers.\n\nThe evaluation presents experiments that report timings of the proposed system against a baseline. Throughput (images/second) improvements of 8-9x are reported, but the contrast point is unclear (it appears to be CaffeSCONE, but there are several unclear points, summarized below). In addition, a set of results reporting a speed up ratio against attained accuracy of a trained VGG11 network, and a set of results reporting speed up against arithmetic intensity of the workload are given.\n\nI lean towards rejection of this draft, as it has several weaknesses:\n- The connection between the evaluation (which mostly focuses on the speed benefits) and the claimed contributions is tenuous at best. This issue is further compounded by clarity issues in the experiments and their description\n- The empirical results are unclear due to differences between simulation of SGX capability vs hardware support of SGX capability. It is not clear what part of the results is influenced significantly by this disparity, and more importantly whether all the comparisons are done in an equal footing (for example the reported results comparing CaffeSCONE with Goten are performed in two different regimes). As a byproduct, there is a confusing \"scaling factor\" described by the authors that is applied to the timings.\n- A brief mention is made of the fact that the proposed system does not in fact provide correctness guarantees (unlike CaffeSCONE), but this is dismissed by reference to utilizing the same trick used by Slalom.  However, this trick is not described or motivated.\n- The writing in the current draft is of relatively low quality, significantly impacting the readability of the paper and making it hard to understand the contributions and whether they are backed by the presented results.\n\n\nReviewer 3:\nThe paper builds a privacy-preserving training framework within a Trusted Execution Environment (TEE) such as Intel SGX. The work is heavily inspired from Slalom, which does privacy-preserving inference in TEEs. The main drawbacks of Slalom when extending to training are (1) weight quantization needs to be dynamics as they change during training, and (2) pre-processing step of Slalom to compare u = f(r) isn't effective as the weights change, and running this within TEE is no better than running the full DNN within TEE. In addition, Goten also makes the weights private as opposed to Slalom. Overall, this is a very important contribution towards privacy preserving training and the paper takes a strong practical and implementation-focused approach by considering issues arising due to memory limitations in TEE and the performance implications of default Linux paging.\n\nThe paper comes up with a novel outsourcing protocol with two non-colluding servers for offloading linear operations in a fully privacy-preserving way and does detailed analysis of the performance implications. Similar to a lot of other methods for training with quantization, the weights are stored and updated in floats while the computation is performed using quantized values. The experimental results suggest a strong improvement over the CaffeSCONE baseline. One drawback with experiments is the lack of comparison with Slalom for inference if Goten is assumed to be a framework for both training and prediction in a privacy-preserving way.\n\nAnother downside of the paper is that a few sections could be improved with their explanation, and there is quite a bit of redundancy in going over the downsides of Slalom and why it can't be used for secure training. For instance,\n- Section 1.1: \"Our results (referring to Section 4.2) show that CaffeSCONE’s performance greatly suffer from the enclave’s memory limit as it needs an inefficient mechanism to handle excessive use of memory not affordable by the enclave\". Here, it's not clear which mechanism is inefficient. Are we talking about mechanisms in CaffeSCONE for reducing memory usage while training and if so, are they somehow inefficient? Or does it mean to imply that we can't train a DNN fully within an enclave due to memory limits?\n-  Last paragraph of section 2.2 is unclear. \"CaffeSCONE guarantees the correctness of both training and prediction. Goten does not provide it as we present it due to page limitation, but we can resort to the trick used by Slalom\". What does the last sentence mean? Does Goten guarantee correctness during training and prediction or not? And what trick from Slalom are we referring to? The blinding trick used for privacy or the Freivalds' algorithm used for correctness?\n\nOverall a strong contribution with supporting experimental results, but the certain parts need further explanation or rewriting for higher rating.\n\nPros:\n- An important contribution in the direction of fully private DNN training and inference within a TEE. Draws inspirations from Slalom and mainly addresses the challenges left to extend the approach to training.\n- Motivation and reasons for why Slalom can't be used for training is very well laid out.\n- In addition to input and output activations, Goten also preserves the privacy of the weights.\n- Good baseline for comparison using CaffeSCONE.\n- Implementation factors considered and analyzed such as tricks as using SGX-aware paging instead of naive Linux paging.\n- Strong experiments and benchmarks\n\nCons:\n- Some sections are not explained well and unclear as mentioned earlier.\n- How does the inference performance of Goten compare to Slalom given the same privacy and correctness guarantees? This isn't clear from the experiments section.\n\nMinor comments:\n- \"Slalom\" is mis-spelt in line 4 of the abstract.\n- There appear to be typos and grammatical errors at many places in the paper. Further proof-reading might be helpful.\n\nReviewer 4:\n\nSummary\n========\nThis paper proposes a framework for privacy-preserving training of neural networks, by leveraging trusted execution environments and untrusted GPU accelerators.\nThe system builds heavily on the prior Slalom system, and uses standard MPC techniques (three non-colluding servers, multiplication triplets) to extend Slalom's inference-only protocol to privacy-preserving training.\nThis is a valuable and hard to reach goal. Unfortunately, the paper's evaluation fails to deliver on its strong promises, by ignoring the high network communication between the non-colluding servers.\nSpecifically, all experiments were conducted with three servers co-located in a public cloud's LAN. In this setting, it is hard to argue that non-collusion is a valid security assumption as the cloud provider controls all servers (alternatively, if the cloud provider is trusted, then there is no need for any trusted execution or cryptography). If the same experiments were conducted on a WAN, the communication costs would alleviate any savings in computation time.\n\nFor these reasons, I lean strongly towards rejection of this paper. \n\nDetailed comments\n=================\nExtending the ideas in Slalom to support privacy-preserving training is a good research question, and Tramer and Boneh had discussed some of the challenges and limitations towards this in their original paper.\nGetting rid of the pre-processing stage for blinding factors by leveraging non-colluding servers is a well-known trick from the MPC literature, but it does not seem easily applicable here. \nThe problem is that the servers need to communicate an amount of data proportional to the size of each internal layer of the network, for each forward and backward pass. If the servers communicate over a standard WAN, the communication time will be much too high to be competitive with the CaffeScone baseline.\nIn a LAN, as in this paper's experiments, the network latency is low enough for the network costs to be dominated by computation. But this begs the question of whether servers running in a same LAN (e.g., hosted by a single cloud provider) can really be considered non-colluding. In the considered setup, the cloud provider (Google in this case), could just observe the communication between all servers, thereby breaking privacy.\n\nAnother security issue with proposed scheme is the lack of computation integrity. This corresponds to the so-called \"honest-but-curious\" threat model which often appears in the MPC literature, and this should be acknowledged and motivated.\n\nOn the experimental side, the considered baseline, CaffeScone, seems pretty weak. In particular, any optimizations that the authors implement for Goten (e.g., better paging) should also be added to their baseline for a fair comparison.\nThe numbers in Figure 3 show that the baseline could be optimized a lot further.  A gap between hardware/simulation modes of ~6x seems indicative of sub-optimal paging. Even the single-core, simulation mode throughput numbers seem low for CIFAR10.\n\nThe experimental setup is quite confusing. Running the baseline and Goten in different environments (e.g., different CPUs) and then re-normalizing throughputs is somewhat convoluted and prone to mistakes. Why not run all experiments on the same setup?\nSimilarly, converting between results in SGX's hardware and simulation modes is also not very indicative. The authors note (p. 8) that in SGX's simulation mode \"code compilation is almost the same as hardware mode except that the program is not protected by SGX, which is fine for our purpose since the DNN training and prediction algorithms are publicly known\". This is fundamentally incorrect!\nSGX's simulation mode provides absolutely no security guarantees. It simply compiles the code using the SGX libraries and ensures that the enclaved code performs no untrusted operations, but it does not provide any hardware protections whatsoever. In particular, code running in simulation mode will not be affected by the overhead of SGX's paging, as the memory is never encrypted.\nAs a result, performance results in simulation mode are usually not indicative of performance in hardware mode. Trying to convert runtimes from simulation mode to hardware mode by comparing times of specific layers is also prone to many approximation errors. \n\nFinally, I had some trouble understanding the way in which Goten quantization works. Section 3.3. mentions that values are treated as floats, but then mentions the use of 53 bits of precision. Did you mean double-precision floats here? But then, aren't modern GPU optimized mainly for single-precision float operations? Section 3.3. also says that the quantization ensures that there are nearly no overflows. What happens when an overflow occurs? I guess that because of the randomized blinding, a single overflow would result in a completely random output. How do you deal with this during training?\n\nMinor\n=====\n- Typo in abstract: Slaom -> Slalom\n- I don't understand the purpose of footnote 3 in Appendix B.2. First, the bibliographic entry for (Volos et al. 2018) explicitly says that the paper was published in OSDI 2018, a top-tier peer-reviewed conference. Regardless, claiming a date for a first unpublished draft of your paper is a little unusual and somewhat meaningless. I'm sure Volos et al. had a draft of their paper ready in late 2017 or even earlier if they submitted to OSDI in XXX 2018. If you want to timestamp your paper, post in to arXiv or elsewhere online."
            },
            {
                "round3": "R1/R2. [Experiment] Simulation mode.\n\nPrograms in hardware (HW) mode has negligible overhead as long as no paging is triggered. Specifically, according to the experimental results in Privado [Tople et al., 2018], the neural networks which do not trigger page-fault do not have any performance overhead.\n\n[Tople et al., 2018] Tople, S., Grover, K., Shinde, S., Bhagwan, R., & Ramjee, R. (2018). Privado: Practical and secure DNN inference. arXiv preprint arXiv:1810.00602.\n\nMoreover, based on our experimental data of non-linear layers, namely, the data for computing scaling factors, the performance of programs in simulation mode is similar to that in HW mode when no page-fault is triggered. Indeed, our SGX-aware chunked operations can totally prevent page-fault in enclaves, and thus the performance overhead due to HW mode is negligible in our case. Furthermore, the values of scaling factors support our claim. The scaling factors are the ratio of 1) the running time of a kind of non-linear layers running in HW mode on a machine equipped with SGX to 2) that in simulation mode on a Google Cloud VM. According to our experiment results, all these values are less than 1, meaning that the running time of the simulation mode on Google Cloud VM overestimates the running time of the full-fledged system.\n\nThat said, we plan to perform experiments for upper-bounding Goten’s running time. To address the concern on the simulation mode, we will run our experiments (of linear layers) on a machine equipped with the SGX module and a weak GPU. Then, we benchmark as a baseline timing the running time in a fine-grained manner, where we record the running time on the CPU, GPU, and the transfer time between CPU and GPU and the networks. To provide a more accurate estimation for the GPU part, we will record its running time and use it to replace the GPU running time in the baseline timing. This evaluation ensures that the running time on the CPU and SGX is authentic. However, such fine-grained timing degrades the measured perform (compared with the true performance) because we synchronize different computing units before any of them starts transferring data or computation. This does not fully utilize the computational resources and the network.\n\n\nR1. [Technical Contributions] Quantization.\nIt is true that most modern GPU are optimized for single-precision float operation, but single-precision floats are not enough for our secret sharing scheme since it only has 23 bits of significand precision for integer operations. Specifically, to achieve both high performance and high accuracy, 23 bits is not enough because integers with 23 bits will easily overflow after multiplication and a bunch of additions, while modulo operations are slow in GPU. Therefore, a sensible choice is to perform computation with double-precision floats, which have 53 bit of significand precision. We use a modern GPU which is also optimized for double-precision float operation.\n\nA high-level idea that we do not run into the overflow problem is that we have a very careful treatment of the available bits (as explained above and Section 3.3).\n\nR3. How does the inference performance of Goten compare to Slalom given the same privacy and correctness guarantees? This isn't clear from the experiments section.\n\nAs argued above (R1/R3), Slalom needs to pre-compute f(r) for computing online f(x). We achieve higher throughput for inference as ours is a real outsourcing solution which the pre-computation is not as expensive as the online computation for the real problem instance.\n\nA primary motivation of Goten is to provide a more comprehensive privacy coverage which was not known to be possible (by Slalom) before, namely, to further achieve data privacy (confidentiality) for the model's parameters. For such protection in training (and inference), Goten does introduce overhead (just like all the subtleties we discussed above). Hence, we found it natural that Slalom has better performance on *inference*, because its design inherently cannot support privacy for model parameter, and it is tailor-made for its weaker privacy guarantee confined to only inference. Simply put, again, it fails to support privacy-preserving training.\n\nR1. [Experiment] Optimizations should also be applied to CaffeScone.\n\nFirst, we provide CaffeSCONE as a baseline approach if one applies a generic solution (SCONE) for making use of SGX for training (not supported by Slalom). Further optimizing it is *not* our goal (for one, it does not use GPU at all). It requires significant engineering effort to implement all our optimization tricks (those without using GPU) to CaffeSCONE. We believe further optimizing it has its own impact as a system research work. \n\n(Until now, no results have applied any optimization on paging to any SGX-protected DNN framework -- a side-evidence that such an optimized framework is not as trivial as one might think, despite the potential impact in the system research community.)\n\nMore importantly, our experiments for comparing CaffeSCONE and Goten is done with *low* paging overhead because these experiments are done in the simulation mode (where programs can run as fast as if they are executed in single-thread normal execution environments). In other words, our results are *not* derived from an experiment which gives Goten unfair advantage. Yet, our experiments showed that Goten can still outperform CaffeSCONE by 2.5x to 18x in different linear layers. (Also see Figure 5(a).) \n\nHence, even in an ideal case where paging overhead is no longer a problem (either by applying our optimization techniques on both frameworks; or by using a non-existent, future version of SGX), Goten can still significantly outperform pure-SGX solutions, specifically, for privacy-preserving training.\n\nR1. [Experiment] Running the baseline [CaffeSCONE] and Goten in different CPUs and re-normalizing throughputs is convoluted and prone to mistakes.\n\nThe experiment setting is disadvantageous to us because CaffeScone ran in the CPU with a higher clock rate (4.2GHz of Intel i7-7700 vs. 2.0GHz of Google VM's), meaning our evaluation overestimates the running time of Goten. Moreover, we only re-normalize the non-linear layers, and thus this evaluation method does not affect the timing on the linear layers, whose performance improvement is our main contribution.\n\nThat said, we felt sorry that we employed different CPUs since we do not have easy access to production-scale hardware resources (GPU) with SGX on the same platform.\n\nR1. [Technical Contribution/Experiment] \"Fundamentally incorrect\"\n\nThe said statement focused on the code compilation and the protection of the algorithms; it is true that it does not use encryption, which we are glad to fix this \"incomplete\" statement (in Section 4.1).\n\nThat said, \"simulation mode will not be affected by the overhead of SGX's paging, as the memory is never encrypted\" does not apply here since we use simple exclusive-or (or one-time pad with pseudorandom sequence) which introduce no space overhead. Specifically, no matter if it is encrypted or not (i.e., no matter which mode we consider), such padding does not elongate the data to be processed. Hence the paging aspect is also not affected. (Also, see the discussion above and below.)\n\nR2. [Experiment] The connection between the evaluation (which mostly focuses on the speed benefits) and the claimed contributions is tenuous.\n\n(Section 4.2) First of all, we show the training throughput of CaffeSCONE in Fig. 3, by which we emphasize that using more cores on CPU cannot improve the performance of such a pure-SGX approach. Moreover, we benchmark the throughput with batch sizes of 128 (a common setting in plaintext setting) and 512 (the setting we adopted for Goten). We confirmed that the former one has better performance for VGG11 in CaffeScone, and thus we adopt it in the latter experiments. Note that we adopt batch size to 512 in Goten because with which Goten has better performance.\n\nTable 1 illustrates the speed up of Goten compared to CaffeSCONE in training phrase.\n\nWe first explain the experimental settings. Goten ran with simulation mode on Google VMs and employed memory-aware measures to reduce the overhead of paging. Moreover, we rescale the running time on non-linear layer, which bases on the running time with the real SGX setting, i.e., the hardware mode on the experimental machine equipped with Intel i7-7700. \n\nAccording to the experimental results on non-linear layers, program running with the real setting are faster than those on Google VMs. Hence, we believe that linear layers in the real setting are also faster as both kinds of layers have similar operation and (linear) access patterns.\n\nAccording to Table 1, in VGG11, Goten outperforms CaffeSCONE by about 8 times on linear layers and non-linear layers.\n\nFurthermore, Table 2 demonstrates how the performance speed up leads to higher convergence rate. The training methods for CaffeSCONE and Goten are different.\nThe former adopts the most common approach, which uses plain single-precision floats. The latter one adopt employs the dynamic quantization scheme SWALP, explained in Section 3.3. Hence, it is natural to wonder whether Goten can attain a higher convergence rate. Our experimental result is confirmative. We record the converge trajectory of both training methods, which was captured in an unprotected setting on GPU, and then rescale the time axis according to the timing from Table 1. The results show that Goten can converge much faster.\n\nTo better emphasize our advantage on the convergence rate, Table 2 lists the speed up (at different levels of accuracy). We can attain 0.88 accuracy by about 7 times faster.\n\nAs our main contribution is the performance speed up on linear layers, we further isolate the performance gain of them and show it in Fig. 5. Moreover, because in Section XX we propose that the performance gain is portional to the arithmetic intensity, we hope to confirm it by more fine-grained benchmark. Hence, we record the performance gain with respect to the shapes of linear layers, which determines the arithmetic intensity. \n\nFig. 5 (a) is derived from the experiments that both CaffeScone and Goten ran on simulation mode and on Google VMs, and Goten does employ the chunked-operations. As we mentioned above, in simulation mode programs run as fast as those in single-thread normal environment, meaning that the paging overhead of SGX does not affect the performance of both of them.\n\nBelow we also provide a partial list of the figures and tables in our paper accompanied by the different settings.\n[Figure 3: Training Throughput of CaffeSCONE]:\nCaffeSCONE runs in HW mode\n\n[Figure 4: Accuracy Convergence in VGG11], [Table 2: Attaining accuracy using GPU-powered Scheme]. [Table 2: Attaining accuracy using GPU-powered Scheme]:\nCaffeSCONE in HW mode. Goten in mixed mode (i.e., linear layers in sim mode and non-linear layers are rescaled to hw mode)\n\n[Figure 5 (a) With Low Paging Overhead] CaffeSCONE: Sim mode. Goten: Sim mode (without memory-aware measures)\n\n[Figure 5 (b) SGX Hardware Mode] CaffeSCONE: HW mode. Goten: Sim mode (with memory-aware measures)\n\nR1. [Technical Contributions] In the considered setup, the cloud provider (Google in this case), could just observe the communication between all servers, thereby breaking privacy.\n\nStrictly speaking, observing the communication alone is not enough to break privacy, but one needs to further look into internal memory. Of course, when the cloud provider controls all the servers, it can as well look into internal memory. This comes to a fine-grained treatment of the concept of \"trust\" beyond a boolean yes/no discussion. Thanks for raising this concern. \n\nIn response, we will make this threat model clear. In practice, abstracting some technical details, access control system can also help in terms of security. Assuming an insider attacker does not have the super-user privilege, and the audit log function of the access control system remains secure, it can then hold the entity with special access-permissions accountable when the access-rights are abused. We add this discussion to Appendix C.2.\n\nAgain, we took a simple treatment with our experiment, but this does not totally nullify the security of our system. As mentioned above, we will also enrich our experiment along this dimension.\n\nR1/R2. [Technical Contributions] Lack of computation integrity (a.k.a. correctness guarantee)\n\nIt is not our aim to provide integrity specifically, which we will acknowledge. Freivald's algorithm (IFIP Congress 1977) used by Slalom provides verifiability for an interactive outsourcing protocol of linear layers. Freivald's technique allows detection with sound error according to the space of the underlying finite field. This is a classical generic technique, which we can use as Slalom since we overate over finite field elements as well. We are happy to discuss more in the appendix.\n\nOur work focuses on privacy, yet, it does not mean we achieve that at the cost of integrity. In our case, privacy and integrity are orthogonal security goals and not inherently conflicting with each other.\n\n(In the \"secure outsourcing\" literature from the cryptography/security community, many major works, for example, searchable encryption, have been studied extensively only in the \"privacy only\" setting. There are also generic transformations that equip integrity to a searchable encryption scheme that does not ensure integrity.)\n\nIn practice, the servers may not have high incentive to jeopardize the training as they invested much cost to establish a system and gather the data to support training (which is different from the federated learning scenario or distributed learning scenario). We will also add this \"motivation/justification.\"\n\nR1. [Technical Contributions / Experiment] Ignoring the high network communication.\n\nOur experiment (despite being in the LAN setting) did not ignore the communication overhead among the servers. We took a simplistic view focusing on computation and accuracy. In the revision, we will provide experiments to shed light on how fast is \"enough,\"  i.e., to estimate the effect of network speed of various WAN settings.\n\nIt is our premise to rely on communicating servers to achieve something which was not possible with the pure cryptographic approach, pure SGX approach, or \"simple\" SGX+GPU approach. In retrospect, Slalom achieves better performance than pure cryptographic approaches, by assuming the existence of a trusted processor (even though security researchers have demonstrated some form of side-channel attacks). In a sense, it further motivates research works (that have been) trying to make the assumption needed by Slalom more realistic.\n\nIn our case, we solved what Slalom failed to solve, i.e., privacy-preserving training. Further, it is efficient (and accurate) with a fast enough connection between the servers. Our suggestion is thus to use our system when the premise (again, we will quantify in our experiments) is satisfied. \n\nWe stress that we are not solving the problem by merely relying on such an assumption. Many challenges remain, as discussed in the paper and our responses (and some of them are mentioned by the reviews as well). For one, our outsourcing protocol minimizes (sequential) communication needed in the original protocol (see \"Parallelizable Pre-Processing without Communication\" in Section 3.2).\n\nPlease see below for further discussions on our experiment setting and how can we realize our assumptions placed on the servers in practice.\n\nR1. [Technical Contributions] In [LAN] setting, it is hard to argue that non-collusion is a valid security assumption as the cloud provider controls all servers\n\nWe believe that non-colluding servers do not *have to* be geographically located far apart. Instead, the crucial point is whether they are really *owned* by the same entity. Moreover, as the network bandwidth is increasing continuously, the communication overhead may not be a bottleneck in certain situations.\n\nEven if one indeed place both servers in the same LAN, there can be other justifications for the security. Let us justify with a real-world example. For user authentication of Facebook, they also maintain an internal server that is \"more secure\" than the public-facing server. E.g., see \"an adversary that compromises the web server and the password hashes it stores must still mount an online attack against the PRF service to compromise accounts,\"  as described  by https://eprint.iacr.org/2015/644.pdf. Simply put, security there is also \"strengthened\" for relying on the \"PRF secret key\" of another server.\n\nFor sure, when both servers are compromised, there is no security. It then comes to the classical discussion from Security 101 on how to fortify a selected machine. We got many possible ways, including but not limited to, i) using different hardware and software configuration from the other server (so a vulnerability in one platform will not lead to an easy compromise of both machines), ii) placing it within the network-level security parameter behind the DMZ, iii) limited access to the machine instead of public-facing, etc. We add this discussion to Appendix C.2.\n\nLet us elaborate more on how non-colluding servers assumption are usually realized in existing works (which is briefly mentioned at the end of \"Secure Outsourcing to GPU\" in Section 1.2). To perform training, companies may join forces and have the motivation to dedicate a better network line between them. A similar argument also applies to the setting in which one of the parties is generally trusted would not collude with others, say, the government. To provide machine learning as a service in the application context of electronic healthcare (e.g., precision medicine), the Department of Health & Human Services (or alike) can take the effort. The better network line is the cost we need to pay to support practical enough privacy-preserving machine-learning training (and inference), something that was not known to be possible (with a comparable level of efficiency and accuracy) before our work.\n\nMeta comment.\nWe thank all the reviewers for the detailed review, which allows us to have concrete revision plan. We are glad that our technical contributions are rightfully observed and appreciated by reviewer 3. Below, we clarify our technical contributions, in particular, how our approach is innovative or at least new, that we can supplement the \"fundamentally incorrect\" statement (because we focused two aspects but missed one) but the missing part does not affect our experiment. We also further explain our experimental settings and lay out our plan for some more experiments. (Thanks for the inputs!) \n\nNo matter whether this is accepted or not eventually, we would appreciate further comments (if possible/applicable) on how well we responded to each issue, and would definitely appreciate and felt encouraged if the score can be at least slightly adjusted (of course, when deemed appropriate).\n\nFor sure, we will revise the paper according to the reviews and the responses below.\n\nR1/R3. [Technical Contributions] The system builds heavily on the prior Slalom system.\n\n1. We share the same general idea of SGX+GPU, but we deviated by just considering one-level lower already. We use 2 SGXes. Our tailor-made outsourcing protocol in Section 3.2 leverages the best of SGX (deriving randomness) and GPU (for batch processing). Existing use of the \"well-known trick\" just consider outsourcing in general and does not take into account the unique characteristics of SGX and GPU.\n\ni. Our protocol thus achieves *real* outsourcing (in terms of total computation time saved). This is different from Slalom's approach, in which the pre-computation time is as much as if it were computed online locally without outsourcing.\n\n(We briefly reiterate Section 1.3 and what is explained by Reviewer 3: Slalom precomputes f(r), outsources the computation of f(x + r), and gets back f(x) by f(x + r) - f(r). The precomputation of f(r) is as slow as computing the real problem instance f(x).)\n\nii. Further, to better pinpoint a benefit of our new design, even if we stick with the same deployment assumption as Slalom, i.e., we do the offline precomputation online, the whole protocol is faster than running by the SGX alone. In this sense, saying we used \"well-known primitive\" is a bit more accurate, but we achieved more.\n\niii. In the literal sense, it did use \"three non-colluding servers, multiplication triplets,\" like some other existing works. From a bird-eye view, it is still \"some form\" of \"outsourcing\" and secure multi-party computation, but we use new design principles (e.g., minimizing the use of encryption to minimal, correlated randomness). We stress that the specific goals we aim to achieve, and hence, the technical details of the seemingly similar conceptual building blocks are different.\n\n2. After all, both works process the neural network. We understand that, from a high-level point of view, they must share a somewhat similar template conceptually to deal with different layers of the neural network. We stress that, in terms of the contribution of the new technical design, we specifically address all the open problems left by Slalom, which hindered them from also supporting training.\n\nSpecifically, our design works under several conflicting constraints to support training for the first time, while Slalom failed to resolve (with an explicit discussion on the challenges, so they are not something some \"easy\" extensions can solve). For one, Slalom's approach inherently leaks the (dynamically changing) weights. An overview is given in the second paragraph of Section 1.3, and details are discussed in Sections 3.3 and 3.4.\n\nTo conclude, we think it may be a bit oversimplifying to say our system relies heavily on the prior *system* and we merely use \"well-known trick,\" at least for the facts that i) existing such protocols were not designed for outsourcing from SGX to GPU but just treating them as \"generic processors\" and ii) we tackled all the challenges left behind by Slalom."
            }
        ]
    },
    {
        "id": "H1lMogrKDH",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: LEARNING DIFFICULT PERCEPTUAL TASKS WITH HODGKIN-HUXLEY NETWORKS\nKeywords: conductance-weighted averaging, neural modeling, normalization methods\nAbstract: This paper demonstrates that a computational neural network model using ion channel-based conductances to transmit information can solve standard computer vision datasets at near state-of-the-art performance. Although not fully biologically accurate, this model incorporates fundamental biophysical principles underlying the control of membrane potential and the processing of information by Ohmic ion channels. The key computational step employs Conductance-Weighted Averaging (CWA) in place of the traditional affine transformation, representing a fundamentally different computational principle. \nImportantly, CWA based networks are self-normalizing and range-limited. We also demonstrate for the first time that a network with excitatory and inhibitory neurons and nonnegative synapse strengths can successfully solve computer vision problems. Although CWA models do not yet surpass the current state-of-the-art in deep learning, the results are competitive on CIFAR-10. There remain avenues for improving these networks, e.g. by more closely modeling ion channel function and connectivity patterns of excitatory and inhibitory neurons found in the brain.  "
            },
            {
                "round2": "Reviewer 1:\nI wanted to first thank the authors for their work on connecting biologically inspired CWA to deep learning. The paper overall reads nicely. \n\nThe authors make the assumption that reviewers are fully aware of biological terminology related to CWA, however, this may not be the case. It took a few hours of searching to be able to find definitions that could explain the paper better. I suggest the authors add these definitions explicitly to their paper (perhaps in supplementary), also an overview figure would go a long way. I enjoyed reading the paper, and before making a final decision (hence currently weak reject), I would like to know the answer to the following questions:\n\n1. Have the authors considered the computational burden of Equation 3? In short, it seems that there are two summations (one for building the probability space over measure h) and one right before e_j. This is somewhat important, if this type of neural network is presented as a competitor to affine mapped activations. \n\n2. It would be nice to have some proof regarding universal approximation capabilities of CWA. In my opinion it is, but a proof would be nice (however redundant or trivial - simply use supplementary). \n\n3. I was a bit confused to see CWA+BN in the Table 1. In introduction, authors write “But CWA networks are by definition normalized and range-limited. Therefore, one can conjecture that CWA plays a normalization role in biological neural networks.” Therefore, I was expecting CWA+BN to work similarly as CWA for CIFAR10. Please elaborate further on this note. \n\n4. Essentially, the CWA changes the definition of a layer in a neural net. Do authors see a path from “CWA works” to “CWA works better than affine?”. If so, please elaborate. Specifically, I am asking this question “Why should/must we stop using affine maps in favor of CWA?”. Now this may or may not be the claim of the paper. It’s ok if it is not; still showing competitive performance is somewhat acceptable, but certainly further insight would make the paper stronger. \n\n\nReviewer 2:\nThis paper focuses on non-spiking Hudghkin-Huxley model, which is different from existing works on spiking neural network-based Hudghkin-Huxley model. \n\nThere are many ways of using neuron firing model as unit to construct neural networks. They choose a specific way (mentioned above). I think the most interesting part would be the CWA method which achieves the normalization. \n\nThey have a fair list of literature in spiking neural networks. But I find the way they illustrate the difference between their model and other models is insufficient. They should focus on the model-wise difference, instead of focusing on whether it’s applied to MNIST or not or what’s the accuracy. \n\nThey don’t include any other SNN model in the paper for experimental comparison. They also mention a few SNN works that work well on MNIST in the related work section which actually have better accuracies than their model. So it is inappropriate to say this proposed method is a state-of-art neuro-inspired method, Because others perform well on MNIST as well, and their limited experiments only investigate MNIST and CIFAR-10, which are less interesting generally. \n\nCWA cannot outperform Affine+BN. \n\nOverall, the idea is somehow interesting, but the experiments are weak. Applying the method to MNIST and CIFAR-10 is far from being called either “interesting computer vision applications” or “difficult perceptual tasks”. They only use perceptual in the title, but the applications are MNIST and CIFAR-10. It feels like they want to learn something big, but they only focus on benchmark datasets. \n\nThey compare nothing with other SNN type of model on other truly difficult perceptual tasks.\n\n\nReviewer 3:\nThis paper proposes a novel neural network architecture inspired by the analysis of a steady-state solution of the Hodgkin-Huxley model. Using a few simplifying assumptions, the authors use conventional backpropagation to train DNN- and  CNN-based models and demonstrate that their accuracies are not much lower than the state-of-the-art results.\n\nThe paper is well-written, sufficiently detailed and understandable. Derived self-normalizing Conductance-Weighted Averaging (CWA) mechanism is interesting in itself, especially contrasting CWA results with those obtained for the non-Batch-Normalized networks. It is also inspiring to see that this model can be derived based on a relatively accurate biological neuron model.\n\nMy main question is actually related to the potential impact of this study. I am curious about the implications and the ways in which these results can inspire other researchers.\n\nAfter reading the paper, I got an impression that:\n\n(a) From the point of view of a machine learning practitioner, these results may not be particularly impressive. They do hint at the importance of self-normalization though, which could potentially be interesting to explore further.\n\n(b) From the point of view of a neuroscientist, the proposed model might be too simplistic. It is my understanding, that neural systems (even at \"rest\") are inherently non-equilibrium (and I assume the presence of simple feedback loops could also dramatically change the stead-state of the system). Is it possible that something similar to this \"steady-state inference\" mode could actually take place in real biological neural systems?\n\n(c) Presented results appear to be important from the point of view of someone who wants to transfer insights from biology into the field of deep learning. But there might be an extent to what is achievable given a simple goal of optimizing a supervised accuracy of an artificial neural network trained using gradient descent (especially considering limitations imposed by hardware). I am optimistic about the prospect of knowledge transfer between these disciplines, but it is my feeling that the study of temporal dynamics, emergent spatiotemporal encodings, \"training\" process of a biological neural system, etc. have potentially much more to offer to the field of machine learning. These questions do appear to be incredibly complex though and the steady-state analysis is definitely a prerequisite."
            },
            {
                "round3": "Thank you for your encouraging comments. Regarding the implications of this work, the primary implication is that neuroscience models can be compared with deep learning results on deep learning benchmarks and with appropriate modelling, the results can be competitive, as one would expect given that the brain performs all these tasks. Furthermore, these comparisons raise questions for neuroscientists that can spur further research. For example, if batch normalization performs better than CWA, why does the brain use CWA? Is it due to evolutionary convenience, or are there other, unexplored benefits to CWA that can explain the choice?\n\nBelow, we respond to the listed impressions in turn.\n\n\nComment: (a) From the point of view of a machine learning practitioner, these results may not be particularly impressive. They do hint at the importance of self-normalization though, which could potentially be interesting to explore further.\n\nResponse: We agree with this assessment; ML practitioners may find some ideas here, but are unlikely to use CWA as a primary method. There are cases, though, where batch norm is not practical or possible, such as in reinforcement learning, and it might be interesting to explore CWA as a normalization method in this context.\n\nComment: (b) From the point of view of a neuroscientist, the proposed model might be too simplistic. It is my understanding, that neural systems (even at \"rest\") are inherently non-equilibrium (and I assume the presence of simple feedback loops could also dramatically change the stead-state of the system). Is it possible that something similar to this \"steady-state inference\" mode could actually take place in real biological neural systems?\n\nResponse: CWA represents limit behavior of the Hodgkin-Huxley differential equations, so it may be possible. Given the need to use spiking to transmit information, the mammalian nervous system appears quite chaotic at the granular level.  However, it is equally true that your perceptions, a product of your brain, are often quite stable.  Spiking, however, is not an essential feature of neuronal function, as many sensory neurons, such as photoreceptors perform computations in the manner we are using here to adjust synaptic release without spiking.   In fact, time dependence is readily incorporated in our model by including a capacitive term.  In this case, the CWA determines the state the neuron is heading towards, and the time constant how quickly it will get there.  The current data sets we have been optimizing against do not require such temporal precision so this has not been incorporated yet into our model.  \n\nComment: (c) Presented results appear to be important from the point of view of someone who wants to transfer insights from biology into the field of deep learning. But there might be an extent to what is achievable given a simple goal of optimizing a supervised accuracy of an artificial neural network trained using gradient descent (especially considering limitations imposed by hardware). I am optimistic about the prospect of knowledge transfer between these disciplines, but it is my feeling that the study of temporal dynamics, emergent spatiotemporal encodings, \"training\" process of a biological neural system, etc. have potentially much more to offer to the field of machine learning. These questions do appear to be incredibly complex though and the steady-state analysis is definitely a prerequisite.\n\nResponse: Yes, the last point seems to be key. A better understanding of the brain should lead to better machine learning, but the path to get there may not be direct. At present deep learning seems to outstrip neuroscience in terms of generating intelligent behavior, and the question is why? Answering this question requires putting neuroscience into a setting where it can be compared directly with machine learning, and that is why we have pursued the question of performance on benchmarks: it should raise questions for neuroscientists that help to address the gaps that exist at present. The identification and understanding of these gaps should lead to gains in the field of neuroscience, which could then benefit machine learning as well. The present study is a somewhat humble step in this direction, but it does offer some novel components, including as far as we are aware the first network with partitioning among excitatory and inhibitory neurons that performs acceptably on benchmark tasks. We are working on temporal dynamics, but there the modeling and computation become quite a bit more complex, which is why this present research is a necessary first step.\n\n\nThank you for reviewing our paper. Our goal was not to study spiking models but to investigate a static-time realization of conductance averaging. We do consider the SNN results to be relevant and hence included several current links into the literature. SNN research brings one aspect of biological networks into artificial neural networks (spiking), while our research brings different aspects, namely, that neuron activity is naturally normalized and range limited by physical limits on the transmembrane movement of ions. We also show that modeling this aspect of neural behavior naturally leads to the introduction of inhibitory and excitatory neurons, which is an interesting recapitulation of real biological networks.\n \nIn what follows, we provide responses to some specific comments.\n \nComment: …it is inappropriate to say this proposed method is a state-of-art neuro-inspired method, ... CWA cannot outperform Affine+BN.\n \nResponse: Our claim was not that CWA was state-of-the-art, but that it was competitive with the state-of-the-art. More importantly, our goal is actually to demonstrate something about the neuroscience rather than to introduce a new practical method based on neuroscience. Thus “neuro-inspired” was not what CWA was intended to accomplish. \n \nMany if not most truly state-of-the-art results in deep learning require substantial engineering in the training methods and network architecture. We have avoided this kind of engineering in order to perform a basic controlled comparison between CWA and affine networks. The results for affine+BN demonstrate how wide this gulf is; 64% is hardly a strong result on CIFAR-10 given what we know is possible. The point is not to demonstrate superiority for CWA but rather mere competence. We want to show that basing a network on Hodgkin-Huxley inspired principles doesn’t destroy performance -- something which has not been shown before.\n \n \n  \nComment: Applying the method to MNIST and CIFAR-10 is far from being called either “interesting computer vision applications” or “difficult perceptual tasks”. They only use perceptual in the title, but the applications are MNIST and CIFAR-10. It feels like they want to learn something big, but they only focus on benchmark datasets.\n \nResponse: Object recognition is of course a perceptual task. We can see where our use of the word “difficult” in the title could set expectations in the wrong way; we have edited the title to replace the word “difficult” with “high-dimensional”, which will hopefully alleviate this issue. Please note that we cannot change the title in the system at this time, but the paper submitted has been revised. \n\nOnce again, our goal was to demonstrate that conductance averaging as performed in biological neurons can succeed at the tasks we tested, which has not been shown previously and is accomplished by these experiments. Our main goal was to test how incorporation of biophysical properties, such as those used in real neurons, affects computations produced by artificial neurons. The use of benchmark datasets is standard in work on artificial neural networks, whence the selected experiments.  Whether this will lead to “something big” can only be determined over time, but we have demonstrated that neuronal biophysical properties can be readily incorporated into computational networks, which will hopefully lead to better modeling and analysis of real biological circuits using the tools that have been developed to create and study artificial neural networks.\n \nComment: They compare nothing with other SNN type of model on other truly difficult perceptual tasks.\n \nResponse: As a first point, CWA is not a competitor to SNN models; at present, we are not aware of results in spiking neural networks that incorporate conductance averaging, which does happen in biological neurons, other than highly complex simulations in specialized programs such as NEURON and GENESIS, which have not been easy to optimize or to use to solve high-dimensional perceptual tasks. If fact, we do see a clear path by which CWA could be combined with spiking networks, or used in neuromorphic chip design in future work. CWA as formulated in this paper is static and not temporal, as can be seen from the implementing code we have added to the supplementary materials.  However, the paths to add temporal features to the network is clear from the Hodgkin and Huxley model and will be a focus of our future work.\n \nWe did not compare with SNNs because there is no simple, controlled comparison there. Biological neurons operate on spike trains, and they also perform conductance averaging. There simply is no conflict, and no comparison would be fair without also including spiking networks that incorporate conductance averaging, which we have not yet explored. \n \nThere are certainly many opportunities to include spiking with this CWA work as future work, but the point here is to merely demonstrate that conductance averaging works competitively.\n\n\nThank you for these comments and questions. We would like to emphasize that the purpose of CWA is to demonstrate that biological concerns can be realized inside of neural networks that can be trained effectively, including channel conductance normalization and excitatory/inhibitory partitioning of neurons. Although we hope these insights are useful for deep learning practitioners as well, that is not the primary goal of this work. Answers to questions follow.\n \nComment: I suggest the authors add these definitions explicitly to their paper (perhaps in supplementary), also an overview figure would go a long way.\n \nResponse: A diagram showing the basic types of connections determining the cell voltage has been added as Figure 1 based on the steady-state biological model. Although we have not been able to do so during the comment period, a glossary can also be added to the supplementary material to clarify unfamiliar terms.\n \n \nComment: Have the authors considered the computational burden of Equation 3? In short, it seems that there are two summations (one for building the probability space over measure h) and one right before e_j. This is somewhat important, if this type of neural network is presented as a competitor to affine mapped activations.\n \nResponse: A PyTorch implementation of CWA layers has been added to the supplementary material. Our implementation of fully connected CWA has two matrix multiplies instead of the usual one matrix multiply for affine networks. Similarly, convolutional CWA has two convolutions instead of one. In our experiments, CWA is about 1.5x slower to run than affine networks overall. CWA is not proposed as a competitor to affine networks, but rather as an idealized model of the computation occuring in biological neurons, which may be useful both for neuroscientists and deep learning practitioners.\n \nComment: It would be nice to have some proof regarding universal approximation capabilities of CWA.\n \nResponse: At this time we do not have a proof of universal approximation capabilities, though we would be surprised if CWA did not have such capabilities, since it reflects the operation of the brain. The main reason for proposing CWA is that it reflects the way neurons in the brain use conductance pathways to perform computations. \n \nIn terms of mathematical tools, it is generally true that every point within a convex set is the barycenter (expected value) of some probability distribution over its extreme points (the Choquet theorem). In this case, the e_j’s take on values in {-1,+1}^d where d is the size of the layer, and this includes all the extreme points of the convex set [-1,+1]^d representing the output voltages. Therefore all possible voltage outputs can be obtained. It remains to show that the inputs are sufficient to generate the necessary distributions in the course of one or more CWA layers. We leave this problem for a later time.\n \nComment: I was a bit confused to see CWA+BN in the Table 1.\n \nResponse: BN adds extra parameters to the network, which may explain why CWA+BN behaves better than CWA alone.  As a normalization technique, BN seems to outperform CWA. However, CWA has a biological implementation, which BN definitely does not. It should be noted in this context that CWA outperforms affine networks without BN in the convolutional experiments on CIFAR-10. This demonstrates that CWA does provide some of the benefits of normalization. In the fully connected case, the best performer is an affine network with no normalization at all, so that we cannot draw a strong conclusion regarding normalization from the CWA+BN result.\n \nComment: Essentially, the CWA changes the definition of a layer in a neural net. Do authors see a path from “CWA works” to “CWA works better than affine?”.\n \nResponse: Although an interesting question, this has not been our main focus and we see no such path at this time, though there may be one. One possibility is to consider more diverse patterns of connectivity, using the brain as a guide. Inhibitory and excitatory connections are not equally treated in the brain, but appear in specific patterns that we have not replicated in this work. Furthermore, networks in the brain generally have more connections, which may make the CWA normalization less sensitive to random variations in the input. Also, if neuroscientists are going to create more realistic hybrid biological neuronal models that do useful things they will have to incorporate CWA principles to be functioning like real neurons; thus CWA is intended as an example for use by neuroscientists. CWA may also prove useful for physical implementation in neuromorphic chips as well, since it can be reduced to a basic circuit without a need for an ALU.\n"
            }
        ]
    },
    {
        "id": "SklEhlHtPr",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: DeepPCM: Predicting Protein-Ligand Binding using Unsupervised Learned Representations\nKeywords: Unsupervised Representation Learning, Computational biology, computational chemistry, protein-ligand binding\nAbstract: In-silico protein-ligand binding prediction is an ongoing area of research in computational chemistry and machine learning based drug discovery, as an accurate predictive model could greatly reduce the time and resources necessary for the detection and prioritization of possible drug candidates. Proteochemometric modeling (PCM) attempts to make an accurate model of the protein-ligand interaction space by combining explicit protein and ligand descriptors. This requires the creation of information-rich, uniform and computer interpretable representations of proteins and ligands. Previous work in PCM modeling relies on pre-defined, handcrafted feature extraction methods, and many methods use protein descriptors that require alignment or are otherwise specific to a particular group of related proteins. However, recent advances in representation learning have shown that unsupervised machine learning can be used to generate embeddings which outperform complex, human-engineered representations. We apply this reasoning to propose a novel proteochemometric modeling methodology which, for the first time, uses embeddings generated via unsupervised representation learning for both the protein and ligand descriptors. We evaluate performance on various splits of a benchmark dataset, including a challenging split that tests the model’s ability to generalize to proteins for which bioactivity data is greatly limited, and we find that our method consistently outperforms state-of-the-art methods."
            },
            {
                "round2": "Reviewer 1:\nThe authors present a model with state-of-the-art performance for predicting protein-ligand affinity and provide a thorough set of benchmarks to illustrate the superiority of combining learned low-dimensional embedding representations  of both ligands and proteins. The authors then show that these learned representations are more powerful than handcrafted features such as circular fingerprints, etc. when combined into a model that jointly takes as input both the ligand and protein.\n\nI originally suggested to accept the paper, but agree with the other reviewers that the novelty of the work in this paper likely doesn't meet the bar for acceptance given that the most significant contributions of this paper are around combining good ideas from other papers without much additional novelty.\n\nMajor comments\n\nUnless I'm misunderstanding the naming conventions used in Table 1, it seems like an omission not to include the performance of DeepPCM+HP+HC in Table 1 to more fully decouple the performance due to the DeepPCM architecture versus the contribution due to using both unsupervised descriptors in combination.\n\nI might expect the performance of the DeepPCM + HP + HC model (not currently shown unless I'm missing it) to exceed the performance of the NIB + HP + HC model based upon the reasoning given in the discussion (i.e., usefulness of joint input training), even if the individual representations were inferior to the unsupervised counterparts.\n\nSince the primary point of the paper is to illustrate the power of combining the unsupervised representations, I'm surprised the aforementioned performance comparison is not prominent. That is, the performance comparison of DeepPCM+HP+HC versus DeepPCM+UP+UC seems like a very central quantity to present and discuss, but appears to be missing currently.\n\n\nMinor comments\n\nTable 1 was a bit confusing to me at first, because it appears that UD = UP + UC, and HD = HP + HC, but this wasn't obvious to me initially; I'd change the NIB rows to be NIB + HP + HC and NIB + UP + UC, and then you don't even need to define the UD and HD acronyms, which simplifies the table's cognitive load for me and also makes the relationship between the DeepPCM variants more obvious.\n\n> \"On the low-coverage-split, we also find that our method significantly outperforms the benchmark.\"\n\nIt would be good to add the %improvement inline here to parallel the other dataset splits listed just before so that you can directly compare the magnitudes.\n\n> \"using unsupervised-learned descriptors than when using handcrafted descriptors\"\n\nWould be more compelling if you added the DeepPCM + HC + HP performance to Table 1.\n\n> \"All hyperparameter optimization of our model was performed on the temporal split\"\n\nI think it would improve the paper if the extent to which various architecture choices were optimized over could be included as well; for example, which types, layer sizes, and depths of network architectures were considered in the hyperparam tuning, and any accompanying justification for these design choices.\n\n\nReviewer 2:\nThis paper tries to solve the protein-legend binding prediction problem in the computational biology field. It uses the learned embedding for protein and legend, separately, from two published papers. Then those two embeddings were inputted to another deep learning model, performing the final prediction. Tested on one dataset, it shows the proposed method can outperform the other baseline methods.\n\nThe paper should be rejected for the following reasons:\n1. The idea of the paper is not interesting and novel enough. It only used the results from two published papers and then applied another deep learning model on them. The novelty of the paper is limited.\n2. The experiment part is not that comprehensive. It indeed performs enough ablation studies. However, all the legendary methods in the computational biology field are not included in the comparison. \n3. The experiments are only performed on one dataset. Usually, for application papers, the experiments should be performed against at least 2 datasets to avoid bias.\n4. The discussion part is not well-developed. For the paper which focuses on only one task, the more in-depth discussion is expected beyond the simple discussion of the performance. For example, the authors may try to explain the result: why the model used embedding from unsupervised learning is better than the hand-crafted features. Since they shared the same model, the unsupervised embedding should contain more information. Then, what is the additional information? \n\n\nSome further questions and comments:\n1. What's the sequence similarity of the 1226 proteins?\n2. Can the model generalize well to a completely new protein? \n3. What's the detailed performance of the model on proteins belonging to different families? \n4. I guess if the authors check the detailed performance, they will find nonuniform performance across different proteins. I think the authors can also further investigate that.\n5. Can the authors train the embedding model as well as the classification model in an end-to-end fashion? This can be more interesting.\n6. One big problem of the assay data is that it would not be able to provide the structure information of the interaction between the protein and the legend. Usually, it is a very important piece of information for the biology people. The computational methods based on the assay data will also inherit the flaw. \n\nReviewer 3:\nThis paper proposes to learn representations of protein and molecules for the prediction of protein-ligand binding prediction.\n\nThe presentation of this paper is a bit lengthy and repetitive in some cases. The long descriptions of protein/drug descriptors are a nice overview,  but it may be unnecessary as the authors in the end use other works’ embedding.\n\nThe author points out that there are interpretability issues & the inability to capture the shape of the substructures with previous ligand descriptors, however, it seems that CDDD also is not interpretable and could not capture the shape as it operates on SMILES strings, although seems to have better predictive performance.\n\nFor the protein descriptor, the author is missing several important descriptors that may not have the issues mentioned such as Protein Sequence Composition descriptor. \n\nThe technical novelty is very limited. It seems the usage of CDDD and UniRep are its only difference from previous works such as DeepDTA, WideDTA, DeepConv-DTI, PADME and CDDD and UniRep are also from other works. It may be more suitable for a domain journal instead of ICLR which focuses on method innovation. \n\n\nThe experimental setup is solid with realistic considerations. However, it is missing many baselines such as DeepDTA, WideDTA, DeepConv-DTI, PADME and more classic methods such as SimBoost and KronRLS. "
            },
            {
                "round3": "- Writing: Table 1 should appear in the results instead of the methods, and the discussion should not introduce new methods (e.g. evaluation of the individual impact of each descriptor type, in the second paragraph of discussion)\n\nWe were not sure where to put the evaluation of the individual impact of each descriptor type, since it is a method but is more for investigative purposes rather than a key point of our results. It seemed redundant to explain it in both the methods and then in the discussion. We will make changes in the revision to improve the flow while ensuring the discussion does not introduce new methods. \n\n\n* UniRep was used for the embedding of proteins, but another method is available (SeqVec). We suggest also trying this encoder and see if there is any improvement over using UniRep. We acknowledge a conflict of interest, as SecVec was produced by our lab (but in another ICLR open review publication (paper) SeqVec seemed to spawn better results then UniRep for the problem of GO annotation prediction) \n\nThank you for this suggestion. We were unaware of the SeqVec – there seem to be several new protein embeddings coming out in the past year, and we felt that benchmarking across all these different embeddings would be out of the scope of this paper. We will evaluate on SeqVec and if the results on SeqVec are better, we will use these results for the paper instead of UniRep. \n\n\n* For the test sets: create three different test sets based on difficulty / novelty of the proteins, e.g. based on similar to the different test sets evaluated on protein-protein interaction prediction information (Evolutionary profiles improve protein–protein interaction prediction from sequence, Hamp&Rost, 2015)\n\nWe thank the commenter for this suggestion – these types of test sets would make the evaluation much stronger. We will explore the addition of more strictly designed test-sets as demonstrated in the paper. \n\n\nWe thank the commenters for their input and for their helpful suggestions for further directions to improve the paper. We address the specific comments below. \n\n- The authors claim that there might be similar data items in the random train and test splits (as similar chemicals or proteins might not be accounted for randomly). However, in the temporal splits there is no way of telling that there is no similarity between a compound being analyzed in the past w.r.t. more recently, leading us to believe that temporal splits address only partly the bias between training and testing\n\nWhile the commenters are correct that the temporal split may not fully address the bias between training and testing, we include the temporal split in the results for two reasons\n     1. The temporal split creates a more realistic test set, since in real-world usage of PCM models one would train on all currently available data. This split simulates this case. \n     2. The temporal split was used by Lenselink et al to benchmark their handcrafted-descriptor models – for the sake of comparability we stick to the same evaluation schema. \nOn revision, we will include these details – in particular to state that the temporal split may not fully address the bias between train and test, but that we include it because it is a realistic split. \nWe note that we are aware of more rigorous splitting schemes based on protein sequence similarity, and of the contribution of Rost & Sander to these ideas in their seminal work from 1993 (https://www.ncbi.nlm.nih.gov/pubmed/8356056)\n\n\n- Validation set for temporal split: it's not clear how the authors split the temporal splits for validation, training and testing. Especially, as this split will very likely have the same bias as the authors describe for the random split\n\nWe apologize for not clearly explaining the construction of the validation set for the temporal split. We used the same split procedure as Lenselink et al, where the test set contains data generated from assays done in 2013 and later, while the training set and validation set is constructed by randomly splitting the pre-2013 data. By ensuring both training and validation is done on pre-2013 data, we seek to closely mimic the real-world case of training on all currently available bioactivity data, and to avoid the bias from random splitting as much as possible.  We will make this clearer in revision.\n\n\n- It is not clear how the authors try to calculate the standard error and how they apply bootstrapping. This might be due to unclear wording or lack of details \n\nWe apologize for our unclear writing in this section. For the random split, each bootstrap set is constructed by separating a random test set, then separating a random validation set from the remaining data, then resampling with replacement from the training set to create our bootstrap training set. For the temporal split, the test set is fixed, containing all bioactivity data generated from assays done in 2013 and later. Then, in each set we randomly partition the pre-2013 data into a training and validation set, then resample with replacement from the training set to create our bootstrap training set. In all cases, we also save our random seeds for future reproducibility. The standard error we report is then computed from the standard deviation of the performance of the bootstrap samples. In revision we will make this section more clear and include the details in the main body or in the appendix. \n\n\n- Low coverage splits: why wasn't a traditional approach (e.g. homology reduction) used for this? \n\nWe ask the commenter to elaborate what they are referring to as homology reduction, other than the mathematical concept in algebraic topology. Do they mean some technique involving construction of a hold-out set where the hold-out set comprises a distinct family of proteins excluded from the training set?\n\n\n- Table 1: the results on the low coverage splits are missing, making it difficult to easily compare across different approaches \n\nThe low-coverage split does not lend itself to computing standard errors in the same manner as the temporal or random splits, since we already create several splits of the data – based on which five proteins have been assigned to the test set. Also, in this case, we find the average performance is not as useful as visualizing the difference in performance with respect to each protein."
            }
        ]
    },
    {
        "id": "HyeG9lHYwH",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Compression without Quantization\nKeywords: Image Compression, Bits-back efficient, Quantization\nAbstract:  Standard compression algorithms work by mapping an image to discrete code using an encoder from which the original image can be reconstructed through a decoder. This process, due to the quantization step, is inherently non-differentiable so these algorithms must rely on approximate methods to train the encoder and decoder end-to-end. In this paper, we present an innovative framework for lossy image compression which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e. it is bits-back efficient. The result is a principled, end-to-end differentiable compression framework that can be straight-forwardly trained using standard gradient-based optimizers. To showcase the efficiency of our method, we apply it to lossy image compression by training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates."
            },
            {
                "round2": "Reviewer 1:\nThank you for the rebuttal. \n\nWe agree that even though the competing method [1] uses quantization, it does not seem to suffer from it. You then argue that the other reason for removing quantization is that it circumvents the restriction for uniform distributions of the posterior, and that therefore your method could potentially surpass the results of [1]. If this is the motivation for your work, then you should demonstrate that by using flexible posteriors in your method, you actually improve upon the results of [1] or come close to it. This is currently not demonstrated in the paper, and I therefore retain my score.\n\n\nReviewer 2:\nThe paper proposes a method for lossy image compression. Based on the encoder-decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end-to-end way.\n\nOverall, I think the current version should not be accepted. \n\nThe detailed comments are as follows.\n\n1. The novelty is not clear. Using continuous latent space has been analyzed for years. So does the negative beta-ELBO . Section 4 starts with importance sampling, then some implementation compromises, which makes the efficacy of resultant method unclear.\n\n2. You mention that REC achieves bits-back efficiency according to (Hinton 1993). However, how is c* selected in their paper? Now that you use importance sampling, does this still hold?\n\n3. The experiments are very insufficient. It only compares with one method, which is not enough. From the main content, the proposed method is improved upon (Havasi 2018). But it is not compared.  Let alone the other methods mentioned in the related works. PNG and JPEG should also be compared. \n\n4. For the reconstruction, it is better to measure the quality quantitively by PSNR, and mark the Bits/Pixel. \n\n5. Ablation study is also needed. PLN without your contributed part should be evaluated alone. So far, I cannot tell how your method works and which part works. \n\n6. There are many broken sentences and typos.\n\n7. Some statements such as the application parts should be properly cited.\n\nLast, the paper may be entitled as 'image compression without quantization' instead. \n\nReviewer 3:\nThis paper studied the image compression problem. Specially, the authors proposed to use neural networks to act as encoder / decoder. The training consists of minimizing the distortion of reconstruction and the difference between approximated posterior and true distribution. The output of encoder is sampled by the proposed relative entropy coding, which extended a previous method by introducing adaptive grouping for acceleration.\n\nIn summary, this paper gets rid of commonly used quantization techniques in image compression by using an approximate importance sampler which produces the encoding of images in a non-deterministic manner. With the construction of parameterized encoder / decoder, end-to-end training is conducted by popular gradient descent.\n\nHere is a question:\nIn experiments, the authors mentioned that the architecture is borrowed from another work. My question is how neural network architecture affects the performance? In other words, how to ensure that the performance is not obtained from the power of the backbone but from the proposed method itself. Are there any possible experiments which can be conducted to show the effectiveness by using different architectures?\n\n\nReviewer 4:\nThe authors propose a new image compression method that does not require quantizing the encoded bits in an auto-encoding style image compression model. The method builds on a VAE with image x, code z, and posterior q(z | x). Instead of directly encoding z, the proposed method samples z_{c^\\star} from posterior and store c^\\star as the compressed representation. A decoder then reconstruct an approximation of x from z_{c^\\star}. The authors show that this framework is bits-back efficient and draw connections to prior theoretical results. Experiments were conducted on the Kodak dataset based on the model of Balle et al. The proposed method works only slightly worse than Balle et al. at low bit-rate region, but the gap becomes larger in higher bit rate regions. \n\nThe method is technically sound and the paper is clearly written. My main concerns fall in practical aspects. In Figure 3, for 3 out of 4 images, the theoretical upper bound of the proposed method still do not outperform Balle et al. This suggests limitations of the proposed method. Discussion on the limitations of the method is limited. The results also beg the question: How much does quantization in existing methods impact performance, and how much will fixing this benefit the overall system. Finally, in my opinions, in some sense the sampling of z_{c^\\star} is also a form of quantization. Does drawing different samples from posterior leads to different reconstructed images? If it does, doesn't it also suffer from similar limitations as existing \"quantization\" methods? \n\nOverall, I think the direction of the proposed method has good potential, but it also leaves important questions unanswered. I think this paper will benefit from additional revisions. \n\nReviewer 5:\nSummary:\nThis paper aims to circumvent the quantization step and associated gradient approximations for compression algorithms that make use of entropy coding for compression. Entropy coding requires a probability mass function over discrete symbols. As an alternative approach the authors adapt the MIRACLE algorithm by Havasi et al. (2018), which was originally used to compress Bayesian neural networks, to work for lossy source compression with probability density functions over continuous latent variables.\nThe algorithm is based on taking several importance samples from the prior p(z) with the encoding distribution q(z|x) as the target distribution. The number of samples is equal to the exponent of the KL between the encoding and prior distribution, which can quickly grow to an uncontrollably large number. The index of the sample with the maximum importance weight is used as the code for the original image. If both sender and receiver have access to the same random sampler, the receiver can reproduce the sample by drawing a number of samples equal to this index, and decoding the last sample. Similar in spirit to Havasi et al, the authors take several steps to ensure that the KL divergence (and therefore the number of samples) does not become prohibitively large. The compression performance is evaluated by training the proposed model and the competing neural network-based method [1] on the Clic dataset, and evaluating it on a subset of the images of the Kodak dataset. JPEG is also used as a baseline. The authors show results achievable if coding using the relative entropy was perfect (denoted with ‘theoretical’), and the practically achieved compression performance (‘actual’). \n\nDecision:\nWeak reject: although the idea of circumventing the quantization step required by the use of entropy coders is certainly valid and interesting, the results in the paper show that the resulting compression performance is worse than the competing method that does quantize. Moreover, the encoding time is significantly longer than this same baseline. \n\nSupporting arguments for decision:\nAlthough the motivation for circumventing the quantization step seems plausible, the authors show no evidence that the competing method [1], which does perform a post-training quantization step, actually suffers from it. The authors even state on page 8 that “ Most notably though, they only used the continuous relaxation during the training of the model, thereafter switching back to quantization and entropy coding, which, they show does not impact the predicted performance, and hence confirming that their relaxation during training is reasonable.“ If post-training quantization is reasonable, then this overthrows the entire motivation. More importantly, the “theoretically” achievable results of the proposed method in seem only competitive in the low-bit rate regime and worse in the higher bit rate regimes. Even more, the “actual” practically achieved compression results are worse than [1] and also considerably worse than the “theoretically” achievable compression results. The authors do provide a reason for why the “theoretical” and “actual” results are so far apart, but are unfortunately not able to overcome this issue.\nIn the conclusion the authors honestly admit that the runtime of their method is much slower than the competitors (1-5 min for proposed method vs ~0.5 s  for [1] for encoding times). I appreciate that the authors mention this. The authors then state “improving the rate factor and the run-time does not seem too difficult a task, but since the focus of our work was to demonstrate the efficiency of relative entropy coding, it is left for future work.” I do not think this paper demonstrates the efficiency of relative entropy coding, the results simply don’t support this claim, and I therefore think that stating that the issues seem not too difficult to overcome is insufficiently convincing. \n\nThe quality of the empirical study can be improved. Another neural network-based compression baseline would make the empirical evaluation of the proposed method more insightful. Now we only see that the result is worse than [1], but it would be good to know how it compares to other baselines such as [2]. Furthermore, the paper does not show compression results aggregated over the entire Kodak dataset, but rather picks 2 images for the main part of the paper, and shows 3 in the appendix. Showing aggregate results gives a more robust estimate of the performance. Individual image results can just be put in the appendix. \n \n\nAdditional feedback to improve paper (not part of decision assessment):\n- In section 5 on the dependency structure of latents in the ladder VAE: is it really necessary to indicate the dependency structure with “topological structure”? Seems unnecessary to me as dependency structure is a clear enough description already without making it sound overly complicated.\n- Page 9: “Further, our architecture also only uses convolutions and deconvolutions as non-linearities.” Convolutions and deconvolutions are not non-linearities.\n- Fig 2: I’m not sure if this figure is relevant enough for such a prominent placement in the paper. It doesn’t discuss anything relevant to the contributions claimed in this paper. \n- I can’t find a definition of O in line 3 of “procedure” in algorithm 2 and in the return statement.\n\n\n[1] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. ICLR 2018.\n[2] Lucas Theis et al.  Lossy Image Compression with Compressive Autoencoders. ICLR 2017\n"
            },
            {
                "round3": "We thank the reviewer for their detailed comments on our work.\n\nWe agree with the reviewer that in our paper we show no evidence that competing methods suffer from quantization, in particular, we do not believe that they suffer from it.\n\nOur work is simply focussed on an alternative, novel approach to lossy compression, that allows a much wider class of algorithms to be used as the encoder and decoder. Concretely, previous approaches required specific assumptions about the compression pipeline (e.g. what kind of quantization is performed. In [2] it is assumed to be rounding, and hence its derivative is replaced by a continuous relaxation that the authors had to choose) or the model (e.g. [1] relies on the latent posteriors to be uniform distributions such that their terms cancel in the ELBO). In contrast, our method works for any generative model where an approximate posterior for the latents is available. This means that an arbitrary valid VAE could be used in our method, with complete freedom of choice for both the latent posteriors and priors.\n\nWe, therefore, believe that the removal of restrictions on the latent space's distributions is a strong motivation, and by using a more flexible family for our VAE, the results of [1] could be surpassed.\n\nIndeed, the model performance degrades more at higher rates than the performance of [1]. A simple explanation might be that of model capacity: our model has been trained with much fewer latent filters than what [1] used (e.g. we used 24 latent filters as opposed to the 128 and 196 that Balle used for lower and higher rates, respectively).\n\nWe agree that a better empirical study could be performed. The main difficulty we found was assessing how different training sets might impact the model performance, and hence decided that the fairest comparison would be if all models were trained on the same dataset. Sadly, this limits the comparisons to works where the code to achieve the reported results is freely available, which was only true in the case of [1].\n\nWe opted to report single image statistics only as we believe that aggregate statistics are not necessarily meaningful [3], though we agree that it might provide a more robust idea of model performance, and hence we will report aggregate results in the next draft.\n\nWe thank the reviewer for additional feedback to improve the quality of our writing.\n\n[1] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. ICLR 2018.\n[2] Lucas Theis et al.  Lossy Image Compression with Compressive Autoencoders. ICLR 2017\n[3]  Johannes Ball´e, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. ICLR 2016.\n\n\nWe thank the reviewer for their comments on our work. We appreciate the thoughtful feedback.\n\nWe agree that the discussion on the limitations of the method is perhaps a bit too limited and could be expanded so that the current challenges of applying our method are clearer. Indeed, even the theoretical upper bound falls off compared to Balle's results. Note that we use Gaussian priors on both levels of our VAE, in particular, standard Gaussians on the second stochastic level, that are not adjusted for the dataset. This is in contrast to Balle's approach, where they utilize a very flexible non-parametric prior for each dimension for their second stochastic level, and separately from the model, they also optimize it for the dataset. We conjecture that the reason this does not adversely affect their test performance is due to the large dataset used for training (~ 1 million high-resolution images). \n\nThe extent to which quantization (or the various relaxations of quantization for training) is an interesting question. Figure 4 in [1] shows a nice comparison of the actual quantization error versus the continuous estimate using uniform dither, as well as the approximation quality of the differential entropy used in training with the actual compression rate. It shows that both relaxations are very accurate, the main limitation of their method is that they are constrained to VAEs where the posteriors are always shifted uniform distributions, whereas our method allows the use of arbitrary posteriors.\n\nWe do not necessarily agree that the importance sampling algorithm is a form of quantization. Quantization is (usually) a rounding operation, used to assign non-zero mass to the symbols we wish to compress. The importance sampling procedure, on the other hand, serving as part of the relative entropy coding scheme, is lossless.\n\nDrawing different samples from the posterior distribution will naturally cause some variation in the output of the (deterministic) decoder, but VAEs have been demonstrated to be robust to noise on their stochastic level [2], and thus if the difference is not beyond floating-point precision, it certainly is beyond human perceptibility.\n\n\n[1]  Johannes Ball´e, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.\nICLR 2016.\n\n[2] Bin Dai, Yu Wang, John Aston, Gang Hua, & David Wipf. Connections with robust PCA and the role of emergent sparsity in variational autoencoder models. JMLR 2018\n\n\nThank you for your feedback on our paper.\n\nOur contribution is the coding scheme, and how it can be used in conjunction with generative models. Thus, our main focus was not to find a good VAE architecture, hence we adopted the architecture of [1].\n \nWe understand that the choice of architecture in our setting is crucial to good performance, and we chose it precisely to be able to compare the performance of the appropriate trade-offs we made compared to the trade-offs of [1]. Concretely, note that we have usual Gaussian latent space on both levels of our network, which we use for relative entropy coding, whereas [1] have uniform posteriors on both levels, a uniform-Gaussian convolution as the first-level prior and a non-parametric prior on the second, \nthat is optimized for the data separately.\n\nPerhaps the most obvious comparison that was left out would be with [2]. The issue is with comparability, namely that [2] trained on a custom dataset and their code was not available so that it could be retrained on the same dataset we trained our model on.\n\n\n[1] Johannes Ball´e et al. Variational image compression with a scale hyperprior. ICLR 2018.\n\n[2] Lucas Theis et al.  Lossy Image Compression with Compressive Autoencoders. ICLR 2017\n\n\nWe thank the reviewer for providing feedback on our paper, we address each point below:\n\n1. The reviewer is right, continuous latent spaces and using the beta-ELBO as training objective have been studied extensively. The novelty of our work is rather in the compression part, where we show how continuous latent distributions could be used for lossless compression of latent variables, as part of a lossy compression pipeline.\n\nOur work is in contrast with all previous neural compression methods, which all used probability masses and entropy coding.\n\nSecond, we also adapt the REC algorithm developed by [1] for BNN compression, to compress the latents of generative models. These adaptations are necessary since i) the structure BNN weights' posterior distribution will differ from the structure of a datapoint's posterior in a generative model, and ii) [1] make use of successive retraining during the compression process, which is far too expensive for a compression codec.\n\n2. In the work of [2] only the possibility of such a compression method is demonstrated. In our work, we provide a realization of this using our importance sampler.\n \n3. We agree that more experiments could be performed, e.g. with [3], and across multiple media, e.g. audio or video compression as well. The principal reason for the lack of more experiments was the issue of comparability: there does not seem to be a clear consensus for what training set to use, hence for best comparison we sought to report methods where we could train on the same data as we used for our model and found that of the relevant sources only [4] had their code publicly available. \n\nAs mentioned earlier [1] have developed their method for BNN compression, but it does not extend to the lossy data compression setting.\n\nComparison to PNG is not quite relevant in our setting, as it is a lossless image compression algorithm and we work in the lossy setting. \n\nWe compare the performance of our method with JPEG in Figures 3, 7, 8 and 9 on various images from the Kodak dataset.\n\n4. We show PSNR comparisons in the above-mentioned figures, as well as MS-SSIM,  plotted against the compression rate measured in bits per pixel.\n \n5. Our architecture is the one used in [4], our contribution is the compression of the latent variables.\n\n6-7. We would appreciate it if you were able to provide concrete examples where changes are needed.\n\n\n[1] Marton Havasi, Robert Peharz, and Jos´e Miguel Hern´andez-Lobato. Minimal random code learning:\nGetting bits back from compressed model parameters. NIPS workshop on Compact Deep Neural\nNetworks with industrial application, 2018.\n\n[2] Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the description\nlength of the weights. In Proc. of the 6th Ann. ACM Conf. on Computational Learning\nTheory. Citeseer, 1993.\n\n[3] Lucas Theis et al.  Lossy Image Compression with Compressive Autoencoders. ICLR 2017\n\n[4] Johannes Ball´e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational\nimage compression with a scale hyperprior. In International Conference on Learning Representations,\n2018."
            }
        ]
    },
    {
        "id": "rJxe3xSYDS",
        "decision": "Accept (Poster)",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Extreme Classification via Adversarial Softmax Approximation\nKeywords: Extreme classification, negative sampling\nAbstract: Training a classifier over a large number of classes, known as 'extreme classification', has become a topic of major interest with applications in technology, science, and e-commerce. Traditional softmax regression induces a gradient cost proportional to the number of classes C, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due a poor signal-to-noise ratio. In this paper, we propose a simple training method for drastically enhancing the gradient signal by drawing negative samples from an adversarial model that mimics the data distribution. Our contributions are three-fold: (i) an adversarial sampling mechanism that produces negative samples at a cost only logarithmic in C, thus still resulting in cheap gradient updates; (ii) a mathematical proof that this adversarial sampling minimizes the gradient variance while any bias due to non-uniform sampling can be removed; (iii) experimental results on large scale data sets that show a reduction of the training time by an order of magnitude relative to several competitive baselines.\n"
            },
            {
                "round2": "Reviewer 1:\nThis work addresses the problem of training softmax classifiers when the number of classes is extreme. The authors improve the negative sampling method which is based on reducing the multi-class problem to a binary class problem by introducing randomly chosen labels in the training.  Their idea is generating the fake labels nonuniformly from an adversarial model (a decision tree). They show convincing results of improved learning rate.\nThe work is very technical in nature, but the proposal is presented in detail and in a didactic way with appropriate connections to alternative methods, so that it may be useful for the non-expert (as me).\nThat is the reason why I recommend to accept this work: even not being an expert I found the paper educative in introducing the problem and interesting in explaining the proposal. \n\nReviewer 2:\nThis paper focuses on efficient and fast training in the extreme classification setting where the number of classes C is very large. In this setting, naively using softmax based loss function incurs a prohibitively large cost as the cost of computing the loss value for each example scales linearly with C. One way to circumvent this issue is to only utilize a small subset of negative classes during the loss computation. However, uniformly sampling this subset from all the negative classes suffers from the slow convergence as such sampled negatives are not very informative for the underlying classification task. \n\nThe paper proposes a method to sample the negatives in a non-uniform manner. In particular, given an example, an adversarial auxiliary model that is tasked with tracking the data distribution samples the hardest (adversarial) negatives for the example. The proposed method to sample negatives has a computational cost log(C) and reduces the noise in the gradient. The authors then demonstrate the utility of their proposed approach on two well-established extreme classification datasets, i.e., Wikipedia-500K and Amazon-670K. The proposed method shows improvement over some natural baselines in terms of the wall-time for the convergence of the training process.\n\nComments\n\n1. The paper has some nice contributions and discusses the key ideas in reasonable detail. However, the reviewer feels that the authors gloss over many relevant prior works and fail to put their results in the right context. There has been quite a bit of work on non-uniformly sampling \"hard\" negative classes. For example, see [1], [2], [3], [4]. In fact, [3] and [4] propose methods to sample negatives from a distribution that closely approximates the softmax distribution at the cost that scales logarithmically in C, essentially providing the hard negative without having to keep an auxiliary model. Can the authors discuss their work in the context of these works?\n\n[1] Reddi et al., Stochastic Negative Mining for Learning with Large Output Spaces.\n[2] Grave et al., Efficient softmax approximation for GPUs.\n[3] Blanc and Rendle, Adaptive Sampled Softmax with Kernel-based Sampling.\n[4] Rawat et al., Sampled Softmax with Random Fourier Features.\n \n2. In experiments, the authors do not include the performance of softmax loss (eq. (1)) due to its large computational cost. However, it would be nice to compare the proposed method with eq. (1) at least for slightly smaller datasets from the extreme classification repository. \n\n3. In Sec. 4, \"We formalize and proof...\" --> \"We formalize and prove...\"\n\n4. In Sec. 1, \"We present experiments on several two classifications...\" ---> \"We present experiments on two classifications...\"\n\n5. Table 1 seems to have some typos. E.g., N is the same for both the data sets. Please fix these issues.\n\nReviewer 3:\nThe paper presents a method for negative sampling for softmax when dealing with classification of data to one from a large number of classes. Its main idea is to negative sample those classes which lead to higher signal to noise ratio than for uniform negative sampling. This is based on building an auxilary model using decision tree from which the adversarial negative classes are sampled, so that the distribution of the negative samples can be close to the positive ones leading to higher SNR while training. The proposed method is compared to other methods for negative samping on two publicly available large-scale datasets from the extreme classification with XML-XNN features. \n\nPositives :\n1. The proposed approach with adversarial negative sampling using an auxilary model seems interesting \n2. It scales well to datasets with large number of classes.\n\nNegatives :\nThe experimental evaluation of the proposed approach lacks completeness and does not look convincing for the following reasons : \n1. It misses out a recent state-of-the-art method (Slice) for negative sampling on same datasets [1], which also addresses the same problem of sampling most promising negative classes but in a different way. Furthermore, [1] also compares against many other sota methods missed out in this paper on the many other datasets datasets including those in this paper but in a more general multi-label setting.\n2. The paper only compares against other negative sampling approaches such as AandR, NCE, and does not show what happens when no negative sampling is done such as done in (DiSMEC) [2]. This is important to understand what (if at all) is lost by doing approximation as proposed. For instance, a quick experiment reveals that DiSMEC can give about 19% accuracy on Wiki500 dataset, which is better than that achieved by the proposed method. Though it is computationally expensive but due to its simplicity, it must be discussed nevertheless to give a complete picture.\nInstead the OVE baseline used in the paper seems quite sub-optimal in the first place, and hence stronger baselines [1,2] for which the code and results are readily available and have been duly tested in the community must be used and discussed.\n\nAnother aspect that the paper misses out is the role of fat-tailed distribution [3,4] of the instances among labels, which is a property of typical datasets in this regime. It is possible that one can get good accuracy but poor performance on tail-labels due to approaximations. The performance on tail-labels on appropriate metrics other than accuracy, such as MacroF1, should be evaluated.\n\nAlso, the proposed approach must be tested on more datasets including the smaller ones such as EURLex (also used in works referenced in the paper) on which it is easier to compare with other methods (such as DiSMEC, Slice and AttentionXML [5]) without encountering computational constraints and also bigger ones such as Amazon3M, also avilable from the repository. \n\nFinally, it must be investigated if the proposed method can be extended to the multi-label setting or are there inherent limitations of the model in this setting. The possibility to extend it to the general multi-label setting would make this approach more promising and directly comparable to wide range of algorithms.\n\n[1] H. Jain,  V. Balasubramanian,  B. Chunduri and M. Varma, Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches, in WSDM 2019.\n[2] R. Babbar, and B. Schölkopf, DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification in WSDM, 2017.\n[3] H. Jain, Y. Prabhu, and M. Varma, Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications in KDD, 2016.\n[4] R. Babbar, and B. Schölkopf, Data Scarcity, Robustness and Extreme Multi-label Classification in Machine Learning Journal and European Conference on Machine Learning, 2019.\n[5] AttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks, NIPS 2019"
            },
            {
                "round3": "We thank the reviewer for pointing us to an extensive list of literature. We admit that related work in the wider field of extreme classification is not sufficiently acknowledged in the Related Work section of our paper and we will add a discussion that includes the suggested references in the final version of our paper. We would like to clarify that our paper is about negative sampling and we would like to stress the contributions in this context.\n\nFirst, we believe that our paper significantly advances the theoretical understanding of negative sampling, with practical consequences. We would like to point out in particular Theorem 2. To the best of our knowledge, this is the first formalization and rigorous proof of the intuition that “hard” negative samples are “better”. While this intuition has been invoked in the literature before, the notions of “hard” and “better” are usually somewhat fuzzy. Our paper formalizes this intuition by defining a well-motivated scalar measure of the signal-to-noise ratio and proving rigorously that this ratio is optimal for negative samples that are “hard” in a well-defined way. This theoretical insight has practical consequences as it allowed us to design a very simple yet effective way to generate near-optimal negative samples.\n\nSecond, we provide experimental results on two established benchmarks and compare against five baselines. We agree that three of our baselines are variants of negative sampling. This is because our paper proposes a simple improvement of negative sampling.\nNegative sampling is a very popular method in practice due to its simplicity (for example, it is very common in the knowledge graph embedding literature).\n\nFinally, the auxiliary model proposed in our paper has merit on its own. It is a simple model that can be fitted deterministically and highly efficiently, requiring tuning of only a single model hyperparameter and no hyperparameters for the training schedule.\n\nGiven these contributions, the paper’s focus on negative sampling, and the already extensive baseline comparisons, we respectfully disagree with the reviewer’s request for more comparisons to non-negative-sampling methods. We acknowledge that the extreme classification community has developed many algorithms with high predictive accuracy. We hope that the community will appreciate that our paper proposes a very simple, theoretically well-founded, and (as the reviewer acknowledges) faster alternative that significantly outperforms very popular approaches similar to it. We also believe that single-label classification is of enormous practical relevance, and that our proposal should not be dismissed on the grounds that it cannot also perform multi-label classification.\n\nHowever, we would like to thank the reviewer for their idea to run experiments on the smaller EURLex data set. Although negative sampling in this regime is somewhat artificial, a smaller data set allows us to compare against full softmax classification, providing insight into the approximation gap due to negative sampling in general. We are running these experiments and will report results in the final version of the paper. We apologize that we could not finish these experiments in time for the rebuttal deadline.\n\nWe thank the reviewer for pointing us to relevant additional literature. While we believe that there is a confusion between “negative sampling” (used in our paper) and “sampled softmax” (used in [3] and [4]), see below, we still find the references relevant and we uploaded a new version of our paper that discusses them in the Related Works section. The updated version also fixes the grammar errors and issues with Table 1 kindly pointed out by the reviewer.\n\nWe like the reviewer’s idea of evaluating our method on smaller data sets where the full softmax loss can be optimized. Although negative sampling in this regime feels somewhat artificial and we don’t expect it to perform as well as real softmax classification, such experiments provide insight into the approximation gap due to negative sampling in general. We are running experiments on the smaller EURLex data set and will report results in the final version of the paper. We apologize that we could not finish these experiments in time for the rebuttal deadline.\n\nBefore addressing the issue of “negative sampling” vs. “sampled softmax” (Refs. [3] and [4] in the review), we would like to stress the theoretical contributions of our paper, in particular Theorem 2. To the best of our knowledge, our paper provides the first formalization and rigorous proof of the intuition that “hard” negative samples are “better”. While this intuition has been invoked in the literature before, the notions of “hard” and “better” are usually somewhat fuzzy. Our paper formalizes this intuition by defining a well-motivated scalar measure of the signal-to-noise ratio and proving rigorously that this ratio is optimal for negative samples that are “hard” in a well-defined way. This theoretical insight has practical consequences as it allowed us to design a very simple yet effective way to generate near-optimal negative samples.\n\n\nNEGATIVE SAMPLING VS. SAMPLED SOFTMAX\n\nReferences [3] and [4] in the review both discuss nonuniform sampling for a method called “sampled softmax”, which is related but different from negative sampling. The main difference is that “sampled softmax” is biased even under a uniform distribution, whereas “negative sampling” with a uniform noise distribution is unbiased. References [3] and [4] thus use a nonuniform sampling distribution for bias reduction whereas our paper uses it for variance reduction.\n\nIn detail, sampled softmax directly approximates the sum over all classes in the softmax loss function (Eq. 1 in our paper) by sampling. This introduces a bias since the sum appears inside the logarithm, which is nonlinear. By contrast, negative sampling does not directly approximate the softmax loss function. Instead, it estimates a different loss function, namely for binary classification (Eq. 2 in our paper). Although the loss function is very different, minimizing it yields the same trained model parameters as minimizing the softmax loss function, as we show in Theorem 1 (in the nonparametric limit). In this sense, negative sampling approximates softmax classification.\n\n\n> In fact, [3] and [4] propose methods to sample negatives from a distribution that closely\n> approximates the softmax distribution [...] essentially providing the hard negative [...]\n\nTo our understanding, [3] and [4] do not generate “hard” negative samples, i.e., negative samples that resemble positive samples from the data distribution. Their sampling distributions are designed to approximate the model distribution, not the data distribution, which is very different at the beginning of training. Also, their use of a nonuniform sampling distribution is not a means to speed up convergence by reducing gradient noise. It is simply a necessity to make the sampled softmax approximation unbiased (see Theorem 2.1 in [3] and end of Section 2 in [4]).\n\n> [...] without having to keep an auxiliary model.\n\nWhile the authors do not refer to it as an “auxiliary model”, the “summary vector” z in Eq. 8 of [3] serves an equivalent purpose. The vector even has to be updated in an expensive operation during training of the main model because the sampling distribution in sampled softmax has to follow the (changing) model distribution. By contrast, our auxiliary model can be kept static, thus simplifying training and leading to a well-defined (static) loss function.\n\n\nCONCERNING REFERENCES  [1] and [2]\n\nRefs. [1] and [2] are orthogonal to our approach: [1] generalizes negative sampling to a “top-k” ranking task but uses only uniform sampling; [2] does not use sampling as far as we can tell. It instead focuses on a deterministic approximation of the softmax loss that is engineered for a computational model of a GPU.\n\nWe thank the reviewer for their favorable review. We are happy to hear that our derivations are comprehensible to a (self-proclaimed) non-expert. We believe that the approachability is also a strength of the proposed method: due to its simplicity, the method can be used as a drop-in replacement for the widely used negative sampling approach."
            }
        ]
    },
    {
        "id": "rkxawlHKDr",
        "decision": "Accept (Poster)",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: End to End Trainable Active Contours via Differentiable Rendering\nKeywords: No keywords\nAbstract: We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images."
            },
            {
                "round2": "Reviewer 1:\nThe paper proposes a straightforward method for end-to-end learning of active contours, based on predicting a dense field of 2D offsets, and then iteratively evolving the contour based on these offsets. A differentiable rendering formulation by Kato et al is employed to make the process of aligning a contour to a GT mask differentiable. \n\nThe model shows rather compelling results on small datasets, and is very simple, with very strong parallels to active contours, which is a strength. The results improve those of DARNet, which to the best of my knowledge is the main published work in the space other than Curve-GCN. One thing that would be helpful, is  to have an experiment on a large dataset, such as Cityscapes -- right now all the datasets are testing the model in only the small-data regime. Perhaps in a supplement, it would also help to do ablation of how input image / dense deformation resolution affects the result quality -- the input can be subsampled by powers of 2 for the experiment. \n\nAs Amlan Kar helpfully points out, the work heavily overlaps with his approach \"Fast Interactive Object Annotation with Curve-GCN\", CVPR 2019, which is not cited or compared to. Curve-GCN similarly utilizes differential rendering (only a different variant) to match the GT masks. To me, the main difference wrt Curve-GCN is that explicit dense displacement fields are generated by the net and used directly for the iterative refinement steps, while Curve-GCN leverages implicit feature embeddings and uses GCN layers for their iterative updates. A second main difference is that Curve-GCN supports splines and interactive editing, while the proposed approach does not. Beyond these, there are multiple other differences that the authors point out, but those are more of a technical nature. Unfortunately, without a more direct comparison, it is very difficult to evaluate the design choices in the two approaches, which I feel is necessary for proper understanding of the paper. \n\nAFTER REBUTTAL: The authors made additions that covered my concerns, so I have switched my recommendation. \n\nA few more minor clarity / presentation issues. \n-- “The recent learning-based approaches are either non-competitive or proven to be effective in the specific settings of building segmentation\". It's not exactly clear what the point is in the context. Which \"learning-based approaches\"? \n-- Typo 'backpropogation'. \n-- A little better explanation of how a differentiable renderer of Kato works would have been helpful. \n-- Figure 3 is not referenced in the text, takes a little bit of thought why it is relevant (helps explain Fig 1, but maybe better to show it prior to Fig 1). \n-- In Eq 4 it’s not clear what F is.  (I see it is explained in Algorithm box, but that's much later)\n\n\n\n\n\n\n\nReviewer 2:\nThis paper investigates an image segmentation technique that learns to evolve an active contour, constraining the segmentation prediction to be a polygon (with a predetermined number of vertices).  The advantage of active contour methods is that some shapes (such as buildings) can naturally be represented as closed polygons, and learning to predict this representation can improve over pixelwise segmentation.\n\nThe authors propose to learn an image-level displacement field to evolve the contour, and a neural mesh renderer to render the resulting mask for comparison with the ground truth mask.  The performance compared to prior learning-based active contour methods is impressive.\n\nIn section 4.3, there’s a reference to a “gap in performance” between the proposed method and DARNet and a reference to a \"low number of vertices,\" but a comparison between the two methods as the numbers of vertices is varied seems to only be present in Fig. 6 -- it would be interesting to see an explanation of the discrepancy for the lower number of vertices seen in this figure.\n\nOverall, due to the relative simplicity of the approach and impressive performance compared to prior learning-based approaches I recommend to accept.\n\nPost-rebuttal:  I maintain my recommendation.\n\nReviewer 3:\nEDIT: The rating changed from '1: Reject' to '6: Weak accept' after the rebuttal. See below for my reasoning.\n\nThe submission considers two-class image segmentation problems, where a closed-contour image region is to be specified as the 'object'/region of interest, vs. 'no-object'/background. The approach taken here is end-to-end learning with an active-contour type approach. The main loss, in contrast to other active contour approaches, contains a direct difference of the estimated polygon area vs. ground truth polygon area.\n\nThe applied method seems conceptually quite simple (as admitted by the authors in Section 5), and the neural rendering approach seems quite neat, but both method presentation (Section 3) and evaluation (Section 4) seem incomplete and leave significant open questions.\n\nOne of my main concerns is related to the fact that the displacement field is static and, according to Figure 1 and Algorithm 1, is evaluated only once per image.\nIf the displacement field J is not conditioned on the current polygon shape (and this does not seem to be the case), then I am wondering why T iterations in the sampling/rendering part are necessary at all. When only considering L_seg, the optimal solution should be found within one iteration, since the displacement field will be able to provide the optimal answer. So maybe these iterations are only necessary when L_B and L_K are incorporated?\nIn any case, it is unclear why even L_seg is accumulated (using unweighted mean) over all T iterations before being backpropagated. Does this mean that these iterations are not meant to yield shape improvements? Why is ||M^t-M|| not evaluated per iteration, for the purpose of minimization?\nIt is also not sufficiently clear whether M^t in Equation 4 is a filled polygon mask, or if the mask is just related to the boundary (with a certain width). In absence of explanatory image material, I am assuming the former.\nOverall the method description remains weak, since obvious questions/concerns such as the above are not addressed.\n\nThe experimental results look good from a quantitative point of view, and indeed, the strongest baselines, e.g. DARNet, are outperformed significantly in many cases.\nSection 4 mostly focuses on quantitative evaluation and lots of picture examples, but fails to give insight into particular behaviors, failure cases, etc.\nThe evaluation procedure is cast a bit into doubt by two things: 1) In Figure 4, the initializations (blue circles) between the DARNet method and the proposed method are very different in size. I am wondering if this then still constitutes a fair comparison, and I have some doubts there. 2) In Figure 6, the proposed method consistently looks much worse than the DARNet baseline (and, in contrast to the baseline, completely fails for 4 vertices), unless the colors were swapped in the description.\n\nOverall, I do not think the submission is in a good enough shape for acceptance.\n\nMinor remarks:\n- The values for lambda_1 and lambda_2 seem to come out of thin air, and they also seem quite small. It needs to be mentioned how they were determined.\n- Data augmentation by rotation seems to be missing several values (between 270 and 260 degrees) and also not evenly spaced. Is this a typo or on purpose? In the latter case, an explanation is needed, since this seems weird.\n- Section 4.3: There is no \"Figure 4.2\", I assume you mean Figure 6, which otherwise remains unreferenced.\n- Section 4.3, Ablation Study: Don't use the word \"derivatives\" when you're talking about variations.\n- Section 4.3, Ablation Study: \"even without no auxiliary loss\" -> remove \"no\" or change \"without\" -> \"with\"\n\n-------------\nPost-rebuttal comments:\n\nI have read the revised version, as well as the other reviews and all authors' comments. The inclusion of an evaluation on a larger-size data set is highly appreciated, and seems to indeed validate the robustness of the method. Typos were fixed, including the switched color descriptions in Figure 7 (which should not have passed initial submission in the first place, if the text had been proofread properly).\n\nSeveral of the open questions (e.g. \"Why is L_seg accumulated before backpropagation?\", \"Why is the algorithm iterative if the displacement map is computed only once, if not for the other loss terms?\", \"Choice of values for lambda_1, lambda_2\", Initial diameter of initialization\") have been somewhat addressed by the authors in the rebuttal comment, though not in great detail.\n\nBased on the quality of the results across data sets, and because I believe that the timely publication of this rather simple method can benefit further research in this area, I have adjusted my score to a 'Weak accept'. That said, I still do not think it is a good manuscript, and my score should be seen as a massive benefit of the doubt toward the authors.\n\nMost importantly, above questions have NOT been adequately addressed in the actual revised text. The authors claim they have \"improved the manuscript considerably\", but yet I see more reasoning for certain choices described in the comment here than in the actual manuscript. Most of the changes are in Section 2 and the new Section 4.3, but not much relevant to my comments changed in Section 3.\n\nFor example, balloon and curvature losses aside, it is still not clear why an iterative approach would be helpful past the first iteration. An ideal displacement map that is not conditioned on the polygon should point, for each pixel, straight to the closest contour pixel. It is clear to me that this may not be what is being learned when multiple iterations are forced, yet it is not addressed why multiple iterations should be beneficial. (I could see why they could be beneficial if the approach was conditioned on the polygon vertices, to avoid vertex collapsing, but it's not.)\n\nA good submission preempts these kinds of questions by addressing them carefully. What seems crystal clear to the authors will not be crystal clear to every reader. The authors should be more careful to include their reasoning in the actual text, which I believe this is essential for proper, easy understanding of the paper."
            },
            {
                "round3": "This is unfortunate and does not seem right. We will treat your paper as concurrent to our work and think that it should be treated as such by all future reviewers.\n\nThank you for letting us know about your paper, which we will cite in the next version. \n\nWe ask that you would also cite our work, noting that it was made public on open review before the publication date of your arxiv manuscript (obviously the two efforts are concurrent).\n\nThank you for the additional feedback and for upgrading the paper’s rating. We appreciate the timely response and apologize for neglecting to proofread the submitted version more carefully. \n\nRegarding the number of iterations. In the original (and revised) submission, Section 4.4 \"Number of Iterations” and Fig.8 (as numbered in the revised version), we experiment with different number of iterations. We noticed that a single iteration is less beneficial across all datasets, while 2-3 iteration results in higher performance.  Nevertheless, as the reviewer hypothesised, a single iteration can already produce very good results, as can be seen from our experiments. \n\nWe have released a new revision, elaborating on two subjects: (i) The initial guess, and (ii) the effect of the number of iterations T.\n\nThank you for the detailed review.\n\nIndeed, our approach predicts the displacement field only once. We believe that this is a strength of our approach and is part of its simplicity. Similarly to RNNs with fixed weights, having the displacement map computed only once, does not mean that iterations are not beneficial. Note also that the error is backpropagated from all iterations. \n\nRegarding the use of the balloon and curvature term, please see the ablation study, which shows that while our method is extremely competitive even without these losses, the two losses contribute to the results.\n\n“Why is ||M^t-M|| not evaluated per iteration” -- As mentioned in the text before Eq.5., M^t is evaluated at each iteration given the updated set of points. Therefore, ||M^t-M|| is evaluated per iteration. The backpropagation is done on the accumulated loss.\n\nClarity regarding M^t in Equation 4 - the mask M^t is a filled polygon rendered from the set of points P^t. We have further clarified this in the revision.\n\nWe did not search for the best initial diameter, and simply fixed it to the size of 16 pixels across all datasets. Please note that DARNet uses multiple initializations (circles) or different sizes for each dataset as can be seen in Fig.4, while we use only one fixed-size circle. This further supports the robustness of our method.\n\nThe caption of Fig.6 (of the original paper, 7 in the revised) is indeed a typo, and the colors were switched. We apologize for this and have fixed it in the revision. The quantitative results in the graphs of Fig.7 (of the original submission, now Fig. 8) support the fact that our method yields better segmentation for simple polygons as well.\n\nThe values of Lambda1 and Lambda2 were fixed early during the development process and used across datasets. These reflect the relatively smaller part that the ballooning force and the curvature loss play, in the optimization. This is further supported by the ablation analysis that demonstrates that our method is extremely competitive even without these. Similarly, the set of rotations was set without much thinking early on during training, and since it worked, we kept it as is. We believe that changing the augmentation would contribute little to the results, and does not justify the pitfalls of multiple hypothesis testing.\n\nOverall, we hope that the simplicity and elegance of our method are not interpreted as a disadvantage. We believe that the power of our method over previous work (as complicated as they’ll be) is in the straightforward approach. Following the reviews, we have provided an additional dataset for comparison, and clarified and fixed the relevant sections and figures.\n\nWith the CVPR deadline in a week, we would appreciate a timely response, in order for us to be able to plan our submission strategy.\n\nThank you for the supportive review.  We are sorry for the reference mistakes, these are all fixed in the new revised version. \n\nThere was a typo in the caption of Fig.6 (Fig. 7 in the revised version), which switched the association between the methods and the colors. We believe that this mistake has led to the remark concerning this figure.\n\nWe thank the reviewer for the comprehensive review. \n\nWe apologize for the typos in the previous draft. These have been corrected.\n\nTo your comments:\n\nComparison to Curv-GCN: as noted by the reviewer, the differences between the methods are in the support of splines and working with an embedding space in Curve-GCN, vs. displacement map. To emphasize: our method employs a single learned network that produces a displacement image in a single forward pass. The CNN used by Curve-GCN predicts an embedding space of size 28x28 that is further processed by graph neural networks.\n\nFollowing the review, we have conducted experiments on Cityscapes, which is the only public dataset available from Curve-GCN experiments (their code is not available). In this dataset, our method obtains SOTA for 6/8 classes and SOTA, by a sizable margin that is larger than the difference between the performance of previous work, in the overall mean mIoU. We believe that this also directly addressed the reviewer’s comment regarding larger datasets.\n\nAll of our models are trained at the resolution of 64x64 pixels. As noted in the original submission when discussing the Vaihingen dataset “we experiment with different resizing factors during training”. Following the review, we share these results in Tab.5 of the revised submission. As can be seen, there is a steep degradation in performance below 64x64, which we attribute to the lack of details. When doubling the resolution to 128x128, the performance slightly degrades, however, that model could improve with further training epochs.\n\n\nThe recent learning-based approaches are either non-competitive or proven to be effective in the specific settings of building segmentation\" — we have clarified in the text that we mean learning-based active contour methods and have limited the scope of the claim. \n\nWe have added a paragraph regarding the use of a 3D renderer for 2D maps. We simply fix the third coordinate and use the code of Kato as is.\n\nThe letter “F” (for faces) is defined at the beginning of the “Method” section.\n\nWe believe that various issues raised by the reviewer were fully addressed in a way that considerably improved the manuscript. With the CVPR deadline in a week, we would appreciate a timely response, in order for us to be able to plan our submission strategy. \n\nFollowing the reviews, we have revised our manuscript to correct the various typos, to clarify some issues and, most importantly, to update the related work section and to compare experimentally with the CVPR 2019 work of Ling et al. As detailed on open review, which this work indeed narrows our novelty claims, there are important differences and the two methods are very much different.\n\nWe are happy to report that on the public dataset on which the CVPR 2019 work has been tested, our method outperforms all previous work in 6/8 categories and shows a clear advantage in the mean performance. This, without performing any modification to our method and despite our method being considerably less involved than the other methods.\n\nThank you very much for pointing us to [1], which is indeed related and would be cited appropriately. We would like to enumerate some of the important differences between the methods. \n1. Supervision and training: We supervise with GT masks only during training, while [1] learns an additional edge branch and a vertex branch. Vertex-based supervision constraints their model to learn a specific location for each point. \n2. Training: While we train our model end-to-end with a single supervision, [1] performs a two-phase learning, where they first train using edges and vertices supervision, followed by fine-tuning with GT masks. [1] also points out that their rendering process is too slow for training end-to-end using only GT masks (Sec. “Training Details”), while we use a fast, fully differentiable renderer.\n3. CNN role: Our method employs a single learned network that produces a displacement image in a single forward pass. The CNN used by [1] predicts an embedding space of size 28x28 that is further processed by other networks.\n4. CNN architecture: We employ a fully convolutional CNN that produces an output that is the same size as the input image, while [1] scales a fixed-sized input to a spatially-limited 28x28 image.\n5. To emphasize: our method is considerably more direct, and learns a 2-D displacement field in the scale of the input by a fully convolutional network. The update in [1] is by a learned GCN that is applied over graph nodes that employ the 28x28 embedding.\n6. Loss: we incorporate two loss terms that are based on time-tested pulling forces from the classical active contour literature: the Balloon and Curvature terms. This allows us to work directly with the contour.\n\n[1] Fast Interactive Object Annotation with Curve-GCN: https://arxiv.org/abs/1903.06874 - CVPR 2019"
            }
        ]
    },
    {
        "id": "HkliveStvH",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Connectivity-constrained interactive annotations for panoptic segmentation\nKeywords: Panoptic Segmentation, Semantic Segmentation, Interactive Segmentation, Integer Programming\nAbstract: Large-scale ground truth data sets are of crucial importance for deep learning\nbased segmentation models, but annotating per-pixel\nmasks is prohibitively time consuming. In this paper, we investigate interactive graph-based segmentation algorithms that enforce connectivity. To be more precise, we introduce an instance-aware heuristic of a discrete Potts model, and a class-aware Integer Linear Programming (ILP) formulation that ensures global optimum. Both algorithms can take RGB, or utilize the feature maps from any DCNN, whether trained on the target dataset or not, as input. We present competitive semantic (and panoptic) segmentation results on the PASCAL VOC 2012 and Cityscapes dataset given initial scribbles. We also demonstrate that our interactive approach can reach $90.6\\%$ mIoU on VOC validation set with an overhead of just $3$ correction scribbles. They are thus suitable for  interactive annotation on new or existing datasets, or can be used inside any weakly supervised learning framework on new datasets."
            },
            {
                "round2": "Reviewer 1:\nThis paper introduces post-processing methods for panoptic (a combination of semantic and instance) segmentation, which are capable of using scribble annotations provided interactively by users. The proposed methods rely on (i) a discrete Potts model making use of RGB or DCNN features of the image, as well as the edge connectivity in a superpixel graph, and (ii) an integer linear program corresponding to a MRF with a pairwise data term. The proposed methods are evaluated on the Pascal VOC 2012 and Cityscapes datasets.\n\nThe paper is generally well written and easy to follow. The problem of panoptic segmentation is fundamental to computer vision, and as such, of relevance to the ICLR community. The proposed methods appear novel and worth pursuing.\n\nA first reservation about the paper is that the method is primarily one of post-processing (after, e.g., extracting primary features from a DCNN), but the most common means of post-processing, namely conditional random fields, are not even mentioned, let alone compared against.\n\nThe other main reservation about the paper is that there are very few comparisons to the abundant literatures on either semantic or instance segmentation, and as such it is difficult to appreciate the paper’s contributions to these areas. Of note:\n\n1. Evaluate on the COCO dataset, which is the current standard for segmentation ;\n2. The scribble supervision method of Lin et al (2016) is mentioned, but not compared against.\n\nSeparately, the paper should compare the proposed method for semantic and instance segmentation with other methods that use weak-labels such as:\n* Laradji, I. H., Vazquez, D., & Schmidt, M. (2019). Where are the Masks: Instance Segmentation with Image-level Supervision. arXiv preprint arXiv:1907.01430.\n* Laradji, I. H., Rostamzadeh, N., Pinheiro, P. O., Vazquez, D., & Schmidt, M. (2019). Instance Segmentation with Point Supervision. arXiv preprint arXiv:1906.06392.\n* Cholakkal, H., Sun, G., Khan, F. S., & Shao, L. (2019). Object counting and instance segmentation with image-level supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 12397-12405).\n* Zhou, Y., Zhu, Y., Ye, Q., Qiu, Q., & Jiao, J. (2018). Weakly supervised instance segmentation using class peak response. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3791-3800).\n* Zhu, Y., Zhou, Y., Ye, Q., Qiu, Q., & Jiao, J. (2017). Soft proposal networks for weakly supervised object localization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1841-1850).\n* Ahn, J., & Kwak, S. (2018). Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4981-4990).\n\nAs the paper currently stands, given the gaps in the experimental evaluation, it is difficult to appreciate the contributions and complementarities of the proposed methods to the panoptic segmentation problem. As such, the paper would require more work before recommending acceptance at ICLR.\n\nReviewer 2:\nThis paper proposes two graph-based deep network feature fusion methods with connection constraints for semantic and panoptic segmentation. By incorporating additional scribble information to DCNN features, the methods yield improved results over the original network predictions on two popular semantic and panoptic segmentation datasets. Through interactively correcting error regions with the scribbles, further performance increase can be obtained. \n\nI am not completely convinced by the novelty and experiments.\n(1) First, the idea of smart annotation can be formalized as a weekly supervised segmentation problem where only part of the annotation is available. Can the authors justify how your work differs from those works solving the weekly supervised problem and what's your advantages. (Seed Expand and Constrain ... Alexander Kolesnikov; STC: A simple to Complex Framework for Weakly... Yunchao Wei; FickleNet Jungbeom Lee; etc..) Or, if possible, could you make a fair comparison with some existed weekly supervised approach on the final (semantic) result. Second, Potts model, MRF, K-nearest cut are known approaches. Thus I would like to know the deeper contribution of this work other than set constraints and solve ILP.\n(2) The authors did not justify the use of less powerful models (DeepLabV2 and DRN) as both the inputs for l0H and ILP-P and the baseline comparison. The authors mentioned the current SOTA model (DeepLabV3+), which has achieved 79.55% mIoU on the CityScapes val set. However, they did not perform experiments using its probability map. It would be more convincing if the same performance gain can be achieved by using the SOTA model as inputs to the algorithms.\n(3) The argument of achieving competitive results for panoptic segmentation is rather weak. To approach the panoptic segmentation problem, the authors essentially used scribbles to separate semantic region predictions into individual instances. Since the proposed algorithm requires as many scribbles to be drawn as there are regions, the baseline network only needs to predict semantic classes, and the algorithms uses the provided region IDs from the scribbles to segment individual instances. While this still has numerous applications in data annotation, it is somewhat unjust to claim that this method achieves competitive results in panoptic segmentation.\n(4) The artificial scribbles for CityScapes experiments do not resemble human-drawn scribbles. Compared to the scribbles data for VOC12, the artificially generated scribbles for CityScapes experiments are visually idealistic. Rather than a stroke through the object, the generated is more similar to an outline of the object, which conveys a lot more information than a single line. Particularly when applied on super-pixels, it seems that super-pixels can easily be merged together by grouping any super-pixels within a scribble outline.\nThere are some other minor suggestions. For example, it might be clearer and easier to read if section 2.2.2 is presented in an algorithm format. Some minor typos and grammatical mistakes should also be corrected.\n\n\nReviewer 3:\nThis paper investigates scribble-based interactive semantic and panoptic segmentation.  The algorithms described build a graph on superpixels and do not require deep features or labels but rely on “scribbles” for supervision.  Given that semantic (and panoptic) annotation is very labor-intensive, advances in scribble-based annotation could significantly improve annotation time for new datasets; in applications where real-time performance is not required, scribble-based refinement of predictions could also be advantageous.\n\nThe experiments compare the proposed algorithms to deep baselines for VOC2012 and Cityscapes panoptic segmentation, and show impressive performance even without deep features. However they do not compare results to other scribble supervision methods to highlight the advantages of their approach over prior work.  I’d like for the experiments section to have a proper comparison to prior scribble algorithms (e.g. in section 4.4, comparing to other algorithms with the SOTA approach as baseline) to clearly show the advantage of their approach.\n\nThe results are impressive compared to the deep learning baseline, but I think further experimental validation should exist for properly comparing to prior work.\n\nPost-rebuttal: I maintain my recommendation.\n\nReviewer 4:\nSummary:\n- key problem: efficiently leveraging scribbles as interactive supervision (at test time) for panoptic segmentation;\n- contributions: 1) two algorithms leveraging scribbles via a superpixel connectivity constraint (one class-agnostic local diffusion heuristic, one class-aware with a MRF formulation), 2) experiments on PASCAL VOC 2012 and Cityscapes showing that both methods i) can achieve good performance without training data (using RGB values or pretrained features), ii) can improve the performance of a fully supervised baseline when using its probability maps as representation, and iii) can significantly improve performance beyond the state of the art on PASCAL when used interactively (90% mIoU with 3 rounds of corrective scribbles).\n\nRecommendation: weak reject\n\nKey reason 1: unclear novelty and relevance to ICLR.\n- The paper proposes to apply two existing algorithms (Nguyen & Brown 2015, Rempfler et 2016) to a new task (interactive panoptic segmentation): what is the claimed novelty? What is specific to panoptic segmentation vs semantic or instance segmentation? Could the difference with related work in Section 2 be discussed more precisely?\n- Furthermore, there seems to be no learning (representation or otherwise) involved in this submission. The paper mentions potential applications to weakly-supervised learning in Section 5, but it does not provide clear insights into what would be the benefits in terms of representation learning (vs. RGB, pre-trained features, or probability maps).\n- Overall, this paper might be more tailored for a Computer Vision venue like CVPR.\n\nKey reason 2: lack of sensitivity / robustness analysis.\n- The scribbles are \"simulated\" using morphological operations on the ground truth (A.2, A.3): does this lead to realistic scribbles? Figure 3 (which is unclear) shows that the \"scribbles\" might be precise outlines or contours, which are very different than the expected scribbles illustrated in Figure 2. Contours provide much stronger information for segmentation, and are much more likely to effectively leverage the connectivity prior (esp. with the diffusion heuristic), but are they really scribbles / cheap supervision?\n- What is the importance of the superpixel coverage by scribbles or missing scribbles or the location of scribbles relative to segment boundaries? What are the impact of realistic deviations from the expected scribble policy that are likely to happen in practice? Measuring sensitivity to different types of noise (by perturbing / dropping out scribbles) seems important to assess the practical usefulness and robustness of the method.\n- PASCAL VOC and Cityscapes are small datasets. Experiments on bigger more recent ones like Mapillary Vistas and COCO are becoming the standard protocol in the instance/semantic/panoptic segmentation community. How would this method fare on those much more challenging datasets? What are the benefits of the proposed interactive methods in terms of scalability?\n\nAdditional Feedback:\n- Fig. 4 is too low resolution / blurry;\n- typos: \"tarining set\", \"weekly supervised\".\n\n## Update following the rebuttal\n\nThanks to the authors for their replies. Sadly, my concerns are only answered at a high-level, and the consensus among reviewers is clear. Hence I confirm my rating to reject. I hope the feedback provided above will assist the authors in improving the work or finding a more suitable venue."
            },
            {
                "round3": "We thank the reviewer for the detailed comments. \n\n1. Our method is not a direct apply of existing algorithms. The heuristic is greatly modified to comply with scribbles and in addition enforces connectivity of each scribbled region. Our ILP formulation extends previous MRF (only for class) to panoptic (both class and instance) by introducing dummy edge variables, and does not increase the complexity of the problem.\n\n2. Although not a learning algorithm, ours are most suitable for annotating ground truth dataset, which is of fundamental importance to the data hungry deep learning method. Extension of our algorithms into the weakly/scribbles supervised learning framework similar to Lin et al (2016) can be a natural next step. In addition, we have conducted extensive experiments using RGB, lower level features and probability map as input to our algorithms. Results show that traditional machine learning algorithms can also benefit from deep learning by taking the its feature layers as input, which may be direction that is worth discovering.\n\n3. Since the main application of our method is to annotate dataset, hence in a data annotation point of view, we argue that the artificial scribble is realistic. We tested drawing more strict (even closer to the boundary than the artificial ones) scribbles on Cityscapes, and it takes on average only 2 minutes per image, which is still a dramatic decrease in annotation time compared to 1.5 hours. On the other hand, by adopting the online available of VOC scribbles, we sort of already validate the robustness of our algorithms (84.6% mIoU and 90.6% after 3 correction scribbles). \n\n4. We are re-running our algorithms that take pixels as input on Cityscapes. We will report the results once it’s done.\n\n5. Our paper is mainly focused on the design of the algorithm and ILP formulation, hence we argue the experiments on two datasets suffice to validate the performance. In the future work when incorporating our algorithms into the weakly supervised learning framework, it is of great interest to test on more challenging datasets.\n\nWe thank the reviewer for the detailed comments. \n\n1. Other scribble supervision method requires deep learning in the loop, while ours not. Since our contribution lies on the design of the algorithm/formulation that enforces connectivity, which could be served as a baseline for any weakly supervised learning method. Although we are pretty optimistic that adding the connectivity constraint would boost up the performance of Lin et al (2016), we leave the implementation and experiments of that as future work.\n\nWe thank the reviewer for the detailed comments. \n\n1. Our contribution is the design of a heuristic optimization algorithm and an ILP formulation that enforces connectivity (NP-hard) for interactive panoptic segmentation. Other than adopting lower level features of any deep learning basenet or its final probability map as input to our algorithms, no learning is evolved in our approach. Hence, it is not suitable to compare with other weakly supervised learning approach.\n\n2. Since our contribution lies on the design of the algorithm/formulation, we did not focus on selecting and fine tunning the SOTA deep learning network. We searched and the networks presented in the paper are the best public available deep nets together with checkpoint on the internet for the time being.\n\n3. We agree with the reviewer on this point, and that is why we added a condition on this statement, “given initial scribbles”.\n\n4. Since the main application of our method is to annotate dataset, we argue that the artificial scribble is realistic for any data annotator. We tested drawing even more strict (closer to the boundary than the artificial ones) scribbles on Cityscapes, and it only takes on average 2 minutes per image, which is still a dramatic decrease in annotation time compared to 1.5 hours. On the other hand, by adopting the online available of VOC scribbles, we sort of already validate the robustness of our algorithms (84.6% mIoU and 90.6% after 3 correction scribbles). \n\nWe thank the reviewer for the detailed comments. \n\n1. We have compared ours to MRF shown in Table 2, and ours is 0.9% and 3.8% better in mIoU.\n\n2. Our paper is mainly focused on the design of the algorithm and ILP formulation with connectivity constraints, hence we argue the experiments on the two datasets presented in the paper suffice to validate the performance. In the future work when incorporating our algorithms into the weakly supervised learning framework, it is of great interest to test on the more challenging datasets.\n\n3. Our focus is the design of the two interactive optimization algorithm/formulation that enforces connectivity, which could be served as a baseline for any weakly supervised learning method. Although we are pretty optimistic that adding the connectivity constraint would boost up the performance of Lin et al (2016), we leave the implementation and experiments of that as future work.\n\nFirst of all, we would like to thank all the reviewers for spending time reading our paper.\nWe have replaced Fig. 4 and corrected all typos mentioned by the reviewers.\n\nWe first want to remind the 4 highlights of our paper:\n1.  We enforce the connectivity constraints (NP-hard) by introducing either a class-agnostic heuristic or a class-aware MRF integer programming, the later being an exact global optimization formulation.\n\n2. Our method is not a learning method, but instead optimization algorithms that are suitable for inference based on RGB or lower level features input, or post-processing on existing learning algorithms (using their probability maps as input). \n\n3. Ours does not necessarily require any available training data, i.e., it can use only the RGB, or the lower layer of any base network trained on arbitrary data set, as the input to our algorithms. \n\n4. Compared to weakly supervised learning approach, the connectivity of scribbled region allow more control within the annotation framework (no outliers as shown in Fig. 1), hence is particularly suitable for annotating dataset.\n\nFinally, the novelty of the paper comes from the re-design of the heuristic algorithm that complies with scribbles, and a novel ILP formulation that introduces dummy edge variables to deal with the multi-instance panoptic segmentation, while not increasing the complexity compared to the MRF for semantic segmentation. In addition, both methods enforce the connectivity prior."
            }
        ]
    },
    {
        "id": "HygPjlrYvB",
        "decision": "Reject",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Learning from Positive and Unlabeled Data  with Adversarial Training\nKeywords: Positive and Unlabeled learning\nAbstract: Positive-unlabeled (PU) learning learns a binary classifier using only positive and unlabeled examples without labeled negative examples. This paper shows that the GAN (Generative Adversarial Networks) style of adversarial training is quite suitable for PU learning. GAN learns a generator to generate data (e.g., images) to fool a discriminator which tries to determine whether the generated data belong to a (positive) training class. PU learning is similar and can be naturally casted as trying to identify (not generate) likely positive data from the unlabeled set also to fool a discriminator that determines whether the identified likely positive data from the unlabeled set (U) are indeed positive (P). A direct adaptation of GAN for PU learning does not produce a strong classifier. This paper proposes a more effective method called Predictive Adversarial Networks (PAN) using a new objective function based on KL-divergence, which performs much better.~Empirical evaluation using both image and text data shows the effectiveness of PAN.  "
            },
            {
                "round2": "Reviewer 1:\nThank the authors for the response.\n\n1. My comment is not about how C and D are implemented but about their mathematical definitions. Specifically, clarifying the input and output space of the function is important. The authors use C(.) for a vector-to-scalar mapping, though C_i is for a vector-to-vector mapping. \n\n3. My major concern is \"whether or not we can obtain the optimal (unbiased) classifier by the optimization in Eq. (3)?\". It is obvious in case of Eq. (2), because D performs worst if C perfectly extracts the positive samples from unlabaled data. On the other hand, it is not clear in case of Eq. (3). When we only consider term I, D is biased since the unlabaled data contain the positive samples. I imagine that term II and III reduce this bias to obtain the unbiased classifier C, but it is not clearly shown. (While I understand the authors' intention described in section 4.1, it is not supported well in theory.) In addition, tuning lambda seems to play almost same role with the class-prior estimation, if the above intuition is correct.\n\nReviewer 2:\nThe paper proposed an interesting idea of using two adversarial classifiers for PU learning. The first classifier tries to introduce samples from unlabeled data that are similar to the existing labeled positive data, and the second one tries to detect if a sample has a ground truth label or drawn from unlabeled data by the first classifier (fake). The idea of the paper is interesting; it is well-motivated and well supported with a range of experiments from NLP and computer vision tasks. The paper is a good read but required a pass of proofreading; some rearrangement of the concepts (for example, in second paragraph C(.) and D(.) is used, but they are introduced properly in section 4. Also, the paper could use some clarifications.\n\n* How the proposed method handles unlabeled positive samples that have a different distribution from P or has a similar distribution with some of the negative samples that might exist in unlabeled samples. \n\n* The experiment section could have enjoyed from an ablation study in which a system that only implements terms I and II from Eq(3). The authors mentioned that such an objective function is asymmetric but didn't explore the implications of such an objective function in the empirical experiments.\n\n* PGAN's results are not compared in the fair condition since the PU version of CIFAR 10 is different from PGAN's version.\n\n* Using MLP classifier for the text classification (e.g., for YELP) makes a very weak baseline for the system. Also, training the word embedding by the system itself is unrealistic. Therefore, the sentence might need to be rewritten.\n\n* Some readability issues: \n(i) C(.) and D(.) needs an introduction in the section \"I Introduction\" before their usage.\n(ii) The idea could be illustrated easily. Such a figure significantly improves the readability of the system.\n(iii) be careful with the use of \\cite{} and \\citep{} interchangeably (\"{Liu et al., 2003; Shi et al., 2018\" -> Liu et al. (2003) and Shi et al. (2018) ..., \n(iv) The first paragraph of section 2 should be split into two from this phrase \"None of these works...\"\n(v) Please rewrite the latter half of paragraph 2 in section 2. Also, please rewrite the beginning sentences of section 4.1 and the final paragraph of section 3.\n(vi) right after equation (2), please change x_s to \\mathbf(x)^s for the consistency of your formulation.\n(vii) favorible -> favorable, radio (in section 5.1) -> ratio,\n(viii) please add a reference for this statement. \"This is one of the best architectures for CIFAR10.\"\n\nReviewer 3:\nThis paper considers the problem of learning a binary classifier from only positive and unlabeled data (PU learning), where they develop a Predictive Adversarial Networks (PAN) method by using the GAN-like network architecture with a KL-divergence based objective function. Experiments and comparisons with SOTA are provided.\n\nPros:\nTheir idea of making an adaption to GAN architecture by replacing the generator by a classifier to select P from U and using the discriminator to distinguish whether the selected data is from P or U for PU learning is interesting, and benefits from not relying on the class prior estimation.\n\nQuestion:\nThe authors claim that Eq. (2) cannot be trained in an end-to-end fashion directly, this statement may need some modification since there are some existing works replacing c(x) by a score function or some other continuous function and then this direct adaptation can be trained, for example, see Eq. (5) in “Discriminative adversarial networks for positive-unlabeled learning. arXiv:1906.00642, 2019”. Can any explanation be given on this?\n\nRemarks:\nThe clarity of the paper could be improved in multiple places. For example, the data generation processes can be mathematically defined in the problem setting part, now it is quite confusing to me. And more details on experimental protocol may be needed: e.g. what kind of hyperparameter tuning was done?\n \nIn general, the paper proposed an interesting GAN-like network architecture to learn from PU data, but some unclear parts need to be improved. \n\nReviewer 4:\n<Paper summary>\nThe authors proposed a novel method for positive-unlabeled learning. In the proposed method, adversarial training is adopted to extract positive samples from unlabeled data. In the experiments, the proposed method achieves better performance compared with state-of-the-art methods. \n\n<Review summary>\nAlthough the idea to utilize adversarial training for PU learning is interesting, the proposed method is not sufficiently validated in theory. In addition, the manuscript is hard to follow due to confusing notations and lack of figures. I vote for rejection.\n\n<Details>\n* Strength\n + The main idea is simple and interesting.\n + The proposed method performs well in the experiments.\n\n* Weakness and concerns\n - Confusing notations and lack of figures.\n  -- Lack of mathematical definition of C and D.\n  -- The argument of P^p and that of P^u are different (x^p and x^u), which implies that those distributions are defined at different space (but actually same).\n  -- Shared index ``i\" for positive and unlabeled data in Eq. (3).\n  -- The notation with ``hat\" often imply the empirically estimated (or approximated) value in the field of ML. \n  -- No figures about the proposed method. Specifically, it is hard to understand the relationship between C and D. \n\n - Since Eq. (3) looks totally different from Eq. (2), why Eq. (3) is reasonable remains unclear. \n  -- About I: first, P^{pu} cannot be calculated, because it requires unavailable labels of x^u. If you treat unlabeled data as negative, it should not be called ``ground-truth,\" and the term I cannot help D correctly recognize positive samples. Second, the positive samples are almost ignored in this term, because the number of positive data should be substantially small in a common setting of PU learning. \n  -- About II: the authors explain the role of this term by min-max game between C and D during optimization, but the most important point here is what will happen when we obtain the optimal C and D after the optimization. What property or behavior do the optimal C and D have? \n\n - What do the authors want to claim with Proposition 1? The right-hand side of Eq. (5) cannot be easily calculated due to the density ratio between P^p and P^u. There is no explanation about what f and eps mean. What ``optimal\" means is also ambiguous. \n\n\n* Minor concerns that do not have an impact on the score\n - Although the problem setting is quite different, the idea of this paper is partially similar to the importance weighting technique adopted in some recent domain adaptation methods [R1, R2]. Do you have any comment on that?\n\n[R1] ``Reweighted adversarial adaptation network for unsupervised domain adaptation,\" CVPR2018 \n[R2] ``Importance weighted adversarial nets for partial domain adaptation,\" CVPR2018\n\n"
            },
            {
                "round3": "Thank you very much for your new comments.\n\nRe: My comment is not about how C and D are implemented but about their mathematical definitions. Specifically, clarifying the input and output space of the function is important. The authors use C(.) for a vector-to-scalar mapping, though C_i is for a vector-to-vector mapping.\n\nResponse> Yes, C(.) is a vector-to-scalar mapping, C_i denotes the probability distribution of the data point x_i being positive and negative, which is a vector-to-2dimension mapping. The 2 dimensions are the outcome positive and negative probabilities, i.e., (C(x_i), 1 – C(x_i)).\n\nRe: My major concern is \"whether or not we can obtain the optimal (unbiased) classifier by the optimization in Eq. (3)?\". It is obvious in case of Eq. (2), because D(.) performs worst if C(.) perfectly extracts the positive samples from unlabeled data. On the other hand, it is not clear in case of Eq. (3). When we only consider term I, D(.) is biased since the unlabeled data contain the positive samples. I imagine that term II and III reduce this bias to obtain the unbiased classifier C(.), but it is not clearly shown. (While I understand the authors' intention described in section 4.1, it is not supported well in theory.) In addition, tuning tuning lambda seems to play almost same role with the class-prior estimation, if the above intuition is correct.\n\nResponse> Our method PAN (Eq. (3) or (4)) basically follows the similar idea to that of Eq. (2). In Eq. (2), D performs worst if C(.) perfectly extracts the positive samples from the unlabeled set, which is correct as D(.) is unable to classify/separate the given positive data and the possible positives x’ extracted by C(.). In the case of PAN, that also means D(.) gives high positive probability scores to x’ like C(.). Thus, the final training result is that D(.) and C(.) give similar predictions, or D(.) cannot move away from C(.) (meaning D(.) also gives high scores to the examples that get high scores from C(.)).\n\nWe agree that Eq. (3) is harder to understand as it is different from GAN and also because for KL-divergence, the probabilities of a distribution can go up or down in order to match another distribution, which makes it more difficult to explain as there are many cases. Yes, your intuition is correct. Terms II and III try to correct the bias of D(.) in term I of Eq. (3). But let us see the idea using Eq. (4), which is derived from Eq. (3) for training and it is much clearer than Eq. (3). Note that the bias in term I in Eq (3) will result in high precision and low recall for the positive class. Now back to Eq (4) and let us imagine that most examples are regarded as negatives by C(.) (low recall). Then, from Eq. (4), we can see the value of term V is below zero. When optimizing D(.), term VI will push D(.) up for these examples, and thus the bias is reduced and the low recall problem is mitigated as in the next optimization iteration, C(.) for the examples will also go up following D(.). We have added more explanation in the paper using Eq. (4) in Appendix C.\n\nRegarding lambda, in our experiments it is fixed to 0.0001 for all experiments. It can have some indirect effect of correcting the bias but we believe the main effect is from above.  \n\nHope our explanation is clear. If you have any additional questions, please let us know. We will address or clarify them quickly.\n\nThank you very much for your helpful comments. We have addressed your concerns in the revised paper (uploaded). Below are our answers to your questions. \n\nRe: “Although the problem setting is quite different, the idea of this paper is partially similar to the importance weighting technique adopted in some recent domain adaptation methods [R1, R2]. Do you have any comment on that?” \n\nResponse> Thanks for pointing this out and the two relevant papers. We read the two papers and have cited them and compared them with our work. Our a-GAN method has some similarity with the weighted adversarial nets (WAN), but our PAN differs significantly from WAN (as PAN differs significantly from a-GAN). That is because although WAN weighted D by w(z) but the adversarial training procedure is the same as the original GAN (similar to our a-GAN). The examples generated by G are fed into D for discrimination. A-GAN uses the same strategy, which did not work well in our case. That is why we designed a new formulation in PAN, which uses KL-divergence and the three terms in Eq. (3) to solve the problem as we discussed in Section 4. \n\nHope our responses are clear. If you have any additional questions, please let us know. We will address or clarify them.\n\n\nThank you very much for your helpful comments. We have addressed your concerns in the revised paper (uploaded). Below are our answers to your questions. \n\nRe: Confusing notations and lack of figures.\n1.\tLack of mathematical definition of C and D:\n\nResponse> Both C and D are neural networks and can be any of existing classification models. The exact architectures used in the paper are given in the experiment section (under Training Details) because C and D have different architectures for text and image. We also added this in the introduction section of the revised paper. \n\n2.\tThe argument of P^p and that of P^u are different (x^p and x^u), which implies that those distributions are defined at different space (but actually same).\n\nResponse> Different notations are used as we want to distinguish where the data come from. In SCAR (Selected Completely At Random) PU learning setting, positive data are randomly selected from the positive population. After that both positive and unlabeled sets are fixed before training. In this case, we use different notations because the unlabeled set is viewed as a whole and thus has a different distribution than the positive set because the unlabeled set also includes hidden negative data.\n\n3.\tResponse> For the notation issues, we have addressed them in the revised version. We also added a figure to explain our method. We hope it is clear now.\n\nRe: -- About I: first, P^{pu} cannot be calculated, because it requires unavailable labels of x^u. If you treat unlabeled data as negative, it should not be called ``ground-truth,\" and the term I cannot help D correctly recognize positive samples. Second, the positive samples are almost ignored in this term, because the number of positive data should be substantially small in a common setting of PU learning.\n\nResponse> Yes, we treat the unlabeled data as negative. So it can be calculated. Yes, treating unlabeled data as negative may not let D correctly recognize positive examples and thus decrease the performance of D. However, (1) we don’t need a strong D in the min-max game (see [1]), and (2) in the original GAN, the generated samples (or examples) also include both positive and negative data, i.e., the generated samples in GAN are basically like our unlabeled set. Term I aims to endow D with the ability to rank positives at the top. With this ability, the min-max game can work well iteratively.  \n\nThe positive samples will not be ignored in this term as we balanced the ratio of positive data and unlabeled data in each mini-batch in training. Thus, the impact of positive and unlabeled data is balanced. Sorry, we did not make this clear in the paper. It is added now. \n\n[1] Dai, Zihang, Zhilin Yang, Fan Yang, William W. Cohen, and Ruslan R. Salakhutdinov. \"Good semi-supervised learning that requires a bad gan.\" In Advances in neural information processing systems, pp. 6510-6520. 2017.\n\nRe: “About II: the authors explain the role of this term by min-max game between C and D during optimization, but the most important point here is what will happen when we obtain the optimal C and D after the optimization. What property or behavior do the optimal C and D have?” and “- What do the authors want to claim with Proposition 1? The right-hand side of Eq. (5) cannot be easily calculated due to the density ratio between P^p and P^u. There is no explanation about what f and eps mean. What ``optimal\" means is also ambiguous.” :\n\nResponse> Intuitively, the behaviors of optimal C and D are that C gives the same prediction as D while D cannot move away from C, which means D also give high scores to examples that get high scores from C. \n\nWe use Proposition 1 to decide the decision surface of our model. However, due to the complexity of the PU learning setting, f() is very complex (but it can be computed). Thus, we showed its properties, which we believe is sufficient. Please see Appendix C. We added explanations of the behaviors of optimal C and D, and moved Proposition 1 to Appendix C.\n\nHope our responses are clear. If you have any additional questions, please let us know. We will address or clarify them.\n\n\nThank you very much for your helpful comments. We have addressed your concerns and improved the clarity of the paper and uploaded it. \n\nRe: The authors claim that Eq. (2) cannot be trained in an end-to-end fashion directly, this statement may need some modification since there are some existing works replacing c(x) by a score function or some other continuous function and then this direct adaptation can be trained, for example, see Eq. (5) in “Discriminative adversarial networks for positive-unlabeled learning. arXiv:1906.00642, 2019”. Can any explanation be given on this?\n\nResponse> We are wondering where we made that claim. We did not make that claim in the submitted version. Could you please check again? If you find that claim, please let us know the location and we will definitely revise it. Actually, our a-GAN is trained in an end-to-end fashion. The recent arXiv paper (which should be done at about the same time as our paper) that you mentioned is similar to a-GAN, but our PAN differs from it significantly. We have cited and discussed it in the revised version. \n\nFor other questions:\nResponse> Thank you for your suggestions to improve the clarity of the paper. We have revised it and make things clearer. About hyperparameter tuning, we gave an analysis in Appendix D.2. Could you refer to that for more details and let us know whether it is satisfactory. \n\nIf you have any additional questions, please let us know. We will address or clarify them.\n\n\nThank you very much for your helpful comments. We have addressed your concerns in the revised paper, which has been uploaded. \n\nRe: * How the proposed method handles unlabeled positive samples that have a different distribution from P or has a similar distribution with some of the negative samples that might exist in unlabeled samples. \n\nResponse> In this paper, we do not consider this case. We assume the examples in the positive set are sampled randomly from the positive population. The problem that you mentioned is very challenging. We will consider it in our future work. \n\nRe: * The experiment section could have enjoyed from an ablation study in which a system that only implements terms I and II from Eq(3). The authors mentioned that such an objective function is asymmetric but didn't explore the implications of such an objective function in the empirical experiments. \n\nResponse> We actually did the experiments before and the results were not good for YELP, 20news and RT. That’s why we employed the third term. We have redone the experiments and added the results in Appendix (D.3).\n\nRe: * PGAN's results are not compared in the fair condition since the PU version of CIFAR 10 is different from PGAN's version.\n\nResponse> Thanks for pointing this out. We apologize. We could not and still cannot find the code of PGAN online and we thus are unable to run the experiments. However, we ran our method PAN in PGAN's setting on CIFAR10 and we get much better results, 91.16 for accuracy and 89.13 for F-score while the F-score of PGAN is 76 as reported in the PGAN paper. This much better result of PAN is mainly because PGAN’s setting uses much more positive data than our setting in the paper. We believe that less positive data is more realistic in practice. Thus, as we cannot compare with PGAN in our setting, we believe it is better to remove that specific result (F-score=76) that we quoted in the submitted version earlier from the revised paper. However, we still cited and compared with it in the related work section. If you know there is a version of the PGAN code online, please let us know and we will run it immediately.\n\nRe: * Using MLP classifier for the text classification (e.g., for YELP) makes a very weak baseline for the system. Also, training the word embedding by the system itself is unrealistic. Therefore, the sentence might need to be rewritten.\n\nResponse> Thanks. We used pre-trained word embeddings learned by the skip gram method of word2vec on the corresponding datasets. Perhaps there is a misunderstanding about classifier. The classifier is a 2-layer convolutional network (CNN), with 5 * 100 and 3 * 100 convolutions for layers 1 and 2 respectively, and 100 filters for each layer. An MLP layer follows to map the output features to the final decision scores. Only the MNIST dataset uses MLP only.\n\n**Thanks for pointing out some readability issues. We have revised the paper and uploaded the new version. If you have any more questions, please let us know. We will address them and make everything clear. "
            }
        ]
    },
    {
        "id": "rJleKgrKwS",
        "decision": "Accept (Poster)",
        "source": "ICLR 2020",
        "statements": [
            {
                "round1": "Title: Differentiable learning of numerical rules in knowledge graphs\nKeywords: knowledge graphs, rule learning, differentiable neural logic\nAbstract: Rules over a knowledge graph (KG) capture interpretable patterns in data and can be used for KG cleaning and completion. Inspired by the TensorLog differentiable logic framework, which compiles rule inference into a sequence of differentiable operations, recently a method called Neural LP has been proposed for learning the parameters as well as the structure of rules. However, it is limited with respect to the treatment of numerical features like age, weight or scientific measurements. We address this limitation by extending Neural LP to learn rules with numerical values, e.g., ”People younger than 18 typically live with their parents“. We demonstrate how dynamic programming and cumulative sum operations can be exploited to ensure efficiency of such extension. Our novel approach allows us to extract more expressive rules with aggregates, which are of higher quality and yield more accurate predictions compared to rules learned by the state-of-the-art methods, as shown by our experiments on synthetic and real-world datasets."
            },
            {
                "round2": "Reviewer 1:\nThank you for answering -- about 2, I mean the extracted rules still do not support less trivial operations such as aggregations (sum, mean, ..) and math operations (such as the sum). There is some work investigating how to learn these using neural architectures, such as https://arxiv.org/abs/1808.00508 . It would have been great to see them in here but I understand it can be tricky.\n\nReviewer 2:\nThanks for the reply and also the comments about handling existential quantifiers. Given this, I think I'm fine with the limitations in the current work. \n\nReviewer 3:\nThanks for the response! However, the response does not resolve my concern about whether the task is significant enough in practice. Also the experiment part is not updated accordingly. Therefore I will not change the rating.\n\nReviewer 4:\nThis paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties. The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don't can be discovered by learning an attention distribution over rules from data.\n\nThe idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as \\leq and \\geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework.\n\nA major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse). To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator.  Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs.\n\nAuthors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines.\n\n\nOne thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.\n\nAnother concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities.\n\n\nMissing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.\n\n\nReviewer 5:\nThis paper proposed several extensions to the Neural LP work. Specifically, this paper addresses several limitations, including numerical variables, negations, etc. To efficiently compute these in the original Neural LP framework, this paper proposed several computation tricks to accelerate, as well as to save memory. Experiments on benchmark datasets show significant improvements over previous methods, especially in the case where numerical variables are required. \n\nI think overall the paper is written clearly, with good summarization of existing works. Also I like the simple but effective tricks for saving the computation and memory.\n\nOne main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general. For example, if rules contain quantifiers, how would this be extended? \n\nMinor comments:\n\n1) 4.1,  “O(n^2/2)” -- just put O(n^2) or simply write as n^2/2.\n2) How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i? In the extreme case it would be all the permutations.\n3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.\n\n\nReviewer 6:\nThis paper proposes an interesting extension to the Neural LP framework for learning numerical rules in knowledge graphs. The proposed method can handle predicates involving the comparison of the numerical attribute values. The authors demonstrate its effectiveness on both synthetic knowledge graphs and the parts of existing knowledge graphs which consider numerical values.\n\nI recommend the paper to be rejected in its current form for the following 3 reasons:\n\n(1) Although the idea of making numerical rules differentiable is interesting, the current proposed method can only deal with one form of numerical predicate, which is numerical comparison. The limitation to such a special case makes the paper somewhat incremental. \n\n(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications. Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement. The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.\n\n(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks. A good place to start with is to visualize(print out) the learned numerical rules and see if they make any sense. The experiment section needs significant improvement, especially when there is space left.\n\n\nThe authors can consider improving the paper based on the above drawbacks. I encourage the authors to re-submit the paper once it's improved. \n"
            },
            {
                "round3": "We appreciate the comments of the reviewer. Please see our reply below.\n\n1) - \"... the current proposed method can only deal with one form of numerical predicate, which is numerical comparison.\"\n\nApart from simple numerical comparison we are also able to deal with complex classification operators that aggregate numerical attributes using linear functions, where the threshold value is selected in a systematic fashion, (see Classification Operators) as well as negated atoms (see Negated Operators on p. 6). We note that such rules are indeed limited to some extent, but they still capture a rather expressive fragment of answer set programs with restricted forms of external computations [Eiter et al., 2012].\nBelow we present examplar rules learned by our framework, which are not restricted to numerical comparisons.\n\n2a) - \"The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.\"\n\nWith the rapid development of industrial and scientific knowledge graphs, we believe (and agree with the Reviewer #2) that learning rules that involve multiple modalities is an important and relevant problem. Indeed, such rules can not only be used for data cleaning and completion, but they are also themselves extremely valuable assets carrying human-understandable structures that support both symbolic and subsymbolic representations and inference.\n\n2b) -  \"The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.\"\n\nTo the best of our knowledge Freebase and DBPedia are the only standard KGs with numerical values [Garcia-Duran et al., 2018] used for the evaluation in state-of-the-art works. This is the reason why we have selected and used them for our experiments. The impact of our approach might appear to be rather modest, since these KGs still have only a limited amount of numerical information. Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets. We would be happy to learn about other datasets suitable for our experiments.\n\n3) - \"The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks. A good place to start with is to visualize (print out) the learned numerical rules and see if they make any sense.\"\n\nAccording to the Reviewer's comment we will extend Section 5 on experimental results by showing more detailed analysis. In particular, we will present the following examples of the learned rules from the considered (real-world and synthetic) datasets:\n\n- FB15K:\n\tdisease_has_risk_factors(X,Z) :- f(X), symptom_of_disease(X,Y), disease_has_risk_factors(Y,Z)\nThe rule states that symptoms with certain properties (described by the function f) typically provoke risk factors inherited from diseases which have these symptoms. Here, the function f is the sigmoid over a linear combination of numerical properties of X.\n\n- DBPedia:\n\tdefends(X,Z) :- primeMinister(Z,Y), militaryBranch(Y,X), f(Y)\nThis rule states that prime ministers of countries with certain numerical properties (described by the function f), are supported by military branches of the given country. The function f is the sigmoid over a linear combination of numerical properties of Y.\n\n- Numerical1:\n\tprefer(X,Y) :- isNeighbourTo(X,Y), hasOrder(X,Z1), hasOrder(Y,Z2), Z1>Z2, max{Z2:hasOrder(Y,Z2)}\nThis rule with a comparison operator states that a person X prefers neighbours with the maximal order that is less than X's.\n\n- Numerical2:\n\tprefer(X,Y) :- isNeignborTo(X,Y), hasBalance(Y,Z1), borrowed(Y,Z2), f(Y)\nThis rule states that neighbours with the largest difference between the balance and the borrowed amount are preferred.\nMore precisely, here f selects among all X those entities, for which the difference between the balance and the borrowed amount is maximal. \n\nWe appreciate the Reviewer's comments, which help us to improve the paper. In the final version of the paper we will take them into consideration. In the following we reply to the main concerns of the reviewer.\n\nQ1 - \"... how general this approach would be? ...if rules contain quantifiers, how would this be extended?\"\nThe extendibility of the Neural LP framework is a very important and relevant question, which we also mentioned explicitly as a possible future work direction.\nIn the rules that we support in our framework all variables are universally quantified. While learning rules with existential quantifiers in rule heads is a difficult endeavor in general, even for classical relational learners, the Neural LP framework in principle can be extended to support them as follows: For every relation p, we can create a fresh diagonal Boolean matrix $M_{\\exists p}$, which has 1 at the position (i,i) iff there exists an entity j, such that p(i,j) is in the KG (similar as for classification operators discussed on p. 5). Incorporating these matrices into the framework and filtering rules that have the respective relations in the head should allow us to extract the target rules. Yet analysing how well such approach performs in practice is still an open problem, which we leave for future work. In any case, we will discuss the extendability of the framework in the paper. \n\nMinor comment 1) - 4.1,  \"O(n^2/2) -- just put O(n^2) or simply write as n^2/2\".\nThis is correct, thank you. We will fix this in the final version.\n\nMinor comment 2) - \"How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i? In the extreme case it would be all the permutations.\"\n\nTo avoid exponential enumeration of the predicate orderings sophisticated transformation of the rules has been applied in the Neural LP framework (see [Yang et al. 2017]).\n\nMinor comment 3) - \"I would suggest a different name other than Neural-LP-N...\"\nThanks for this suggestion. We will certainly consider renaming the approach and fixing this in Table 2.\n\nWe are really thankful for the positive feedback. Here we give detailed answers to the Reviewer's concerns.\n\n1) - \"... in Table 2, AnyBurl ... yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.\"\n\nThanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.\n\n2) - \"... the expressiveness of the learned rules can be somehow limited,...\"\n\nWe remark that our framework supports rules with negation, comparison among numerical attributes and classification operators, where linear functions over attributes can be expressed. Such rules capture a fragment of answer set programs, where a limited form of aggregation [Faber et al., 2011] and restricted external computation functions [Eiter et al., 2012] are allowed. While these rules might not cover all possible knowledge constructs, they are still valuable and rather expressive for encoding correlations among numerical and relational features. Moreover, to the best of our knowledge they have not been directly supported by previous works on rule learning.\n\n3) - \"Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 ...\"\n\nThanks for referring us to this important work! We will certainly add this reference to the paper.  "
            }
        ]
    },
    {
        "id": "yRP4_BOxdu",
        "decision": "Reject",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Joint Learning of Full-structure Noise in Hierarchical Bayesian Regression Models\nKeywords: Full-structure Noise, Hierarchical Bayesian Regression Models, Sparse Bayesian Learning, Unsupervised Learning, Brain Source Imaging, Covariance Estimation.\nAbstract: We consider hierarchical Bayesian (type-II maximum likelihood) models for observations with latent variables for source and noise, where both hyperparameters need to be estimated jointly from data. This problem has application in many domains in imaging including biomagnetic inverse problems. Crucial factors influencing accuracy of source estimation are not only the noise level but also its correlation structure, but existing approaches have not addressed estimation of noise covariance matrices with full structure. Here, we consider the reconstruction of brain activity from electroencephalography (EEG). This inverse problem can be formulated as a linear regression with independent Gaussian scale mixture priors for both the source and noise components. As a departure from  classical sparse Bayesan learning (SBL) models where across-sensor observations are assumed to be independent and identically distributed, we consider Gaussian noise with full covariance structure. Using  Riemannian geometry, we derive an efficient algorithm for updating both source and noise covariance along the manifold of positive definite matrices. Using the majorization-maximization framework, we demonstrate that our algorithm has guaranteed and fast convergence. We validate the algorithm both in simulations and with real data. Our results demonstrate that the novel framework significantly improves upon state-of-the-art techniques in the real-world scenario where the noise is indeed non-diagonal and fully-structured."
            },
            {
                "round2": "Reviewer 1:\nThank you for the responses.\n\n1. Venue: You are certainly correct that the overall topic fits ICLR, and I did not intend to question this. The appropriateness is not a binary call, but depends heavily on the writing style and nature of the technical contribution. I still retain my opinion that the paper is not an ideal fit for this conference, even though it naturally fits within the broad borders of the topic definition. As a concrete example, both AISTATS and NeurIPS where many of your examples were published would be slightly better matches.\n\n2. Other models: This is one side of what I was looking for, identifying possible uses cases in other tasks. However, in this form the relation still remains on very high level. It is quite obvious full-rank noise is relevant in many applications, but the more interesting aspect of the related work would be in more detailed discussion on practical level. After all, you rely on specific learning algorithms that cannot be directly plugged in to various specialised algorithms, and it would be more interesting if you were able to point out some examples where your technique could be directly applied.\n\nOverall, I still think the paper is interesting and worth publishing, but in the current form does not advance the field sufficiently from the perspective of this particular venue. I encourage you to keep improving the presentation and looking for the best possible publication venue.\n\nReviewer 2:\nThe paper proposes an efficient optimization method for estimating the full noise covariance in a hierarchical Bayesian framework. It's shown in the experiment that the optimization method could recover the true noise covariance in a simulated example and estimating the full covariance has better performance than homo- and heteroscedastic covariance.\n\nI think the proposed method is an effective tool to estimate the full noise covariance especially for the problem setting in this paper. But the overall novelty and contribution are not strong enough for the ICLR community.\n\nPapers in fMRI literature [Michael Shvartsman et al 2017, Anqi Wu et al 2019] have proposed to work with full noise covariance in more complicated models such as factor analysis, Gaussian process regression. The basic model in this paper is a bit too simple compared with other models preventing from making significant methodological contributions. It might fit a signal processing or brain source imaging specialized publication better.\n\nAlso in many applications (especially with brain data), it's shown that a full rank noise covariance is not always preferable given that there are usually some correlations among measurements that lead to lower dimensional subspace at the noise level. So I'm not quite sure whether a full covariance without any structural or subspace assumption would really outperform low-rank full covariance when applying to the real data.\n\nAnother issue in this paper is there is no real data application. I'm not very convinced that simulated data generated from a realistic lead field matrix is considered as the real-world data. \n\n\nReviewer 3:\nThe authors propose a methodology for type-II maximum likelihood on a hierarchical Bayesian model for EEG signals. The particular feature of the model, which separates it from other EEG models, is the consideration of a full covariance matrix which makes the noise correlated and heteroskedastic. \n\nThe model, as claimed by the authors, is fully Gaussian and therefore tractable. As a consequence, the inference poses no challenges other than the computational complexity. To address this, the authors propose a mechanism for, what they claim is, efficient optimisation. This contribution alone is not sufficient (over the standard literature) for publication as a theoretical improvement. \n\nGiven the lack of a theoretical advancement, I was hoping that the contribution of the article came in the experimental treatment, however, it was not the case.  A single set of experiments using synthetic data was considered, where the proposed method was compared against other benchmark. It is far form surprising when the authors deal with exact inference on a model where the observations where produced under the same statistical assumptions.\n\nI also would like to emphasise that the discussion of the paper states that  \"This paper proposes an efficient optimization algorithm for jointly estimating....\" and \"The benefits of our proposed framework were evaluated within an extensive set of experiments \". None of these claims are true or at least they not validated by any supporting evidence in the paper. \n\nPerhaps with the stated future work and stronger experimental results (real data), this paper can be improved. \n\n\n\nReviewer 4:\nSummary:\nThe paper generalised Type-II ML regression models for scenarios where different noise dimensions cannot be assumed independent, but instead one needs to model the full covariance structure. This is clearly an important problem and it is well motivated in the work.\n\nReasons for score:\nI recommend rejecting the paper even though it represents high-quality work in statistics, because I think it is somewhat tangentially related to ICLR and the contribution would be better appreciated in a different venue.\n\nStrong points:  (1) Addresses an important problem. (2) Seems to work well in practice\n\nWeaknesses: (1) Limited conceptual novelty. (2)Technical contribution hidden in Appendix\n\nDetailed review:\nThe work addresses a relevant statistical question of accounting for correlated noise in hierarchical linear regression, but feels somewhat of a poor fit for ICLR. It formally fits within the scope, but still feels out of place in the sense that neither readers interested in the theoretical contributions nor people looking to apply these methods would consider ICLR as a natural venue to look for the information. The development is restricted to a specific, relatively simple, model family that is frequently used in several fields but that is not at the core of the ICLR community. This is highlighted also by the fact that the technical contribution is largely in statistical properties of the covariance estimator, and for this audience gets hidden in the Appendix. Consequently, I believe that paper would much more naturally fit into a publication forum in statistics.\n\nThe proposed approach itself is sound and well developed. Accounting for correlated noise is a very obvious thing to do, but the technical details are non-trivial. The authors rely on Riemannian optimisation for covariance matrices and are able to use the recent Champagne algorithm for SBL. The detailed derivation of Theorem 2 shows non-trivial technical contribution, but remains somewhat isolated as it is hidden in the Appendix. For example, there is no discussion on whether the result derived here would have uses also in other model families. I can see several potential uses for better tools for learning full covariance noise e.g. in matrix factorisation models (e.g. probabilistic CCA relies on covariance estimates) or non-linear regression models, but the authors do not discuss this at all. A proper discussion on this would be important to link the work more closely to the broader activities in the field, to extend the contribution beyond the current viewpoint of a very specific model.\n\nThe empirical experiments are well carried out and demonstrate the value of learning the full covariance matrix compared to methods that only operate with diagonal noise. This is sufficient, since no clear comparison methods accounting for full covariance are available.\n\nReviewer 5:\nJoint Learning of Full-structure Noise in Hierarchical Bayesian Regression Models\n\nSummary:\n\nThe paper argues that modeling the full covariance structure in a sparse bayessian learning setting leads to significantly better results in eeg inverse problems. The paper details a majorization-minimization type algorithm leading to a set of fairly simple update rules. The proposed method is evaluated on simulated data.\n\nPositive:\n\n1. The proposed method is well motivated and the problem is highly relevant.\n2. The mathematical details regarding the algorithm are presented in sufficient detail.\n3. The paper is well written and easy to follow for the most part.\n4. Experiments are reasonable and presented clearly.\n\nNegative:\n\n1. The abstract could be improved to more clearly describe the problem and contributions of the paper in a self-contained manner.\n2. Has this particular problem (sparse bayesian regression with full covariance noise) not been considered by others? The main contribution, in my view, is algorithmic; which other algorithms have been used previously for this type of problem? (I think both ML-II and MCMC and possibly other methods have previously been used.) I would have liked a review and experimental comparison.\n3. While the experimental evaluation is reasonable, I think the paper would benefit from a demonstration and benchmarking with competing approaches on a real data task.\n4. Experiments on simulated data highlighting more clearly the *algorithmic* advantages of the proposed method would be appreciated.\n5. I did not notice a link to software implementing the proposed method? Sharing software implementations will significantly strengthen the contribution and allow the community to reproduce the results.\n\nRecommendation:\n\nWeak reject.\n\n\n"
            },
            {
                "round3": "We thank all the reviewers sincerely for their constructive comments and valuable suggestions. We studied the reviews and discussions carefully and modified our paper accordingly. Below we provide an overview of the key changes included in [our revision](https://openreview.net/pdf?id=yRP4_BOxdu). \n\n*Major Updates:*\n1. The abstract of the paper has been revised in correspondence to the comment by R4.\n2. The structure of the introduction has reversed for reflecting a general to specific strategy regarding our proposed method instead of having a focused domain-level problem at first.\n3. Theorem 3 is now added to the paper summarizing the convergence guarantees and implications of theorem 2. The corresponding proof is also modified, accordingly. \n4. A real-data analysis is added to the manuscript, which accessed our proposed method for the auditory evoked field (AEF) dataset. \n5. Discussion Section: This part has been changed dramatically for reflecting the broader impact of our proposed algorithm in correspondence to the reviews. This part can be summarized into three parts:\n\na) We mentioned other signal processing and machine learning problems that can be formulated into our setting, e.g., kernel width learning in GP and matrix-norm problems. \n\nb) We listed the papers in fMRI literature that tackles the noise learning problem. \n\nc) We focused on two favorable properties of our proposed method in comparison to the current works in the literature: 1) in comparison to fMRI literature that relies on using EM, we proposed an efficient MM approach with provable convergence guarantees and 2) in comparison to current noise learning approaches that either are relying on Type-I techniques or assuming full knowledge of baseline data, we proposed a joint learning scheme of source and noise within the type-II ML framework. \n\n\n[1] P. Ablin et al., \"Spectral independent component analysis with noise modeling for M/EEG source separation.\", arXiv preprint arXiv:2008.09693. \n\n[2] R. Prasad et al., \"Joint channel estimation and data detection in MIMO-OFDM systems: A sparse Bayesian learning approach.\", IEEE Transactions on Signal Processing, 63 (20), (2015), 5369–5382.\n\n[3] P. Gerstoft et al., \"Multisnapshot sparse Bayesian learning for DOA.\", IEEE Signal Processing Letters, 23 (10), (2016), 1469–1473.\n\n[4] S. Haghighatshoar and G. Caire, \"Massive MIMO channel subspace estimation from low-dimensional projections.\", IEEE Transactions on Signal Processing, 65 (2), (2017), 303–318.\n\n[5] M. B. Khalilsarai et al., \"Structured channel covariance estimation from limited samples in Massive MIMO.\", in IEEE International Conference on Communications (ICC), IEEE, (2020), pp. 1–7.\n\n[6] Y. Feng et al., \"A signal processing perspective on financial engineering.\", Foundations and Trends® in Signal Processing 9 (1–2), (2016), 1–231.\n\n[7] B. Ottersten et al., \"Covariance matching estimation techniques for array signal processing applications.\", Digital Signal Processing 8 (3), (1998), 185–210.\n\n[8] K. Werner et al., \"On estimation of covariance matrices with Kronecker product structure.\", IEEE Transactions on Signal Processing 56 (2), (2008), 478–491.\n\n[9] K. Greenewald and A. O. Hero, \"Robust Kronecker product PCA for spatio-temporal covariance estimation.\", IEEE Transactions on Signal Processing 63 (23), (2015), 6368–6378.\n\n[10] T. Tsiligkaridis and A. O. Hero, \"Covariance estimation in high dimensions via Kronecker product expansions.\", IEEE Transactions on Signal Processing 61 (21), (2013) 5347–5360.\n\n[11] T. Tsiligkaridis et al., \"On convergence of Kronecker graphical lasso algorithms.\", IEEE Transactions on Signal Processing 61 (7), (2013), 1743–1755.\n\n[12] A. M. Zoubir  et al., \"Robust statistics for signal processing.\", Cambridge University Press, 2018. \n\n[13] A. Benfenati et al., \"Proximal approaches for matrix optimization problems: Application to robust precision matrix estimation.\", Signal Processing 169, (2020), 107417.\n\n[14] E. Ollila et al., \"Shrinking the eigenvalues of M-estimators of covariance matrix.\", arXivpreprint arXiv:2006.10005.\n\n[15] S. Kumar et al., \"A unified framework for structured graph learning via spectral constraints.\", Journal of Machine Learning Research 21 (22) (2020) 1–60.\n\n[16] A. Flinth and A. Hashemi, \"Approximate recovery of initial point-like and instantaneous sources from coarsely sampled thermal fields via infinite-dimensional compressed sensing.\",  in 26th European Signal Processing Conference (EUSIPCO), IEEE, (2018), 1720–1724.\n\n[17] H. Wei et al., \"Bayesian fusion and multimodal DCM for EEG and fMRI.\", NeuroImage 211, (2020) 116595.\n\n\nWe thank the reviewer for insightful and constructive comments. We are also very glad that the reviewer found our contribution valuable from a statistical perspective.  Here are our responses  to the main points raised by the reviewer: \n\n1. _Submission is appropriate for the ICLR conference:_ The *“Call for papers”* of the general track of the ICLR submission page encourages authors to submit contributions in statistical fields with neuroscience applications. Besides, recently published works in prestigious ML conferences like NeurIPS, ICML and AISTAT, e.g., [[J-A Chevalier, et al. NeurIPS 2020](https://papers.nips.cc/paper/2020/hash/1359aa933b48b754a2f54adb688bfa77-Abstract.html)],  [[Tao Tu, et al. NeurIPS 2019](https://papers.nips.cc/paper/2019/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html)],  [[D. Sabbagh, et al. NeurIPS 2019](https://papers.nips.cc/paper/2019/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html)], [[A. Farshchian et. al, ICLR 2019](https://openreview.net/pdf?id=Hyx6Bi0qYm)], [[M. Shvartzman, et. al, AISTAT 2018](http://proceedings.mlr.press/v84/shvartsman18a.html)] (_pointed out by the first reviewer_), [[M. B. Cai, NeurIPS 2016](https://papers.nips.cc/paper/2016/hash/b06f50d1f89bd8b2a0fb771c1a69c2b0-Abstract.html)], [[D. Bartz, NeurIPS 2014](https://papers.nips.cc/paper/2014/hash/fa83a11a198d5a7f0bf77a1987bcd006-Abstract.html)], and finally [[S. Hitziger et. al, ICLR 2013](https://openreview.net/forum?id=4eEO5rd6xSevQ)]; strongly motivate us to target ICLR as our publication venue. \n\n2. _Our algorithm can be incorporated into more complex graphical models:_ \nWe agree with the reviewer on this point and will dedicate a part in the discussion section to point out the potential benefits of our method in other signal processing and machine learning fields. This section will indeed include the examples provided by the reviewer in addition to some practical examples in which model residuals are expected to be correlated; and therefore, using full-structural noise learning may also prove useful, e.g., spectral independent component analysis [1], direction of arrival (DoA) and channel estimation in massive Multiple Input Multiple Output (MIMO) systems [2-5], robust portfolio optimization in finance [6],  covariance matching and estimation [7-14], graph learning [15], thermal field reconstruction [16], and brain functional imaging [17]. Besides, we will also modify the introduction to better reflect this perspective so that it could convey to the reader this fact that the idea of learning full-structural noise can be generally used in a broader aspect of hierarchical Bayesian regression problems such as variational Bayes. We have incorporated an abridged version of this point in the revised manuscript.\n\n3. _Incorporating technical details in the main text:_ We will modify the manuscript by moving some of the technical contributions from the appendix to the main text, including the convergence guarantees of the proposed method building on the MM framework (Theorem 9). \n\n \n\n\nWe thank the reviewer for insightful and constructive comments. We also really appreciate the positive points regarding the manuscript that are raised by the reviewer. Here are our responses to the negative aspects that are pointed out by the reviewer: \n\n1. *Revised Abstract:* We consider hierarchical Bayesian (type-II maximum likelihood) models for observations with latent variables for source and noise, where parameters of priors for source and noise terms need to be estimated jointly from data. This problem has application in many domains in imaging including biomagnetic inverse problems. Crucial factors influencing accuracy of source estimation are not only the noise level but also its correlation structure, but existing approaches have not addressed estimation of a full-structure noise covariance matrix. Here, we focus on sparse Bayesian learning (SBL) in regression models specifically for the application of reconstruction of brain activity from electroencephalography (EEG). This problem can be formulated as a linear regression with independent Gaussian scale mixture priors for both the source and noise components. As a departure from the classical SBL models where across sensor observations are assumed to be independent and identically distributed, we consider Gaussian noise with full covariance structure. Using ideas from Riemannian geometry, we derive an efficient algorithm for updating both source and the noise covariance along the manifold of positive definite matrices. Using the majorization-maximization framework we demonstrate that our algorithm has guaranteed and fast convergent properties. We validate the algorithm both in simulations and with real data. Our results demonstrate that the novel framework significantly improves upon state-of-the-art techniques in the real-world scenario where the noise is indeed non-diagonal and fully-structured.\n\n2. *Benchmarks:* Regarding algorithmic comparison, we would like to emphasize that to the best knowledge of the authors, the proposed method here is the first *ML-II* work that learns full-structural noise jointly with estimating the sources. Therefore, it would be highly appreciated if the reviewer updates us with similar ML-II works in the literature. Regarding the MCMC methods, we would like to add that due to the high-dimensional nature of existing regression problems in the biomedical field including our target application, e.g., EEG/MEG inverse problem, MCMC techniques might not be a great candidate as they suffer dramatically from their expensive computational complexity, and are not computationally feasible for the general class of large-scale regression problems. This fact has been confirmed in several works in the literature for practical examples such as magnetic resonance fingerprinting [[Metzner et. al, 2018](https://link.springer.com/article/10.1007/s10182-018-00334-0)] as well as in the line of research by Tamara Broderick. please see [[ICML 2018, Variational Bayesian inference and beyond](http://people.csail.mit.edu/tbroderick//tutorial_2018_icml.html)] for future references. The proposed method in this paper on the other hand scales very well for high-dimensional settings. Finally, as we pointed out in the discussion section, only a couple of recent type-I (MAP) techniques tackled the problem of estimating source and full-structural noise jointly. To highlight the benefits of our proposed method, we will specifically add another analysis to compare the performance of our algorithm with the existing type-I techniques with this assumption that they have access to a noise covariance learned from the baseline data.\n\n3. *Real data analysis:* We thank the reviewer for bringing to our attention the importance of having real data analysis for the ICLR conference. We certainly plan to add a real data analysis to highlight the benefits of our proposed algorithms in practical scenarios. \n\n4. *Open-source code:* To address the reviewer's concern, we will poolish our codes and put them on Github for making them conveniently accessible to the public. We will also put a link to this GitHub repository in the modified draft of the manuscript.\n\n5. *Analysis of showing algorithmic advantages:* We will add extra analysis for highlighting the benefits of our proposed algorithm compared to the previous ML-II methods in terms of the value of the negative log-likelihood loss in addition to the computational complexity analysis of the algorithms in terms of run times. \n\nWe thank the reviewer for insightful and constructive comments.  Here are our responses  to the main points raised by the reviewer: \n\nWe would like to emphasize the following points to highlight the main contributions: \n\n1. *Tractable EM algorithm yields slow and non-convergent methods:* Although assuming Gaussian priors certainly simplify the problem, solving the *“Hierarchical”* Bayesian inference problem in the presence of full-structural noise is quite involved since both the source and noise covariances contribute to the covariance matrix of the measurements, i.e., $\\Sigma_y= \\Lambda+L\\Gamma L^{\\top}$. This phenomena dramatically deteriorates the performance of algorithms that are only able to model homo- or heteroscedastic noise. It is difficult to estimate source and noise covariance simultaneously as these parameters are almost indistinguishable from a structural perspective. Note that when jointly estimating sources in the presence of homo- or heteroscedastic noise, there exist a major difference between the structure of the noise and source covariance since $\\Lambda$ has a diagonal structure, while $L \\Gamma L^{\\top}$ forms a full-structural matrix. We believe that using Riemannian geometry as presented in this work is the key to tackle this ambiguity. To highlight the difficulty of the inference problem, we would like to draw the attention of the reviewer to the following reference that elaborates on this matter in the context of type-I regression problem, e.g., solving Lasso in the presence of full-structural noise: [[Massias, et. al, AISTAT 2018, “Generalized Concomitant Multi-Task Lasso for Sparse Multimodal Regression”](http://proceedings.mlr.press/v84/massias18a.html)]\n\n2. _Inference focusses on convergence properties and provides closed-form update rules per iteration not computational complexity:_ We completely agree with the reviewer that focusing on computational complexity is one of the major contributions of this paper compared to other techniques in this area such as the EM algorithm. But it is also worth noting that the proposed method, which is built on the MM principle, also benefits from theoretically proven convergence guarantees, which are not easily achievable by relying on the EM technique that is commonly used in the fully-Gaussian setting.  It is worth emphasizing our contribution within the MM optimization context, as well. If we restrict our attention to the MM class of algorithms, the constructed surrogate convex functions are commonly minimized using an iterative approach. Our proposed MM algorithm, however, obtains a closed-form solution for optimizing the surrogate function at each iteration of the algorithm, which further advances the efficiency of the algorithm.\n\n3. _Broader implications for a larger class of problems:_ Regarding the novelty of the paper, we would also like to emphasize the fact that what we focus on in this article is the specific sparse regression problem within the Hierarchical Bayesian regression framework, but our work certainly has larger implications. For instance, full-structural noise learning could be replaced with other learning parameters like kernel widths in Gaussian process regression or dictionary elements in the dictionary learning problem. This perspective shows that it is straightforward to apply our procedure within more complex models with hierarchical priors where particular variational approximations lead to subproblems as defined in this paper. We now include this point in the revised discussion.\n\nTo address the reviewer’s concern on stronger experimental results with real data, we plan to add real data analyses within the next days in order to highlight the benefits of our proposed algorithms in practical scenarios. \n\nWe respectfully disagree that the sentence \"This paper proposes an efficient optimization algorithm for jointly estimating...\" is not a valid claim of the paper as the proposed method is indeed an efficient optimization algorithm that learns both noise and source covariance in a joint manner.\n\n\nWe thank the reviewer for insightful and constructive comments. We also really appreciate the reviewer for taking the time to carefully read the paper. We would like to stress the fact that our proposed framework applies to more advanced models like matrix-normal (MN), factor analysis (FA), and Gaussian-process (GP) models. All of these models can be decomposed into two components - learning of the source dimension and learning of the noise. Typically in all of these models, the noise is assumed to either be a scalar, or heteroscedastic or have some known structure. To our knowledge, there is no work that describes joint learning of the source dimensions with learning of full-structure noise as outlined in the current paper.  We thank the reviewer for bringing to our attention these algorithms as applied to fMRI data, and we believe that our algorithm for full-structure noise learning could be incorporated to improve these algorithmic frameworks for noise robustness.\n\nAnother important contribution of the proposed method in contrast to existing approaches is that we propose an efficient optimization strategy with closed-form updates in each step, which is accompanied by convergence guarantees. The majority of algorithms in the literature, including the papers suggested by the reviewer, rely on the expectation-minimization (EM) framework, which is quite slow in practice and has convergence guarantees only under certain strong conditions. In contrast, our approach uses the majorization-minimization (MM) framework and by constructing tighter convex bounds on the original non-convex negative log-likelihood cost function, the proposed algorithm benefits from faster and guaranteed convergence compared to EM. \n\nHere are more specific details to questions:\n\n1. Even in the isolated case of estimating the spatial and temporal noise covariance separately, the papers currently available in the fMRI literature assume AR(1) structure for modeling the temporal noise covariance and a diagonal structure for modeling the spatial noise covariance in order to make the implementation tractable, e.g., [[Chapter 3.1, Shvartsman et. al, AISTAT 2018](https://www.sciencedirect.com/science/article/abs/pii/S1053811905002491)]: “In practice, we restrict the form of both the spatial and temporal residuals to be diagonal or autoregressive, since estimating unconstrained $\\Sigma_v$ and $\\Sigma_t$ is still intractable at fMRI scale.” Please also see the following Github repositories links,  [link 1 for Bayesian RSA example](https://github.com/brainiak/brainiak/blob/master/examples/reprsimil/bayesian_rsa_example.ipynb) and [link 2 for MN-RSA](https://github.com/brainiak/brainiak/blob/master/examples/matnormal/MN-RSA.ipynb), regarding the implementation details that have been considered for these methods [[Chapter 5.2, Shvartsman et. al, AISTAT 2018](https://www.sciencedirect.com/science/article/abs/pii/S1053811905002491)], [[Ming Bo Cai, NeuriPS 2016]](https://proceedings.neurips.cc/paper/2016/hash/b06f50d1f89bd8b2a0fb771c1a69c2b0-Abstract.html). We can easily show that our full-structure noise updates can be incorporated within this framework. Furthermore, some papers in the EEG/MEG literature have shown that using a combination of different spatiotemporal structured noise to model richer structures does not significantly improve reconstruction (source dimension estimation) with great increases in computational complexity [[Bijma et. al, NeuroImage 2002](https://www.sciencedirect.com/science/article/abs/pii/S1053811903002155)], [[Bigma et. al, NeuroImage, 2005](https://www.sciencedirect.com/science/article/abs/pii/S1053811905002491)]; and therefore, the source localization accuracy is sufficiently enhanced by taking into account the spatial correlations only. \n\n2. We would like to point out that it is possible to extend our proposed method to low-rank constraints on the noise covariance. This is because the basic update rule for the noise learning does not significantly change, and only extra updating rules need to be added to the estimation procedure. We are currently working on including such constraints in our model. To elaborate this more, we should note that low-rank assumption can be incorporated using that the noise can be decomposed by the Cholesky technique, e.g., let $\\Lambda =AA^{\\top}$, where $A$ is a low-rank matrix. Therefore, it is really straightforward to embed the assumption of low-rank  noise into the full-structural noise updating rule by replacing $\\Lambda$ with $AA^{\\top}$, and then estimating matrix $A$ instead of $\\Lambda$. We are specifically exploring incorporating low-rank assumptions within a Riemannian framework so that we can exploit the full features of this approach.\n\n3. Given the theoretical nature of the conference, we did not include real data analyses; however, we will add real data analyses to the paper within the next days in order to illustrate the efficacy of our approach. \n\n"
            }
        ]
    },
    {
        "id": "zFM0Uo_GnYE",
        "decision": "Reject",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: On the Importance of Looking at the Manifold\nKeywords: Topological Learning, GNN, VAE\nAbstract: Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data."
            },
            {
                "round2": "Reviewer 1:\n**Summary**:\nThis paper investigates different ways of incorporating topological information about the data in the machine learning models. The paper introduces a novel loss that aims to enforce the relational information between data points into the embedding space learned by a Vae on the node features. The experiments demonstrate that for data with a certain topology type, the introduced loss can provide performance when used together with existing methods. The paper opens up possibilities of further investigation into incorporating topological information (if available) into the learning procedure.\n\n**Pros**:\n1. The paper is very well-written and easy to follow. The illustrations also present the idea clearly.\n2. The problem of understanding the importance of topological information is interesting, and could lead to future works.\n\n**Cons**:\n1. I am not sure if there is enough novelty for acceptance to ICLR, especially when the proposed method does not provide obvious benefit over existing methods, both in theory and practice. Specifically, the GR-VAE is a simple extension of VAE which does not yield good performance, unless it’s combined with DGI. Even in that case, it is not obvious to me the benefit is significant except in one case (namely for text representations), but when combined with DGI, it becomes unclear whether the performance boost is actually coming from the proposed loss or some other unintended regularization effect since DGI already uses message passing to incorporate the (local) topological structure. Am I missing something here?\n2. Since the paper is positioned to be an experimental study, it is perhaps acceptable to have limited novelty or improvement. However, the findings in the papers are somewhat expected. For example, we already know that GNN [1] based methods are superior to other methods on citation benchmarks since they account for both features and topology. In this light, I feel there is not too much new insight in the paper to warrant a publication at ICLR.\n3. For an empirical study, considering only four models may not be enough (e.g. different GNNs / graph models encode different topological information and that should be taken into account for a full spectrum). The same holds for feature-based methods. Some examples of this are Deep walk [3], GNN architecture or different VAE architecture and loss (especially there are so many variations of VAE). Most importantly, a comparison to [2] is missing.\n\n**Comment**:\n1. “Regularized” -> “regularizer” in conclusion line 6 paragraph 2?\n2. For a double-blind reviewing process, the funding information should perhaps be removed, although the one in this paper does not expose the authors' identity.\n\n**Conclusion**:\nWhile the work has interesting motivations and is well-written, it has not done a convincing job at demonstrating the effectiveness of their proposed method or shown a thorough experimental analysis. As such, I am inclined to reject the paper in its current form.\n\n**Reference**:\n\n[1] Semi-Supervised Classification with Graph Convolutional Networks (https://arxiv.org/abs/1609.02907)\n\n[2] Variational Graph Auto-Encoder (https://arxiv.org/abs/1611.07308)\n\n[3] Deepwalk: Online learning of social representations (https://arxiv.org/abs/1403.6652)\n\n\n**================================== Update after rebuttal ==================================**\n\nIt seems that the authors have only provided a general comment for all reviewers, which is understandable since most criticisms from all reviewers are on the same weaknesses of the paper.\nWhile I appreciate the author's effort in adding more experiments, I do not think the added experiments and reply properly addressed the concerns shared by other reviewers and myself. For example, it is still unclear what the advantage of the proposed method is or what insights we could gain from this study.\nI think this paper is not ready for publication. As today is the last day of discussion period, I will maintain my original assessment.\n\n\nReviewer 2:\nSummary:\n\nThe authors present a regularisation term for Variational Auotencoders that forces the distance of mapped points in the embedding space to be similar to the distance of those points in the metagraph of the data derived from relational information about these points. The intention of the regularisation term is to enforce a consistent graph between the original representation and the embedded representation in a manner that is agnostic to the structural choices of the model to be estimated. \n\nStrengths:\n\n1) The paper provides a good organisation of existing methods for utilising topological information, and, thus, positions its contributions well in relation to existing work.\n\n2) The empirical results are presented honestly, even when they do not support the proposed method.\n\nWeaknesses:\n\nOrdered form less to more specific:\n\n1) The intention of the paper is unclear.\n\tIs this intended as a review paper of recent research on graph neural networks or to present a new regularisation term? The title and abstract seem to imply that this is intended as a review, but the paper only considers three existing methods and a single modification to VAEs. Unfortunately, the paper is not convincing in either regard, and the idea of analysing topological information to improve classifications and representations is already well discussed in the literature and a very active area of ongoing research.\n\n2)  The efficacy of the proposed regularisation is not convincingly supported either theoretically or empirically.\n\tThe authors state, ‘Notably, GR-VAE is devised to infer topological information solely from a soft constraint, without any architectural requirements such as graph convolutions’ (line 1, p. 4), but this is not discussed further. The intention of graph convolutions is to explicitly encode assumptions about the relationships present in the data, in this situation why would I prefer a soft constraint to a well motivated, explicit one? If structural constraints were unduly restricting the expressiveness of the models, I would expect to see this borne out in the empirical results, but this is not the case across the chemical reaction and citation network experiments. \n\n3) The paper struggles with clarity at points.\n\tSpecifically, equation (1), which describes the regularisation term is unclear as written: does the plus sign in the exponent denote absolute value or something else? This notation is non-standard and I would not be able to faithfully recreate the results as written.  \n\tAdditionally, the method for constructing the meta-graph G should be discussed in more detail. From what is this graph derived? Is it an existing observed graph that describes the observed relationships between the data, ex. the citation network or pixels in an image, or does it describe adjacency of the observations as defined by the observed labels or other meta-information. My concern is that using a soft-constraint which effectively focusses the model on the labels in an ‘easy’ task such as MNIST classification or the synthetic data task hides the fact that the constraint is too soft to produce a useful regularisation of the model, as evidenced by the failure of GR-VAE relative to DGI or even vanilla VAE in the citation network and chemical reactions tasks. \n\nReasons for score: \n\nI vote for rejecting the paper, as while I really do appreciate that the results of the paper are presented honestly, I think there are concerns with the current draft. \n\nQuestions for the rebuttal period:\n\nPlease refer to the questions in the weaknesses section. \n\nReviewer 3:\n==== Summary ==== \n\nThis paper proposes a variant of Variational Autoencoders (VAEs) which takes extra topological information (e.g. adjacency matrix) into account in the loss function during training. The principle objective of the proposed GR-VAEs is to use the learned latent space features as the input for improvement on downstream tasks especially for classification. \n\n==== Pros ====\n\n+ This paper is well-written and the idea is clear and easy to follow.\n+ The proposed GR-VAE, specifically the extra loss function on preserving the geodesic distance seems to be effective for learning desired latent space features from experiments.\n\n==== Cons ====\n\nMy main concern is on the experiment on showing the improvement for downstream tasks from the embedding learned by GR-VAE.\n\n-  I think the goal of the experiment is to demonstrate that, the embedding learned by GR-VAE is superior comparing to other kinds of features such as from the vanilla VAE, so I expect the embedding by GR-VAE is applied to different models (e.g. GraphSAGE, GCN, DeepWalk) instead of just DGI (at least I only notice that DGI is adapted), and on each model the result from using GR-VAE can outperform others using raw data features or other kinds of finetuned features. I think showing the improvement on different models can greatly enhance the soundness of this paper.\n\n- In addition, for the experiment on chemical reaction I think there should be another baseline showing the result of DGI by using the raw data features if possible, this can further demonstrate the importance of GR-VAE. Also, for the experiment on text representation I also expect some result like using finetuned GR-VAE as the input for DGI in the chemical reaction experiment, currently the text representation experiment only shows the strength of the DGI itself, which does not make sense to me.\n\n==== Reason for scoring ====\n\nOverall, I think the proposed GR-VAE is sound if its strength can be demonstrated by more experiment mentioned above, and I am willing to upgrade my rating and vote to accept if such concern can be addressed during the rebuttal period.\n\n==== Minor Comments ====\n\n- The plots in Figure 3 is too blurry to distinguish between the cross marker and round marker.\n- I notice for the synthetic dataset the direction of edges for each node is used as part of the input features, so what is the definition for the edge direction? Also, if we directly combine the raw data feature with embedding by some manifold learning technique, and input it into the vanilla VAE, can we get similar result (the graph topology is preserved) as GR-VAE has? \n\n\n\nReviewer 4:\nThe paper focuses on studying the importance of utilising manifold/topology information for machine learning tasks. To this end, the authors benchmark four different approaches, including VAE, GR-VAE (using graph distances to regularise  embedding distances (as shown in Eq. 1)).  The paper performs experiments on four tasks, including synthetic data, MNIST, text representation, and chemical reactions. As conclusion, the paper demonstrates that in some cases, adding relational information is beneficial, while in other cases, the effect is subtle. Thus, the paper aims to provide a metric for understanding when and how manifold/topology information is needed. \n\nPros:\n\n1. Instead explicitly learning a graph, this paper proposes an implicit method with graph regularisation.\n\n2. The related work is well-explained. The paper did a well summarization of previous methods.\n\n3. The paper performs extensive experiments to study the importance of manifold for prediction tasks.\n\nCons:\n\n1. The latent graph method is not novel enough, since the method can be categorised as graph regularisation, which is a widely used method in recommendation and information retrieval. Could the author explain why this regularisation is picked from a spectrum of graph regularisation algorithms?\n\n2. The comparison of the paper is problematic. First, the methods (DGI, node2vec, GR-VAE, VAE) compared are quite different methods. Can the authors confirm that the comparison is fair and meaningful (e.g. eliminating other confounding factors like controlling the number of parameters)? Second, I am not sure whether this comparison is optimal. In particular, to study the importance of relational information, other method can be used. For example, we can control adjacency matrix received by graph neural networks. We can totally ignore the edge information (like VAE in the paper) or use a predefined graph (e.g. a fully connected graph like in Transformer). In between, we can corrupt input graphs (e.g. randomly adding or deleting some edges) before feeding it to graph neural networks. This approach seems more reasonable to me for studying the importance of manifold. It is difficult to control these in this paper because the methods used in this paper are totally different  (e.g. DGI and GR-VAE differs in both loss function and input format). So, the conclusion of the paper is skeptical. It mainly justifies which method can perform better in downstream tasks instead of justifying the importance of manifold. \n\n3. The introduction is lengthy and should be more focused on the contribution of this paper. Similarly, the other sections need a major revision to highlight the contribution, as the main contribution of the paper lies in the implicit graph regularisation and a comparison of a series of methods with/without relational information.\n\n4. Some baseline methods are not considered, for example the methods learning latent graphs: Semi-supervised classification with graph convolutional networks and Glomo: Unsupervised learning of transferable relational graphs.\n\n5. The acknowledgement of the paper reveals location information, which may be a violation of anonymity. \n\nBased on these cons, I think a more rigorous comparison is needed. "
            },
            {
                "round3": "Answer to all reviewers.\n\nFirst of all we thank all the reviewers for the time and effort dedicated to reading and reviewing our paper. Seeing that several cons were shared across reviewers we decided to address them together.\n\nThe intent of the paper was to study the influence of the topology across the scale shown in the paper. The addition of the GR-VAE is motivated by the intent of filling a particular shade of that spectrum.\n\nWe appreciate the recommendation of including more more models into the study. We added text baselines for GAE and GraphSAGE in the text study, and we are aware that the study could be further expanded with more models. Furthermore, we expanded the text classification results for the DGI using subsampled graphs (i.e. same sampling method by which GR-VAE is trained) and incorporating the representations of the variational autoencoders as features.\n\nWe want to make the same point about the datasets, it has been commented that downstream prediction tasks may not be the optimal comparison method. It was a direct way for us to asses the goodness of the representations in respect to a specific task, however we are aware that can in itself be limiting. We welcome the different suggestions done in the comments (e.g. controlling adjacency or corrupting the input graphs) and we will be looking at them.\n\nFinally, just reiterate our thanks for the reviewers' comments and time invested in looking into our work. All the feedback we received is highly welcome and a helpful light on how to improve and iterate this work."
            }
        ]
    },
    {
        "id": "JFKR3WqwyXR",
        "decision": "Accept (Poster)",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering\nKeywords: Neural ODE, conditional expectation, irregular-observed data modelling\nAbstract: Combinations of neural ODEs with recurrent neural networks (RNN), like GRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time series. While those models outperform existing discrete-time approaches, no theoretical guarantees for their predictive capabilities are available. Assuming that the irregularly-sampled time series data originates from a continuous stochastic process, the $L^2$-optimal online prediction is the conditional expectation given the currently available information. We introduce the Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn, continuously in time, the conditional expectation of a stochastic process. Our approach models the conditional expectation between two observations with a neural ODE and jumps whenever a new observation is made. We define a novel training framework, which allows us to prove theoretical guarantees for the first time. In particular, we show that the output of our model converges to the $L^2$-optimal prediction. This can be interpreted as solution to a special filtering problem. We provide experiments showing that the theoretical results also hold empirically. Moreover, we experimentally show that our model outperforms the baselines in more complex learning tasks and give comparisons on real-world datasets."
            },
            {
                "round2": "Reviewer 1:\nI genuinely thank the authors for putting in the effort to try address my concerns. The draft is now more appealing to the average machine learner to which this conference targets. \n\nAfter carefully pondering specific points, I still have many concerns, which I believe should be carefully addressed:\n\n1. After reading Algorithm 1 for multiple times, I cannot see the motivation behind fixing a step size \\delta t. It seems the inner loop can be rewritten as a single ODE solve, with the the output y_t being the result of applying the outputNN with the corresponding h_t. The overall model can thus be described as \"the state follows an ODEs in between observations, and is applied a jump (defined by an NN at observations)\". This is basically the high-level framework of both GRU-ODE-Bayes and ODE-RNN. \n\nTake GRU-ODE-Bayes as example, the ODE defining the state's evolution in between observations is the continuous-time version of a gated recurrent unit, while the jump update is the gated recurrent unit's update. \n\nIf this accurate, then in my opinion the difference between the proposed NJ-ODE and GRU-ODE-Bayes is that 1) NJ-ODE uses a general NN to define the ODE between observations, as opposed to the cont. time version of GRU, and 2) uses a general NN to define jump updates at observations, as opposed to the GRU update. \n\nWhile the authors have mentioned this distinction vs ODE-RNNs, I believe the authors should also include a description comparing to GRU-ODE-Bayes. \n\n2. After carefully reading eq (7), I sense a striking resemblance compared to the loss in the GRU-ODE-Bayes paper. Loosely speaking, the \"jump part at observation\" can be mapped to the \"preLoss\" in GRU-ODE-Bayes (assuming observation), with the caveat that in the GRU-ODE-Bayes paper, the predictive mean is based on the hidden state prior to jump. The \"continuous part between two observations\" can be mapped to the \"postLoss\" in GRU-ODE-Bayes, with the caveat that p_obs needs to be ignored there (so that the KL between Gaussians is the norm of mean scaled by a function of the shared variance). \n\nApart from theoretical convergence reasons, what are the motivations in making these changes? How does changing specific things (e.g. use pre-jump vs post-jump update for preloss) affect results in practice? What happens if only one aspect of the overall loss is changed? Are the differences significant? \n\n3. I thank the authors for attempting to address my concern regarding the theory. \n\nWhile I agree that it is possible to attain global convergence with sim. annealing, I shall emphasize that this is definitely not what we typically use in practice. While there are also special architectures/training paradigms (e.g. NTK) that guarantee global convergence, there are also ample evidence that these architectures and paradigms differ from what's happening in practice when SGD is used to train an NN. For this reason, I don't think the theory is particularly relevant for justifying the modeling part. \n\nThe theoretical argument, while being a new statement, relies mostly on standard results to prove and does not introduce new math/stats techniques, and therefore is unlikely to benefit theoretician working in related subfields. \n\nReviewer 2:\nFollowing https://openreview.net/forum?id=JFKR3WqwyXR&noteId=akFNuozOZ1p\nI have updated my rating.\n\nConcerns on clarity and motivation have been addressed properly\n\nReviewer 3:\n##########################\nThe paper proposes an algorithm and an analysis of its convergence.\nThe algorithm propose to learn a model of temporal data y_1 , ... , y_T given input x_1, ..., x_T\nThe observations are assumed to arise from a deterministic latent h \ngoverned by a piecewise continuous ode (in between consecutive times t_i, t_i+1)\nwith additional deterministic jumps at transitions.\n\nIn the ODE-RNN paper, the latent h can be expressed as a single ODE for the whole time horizon (rewriting the\njump with a skip transition).\n\nThis paper appears to me as taking this expression and choosing a particular bounded form for the\ndynamics, jump and readout functions\nA statistical asymptotic analysis of the convergence of the algorithms, for random times and inputs is given. \n\n##########################\n\nMethodology:\nI find the paper quite difficult to read, I blame both its structure and my lack of ease with the mathematics used here.\nHowever from what I have understood of the algorithm proposed, I find the methodological contribution very limited.\n\nClarity:\nI come from the machine learning community and read with no difficulty \npapers cited in the related work section.\nIn comparison, I find this paper extremely difficult to read and parse despite containing the same kind of information.\n\nAsymptotic analysis.\nI leave to other reviewers the evaluation of the convergence analysis.\nMy evaluation being partial, my confidence rating is set accordingly.\n\n* For a machine learning paper presenting in the end a 3 line simple algorithm, the paper contains\na lot of superfluous mathematical notation that crowds the paper and make the reading very tedious.\nMany of the papers cited Brouwer 2019, Rubanova 2019, Li 2020, offer a much smoother read in that respect.\nAs is, this paper feels better suited to a more specialist statistics venue.\n\n* For example, many elements are introduced in the main text and are not really necessary to understand what the paper does\nThe detailed section on random inputs is used only in a theorem coming later, why have it in the main text in so much details.\nOn the other end, a description of the method this paper builds on is left into appendices.\n\n#########################\n\nAdditional comments:\n* the formatting of the references is very inconsistent, please update\n\n\n\nReviewer 4:\nSummary: This paper introduces Neural Jump Ordinary Differential Equations as a method for learning models of continuous-time stochastic processes sampled at random time epochs. Specifically, the paper studies the problem of estimating the marginal conditional expectation (i.e., the L2 optimal approximation conditional on the available information) by estimating an auxiliary stochastic differential equation, parameterized by  neural networks, that approximates the conditional expectation of the process of interest at each point in time. The neural networks are trained by using a “randomized” mean squared-loss objective. The main theoretical results in the paper include asymptotic consistency of the optimal objective value in the limit of a large neural network, as well as consistency of a Monte Carlo sample average estimator of the value. The paper also establishes the L2 convergence of the estimated auxiliary solution to the marginal conditional expectation.\n\nThe technical details in the paper are mostly sound, and I believe it should be of interest to a wide community. The question of estimating stochastic models sampled at regular or irregular intervals is of broad utility. There are some technical issues however, but these can be resolved I believe. In particular, in the conclusions of Theorem 4.1 and 4.2, it seems as though the authors claim almost sure convergence, unless I am misunderstanding their statement.  What the authors establish is convergence in L2, but why does is imply almost sure convergence? Wouldn’t one require uniform integrability to conclude more? Furthermore, this is not a process level convergence result, and therefore I do not believe that they can conclude (as in Remark G.3) that the limit holds almost surely. (Also, the authors seem to suggest tin Remark G.2 that they’re not establishing L2 convergence, but this could be a problem with the writing). \n\nComing to the writing, I note that I did find the paper somewhat sloppy in its use of terminology and notation. For instance on p.1 the authors state “...while stochastic processes are continuous in time...” This is not quite true, since one can define discrete-time stochastic processes. I also found the discussion around  justifying “irregular” sampling of the stochastic process to be poorly written. In particular, it is stated that “...dividing the time-line into equally-sized intervals...is again making assumptions and information is lost...” well, any sampling will involve a loss of information, and the randomized sampling process described in this paper also involves assumptions. I don’t think this comment is appropriate. Furthermore, the authors do not make a clear case for why their irregular sampling procedure is appropriate. I’m quite certain that the sampling process introduces bias into the estimation; for instance, Theorem 1 of ref. [1] below provides sufficient conditions under which an “irregularly” sampled estimator of a functional of an SDE is unbiased. The authors must do a better job of justifying their method. I would also urge them to add an example of a randomized sampling process; for instance, a Poisson process sampler would satisfy their definition, in which case the sampling time epochs form an ordered statistic.\n\nComing to the development of the stochastic model, it is unclear to me as to why all of the random “objects” cannot be defined on the same sample space. Essentially, couldn’t one view the sampling process as a point process on the same sample space supporting the SDE? \n\nNext, in Prop. 2.1, the authors state that the optimal adapted process approximating process is \\hat{X}_t — but \\hat{X_t} is only defined pointwise (i.e., at each time ‘t’) and it is not defined as a stochastic process. Indeed, for that the authors must describe the finite dimensional distributions for all finite sets of time epochs to define the stochastic process. I believe it is inappropriate to call this a stochastic process. This doesn’t affect the main results, since the authors only establish convergence in an L2 sense, where the full distribution is not necessary. \n\nSome further minor comments:\n1. Change the term “observation dates” to “observation epochs”.\n2. Change “amount of observations” to “number of observations” (or samples).\n3. On P.3 in the definition of \\lambda_k, the set \\mathcal{B}([0,T]) is undefined.\n4. The notation defining the function \\tilde{\\mu} is very confusing, please change.\n5. P.4 “...since the variation of u...” should be “...since the total variation of u...”\n6. What do you mean by “ergodic” approximation of the objective? Isn’t it simply a sample average approximation? Which ergodic theorem is playing a role here?\n7. I would also urge you clearly define what you mean by \\mathbb{L}-convergence, for completion. \n\n[1] Unbiased Estimation with Square Root Convergence for SDE Models, Chang-Han Rhee and Peter W. Glynn.\n\nReviewer 5:\nThe authors propose a method for learning the conditional expectation of stochastic process in an online fashion. The paper bears a considerable theoretical treatment, derived from the stochastic filtering literature, which is present both in the main body of the paper and the appendix. Besides the model, the paper also aims to provide a theoretical justification of the convergence of their method. \n\nI find the contribution of the paper somewhat obscure, its aims to be incremental with respect to the previous literature, and the experimental validation heavily unconvincing. I support my recommendation through the following points: \n\n- Following the well known (by now) neural ODE and neural jump SDE, the contribution of the paper seems minor. The authors state that they focus on giving theoretical guarantees, however, these are specific and loosely validated experimentally\n- There is a fair amount of space dedicate to the theoretical presentation of the background, I agree with the importance of theory, but I failed to see how that theory supports the claims of the paper. \n-the experiments are limited: only 3 synthetic examples and only one real-world one.\n-the authors states that their method focuses on approximating (directly) the conditional expectation, this seems to be a different with the previous literature. However, if that's the case, the authors should consider more benchmarks such as linear filters (adapted to non-uniformly-spaced data), Gaussian processes, or general time series models. \n\nThis paper does have a contribution. My recommendation is that  the authors show it in a clearer (to the point) manner with an improved experimental validation.  \n\nReviewer 6:\nThe submission studies a simplified model of ODE-RNN and GRU-ODE-Bayes theoretically, proves convergence results, and presents experimental results in companion with the theoretical results. \n\n\nThe paper does a good job in defining concepts precisely. Though, this has come at a cost of highly complex notation, which may hinder the average researcher in the ML+diffeq community, who may not have a strong background in probability theory, to understand the paper. I would therefore recommend the authors to simplify the notation by deferring the precise mathematical definition of concepts such as information sigma algebras to the appendix. \n\n\nThe section (sec. 2.4) on optimal approximation of a stochastic process in the main text is somewhat vague. Optimality certainly depends on the cost function being considered, in which case, the appendix states that the 2-norm is used here. The particular norm being used is somewhat independent of the construction of the probability space, e.g. we could consider the same prob. space and evaluate the difference between the random variable and the fixed prediction using some other function, say the metric function induced by the L1-norm). This makes terms such as “L^2(omega X omega tilde, ...)-minimizer” somewhat confusing. Note my comment here is somewhat handwavy about the precise technicalities, but it should convey the relevant idea. \n\n\nMy main concern regarding the paper is about novelty. It seems that the model considered in section 3 falls broadly in line with ODE-RNN and GRU-ODE-Bayes. On the other hand, the experiments section also doesn’t compare against latent ODE, which is a strong but relevant baseline. \n\n\nThe section (sec. 4) on theoretical convergence results mostly assume that the ERM can already be found. This rather strong assumption therefore leaves the theorems in that section not unexpected, and at the same time, less relevant for practitioners. It is also unclear whether convergence rates can be derived. \n\nThe paper does a decent job in clarifying its relationship with prior work. \n\n\nPost-rebuttal:\n- I thank the authors for improving the presentation of the paper and including additional experiments comparing to latent ODE. "
            },
            {
                "round3": "We thank you and all the reviewers for all the valuable feedback that contributed to considerably improving the paper.  \nWe changed the title of the paper based on your suggestion, thank you. \n\n\n__3. Theory__  \nWe completely agree with you that those methods that provably attain a global optimum are not the ones used in practice. We would like to emphasize that the convergence guarantees we provide are independent of the choice of the optimization method used to find the optimal weights. By citing those algorithms with provably convergence, we only wanted to mention that if you really wanted to have theoretical convergence from A to Z, you could get it.\n\nWe are fully aware that the study of global convergence of the stochastic gradient descent based methods with non convex objective functions and more specifically neural networks is an active research topic with multiple open questions. Therefore, it is natural not to try to solve this hard and independent problem here. However, even if a proper convergence study of this part is not available, it does not mean that the rest of the convergence study is not relevant for justifying the modeling part. Let us recall our main theorem.\n\n(1) We assume that for each number of sampling and for every size of the neural networks, their weights are chosen optimally, as to minimize the loss function. (2) Then, if the number of paths and the size of the neural networks tend to infinity, the output of our model converges in mean ($L^1$-convergence) to the conditional exception of the stochastic process $X$ given the current information.\n\nThis theorem proves that our algorithm with our specific loss function converges to the target process (the conditional expectation of the stochastic process). We want to emphasize that assuming to have the optimal weights (with respect to the loss function) does not immediately imply that the output of the model will give the desired result. For example, when we first started to construct our loss function, the convergence to conditional expectation was not guaranteed. We had to change the loss function in order to guarantee this convergence. For example, in GRU-ODE-Bayes, this type of convergence is not given, meaning that they cannot claim that the output of their model will always give the desired result. For instance, in an additional empirical example we provided (Heston model), their algorithm cannot manage to reach the desired result and produces worse outputs when the size of the network is increased. For this reason, we think the theory is particularly relevant for justifying the modeling part, even if we assume to have found the optimal weights. \n\nConcerning the theoretical argument, while we don’t introduce new tools but rather make use of several existing mathematical tools (Probability theory, Stochastic Calculus and the universal theorem of approximation), we provide a new proof showing that the output of the model converges to the desired result. We are confident that this proof can inspire further research to derive similar theoretical guarantees. For instance, one could be inspired by this proof to show convergence of the ODE-RNN or GRU-ODE-Bayes, or to modify them in order to guarantee convergence. This proof might be adjusted for other similar problems. Therefore, we think that the given theoretical argument can be beneficial to related subfields.\n\nWe thank you for initiating this discussion about the need of theoretical guarantees. \n\n__Summary__  \nWe hope this addresses all of the reviewer's concerns. We thank you again for the interesting discussion and valuable feedback.\n\n\nWe thank you for taking the time to study the updated paper and the answers we provided. We also thank you for your new valuable remarks and concerns which contributed to further improvements of the paper.\n\n__1. Algorithm__  \nYou are completely right that, from a theoretical point of view, the inner loop of the algorithm could be expressed by a single ODE, where continuously-in-time the outputNN is applied to the latent variable $h_t$ producing $y_t$ for all $t_i < t < t_{i+1}$. Actually, this is what we describe in equations (30) and (31) in the appendix. However, if we want to give an implementable pseudo-algorithm, this continuous-in-time procedure needs to be discretized. Since the outputs between observation times are important, we wanted to show how this discretization can be done. We think that it gives additional insight for the practical point of view, which was not provided by GRU-ODE-Bayes and ODE-RNN.\n\nYou are correct about the high-level framework, which is the same as in GRU-ODE-Bayes and ODE-RNN. To establish the connection to this, we added a description similar to the ODE-RNN description (new equation (7)), and clarified that the pseudo-algorithm is an implementable version of it. Thank you for pointing this out.\n\nYour description of the GRU-ODE-Bayes architecture and the difference to NJ-ODE is completely correct. As we described in the paper in the paragraph “GRU-ODE-Bayes” after equation (6), this architecture can be understood as a special case of the ODE-RNN architecture. In particular, a GRU is used for the RNN and a continuous version of the GRU for the neural ODE. Therefore, we think it is enough to explain the difference between our model architecture and the ODE-RNN architecture.  Thank you for pointing this out, we clarified that in the paragraph where the GRU-ODE-Bayes is presented. \n\n__2. Loss function__  \nYou are right that there exist similarities between the loss function of GRU-ODE-Bayes and NJ-ODE. This is not surprising, since both models try to control the jumps and also the behaviour between the jumps, which is done by the respective parts of the losses.\nTo respond to your question, apart from theoretical convergence reasons, there is no other motivation for our choice of loss function. Those choices were completely guided by the convergence proof. We did not try to change specific things as you suggested (combination of different loss functions) for the following reason. \n\nTo apply a negative log-likelihood loss function (pre-loss) and a loss function based on the KL-divergence (post-loss), an assumption on the conditional distribution is needed. In particular, GRU-ODE-Bayes makes the assumption that the conditional distribution is gaussian and models its time-dependent parameters. This assumption is restrictive, if the true underlying conditional distribution can not be described by a gaussian distribution (as for example in the case of the Heston dataset). We go another way, where we do not make any assumption on the underlying distribution. This implies that we cannot use a loss term where such an assumption would be needed.\n\n\n\nWe thank you for having studied our updated paper and for updating your score. We are glad that you are satisfied with the new version.\n\n\nThank you for your review.\n\n__Simplify the paper__  \nWe have simplified the main paper a lot, by moving the precise mathematical definition of concepts and the theorems to the appendix. The paper is now written in a similar way as other papers cited, in order to reach a bigger audience of the Machine Learning community.\n\n__Section on optimal approximation is vague__  \nYou are completely right that this section (but also the corresponding references in the abstract and introduction) was vague and that a different norm would yield different optimizers. Therefore, we put more emphasis on clarifying that we consider the minimization problem with respect to the $L^2$-norm throughout the paper.\n\n__Novelty__  \nOur model is different from the previous work. In the ODE-RNN and in the GRU-ODE-Bayes, it is a recurrent neural network, where between two observations, the hidden state is modeled by a neural ODE. In our model, we don’t use any recurrent neural network. This makes the model much easier and faster to train. We instead have a neural ODE that takes three more parameters (the last observation, the current time and the duration between the current time and the last observation). Moreover, we want to emphasize that we provided a novel training framework. For the first time, we provided a mathematical formulation, a rigorously defined problem statement and based on the new objective function, the theoretical guarantees that our algorithm works, which would not have been possible with a different training framework. In the GRU-ODE-Bayes paper, the authors do not give any theoretical guarantees for the model, only an empirical study.  In contrast, our method is proven to converge to the optimal solution.\n\n__Added comparison with latent ODE__  \nIn our paper we consider exactly the same task as the GRU-ODE-Bayes paper. The latent ODE paper, although being very similar, does not consider the same task. In particular, the latent ODE (as it is) can not be used for online forecasting. This is reflected in the way it is applied to the extrapolation task, where the model is trained in a supervised learning setting, mapping the first half (input) of the time series to the second half (target). Compared to this, our (and GRU-ODE-Bayes) approach can be interpreted as unsupervised learning. In contrast to the latent ODE, the ODE-RNN might be used for the online forecasting task, but the authors emphasize (in their paper and their official implementation of the models) that ODE-RNN should be used for interpolation tasks only.\nThis is the reason why we did not compare our method to the latent ODE in the first place. \nAlthough our approach is different from latent ODE’s approach, their extrapolation task is one that can be tackled by both models. We have added an experiment to the paper, where we apply our model in the exact same setting as the latent ODE for the extrapolation task on physionet. Our model achieves a performance of $1.945 \\pm 0.007$ ($\\times 10^{-3}$) and outperforms the latent ODE with a reported performance of $2.208 \\pm 0.050$ ($\\times 10^{-3}$).\n\n__Assumption that ERM can be found in convergence results__  \nWe deliberately constructed the loss function in such a way that convergence can be proven, therefore, it is not surprising that our results are expected. However, it is the first time that such a proof was provided for the class of neural ODE based models.\nWe have added a paragraph after the (informal) theorem outlining that the assumption that the ERM can already be found is not restrictive. There exist global optimization methods, as for example simulated annealing, that provably converge to a global optimum in probability. Apart from that, several works try to show that most local optima of neural networks are nearly globally optimal. This implies that in practice, using standard stochastic gradient descent methods which converge to local minima, will supply nearly optimal weights that should be good enough for the approximation.\nSince the proofs of our theorems depend on the universal approximation theorem, which does not provide convergence rates, we also cannot derive convergence rates from our analysis.\n\n__Summary__  \nWe hope we have addressed every concern that the reviewer has raised. We would be very happy to have further discussion if there are any other obstacles to raising the review score.\n\n\nThank you for your review.\n\n__Show in a clearer manner__  \nWe have rewritten the main paper and all the rigorous and precise mathematical formulations are moved in the appendix to let place for the important message we want to transmit. It is now written in a very simple way, in order to reach a bigger audience. Thank you for this input.\n\n\n\n__Contribution over existing work__  \nOur model is different from the previous work. In the ODE-RNN and in the GRU-ODE-Bayes, it is a recurrent neural network, where between two observations, the hidden state is modeled by a neural ODE. In our model, we don’t use any recurrent neural network. This makes the model much easier and faster to train. We instead have a neural ODE that takes three more parameters (the last observation, the current time and the duration between the current time and the last observation). Moreover, we want to emphasize that we provided a novel training framework. For the first time, we provided a mathematical formulation, a rigorously defined problem statement and based on the new objective function, the theoretical guarantees that our algorithm works, which would not have been possible with a different training framework. In the GRU-ODE-Bayes paper, the authors do not give any theoretical guarantees for the model, only an empirical study.  In contrast, our method is proven to converge to the optimal solution.\n\n\n__Additional experimental validation__  \nThe main contribution of our project is the theoretical justification and the rigorously defined framework with the theoretical guarantees of convergence. This was not done by any paper prior to our work. For that reason, we think that having tested our model on those three synthetic datasets and on a real world dataset in addition to our theoretical guarantees was suffisant to prove that our algorithm works well in different scenarios.\nHowever, we have improved our experiment validation by adding the following experiments:\n- Heston model without the Feller condition. \n- Switching regime. In the first half of the path, the stochastic process is following a model M1 and in the second half of the path a model M2. \n- Model with explicit time dependence, i.e. where the drift of the SDE depends on t.\n- Convergence study also on Ornstein-Uhlenbeck and Heston dataset.\n- Experiments on Physionet in the same setting as the extrapolation experiment of the latent ODE (ODE-RNN), adding another comparison to a baseline model.\n\n__Different task than in previous literature__  \nActually, the task of predicting the conditional expectation is not new. It is different from the tasks considered in the latent ODE, but very similar to what the GRU-ODE-Bayes does. They also estimate the conditional expectation together with the standard deviation under normality assumption. More precisely, they try to predict the conditional distribution, under the assumption that it is given by a normal distribution. The predicted mean parameter of the normal distribution therefore is exactly the conditional expectation, which is the main interest in all real world forecasting applications. This is the reason why we mainly compared our method to GRU-ODE-Bayes.\n\n__Summary__  \nWe hope we have addressed every concern that the reviewer has raised. We would be very happy to have further discussion if there are any other obstacles to raising the review score.\n\n\n\nThank you for your review.\n\n__Some unclarities in conclusion of Theorems__  \nThere was a misunderstanding, we thank you for pointing this out. We try to clarify the points. The conditional expectation is the $L^2$-minimizer for the considered forecasting task. However, we can show convergence of the output of our model to the conditional expectation only with respect to the $L^1$-norm (additional assumptions would be needed for convergence in $L^2$-norm). It is important to differentiate here between \nthe 2-norm that is used to make the d_X-dimensional random variables 1-dimensional inside the expectations and \nthe $L^1$-norm used to show $L^1$-convergence to 0 for this 1-dimensional random variable. \nSince the 2-norm is equivalent to any other norm on $\\mathbb{R}^{d_X}$, this choice does not influence the result in any way. \nAs correctly remarked by the reviewer, convergence in $L^1$ does not imply almost sure convergence. However, we did not claim almost sure convergence, but only that the limits are equal almost surely, which is a direct consequence of $L^1$-convergence. To make our claim clearer, we added Lemma E.6 stating this consequence and reference to it.\n\n__Sloppy use of notation and terminology__  \nWe agree that the terminology wasn’t used appropriately at the outlined points and thank him for bringing this to our attention. We changed the passages to be more precise and appropriate. \n\n__Irregular sampling procedure & sampling process as point process__  \nWe are not sure to correctly understand the remark. The irregular observation dates are needed to describe data that is observed at irregular times. In particular, we do not take the point of view that we have a model of which we can sample as often as we want. Instead, we try to give a mathematical description for data that is irregularly observed at random time points. We changed the terminology in the paper the better explain our point of view. We tried to keep the definition of the irregular observation times very general, under the  assumption that the observation times are independent of the stochastic process. \nThe reviewer is right that point processes on the same probability space are a way to define observation dates that might be correlated with the stochastic process. We had the  impression that the given way of defining the observation dates make them easier to understand, even though the product probability space consequently has to be considered. As suggested, examples for randomized sampling processes were added to the paper, including the suggested Poisson point process.\nWe do not analyse the bias of our algorithm, but only show that it is consistent. It is clear that the sampling procedure can introduce a bias to an estimator. An extreme case would be, that the time interval is divided in half and observations are only made on the first half, for which any estimator could hardly learn anything about the second half. However, this is already incorporated in our convergence results through the dependence on the probability measures $\\lambda_k$. \n\n__Is__ $\\hat{X}_t$ __a stochastic process?__  \nYes, in contrast to a Gaussian process, in its basic definition, a stochastic process is just a collection of random variables indexed by some index set (Wikipedia). Often this set is the time interval, as for example in the definition in [1, page 3 after Theorem 1]. In particular, a stochastic process is defined pointwise at each $t$. Therefore, this definition applies to $\\hat{X}_t$, which is defined pointwise.  \n[1] P. Protter. Stochastic integration and differential equations. 2005.\n\n__About further minor comments__\n1. We do not understand why the term “observation epochs” would be suited better. In particular, observations are always made only at discrete time points, rather than on time intervals (which would correspond to “epochs” from our point of view). Maybe we misinterpreted this comment?\n2. We changed it, thanks for pointing this out.\n3. The definition of $\\mathcal{B}([0,T])$ was added, thanks.\n4. We made a small change to the notation. We are not sure which notation would be better or less confusing for $\\tilde{\\mu}$. Would you have any suggestions?\n5. Correct, we changed it, thanks for pointing out.\n6. What we mean here is that we take an average base only on one realization of the path, by averaging over the different observations in time rather than by averaging over multiple realizations of the paths. Such a time-average only equals the sample average under ergodicity assumptions, which we assume to be satisfied here. We do not explicitly use an ergodic theorem, but suppose that the claim of such a theorem is satisfied in the stated way.\n7. The definition was added to the paper (now in appendix).\n\n__Summary__  \nWe hope this addresses all of the reviewer's concerns. If the reviewer has any further questions by which our paper and their score may be improved, then we would be happy to address these as well.\n\n\nThank you for the review.\n\n__Difficult read__  \nWe simplified the paper a lot. Everything is now described with simple words and there are no more mathematical technicalities in the main paper. The rigorous problem statement, the mathematical description and all the theoretical guarantees are moved into the appendix. To clarify even more the paper, we describe the previous methods and we explain how we built our model in the main paper as you suggested.  \n\nWe believe that it is important to have a solid and rigorous mathematical explanation of those recent techniques and we think that this is a significant contribution for the machine learning community. However, we completely agree with you that it can be explained in a better and simpler way, such that a bigger audience can understand and take advantage of our contribution. This is what we tried to do and we hope that you will appreciate the current version. Thank you for taking the time to go through it. We also changed the formatting of the references as you suggested.\n\n__On choice of venue__  \nWe have submitted to ICLR because our paper is about improving an already-existing machine learning technique. \n\n__Novelty__  \nOur model is different from the previous work. In the ODE-RNN and in the GRU-ODE-Bayes, it is a recurrent neural network, where between two observations, the hidden state is modeled by a neural ODE. In our model, we don’t use any recurrent neural network. This makes the model much easier and faster to train. We instead have a neural ODE that takes three more parameters (the last observation, the current time and the duration between the current time and the last observation). Moreover, we want to emphasize that we provided a novel training framework. For the first time, we provided a mathematical formulation, a rigorously defined problem statement and based on the new objective function, the theoretical guarantees that our algorithm works, which would not have been possible with a different training framework. In the GRU-ODE-Bayes paper, the authors do not give any theoretical guarantees for the model, only an empirical study.  In contrast, our method is proven to converge to the optimal solution.\n\n__Summary__  \nWe hope we have addressed every concern that the reviewer has raised. We would be very happy to have further discussion if there are any other obstacles to raising the review score.\n"
            }
        ]
    },
    {
        "id": "eIPsmKwTrIe",
        "decision": "Reject",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Using Deep Reinforcement Learning to Train and Evaluate Instructional Sequencing Policies for an Intelligent Tutoring System\nKeywords: Deep Reinforcement Learning, Intelligent Tutoring Systems, Adaptive policy, Instructional Sequencing\nAbstract: We present STEP, a novel Deep Reinforcement Learning solution to the problem of learning instructional sequencing.  STEP has three components: 1. Simulate the student by fitting a knowledge tracing model to data logged by an intelligent tutoring system. 2. Train instructional sequencing policies by using Proximal Policy Optimization. 3. Evaluate the learned instructional policies by estimating their local and global impact on learning gains.  STEP leverages the student model by representing the student’s knowledge state as a probability vector of knowing each skill and using the student’s estimated learning gains as its reward function to evaluate candidate policies.  A learned policy represents a mapping from each state to an action that maximizes the reward, i.e. the upward distance to the next state in the multi-dimensional space. We use STEP to discover and evaluate potential improvements to a literacy and numeracy tutor used by hundreds of children in Tanzania."
            },
            {
                "round2": "Reviewer 1:\nThe paper intends to contribute “a novel framework for optimizing instructional sequencing in ain intelligent tutoring system”. More specifically, this framework uses deep reinforcement learning and evaluates learned policies on historical data.\n\nA major strength of this paper is working with real human data from an application with obvious positive human impact. That working with this rich data comes necessarily with only working with a small amount of data is understandable, and it is not a weakness of the paper.\n\nThe most significant weakness of the paper is that it does not articulate a contribution that centers on representation learning -- the focus of this conference. A representation of student knowledge over time is learned (the 118-parameter HOT-DINA model), but this representation was already contributed by past researchers and does not seem to match the intended contribution of the paper. An action policy for controlling tutor behavior as a function of student knowledge state is also learned, but policy (or its internal details) are not examined from a representation learning standpoint. (For example, does some aspect of the different learned student-specific policies appear to recover/mimic another known aspect of those student identities? If so, would that be a good or bad result?)\n\nThe next most significant weakness is that when the paper makes novel choices, those choices are neither evaluated nor strongly motivated based on the past literature. Why PPO over DQN? (PPO can work in certain settings that DQN cannot, but past work already demonstrated DQN for a very similar setting.) Why HOT-DINA over BKT? (HOT-DINA is a much more expressive model, but the small amount of valuable historical data for this setting may limit the effectiveness of expressive models due to overfitting.)\n\nAdditional weaknesses are noted in the additional section-by-section feedback below.\n\nThis reviewer recommends (2) strong rejection. This is not inherently a paper about representation learning, and, even as a generic applied machine learning paper, it does not sufficiently motivate or evaluate the intended contribution of “a novel framework” for using machine learning in a specific application.\n\nQuestions for the authors:\n\n* Are there structural reasons why Q learning (e.g. DQN) cannot be applied in this setting?\n* Is there a way to verify that HOT-DINA is not overfitting in a way that makes evaluating the system on the same historical information used to train it unsound?\n\nSection-by-section notes:\n\nTitle\n- Focus on “instructional sequence policies” (for ITS, using RL), was hoping for something that sounded immediately relevant to representation learning.\n\nAbstract\n- It sounds like the method might not be as important as the application.\nHopefully we’ll hear more about the representation learned because this is an ICLR submission.\n\nIntroduction\n- The introduction doesn’t state the intended conclusion or motivate the novel parts of the work for the reader.\n- There’s a learned policy in here, but what is the learned representation you want to highlight for this specific venue on representation learning (ICLR)?\n\nSimulating\n- This is how you fit a pre-existing representation to new data, it doesn’t seem like this is the contribution of the paper.\n\nTraining\n- Missing use of domain knowledge:\n  * Is the the 100 timesteps considered in PPO training based on 100 being a typical number of interaction steps with the ITS among the Tanzanian students?\n  * Even with a finite horizon, the rate at which students decide to exit the activity could be used to motivate a discount factor < 1. Presumably you have this information as well.\n- Notes like “Useful to mention this?” suggest the paper was submitted in an incomplete state.\n- It’s interesting to list the design alternatives for representing actions, but each should be contextualized with references to past research that used something similar.\n\nEvaluating\n- The evaluation in terms of local and global impact is unfamiliar to this reviewer (who knows other RL+ITS work). Not enough information is given to pin down exactly what local impact measures.\n- What is the source and meaning of the historical baseline number?\n- Figure 3 is too busy to interpret. Consider presenting it as a chart that aggregates across students using a single line (plus error bars) to represent the mean (plus stddev) state over time for the two policies.\n- The way student-specific models are “evaluated against historical data” on those specific students suggests we are just testing on the training data. Why is this a valid methodology for this application? Cite past work on evaluating RL-based ITS systems to motivate your methods.\n\nRelationship\n- Consider covering work by others much much earlier in the paper so that the reader can understand why you made the choice you did (and that you can convince them that you know RL has been applied to ITS across many decades previously).\n- Near “STEP uses a more powerful deep RL method” -- it is true that policy gradient can be applied in certain applications where Q-learning cannot, but if we aren’t given a note as to whether this is the case in the current application. Based on previous work, it seems like Q learning was applicable. Thus, using policy gradient methods (including PPO) would seem to add needless complication.\n- This section indeed state how the current work is different from past work, but it does not motivate the differences. If others were successful with different methods, why change them for this paper?\n\nConclusion\n- “This paper contributes a novel framework” -- what is novel is the combination of the HOT-DINA student knowledge model with the PPO reinforcement learning approach (within a larger framework shared by many other papers).\n\n\nReviewer 2:\nSummary:\nThe paper describes variants of an intelligent tutoring system (ITS) developed using a newer (but previously published) variant of Knowledge Tracing (HOT-DINA) for assessing student proficiency and an RL algorithm (PPO) for making decisions on items and content areas to try next.  An empirical simulation calibrated to 8 students is reassessed on the same student simulations and improvements over the original tutoring system are empirically demonstrated.  Four variants with differing levels action granularity and knowledge racing are analyzed.\n\nReview:\nI really like the direction this paper is going in but the results as currently reported seem rushed and under-analyzed.  The paper is shorter than the max length of ICLR papers yet omits crucial details about the action selection strategies and presents the empirical gains with charts that lack labels and no textual analysis (just graphs without context) of the individual student trajectories.  There is also no comparison to the existing state-of-the-art from Shen et al. (cited).  Also, the empirical study seems to have been done on the same 8 students on which the model was calibrated, which likely caused significant overfitting and puts the generality of the empirical results in doubt. Finally, much of the IRT terminology is not defined until later in the paper and the related work on RL for intelligent tutoring systems could use some additions (mentioned below).  In summary, this is a good idea and the results are showing promise, but the paper is not ready for publication in its current form.\n\nDetails:\nMy biggest concern with the paper is the lack of rigor in the empirical analysis.  First and foremost, it appears the data from the same 8 students were used for calibrating the model (both the knowledge tracing and the PPO decision-making training) and then those same students were considered in the simulation (testing) of the models.  It seems likely the models were overfit to the data from these students.  A proper empirical study in this kind of educational setting needs to train the models on data from one set of students and then use a holdout set to test it.\n\nNext, the metrics and charts reported in the paper do not make the improvement clear.  The “local impact” metric reported in the paper does not seem like a good way of assessing improvement since (as the authors admit) it multi-counts improvements from one step in subsequent steps.  I suggest removing that metric for the more grounded global metric.  The charts reported in Figure 3 are not understandable or analyzed in the text.  Why are some of the sub-plots blank?  And where are the labels saying which chart is associated with each of the 4 variants?   We can’t tell which problem setting matches each chart.  Finally, while the green line (new method) outperforms the baseline in the majority of charts, there are some where it does not.  But no analysis or explanations are provided despite there being plenty of room in the paper.\n\nFinally on the empirical side, the authors reference the work of Shen et al. who applied DQN to this ITS problem, but the authors do not provide an empirical comparison to this DQN based approach.  Since that is the state of the art, it seems necessary to apply that algorithm here and see if the gains over the baseline are comparable to the new algorithm.\n\nOn the algorithm side, while most of the approach is fairly clear, the description of the “Type 1 agent” omits crucial details about how items or subject areas are actually selected.  Unlike the other 3 cases, where actions are clearly related to items, Type 1 has an action that moves a threshold.  But how is that helpful?  Can’t an agent just move the threshold very low so it thinks all students have mastered all skills?  And how does moving a threshold determine an item to be given to a student.  More detail is needed to understand this case, which seems much different from the other 3.\n\nOn terminology, the paper often uses terms (such as (Guess, Slip, Learn…). on page 3 or  “b” in the last paragraph of page 3)or presents results (for instance the thetas in table 1) before the definitions of these terms.  Since readers at ICLR are unlikely to have an IRT background, the definitions on page 4 need to be moved up to a terminology section towards the beginning.\nOn related work, the paper did a good job referencing several very recent papers but failed to reference some of the papers related to core concepts and also lacks references to other slightly older works that used the same mix of student knowledge tracing with offline RL.  \n\nExamples:\nHOT-DINA first appears on page 3 but no citation is given\nItem response theory is mentioned on page 3 with no citation\n\nOther RL for tutoring systems work;\n“Cognitive modeling to represent growth (learning) using Markov decision processes” – builds a Bayesian representation of student skills and then uses a POMDP to plan in the belief space over skills (similar to the current work’s representation)\n\n“Learning a Skill-Teaching Curriculum with Dynamic Bayes Nets” – similar to current work, calibrates a Bayes Net based on student data  and uses RL to create new policies.\n\nThe last sentence of the first paragraph of section 3.1 seems to be an author’s note or question to co-authors.\n\n\nReviewer 3:\n\t1. Summary of paper:\n\t\ta. The paper contributes a deep RL approach to learning instructional sequencing. The approach called STEP starts by simulating tutor and student models. The tutor simulation is based on the RoboTutor ITS and the student simulations are fit to historical data of children interacting with RoboTutor using the HOT-DINA approach. The instruction sequencing model is then trained using PPO and evaluated using novel measures of sequencing decision impact on local and global learning gains (estimated from running student simulation). The paper contributes one set of experimental comparisons between four variations of the sequencing agent and the RoboTutor baseline in 8 runs of the two systems.\n\t2. Strengths\n\t\ta. At a high level, the proposed approach is well motivated, using RL to optimise parameters in the existing tutoring system or more granularly make sequential decisions about what activity to provide to the student.\n\t\tb. The local and global reward design is a strong contribution that uses historical data and the student simulation (knowledge tracing) to estimate credit for policy decisions in a counterfactual manner.\n\t\tc. The future work discusses the limitation of not using actual children's scores to evaluate this learning model.\n\t3. Weaknesses\n\t\ta. Novelty/impact and context within related literature: \n\t\t\ti. It is difficult to judge the novelty of this work since the related work section is too brief and does not actually describe several works compared against. Additionally, table 4 is not descriptive enough, has potentially relevant work undescribed (Whitehall & Movellan's 2017 POMDP/policy gradient approach), and has work referenced previously missing from it (Yudelson et al. 2013, Pardos & Heffernan 2011).\n\t\tb. Experimental rigour:\n\t\t\ti. Two strong claims are made in the related work section that do not have sufficient experimental evidence. 1) A direct comparison of BKT to HOT-DINA for modelling student knowledge gains is required to show the impact of this central claimed contribution. 2) A direct comparison against Shen et al. 2018 is required to show that PPO is indeed more effective in this domain. This is also important since it is a central claimed contribution.\n\t\tc. Clarity: The clarity of the paper needs significant effort to improve. Instances below.\n\t\t\ti. The structure of the paper reads like a technical report rather than an empirical investigation. The contents would be far easier to understand with a different structure emphasising a research question, background to understand/motivate it, methodology to answer it, results, and discussion.\n\t\t\tii. Several sections are far clearer in the supplementary data document. The reproducibility of the paper is boosted by this. Given the extra space available, several sections could stand to be transferred to the main paper. My original review did not include supplementary data and points around describing data and examples are boosted by adding them to the main paper.\n\t\t\tiii. Until significant rereads, it isn't clear what the relationship is between RoboTutor and STEP/this work. With my current understanding, RoboTutor is an external system that has been used to collect data on children learning various skills using it. This data was then used to evaluate STEP in terms of estimated learning gains.\n\t\t\tiv. The tutor simulator section describes how RoboTutor functions. The student simulator describes how the knowledge tracing model works. Example differences between activities, skills, steps, etc. would make the content clearer. \n\t\t\tv. In the tutor simulation section it states that the child can select activities, so this part is replaced by RL decision-making in agent type 3 and 4, right? This relates to my question about RoboTutor and this work. I am understanding that the current work simulates the exact decision-making process of RoboTutor but can vary/change that process according to the different agent types. Is that correct?\n\t\t\tvi. Bayesian Knowledge Tracing needs a citation and a brief explanation in a background section.\n\t\t\tvii. HOT-DINA needs a citation and a brief explanation in a background section.\n\t\t\tviii. Item Response Theory needs a citation and a brief explanation in a background section.\n\t\t\tix. What is the difference in computational cost between a BKT approach and HOT-DINA?\n\t\t\tx. A clearer highlighting of what data was used would make it easier to read the article. What was the contents of the data collected to enable knowledge tracing in the student simulator? This also ties in to the comment about examples. Adding a running example of an activity, skill, step for the tutoring task would enable a description of what data is collected to measure student knowledge in the data set.\n\t\t\txi. What do the guess, slip, learn, etc. Parameters measure?\n\t\t\txii. \"We use MCMC sampling for Bayesian inference with PyStan rather than the OpenBUGS Gibbs sampling package used in the original HOT-DINA work because PyStan is faster and handles larger datasets.\" This line needs references for all software used, but most importantly, a reference to the original HOT-DINA work is necessary.\n\t\t\txiii. The sole paragraph on page 3 is difficult to parse and seems important to understand how the student simulator works. The paragraph is dense and conversational in style referencing a sequence of steps without making them clearer and referencing equations that are on the next page. It would be much clearer to have the exposition and equations interspersed and use pseudo-code or a flowchart to explain the steps performed to simulate the student (at least a list).\n\t\t\txiv. \"We can train different types of RL agents depending on their state space and range of actions, which depend on how far they depart from RoboTutor’s current decision policy.\" This would make sense as the start of a new section on the experimentation.\n\t\t\txv. A stronger partitioning of content would also be achieved by calling a potential new section at this point methodology, experiments, or evaluation. This would also help organise the next section of state, action, and agent types into a concrete experiment for which the paper is describing the state and action spaces.\n\t\t\txvi. In section 3.2, it is confusing to have states, then actions, then agent types described. It seems like one experiment with 4 experimental variants (agent types) and a baseline (RoboTutor).\n\t\t\txvii. Table 2 does not convey much more information than the explanation before it.\n\t\t\txviii. Figure 4 is very difficult to parse. Instead of showing 36 subplots (with 4 empty subplots) for 8 students and 4 agents comparing against the RoboTutor baseline, it would be far easier to compare performance against students or against agent types, by combining all 8 student type runs for each variant into a single variance-shaded run in a single graph (e.g. using https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.fill_between.html). That would allow for much higher information density and an at a glance comparison between the 5 runs being compared combined across all 8 students. At the very least this should be done by combining all 5 runs into 8 separate graphs, though the previous approach is preferable.\n\t\t\txix. Related work is far too brief and does not make clear what is being compared for many cited works. E.g. It isn't clear why the works that specify reward a certain way in Doroudi et al. (2019) show a disadvantage to the current approach, the works in table 4 don't specify why the current approach is an advance over their contributions.\n\t\td. Reproducibility: \n\t\t\ti. Section 3 (and the paper in general) contains far too little information about the policy representation, learning hyperparameters, network architecture, etc. to understand the contribution.\n\t4. Recommendation: \n\t\ta. Per the weaknesses in the review above, I recommend the paper for rejection. I don't think the weaknesses in experimental rigour can be fixed in time, content space, or degree to support acceptance. The significant editing required to fix the other issues also seem unrealistic in time and space.\n\t5. Minor Issues\n\t\ta. \"(Previous methods used reward=0 or 1 based on correct attempt or something else. Useful to mention this?)\" There is a comment remaining in the paper that should have been removed."
            },
            {
                "round3": "AnonReviewer3 - thanks a lot for your valuable reviews/feedback. We shall keep these comments in mind as we try to improve our work in the future.\n\nAnonReviewer1 - thanks a lot for your valuable reviews/feedback. We shall keep these comments in mind as we try to improve our work in the future.\n\nAnonReviewer2 - thanks a lot for your valuable reviews/feedback. We shall keep these comments in mind as we try to improve our work in the future."
            }
        ]
    },
    {
        "id": "PUkhWz65dy5",
        "decision": "Accept (Spotlight)",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Discovering a set of policies for the worst case reward\nKeywords: No keywords\nAbstract: We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite."
            },
            {
                "round2": "Reviewer 1:\nI'd like to thank the authors for the detailed explanations. They made good points on the interest of studying the $\\ell_2$ ball rather than the $\\ell_infty$ one. As a consequence, I recommend accepting the paper.\n\nReviewer 2:\nFirst of, sorry for the bad formatting, I fixed it. I've also noticed only now that we could use the latex math mode and I'll use it from now on. Finally, sorry for not having been able to answer before.\n\nThank you for your clarifications. They addressed most of my point but I am still confused about my first one, which is also unfortunately my main concern. Let me first recall my initial questioning:\n- although the reward-set setting is quite general: within a ball in the feature space, it includes very unnatural cases that make the problem artificially too complex, in my opinion. Indeed, usually, one should consider that the worst case reward function $r_{min}$ in a reward function family R is the one that is minimal in every state-action-state : $r_{min}(s,a,s') = \\inf_{r\\in R} r(s,a,s')$. In this case, the solution to equation (7) is straightforwardly the single policy that optimizes the return on $r_{min}$. Please discuss more the interest of considering $r=\\psi \\dot w$ with w in a ball (it implies that if some feature takes sometimes positive and sometimes negative values, there is not clear $w_{min}$, and therefore no clear $r_{min}$).\n\nNow, indeed, I've been mistaken, because I was thinking about the $\\ell_\\infty$ ball (infinite norm), not the $\\ell_2$ ball (Euclidean norm). Can we agree that, if we take the $\\ell_\\infty$ ball instead, then the solution is trivial? So, why use the $\\ell_2$ ball? Is it closer to the model uncertainty we need to represent? If this is the case, what is the loss of using the $\\ell_\\infty$ ball instead?\n\nReviewer 3:\nThank you very much for the clarifications, and for the updated paper and experiments. The authors' interpretation of risk/robustness as it relates to the framework is quite interesting. It could be interesting to see the emergence of \"safer\" behaviors of the agent on complex tasks such as driving, in the absence of the true reward, in future work. I am happy with the response and do not have further questions.\n\nReviewer 4:\nI would like to thank the authors for the clarifications. I am happy with the response, as I believe it addresses the issues that I raised in my review.\n\nReviewer 5:\nThank you for addressing the concerns in the above comments.\nIt's definitely interesting the fact that the performance seems to be a bit better than the baseline, to be honest, I would have expected it to not perform as well.\n\nJust to clarify, these are the same tasks on the grid-world from Section 5, correct?\n\n\nI'm adjusting my score based on your responses.\n\nReviewer 6:\nThank you for taking the time to address the comments, I really appreciate the effort.\nBelow are my comments to the response.\n\n- What is a use-case for this approach? So, you give an example of a robot learning locomotion in an unsupervised manner, and argue that this would help the agent prepare for worst-case in subsequent tasks. I'm trying to picture what this would look like in practice. Is what you are suggesting that by simply learning locomotion, the agent might not have covered scenarios that would encounter in follow up tasks, so worst-case performance in the locomotion skill could be poor enough that the new tasks becomes unlearnable?\n\n - Thanks for the clarification in the linear features. This makes sense.\n\n-  Thanks for the clarification in figure 4. It would also be useful to see how the actual performance of the agent compares in traditional methods. The fact that the worst-case performance is better  than for other methods would not be very useful, if the best-case performance is not good enough to complete the task. Even if for a few domains/task, I think this figure must be there.\nIn other words, by ensuring that our worst-case performance is not too bad...what are we losing from the best case performance. Depending on the scenario, it might be acceptable or it might not.\nIf time permits, please include such results.\n\n- On Lemma 3...thanks for the clarification. I see now how that would be useful.\n\n- I understand the time constraint for providing the one baseline I suggested. If you can include that, it will be very appreciated, but if you can't I won't hold it against you :)\nIf you can only add one of the suggestions I have, please make it the comparison I suggested to add for figure 4.\n\n\nReviewer 7:\n= Overview = \n\nThe paper introduces an approach that, given a set of \"basis\" policies, constructs a high-level policy from the basis policies that is able to perform well in a variety of distinct (but related) tasks. Such tasks are described by MDPs with similar state-action spaces and similar dynamics, and differing only on the reward functions, all of which are built as a linear combination of common features.\n\nGiven a set of policies, the paper introduces the notion of \"set improving policy\" as a policy that outperforms any policy in the given set on the family of considered tasks. It provides two examples of such policies (SMP and GPI) and formalizes the problem of computing a SIP with maximal worst-case performance on the set of considered tasks as a max-min problem. It then contributes an incremental algorithm for this problem. The proposed approach is tested in a grid-world environment and the DM control suite.\n\n= Positive points =\n\nThe paper is very well written, with the proposed approach clearly motivated, presented and analyzed. The proposed approach is novel, to the extent of my knowledge, and analyzed both theoretically and empirically. \n\n= Negative points =\n\nMy main criticism is, perhaps, some lack of detail on the experimental evaluation -- particularly in the DM control suite.\n\n= Comments = \n\nOverall, I really enjoyed reading the paper. The problem addressed -- that of building a policy that performs well in a number of related tasks from a set of \"simpler\" policies -- is, in my view, quite relevant for the RL community, and has potentially interesting applications in domains such as robotics.\n\nThe proposed approach is, as far as I know, original and contributes to the state of the art. The paper briefly links its contributions to the existing literature on apprenticeship learning and hierarchical RL, but I would have appreciated some more discussion on these topics -- particularly, I'd like to better understand how the learned policy relates with policies taught through apprenticeship learning.\n\nOverall, the ideas in the paper are presented in a very clear and elegant manner and the results strike me as technically sound. The proposed approach focuses on building a set of \"basis\" policies in such a way that the policy built from them performs as well as possible in all the considered family of tasks. The method is derived from first principles, and the performance bounds provided (framed in terms of the performance of the SMP policy) are then validated empirically. \n\nFinally, the paper is evaluated in a smaller grid-world domain and in the DM control suite. One aspect that could, perhaps, be improved is concerned with the description of the empirical evaluation in the DM control suite: the paper does describe how the family of rewards for these tasks were built, but it would be good to provide some description of the types of policies that the different rewards lead to and how the policy computed by the proposed approach relates (or differs) from them.\n\nReviewer 8:\nSummary: the authors propose to solve a family of related tasks with shared features and rewards that are linear in the features and equivalent up to scaling factor. The main contributions are as follows:\n- a novel framework for analyzing a broad family of generalized policies (policies that are generalized to arbitrary rewards in the task space), including the concept of a set improving policy (SIP), and providing two practical examples that fit this definition, namely the worst case set max policy (SMP) and the well known and studied generalized policy iteration (GPI). It is shown that it is always better to use GPI over SMP, making it an instance of SIP. \n- a novel iterative method for building a policy library for solving the worst-case reward, formulated as a convex optimization problem, along with policy improvement guarantees, an informed method for stopping the algorithm, and the ability to remove redundant policies (termed inactive policies)\n- an empirical evaluation that connects the proposed method to learning a policy library with a diverse set of skills. The theoretical results are also validated experimentally, on a grid world example and control problems from Deepmind.\n\nPros:\n- the work is of very high quality, all motivations seem sound and the theoretical results seem correct\n- the idea of active task selection for building the policy library is very interesting, and it is surprising that this has not been considered within the framework of Barreto et al., 2017 so far\n- the work could be of significance in the apprenticeship/curriculum/meta-RL community, and it is nice to see a more theoretical treatment of this topic\n\nQuestions:\n- If my understanding is correct, the authors use the orthogonal and random basis to propose w at each iteration, but evaluate the resulting SMP policies with respect to the optimized rewards from (8). I am wondering if this is a fair evaluation for the baselines, given that the policies are always evaluated on $w_t^{SMP}$, or whether a new set of tasks (a proper \"test\" set) sampled from B (the standard ball) should be used to fairly compare (8) with the baselines? This would really test the generalization of the method on new instances as well, and is also often standard in the literature for evaluating the performance of a learning policy set. In other words, how robust is the resulting policy library to solving new task instances not previously seen before?\n- Also, one thing that could explain the poor performance of the orthogonal baseline is that the reward seems to be quite sparse when most of the basis elements are set to zero (in the one-hot phi case, wouldn't they be almost always uninformative?) In this case, a more suitable baseline that directly targets diversity could be defined as finding the $w_1, w_2 \\dots w_T$ such that their coverage of the task space is maximized under some prior belief over w (e.g. the standard ball). If I am not mistaken, this problem is similar to the maximum coverage or voronoi tessellation problem, which could be solved in advance and then deployed. (e.g. Arslan, 2016)\n- Performing well relative to the worst-case performance seems reasonable so that the agent does not do poorly on any one task, but it could also be overly conservative. That is, could there be situations where optimizing the worst case leads to the agent not successfully completing the desired objective (e.g getting stuck on locally optimal solution)? \n- at each iteration when the new optimal policy is learned with respect to $w_\\Pi^{SMP}$, is the idea of SMP or GPI and previously learned policies used to help learn this new policy, or is it learned entirely from scratch (e.g. by simple epsilon-greedy)?\n\nMinor comments:\n- the legends in Figure 1a/b and the axis font in Figure 1c could be increased, same with Figure 2\n- is the $\\max_i$ necessary in equation (8)?\n\nOverall, this works proposes a coherent theory for policy improvement, that also leads to useful implementation and interesting empirical insight (and cool visualizations). It can often be hard to obtain all of these at once.\n\nArslan, Omur, and Daniel E. Koditschek. \"Voronoi-based coverage control of heterogeneous disk-shaped robots.\" 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.\n\nReviewer 9:\nGiven a rewardless environment MDP, the authors want to find a set of policies for the worst case reward function. Their process involves two steps: first to select the right set of policies and second to combine them to generate a new policy. The policy selection is made with the only goal to maximize the expected return of highest achieving policy of the set in the worst-case reward function (Equation (7)).\n\nUnfortunately the submission suffers from several serious weaknesses:\n- although the reward-set setting is quite general: within a ball in the feature space, it includes very unnatural cases that make the problem artificially too complex, in my opinion. Indeed, usually, one should consider that the worst case reward function r_min in a reward function family R is the one that is minimal in every state-action-state : r_min(s,a,s') \\eqdef \\inf_{r\\in R} r(s,a,s'). In this case, the solution to equation (7) is straightforwardly the single policy that optimizes the return on r_min. Please discuss more the interest of considering r=\\psi.w with w in a ball (it implies that if some feature takes sometimes positive and sometimes negative values, there is not clear w_min, and therefore no clear r_min).\n- besides ensuring that SMP performance is at least achieved, could the authors elaborate a bit more on why optimize the policy set according to SMP?\n- the authors never theoretically consider combining the policy, apart for stating that a good combination of policy should achieve higher performance than the best of the policy set. For clarity, I would recommend to either stick to the simplest setting of choosing the best policy given a reward function, or to consider policy election that take into account the way the policies are going to be used/combined.\n- the formalization is messy and sometimes unnecessarily confusing. Please see the series of comments below:\n- Definition 3 and Eq. 5: the argmax returns an index, not a policy. (minor)\n- v is not a value, it's a policy performance. It has been very confusing to me, as it led me to think for too long that Lemma 1 was false: choosing the policy that maximizes the value in each state is a form policy improvement that may lead to policies that are stricty better than the best of the policies. Also, I would not use the notation v, that is usually the value-function: a function of the station and not the policy performance like here, i.e. the expectation of the value-function over the initial states distribution. (easy to fix)\n- Definition 4: the argmax returns an action not a policy. (minor)\n- Equation 7: \\Pi lives in? Also instead of max_{\\psi\\in...} \\psi.w, I would use max_{i\\in [n]} \\psi^i.w. (minor)\n- Lemma 3: what is d? (minor)\n- I am still not understanding Definition 6 and Theorem 2. How do we know that the worst-case reward is unique? If we keep only the policies that achieve max performance on \\overline{w}, then we probably only keep one? How do we ensure that there is not another w that makes this policy (or set of policies) to fail?\n\nFor all these reasons, I recommend to reject the submission.\n\nReviewer 10:\nThis was a well-written, and interesting paper to read!\n\nI went over the paper many times, and I am still failing to see the use case for such an approach. I have some questions and comments that need some clarification for me to properly evaluate the submission. Please, take the time to answer the following, so my review can better reflect the paper.\n\n1 - The theory developed in the paper relies on reward functions that can be represented as linear combinations of the features of the MDP. This seems to be restrictive, and intuitively, this would be the exception rather than the rule.\nWhat class of problem could be modeled under this restriction?  In many problems, there is no linear reward function that would allow an agent to achieve the desired behavior, so these techniques would not be helpful. What are some practical setting where this approach would be beneficial?\n\n2 - Lemma 3... this statement is putting an upper-bound on the worst case performance, but since the paper focuses on improvement of worst case performance, it would be beneficial to have a lower bound, but an upper bound doesn't seem too useful.   Essentially, this lemma is saying \"I can guarantee that the worst-case won't be better than this upper bound, and that for some MDP with linear reward function this upper bound is attainable.\" The problem is that we don't know what that MDP is, how likely it is that we would find it, and this lemma allow for the worst case performance to be arbitrarily bad.\nI don't think this lemma, as is, is particularly useful.\n\n3 - On the experimental section, I think there's a baseline that should be included that's missing. What if we have 1 policy and add a task descriptor or extra features to the features vector that corresponds to the type of task? How would the performance empirically compare?\n\n4 - In the learning curves for fig 1.a or 2.a, what does \"value\" (y-axis) represent? If it the return of the agent after training? If so, is it using the extrinsic reward or the transformed linear reward described in line 5 of \"DeepMind Control Suite\"?\n\n5 - Based on equation 4, for definition 2 of SIP. There is always a trivial set improving policy, right? That would correspond to picking the policy for max(v^i_w).\n"
            },
            {
                "round3": "We would like to thank the reviewer for their response. The reviewer raised a good point. There are many reasons to choose the $\\ell_2$ ball: it includes all the directions; the magnitude of the reward doesn’t change the optimal policy in tabular MDPs; in robust optimization it is used when the uncertainty about some parameter is Gaussian and consequently the scaled $\\ell_2$ ball contains the true parameter with high probability; it is the standard assumption in related work and specifically in Apprenticeship Learning. We now highlight one more property that distinguishes the $\\ell_2$ ball from the $\\ell_\\infty$ ball and is in particular relevant to the reviewer’s question. \n\nImplied from the reviewer’s response, is that a possible solution to Eq 7 is to solve the following problem:\n$$\\max_{\\pi\\in\\Pi} \\min_{w\\in\\mathcal{W}} \\psi(\\pi)\\cdot w. $$\n\nThis formulation is similar to Apprenticeship Learning (AL) without an expert, which is different from our approach that is hierarchical. We refer the reviewer to our latest response to Reviewer 3 for a discussion on the similarities between our approach and AL and to Sec B in the supplementary material for more details. \nAs the reviewer suggested, in the case where $\\mathcal{W}$ is the $\\ell_\\infty$ ball, the internal minimization problem in the above has a single solution - a vector with -1 in all of its coordinates. However, with other norm balls the solution to the internal minimization problem is a function of the policy. In the case of the $\\ell_2$ ball, it is the negative SFs normalized. This is important, since it clarifies that solving the min-max AL problem is not as easy and typically requires solving an MDP in each iteration (see the reference for AL above for more details).\n\nNow, the fact that the worst case reward is a function of the policy (or the policy set) forces it to make a tradeoff -- it has to “choose” the coordinates it “wants” to be more adversarial for. This tradeoff is what encourages the worst case reward to be diverse across iterations (w.r.t different sets) and as a result it induces a diverse set of policies. Diversity was an important goal in this work -- in addition to minimizing Equation 7, we were interested in the diversity of the  policies that result from that process.\nThank you for pointing this out. We hope that we answered your question. We will add this discussion when we introduce the $\\ell_2$ ball in the paper. \n\n\n\nWe would like to thank the reviewer for replying to our response and for increasing their score. We appreciate your feedback and believe that it helped us to improve the paper. \n\nRegarding the clarification question: indeed, these are the tasks in the grid world from Section 5. \n\nDear reviewers, we would like to thank Reviewers 1 and 4, for expressing interest in the performance of our algorithm on a test set of rewards. This is an interesting setup which we didn't consider when we submitted our paper. We have now uploaded a new version of the paper where we performed this experiment in the supplementary material Section D. We hope that this is what you referred to in your review, but if it isn't please let us know. \n\nFor your convenience, we also provide a short description of the experiment here. We trained our algorithm and the two baselines in the same manner as we did before. During evaluation, we tested the performance of each method on a holdout set of rewards that were sampled uniformly over the unit ball. The results suggest that our algorithm achieves better performance than the two baselines when measured on this set of unseen rewards. More importantly, to achieve the same level of performance, our algorithm requires significantly fewer policies than the baselines. For more details, please refer to the Supplementary D. \n\nDear reviewers, we have uploaded a revised version of our paper to reflect your comments. For your convenience, most of the changes are marked in blue color. Smaller notation changes were also fixed. \n\nQuestion 2: lemma 3. \nGiven a set of policies, we have a lower bound on the worst case performance of the SMP. This is equivalent to computing the worst case reward w.r.t the current set and measuring the performance of the SMP. We do not, however, have a lower bound on what that value would be given that we run our algorithm for n iterations. That would indeed be a great contribution; we will mention it in the paper as a promising direction for future research. \n\nThat said, we want to emphasize that Lemma 3 is useful, as it provides a clear criterion for the convergence of our algorithm: whenever the upper bound provided in the lemma is achieved, we can stop adding more policies. We would like to point out that this in fact happens in practice, as we illustrate in our experiments (Figure 1a). This is surprising since often upper bounds are not attainable in practice, as the reviewer implied, which makes Lemma 3 even more relevant. \n\nQuestion 3 (experiments). \nThis is an interesting suggestion, and we are working on performing this experiment, although we are not sure if we can make it on time. Our intuition is that that sort of a baseline will be less practical to use as it typically requires many iterations until it will be able to generalize to new tasks (e.g. UVFA [4]) while our algorithm performs well after a few iterations.\n\nQuestion 4, value in figures. \nThe values shown in Figures 1a and 2a correspond to the SMP’s value, given in Definition 5. The way we compute it as follows. For each policy computed by our algorithm, we estimate the associated successor features (SFs) using Monte Carlo estimation: that is, we fix the policy and run at multiple times to estimate the SFs. We run it enough times to guarantee that the estimate is accurate (see, for example, theorem 2 in [2] or lemma 5 in [3],  for a concentration bound on the approximation error for a given number of samples). Now, given a set o n policies pi (and their associated SFs), we compute the worst possible w, which we call w*, (Equation 8). We then compute the inner product between the n SFs and w* and pick the maximum of these n values (Definition 5). This is the value shown in the figures, which represent the worst possible performance of the set of n policies across all possible tasks.   \n\nQuestion 5. \nYes, you are correct: this SIP is exactly the SMP (Definition 3).\n\n\nWe would like to thank the reviewer for their feedback. Our response is split to two parts.\n\nQuestion: “what can be a use case of such an approach?” We believe that we presented an interesting learning framework in this work: in a world without an explicit reward, can an agent define goals to itself and discover interesting behaviors by doing so? Our experiments in DM control suggest that diverse and interesting locomotion skills (such as salta jumping) can emerge from following this process which is an interesting scientific observation. For a more concrete use case, imagine that a robot can teach itself how to move and locomote in an unsupervised discovery phase, later to use these skills when instructed to do more complicated tasks. Since the skills that the robot discovers prepare it for the worst case, some of them are likely to be useful in the future from a robustness perspective, that is, no matter what the robot discovered, we are guaranteed that the robot won’t do too bad when faced with a new task. Thus, the learned skills can also be used to initialize the robot’s behaviour, to be followed by a learning phase. Another interesting contribution of this work is the connection between robustness and diversity in RL: we have demonstrated that by optimizing for the worst case, a diverse set of skills emerges. \n\nQuestion 1 (the linearity assumption):\n\nIt is indeed true that for a *fixed* set of features the linearity assumption is restricting. However, it is this assumption that allows us to develop theory and thus provide theoretical guarantees for the proposed algorithm. Note that the guarantees we provide are in fact applicable in practice, as we illustrate in our experiments. For example, the experiments in the tabular MDP follow the theoretical framework exactly and achieve the upper bound that we developed (see more on that in our answer to point 2 below). The experiments in DM control are also very close to the theory, the only deviation is that we use DRL to optimize the policy for a given reward. We note that this agreement between theory and practice is in fact one of the strengths of our work, since more often than not there is a gap between the two.\n\nAll that said, we point out that in the most general scenario we are free to define the features themselves. Note that, although the rewards are linear in the features, the features themselves can be arbitrary nonlinear functions of state-action pairs. This means that when we are able to define the features the linear assumption is in fact not so strong: for example, in [1], the authors discuss how in the tabular case this is not a restriction at all, and how in continuous state spaces we can find features that approximate any reward function with a certain accuracy.   \n\nAs a somewhat counterintuitive observation, we note that in many problems it is in fact easy to handcraft simple features that generate useful behaviour. This is illustrated in our experiments with DM control, in which using the standard features provided in the suite we were able to generate rich behaviour. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) by simply using linear combinations of the standard features. \n\nThirdly, our experiments suggest that the assumption is not too restricting in interesting problems. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) under this assumption. \n\nLastly, we believe that it should be easy to generalize our approach to a more general setup where the reward is represented as a nonlinear (perhaps a DNN) of the features. In this case, the minimization over w will not be  convex but will still be possible via the same techniques (SGD). This kind of algorithm will resemble GAIL (with a GAN) and we believe that it is an exciting direction of research for future work.\n\n\n[1] Barreto, A., Hou, S., Borsa, D., Silver, D., & Precup, D. \"Fast reinforcement learning with generalized policy updates.\" Proceedings of the National Academy of Sciences (2020).\n[2] Abbeel, Pieter, and Andrew Y. Ng. \"Apprenticeship learning via inverse reinforcement learning.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n[3] Zahavy, Tom, Alon Cohen, Haim Kaplan, and Yishay Mansour. \"Apprenticeship Learning via Frank-Wolfe.\" AAAI (2020).\n\nIt is indeed true that for a *fixed* set of features the linearity assumption is restricting. However, it is this assumption that allows us to develop theory and thus provide theoretical guarantees for the proposed algorithm. Note that the guarantees we provide are in fact applicable in practice, as we illustrate in our experiments. For example, the experiments in the tabular MDP follow the theoretical framework exactly and achieve the upper bound that we developed (see more on that in our answer to point 2 below). The experiments in DM control are also very close to the theory, the only deviation is that we use DRL to optimize the policy for a given reward. We note that this agreement between theory and practice is in fact one of the strengths of our work, since more often than not there is a gap between the two.\n\nAll that said, we point out that in the most general scenario we are free to define the features themselves. Note that, although the rewards are linear in the features, the features themselves can be arbitrary nonlinear functions of state-action pairs. This means that when we are able to define the features the linear assumption is in fact not so strong: for example, in [1], the authors discuss how in the tabular case this is not a restriction at all, and how in continuous state spaces we can find features that approximate any reward function with a certain accuracy.   \n\nAs a somewhat counterintuitive observation, we note that in many problems it is in fact easy to handcraft simple features that generate useful behaviour. This is illustrated in our experiments with DM control, in which using the standard features provided in the suite we were able to generate rich behaviour. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) by simply using linear combinations of the standard features. \n\nThirdly, our experiments suggest that the assumption is not too restricting in interesting problems. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) under this assumption. \n\nLastly, we believe that it should be easy to generalize our approach to a more general setup where the reward is represented as a nonlinear (perhaps a DNN) of the features. In this case, the minimization over w will not be  convex but will still be possible via the same techniques (SGD). This kind of algorithm will resemble GAIL (with a GAN) and we believe that it is an exciting direction of research for future work.\n\n\n[1] Barreto, A., Hou, S., Borsa, D., Silver, D., & Precup, D. \"Fast reinforcement learning with generalized policy updates.\" Proceedings of the National Academy of Sciences (2020).\n[2] Abbeel, Pieter, and Andrew Y. Ng. \"Apprenticeship learning via inverse reinforcement learning.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n[3] Zahavy, Tom, Alon Cohen, Haim Kaplan, and Yishay Mansour. \"Apprenticeship Learning via Frank-Wolfe.\" AAAI (2020).\n\nWe would like to thank the reviewer for their constructive comments, we are sure that our changes to the manuscript following their suggestion have improved the quality of this work. \n\nPlease note that we made substantial changes to the paper following your suggestions. This includes addressing the $\\arg\\max$ in Definitions 3&4, and in Eq 5, and switching the notation in Eq 7. Regarding the question “where does $\\Pi$ live in?”: this is a good point, since the original submission confusingly used the same notation $\\Pi$ for the subset of policies and the set of all the policies in the MDP. We corrected that in the revised draft, such that it's clear that we optimize over a subset $\\Pi^n$ of $\\Pi$ where $\\Pi$ is the set of all the policies. \n\nScalar value functions. Thank you for the comment, we have clarified the notation regarding the value in Equation 3 to clarify that it is the expected value under the initial state distribution of the value function. \n\nQuestion: “keep only the policies that achieve max performance....?”, we would like to refer the reviewer to the reformulation of reward minimization problem in equation 15. There you can see that for any feasible solution w, there is a soft inequality restricting all the policies to have a lower or equal value to that of the optimal policy. These constraints can be binding or not, but both cases are feasible. In fact, if the set of policies is diverse enough, the worst case reward will be chosen such that there is equality. For example think about the simple case where the SFs are: $(0,1)$ and $(1,0)$. The worst case reward will be $(\\frac{-1}{\\sqrt(2)}, \\frac{-1}{\\sqrt(2)})$, and the two policies will maximize it. For more general cases, we refer the reviewer to Figure 2, a, where we validate that empirically in DM control. The blue bars correspond to the number of active policies (that attain the max) after each iteration. The number of such policies is increasing and is clearly larger than 1.\n\nQuestion: “why there is not another w that makes this policy fail”, this is exactly what our proof shows. We would like to refer the reviewer to the definition of the features in the second paragraph of the preliminaries section. From there, it is clear that the features are always positive, and are for dimension d. The later answers the reviewer question regarding d in Lemma 3. The former is important for the proof of the uniqueness of the worst case reward. We hope that Lemma 5 in the revised version will answer the reviewer’s concerns. Note that the places that were changed are in blue color. We explain some parts clearer and more rigorously than in the previous version and we hope that you will find that satisfactory. \n\nRegarding the reward-set setting. Please note that we defined the features to be positive. Although the features are positive w can be negative and in general will be negative. So, for a given set of policies, the solution for the worst case reward is not just the min reward at each state. For example, if your set includes only the policy $(1,0)$ then the worst case reward will be $(-1,0)$ and not $(\\frac{-1}{\\sqrt(2)}, \\frac{-1}{\\sqrt(2)})$. We hope this addresses your concern regarding w_min. Regarding your question about the linearity in the features, see our answer in a separate post(note that this answer is also an answer to R1). \n\n“why optimize the policy set according to SMP?” This is a good question. The focus of this work is on the case that the reward is unknown. Therefore, if our set only includes a single policy, then the worst case reward w.r.t to it will always be “devastating”. Mathematically, this means that if the set of the policies is not diverse, then the worst case reward can choose to be minus the SFs of one of the policies (normalised), that is, to be as adversarial as possible w.r.t a single policy. When the set includes more than one policy, then the policies, under an SMP may complement each other. That is, if the reward is too adversarial w.r.t to a single policy, then it is likely that another policy in the set will be better w.r.t it. This is what happens in practice, when there is more than one policy that maximizes the worst case reward (active policies). In that case, the analytical solution of the worst case reward is not minus the SFs of one of the policies in the set. As a result, the value of the SMP w.r.t the worst case reward is better than that of that of the best policy in the set in isolation. \n\n“stick to the simplest setting of choosing the best policy” -- We revisited the problem formulation paragraph to make it clearer that we focus on the SMP as the mechanism that select policies. Please note that once algorithm 1 finishes and returns a set of policies, this set can be used by other SIPs (such as GPI) to yield better performance. We verify that empirically in Figure 1a. \n\nWe hope that our response addressed all the reviewer’s concerns, but if it didn’t, please point us to the parts that we missed. \n\nWe would like to thank the reviewer for they/there feedback. \n\nQuestion 1.\n\nThe reviewer is correct: for the baselines, we add policies to the set by following the baseline rule (either by sampling a random reward and optimizing it or by adding an orthogonal reward and optimizing it). For each method (including ours and the baseline) we have a different set of policies $\\Pi^t$ in each iteration. At iteration t we compute the worst-case reward w.r.t to SMP’s current set, $w^*(\\Pi^t)$, and report the performance of all the methods under this reward ($w^*$ is computed through (8)). This means that the reported performance of SMP is the worst possible across all the tasks, while this is not necessarily true for the baselines (that is, there might be tasks different from $w^*$ in which their performance is worse).\n\nThe reviewer’s suggestion of adding a “test set” of tasks is interesting. We will do our best to add this experiment to the paper before the end of the rebuttal phase, and will certainly have it in the final version of the paper. As noted above, if we replace the task $w^*$ used in our evaluation with any other task, the performance of our algorithm (SMP) will improve. So, for example, if we report the average performance of our algorithm on a “test set”, as suggested, this value will be greater than the one reported. This is not necessarily the case for the baselines. On the other hand, the performance of the baselines on the test set can in fact be better than SMP’s, since this is not what our algorithm is optimizing. The choice between maximizing the worst-case performance and the expected performance involves several interesting trade-offs; we elaborate on this point below. \n\nQuestion 2.\nThis is also a very interesting suggestion, it would indeed be an interesting reference point. Such an algorithm should work well if we have some prior knowledge of the distribution of w or are able to sample from it (for example, by learning online as tasks are presented to the agent). However, we conjecture that the number of policies needed by such an approach to reach a good performance level would in general be considerably larger than the number of policies used by our algorithm (since in this case one has to cover all the support of the distribution over w with non-negligible probability mass).\n\nOptimizing for the worst case scenario gives us some benefits. First, we do not need to know anything about the distribution of w. This allows us to do things like building the library of policies in a completely unsupervised way, before ever seeing an actual task. Second, we can be very efficient: in our experiments we always found a set of diverse policies after only a few iterations. Focusing on the worst-case reward can also be very useful in scenarios where bad performance has a high cost associated with it: for example, if the agent is an autonomous vehicle, the priority might be to avoid accidents.  \nAll that said, we believe that the two approaches (optimizing for the worst-case or expected performance) are in fact complimentary. Finding a feasible way to cover the space of rewards and combine it with our approach is an exciting direction for future work. \n\nWe will add the discussion above to the paper.\n\nQuestion 3.\nThis is related to the previous question, and also an interesting point. We did observe in our experiments that sometimes our algorithm converged to the optimal value “too fast”. Concretely, this meant that after adding 2-4 policies to the set, newly-added  policies were not diverse or meaningful because w*, the worst-case reward w.r.t the SMP computed through (8), was very close to being a vector with -1 in all of its coordinates. That is, after a few iterations, the benefit of adding a new policy diminished quickly. One direction that we explored to alleviate this issue was to regularize the worst-case reward w* to have zero mean. Note that removing the mean does not change the task but potentially increases the difference in the relative magnitude of the entries in w*. This did indeed help in making the policies more diverse. These experiments have been added to the supplementary material (Section C).\n\nQuestion 4.\nWe used a simple actor-critic agent with experience replay to learn each policy. We experienced both with the case where the parameters are learned from scratch and with the case where they are transferred from one task to another, but it did not seem to make a big difference (same for the experience replay). We did not use SMP or GPI to learn the new task, although this is a great idea to be explored in the future (we will mention it in the paper).\n\n\nWe would like to thank the reviewer for they/there feedback. The reviewer raised a question about the policies learned by following our algorithm: how do they relate to Apprenticeship Learning (AL), and can we better show if they learned similar/different things from one another.\n\nRelation to AL. This is a great question and we debated about it when working on this paper. Both AL and our algorithm can be used to solve the same goal: achieve good performance w.r.t the worst case reward. However, AL is concerned with finding a single policy, while our algorithm is explicitly designed to find a set of policies. To be more specific, we need to refer to a specific AL algorithm, so we will refer to the projection algorithm by Abbeel & Ng (04). The basic idea in this algorithm is that given an estimate of the SFs of the expert (we use our notation though in their paper they refer to a similar quantity as feature expectations) we want to find its projection onto the SFs polytope as well as a policy whose SFs equal this projection. We refer the reviewer to Sec B in the supplementary material for more details, but provide a short summary here for the following discussion. The algorithm achieves that by maintaining a set of policies, and  convex combination of coefficients over the set of policies, known as a mixed policy, such that its SFs are the convex combination of the SFs in the set. The goal is to find the convex combination whose corresponding SFs are closest to those of the expert in the L2 norm. In each iteration, a policy is added to the set by maximizing a reward signal that is defined to be the negative gradient of this objective. \n\nThere is no direct connection between the policies that are discovered from following these two processes. This is because the intrinsic rewards that are maximised by each algorithm are essentially different. Another way to think about this is that since the policy that is returned by AL is a mixed policy, its goal is to return a set of policies that are similar to the expert, but not diverse from one another. From a geometric perspective, the policies returned by AL are the nodes of the face in the polytope that is closest to the demonstrated SFs. Even more concretely, if the SFs of the expert are given exactly (instead of being approximated from trajectories), then the AL algorithm would return a single vertex (policy). Finally, while a mixed policy can be viewed as a composition of policies, it is not a SIP. Therefore, it does not encourage diversity in the set. Our algorithm, on the other hand, is explicitly designed to return a diverse set of policies. \n\n“Description of the types of policies that the different rewards lead to and how the policy computed by the proposed approach relates (or differs) from them”. We would like to refer the reviewer to Figure 3, where we visualize the policies learned by our algorithm in DM control. There we show a snippet of the trajectories taken by different policies. In the supplementary material, we further provided videos that we recorded from these policies. So, for example, the pendulum balances itself up and down, and the cheetah tries to stand on either leg, or to walk in either direction. The walker and the hopper discovered other locomotion skills that are not expected, but the key finding is that they are indeed very diverse from one another. We hope that this is what the reviewer asked for, but in case the reviewer believes that there are some missing details, or in case that we didn’t answer all of they/there questions, please let us know what you think is missing and we would provide more details.\n"
            }
        ]
    },
    {
        "id": "98ntbCuqf4i",
        "decision": "Reject",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\nKeywords: No keywords\nAbstract: The principle of optimism in the face of (aleatoric and epistemic) uncertainty has been utilized to design efficient exploration strategies for Reinforcement Learning (RL). Different from most prior work targeting at discrete action space, we propose a generally information-theoretic exploration principle called Max-Q Entropy Search (MQES) for continuous RL algorithms.\nMQES formulates the exploration policy to maximize the information about the globally optimal distribution of $Q$ function, which could explore optimistically and avoid over-exploration by recognizing the epistemic and aleatoric uncertainty, respectively. To make MQES practically tractable, we firstly incorporate distributional and ensemble $Q$ function approximations to MQES, which could formulate the epistemic and aleatoric uncertainty accordingly. Then, we introduce a constraint to stabilize the training and solve the constrained MQES problem to derive the exploration policy in closed form. Empirical evaluations show that MQES outperforms state-of-the-art algorithms on Mujoco environments."
            },
            {
                "round2": "Reviewer 1:\n\nThis paper proposes MQES, a Max-Q entropy search for policy optimization in continuous RL. The authors propose to combine advantages of the information-theoretic principle and distributional RL, in which epistemic and aleatoric uncertainty are estimated using similar entropy-search acquisition functions in the Bayesian Optimization (BO). As said, this is a new method to introduce a more efficient exploration strategy. As a result, policy improvement is formulated as a constraint optimization problem where a next exploration policy can be solved in a closed-form. The proposed method is evaluated on Mujoco tasks and compared against other off-policy approaches, SAC, and DSAC. The results show MQES outperforms other methods in domains where exploration is needed.\n\nThe main contribution of the paper is to introduce an approximation to distribution Q functions that are based on the epistemic and aleatoric uncertainty. The main objective is based on the mutual information maximization as described in 4.1. A practical implementation is proposed in 4.1. The idea makes sense however the presentation and its experiment results make it hard to understand some important details. Some of my major comments are as follows.\n\n1. Although the idea is interesting, it's not yet clear how the proposed method can be used as an additional module on top of other entropy-regularized off-policy approaches like SAC or TD3, etc. The paper can benefit more if it can be formulated in such a more general way.\n\n2. The practical implementation seems to make sense. However, it's still unclear to me how policies \\pi_E and \\pi_T are parameterized. Although a sketch of the main idea is described in Algorithm 1, it's not clear to me each term is parameterized and computed based on a particular parameterization. \n\n\n3. An updated policy as a solution of (18) gives an update on the mean but keeps the covariance unchanged. I was wondering then how this policy can adaptively change its exploration through the progress of learning?\n\n4. The experiment results are quite preliminary. There are no experiment settings. There are a number of hyperparameters of MQES that might affect overall performance of MQES, e.g. N, \\beta etc. but not discussed and ablated? The comparisons might also take into account other distributional policy search methods.\n\n5. And some minor comments\n\n\t- Eq. 4 and 5, 6: min and log instead of arg functions?\n\n\t- Eq. 9: What is the difference between posterior p(a|Z^*(s,\\pi^*) and \\pi^*? Is the posterior not the optimal policy since Z^* is the optimal distributional value function estimate? A detailed derivation for Eq.9 is expected.\n\n\t- Definitions for terms in 4.1:  p(a|Z^*(s,\\pi^*) vs  p(a|Z^*(s,\\pi) p(a|Z^*(s,\\pi^*) p(a|Z^*(s,\\pi) etc.\n\n\t- in Eq.14: Should the aleatoric uncertainty be the variance of the {min_{\\theta_k} z_i(\\theta_k)} instead of en expectation over \\theta_k. Because z_i is estimated as the min of two estimates, e.g. in Eq.4\n\n\t- Some theoretical steps are not clearly justified of why.\n\n\t- is $n$ used in Proposition 2?\n\n\t- the conventions of CDF in Proposition 2 and in Eq.21 should be made consistent with its first definition in Eq.11\n\n\t- definition of G() is not used after its first definition.\n\n\t- why the horizon is set to 100, instead of 1000 environment steps like in the SAC paper?\n\nReviewer 2:\nThis work introduces max-Q Entropy Search (MQES) exploration principle  for continuous RL algorithms. MQES addresses the exploration-exploitation dilemma that constitutes a fundamental RL problem. Actually, MQES defines an exploration policy able to explore optimistically and avoid over-exploration. One of the main advantages of MQES is its ability to recognise the epistemic and aleatoric uncertainty. Empirical analysis has been conducted on Mujoco, showing that the performance of MQES is comparable to  those of other state-of-the-art algorithms.\n\nIn general the paper is well written and can be easily followed by the reader. Nevertheless some parts of the MQES should be explained in more detail. For instance, authors should give more details about the target policy introduced at Section 4.3. Actually, the reader should check Algorithm 2 at Appendix in order to understand its purpose and how the target policy is updated. I think that it would be better Algorithm 2 to be moved in the main paper if it is possible. Another point that should be discussed more clearly is the impact of the 3 hyper-parameters (\\alpha, \\beta, and C) on the performance of MQES. To be more specific, why did you set the uncertainty ratio equal to 1.6? Finally, the empirical results are not discussed at all. It seems for example that the performance of MQES_G is more stable compared to that of MQES_Q. Moreover, the performance of DSAC is almost equal (or better) to that of MQES_Q. All these points should be explained or discussed by the authors.\n\n\nReviewer 3:\n---\nSummary\n\nThis paper studies the problem of efficient exploration in continuous environment. It proposes a novel algorithm, Max-Q Entropy Search (MQES), and utilizes Distributional SAC (DSAC) to formulate the uncertainties. Experiments show that the proposed algorithm, MQES, outperforms the baselines (SAC, DSAC). \n\n---\nComments\n\nHowever, MQES doesn't show significant improvement over DSAC. For example, Except Sparse-HalfCheetah-v2, MQES_Q has almost same performance as DSAC. Except Sparse-HalfCheetah-v2 and Ant-v2, MQES_G has almost the same performance as DSAC. \n\nAnother question is: The horizon is cut to 100 while most papers and OpenAI Gym use 1000 by default. Why do the authors choose 100? How does MQES perform with longer horizon, like 1000? \n\nThe paper shows that exploring using both aleatoric and epistemic uncertainty can improve the performance. What if we consider only one of them, e.g., using only aleatoric uncertainty? I'd like to see this as ablation. \n\n\n---\nWriting Quality\n\nThe writing can also be improved. \n\nTable 1: Could you please highlight all algorithms within 1 std to the best? \n\nSparse-HalfCheetah-v2: Could you please provide more details about the environment? \n\nWhat does the mutual information between $(Z^*, \\pi^*)$ and $(Z^{\\pi_E}, \\pi_E)$ mean? Are $\\pi^*$ and $\\pi_E$ random variables? Moreover, in deterministic environments (as in Mujoco environments), $\\pi^*$ is also deterministic, so is $Z^*(s_t, a_t)$. \n\nEq 8: LHS is a scalar (probablity), while RHS is a policy. Please clarify notations to avoid confusion. \n\nEq 9: What is $p$? The first input of MI is simply a $Z^*$ while the second is a pair $(Z^\\pi, \\pi(a_t | s_t))$. Please elaborate. \n\n> To measure the intractable distribution of $Z^*$ during training, we use the $\\hat Z^*$ for approximation \n\nPlease rephrase it and say that $\\hat Z^*$ will be defined later. \n\nEq 20: This is not an unbiased estimation, as $\\mathbb{E}[X^{-1}] \\neq \\mathbb{E}[X]^{-1}$. Also please clarify K. \n\n\n\n\nReviewer 4:\nThe paper proposes an exploration scheme for RL in continuous action spaces using the principle of information maximization for globally optimal Q distribution.\n\n1. I felt that the paper isn't well written and discusses a lot of different concepts in a haphazard manner.  There are a lot of equations and symbols in the text without proper explanation and context which make it difficult to gather the main contribution. The language used gets vague in many statements made in the paper. For ex. \"Proposition 1. Generally, the posterior probability is as follows\".\n\n2. A lot of algorithm adaptations are proposed without actually carrying out ablations which make it difficult to discern if the proposed MI maximization is indeed responsible for performance. For ex. \"Since the target for critic in the advanced algorithms, like SAC and TD3, is usually estimated pessimistically..\". The authors should actually present ablations to support if a pessimistic estimate is indeed required for their adaptation for these methods. \n\n3. Why haven't the authors included OAC as a baseline given that it outperforms SAC in several tasks? Further the results show little difference in performance in comparison with DSAC on the mujoco tasks, given that only 5 seeds were used in evaluation, it brings the significance of the results under question. The authors should provide appropriate measures like P-values to support the experiments.\n\nReviewer 5:\nThe paper proposes an information-theoretic approach to exploration in model-free RL, by encouraging an exploration policy that is maximally informative about the optimal (distributional) value function. The authors discuss tractable approximation to this objective which can be implemented in continuous MDPs. The method is evaluated on benchmark continuous control tasks (Mujoco).\n\nThe basic underlying idea is interesting and, to the best of my knowledge, novel. However there are some major issues and/or limitations which should be addressed prior to publication.\n\n**Clarity**\nThe paper is hard to follow and understand. \n* There is a use of terminology which might not be clear or known for the general RL/exploration audience (\"acquisition functions\", \"heteroscedastic aleatoric uncertainty\"). The entire discussion of the two types of uncertainty seems somewhat disconnected from the method itself. A short explanation of why and how the two types of uncertainty are important *for the exploration problem* would be very helpful.\n* There are some notation obscurities or inaccuracies. This is most notable in Section 4.1, which is unfortunate since this is where the key ideas of the approach are discussed.\n    * Equation 8, which is central to what follows, is rather confusing. The text mentions that \"$\\pi_E$ selects action $a_t$ that (...)\", but the equations then seems to define an entire policy. And the optimization problem (in the same Eq.) is in itself dependent on $a_t$, so it's not even clear one gets a valid policy/distribution from something like $\\pi_E = \\arg\\max_\\pi F^\\pi(s_t,a_t)$. \n    * Following the previous point, Eq. 9 is also confusing. It's not clear to me what the authors mean by measuring MI between $Z^*$ and *the pair* $(Z^\\pi, \\pi)$? It's also not clear what is the meaning of the \"posterior distribution\" denoted by $p$, and why the mutual information is measured for the Z parameters (return probs) but then re-expressed as the difference in entropies for the policies (action probs).\n\n**Quality**\nThe paper has a good balance of a theoretically motivated algorithm, a practical implementation of it, and some basic empirical evaluation. Other than clarity issues discussed before, I have some concerns regarding the evaluation, and one more conceptual concern regarding the general idea:\n* Since exploration here is encouraged by choosing informative actions about Q*, it's not clear that this method will be helpful in very sparse-reward settings (which are a central motivation for sophisticated exploration techniques). Put differently, relying on Q* to guide exploration ultimately couples exploration to the external reward, which seems rather undesirable to me. The method might be helpful from other perspectives (optimization, controlling the level of \"over-exploration\" etc), but it's not clear that it is helpful as an exploration method per se. \n* The empirical evaluation is done only against rather limited baselines (basically random exploration baselines). I would encourage the authors to compare their method to other forms of exploration. Particularly relevant to this paper is the work by Houthooft et al. 2016 (VIME) which also uses information-theoretic objective for exploration in continuous problems. The authors should at least cite this paper in their related work section.\n* Following the last two points, a great improvement could be if other than just demonstrating performance in terms of reward, the authors would evaluate the exploratory behavior itself of an agent trained with their method, in some simple environment (i.e in terms of novel states visited / distance traveled / etc.)\n\nThere are some more minor issues which should be addressed as well: \n* In the abstract: the use of \"optimism in the face of uncertainty\" is definitely not a \"recently\"\n* \"The above methods are not efficient\" (3rd paragraph, Introduction): This is not accurate. UCB (and other methods) **are** provably efficient for several problems/assumptions.\n* Some relevant literature is missing from the related work. Most notable is the VIME paper mentioned earlier (Houthooft et al. NeurIPS 2016) and the Fox et al. 2018 ICLR paper (DORA The explorer) which combines counter-based like exploration with an optimism principle for high-dim MDPs.\n* Section 3.2: $T^\\pi$ is **not** the \"Bellman optimiality operator\" but rather the bellman operator for policy $\\pi$.\n\n**Conclusions**\nThis work has some interesting idea which could be useful for training RL agents in continuous problems. However in the current form of the paper it's hard to evaluate and understand some of the key ideas of the work. Given this, and together with the more conceptual concerns regarding evaluation and the basic approach, I think the paper is not ready for publication.\n\n"
            },
            {
                "round3": "【Q4：The empirical evaluation is done only against rather limited baselines (basically random exploration baselines). I would encourage the authors to compare their method to other forms of exploration. Particularly relevant to this paper is the work by Houthooft et al. 2016 (VIME) which also uses information-theoretic objective for exploration in continuous problems. The authors should at least cite this paper in their related work section.】\n\nActually, VIME is different from our method. In VIME, the environment dynamics is estimated with stochastic parameters, and derive intrinsic reward sequentially. It is more like model-based method. However, in MQES, we model the uncertainty by using distributional and ensemble Q function approximations. So, MQES is a model-free method. \n\n【Q5：Following the last two points, a great improvement could be if other than just demonstrating performance in terms of reward, the authors would evaluate the exploratory behavior itself of an agent trained with their method, in some simple environment (i.e in terms of novel states visited / distance traveled / etc.)】\n\nDemonstrating the insights of MQES in the simple environments is definitely a good way to improve the quality of our method. But due to the time limit of the rebuttal, we will put it as future work.\n\n【Q6：In the abstract: the use of \"optimism in the face of uncertainty\" is definitely not a \"recently\"】\n\nWe have removed \"recently\" in the revised version.\n\n【Q7：\"The above methods are not efficient\" (3rd paragraph, Introduction): This is not accurate. UCB (and other methods) are provably efficient for several problems/assumptions.】\n\nPlease see the first part of this sentence, i.e., \"However, since the aleatoric uncertainty in the RL systems are heteroscedastic, ...\". We point out that UCB could be not efficient when the aleatoric uncertain\n\n【Q8： Some relevant literature is missing from the related work. Most notable is the VIME paper mentioned earlier (Houthooft et al. NeurIPS 2016) and the Fox et al. 2018 ICLR paper (DORA The explorer) which combines counter-based like exploration with an optimism principle for high-dim MDPs.\n】\n\nSince VIME and DORA are all intrinsic motivated exploration method, We refer the in the first paragraph in the Sec. 2.\n\n【Q9：Section 3.2: $T^\\pi$ is not the \"Bellman optimiality operator\" but rather the bellman operator for policy $\\pi$.】\n\nWe have corrected \"Bellman optimality operator\" as \"Bellman operator\" in the Section 3.\n\n【Q1： There is a use of terminology which might not be clear or known for the general RL/exploration audience (\"acquisition functions\", \"heteroscedastic aleatoric uncertainty\"). The entire discussion of the two types of uncertainty seems somewhat disconnected from the method itself. A short explanation of why and how the two types of uncertainty are important for the exploration problem would be very helpful.】\n\nThe introduction of \"acquisition functions\" is not related our main point of paper, hence we remove it in the revised version. Moreover, we describe the ”heteroscedastic aleatoric uncertainty\" more detailed in the third paragraph in Sec. 1 \n\nWe add a short explanation of the benefits of uncertainty for RL exploration in the second paragraph at Sec. 1. \n\n【Q2：  There are some notation obscurities or inaccuracies. This is most notable in Section 4.1, which is unfortunate since this is where the key ideas of the approach are discussed; Equation 8, which is central to what follows, is rather confusing. The text mentions that $\\pi_E$ selects action $a_t$ that (...)\", but the equations then seems to define an entire policy. And the optimization problem (in the same Eq.) is in itself dependent on $a_t$, so it's not even clear one gets a valid policy/distribution from something like $\\pi_E(a_t|s_t) = \\arg\\max_{\\pi} {\\bf{F}} ^\\pi(s_t, a_t)$; Following the previous point, Eq. 9 is also confusing. It's not clear to me what the authors mean by measuring MI between $Z^*$ and the pair  $(Z^\\pi,\\pi)$ ? It's also not clear what is the meaning of the \"posterior distribution\" denoted by $p$ , and why the mutual information is measured for the $Z$ parameters (return probs) but then re-expressed as the difference in entropies for the policies (action probs).】\n\nWe admit that the theory presented in Sec. 4.1 is vague. In the revised version, we rewrite the equations to make them more accurate: \n\nFor equation 8, the mutual information is conditioned on the state, Hence, to make it more rigorous, equation 8 is rewritten as: $\\pi_E = \\arg\\max_{\\pi\\in \\Pi} {\\bf{F}} ^\\pi(s_t).$\n    \nFor MQES, we consider actions as random variables and policies are the distributions that the actions follow. Hence, given state, the conditional mutual information is actually between the exploration action random variable $A_E\\sim\\pi_E$ and random variable of globally optimal $Q$ function  $Z^*(s,a^*)$ in Eq. 9, which are with value $a\\in\\bf{A}$ and $z^*(s,a^*)$, respectively. And the posterior probability $p(a|z^*(s,a^*),s)$ describes the distribution of exploration action conditioning on the state and globally optimal $Q$ function $z^*(s,a^*)$. Hence, Eq. 9 is rewritten as: $ {\\bf{F}}^\\pi(s_t) = {\\bf{MI}}(Z^*(s,a^*),A|s = s_t) ={\\bf{H}}\\left[ \\pi(a_t|s_t)\\right]-{\\bf{H}}\\left[ p(a_t|z^*(s_t,a^*), s_t)\\right].$\n\n 【Q3：Since exploration here is encouraged by choosing informative actions about Q*, it's not clear that this method will be helpful in very sparse-reward settings (which are a central motivation for sophisticated exploration techniques). Put differently, relying on Q* to guide exploration ultimately couples exploration to the external reward, which seems rather undesirable to me. The method might be helpful from other perspectives (optimization, controlling the level of \"over-exploration\" etc), but it's not clear that it is helpful as an exploration method per se.】\n\nActually, epistemic uncertainty introduced by the estimation of $Q^*$ could offer extra information for exploration when external reward is sparse. If the states that are seldom visited, the epistemic uncertainty at those states will be relatively large and the exploration should be encouraged. Hence, it is expected to perform better at the sparse environments, and we also conduct the experiments at sparse mujoco environments to show the improvements. \n\nHowever, if we only formulate the uncertainty using ensemble critics, the formulated uncertainty is the mixture of the aleatoric and epistemic uncertainty, where the aleatoric uncertainty is caused by the randomness of the environment and cannot be eliminated. Hence, if we do not distinguish these two uncertainties and formulate them separately, we may explore the states visited frequently but with high randomness, i.e., low epistemic uncertainty and high aleatoric uncertainty, which is undesirable.\n\n\n【Q1：I felt that the paper isn't well written and discusses a lot of different concepts in a haphazard manner. There are a lot of equations and symbols in the text without proper explanation and context which make it difficult to gather the main contribution. The language used gets vague in many statements made in the paper\nFor ex. \"Proposition 1. Generally, the posterior probability is as follows\".】\n\nWe admit that we bring a few concepts that are not generally known to RL researchers. To make the background easy to follow, we firstly explain the motivation of epistemic and aleatoric uncertainty encouraging exploration in Sec. 1; Then, we remove the introduction of acquisition function, which is not our main point.\n\nFor the equations, we rewrite Sec. 4.1 to make the derivation of MQES more rigorous, which is the main theoretical contribution of our paper; Then, we check all the symbols and try to clarify them.\n\n\t\nFor the language, we have polished the paper.\n\n【Q2：A lot of algorithm adaptations are proposed without actually carrying out ablations which make it difficult to discern if the proposed MI maximization is indeed responsible for performance. For ex. \"Since the target for critic in the advanced algorithms, like SAC and TD3, is usually estimated pessimistically..\". The authors should actually present ablations to support if a pessimistic estimate is indeed required for their adaptation for these methods.】\n\nWe admit that we need more ablation experiments. In the revised version, we added Sec. 5.4 to conduct ablation experiments, regarding to sesitivity to the hyper-parameters and the gain of distinguishing two types of uncertainty. \n\nHowever, it is worth noting that we can utilize other methods to formulate $Z^{\\pi_E}$, like mean estimation, i.e., $\\mathbb{E} \\left[Z^{\\pi_E}\\right]=\\mu_Z (s, a;\\theta)$ and $z_i^{\\pi_E}(s,a;\\theta) = \\mathbb{E}_{k = {1, 2}} \\left[z_i(s, a; \\theta_k)\\right]$. But, it only affects the choice of hyper-parameter $\\beta$ and do not affect the final performance. \n\n【Q3： Why haven't the authors included OAC as a baseline given that it outperforms SAC in several tasks? Further the results show little difference in performance in comparison with DSAC on the Mujoco tasks, given that only 5 seeds were used in evaluation, it brings the significance of the results under question. The authors should provide appropriate measures like P-values to support the experiments.】\n\nWe compare with OAC in Sec. 5.4.1, whose performance is similiar with DSAC, since it cannot avoid the effects of aleatoric uncertainty. Besides, we add more experiments on sparse Mujoco tasks and the results are discussed more detailed in Sec. 5. In standard Mujoco, MQES does perform slightly better than DSAC in those easy tasks such as Hopper-v2 and Walker2D-v2. However, in those hard and sparse-reward tasks, MQES performance significantly better than DSAC, and MQES\\_Q demonstrates the advantages of stability than MQES\\_G. \n\n【Q1: However, MQES doesn't show significant improvement over DSAC. For example, Except Sparse-HalfCheetah-v2, $MQES_Q$ has almost same performance as DSAC. Except Sparse-HalfCheetah-v2 and Ant-v2, $MQES_G$ has almost the same performance as DSAC. 】\n\nAs shown in Figure 1, both MQES\\_G and MQES\\_Q show better performance than DSAC in the difficult tasks, and in other easy tasks as shown in Appendix F, MQES can also show slightly better than DSAC. It seems that exploration is not the main bottleneck in the easy tasks, i.e., standard Hopper-v2 and Walker2D-v2. However, in those harder or sparse reward tasks, MQES performs significantly better than DSAC, and MQES\\_Q demonstrates the advantages of stability than MQES\\_G. \n\n【Q2: Another question is: The horizon is cut to 100 while most papers and OpenAI Gym use 1000 by default. Why do the authors choose 100? How does MQES perform with longer horizon, like 1000? 】\n\nOne consideration for our shortened episode length is training efficiency. Also, if the environment, sampling and training settings are the same as baseline, such comparison is fair, so we believe that the horizon doesn't matter as long as it isn't extremely outrageous.\n\n【Q3: The paper shows that exploring using both aleatoric and epistemic uncertainty can improve the performance. What if we consider only one of them, e.g., using only aleatoric uncertainty? I'd like to see this as ablation.】\n\nIn Sec 5.4.1, we provide an ablation study to show the performance gain brought by distinguishing the two types of uncertainty. \n\n【Q4:  Table 1: Could you please highlight all algorithms within 1 std to the best?】\n\nIn the revised version, we highlight the mean to the best, which is more appropriate\n\n【Q5: Sparse-HalfCheetah-v2: Could you please provide more details about the environment?】\n\nWe have added experiments on more sparse tasks, which is shown in Sec. 5.3. We describe detailed settings for sparse reward and show that MQES can show stable and consistent advantage over DSAC in those sparse tasks. Briefly, standard Mujoco gives precise reward each step, while in sparse setting, the reward is given only when the agent moves through the threshold (see section 5.3 for more details.).\n\n【Q6： What does the mutual information between $(Z^*,\\pi^*)$  and $(Z^{\\pi_E},\\pi_E)$ mean? Are $\\pi^*$ and $\\pi_E$ random variables? Moreover, in deterministic environments (as in Mujoco environments), $Z^*$ is also deterministic, so is $\\pi^*$; What is $p$? The first input of MI is simply a  while the second is a pair . Please elaborate.】\n\nIn the revised version, we have rewritten theoretical part, i.e., Section 4.1. Specifically, for MQES, we consider actions as random variables and policies are the distributions that the actions follow. Hence, given state, the conditional mutual information is actually between the exploration action random variable $A_E\\sim\\pi_E$ and random variable of globally optimal $Q$ function  $Z^*(s,a^*)$ in Eq. 9, which are with value $a\\in\\bf{A}$ and $z^*(s,a^*)$, respectively. And the posterior probability $p(a|z^*(s,a^*),s)$ describes the distribution of exploration action conditioning on the state and globally optimal $Q$ function $z^*(s,a^*)$. Hence, Eq. 9 is rewritten as: $ {\\bf{F}}^\\pi(s_t) = {\\bf{MI}}(Z^*(s,a^*),A|s = s_t) ={\\bf{H}}\\left[ \\pi(a_t|s_t)\\right]-{\\bf{H}}\\left[ p(a_t|z^*(s_t,a^*), s_t)\\right].$\n\n【Q7： What does the mutual information between $(Z^*,\\pi^*)$  and $(Z^{\\pi_E},\\pi_E)$ mean? Are $\\pi^*$ and $\\pi_E$ random variables? Moreover, in deterministic environments (as in Mujoco environments), $Z^*$ is also deterministic, so is $\\pi^*$.】\n\nFor Eq. 8, the mutual information is conditioned on the state, which is the expectation of the exploration policy. Hence, to make it more rigorous, Eq. 8 is corrected as $\\pi_E = \\arg\\max_{\\pi\\in \\Pi} {\\bf{F}} ^\\pi(s_t).$\n\n【Q8：To measure the intractable distribution  of $Z^*$  during training, we use the  $\\hat{Z}^*$  for approximation Please rephrase it and say that $\\hat{Z}^*$  will be defined later； This is not an unbiased estimation and also please clarify K】\n \t\nWe have corrected the corresponding part.\n\n\n【Q1: authors should give more details about the target policy introduced at Section 4.3. Actually, the reader should check Algorithm 2 at Appendix in order to understand its purpose and how the target policy is updated. I think that it would be better Algorithm 2 to be moved in the main paper if it is possible】\n\nWe agree that it is definitely better to move Algorithm 2 to the main paper However, due to the paper length limit, maybe we could not move it now。\n\n【Q2: Another point that should be discussed more clearly is the impact of the 3 hyper-parameters ($\\alpha$, $\\beta$, and $C$) on the performance of MQES. To be more specific, why did you set the uncertainty ratio equal to 1.6?】\n\t\nTo clarify how the hpyer-parameters affect the performance, we conduct ablation study on hyper-parameters and results are shown in Sec. 5.4.2 and appendix F, and discuss the impact of the hyper-parameters on the performance accordingly. \n\n【Q3:  Finally, the empirical results are not discussed at all. It seems for example that the performance of $MQES_G$ is more stable compared to that of $MQES_Q$. Moreover, the performance of DSAC is almost equal (or better) to that of $MQES_Q$. All these points should be explained or discussed by the authors.】\n\nIn the standard mujoco tasks and the reward is dense,  MQES\\_G is more stable compared to that of MQES\\_Q in the easy tasks, e.g., Hopper-v2 and Walker-v2, but in the difficult tasks such as Ant-v2 and the sparse reward mujoco tasks (the tasks shown in Sec 5.3), MQES\\_Q is shown to be more stable. It is mainly because the policy follows Gaussian distribution, which renders the value function random variable more Gaussian in the easy tasks due to the relatively low action and state space dimension. Hence, in the easy tasks, the Gaussian formulation incorporates this prior into the learning. Hence, the MQES\\_G should be more stable than MQES\\_Q in the easy tasks.\n\nHowever, in the difficult tasks, which are with relatively higher state and action space or sparse (or un-smooth) reward, the Gaussian assumptions will not hold, and the quantile formulation should be more reasonably since it can represent distribution in the more flexible way.\\par\n\nAs shown in Figure 1, both MQES\\_G and MQES\\_Q show better performance than DSAC in the difficult tasks, and in other easy tasks as shown in Appendix F, MQES can also show slightly better than DSAC. It seems that exploration is not the main bottleneck in the easy tasks, i.e., standard Hopper-v2 and Walker2D-v2. However, in those harder and sparse reward tasks, MQES performs significantly better than DSAC, and MQES\\_Q demonstrates the advantages of stability than MQES\\_G. \n\n【Q8:  is $n$ used in Proposition 2?】 \n\nIn proposition 2, the length of vector $m$ is the action dimension $n$. \n\n【Q9: the conventions of CDF in Proposition 2 and in Eq.21 should be made consistent with its first definition in Eq.11】 \n\nWe unify the conventions of CDF in the revised version.\n\n【Q10: definition of $G(\\cdot)$ is not used after its first definition.】 \n\nIn the revised version, we use $G(\\cdot)$ in the Proposition 2.\n\n【Q11: why the horizon is set to 100, instead of 1000 environment steps like in the SAC paper?】\n\nOne consideration for our shortened episode length is training efficiency. Also, if the environment, sampling and training settings are the same as baseline, such comparison is fair, so we believe that the horizon doesn't matter as long as it isn't extremely outrageous. \n\n\n【Q1: Although the idea is interesting, it's not yet clear how the proposed method can be used as an additional module on top of other entropy-regularized off-policy approaches like SAC or TD3, etc. The paper can benefit more if it can be formulated in such a more general way.】\n\nActually, MQES is indeed a generally exploration framework for off-policy actor-critic algorithms. Nevertheless, as an example, we incorporate MQES-based exploration into SAC in Section 4.2. Furthermore, if the policy is deterministic (like TD3), the exploration policy is just a special case of Eq. 13, which is without covariance matrix.\n\n【Q2: The practical implementation seems to make sense. However, it's still unclear to me how policies $\\pi_E$ and $\\pi_T$ are parameterized. Although a sketch of the main idea is described in Algorithm 1, it's not clear to me each term is parameterized and computed based on a particular parameterization.】\n\nIn Algorithm 1, we show that the target policy $\\pi_T$ is parameterized by $\\phi$. And we use Eq. 13 to derive $\\pi_E$, which introduces no extra parameters. \n\n【Q3: An updated policy as a solution of (18) gives an update on the mean but keeps the covariance unchanged. I was wondering then how this policy can adaptively change its exploration through the progress of learning?】\n\nIn the proof of Proposition 2, we prove that the covariance matrices of target and exploration policy are equal. Please check Appendix B for more details.\n\n【Q4: The experiment results are quite preliminary. There are no experiment settings. There are a number of hyper-parameters of MQES that might affect overall performance of MQES, e.g. N, $\\beta$ etc. but not discussed and ablated? The comparisons might also take into account other distributional policy search methods】\n\nIn the revised paper, we explain the experiment settings in more detail in Sec. 5.1. \n\nAlso, we conduct ablation study on hyper-parameters and results are shown in Sec. 5.4.2 and appendix F, and discuss the impacts of the hyper-parameters on the performance. \n\nFor the benchmarks, as mentioned in Sec. 5.1, DSAC has compared with TD4 (the distributional version of TD3), and outperformed TD4 in most of tasks. So there's no need comparing with TD4, and we choose to implement our MQES based on DSAC and also compared with it, which obtains good results.\n\n【Q5: Eq. 4 and 5, 6: min and log instead of arg functions? 】\n\nSorry for our negligence, and we have corrected the typo.\n\n【Q6: Eq. 9: What is the difference between posterior $p(a|Z^*(s,\\pi^*)$ and $\\pi^*$? Is the posterior not the optimal policy since $Z^*$ is the optimal distributional value function estimate? A detailed derivation for Eq.9 is expected; Definitions for terms in 4.1: $p(a|Z^*(s,\\pi^*)$ vs $p(a|Z^*(s,\\pi) p(a|Z^*(s,\\pi^*) p(a|Z^*(s,\\pi)$ etc; Some theoretical steps are not clearly justified of why.】\n\nWe admit that the theory presented in Sec. 4.1 is vague. In the revised version, we rewrite the equations in Sec. 4.1 to make them more accurate: \n\n 1.  For equation 8, the mutual information is actually conditioned on the state. Hence, to make it more rigorous, equation 8 is rewritten as: $\\pi_E = \\arg\\max_{\\pi\\in \\Pi} {\\bf{F}} ^\\pi(s_t).$\n    \n2. For MQES, we consider actions as random variables and policies are the distributions that the actions follow. Hence, given state, the conditional mutual information is actually between the exploration action random variable $A_E\\sim\\pi_E$ and random variable of globally optimal $Q$ function  $Z^*(s,a^*)$ in Eq. 9, which are with value $a_E\\in\\bf{A}$ and $z^*(s,a^*)$, respectively. And the posterior probability $p(a|z^*(s,a^*),s)$ describes the distribution of exploration action conditioning on the state $s$ and value $z^*(s,a^*)$ of globally optimal $Q$ function random variable $Z^*(s,a^*)$. Hence, Eq. 9 is rewritten as: $ {\\bf{F}}^\\pi(s_t) = {\\bf{MI}}(Z^*(s,a^*),A|s = s_t) ={\\bf{H}}\\left[ \\pi(a_t|s_t)\\right]-{\\bf{H}}\\left[ p(a_t|z^*(s_t,a^*), s_t)\\right].$\n\n\n【Q7: Should the aleatoric uncertainty be the variance of the ${min_{\\theta_k} z_i(\\theta_k)}$ instead of en expectation over $\\theta_k$. Because $z_i$ is estimated as the min of two estimates, e.g. in Eq.4】\n\nThe aleatoric uncertainty is a property of the environment itself, whereas $Z_{\\pi}$ is the result of our pessimistic estimation, and the two are not directly related. At the same time, we need to estimate the aleatoric uncertainty of the environment realistically in order to accurately avoid it in our explorations, and it makes no sense to estimate aleatoric uncertainty.\n\nWe greatly appreciate the reviews you provided on our paper. We are very pleased to get the valuable comments and excellent suggestions for further improving our work. Revisions have been made in the paper accordingly. The revisions are summarized as follows:\n\n1.  For the theoretical part, i.e., Sec. 4.1, we rewrite the Eq. 8, 9 and 10 to \n make them more rigorous and show our theoretical contribution more clearly.\n\n\n2.  For the results part, to make our results moreconvincing,  we conduct experiments in the sparse Mujoco environments, and describes more details of the setting. Also, we have the ablation study to show sensitivity of hyper-parameters and gain of distinguishing two types uncertainty.\n\n\n3. Expect the theoretical and results part, to make our paper easier to follow, we polish our paper from the perspective of paper structure, symbol clarifications and language, according to the comments\n\n\nA point-by-point comment-response section is given next.  Please note that all the equations and references are referred to those in the revised version of the manuscript unless otherwise indicated. Main changes in the revised manuscript are highlighted in red for the ease of cross references. Hope our responses would clarify your concerns. We are looking forward to your future comments if any."
            }
        ]
    },
    {
        "id": "ZDnzZrTqU9N",
        "decision": "Accept (Poster)",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Modeling the Second Player in Distributionally Robust Optimization\nKeywords: distributionally robust optimization, deep learning, robustness, adversarial learning\nAbstract: Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the \"uncertainty set\"). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as $f$-divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines."
            },
            {
                "round2": "Reviewer 1:\nThanks to the reviewers making the effort of responding to the reviewers' concerns in-depth. The additional experiments were also a good idea. One reservation though: the gains of the proposed methods compared to the baselines (see response to all reviewers) seems too high. This kind of disproportionate performance is usually due to one of two things: (1) An issue a specific regime where the proposed methods are particularly better than the rest, and / or (2) A bug in the evaluation pipeline / protocol. I'd advise the authors to double-check. In the benefit of doubt, I'm increasing my score to 7.\n\n\nReviewer 2:\nThank you for the extensive response --- As stated in the original review, I agree that the general idea still makes sense and that the paper demonstrates its usefulness. \n\nI am delighted that the authors aim to include the explicit distinction between empirical and theoretical KL-uncertainty set because it is of both theoretical and practical relevance. \n\nFinally, with this I will also note that the authors have addressed the two concerns I had. Including a more critical discussion of the KL-reversal together with a more explicit notational distinction between idealized and empirical data distributions will fix my main issues with the paper.\n\nReviewer 3:\nThanks for the swift response!\n\nRegarding the second point, that makes sense. I am still dissatisfied with the reasons for suddenly flipping the KL because it fundamentally changes the optimization problem, but I don't have to die on this hill.\n\nFor the first point however, I am not sure you are understanding my concern. If you use the empirical measure, then this means that $p_{\\tau, \\theta}^*$ is supported only on finitely many points. This means that the KL that you *actually* comptue in practice is \n$$ \\sum_{i=1}^n q_{\\psi}(x_i) \\log(q_{\\psi}(x_i) / p_{\\tau, \\theta}^*(x_i)),$$ \ni.e. you discretize the density $p_{\\tau, \\theta}^*$ into a new measure supported only on (finitely-many) support points $x_i$. Defining this discrete measure (for the dirac delta $\\delta_x(y)$ being $0$ everywhere except if $y=x$) as\n$$ m_{\\psi}(x)=1/n\\sum_{i=1}^n\\delta_{x_i}(x) \\cdot q_{\\psi}(x) $$\nthis means the uncertainty set that you are *actually* computing is with respect to\n$$\\text{KL}(m_{\\psi}\\|p_{\\tau, \\theta}^*) \\neq \\text{KL}(q_{\\psi}\\|p_{\\tau, \\theta}^*)$$\nNote that this is not some stickler remark: If the two measures $\\nu$ and $\\mu$ are not absolutely continuous with respect to one another, then (by definition!) we have that $\\text{KL}(\\mu \\|\\nu) = \\infty$. In other words, while $\\text{KL}(m_{\\psi}\\|p_{\\tau, \\theta}^*)<\\infty$, the mismatch of support problem means that $\\text{KL}(q_{\\psi}\\|p_{\\tau,\\theta}^*) = \\infty$.\n\nFor a definition of the KL in the measure-theoretic sense, see e.g. Def. 359 here: \nhttps://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjW2aHA-pDtAhU7QkEAHbTYBWwQFjANegQIYxAC&url=https%3A%2F%2Fwww.stat.cmu.edu%2F~cshalizi%2F754%2F2006%2Fnotes%2Flecture-28.pdf&usg=AOvVaw1tPO-Fq79f_PqetUPJuYfD\n\nReviewer 4:\nI thank the authors for the extensive answer and appreciate the time they took!\n\n(1) Regarding the finiteness of the KL, I am afraid your comment does not address my concern completely: Even though your KL-ball is well-defined between true data-generating distribution and the model (provided that both admit densities that are absolutely continuous with respect to one another), there is a remaining problem. Specifically, since you have no access to the *true* data generating mechanism, you will have to approximate the KL-ball with the empirical data distribution for computation. If I understand correctly, eq. (7) will still depend on the empirical distribution ($p(x,y)$) via $q_{\\tau, \\theta}^*$. This means that the problem would persist---unless you only evaluate $q_{\\psi}$ at the finitely many support points of $p(x,y)$. What do you do in practice? Whatever you do, it should go into the paper :)\n\n(2) I have to say that the arguments for reversing the KL are really unsatisfactory. Of course it's not your fault that the KL behaves badly when optimized, but it raises the question why you would like to define your objective the way you do. Could you not reverse the direction of the KL in your uncertainty set so that it directly appears in the 'correct direction' in eqs (6) and (7)? I don't see any part of your argument chain that would prevent you from doing that, and saying that \"optimizing the forward direction is hard\" would directly justify why you are defining the uncertainty set based directly on the reverse KL? Please let me know if this would be impossible, but I don't see why it would be. \n\n\nReviewer 5:\nGood points\n----\n- The objective of the paper is sound: fight distributional shift in systems whose predictions\nmight have life-changing consequences (e.g data bias toxicity prediction models, etc.).\n- The paper is well-written and easy to follow.\n\nBad points\n----\n- I don't see just how this model is \"parametric\". In statistics, \"parametric\" the adversarial\ndistribution is modeled as a gaussian, etc. with sought-for parameters (mean, covariance, etc.).\nIn the absence of that, I would have expected \"parametric\" to mean parametrizing the adversarial\ndistribution as the (softmax) output of a neural network. Neither of the above is the case in \nthis paper. So, what are the \"parameters\" in the proposed DRO adversary ? All I can see is that\nthe authors do  a full search over all distributions, subject to a KL constraint (see sections\n2 and 3.2).\nThere is nothing \"parametric\" about this.\n- The authors say \"In particular, direct gradient descent on the uncertainty set suffers from\ninstability due to the large variance of the gradients (Greensmith et al., 2004), and\nhyper-parameter selection is not straightforward.\" I'm not sure about this claim (which\nis one of the main premises of the manuscript. What do the authors make of this paper\nfor example Faury et al. (AAAI 2020) \"Distributionally Robust Counterfactual Risk Minimization\" ?\nThe authors of that paper demonstrate how to efficiently formulate and solve KL-based DRO\nproblems. That paper also contains both theoretical and practical insights.\n- The technical contribution of the paper is negligible (if any).\n- The arguments in the paper very heuristic.\n- Since the paper is supposed to be empirical (see previous points), I would have expected\nexperiments on real datasets.\n\n\nErrors\n---\n- Change \"solve the inner-max efficient\" to \"solve the inner maximization problem efficiently\"\n- Change \"$x, y ~$ \" to \"$(x,y) ~ $\" all through the manuscript\n- Eqn (5): why not take $p$ and $q_{\\psi_0}$ to equal the empirical distribution (as is usually\ndone) in DRO ?\n- In eqn defining $q_{\\psi_0}$, replace $\\arg\\max_{q_\\psi}$ with $\\arg\\max_\\psi$\n\nReviewer 6:\nThe paper proposes to define the uncertainty set in the DRO problem as a family of parametric generative models, which is to allow more flexibility in the choice of the uncertainty set architecture. To realize this idea, the paper first proposes a new relaxation of the DRO game's inner maximization problem (with KL constraints) so as to improve the training stability. It then develops a principled approach to select the hyper-parameters of the proposed method.\n\nStrengths:\n+ The paper is well-written.\n+ The proposed method is novel and important for the DRO community.\n+ Experiments with real-world problems are conducted to evaluate the effectiveness of the proposed method. I particularly like the experimental analysis the authors conducted to understand the behavior of their proposed method. \n\nWeaknesses:\n- The experiments are only on NLP tasks.\n\nI have few questions to the authors:\n1) How good the adversary model needs to be for the proposed method to perform well? In the experiments, an auto-regressive transformer model based on the GPT-2 language model is employed. What is the accuracy of this model on the train dataset of the DRO problem? Will the proposed method performance be too sensitive to the accuracy of the adversary model?\n2) In the experiment (last paragraph of Section 5.1), the temperature \\tau and the normalizing window k are fixed whilst the adversary learning rate \\lambda is searched by grid-search. So how \\tau and k are selected in practice? What is the performance of the proposed method when \\tau and k vary? \n\n\nReviewer 7:\nTL;DR: The paper makes an interesting contribution from a practical point of view, but two important theoretical concerns need to be addressed in the rebuttal for acceptance.\n\nThe paper proposes the use of ideas taken from the literature on distributionally robust optimization within a parametric framework. More precisely, the main idea is to consider only a (parameterised) subset of the traditional KLD-uncertainty sets. As this avoids the need for elegant analytic solutions (at the expense of a more brute force computation), it has the flavour of a more ‘black box’ approach towards the deployment of DRO.  Overall, I really enjoyed the way this paper was written. Purpose and use of the contributions are clear throughout, and the reader is drawn in.  I also liked the contribution and believe that the paper demonstrated its ideas to be useful. There are however two points of major concern from a more theoretical side. In my mind, these are rather substantial, and I will list them below. To recommend that the paper be accepted, these points will have to be addressed in a future version of the paper:\n\n(1) How do you ensure that the KLD between $q_{\\psi}$ and $p$ is finite? p clearly is the empirical measure (as is emphasised e.g. just above eq. (5)), but $q_{\\psi}$ will be continuous. This means that the KLD between the two distributions is not defined/infinity for any value of \\psi (Mismatch of support problem). These kind of problems are the precise reasons why other quasi-distances (like the Wasserstein distances) have become increasingly interesting for ML. As far as I can tell, this problem is not elaborated upon anywhere in the paper. \n\n(2) It is totally unclear to me why it should be viable to suddenly flip the direction of the KLD. The KLD is not symmetric and in general will not even have the same minimum. In fact, generally speaking the only time the minimum will be the same in either direction is when the KLD’s global minimum is such that $q_{\\psi} = q_{\\tau, \\theta}$ (i.e. we can drop the KLD term for the loss in (7) completely, so that it simply equals $C$). Given the definition of $q_{\\tau, \\theta}$, it is unreasonable to assume that this global minimum is attained. This makes the flipping of the KLD’s direction questionable at best. Calling the outcome an ‘approximation’ is then grossly inaccurate.  (See e.g. the visualisations here: https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/)\n\nLastly, since the chief concern of the paper is the construction of new uncertainty sets, I would have liked to see two additional recent references discussed which have produced uncertainty sets purely based on moments (https://arxiv.org/abs/2007.04458, ICML 2020) as well as on general IPMs (https://arxiv.org/abs/2006.04349, NeurIPs 2020). Both these types of uncertainty sets do *not* suffer from the mismatch of support problem, and—like the famous f-divergence based uncertainty sets—have elegant dual forms. \n\n\nPOST-DISCUSSION: The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality (as opposed to theoretical rigour). I will thus raise my score to a weak accept.\n\nReviewer 8:\nThis paper considers distributionally robust optimization (DRO) and uses the neural generative models to characterize the uncertainty sets. To tackle the optimization challenges, several implementation tricks are incorporated to solve the minimax problem. The proposed robust method is validated on NLP tasks. \n\nThis paper is well-written and of a good structure. Although the main idea is simple, the authors make several modifications to the algorithm to make it tractable and with performance guaranteed heuristically. To summarize, the main contribution of this paper is a new algorithm that combines standard techniques, such as Lagrangian relaxation and KL reverse, into the DRO problem with KL uncertainty sets. And this algorithm was shown to perform well under synthetic and real-data NLP tasks. Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework, overall, I think this is a borderline paper due to its limitations in theoretical development and technical novelty. \n\nMoreover, if the main focus of this paper is on developing a new computational framework that can lead to more robust results, then the authors should compare with more benchmark methods, while I only see the comparison with ERM, Topic-CVaR, etc. For example, I am wondering is it applicable to compare with Wasserstein DRO or Huber's classical work of Total variation based DRO, or some other DRO works in the literature, so that it will be more convincing on the performance of the proposed method. \n\nA minor typo in the paper: in section 6, there is a duplicated \"produce\" in the sentence: \"In such cases where good quality generative models are unavailable, or such model cannot produce produce densities efficiently\".\n"
            },
            {
                "round3": "We again thank all four reviewers for their feedback, and in particular R1 for the insightful discussion.\n\nWe updated the paper to incorporate the reviewers’ comments. Here is a summary of the changes:\n\n- Added another baseline: non-parametric KL-constrained DRO. This is in response to R3’s comments on the lack of baselines, and also intended to clear up confusions between parametric and non-parametric approaches to DRO brought up by R2\n- Clarified the presentation to address R1’s concerns, specifically by:\n    - Making the distinction between empirical and theoretical distribution more explicit where necessary, in particular in how it relates to estimating the KL divergence\n    - Including a more nuanced discussion of the KL reversal\n- Included additional experiments in the appendix to visualize the effect of various hyper-parameters, as suggested by R3.\n- Highlighted appendix C.2 (experiments with a smaller adversary) better in the main text (in response to R3’s comment)\n- Included additional references suggested by reviewers: Faury et al. (AAAI 2020; R2), Husain, 2020 and Nguyen et al. 2020 (R1)\n- Fixed minor typos/presentation issues brought up by R2 and R4\n- Adjusted a number (Greedy-minmax 30.43 -> 32.17) in Table 2a after a minor bug was fixed in our analysis code.\n\n\nAgain, we thank the reviewer for taking the time to clarify their concern.\n\nWe will attempt to rephrase the order of operations in Section 3.2, and what we believe to be the reviewer’s core issue. First, for clarity’s sake, we define the following notation\n\n- $p$: the true data distribution and $m$ the empirical distribution\n-  $q^*=q_{\\tau,\\theta}^*=\\frac 1 Z p e^{\\ell/\\tau}$ and $m^*$ its restriction to the empirical distribution\n- $q_\\psi$: the adversary and $m_\\psi$ its empirical counterpart (as defined by the reviewer)\n\nIn our intended presentation, Section 3.2 proceeds as follows:\n\n1. Write the lagrangian relaxation $-E_{q_\\psi}\\ell + \\tau KL(q_\\psi || p) + Constant = KL(q_\\psi || q^*) + Constant$\n2. Reverse the order of the KL: $KL(q^* || q_\\psi)$\n3. Plug-in the empirical distribution: $KL(m^* || m_\\psi)$\n\nIn particular in the last step, the plug-in of the empirical distribution on the left argument of the KL is standard practice and does not raise issues of absolute continuity. Insofar as one accepts the KL reversal, we believe that the use of the empirical distribution should be acceptable in this formulation.\n\nOn the other hand, if we understand correctly, the reviewer reads our order of operation as:\n\n1. Write the lagrangian relaxation $-E_{q_\\psi}\\ell + \\tau KL(q_\\psi || p) + Constant = KL(q_\\psi || q^*) + Constant$\n2. Plug-in the empirical distribution $KL(m_\\psi || m^*)$\n3. Reverse the order of the KL: $KL(m^* || m_\\psi)$\n\nIn this case we agree with the reviewer’s assessment that the transition from 1 to 2 is problematic because of absolute continuity issues. In fact, as far as we can see, there is no easy way to estimate the “correct” KL $KL(q_\\psi || q^*)$ without running into the aforementioned issues with the empirical distribution.\n\nEverything considered, the two derivations yield the same final objective (step 3., equation 8 in the paper), so the discussion ultimately comes back to the KL reversal. As we have argued above (and shown empirically in the paper), this approximation, while unsatisfactory, still allows us to train robust models in a tractable fashion.\n\nFinally, we agree that this discussion is important, and we will strive to make it clear in the paper, but we don’t think that it discounts the general idea of the paper, nor the experimental results.\n\n\nWe would like to add additional comments with regards to the reviewer's enquiry as to the importance of the adversary's accuracy:\n\n\\> 1. How good the adversary model needs to be for the proposed method to perform well? In the experiments, an auto-regressive transformer model based on the GPT-2 language model is employed. What is the accuracy of this model on the train dataset of the DRO problem? Will the proposed method performance be too sensitive to the accuracy of the adversary model?\n\nWe have replicated the experiments on BiasedSST, using a simple, one-layer LSTM model as the adversary. As it turns out, we had run these experiments already and they were in fact included in the original submission (albeit in the appendix; C.2). The setup is almost exactly the same as that of Section 3, except we only search for the learning rate in order to reduce the number of experiments (moreover, as outlined in our previous response, we find that the other hyper-parameters k and \\tau have limited effect). \n\nThis adversary achieves lower generative modeling performance than our autoregressive transformer (test perplexity 227.01 vs 49.84) on the biasedSST dataset. Yet, we find that it allows P-DRO to achieve robust test accuracy of 43.68 on biasedSST, which also outperforms all baselines.\n\nIn fact, this is even higher than our results obtained with the transformer model. We find that this surprising result can be explained as an effect of the minmax validation strategy. When we remove this factor and choose hyper-parameters and early stopping with robust accuracy instead, we find that the LSTM adversary reaches a robust accuracy of 45.68 versus 47.53 for the transformer adversary, giving the latter a slight edge.\n\nIn summary, we find that the size of the adversary as a generative has a limited effect when restricted to neural models. We will clarify the discussion in the paper and make sure to attract more attention to these results in the main text.\n\n\\> I thank the authors for the extensive answer and appreciate the time they took!\n\nLikewise, the reviewer’s willingness to engage in discussion and clarify their concerns is much appreciated.\n\n\\> (1) Regarding the finiteness of the KL, [...] What do you do in practice? Whatever you do, it should go into the paper :)\n\nIn practice, we reverse the $KL(q_\\psi || q_{\\tau, \\theta}^*)$ to $KL( q_{\\tau, \\theta}^* || q_\\psi)$ which can be re-written  ($\\mathcal L_{rev}$, Eq. 8) as the ratio of $\\mathbb E_p e^{\\ell/\\tau}\\log q_\\psi$ and the normalizer $Z_{\\tau,\\theta}=\\mathbb E_p e^{\\ell/\\tau}$. Both are expectations (over $p$) of random variables which do not depend on $p$ (and hence can be computed directly). In both cases, we indeed plug-in the empirical distribution in place of $p$ in the expectation. This will be clarified in the revision (in Section 3.2 after Eq. 8) by adding the following sentence: “In practice, we compute the expectation by sampling from the empirical distribution.”.\n\n\\> (2) I have to say that the arguments for reversing the KL are really unsatisfactory. [...] Could you not reverse the direction of the KL in your uncertainty set so that it directly appears in the 'correct direction' in eqs (6) and (7)?\n\nThis is an interesting remark. In fact, defining the uncertainty set based on the reverse KL would lead to similar optimization issues, at least without some additional approximations or tricks. The reason for this is that the resulting lagrangian ($\\mathbb E_{q_\\psi} \\ell + \\tau E_p \\log q_\\psi + [\\text{Constant in }\\psi]$) still contains a term in $\\mathbb E_{q_\\psi}$ which is difficult to optimize for $\\psi$. On the other hand, the reversal of the KL in Eq 7 (leading to the $\\mathcal {L}_{rev}$ objective in Eq. 8) allows us to avoid taking an expectation on $q_\\psi$ which is the main optimization hurdle. Because of this, our formulation is not directly equivalent to reversing the direction of the KL constraint on the uncertainty set.\n\n\n\nWe thank the reviewer for their encouraging feedback. We address their specific concerns below, and we are happy to continue discussing any of these points or answer follow-up questions.\n\n\\> The experiments are only on NLP tasks\n\nWhile in general our proposed approach can be applied to any modality, the reviewer is correct that we only experiment on NLP datasets (except in our toy experiment in Section 4.4). As mentioned in the paper, this is motivated by the widely recognized success of language models, which make them a prime candidate for testing P-DRO.\nFor other modalities where generative models are either not readily available or can’t provide normalized probabilities efficiently, such as GANs, an alternative solution might be to model the likelihood ratio q_psi/p directly, however this poses a variety of other challenges, which we defer to future work.\n\n\\> How good of a generative model is the adversary, and how important is its performance?\n\nAs pointed out by the reviewer, in most of our experiments, the adversary is a transformer model based on the GPT-2 architecture (albeit with fewer parameters than the actual GPT-2 model). On the BiasedSST dataset, this model attains a “perplexity” of 49.84 (note: this model predicts both label and text, as such the perplexity is not directly comparable to regular language models). Measuring the effect of the adversary’s performance on the effectiveness of P-DRO is an interesting ablation study. Should time and computing resources permit, we will make our best efforts to obtain additional results with smaller adversaries during the rest of the rebuttal period. \n\n\\> How are \\tau and k chosen in practice?\n\nAs shown in Section 4, \\tau and k can be chosen via grid-search using the Minmax criterion described in Section 3. For the experiments in Section 5 specifically, we fixed \\tau and k in order to reduce the search space and make grid search more manageable. Possibly, better results could be obtained in Section 5 by searching for better \\tau and k. We will edit the paper to clarify this.\n\nAs to the effect of the choice of k and \\tau on performance, we performed an ablation study on BiasedSST. We start from the configuration \\lambda=10^-4, k=5 and \\tau=0.01 and vary either k or \\tau. We report two numbers for each configuration: robust accuracy of the best model using Greedy-Minmax stopping and using Oracle stopping. The latter is useful to disentangle the effect of the stopping criterion.\n\n||Robust Accuracy (Minmax stopping)|Robust Accuracy (Oracle stopping)|\n|-|-|-|\n|k=0 | 41.98 ± 4.48 | 49.60  ±  5.39 |\n|k=5 | 44.74  ± 3.24 | 50.43 ± 5.05 |\n|k=10 |  32.17 ± 11.20 | 50.95 ± 5.01 |\n|-|-|-|\n|\\tau=0.1 | 39.72 ± 5.55 | 50.00 ± 4.98 |\n|\\tau=0.01 | 44.74  ± 3.24 | 50.43 ± 5.05 |\n|\\tau=0.001 |  44.74  ± 3.24 | 50.87 ± 5.09 |\n\nInterestingly, neither k nor \\tau have a strong effect on robust performance when using Oracle stopping. We will add these ablation studies to the updated manuscript.\n\n\nWe appreciate the reviewer’s enthusiasm for our approach, and are grateful for the insightful feedback. We address their specific concerns below, and we are happy to continue discussing any of these points or answer follow-up questions.\n\n\\> How do we ensure that the KL divergence between q_\\psi and the empirical distribution p is finite or even well-defined?\n\nThe reviewer makes an keen observation that the empirical distribution, being of finite support, does not have a finite KL divergence with q_\\psi. In truth, we are only interested in the KL divergence between q_psi and the true underlying data distribution, which we can reasonably assume is finite. We agree that the phrasing of the paper is confusing in this regard, as we interchangeably refer to both the “true” data distribution and the empirical distribution (of finite support over the training data) as p. We will edit the paper to make this clearer.\n\n\n\n\\> Why is flipping the KL viable?\n\n\nThe reviewer is correct that the KL is not symmetric, and as such the reversed loss L_rev is not equivalent to the original “forward” KL minimization problem. First, we would like to clarify that we did try to optimize the forward-KL constrained objective and found in our toy experiments (Section 4.4) that this generally failed. This failure is echoed by a variety of previous work (eg. RAML (Norouzi et al., 2016), but also in the RL literature). We do agree that flipping the KL divergence is an unsatisfactory approximation (and we will update the paper to further emphasize this point), however as shown empirically in previous work, it seems to be effective in practice.\n\nUltimately, we choose to make concessions to the optimization concerns, to the expense of theoretical exactness. We do believe that attempting to directly minimize the forward KL is a promising future direction. However, the current version of the paper demonstrates empirically that the KL reversal is not only viable, but is also sufficient for P-DRO to yield more robust models. If performing P-DRO with the forward KL results in superior results than the results that we have obtained with reverse KL, then we argue that it would only further improve the utility of our already-promising approach.\n\n\\> Missing references (Husain, 2020 and  Nguyen et al. 2020)\n\nWe thank the reviewer for pointing out these recent relevant references, which we’ll include in the upcoming revised version of the paper.\n\n\nWe thank the reviewer for their encouraging comments and helpful feedback. We address their specific concerns below, and we are happy to continue discussing any of these points or answer follow-up questions.\n\n\n\\> Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework, overall, I think this is a borderline paper due to its limitations in theoretical development and technical novelty\n\nWe do agree with the reviewer that the paper does not make a strong theoretical contribution, and our approach is very much motivated by proposing a method that works in a practical scenario rather than what is most satisfying theoretically.\n\nThe main novelty of the paper is the use of a parametric family as the uncertainty set in DRO. However, our contribution goes beyond the brute-force approach of simply plugging parametric models into the classical DRO min-max (which doesn’t work, as demonstrated in our toy experiments in Section 4.4). In particular while a number of the adjustments detailed in Section 3 are not novel by themselves (lagrangian relaxation, KL reversal...), the fact that they can be combined and applied to the problem of parametric DRO is (in the authors’ opinion) far from being a given.\n\n\n\\> More baselines\n\nThe reviewer’s point regarding more baselines is well taken. First, we would like to point out that Wasserstein DRO presumes the existence of a canonical metric on the input space, of which there is none for discrete sequential inputs such as natural language sentences. Adaptation of Wasserstein DRO to NLP is an interesting direction, but it is far from straightforward, and would warrant a more thorough investigation of its own. Huber’s work on robust statistics solves a related but different setting: ensuring that models are robust to an adversary who modifies the training data. Our paper considers the problem where the training data is fixed, and the test distribution is potentially different. To our knowledge, Huber’s robust statistics approaches do not directly address KL-robust DRO problems.\n\nThat being said, we did run the additional baseline of non-parametric KL-constrained DRO, inspired by the formulation of Hu et al. (2016) (https://arxiv.org/abs/1611.02041). We refer to our general response to all reviewers for more details and initial results. We are currently working towards adding these additional baseline results throughout the paper.\n\n\nWe thank the reviewer for their detailed feedback. We address their specific concerns below, and we are happy to continue discussing any of these points or answer follow-up questions.\n\n\\> I don't see just how this model is \"parametric\"\n\nThe proposed approach is parametric in the sense that the confusion set is represented by a parametric family of models. To take the reviewer’s example, in one of our ablation experiments (Section 4.4), the adversary is indeed a gaussian, and its sought-for parameters are the mean.\nIn our other experiments, the uncertainty set is composed of transformer-based language models. This setting is also parametric in the sense that each possible test distribution in the uncertainty set is associated with a set of parameters for a transformer model, and these parameters are optimized jointly with the classification model following the procedure described in Section 3.\n\nTo clarify this point, we have added results for a non-parametric KL-constrained baseline. Please refer to our general response to all reviewers for more details.\n\n\\> Comparison to Faury et al. (AAAI 2020) \"Distributionally Robust Counterfactual Risk Minimization\"\n\nWe thank the reviewer for pointing out this relevant citation, which we’ll include into the next revision. As far as we can tell, this paper proposes a non-parametric KL-constrained formulation of DRO which is very similar to that of Hu & Hong (2013) or Hu et al. (2016), but applied to CRM. In fact, Faury et al. corroborate our point: they state (eg. Section 2.2) that  “We are interested in DRO instances that are amenable to direct optimization. To this end, we focus here only on uncertainty sets U based on information divergence”.\nIn our work, we consider the challenges that occur when moving outside this tractable set of uncertainty sets, and consider intersections of the KL uncertainty set with parametric models where the inner-maximization problem becomes intractable (hence the need for the approximations described in Section 3).\n\nAgain, we refer to our general response for more details on an additional, relevant baseline.\n\n\\> The technical contribution of the paper is negligible (if any)\n\nTo the best of our knowledge, we are the first to investigate DRO with neural-network based parametric confusions sets and to address the associated challenges (intractability of the inner-max, difficulty of enforcing the KL constraint...). We believe these are all technical contributions that are not attested to by previous research, and all required a significant amount of thought, design, implementation, and empirical validation.\n\n\\>Since the paper is supposed to be empirical (see previous points), I would have expected experiments on real datasets.\n\nWe would like to point out that the experiments in the final section of the paper are performed on two toxicity detection datasets, which are well established datasets addressing an important real problem and widely used in the community: Davidson et al. (2017) and Founta et al. (2018)  (760 and 109 citations respectively according to google scholar).\n\n\\> In Eq. why not take q_psi_0 and p to equal the empirical distribution (as is usually done) in DRO ?\n\nDue to its parametric nature, the support of the adversary q_\\psi is larger than that of the empirical distribution, therefore, we need to use the true data distribution p (which we assume has the same support as q_psi) in the denominator. In practice, since p is unavailable, we resort to the MLE q_\\psi_0, which also has the same support.\n\n\nWe thank all four reviewers for their feedback. We address each reviewer’s specific concerns in separate replies, and are happy to continue discussing any of these points or answer follow-up questions.\n\nA few reviewers brought up the comparison to the non-parametric KL-constrained approach which is similar to our proposed approach. We ran additional experiments to compare to this approach, inspired by the formulation of Hu et al. (2016) (https://arxiv.org/abs/1611.02041). We slightly adapted the algorithm to our setting (minibatch training of large models, we will outline those modifications in more detail in the upcoming revision of the paper). In particular, we experiment with 4 values for the radius of the KL ball (which controls the size of the uncertainty set): 0.01, 0.1, 1 and 10.\n\nWe report initial results on BiasedSST for two variants:\n\nAverage: we use average accuracy for both stopping and hyper-parameter selection\nMinmax: we adapt our proposed Minmax criterion for stopping and hyper-parameter selection.\n\nResults are as follows (robust test accuracy):\n\n| Method| Robust Accuracy |\n| --|--|\n|ERM | 2.15  ± 0.97|\n|Topic CVaR | 5.18  ± 1.46|\n|Non-param (Average) | 8.51  ± 4.62|\n|Non-param (Minmax) | 21.68  ± 4.85|\n|P-DRO  | 34.98  ± 9.39|\n|Oracle DRO | 67.71  ± 3.03|\n\nFirst, we confirm that P-DRO yields more robust models than its non-parametric counterpart. Second, this further confirms the effectiveness of our proposed Minmax validation criterion, which also significantly improves the results of the non-parametric model.\n\nWe are currently working towards adding these additional baseline results throughout the paper.\n"
            }
        ]
    },
    {
        "id": "_IM-AfFhna9",
        "decision": "Accept (Poster)",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Generalized Variational Continual Learning\nKeywords: No keywords\nAbstract: Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration."
            },
            {
                "round2": "Reviewer 1:\nThank you for the response with clarifications. So the improvement comes from both GVCL and FiLM layers, rather than the GVCL framework alone. As a result, I think it is essential to highlight this in the main text, or even in the title. Otherwise, it gives one impression that GVCL is the solution. \n\nReviewer 2:\nThank you for your efforts to address my comments and to improve the paper.\n\n1.\nI find the link to cold posteriors interesting, it would explain the choices more consistently from an inferential point of view. This analysis can also be deepened somewhat in future work, as the importance of 'cold' posteriors seems under-explored in the context of continual learning.\n\n2.\nThank you for clarifying that. I suspected FiLM layers would also work well with VCL itself in this case, I am glad this gets confirmed, as it may inform users running VCL systems that structure akin to FiLM layers may immediately help improve their systems. It is also quite interesting that GVCL outperforms VCL when using FiLM layers. I think this is overall a valuable addition to the paper and makes the case for this 'second' idea in the manuscript more succinctly.\n\n3.\nI would be presumptuous to suggest a title and I exclude this from my evaluation of the paper of course, but my mental hash function for this manuscript stored it under 'unifying VCL and EWC' rather than 'generalizing VCL' as the suggested changes are not strictly advances in the field of approximate inference.  I don't know if my hash function is useful enough to the authors for considering titles that would more directly inform the reader about the content, it is anecdotal.\n\nOverall, I remain convinced this is a strong paper and continue to suggest acceptance.\n\nReviewer 3:\nThanks again for further clarifications. \nI will read the paper again (after the revision) and update my scores. \n\nReviewer 4:\nThanks for the clarifications!\n\nLaplace propagation and Online Structured Laplace Approximations (Ritter et. al) do compute the Hessians though. \nI understand now that you do not need to compute the Hessians for learning continually because of the cancellation. However for the posterior predictive distribution, computing the \"right\" covariance would be necessary. \nI would have expected that the variational resulting distribution from optimizing the ELBO is a posterior approximation, which is the case for VCL and for Laplace's approximation, but for GVCL this is arguably not the case.\nI think this should be discussed. \n\nI suppose, we could do a Laplace's approximation in GVCL as well, if we compute the Hessian at mu, ignoring the learnt variance (which does not correspond to either variational or Laplace approximation). \n\nReviewer 5:\nThanks for the clarification. Fig. 8 now makes sense to me. \n\nRegarding 2), I am still trying to understand whether or not the variance resulting from optimizing the beta-ELBO is the same as the variance you would obtain when computing the Hessian at mu_t. \n\nLets start again with i) beta=0 (exactly zero) and then again ii) beta --> 0 (close to zero).\nIn i), it is clear that we will always obtain just the MLE estimate (not even MAP) with 0 covariance in every step, as we will always completely ignore the KL term. Now I don't see how for ii) the covariance suddenly jumps to the correct covariance for an infinitesimal change. \n\nLets try to get there through your results in App. C.\nStep 1) dL/dSigma = 0: We obtain an optimal (local minimum with derivative zero) precision matrix in (9) under the condition that beta is close to zero. I also agree with the resulting recursion of the precision. \nNote: So far, we have not concluded whether or not this optimal precision matrix is the same as we would get from Laplace's approximation. It is just whatever we get from optimizing the beta-ELBO, and in case of beta=0, precision will be inf. \nStep 2) Eq. (10) now looks at the ELBO when we use the previous posterior resulting from optimizing the beta-ELBO.\nNow, I agree that if beta is only *close* to zero, it will cancel (for the Hessian). \nAnd I also agree that the beta-ELBO then looks like the optim. from Laplace propagation. \nBut that is only for optimizing mu! \nSo I agree also that each iteration (time-step) t should in theory find the same mu_t as we would for Laplace Propagation. \nHowever, my understanding is that we will have a too sharp posterior at t-1 and then because we down-weight the KL term through beta, it will act as if it had the correct variance. So in the limiting case, we have a dirac distribution and it is weighted with zero, canceling completely. \n\nIn other words, the resulting posterior approximation at every iteration is not identical to Laplace's approximation, but the mean is. This means that posterior predictives will not use the right posterior (although we could apply Laplace's approximation just for posterior predictives while doing continual learning as proposed).\n\nDo you think there is still a misunderstanding or would you agree? \n\nReviewer 6:\nThe authors propose Generalized VCL in this paper, which consists of multiple ideas: first, the authors introduce a beta-Elbo, which facilitates downweighting the KL-term of VCL. If beta taken to the limit towards zero, the authors show that the beta-elbo recovers the online EWC learning criterion, which draws an interesting link between VCL and EWC.\nThe authors also discuss reweighting terms to introduce a parameter lambda as in EWC, which they incorporate via a lambda-kl divergence term.\nFinally, furnished with this learning objective that interpolates between VCL and EWC, the authors propose to combine the learning objective with the architectural choice of Film layers, which they show facilitate overcoming the pruning behavior that their method inherits from VCL by offering ways to prune nodes without injecting noise into the network.\n\nExperiments are broad on multiple interesting datasets and quite clearly show that their proposed combined model performs best.\n\nPositives:\nThe paper draws an interesting unification between EWC and VCL, and in fact also other related works, as subtle modifications in a regularizer. This by itself is an interesting contribution. The fact that the authors study the interplay of their learning arlgorithm with architectural biases, i.e. overcoming early pruning via film layers, is also a valuable idea that I find not just interesting in itself, but also stylistically valuable as an approach to studying  deep learning. While the Film layers per se also appear somewhat ad hoc, their empirical benefits -particuarly when paired with the lambda-elbo, are impressive and well put together.\n\nCriticisms:\nWhile I really enjoy the derivation of the beta-elbo in the zero limit, I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified. It feels as if it is reverse engineered to match the desired criterion from EWC. I think the authors should dig deeper here for better justifications for such choices, as they did a good job having a mathematically interesting framework to derive earlier.\n\nAdditionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well? This is empirically confusing, it would be great to get some more help to understand the relative merits of each components here and clarify more how these pieces fit together empirically. I do enjoy the appendix discussing this qualitatively, but I would like to understand it quantitatively better, as theoretically film layers plus VCL (without this paper's innovations) should also benefit similarly.\n\nOne additional criticism is that the title is somewhat misleading, as it does not generalize VCL to broader settings, but rather collapses it towards the limit beta towards zero. The title raised hopes for a richer variational treatment rather than a unification to EWC and an architecture change. The authors might want to consider tweaking the title to sth that is closer to the paper's actual contributions.\n\n\nOverall:\nThis paper takes an interesting approach towards adding to the EWC and VCL literature by unifying them and offering an architectural fix for a key problem in these scenarios. While the contributions are mixed and not consistently derived from clear modeling assumptions, their interplay is well studied and highly relevant to the understanding and improvement of practical continual learning. I also want to again applaud the authors for studying and explaining the interplay of pruning and film layers, I enjoyed reading the supplementary information on this. I wish more papers that discover methods that perform well empirically would study the interplays of algorithm and architecture similarly to expose interesting effects.\n\nReviewer 7:\nThis work considers online variational Bayesian approaches to continual learning. The authors propose a beta-ELBO objective which they claim interpolates between Gaussian variational inference (beta = 1) and Laplace’s approximation (beta = 0).\nFurthermore, the authors propose task-specific, non-probabilistic (point estimation) FiLM layers that apply an element-wise transformation to the activations.\n\nTheory / Contribution:\nThe two contributions seem quite orthogonal to each other and each of them is rather minor in novelty. \nIt is obvious that using beta=0 leads to MAP estimates from which Laplace’s approximation can be computed. However, I am quite confused what exactly the authors do here and there could be a major mistake:\nFrom the paper, I am not sure if the authors a) compute Laplace’s approximation in the end, at the resulting mean of q, for any beta value? As far as I understand, the authors instead b) only optimise the variance through the beta-ELBO. \nHowever, in this case, the resulting approximation would *not* identical to Laplace’s approximation!\nI need clarification what the authors are doing here.\nConsider the case of beta=0, the covariance will be the dirac distribution as the authors note in Sec. 2.2 or the supplementary material. The authors then go on and write the optimal covariance matrix for which the derivative of the beta-ELBO is zero.\nYou have first postulated that the covariance is zero, in order to be able to pull out the expectation, and then you again allow for a non-zero beta-elbo-minimizing covariance. This would be a contraction. This makes me guess you do compute Laplace’s approximation instead. But then it is not discussed how you deal with beta>0. \n\n\nRelated work:\nThe related work section is rather short mentioning only very few related approaches. More effort is required here.\n\n\nExperiments:\nThe experimental evaluation is thorough and seems promising. Although I am wondering why e.g. Fig. 2 does not include VCL and EWC. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong? For Fig. a), the largest beta=10 seems to be a good approximation and also the most local. In case of Fig. B) and C) it is unclear / subjective (from visually inspecting the likelihood function) which is the best approximation. In A), beta=0.1 is the least local approximation, in B) beta=10 and in C) beta=1. I cannot follow the intuition provided here.\n\n\nSummary:\nI am sceptical about the correctness regarding the equivalence between VI and Laplace’s approximation; the exact approach proposed in the paper is unclear and may be based on a contradiction. In case I have a misunderstanding here, I hope the authors will point this out and update the manuscript. \n\n\nUpdate after Rebuttal:\nThe authors provided clarifications and improved the manuscript. \nIn particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. \nI am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. \nBased on this, I changed my evaluation and now suggest acceptance. \n\nReviewer 8:\nThis paper proposes Generalized Variational Continual Learning (GVCL). It is shown that Online EWC and VCL are special cases of GVCL, along with other theoretical contributions. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL. GVCL and GVCL-F are applied to a number of continual learning tasks and demonstrate competitive performance. \n\nAlthough GVCL and GVCL-F do not outperform baselines, particularly in hard settings (split-mnist and mixed vision), GVCL is an original and excellent contribution. The paper is clear and well-written, the proposed algorithm is theoretically motivated and analysed, experiments are comprehensive, demonstrating the empirical performance of GVCL. \n\nI have the following comments:\n- It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n- Why is GVFL significantly worse than baselines for split-mnist (Figure 2c)?\n- Why is split-mnist omitted from Figure 3?\n- The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nMinor:\n- \"the node is *effective* shut off\" -> effectively\n\nReviewer 9:\nThis paper proposed a generalized variational continual learning (GVCL) framework using the \\beta - ELBO, and then combined with FiLM layers. The idea is interesting but there is a lack of preciseness. The pros and cons are as follows. \n\nPros: \n1. The proposed GVCL proposed a different and interesting perspective on the online EWC, viewed as a special case of \\beta \\to 0;\n2. FiLM layers are introduced to combine with GVCL, which lead to significant improvement in the performance;\n3. Various experiments are performed, showing some level of advantages. \n\nCons:\n1. The new perspective that online EWC could be viewed as a special case of the GVCL framework is lacking preciseness. First of all, as described in Sec. 2.3, the result of the \\beta-ELBO, even with \\beta \\to 0, does not lead to the key hyper parameter \\lambda in online EWC. To compensate this, the authors introduce a modified KL divergence to make them similar. However, it is not justified, from a unified Bayesian or some other theoretical perspective , why the previous \\beta-ELBO needs to be modified. It is kind of wired to start from the Bayesian  framework and then go back to the non-Bayesian perspective to design a Bayesian algorithm to improve the performance, and then claim that the previous non-Bayesian algorithm is a special case of the unified Bayesian framework. Moreover, as described in Sec. 2.3, the  resultant GVCL when \\beta \\to 0 is actually different from the previous online EWC algorithm. As a result, strictly speaking, it is not approperiate to claim that the online EWC could be recovered as a limiting case. \n\n2. If it is true that the proposed GVCL is a generalization of VCL and Online EWC, which allows interpolation between the two, then it is expected and reasonable that the GVCL alone (without additional FiLM layers) should perform at least the same as VCL and online EWC. Otherwise, the statement is not true and there is no advantage of the proposed GVCL framework . However, as shown in experimental results, e.g., Table 1, GVCL alone performs worse than Online EWC in large datasets, which is really wired. The authors also acknowledged this point and claimed that this is due to the difficulty in optimizing GVCL with small \\beta. It would be better to make such statement more precise because this is really important point for this paper. Otherwise, it implies that the so-called interpolation between VCL and online EWC has no additional advantage. \n\n3. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers, rather than the GVCL framework itself.  To make this more clear and for a more fair comparison, it is highly suggested to compare other methods (online EWC, VCL, HAT, etc) with FiLM layers. Otherwise, the current improvement of the performance is unclear. In addition, the improvement of GVCL-F over the baseline is not consistent. \n\n\n\n"
            },
            {
                "round3": "Thank you for your response. As you suggested, we have now modified the Abstract and the Introduction in the latest version of the paper. This is to make the importance of the synergy between GVCL and FiLM layers more obvious. For example, the last paragraph of the Introduction now explicitly says that experiments are included with GVCL+FiLM layers, and we added a sentence, “In Section 5.4 we show that FiLM layers provide a disproportionate improvement to variational methods, confirming our hypothesis in Section 3.”\n\nWe have now updated the paper to include the following discussion of the predictive distribution at the end of section 2.2:\n\nWhen performing inference with GVCL at test time, we use samples from the unmodified $q(\\theta)$ distribution. This means that when $\\beta = 1$, we recover the VCL predictive, and as $\\beta \\to 0$, the posterior collapses as described earlier, meaning that the weight samples are effectively deterministic. This is in line with the inference procedure given by Online EWC and its variants.\nIn practice, we use values of $\\beta = 0.05 - 0.2$ in Section 5, meaning that some uncertainty is retained, but not all. We can increase the uncertainty at inference time by using an additional tempering step, which we describe, along with further generalizations in Appendix D.\n\nIf you have any suggestions, we are happy to modify the paragraph, or add more details in one of the appendices. \nThe other changes in the latest revision are given in the latest general response.\n\nIn the latest revision, we made some minor changes to some figures and section 2.2, which we outline here:\n\n1. Added a paragraph clarifying how predictions are made with GVCL at the end of section 2.2\n2. Updated outdated figures in appendix K which describe the easy-CHASY benchmark\n3. Fixed an error in the newly added plots from the last revision in appendix J where the joint training lines were plotted wrong\n4. fixed a problem with the SGD, Separate, SGD-Frozen, and Joint (MAP) training on CHASY datasets where early stopping was performed on slightly the wrong criteria. This affects table 1 and figure 2ab (in the backwards and forwards transfer metrics by ~0.2% and the plot of the joint training line). The change is very minor and does not affect our analysis or conclusions\n\n24/11 Small revision:\n1. Modified the abstract and introduction so that it is more clear that the improvements are from both GVCL and FiLM layers\n\nThanks again for your response.\n\n1. In the next revision, we will add a discussion of how predictions are performed in Section 2.2 noting that the VCL predictive is recovered when $\\beta=1$ and the Online EWC predictive is recovered when $\\beta \\to 0$ as the weight uncertainty disappears. In the paper, we typically use values of $\\beta$ between 0.2 and 0.05, so there is still some uncertainty retained.\n\n2. It is not clear to us whether Ritter et al. do use weight uncertainties when making predictions e.g. in section 2, paragraph 1 (https://papers.nips.cc/paper/2018/file/f31b20466ae89669f9741e047487eb37-Paper.pdf),\nthe paper says their aim is to find a MAP estimate to the posterior over all datasets. This would imply that they do not predict with uncertainty for *all* of the algorithms, including e.g. the one called \"Online Laplace\", even though this might be confusing. Furthermore, they describe EWC as “approximat[ing] the posterior .. with a Gaussian” (section 3 paragraph 2), but EWC also does not use weight uncertainty either, casting doubt on whether they use uncertainty too.  Moreover, the only (non-official) implementation we have found also only uses deterministic weights (https://github.com/hannakb/KFA).\n\nIndeed, generally, if you use Laplace's approximation for Bayesian neural networks with Monte Carlo sampling for forming the predictive, you get very poor results (see e.g. https://arxiv.org/abs/1906.11537). So you have to either temper the posterior by a large amount (e.g. by removing all uncertainty) or use the linearisation approximation discussed in the above paper.\n\nThank you for your quick response, and for your time.\n\nYou are correct when you state that the resulting posterior is not identical to that recovered using Laplace’s approximation: the means are the same, but the covariance is near-zero. Therefore the approximate posterior over the weights has no uncertainty. This is in fact exactly how Online EWC performs predictions (Schwarz et al., 2018, “Progress & Compress:  A scalable framework for continual learning”, Section 4). That is, predictions at test-time are made with deterministic weights (no uncertainty), and the Hessian / the link to Laplace’s approximation is only used for updating this mean parameter value.\n\nIf Laplace’s approximation is used to form a non-deterministic posterior which is used to make predictions, then that is a slightly different algorithm to Online EWC, which we agree does not fall under the GVCL family.\n\n\nMany thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. It is kind of weird to start from the Bayesian framework and then go back to the non-Bayesian perspective.\n\nWe did not intend to claim that our approach is entirely Bayesian, and our resulting algorithm is not. Rather, we considered this work to be Bayesian inspired: we looked at the Bayesian approach (VCL) and made careful adjustments to fix key issues affecting it. Please also see the general response, point 2 for more discussion of these points.\n\nIn the latest version of the paper, we re-interpreted $\\lambda$ so that it falls into the general theme of the paper as tempering different parts of the ELBO in light of the cold-posterior effect. Note that tempering is not Bayesian, but rather a commonly-used workaround for dealing with the poor performance of Bayesian methods.\n\n2. Moreover, as described in Sec. 2.3, the resultant GVCL when $\\beta \\to 0$ is actually different from the previous online EWC algorithm\n\nWe assume you are referring to the difference in $\\tilde{\\lambda}$ and $\\lambda$. These two approaches differ only in their treatment of the prior variance. In the original EWC paper, and the paper proposing Online-EWC, it is unclear what the treatment of the prior covariance should be, hence this difference arises due to an ambiguity in the original algorithm. Furthermore, note that with increasingly small $\\beta$, the difference between $\\tilde{\\lambda}$ and $\\lambda$ vanishes, as the prior term becomes negligible compared to the Hessian.\n\nWe have changed the text to make this clearer in Section 2.3.\n\n3. GVCL ... should perform at least the same as VCL and online EWC.\n\nAs we mentioned previously, GVCL performs worse than Online EWC sometimes due to optimization issues with convergence as $\\beta \\to 0$. However, note that GVCL-F always outperforms VCL-F and EWC-F as expected. We also note that there is a benefit to having a unifying framework that encompasses a range of existing approaches allowing them to be better understood.\n\nWe have now added a toy example of GVCL in a toy 2d regression dataset showing the convergent behaviour of GVCL to Online EWC. In this toy example, it takes 10 times longer to achieve convergence for very small values of $\\beta$ (1e-4) compared to $\\beta = 1$. Note that these values of $\\beta$ are smaller than we had in our neural network examples, and given that the toy example has only 3 parameters yet still took a very long time to optimize, it is likely that we cannot practically reach the Online EWC limit on a neural network of even modest size. \n\n4. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers\n\nThis is not a correct interpretation of the results -- the improvement comes from both GVCL and FiLM layers, with significant contributions coming from both. We show this explicitly in the new Section 5.4, which as you suggested, looks at the performance gains from adding FiLM layers to GVCL, VCL and Online EWC. It shows for example:\n\nSplit CIFAR: moving from VCL to GVCL is 25% gain, adding FiLM layers is a further 11% gain\nMixed Vision: moving from VCL to GVCL is 24% gain, adding FiLM layers is a further 30% gain\n\nAlso note adding FiLM layers to Online EWC only gives 0.1% and 7.7% improvement on these datasets respectively. See Table 2 and General Response point 3 for additional information.\n\nSo we see that FiLM layers alone provide little benefit to Online EWC, and a key insight in this paper is how FiLM layers interact with variational methods to fix the pruning issue. Unlike HAT, because of the interaction with the prior, we do not need more complex training procedures to learn the FiLM parameters. Note that we did not include HAT + FiLM layers since it already has per-task channel-wise gating layers. \n\n\nMany thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n\nWe purposefully included only the best algorithms in Figures 2 and 3 so that the y-axis covers an appropriate range. VCL and Online EWC perform relatively poorly, so we left them out so that it is easy to compare the performance of the better algorithms. We have added additional figures to Appendix K which show these plots, along with the performance of additional algorithms.\n\n2. Why is GVCL significantly worse than baselines for split-mnist (Figure 2c)?\n\nThe baselines in Figure 2c are potentially confusing, and it is unfair to compare vanilla GVCL to them. HAT and GVCL-F store task-specific parameters, and hence have growing memory demands with number of tasks. Therefore the natural comparison for HAT is GVCL-F, and not GVCL (note that Joint-MAP is not a continual learning algorithm). It appears that this is much more important for Split-MNIST than the CHASY benchmarks.\n\n3. Why is split-mnist omitted from Figure 3?\n\nMany of the tasks all effectively are at 100% performance, so including it does not provide much useful information. However, we have included the plot in Appendix K.\n\n4. The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nWe have added more explanation in the main text (end of Section 2.2). We have also now added a small toy example showing the convergent behaviour of GVCL to Online EWC in Appendix B, which includes a range of $\\beta$ values. \n\n\n5. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong\n\nThe legends are correct, and there is likely some misunderstanding here. We clarify that by “local” we refer to the immediate vicinity around a point, i.e. if we zoomed in very close to a point. We also reiterate that a “good” and a “local” approximation are not to be conflated, particularly in the toy examples presented.\n\nFor Figures 8b and 8c, there is a cusp at the mode of the distribution. Therefore, a local approximation would approximate the function immediately surrounding that cusp, and ignore the regions far away. We see that this is exactly what $\\beta = 0.1$ (very small $\\beta$) does: it has a very sharp curve. Similarly, for Figure 1, if zoomed in very close to the mode of (a), we would notice that the true distribution is nearly flat. This means that a local fit would match that flatness, and ignore the fact that it begins to curve further away from the mode. $\\beta = 0.1$  does exactly this. It appears to be a bad fit because of the scale of our graphs, but if we zoomed in very closely to the mode, it would appear to be the best fit, because it is the most local fit.\n\n\nMany thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. The two contributions seem quite orthogonal to each other and each of them is rather minor in novelty.\n\nWe disagree that these contributions are minor. In this paper, we show that several continual learning algorithms all arise from a single unifying framework, and certain choices and hyperparameters in each algorithm all arise by making different choices in the tempering of posteriors, priors, and likelihoods. For example:\n\n* VCL occurs when no tempering is performed\n* Online-EWC, Online-Structured Laplace and SOLA are instances of GVCL where $\\beta \\to 0$, and the choice of $Q$ distribution is changed\n* $\\lambda$ arises by tempering the posterior and prior using the same temperature in the KL-divergence\n* Online-EWC’s $\\gamma$ arises by tempering the posterior and prior using different temperatures in the KL-divergence\n\nA unifying algorithm immediately opens the door for different choices of these parameters, and lets us understand the relationship between and broader context of these algorithms. Naturally, it means that improvements and innovations in one of these algorithms can readily be applied to others and paves the way for rapid and systematic progress.\n\nOur second contribution, the usage of FiLM layers, addresses a key limitation in variational methods, which is particularly problematic in the continual learning setting. While this contribution is orthogonal to GVCL, it is particularly synergistic. What differs between our version of FiLM layers and other similar algorithms, such as HAT, is its synergy with variational methods. Because of the pruning effect and the prior, no special algorithm is needed to fit these FiLM layers, and the resulting gain for variational methods is over 10%, compared to merely 2% for non-variational methods. We have added a new section in the revised version of the paper to make this clear (Section 5.4).\n\n2. I am not sure if the authors a) compute Laplace’s approximation in the end, at the resulting mean of q, for any beta value?\n\nThis is a misunderstanding. We have responded to this point in the general response (point 1), but add further clarification below.\n\nTo be clear, we never compute Laplace’s approximation directly. We update $\\Sigma$ using the $\\beta$-ELBO and show that this recovers a version of Laplace’s approximation in a limiting case. In the derivation of this result, we did not assume $\\beta = 0$, but rather assume that $\\beta$ is very close to zero as we take the limit. When this is done, there is a cancellation in the $\\beta$-ELBO whereby the beta-dependence in the previous posterior cancels with the beta term in the $\\beta$-ELBO, giving rise to the EWC regularisation (see equations 10 and 11 in Appendix C).\n\nIn our derivation, we did not assume the covariance was zero, but we assumed it was $\\textit{near}$-zero, so we can still apply normal arithmetic to it. \n\nWe agree that in Appendix C, the statement that “q approaches a delta function” was rather imprecise. To amend this, we have now added a proof that moving from Equation 8 to 9 is valid for small $\\beta$. This is included in Appendix C.1.\n\n3. Related work: The related work section is rather short mentioning only very few related approaches. More effort is required here.\n\nNote that many related works are mentioned previously in the text (e.g. 16 unique texts in sections 2 and 3), not only in the Related Works section. Also, we have a longer related work section in Appendix I, which we were unable to include in the main text due to space constraints. As we now have additional space (from gaining an additional page), we are happy to expand this section in the main text. Please let us know of the specific references you have in mind.\n\n4. I am wondering why e.g. Fig. 2 does not include VCL and EWC\n\nWe have included the requested plots in Appendix J. We only included the best performing algorithms in figure 2 since VCL and EWC performance lies well below the range of the graph. \n\n\nMany thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified...I think the authors should dig deeper here for better justifications for such choices…\n\nWe agree that the writing in Section 2.3 could be improved and have re-written this section connecting the introduction of the parameter $\\lambda$ to the literature on cold posteriors. Please also see the general response, point 2 for more information.\n\n2. ”Additionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well?”\n\nWe have added Section 5.4, which shows the relative performance gain from adding FiLM layers to VCL and Online EWC. We see that GVCL and VCL both see large performance gains while Online EWC only receives marginal gains. This suggests that FiLM layers are particularly synergistic with VI based methods. Additionally, GVCL+FiLM outperforms VCL+FiLM.\n\n3. The title is somewhat misleading\n\nWe selected the title as the paper generalizes several existing continual learning algorithms under a single variational framework, related by the choice of Q distribution class and choices related to the tempering of distributions. We are happy to consider alternative titles and would be open to hear your suggestions.\n\n\nHere we note the changes we have made to the revised version of the paper:\n\n1. A derivation of the quadratic term multiplier based on tempering the posterior and prior in Section 2.3.\n2. Section 5.4 (and Table 2), which includes performance gains from adding FiLM layers to EWC and VCL.\n3. Appendix B, which empirically shows the convergence of GVCL to Online EWC in a toy example and highlights the difficulty of 4. achieving this limit in practice. \n5. Appendix C.1, which includes a proof that the delta-function argument used in Equation 8 to 9 is valid. \n6. Appendix C.3, which shows how Online-EWC’s $\\gamma$ arises by tempering the posterior and prior in the KL-divergence by slightly different temperatures\n7. Additional results of EWC + FiLM and VCL + FiLM added to the full result tables in Appendix J, as well as additional figures showing each algorithm’s performance on each task (similar to Figures 2 and 3).\n\n\nWe thank all the reviews for their thoughtful criticisms and suggestions. Here, we address the main points raised by reviewers and outline the changes we have made to the paper in response. More specific points are addressed in the response to each of the individual reviewers.\n\n\n1. Reviewer 2 has concerns about the mathematical limit that connects generalized variational inference to Laplace’s approximation \n\nThe reviewer has misunderstood the theory in our paper associated with the limit $\\beta \\to 0$. We briefly outline the key result: \n\n* Consider running GVCL with a common $\\beta$ value used across all tasks\n* Now take the limit of this procedure as $\\beta$ tends to zero\n* In this case, all the approximate posteriors (qs) limit to deltas around the MAP estimate\n* The inverse variances (precisions) of these approximate posteriors tend to sums of the Hessians at the MAP value scaled by 1/$\\beta$\n*Critically, the objective functions for each task become equal to online EWC due to a cancellation of the terms involving beta \n\nWe have improved the discussion of this limit by adding more detail to Appendix C and C.1, in particular we justify the argument that for small $\\beta$ the expectation in Equation 8 becomes approximately the Hessian at the mean (Equation 9).\n\n\n2. Reviewers 3 and 4 are worried that the introduction of $\\lambda$ -- which is necessary to recover Online EWC in a general way -- is not theoretically-well justified\n\nIt is true that, from a Bayesian perspective, it is not straightforward to justify the introduction of the parameter $\\lambda$. In the revised version of the paper, we add a theoretical explanation for reweighting the quadratic term with $\\lambda$ as well (Section 2.3).\n\nIn the re-written Section 2.3, we show that $\\lambda$ arises if we make use of tempering, as has been proposed in the context of cold posteriors (Wenzel et al. 2020). Specifically, at each step we temper the previous posterior before applying variational inference. We believe that this new interpretation sheds light on the relationship between the effectiveness of $\\lambda$ and cold posteriors.\n\nWe would also like to reiterate that our final algorithm cannot be strictly considered “Bayesian,” nor do we claim it to be. Rather, there is a general trend in the Bayesian Deep Learning community whereby Bayesian methods are used to develop new algorithmic approaches to deep learning problems and then relaxations of these approaches are considered, with additional parameters, that perform better empirically than the pure-Bayesian method. EWC was developed using this approach (and indeed this resulted in the same $\\lambda$ parameter being introduced without rigorous justification) and more recent work has followed this example (Kirkpatrick et al. 2016, Ritter et al. 2018, Osawa et al. 2019 , Pan et al. 2020, Wenzel et al. 2020,  Higgins et al 2017, Alemi et al. 2017). This class of approaches has been called ‘Bayesian Inspired’ and we see the current work as belonging to this pragmatic vein.\n\nIn this paper we take the more strictly Bayesian VCL algorithm, and improve it by addressing the main shortcomings of variational Bayesian methods. Namely, we address the poor data fit problem by considering tempered likelihoods with $\\beta$ and $\\lambda$, and fix the pruning issue using FiLM layers.\n\n\n3. Reviewers 3 and 4 question “whether the performance gains are from FiLM Layers or GVCL”\n\nWe have added results on all benchmarks with Online EWC + FiLM layers, and VCL + FiLM layers in section 5.4. These results show that (i) FiLM layers provide a significant benefit to the variational algorithms (VCL and GVCL), while not so much for Online EWC, (ii) GVCL + FiLM outperforms all competing algorithms, with both innovations contributing to the improved performance.\n\nTo summarise, in the revised Section 5.4 in Table 2 and Appendix J we see:\n* VCL+FiLM >> VCL and GVCL+FiLM >> GVCL, while EWC + FiLM $\\approx$ EWC.\n* GVCL+FiLM > GVCL > VCL+FiLM >> VCL \n\n\n4. AnonReviewers 1 and 2 have some questions about the omission of certain baseline algorithms from figures\n\nFor Figures 2 and 3, we only included the GVCL, GVCL-F, and the top performing baseline algorithm. This was done to keep the figure uncluttered, and to keep the y-axis in a reasonable range as these baseline algorithms perform poorly. However, as requested by reviewers, we have now added some extra figures in Appendix J. "
            }
        ]
    },
    {
        "id": "CBmJwzneppz",
        "decision": "Accept (Poster)",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: Optimism in Reinforcement Learning with Generalized Linear Function Approximation\nKeywords: reinforcement learning, optimism, exploration, function approximation, theory, regret analysis, provable sample efficiency\nAbstract: We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call ``optimistic closure,'' which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of $\\widetilde{O}\\left(H\\sqrt{d^3 T}\\right)$ where $H$ is the horizon, $d$ is the dimensionality of the state-action features and $T$ is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions."
            },
            {
                "round2": "Reviewer 1:\nI thank the authors for their response, it helped in understandin the author's submission. Still, after considering the author’s response, I feel my major concerns are still valid and I see little reason to change my overall assessment\n\nOn points 1) and 2), I brought up the issues to give the authors a chance to make their submission easier to understand for other readers. For example, if the only way to understand that their approach is optimistic is by carefully examining line 10 of the algorithm then I believe there’s room for improvement in understandability. The authors seem to believe the paper is fine as is, and as understandability is very subjective we’ll have to agree to disagree on these points.\n\nOn the weakness of assumption 2, the authors respond that “ we do not know of any weaker assumptions that permit computationally and statistically tractable RL.” This ends-justify-the-means argument does not advance science. Just because one wishes to prove something, and hasn’t come up with realistic assumptions by which to prove the thing doesn’t justify making unrealistic assumptions.\n\nIt therefore saddens me that the authors didn’t comment on whether the roadmap for weakening assumption 2 as laid out in Zanette would work for the author’s algorithm. While I agree that Zanette’s algorithm isn’t computationally tractable while the author’s proposed algorithm is, the analysis methods used there (splitting the error into an approximation and variance error term) could, in theory, be applied to the author’s optimistic algorithm. And that is exactly what I wrote in my comment, saying that the authors could perhaps better analyze their algorithm by using the strengths of Zanette’s theoretical approach. I would have hoped the authors address this question instead of addressing the unrelated issue of the intractability of Zanette’s algorithm.\n\nFinally, as a comment not on the paper itself but the author's response, I find the comments about what appeared when on arxiv unbecoming for three reasons:\n\n- I can’t verify the authors claim without also discovering the author’s identity\n- By claiming that “Zanette et al., cites this paper” the authors are giving hints about their true identity, which is unprofessional\n- The intent of mentioning Zanette’s work was not to say the author’s work is unoriginal or isn’t novel, but as a hint on how the author’s can strengthen their work. Therefore, who published first is irrelevant in this context. I wish the authors had constructively commented on this instead of bringing up irrelevant who-published-first debates.\n\n\nReviewer 2:\nI thank the authors for your response. I would like the authors to respond to some of the questions in the minor comments as well before I make my final recommendation. I put them as minor comments because I assume that these technical minors should not affect the main conclusions and theorem of the paper, but they might improve the paper and I would like to see how the authors will take these. For convenience, I will explicitly rewrite some least minor questions here that I would like to hear a response to.  \n-\tThe equation between eq. (5) and eq. (6) on page 14 does not look very right. I think the correct one should be the one with the RHS replaced by $\\langle x_{\\tau}, \\hat{\\theta} - \\bar{\\theta} \\rangle f’(\\langle x_{\\tau}, s \\hat{\\theta} + (1-s) \\bar{\\theta} \\rangle)$ for some $s \\in [0,1]$ (according to the mean value theorem). If this is true, I am afraid the bounds of the difference between $D_{\\tau}$ (after Eq. (10)) might not be precise. \n-\tIn Corollary 4, shouldn’t it be **2** $\\gamma \\| \\phi(s,a) \\|$ instead of $\\gamma \\| \\phi(s,a) \\|_{…}$?\n-\tOn page 15, the paper says that E[   xi_tau^# | x_{1:tau}, xi_{1 : tau-1}^#   ] = 0. Do we really need that martingale structure when we already consider a fixed g_{epsilon}? Given a fixed g_{\\epsilon}, we already have E[   xi_tau^# | x_tau   ] = 0.   \n\nReviewer 3:\nSummary After Discussion Period:\n-----------------------------------------------\nAfter corresponding to the authors and reading other reviews, my assessment hasn't changed much, which is that the paper is a good line of research but still needs improvement readability and strictness of assumptions.\n\nThe authors and reviewers all point out that this work is a relaxation over some previous works, e.g. Jin et al. Yet [1] has assumptions which are relaxed further than in this paper, and show that at regret bounds are possible with weaker assumptions. \n\nThe author's correctly point out their algorithm is computational efficiency while [1]'s algorithm isn't, which is a point in favor of the author's algorithm. Unfortunately, the benefits in computational efficiency were not clear to me and none of the other reviewers highlighted computational efficiency as one of the algorithm's strengths. If indeed one of the author's algorithm's main advantage over other work wasn't clear to the reviewers, then the paper still has room for improvement in readability.\n\nSummary: \n--------\nIn this paper, the authors propose a Q-learning method to solve episodic RL problems. Key to their method is assuming the Q-function takes the form of a generalized linear function plus an optimism term. Once this assumption has been made, they demonstrate that their algorithm, The LSVI-UCB algorithm provably finds a policy with bounded regret.\n\nPros:\n-----\nI like that the authors were able to show that using generalized linear functions for the Q function opens the doors to many theoretical analysis possibilities, and I like the modification they made to allow the Q-function to be optimistic. I thought a further strong point of the paper was the proof which shows that using general linearized q functions isn't a stricter assumption than the linear MDP assumption.\n\nAnd in general, I like the idea. I think it's a good line of research, and an idea which will yield important progress in the field of RL research.\n\nCons:\n-----\nI found this paper hard to follow. After multiple readings, I was still confused in multiple areas. This included \n\n  - What is the motivation of the function set $G_\\text{op}$?\n  - What are some examples of common RL problems for which the Q-function is / is not a generalized linear function\n  - Where does the matrix $A$ come from in $G_\\text{op}$?\n  - What is the motivation for the $\\Lambda_{h,t}$ in the algorithm\n  - Links to previous work, for example using the generalized linear models as a Q function, it's unclear if this is a new idea or is already present in previous work.\n  - It would be good to point out links not just in the related work section, but also while introducing concepts.\n\nTo discover why the author's algorithm is optimistic, we need to look at the details of the LSVI-UCB algorithm, a clear explanation isn't given anywhere else.\n\n\nThe other issue I had was with assumption 2. It is a fairly strong assumption, and although the author's show it holds for the linear MDP setting, it isn't nicely motivated why this assumption is realistic for other settings.\n\nIn any modeling setting, there's always a bias-variance tradeoff. As the model becomes more complex, it better captures the observations but is more prone to fitting the noise. By assuming the model can perfectly fits the true Q-function, the author's have assumed there's no \"bias\" in this bias variance tradeoff, and it is not surprising that they can then report lower regret as compared to other methods.\n\nI feel a better analysis should be more along the lines of [1], where they introduce the \"inherent bellman error,\" an error stating how far the true Q function is from the best estimate. One sees that this inherent bellman error then factors prominently in the regret bounds they show. There, they recognize that such a bellman error is generally non-zero, and prove their results by splitting the regret into an approximation and a variance term (like the bias variance trade-off).\n\n\nMinor concern:\n---------------------\nIn theorem 1, you state the regret is $O(H\\sqrt{d^3T})$ while in the abstract it's $O(\\sqrt{d^3T})$.\nPlease keeps it consistent.\n\nIn the bibliography, many of the works have been published. It's nice to cite the published version (i.e. the ICML or NeurIPS version) instead of the Arxiv version.\n\nConclusion:\n----------------\nIf the authors could better address assumption 2 (ideally by doing an analysis akin to [1]), this\nwould make the theory a good contribution to RL research. And if the authors could write their paper to tell a compelling story, where the different facts, assumptions, definitions and theorems nicely flow into one another, and one understands where things are coming from and where they are going, then this would be a good submission. But in it's current form, with an assumption which masks a large source of regret and a story which is hard to follow, I don't believe this paper is ready for submission.\n\n[1] Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near\noptimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020a\n\n\n\nReviewer 4:\nThe backdrop for this work is the linear MDP model. In linear MDPs, typically the transition function is assumed to be a low rank matrix in the span of d feature vectors (over S, A); such an assumption lends itself to regret bounds that only scale with d (and not explicitly with the size of the state space). \n\nThe first contribution here is to establish that it is enough to assume that the function approximation class (for Q functions) is closed under an optimistic (~inverse covariance bonus) version of Bellman update. Qualitatively, this is desirable because this is an assumption on the Q-function class and does not present an explicit assumption on dynamics, unlike linear MDPs. The paper establishes that this is strictly more general the linear MDP assumption, where the above-discussed closure holds for backups of all functions (and not just linear Q functions). It must be noted that Jin et al had already noted & observed that such an assumption is enough, and that their proofs accommodate this. \n\nThe second contribution is that the Q function class is generalized here to accommodate generalized (vs just) linear models.\n\nStrengths:\n+ I think this is an important relaxation in assumptions to point out. Bellman closure of the policy class seems like a necessary precondition; optimistic variant is a bit further, yet more palatable than a factorization of the dynamics matrix.\n+ The GLM part of the extension could be significant in practice, given similar observations in supervised learning.\n+ The proof exposition (Appendix A) here is potentially cleaner that Jin et al.\n\nComments:\n+ Regarding the first contribution, did the authors think it was necessary to modify any part of the proof from Jin et al? From my reading, since all concentration arguments were always made on backups, it seemed their proof did indeed go through.\n+ Regarding the second contribution, what changes did does this work introduce to handle GLMs? I understand part of the answer may be in Lemma 6.\n+ Typo: Page 5 > linear MPD?\n\nReviewer 5:\nThe authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi-definite quadratic form. They motivate the assumption by showing that the assumption allows the tabular MDP case to be modeled, and that the Optimistic Closure is in fact a strictly weaker assumption than the linear MDP assumption made in previous related works. The authors then proceed to the design and analysis of the LSVI-UCB algorithm, which involves estimating the the parameter of the GLM model by a ridge estimator and adding an optimistic exploration bonus to the Q function. The authors propose a regret bound for the algorithm.\n\nThe proposed work is an interesting development to the line of research on RL with function approximation, and is large well written. I am in favor of acceptance, given that it provides a non-trivial extension to what is known and the Optimistic Closure assumption seems to me to be closer to the reality than the linear MDP assumption. One suggestion is to investigate if other large scaled but structured MDP models, such as the Factored MDP model by Osband and Van Roy 2014 : https://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps, and the LQG model, satisfy the Optimistic closure assumption with appropriate choices of $\\phi, f$.\n\n\nMinor comments:\n\nIn the abstract, brackets are missing for the d^3\\sqrt{T} regret bound.\n\nOn page 3, $\\Gamma$ should be replaced by $\\gamma$.\n\nOn page 5, MPD -> MDP\n\nReviewer 6:\n### Summary  \nThis paper analyses an existing algorithm (LSVI-UCB) with generalized linear function approximation instead of conventional linear function approximation.  Under this generalized linear setting, they propose a so-called “optimistic closure” assumption which is shown to be strictly weaker than the expressivity assumption in the conventional linear setting. The paper then proves that LSVI-UCB still enjoys sub-linear regret in the generalized linear setting with strictly weaker assumptions. The paper also derives a general error propagation through steps that do not require a closed-form expression of the empirical dynamic and reward functions as in the linear case; this could be applicable to general function approximations.   \n### Strong points \n-\tNovelty: The generalized linear setting appears novel and generalizes the linear settings. \n-\tSignificance: The optimistic closure appears novel and is strictly weaker than the linear MDP assumption in the prior works. \n-\tCorrectness: A complete analysis that successfully retains a sublinear regret and honest comments on the limitations of the present work.   \n\n### Weak points \n-\tThe work is almost merely about analysis of an existing algorithm with modest algorithmic contribution (which however is not a big problem). There are some parts of the proofs pointed out in the Minor comment section that potentially require some attention (but I believe these are minor points which could be fixed if there is any issue)\n\n### Minor comments     \n-\tPeriod ‘.’ after the first sentence of the second paragraph of section 2.\n-\tFirst sentence of section 3: ‘MPD’ -> ‘MDP’\n-\tLemma 1: Should it be $\\pi_{h,t}$ instead of $\\pi_t$ there?\n-\tIn Appendix A: “We believe these technical results will be useful in designing RL algorithms for general function classes”. It seems that an analysis of LSVI-UCB with general function classes has recent done in [1] (?)  \n-\tIn Corollary 4, shouldn’t it be **2** $\\gamma \\| \\phi(s,a) \\|$ instead of $\\gamma \\| \\phi(s,a) \\|_{…}$?\n-\tAt the end of Page 12: “The first term forms a martingale” -> shouldn’t it be a “difference martingale” instead?\n-\tThe equation between eq. (5) and eq. (6) on page 14 does not look very right. I think the correct one should be the one with the RHS replaced by $\\langle x_{\\tau}, \\hat{\\theta} - \\bar{\\theta} \\rangle f’(\\langle x_{\\tau}, s \\hat{\\theta} + (1-s) \\bar{\\theta} \\rangle)$ for some $s \\in [0,1]$ (according to the mean value theorem). If this is true, I am afraid the bounds of the difference between $D_{\\tau}$ (after Eq. (10)) might not be precise. \n-\tThe second paragraph on page 12: “Hence $y_{h, \\tau}$ is not measurable with\nrespect to the filtration $F_{\\tau}$ ,  which prevents us from directly applying a self-normalized martingale\nconcentration inequality”. Should it be $F_{\\tau-1}$ instead of $F_{\\tau}$?\n\n-\tOn page 15, the paper says that E[   xi_tau^# | x_{1:tau}, xi_{1 : tau-1}^#   ] = 0. Do we really need that martingale structure when we already consider a fixed g_{epsilon}? Given a fixed g_{\\epsilon}, we already have E[   xi_tau^# | x_tau   ] = 0.   \n\n\n### Questions for the authors \n-\tIn Chi Jin et al. 2019, the regret is the difference between the optimal value function and the value function estimate while in the present paper, the regret is the difference between the optimal value function and the expected value of the cumulative rewards by the algorithm. What is the difference between these two notions of regret? Can it make the two results comparable? \n-\tIn the proof of ‘Fact 1’, why Q^*_H \\in \\mathcal{G}? For that to hold, it seems to require that the expected reward \\mathbb{E}[r_H] has a generalized linear form of \\mathcal{G}? If so, one way to fix it is maybe letting 1 <= h <= H (instead of 1 <= h < H) in Assumption 2? \n-\tIt seems that [1] already analyses LSVI-UCB with general function approximations which means that [1] is more general than the present work (?) If so, could the authors comment on the benefit of this work for a generalized linear function class given that an analysis for a general function class has been done? For example, does the present work give a tighter bound when considering generalized linear function as compared to the bound for a general function class in [1]?\n\n\n### My initial recommendation \nOverall, I vote for accepting. An extension from linear settings to generalized linear settings is novel and natural, and it must be done at some point. I think this work is nice for filling in that gap. \n\n### My final recommendation \n\nI remain my initial score after the discussion. \n\n### References\n[1] Ruosong Wang et al. “Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension”\n\n\n### Additional comments about the correctness of the proof of Lemma 8\n\nI have recently checked their proof of Lemma 8 and noticed one thing that looks a bit strange to me. Since the discussion is over, I hope the authors will clarify/fix it in their final paper. That is, in the proof of Lemma 8 in the step where they applied Lemma 7 (Azuma's),  they used $c_{\\tau'} = |q(u_{\\tau'}, \\phi')|$, but the Azuma's inequality requires that $c_{\\tau'}$ is a constant while here $|q(u_{\\tau'}, \\phi')|$ is a random variable (depending on the random variable $u_{\\tau'}$). How is this possible to apply Azuma's inequality here when $c_{\\tau'} = |q(u_{\\tau'}, \\phi')|$ is random? "
            },
            {
                "round3": "We highlighted the computational efficiency point above, because with current techniques it does not seem possible to get the bias-variance decomposition in a computationally tractable manner. It is not just an issue of the analysis, but rather the algorithm itself. Put another way, we do not think that the optimistic algorithm will be robust to approximation errors, due to subtleties regarding error propagation. On the other hand, we do believe that we can get a computationally _inefficient_ algorithm that is robust even in the GLM setting (which would generalize Zanette et al.), but it would look very different from our optimistic algorithm here. \n\nThe point is that these two issues are _not_ unrelated at least given current techniques. We must give up robustness to approximation error to enjoy computational efficiency, so you have to choose which you care about more. Perhaps this is a matter of taste, but we felt that computational efficiency is more important than handling approximation errors. We apologize if we did not make this clear in our earlier comment.\n\nThanks for bringing up the minors again, here are some responses:\n\n1. There are two typos here and thank you for noticing them. In the middle term, we are integrating the derivative f'(s) instead of f(s); in the last term the integrating term should be $f'(<x_\\tau, s\\hat\\theta+(1-s)\\bar\\theta>)$. This chain of equalities actually holds by the fundamental theorem of calculus, instead of the mean value theorem.\n\n2. Yes you are right, there should be a factor of 2 here.\n\n3. Both are correct. We can mention this in the final version.\n\nWe'll make the appropriate changes in the final version. Thanks!\n\nWe thank the reviewer very much for his/her helpful suggestions. Below we respond to the main concerns/questions from the reviewer.\n\n1. Difference in regret definition with Jin et al: The objective in Jin et al., is *not* the value function estimate, but rather the true value function of the deployed policy \\pi_k. Due to the fact that we are taking an expectation (and we do not update the policy during the episode), the two objectives are actually the same. Note also that the actual collected rewards differ from this quantity by at most H\\sqrt{T} due to Azuma-Hoeffding.\n\n2. In the proof of Fact 1, why Q_H^\\star is in \\mathcal{G}? Indeed, it is required (as the base case of the inductive proof) that the Q_H^* for the last episode belong to the function class G. We will clarify this in the revised paper, as suggested by the reviewer by strengthening the Assumption 2 to make sure that Q_H^* is in \\mathcal{G}.\n\n3. Comparison to the results of Ruosong Wang et al. We would like to clarify that, the paper mentioned by the reviewer is actually a *follow-up* paper of our results. Indeed, our paper was arxived in December, 2019 and the paper of Ruosong Wang et al. was arxived in May, 2020, in which they clearly cited our paper as a starting point/prior literature.\n\nWe thank the reviewer very much for his/her appreciation of our paper and the helpful comments. We do not think that the Factored MDP model satisfies optimistic closure for any \"small\" Q-function class. Indeed, it is known that the optimal Q function for a factored MDP is in general extremely complicated (formally cannot be expressed as a polynomially sized circuit), and for this reason, all known provably efficient approaches for Factored MDPs are model based. We do not expect model-free methods to be sample-efficient in these environments.\n\nRegarding the LQG, we do know that the simpler LQR satisfies completeness using quadratic value functions, but unfortunately we do not believe it satisfies optimistic completeness. The reason is that with a quadratic value function at time h+1, the one-step optimal policy is linear, which results in a quadratic value function at time h. But the optimism bonus results in a quartic value function at time h+1, which does not admit a closed form optimal policy.\n\nWe thank the reviewer very much for his/her helpful suggestions.\n\nIt seems the main concern of this reviewer is regarding the contribution of this paper relative to the results from Jin et al. As remarked by the reviewer, Jin et al's proof goes through as is for linear functions, as they only back up functions in our class \\Gcal_{up}. However, as remarked, we feel this is a conceptual point worth emphasizing as the linear MDP does not naturally accommodates the GLM structure.\n\nOn the technical side, Our analysis has some differences with that of Jin et al., to address the GLM setting. For example, we use the constrained least squares objective, rather than the regularized objective. This manifests in lemma 6, where we also incorporate the required changes to address GLMs.\n\n\nWe thank the reviewer for the helpful suggestions. For the concerns raised by the reviewer, we respond as follows:\n\n1. Some intuitive explanation of notations: The motivation for G_up (not G_op, as the reviewer mistakenly copied) is to define a function class that covers all optimistic policies. The matrix A in the definition of G_up is part of the confidence interval, similar to the role of the sample covariance in the construction of confidence intervals for linear contextual bandit. The motivation of Lambda_{h,t} is the sample covariance matrix which will be used to construct confidence intervals.\n\n2. It's not clear that the LSVI-UCB algorithm is optimistic: We would argue that the optimistic nature of the LSVI-UCB algorithm is very clear, from line 10 of the algorithm that clearly appends a confidence interval term to the generalized linear estimates of the Q function. This kind of optimism term appears in many other settings, including (generalized) linear bandits, so there's no need to look into further details of the algorithm to see the optimistic structure.\n\n3. Assumption 2 is fairly strong and not realistic: This is true to some extent, but several points are worth emphasizing. First, Assumption 2 is strictly _weaker_ than the linear MDP assumption that has become quite popular in the theoretical analysis of RL (as we show). Second, it is unlikely that these kinds of optimistic algorithms provably succeed under much weaker assumptions, indeed very recent work shows that just assuming realizability of Q^\\star would be insufficient. Third, we do not know of any weaker assumptions that permit computationally and statistically tractable RL (to date, there is no computationally efficient method for the the low IBE setting of Zanette et al.). Thus our results represent the weakest tractable assumptions to-date and are close to what is information-theoretically possible.\n\n4. Bias-variance tradeoff: While our analysis indeed assumes there exists a perfect fit of the Q functions in generalized linear forms, we would like to clarify that reporting lower regret is NOT the focus or objective of this paper. The main objective/message of this paper is to show that the regret analysis and algorithms that are previously developed for purely linear Q approximation functions can be extended to the much more general model classes discussed in this paper, thereby making the analysis/algorithm more applicable to reinforcement learning questions.\n\nWe would also like to point out that Zanette et al. appeared on arxiv several months after this paper first appeared. Indeed Zanette et al., cites this paper!\n\nMinor issues: Yes we can update both the H-dependence and the venues in the bibliography"
            }
        ]
    },
    {
        "id": "VyDYSMx1sFU",
        "decision": "Reject",
        "source": "ICLR 2021",
        "statements": [
            {
                "round1": "Title: End-to-End on-device Federated Learning: A case study\nKeywords: Federated Learning, Machine Learning, End-to-End Learning, Artificial Intelligence\nAbstract: With the development of computation capability in devices, companies are eager to utilize ML/DL methods to improve their service quality. However, with traditional Machine Learning approaches, companies need to build up a powerful data center to collect data and perform centralized model training, which turns out to be expensive and inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case, the wheel steering angle prediction in the field of autonomous driving. Our results show that Federated Learning can significantly improve the quality of local edge models and reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to real-world embedded systems."
            },
            {
                "round2": "Reviewer 1:\nThis paper applies federated learning to  steering wheel prediction for autonomous driving. \"Federated learning\" in this draft mainly refers to an on-device distributed training algorithm where each edge device hosts its private data and performs local updates (model training) and send the updates back to a central server to aggregate. More specifically, this paper uses the most well-known algorithm in federated learning, FedAvg (McMahan et al. 2017). \n\nPros\n+ The application is real and seems important. \n+ Distributed/federated learning makes sense for this application. \n\nCons\n- The main contributions of the draft are not clear. It looks to me such empirical studies of a well-known algorithm on a specific application will better fit a more application-oriented or system-oriented venue, e.g., CVPR, SysML. \n- How are the hyperparameters tuned for centralized and federated setting? \n- What are the hardwares on edge devices/vehicles, and what are the hardwares in datacenter for centralized training? The draft mentioned Tesla T4 GPUs, but it seems not clear exactly how much computation power has been used. \n- Could the authors clarify \"companies need to build up a powerful data center to collect data and perform centralized model training, which turns out to be expensive and inefficient. Federated Learning has been introduced to solve this challenge.\"? As far as I know, the primary motivation for federated learning is privacy protection. Edge devices has far less computation power and big communication barrier, why would it solve \"this challenge\"?\n- The following sentences seem to against the anonymous rules? \"Our previous research shows the challenges of deploying AI/ML components into a real-world industrial context. As we defined in ”Engineering AI Systems: A Research Agenda” (Bosch et al., 2020), AI engineering refers to AI/ML-driven software development and deployment in production contexts. We found that the transition from prototype to the production-quality deployment of ML models proves to be challenging for many companies (L’heureux et al., 2017b) (Lwakatare et al., 2019).\"\n\nSome minor improvement:\nThe abbreviation “ML/DL” seems never introduced\nIt seems unnecessary to capitalized “Machine Learning”, “Federated Learning”. \nConsider cite the original FedAvg (McMahan et al. 2017) paper instead of (Li et al., 2019).\n\n====== post rebuttal ======\n\nI do not think the response addressed my concerns. I would strongly suggest authors reconsider the design choices where I raised questions. Note that these are not only clarification questions, but also fundamentals of machine learning and federated learning.\n\n\nReviewer 2:\n******************************************************************************\n\nThe paper is not properly anonymized. The intro refers to “Our previous research” and says “As we defined in Engineering AI Systems: A Research Agenda, (Bosch et al., 2020), … .” As such it violates the anonymity policy. \n******************************************************************************\n\nThis paper describes end-to-end implementation of Federated Learning (FL) on a use case of steering wheel prediction in autonomous driving. It provides empirical evaluation on real-world autonomous driving datasets and shows improved performance compared to centralized learning methods. \n\nPros:\nIs it interesting to see an implementation of FL on a real-world use-case. The paper also does well in comparing different factors such as training time and bandwidth cost for FL and centralized training.\n\nCons:\nThe paper doesn’t have enough technical depth to be accepted at ICLR and reads more like a report than a paper. It mainly describes the implementation of FL for a real-world application, which, although important, does not contribute to the field in terms of developing better algorithms or better understanding the current ones.  \n\nA large part of the experiment section describes the hardware features, network structure and training method in great details, which seems redundant or unnecessary for an ICLR submission. For example, section 4 reads “The weights of the CNN are adjusted using back propagation to enforce the model output as close as possible to the desired output.”, which is obvious to most readers.  \n\nThere are also some statements in paper that are not quite scientific or concrete. For example, the intro reads “due to the characteristics of Federated Learning, on-device training becomes possible.” This is not true as on-device training is not becoming possible due to FL, though FL certainly requires it. \n\n\nReviewer 3:\nThis paper presents a case study that applies Federated Learning for steering angle prediction in self-driving cars. All methods used have been previously proposed in the literature.\n\nPros:\n+ A case study for an industrial use of federated learning (in autonomous driving application).\n+ Results do show that Federated Learning can give accuracy close to a centralized model for this application but without having to send data to the server (thus saving training time and communication bandwidth requirements).\n\nCons:\n- No actual research contribution since nothing new is proposed in this paper.\n- While the training time and communication bandwidth savings are a good validation, this is not surprising since Federated Learning has been shown to have this benefit for many applications. \n\n========== UPDATE AFTER REBUTTAL ===========\nI have read the author's response. While the case study for industrial applications is important, it would probably be much more impactful if the same study was done on a much larger/realistic scale. For instance, right now it appears that each edge vehicle gets an already available dataset for federated learning, which may have been cleaned and preprocessed properly. For claiming a real industrial deployment/importance, it would have been great if the study was conducted with vehicles receiving real-time data from real vehicles which is prone to be extremely noisy (although the reviewer is not sure if this would be possible for regulatory reasons (e.g., if such learning experiments would be safe enough on real autonomous vehicles as these applications are safety-critical)). Currently, the paper neither has significant enough contributions from novelty side, nor from industrial deployment angle. Hence, as such, the paper cannot be accepted. Perhaps more application-oriented conferences maybe more suitable for this kind of work. \n\n\n\nReviewer 4:\nThe study evaluates federated learning (FL) in the context of steering wheel angle prediction, which is relevant for autonomous driving systems. Authors compare against two baselines a centrally-computed and locally-computed models and measure prediction error, training time and bandwidth cost. The work evaluates an existing approach and therefore its novelty and impact is limited. It does provide an interesting evaluation of FL for a relevant use case. Federated learning, as the authors indicate in the manuscript, is a promising approach for training ML applications while preserving user privacy, which is key to many industrial ML applications such as voice assistants and computer vision algorithms. For that reason, the impact of the paper is significant despite not being very original. The authors carry out a very simple study, but which seems sufficient to demonstrate that FL can have computational advantages, namely reduced training times and bandwidth costs. A challenging application of FL are ML applications that run on small devices that people carry around all the time, such as mobile phones and wearable devices.  In that scenario, there is the additional constraint that resources for training models on device are typically limited,  the smaller the device the more limited. An interesting extension of this study would be evaluate amount of computational resources used on the device as an additional evaluation metric. It would be great if the authors could add this metric to the present paper, but it could also be something for a followup publication, in other words, I do not think is needed for this paper to be published.   \n\n[Update after author's rebuttal]\nI do not see any reason to modify my rating. I also identified the self-citation , but it did not affect my rating or evaluation of the paper. "
            },
            {
                "round3": "No author response available"
            }
        ]
    },
    {
        "id": "DXU0DQUDWLA",
        "decision": "Reject",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Disentangling One Factor at a Time\nKeywords: unsupervised representation learning, disentanglement, Variational Autoencoders, Generative Adversarial Networks\nAbstract: With the overabundance of data for machines to process in the current state of machine learning, data discovery, organization, and interpretation of the data becomes a critical need.   Specifically of need are unsupervised methods that do not require laborious labeling by human observers.  One promising approach to this enedeavour is \\textit{Disentanglement}, which aims at learning the underlying generative latent factors of the data.  The factors should also be as human interpretable as possible for the purposes of data discovery. \\textit{Unsupervised disentanglement} is a particularly difficult open subset of the problem, which asks the network to learn on its own the generative factors without any link to the true labels. This problem area is currently dominated by two approaches: Variational Autoencoder and Generative Adversarial Network approaches.  While GANs have good performance, they suffer from difficulty in training and mode collapse, and while VAEs are stable to train, they do not perform as well as GANs in terms of interpretability. In current state of the art versions of these approaches, the networks require the user to specify the number of factors that we expect to find in the data.  This limitation prevents \"true\" disentanglement, in the sense that learning how many factors is actually one of the tasks we wish the network to solve.  In this work we propose a novel network for unsupervised disentanglement that combines the stable training of the VAE with the interpretability offered by GANs without the training instabilities.   We aim to disentangle interpretable latent factors \"one at a time\", or OAT factor learning, making no prior assumptions about the number or distribution of factors, in a completely unsupervised manner.  We demonstrate its quantitative and qualitative effectiveness by evaluating the latent representations learned on two benchmark datasets, DSprites and CelebA. "
            },
            {
                "round2": "Reviewer 1:\nI appreciate the authors' effort in addressing my questions. The authors’ rebuttal clarifies some of my concerns. As the other reviewers point out and the authors also agree, the paper needs significant improvement. Therefore, I will keep my score unchanged. \n\nReviewer 2:\nThank you!\n\nRegarding Q1 and Q2: while your claims might be true, you need to provide experimental evidence to justify them (e.g., \"we believe that no GAN-based approach can actually recover the 'correlated, nuisance or noisy factors' that it has modeled from the data points\", \"it is unclear if the real disentanglement comes from the use of the -TCVAE as a backbone, or if it is a result of their contributions.\").\n\nRegarding the results: thank you for adding the results on CelebA, which look promising. I will increase the score to reflect that. However, I think it is still not sufficient to demonstrate the claimed benefit of 'disentangling real-world datasets with an unknown number of factors'. \n\nTherefore, I will still keep the score negative.\n\nReviewer 3:\nThank you for the reply. Which is '\\vsp{cite}' is your response?\n\nReviewer 4:\nsummary_of_the_paper: This paper presents an unsupervised learning model for disentangled latent representation learning. Specifically, the model works on a combined latent space including both entangled variable and separable variable. The most impressive part is the one-at-a-time (OAT) factor learning approach that iteratively uncovers disentangled dimension and learned from reconstructed samples. The OAT approach generally sounds. \n\nmain_review: The paper is well motivated and the OAT technique is kind of novel and impressive. However, paper clarity can be improved, which has impeded reader's understanding and rating of the paper.\n\nContribution wise, this paper is not the first ones to use an \"entangled+disentangled\" latent space (eg paper named \"Toward Controlled Generation of Text\" in 2017), but the OAT learning approach is interesting. \n\nThe presentation can be improved, given so many typos, improper subscripts, equation-naming (Eq 6 or Eq 3.4.2), etc. Moreover, the central equation (Eq.6) is hard to follow, say how q(x) and its KL divergence is evaluated is unknown.\n\nIn Eq 5, whether or not the p(z_1) in last KL term is also factorizable. \n\nSince the intervention is key idea, it's a miss to see it not shown in Figure 1 plot.\n\nThe paper needs more explanation on how to avoid posterior collapse and avoid the collapse of disentangled latent variables.    \n\nThe table column in Table 1 is hard to follow. From Table 1, it seems OAT model has not outperformed other baselines, right?\n\nsummary_of_the_review: The paper is well motivated and the idea of OAT technique generally sounds. However, the paper reads like a manuscript in a rush: we see many typos, unclear subscripts. Moreover, we see unclear equations, such as Eq. 6, which is hard to follow, but the most important equation for the paper. Weird Table 1 columns and not strong experimental metrics. Overall, the paper needs a polish to improve its clarity and readiness to publish.  \n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 5:\nsummary_of_the_paper: This study proposes a disentangled representation learning method called \"one at a time\", or OAT factor learning which is a VAE-GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner, without knowing the number of ground-truth factors a priori. The authors formulate the latent space as a union of two disjoint sets, $z_1$ and $z_2$, where $z_1$ corresponds to the variational (disentangled) factors, and $z_2$ denotes either noise or sample-specific factors. To train the proposed network, the authors use a two-step training approach in which they first train the VAE network with only $z_2$ latent factors. Then, they start to learn $z_{1_k}$, one dimension at the time. To enforce $z_1$ to encode the disentangled factors, they use an intervention-based approach, in which the value of one dimension in $z_1$ is changed, while the remaining $z_1$ factors are kept unchanged. The authors reported their experimental results for two datasets, i.e., dSprites and CelebA.\n\nmain_review: Originality: \nI think this work lacks notable originality and novelty. The VAE-GAN networks (proposed by Larsen et al., 2016) have been studied in many papers. The proposed formulation of latent space that splits latent factors to two disjoint sets is also not novel. The only novel part is assuming an upper bound for variational factor, where it is not clear how the model can find the true number of factors, while $|z_1|$ is gradually increasing up to $K$ over iterations of training.\n\nQuality: \nThe motivation of the work is good; learning a disentanglement representation is quite a challenging problem. However, the paper could not appropriately address this challenge and the authors’ contribution is quite limited. The paper does not offer any theoretical analysis or profound experimental studies. The reported results for dSprites and CelebA do not showcase the superiority of the proposed method, neither in terms of existing disentanglement scores nor quality of the reconstructed images. \n\n\nLimitations: \n- It is not clear how the suggested formulation for the latent space is enhancing the disentitlement.  I cannot see why we need to encode $z_2$ while it is expected to encode noise or sample-specific variations, which is not desirable in the latent representation learning. \n\n- While the authors claim that the model does not need to know $|z_1|$, it is not clear how the model learns the true $|z_1|$. If I am not wrong the model iteratively unmasks the dimensions of $z_1$, without any stopping criterion. \n\n- The experimental study is limited to two datasets, without sufficient ablation studies on each dimension of $z_1$ and $z_2$. It is critical to explore the impact of $z_2$ vs. $z_1$ and quantify the correlation between these factors. \n\n- Additionally, I think the recent study by Träuble et al. ICML2021, and its discussion on the correlated latent space is quite relevant to this work. The authors might need to refine the discussion on the casual factors and disentailment studies, based on the findings of this paper.\n\nMinors comment:\n- Page 7: “The value of $\\beta$ is increased linearly during training to encourage the model to encode information in $z_1$ instead of $z_2$.” $\\beta$ or $\\gamma$? I got confused. \n- There are some typos in the text, e.g., “/mse”, “a a” at page 3.\n- The table and Figures are not cited in the manuscript. \n\n\nsummary_of_the_review: Learning a disentangled and interpretable representations is an important topic in the machine learning. The proposed method tried to address this challenge. However, I think the contribution is not notable and the paper lacks theoretical and empirical justifications to support its claim. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 6:\nsummary_of_the_paper: The authors propose a new method to learn a disentangled representation. The proposed method is Variational Autoencoder (VAE) based. The latent variables consist of nuisance factors and disentangled factors that form the $k$ generative factors per dataset. To help learn the disentangled factors of variation the authors propose to use an additional discriminator and interventions on the disentangled factors.\nThe authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset.\n\nmain_review: ### Strengths\n- Interesting approach to combine the strengths of VAE for disentangled representation with the power of a discriminator.\n\n### Weaknesses\n- Not really convincing experimental results in Table 1\n- Lack of clarity and details regarding the training procedure (see questions)\n- Lack of ablations with respect to the number of learned disentangled factors $k$, which is a central part of the proposed pipeline\n\n### Questions\n- The authors of the ID-GAN paper (Lee et al: High-fidelity synthesis with disentangled representation, 2020) write about the trade-off between generative quality and disentangled representation. Did the authors also experience that? Or was the generative quality not part of their evaluation?\n- What about model selection? The authors do not write much about how they selected their models. The training procedure involves multiple steps and to me it is not straightforward how to select the model in an unsupervised manner for every training step out of a range of multiple possible hyper-parameters. It would be interesting to also see an ablation study regarding the most important hyper-parameters.\n- Table 1 is a bit confusing. What is the scheme of making some entries bold? It does not seem the highest value per column. In general, I would have appreciated a more detailed description of the results presented in Table 1. Also, I cannot find a reference to table 1 in the text.\n- There is no ablation on the number of learned factors of variations $k$. To me, it is not obvious that the right number of disentangled factors can be found. And for table 1 it is not even disclosed what the respective k is, nor compared to the latent space size of previous works.\n- What are the motivation for another researcher to use your method? It does not seem to be able to outperform previous methods, nor does it seem to have a simple training procedure.\n- In section 3.4.2., step 1 the authors write that the $\\gamma_i$ are set to zero and, hence the corresponding latent factors are not trained. Although it is true, the regularisation for these factors is not part of the objective, in my opinion,  information flows through this part of the network nevertheless and the weights will be updated through their contribution to the reconstruction loss. I would appreciate the authors' thoughts on that.\n- Also in section 3.4.2, step 1, the authors describe their one-at-a-time training approach by switch the $\\gamma_i$ from 0 to 1. I cannot find any information on how this is scheduled during training nor how it is stopped. I understood that the number of disentangled factors $k$ is not a hyper-parameter (only upper-bounded) as it is learned during training. I would appreciate a bit more details on that.\n- Is it a goal of the proposed method that no information correlated to the disentangled factors $z_1$ is encoded in $z_2$ (low mutual information between the two representation)? If so, how is this achieved? Did the authors explore that direction?\n\n\nsummary_of_the_review: Although this work presents an interesting approach to learning disentangled representations, in my opinion it is not ready to be published at ICLR at this stage. It lacks convincing arguments why a researcher in this area should choose the proposed method. In my opinion what is needed to improve the paper is more clarity and details regarding the training procedure, e.g. model selection criteria for the different training stages, ablation studies on the learned number of disentangled factors $k$ and a better description of their central and only results table.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 7:\nsummary_of_the_paper: The paper proposes a new approach for training disentangled generative models. On top of VAE, it separates the latents into two disjoint groups: disentangled and entangled ones, and progressively increases the size of the disentangled group one at a time. To encourage disentanglement, it changes one random dimension of disentangled latent and pushes it to decode and then encode into the same one, and has a GAN loss to make sure that the changed generated distribution matches the ground-truth distribution. The paper shows the performance of the proposed approach on dSprites and CelebA datasets.\n\nmain_review: The paper addresses an important problem. However, it has several issues, including inaccurate claims, insufficient experiments, and many typos. See below for more details.\n\n*** Inaccurate claims ***\n\n* Page 2: 'Current state of the art methods make the assumption that there are a ﬁxed number of independent factors for all the data points in the dataset. However in real datasets, in addition to the independent factors common to all points in the dataset, there might also be some correlated, nuisance or noisy factors pertinent to speciﬁcally only certain data points'. Note that many GAN-based approaches (e.g., InfoGAN) only regularize a small part of latents; they can indeed model 'correlated, nuisance or noisy factors' (using the non-regularized latents) besides the 'independent factors' (using the regularized latents). Please fix this statement. (I see that you do admit it later at the end of Section 3.2.)\n\n* The paper mentions in many places that the idea of progressively learning one factor at a time is new. However, a similar idea has already been proposed in 'Robust Disentanglement of a Few Factors at a Time' at NeurIPS 2020.\n\n*** Insufficient experiments and results ***\n\n* From table 1 it seems like the proposed approach performs much worse than the state-of-the-arts like FactorVAE and InfoGAN-CR. \n\n* If the main selling point of the paper is the ability to disentangle real-world datasets with **unknown number of factors**, then the paper should conduct experiments to demonstrate that (i.e., on the settings when the approaches that set a pre-defined number of factors fail, and show how your approach can improve that)\n\n*** Typos ***\n\n* Page 2: 'related Work'\n\n* Page 3: '/mse'\n\n* Page 5: missing a period after 'factors are integrated out'\n\n* Figure 1 caption: 'so we ﬁrst group the correlated latents into one space, z_1'. Here z_1 should be z_2?\n\n* Page 7: 'z_2.For brevity' -> 'z_2. For brevity'\n\n* Page 7: '\\gamma_ii = 1:K' -> '\\gamma_i i = 1:K'\n\n* Page 7: '\\gamma_1=1 and \\gamma_i1=2: k=0'\n\n* Page 7: sometimes you use k to denote the number of learned disentangled latents (e.g., the line before Eq. 5), and sometimes you use K instead (e.g., Eq. 5 and 'We then uniformly select a dimension k ∈ [K]').\n\n* Page 8: 'Kim & Mnih (2019)use' -> 'Kim & Mnih (2019) use'\n\n*** Other suggestions ***\n\n* Page 2: You could probably add refs to the corresponding sections for 'related work' and 'preliminaries'\n\n* Eq. 6: I understand from the text that you are matching the intervened latents, but this equation does not express it precisely.\n\n* Figure 2, 3: You could label the factors besides the images to make them easier to read.\n\n\nsummary_of_the_review: In summary, (1) the main selling idea of 'disentangling one factor at a time' is not new, (2) the results are worse than the state-of-the-arts, and are not sufficient to show the benefit of 'disentangling one factor at a time'. Therefore, I have to give a negative score.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: The relevant citation has been added. We apologize for the typo. \n\ncomment: We  have added more experiments, including a new dataset, and ablation studies. Many of the typos have been fixed. We have addressed in more details why we believe our claims as stated are valid. \n\n1. Page 2: 'Current state of the art methods make the assumption that there are a ﬁxed number of independent factors for all the data points in the dataset'. Note that many GAN-based approaches (e.g., InfoGAN) only regularize a small part of latents; they can indeed model 'correlated, nuisance or noisy factors' (using the non-regularized latents) besides the 'independent factors' (using the regularized latents). Please fix this statement. (I see that you do admit it later at the end of Section 3.2.)\n\n> While we agree with the above mentioned point, we believe that no GAN-based approach can actually recover the 'correlated, nuisance or noisy factors' that it has modeled from the data points. This is especially important for the downstream tasks which might rely on these correlated factors.\\textbf{ In our proposed method we actually learn these 'correlated, nuisance or noisy factors' from the data using an encoder and can recover these factors given any data for the downstream tasks.} Moreover, as mentioned in previous works [1] the posterior collapse that GANs suffer from might not ensure that all such factors are indeed modelled. Since the VAE-based method is trained by the reconstruction loss, they are constrained to model all these factors given appropriate capacity of the latent variables.\n\n2. The paper mentions in many places that the idea of progressively learning one factor at a time is new. However, a similar idea has already been proposed in 'Robust Disentanglement of a Few Factors at a Time' at NeurIPS 2020.\n\n> The idea proposed in the cited paper is different asthe  employed method cannot learn all the factors and \\textbf{they have to modify the dataset to learn the rest of the factors}. In addition, they use the $\\beta$-TCVAE to learn the disentangled factors - \\mse{ it is unclear if the real disentanglement comes from the use of the $\\beta$-TCVAE as a backbone, or if it is a result of their contributions.} Third, their method supports datasets with \\textbf{a fixed number of true generative factors of variation.} Our model architecture instead has been designed to progressively learn as many factors from real and not synthetic datasets. \n\n3. From table 1 it seems like the proposed approach performs much worse than the state-of-the-arts like FactorVAE and InfoGAN-CR.\n\n>While our method supports more effectively  datasets with countably infinite factors, we show that we perform reasonably well on most other metrics. On certain metrics the performance may not be as good but we provided a justification in the paper under the discussion section. Also the representations learned by our model outperforms the previous state-of-the-art VAE-based models on certain downstream tasks as updated in the Appendix section. \n4. If the main selling point of the paper is the ability to disentangle real-world datasets with unknown number of factors, then the paper should conduct experiments to demonstrate that (i.e., on the settings when the approaches that set a pre-defined number of factors fail, and show how your approach can improve that)}\n\n> We have partially addressed this with the CelebA experimental results, and we are currently in the process of demonstrating the effectiveness of our method in downstream tasks (for real-world datasets) for datasets other than CelebA.\n\n5. Page 2: 'related Work'\n\n> Fixed it. Thanks\n\n6. Page 3: '/mse'\n\n> Fixed it. Thanks\n\n7. Page 5: missing a period after 'factors are integrated out'\n\n> Fixed it. Thanks\n\n8. Figure 1 caption: ``so we ﬁrst group the correlated latents into one space, $z_1$\". Here $z_1$ should be $z_2$?\n\n> You are right. Thank you for pointing that out. \n\n9. Page 7: 'z_2.For brevity' -> 'z_2. For brevity'\n\n> Fixed it. Thanks\n\n10. Page 7: '\\gamma_ii = 1:K' -> '\\gamma_i i = 1:K'\n\n> Fixed it. Thanks\n\n\n11. Page 7: sometimes you use k to denote the number of learned disentangled latents (e.g., the line before Eq. 5), and sometimes you use K instead (e.g., Eq. 5 and 'We then uniformly select a dimension k ∈ [K]')\n\n> This has been fixed in the revised draft. \n\n12. Page 8: 'Kim & Mnih (2019)use' -> 'Kim & Mnih (2019) use'\n\n> Fixed it. Thanks\n\n13. Page 2: You could probably add refs to the corresponding sections for 'related work' and 'preliminaries'\n\n> This has been added in the revised draft\n\n14. Eq. 6: I understand from the text that you are matching the intervened latents, but this equation does not express it precisely.\n\n> Equation 6 has been modified. \n\nWe thank you again for your detailed response and appreciate your pointing out the typos and the corrections. \n\n[1] https://arxiv.org/abs/1905.11672\n\ncomment: 6. Also in section 3.4.2, step 1, the authors describe ...a bit more details on that.\n\n> During training, the first latent variable of the set $z_1^1$ is trained and thus $\\gamma_1$'s value is changed from 0 to 1. This latent is trained using interventions until it's KL-divergence saturates and it can encode no more information. Then, we start training the next latent variable from the set $z_1$ i.e. $z_1^2$ and the value of $\\gamma_2$ is switched from 0 to 1. The stopping criteria for this process depends on the dataset. If the dataset has a fixed number of factors which are independent like in the synthetic datasets dSprites and 3DShapes, the process of adding more latents is stopped when the KL-divergence of the entangled set, $z_2$, goes to zero and it encodes no more information. That is when we can say that all the factors of variation have been found. For real datasets, we can find continue finding factors until the desired number of factors have been found. \n\n7. Is it a goal of the proposed ... Did the authors explore that direction?\n\n> We claim that the information correlated to the disentangled factors can be encoded in $z_2$. There are no constraints that enforce a low mutual information between the two sets of latent variables.. \nThank you for the detailed feedback and questions. This will help us improve the paper with more details. \n\ncomment: Table 1 has been updated to reflect the superior performance of our algorithm on the more comprehensive and reliable metric DCI. To further solidify our results, we have now added one more dataset 3D Shapes with superior performance on DCI compared to all the rest of the methods.\n\n1. The authors of the ID-GAN paper (Lee et al: High-fidelity ... Or was the generative quality not part of their evaluation?\n\n> While there is a trade-off between the generative quality and disentangled representations with the current paradigm, \\textbf{the authors of the ID-GAN paper tackle this problem by first learning disentangled representations using a VAE-based method and then use the learned latent space as a conditioning variable for a very powerful generative model to generate high fidelity images.} In our case we posit that the reconstruction quality of the VAE is poor if the latent space is constrained to be orthogonal, which means that a lot of the factors are ignored and not encoded in the latent representation. This is especially true in the datasets with unknown number of factors of variation (such as natural datasets). \\textbf{Thus the focus of our work is to encode all the informative factors of variation which help with the downstream tasks rather than just the generative quality.} This also aids to improve disentanglement of the factors as the latent space of $z_1$ is not solely optimized by the reconstruction loss as in standard VAE-based approaches, but by interventions.\n\n2. What about model selection? The authors do not write much about how they selected their models...ablation study regarding the most important hyper-parameters.}\n\n> The ablation studies with respect to the most important hyper-parameters have been added in the appendix, and they all strongly support the components of our model.  The two steps are carried out in succession so that we don't have to explicitly select any particular model during the training phase.  [There might be a slight misunderstanding of the model, in that there is no need to select different hyper-parameters at each step during training.  We will try to make this clearer in the updated version.]\nTable 1 is updated to reflect the fact that our algorithm outperforms all the rest on the DCI metric. Detailed description is added in the appendix and discussions of the results are updated in the revised paper.\n\n3. There is no ablation on the number of learned factors of variations k...latent space size of previous works.}\n\n> In our revised draft, the ablation studies have been added. We would like to make a minor correction that we never make claim that we can find the right number of disentangled factors (in fact, no such number may be appropriate for natural datasets). Rather, as answered previously, our model has the natural stopping criterion for finding the most relevant latent factors via a measure of KL divergence.\n4. What are the motivation for another researcher to use your method?\n\nThe motivation that researchers can use our method for datasets with unknown number of factors such that the disentangled representations can be faithfully used for downstream tasks, given that the model gradually uncovers factors one at a time, as we've touched on in previous answers. Previous works on disentanglement either disentangle only a sub-set of the factors from the data (GAN-based approaches) or constraint the latent space with a fixed number of latent variables to be orthogonal thus ignoring some of the factors which could be used for the downstream tasks (VAE-based approaches). The experiments with downstream tasks have been added in the appendix which corroborate this point. \n\n5. In section 3.4.2., step 1 the authors write that the $\\gamma$ are ...thoughts on that.\n\n> By saying that we don't train the corresponding latent dimensions, we mean that we do not calculate the gradients with respect to the weights of that particular latent variable nor are they updated when their corresponding $\\gamma_i$ is zero. $\\gamma_i$ is the mask for the objective function. Thus the weights also remain untrained. We have added some details regarding this in the revised version. \n\ncomment: 5. It is not clear how the suggested formulation for the latent space is enhancing the disentitlement. I cannot see why we need to encode $z_2$ while it is expected to encode noise or sample-specific variations, which is not desirable in the latent representation learning.\n\n> The VAE-based methods are trained to maximize the data log-likelihood or minimize the reconstruction loss. The presence of noise or sample-dependent features, which can produce a significant change in the reconstruction loss, can affect the latent space encoding to account for them. Thus we constraint $z_2$ to encode such information and it is really the encoding in z1 that we are considering \"disentangled\". Encoding $z_2$ ensures that all the informative factors are encoded in the most optimal way. Since the only constraints on the latent space of $z_2$ is to be close to the prior $p(z)$ as in a standard VAE, the latent space has more capacity to encode all the informative factors needed to faithfully reconstruct the data. From this unconstrained latent space of $z_2$, we try to disentangle the factors present across all the data points into $z_1$ while maintaining the reconstruction fidelity. This is immensely useful when the data has many factors of variations such as CelebA. Separating the latent space into two sets will ensure that all the informative factors about the dataset are captured and this in turn will help with the downstream tasks from the learned latent space.\nMoreover, the previous works impose independence constraints on the latent space which can hinder disentanglement of the factors if the factors are correlated for that particular dataset as shown in Trauble et al. ICML 2021. \n\n6. While the authors claim that the model does not need to know $|z_1|$, it is not clear how the model learns the true $|z_1|$. If I am not wrong the model iteratively unmasks the dimensions of $z_1$, without any stopping criterion.\n\n> Our method is geared more towards the case where we do not know the true number of factors of variation and they can be countably infinite. The idea is to iteratively learn more factors until all factors of interest have been uncovered. For datasets with a fixed, pre-determined number of factors of variation, more and more factors are learned until the KL divergence of $z_2$, which encodes the residual factors, is zero i.e. encodes no information.\n7. The experimental study ... between these factors.\n\n> To address this question, we have added the results of another dataset 3D shapes, which corroborate our findings. In addition, we perform ablations studies for different number of $|z_1|$ and $|z_2|$ and have added the results. We find that $|z_2|$ does not have a big impact on the performance, whereas changing the number of $|z_1|$ affects how the factors  that are encoded in the different dimensions of $z_1$. For example when $|z_1|$ is large, some factors are encoded in more dimensions of $z_1$, whereas when $|z_1|$ is small, more than one factor is forced into single dimension of $z_1$. We are performing more experiments to see how to model performs if given the true number of factors.\n\n8. Additionally, I think the recent study by Träuble et al. ICML2021 ... based on the findings of this paper.\n\n> The study by Träuble et al. ICML2021, introduces correlations in datasets with a fixed number of factors of variations and then proceeds to disentangle the correlated factors using supervision. Their work in fact corroborates our findings that the current paradigm of VAE-based models with orthogonal constraints on the latent space is not sufficient to disentangle datasets with correlated factors. In our paradigm, we train the disentangled latent space $z_1$ using independent interventions, thus breaking the correlations that might be present in the factors. Our method is particularly effective in the case where the true number of factors of variation is not known and the factorization of the factors is not known. \n\n\n9. Page 7: “The value of $\\beta$ is increased linearly ... $z_2$.” \n$\\beta$ or $\\gamma$? I got confused.\n\n> A higher value of $\\beta$ ensures that the distribution of the latent space is more like the uninformative prior distribution $p(z)= \\mathcal{N} (0, I)$. Thus a higher value of $\\beta$ reduces the capacity of the latent space to encode information. So in order to encode the information in $z_1$ instead of $z_2$, the value of beta, which is the scaling factor of the KL divergence of $z_2$, is increased. \nThank you for pointing out the typos. These have been taken care of in the revised manuscript.\nWe believe that with the addition of the new experimental results and the ablation studies,  we have performed the same level of experimental analysis, i.e. ran and tested our network on the same standard disentanglement datasets, as the major disentanglement works in the field and used more evaluation metrics than prior works.\nWe thank you for your time and questions. Hope we could address some of them.\n\ncomment: 1. Originality: I think this work lacks notable originality and novelty. The VAE-GAN networks (proposed by Larsen et al., 2016) have been studied in many papers. \n\n> The novelty of this work lies in combining the VAE-GAN networks, as in the cited paper, along with the process of interventions for disentanglement. Additionally, we employ a two step process where we start with the intervened latent space and try to reconstruct it after passing it through the decoder and re-encoding it through the encoder. The discriminator in our case acts like a regularizer for the distribution of the generated intervened images to be close to the distribution to the training images while focusing on the reconstruction of the latents. \\textbf{To the best of our knowledge this process of using a VAE-GAN to learn disentangled representations in an unsupervised manner via interventions is novel.} \n\n2. The proposed formulation of latent space that splits latent factors to two disjoint sets is also not novel.\n\n> In our model, we split the latent factors into two disjoint sets, which are both learned by the encoder. \\textbf{To the best of our knowledge we have not come across any work where both the sets of continuous latent variables are learned by the encoder with different conditions imposed on them for unsupervised disentanglement.} In addition the disentangled latent set is not pre-determnined but only upper-bounded.\n3. The only novel part is assuming an upper bound for variational factor, where it is not clear how the model can find the true number of factors, while $|z_1|$ is gradually increasing up to K over iterations of training.\n\n> In addition to the novelty of our method mentiond above, the proposed work is aimed towards datasets where the true number of factors might not be known. In some datasets such as CelebA the true number of factors can be very large depending  on how you define the factors. The idea of our work is to keep trying to find new factors from the informative entangled representations. \\textbf{No previous work has yet claimed to find the \"true\" number of factors of variation in an unsupervised manner, nor do we claim that.} However, we do have a stopping criteria when the true generative factors are independent and their number is fixed. We stop increasing $|z_1|$ when the KL-divergence of $z_2$ is zero, or when the entangled latent variables encode no information. \n\n4. Quality: The motivation of the work is good; learning a disentanglement representation is quite a challenging problem...nor quality of the reconstructed images.\n\n> We performed the same standard experiments that are considered the gold standard for disentanglement papers. We did show that our model is superior in the following ways (please see updated Table and related discussions in the revised paper\n\n   1. The quality of our image reconstructions outperforms previous methods, especially for CelebA.\n   2. We learn more number of factors as compared to the previous methods as can be seen from the details in the reconstructed images and the downstream tasks in the appendix.\n   3. For the DSprites dataset, we significantly beat all the baselines on the DCI metric which is arguably the most comprehensive and reliable metric.\n   4. Our method is theoretically grounded as an extension of the wake-sleep algorithm for learning disentangled representations.\n\n\ncomment: Thank you for your questions. We have tried our best to address them below:\n\n1. Contribution wise, this paper is not the first ones to use an ``entangled+disentangled\" latent space (eg. paper named ``Toward Controlled Generation of Text\" in 2017), but the OAT learning approach is interesting.\n\n> While the mentioned paper uses an entangled+disentangled subspace, there are certain key differences with our work. First, the disentangled subspace is not learned by the encoder of the VAE but instead is set to certain values as conditional variables.  Second, and more critically, the disentangled subspace is learned  \\textit{via supervised learning mechanisms} which is necessary to learn the specific attribute discriminators. Instead, both our entangled and disentangled subspaces are learned by the encoder from the training data in a \\textit{completely unsupervised} way under different constraints on the latent space. \n\nAlso a few difference in the approaches is that the mentioned paper aims to disentangle the representation of certain specific attributes whose labels are provided, whereas we try to find new factors from the data without any supervision.\n\n2. The presentation can be improved, given so many typos, improper subscripts, equation-naming (Eq 6 or Eq 3.4.2), etc. Moreover, the central equation (Eq.6) is hard to follow, say how $q(x)$ and its KL divergence is evaluated is unknown.\n\n> Our revised draft has fixed the typos and the equation-naming problem. More explanation about Eq. 6 has been added. Moreover, some of the variables in Eq. 6 have been changed for better understanding. The Eq. 6 now reads as follows:\n\n\n $$\\mathcal{L}_2 = \\mathbb{E}_{p_{(z)}} [\\mathbb{E}_{p_\\theta (\\hat{x}^k|{z}^k)} [q_{\\phi} ({z}^{k} | \\hat{x}^k)] - \\beta \\text{KL} (p_\\theta (\\hat{x}^k|{z}^{k}) || q(x))] $$\n\nAs per equation 1, $q(x) = \\frac{1}{N} \\sum_{i=1}^N \\delta(x_i)$, which is the training data distribution. Since the data is high-dimensional, the KL divergence is not calculated explicitly but instead is replaced by its lower-bound, the Jensen Shannon divergence. This divergence is minimized by the decoder, while being trained as a generator as in the GAN paradigm, assuming a perfect discriminator.\nMoreover, both equations 5 and 6 are the central equations as mentioned on page 8 and both the steps are performed iteratively. \n\n3. In Eq 5, whether or not the $p(z_1)$ in last KL term is also factorizable.\n\n> It is factorizable since all of $z_1$ are independent and learned one after the other. $p(z_1) = \\prod_{k=1}^K p(z_1^k)$, where $p(z_1^k) = \\mathcal{N}(0,1)$, which is the standard prior used in the VAE training procedure. We have added this clarification in the revised draft. \n\nThough intervention is indeed a key idea but we are not sure about how to include it in the diagram without making the diagram confusing. We welcome any suggestions in this regard.  \n\n4. The paper needs more explanation on how to avoid posterior collapse and avoid the collapse of disentangled latent variables.\n\n> The posterior collapse of the disentangled variables is avoided in two ways. First, the process of interventions ensures that the encoder is trained to reconstruct the value of the intervened variable from the corresponding generated image. During interventions the value of one latent variable is changed randomly to a different value and both the encoder and the decoder are trained to reconstruct the value exactly, thus ensuring that the encoder learns to map the generated intervened images to their corresponding values. This in turn forces the encoder to encode some information in the disentangled set of latent variables. \n\nSecondly, the value of $\\beta$ is increased during training which restricts the encoding capacity of $z_2$ thus encoding more information in $z_1$ to be able to faithfully reconstruct the training data. These two constraints in tandem help avoid the posterior collapse of the disentangled set. \n\n\n4. The table column in Table 1 is hard to follow. From Table 1, it seems OAT model has not outperformed other baselines, right?\n\n> The table has been updated to reflect the fact that our algorithm clearly outperforms all the other baselines on the most critical metric, namely DCI, and also performs reasonably well on the other metrics. Given the narrow focus of the non-DCI metrics, we have provided an explanation in the revised paper. Basically, we cannot constrain every factor to be encoded in only one latent variable. Some more details based on ablation studies are provided in the appendix of the revised draft. Interventions ensure a smooth transition and sometimes it is more convenient to encode only a range of values of a factor in one latent variable. \n\nWe hope the revised edition addresses the stated concerns and we again thank you for your time and efforts to consider this paper. \n\ncomment: We thank the reviewers and the AC for their detailed and thoughtful feedback which will help us to improve the quality of the paper and make a much better contribution to the Disentanglement community. Based on the feedback we have made some major changes in the revised submission as follows:\n1. We have added both the quantitative and qualitative results on a new dataset (3DShapes).\n2. Section 3 (Our Method) was re-written to provide more clarity. Equation 6 has been changed to better reflect our objective function. The naming convention has been changed in an attempt to provide more clarity.\n3. A paragraph has been added to better describe the training procedure and the stopping criteria.\n4. Ablation studies have been added to assess the importance of the hyper-parameters K (cardinality of the disentangled set of latent variables) and d (dimension of the entangled set).\n5. We have added results of experiments with downstream tasks on the CelebA dataset in the appendix, which shows that our model learns more informative features.   \n\nA summary of our key contributions is as follows:\n1. Our experimental results show superior performance under arguably the most comprehensive and reliable metric, namely DCI, for dSprites and 3D shapes (new) and the reconstructed images for CelebA are the best compared to previous works (in addition to generating key factors of variation). None of the existing methods show superior performance along any metric or across several metrics while our method performs consistently well across the metrics with superior performance in the most reliable DCI metric.\n\n2. Our approach can uniquely address datasets with unknown or very large number of factors of variation as is the case for natural datasets, while ensuring that all informative factors are encoded by $z_2$ in an optimal way. Previous works on disentanglement either disentangle only a sub-set of the factors from the data (GAN-based approaches) or constraint the latent space with a fixed number of latent variables to be orthogonal thus ignoring some of the factors which could be used for the downstream tasks (VAE-based approaches). \n\n3. Ablations studies have been added which illustrate the importance of the hyperparameters and the critical roles of the two sets of latent variables.\n\nWe hope that some of these points address the big concerns with the proposed approach and we welcome any feedback with respect to the revised submission. Finally, we thank all reviewers for their time in reading this rebuttal and considering the publication of this work."
            }
        ]
    },
    {
        "id": "Ve0Wth3ptT_",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: DEGREE: Decomposition Based Explanation for Graph Neural Networks\nKeywords: XAI, GNN\nAbstract: Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks."
            },
            {
                "round2": "Reviewer 1:\nMost of my concerns have been addressed. I would like to increase my score by one point (from 5 to 6).\n\nReviewer 2:\nThe author responses clear up some of my questions. I am willing to revise my rating to 6, i.e. borderline accept. \n\nReviewer 3:\nsummary_of_the_paper: This paper proposes a decomposition-based explanations method for graph neural networks.  In detail, the authors design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes, so as to achieve the faithful explanation for GNN predictions. They demonstrate the effectiveness of the proposed method on synthetic and real-world datasets. \n\n\n\nmain_review: \nThe main concern of this paper is the decomposition assumption.  Because of the nonlinearity property in neural networks, how to decompose to the target portion and the background portion for the input features/representations.  It likely has potential interactions between the target portion and the background portion. \n\nAs for the faithfulness issue, there exist a few decomposition-based explanations methods, e.g.,  Layer-wise Relevance Propagation (LRP) [1], Excitation BP [2], and GNN-LRP [3]. Most of their methods are proposed to employ score decomposition.\n\nThe subgraph-level interpretation search algorithm has been already explored by others [4]. It would be better if authors can compare with them.\n\n[1] Explainability techniques for graph convolutional networks\n\n[2] Explainability methods for graph convolutional neural networks\n\n[3] Higher-order explanations of graph neural networks via relevant walks\n\n[4] On Explainability of Graph Neural Networks via Subgraph Explorations\n\n\nIt is not easy to understand the description in the qualitative evaluation section, especially the description of Figures 2 and 3. \nMeanwhile, it would be better if authors can compare with other explanations methods in the qualitative evaluation section.\n\n\nsummary_of_the_review: The main concern of this paper is the decomposition assumption.  Because of the nonlinearity property in neural networks, how to decompose to the target portion and the background portion for the input features/representations.  It likely has potential interactions between the target portion and the background portion. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: The paper aims at tackling the black-box nature problem of GNN by introducing a new type of explainable GNN framework called DEGREE (Decomposition based Explanation for GRaph nEural nEtworks). There are mainly two innovations. The first one lies in its ability to track contribution of components in the input graph. The second one is the algorithm for subgraph-level explanation via agglomeration. The model achieves a good compromise between performance and time efficiency.\n\nmain_review: Strengths:\nS1. The paper innovatively proposes a new explainable framework that decomposes the information into target and background portion. In contrast of previously used approximation-based, perturbation-based, additive feature attribution methods, the authors claim that the newly proposed method has the advantages of higher fidelity and node-level explainability.\nS2. Two popular GNN framework GCN and GAT are used as examples for decomposition operation, which demonstrates the methods compatibility to the current mainstream. A new subgraph construction algorithm is also proposed to tackle the problem arising during finding most important subgraphs, i.e., it’s impossible to enumerate all subgraphs.\nS3. The quantitative evaluation shows in two metrics: Explanation AUC and time efficiency. The proposed method significantly outperforms other benchmarked methods in at least one of the metrics (some benchmarked methods are only shown for Explanation AUC, not for time efficiency though), potentially making it a new choice under different use scenario. A qualitative evaluation was also included for a visual check of the result quality.\nS4. The paper is well organized and clearly written. Figures are provided with descriptive caption, including details needed for comprehension. Datasets includes both synthetic datasets and real-world datasets for diversity.\n\nWeaknesses: \nW1. The transition from absolute contribution scores to relative contribution score in Sec 4.1 is abrupt without detailed reasons. Why are relative scores preferred? Also, if they are preferred, why use absolute score in Sec 3? Providing answers to these two questions will make the logic flow more smoothly.\nW2. The exclusion of Graph-SST2 dataset from quantitative evaluation raises the question of whether the model has a poor performance on this dataset and hence has little generalization power. Providing the reasons for not including this dataset makes the benchmarking more supportive for the claimed superior model performance.\nW3. The limitation of the model was shown in the wrong prediction example in the qualitative evaluation but was not discussed in detail. What could have caused the wrong prediction and how to circumvent them?\nW4. Missing related work that should be discussed: On Explainability of Graph Neural Networks via Subgraph Explorations (ICML 2021)\n\nMinor comments:\n1. In Figure 2, I assume the shade of color means the value of the score, but it is probably a better idea to explicitly state it in the caption or have a legend in the figure.\n2. It would be preferred to include the statistics such as the number of graphs for synthetic datasets as well.\n\n\n\n\nsummary_of_the_review: The paper innovatively proposes to decompose the information flow for explainability and provides a new algorithm to construct subgraphs to reveal more complex interactions. This novel way of explaining graph component contributions give insights into a new explainable GNN framework and is potentially inspiring for other works. The experiments also show its superior performance and time efficiency compared with other methods (although some related work is missing). Thus, I am slightly positive of this paper.\n\n====Post Response====\n\nAfter reading the author response, they have indeed addressed some of my concerns. However, after also reading through the other reviews and their respective replies, I am inclined to keep the score of \"marginally above the acceptance threshold\" because I am still more positive towards the work. I have additionally increased the novelty and significance from a 2 to a 3. Thank you for providing the reply, especially the details regarding my original concern about Graph-SST2. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 5:\nsummary_of_the_paper: The paper provides DEGREE, which decomposes the feedforward propagation mechanism of a GNN to understand it. They give realistic decomposition techniques for those typically used layers in GNNs after presenting the key guidelines for developing decomposition-based explanations. They also devise an approach for providing subgraph-level explanation via agglomeration, which makes effective use of graph topology. DEGREE surpasses baselines in terms of fidelity and can capture important structures in graph data, according to experimental results.\n\nmain_review: The paper focuses on the explanation of GNN performance, which is an interesting and critical problem. The paper proposes a reasonable method for the problem to decomposite the graph. The experiments are also comprehensive. However, there are still some points required to be clarified.\n\n1,  The new scoring function proposed in the paper uses a random walk process, so will the calculation process has a high time complexity and space complexity (storing sample graphs)?\n\n2, In q · maxv′ r(v′), what is the impact of the hyper-parameter q on the subgraph expansion?\n\n3, The AUC used in the experiments should be explained in detail. For me, I have to search previous works to understand how to use it in GNN explanation issue. It is not friendly to common readers.\n\n\n\nsummary_of_the_review: The paper is easy to follow and has a reasonable method. Besides, the comprehensive experiments, especially the visualization of results, verify the effectiveness of the proposed method. I surely recommend acceptance after my above doubt is solved.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 6:\nsummary_of_the_paper: This paper proposes a decomposition-based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from drawbacks. To address the issue of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real-world datasets verify the improvements over existing works. \n\nmain_review: Strengths: \n- The problem studied is of importance and interesting. \n- The paper is generally well organized. \n- Quantitative experiments are done, which is a plus for subjective topics like XAI. \n\nWeaknesses: \n- Concepts are not defined or hard to follow. See details. \n- Qualitative experiments are hard to follow. See details. \n- Questions/unclear points regarding technical details. See details. \n- The decomposition method seems straightforward, especially the GCN decomposition (as it is largely linear). \n\nDetails. \n- Some technical concepts are not defined or hard to follow. \n    - Section 3.1. The goal of explanation is to find the most important subgraph in $\\mathcal{G}$ given $f(\\mathcal{G})$. The concept is vaguely defined. What is the concept of 'important'? Further, in the experiments, the task is actually to predict whether a node/edge is important or not. This seems to deviate from the definition. \n    - Section 4. 'We could compute the contribution score of any given node groups'. However, the notion of 'contribution score' is not defined previously. This makes this section hard to understand. \n- Qualitative experiments are hard to follow. \n    - Figure 2. What does it mean 'the process goes from left to right'?\n    - The authors claim that 'existing works suffer from adversarial triggering issues (perturbation) or inaccuracy (approximation)'. However, the claim is not supported, especially the perturbation-based drawback of unrealistic structures. I think this point can be better supported by showing examples that perturbation-based methods indeed generate such samples. At this point, I find it hard to evaluate the qualitative experiments. \n- Questions regarding technical details. \n    - Section 3.1, $f: \\mathcal{G}\\mapsto \\mathbb{R}^{|\\mathcal{V}|}$ or $\\mathbb{R}$. It seems that the formulation only works on binary classification or regression. Can this framework (respectively, DEGREE) be extended to the multi-class setting? \n    - Section 3.3 Intuition (2): ideally there should be little interaction between the target portion and the background portion. However, in graphs, this intuition can hardly be satisfied because nodes are connected to each other. How does the framework work when this intuition does not hold? Or can you provide evidence that this intuition somehow holds in real-world data? \n    - Equation 13, this equation is hard to understand. First, what does it mean $softmax(\\mathbf{X}[t])$? Second, what does it mean $\\exp(|\\mathbf{X}^\\gamma[t]|)$? Third, what does it mean to divide vectors with vectors (given that $\\mathbf{X}[t]$ is a vector)?\n\n- Minor issues. \n    - Please change the coloring of the figures such that they are visible when printed out. \n    - Section 4.2, how should we initialize $\\mathcal{E}$, $\\mathcal{S}_i$? How should we tune the parameters $I, q$? \n    - How is the definition of 'importance' in Graph-SST2? \n    - Section 5, last paragraph, 'detect non-linear interactions', what does it mean non-linear interactions? Does it mean language conjunctions such as 'if', 'but'? \n\nsummary_of_the_review: In summary, this paper has its merits. The paper tackles an interesting problem. The proposed framework is generally sound. The paper is organized well in general. However, given its unclear points in experiments and technical details, I do not recommend acceptance at this point. \n\n====Post Response====\n\nMy doubts and suggested weaknesses are addressed partly. I am willing to suggest borderline accept. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: \nDear Reviewers,\n\nWe are grateful to all reviewers for their many constructive comments and helpful feedback. We are pleased to find that they find our contribution novel (81sK), clearly written (81sK, XaeP), the experiments comprehensive (eChJ, XaeP) and the good visualization(eChJ).\n\nTo address your main concerns, we have done our best to improve our work. We have added more baselines (GNN-LRP, SubgraphX) to compare DEGREE quantitatively with their interpretation at the subgraph level, demonstrating that DEGREE is competitive among them. We added a qualitative comparison of DEGREE with SubgraphX to the MUTAG dataset, showing that DEGREE is better at detecting significant subgraphs. We have also updated the color bars in Figures 2 and 3 to improve readability. For details of the experiments, please refer to our detailed answers to these questions or to __Appendix E__ of our revised manuscript. We would like to point out that our proposed method, DEGREE, is non-additive and can therefore discover non-linear relationships between nodes.\n\nWe appreciate all the suggestions made by reviewers to improve our work. It is our pleasure to hear your feedback and we look forward to answering your follow-up questions.\n\nPaper4703 Authors\n\ncomment: Dear Reviewer XaeP ,\n\nThanks again for your valuable suggestions! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have.\n\nThank you, author\n\ncomment: Dear Reviewer eChJ ,\n\nThanks again for your valued advice! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have.\n\nThank you, author\n\ncomment: Dear Reviewer 81sK ,\n\nThanks again for your valued advice! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have.\n\nThank you, author\n\ncomment: Dear Reviewer gPGV ,\n\nThanks again for your valued advice! We have responded to your initial comments. We are looking forward to your feedback and will be happy to answer any further questions you may have.\n\nThank you, author\n\ncomment: ### Q3. Questions regarding technical details.\n#### __Q3.1 Can this framework (respectively, DEGREE) be extended to the multi-class setting?__\nYes, the contribution score is defined as the target portion of the logits of the class of interest. Thus it can be extended on the multi-class setting.\n#### __Q3.2 How can intuition hold because nodes are connected to each other?__\nGNNs use an aggregation mechanism to aggregate information between nodes. It is essentially a summation, and different GNNs have different weighting methods and objects of the summation. For example, GAT uses attention coefficient as weighting.  The summation is separable, thus the information flow in aggregation could be separable. The intuition that information aggregation is distributive in nature is also demonstrated by Eq. 4 in the Sec 3.3.\n\nIn the early stages of this work, we conducted a simple prospective experiment. We modified the MUTAG dataset by selecting half of the graphs in the dataset and randomly picking a node in these graphs and giving it a special feature value while giving the other nodes a background feature. A 3-layer GCN was trained to predict whether a graph contained a special node, and then DEGREE was used to find the node with special value; DEGREE was able to locate the special node with 100% accuracy. This simple experiment illustrates DEGREE's ability to distinguish the target portion of the information from the background portion. There is still some information that may interact with each other, and we will explore how to decouple them in future work.\n\n#### __Q3.3 Equation 13, this equation is hard to understand.__\nWe thank the reviewer for pointing out the unclearness. The notation in the paper is not appropriate. As shown in Eq. 11, to calculate the attention coefficient, we need to first calculate the pre-normalized attention coefficient between node i and node j as:\n\n$$\n\\tilde{\\alpha}_{i,j} = LeakyReLU([X_i[t]W || X_j[t]W]a)\n$$\n\nAnd we use $ \\tilde{\\alpha}_{i,j}$\nto denote a vector which consist of the pre-normalized attention coefficients between node $i$ and its neighbors. Then we calculate the normalized attention coefficient of node $i$ via $Softmax$ over its neighbors:\n\n$$\n{\\alpha}_{i} = Softmax(\\tilde{\\alpha}_i)\n$$\n\n We use $Softmax(|\\cdot|)$ to measure the dimension-wise magnitude, and let them compete for the original value. The division between two vectors is element-wise.\n\n### Q4.Minor issues.\n#### __Q4.1 Please change the coloring of the figures such that they are visible when printed out.__\nWe will update the coloring and add color bar to make it easy to read.\n#### __Q4.2 How should we initialize $\\mathcal{E}$ and $\\mathcal{S}_{i}$. How should we tune the parameters $I$, $q$.__\nAt the beginning of our algorithm, the $\\mathcal{B}_{m}$ is an empty set, and starts by measuring the contextual contribution of each individual node. We select nodes with large score  magnitude to form $\\mathcal{S}_1$. \n\nWe use $q$ to control the ‘granularity’ of the agglomeration process, i.e. the larger q will let the agglomeration choose less nodes to be merged. The process will terminate until all nodes are included, and $I$ is the number of $\\mathcal{S}$. While we can also set a maximum $I$ as the budget and stop the search early.\n\n#### __Q4.3 How is the definition of 'importance' in Graph-SST2?__\nThere is no ground truth of important words in Graph-SST2. We conducted another quantitative experiment without ground truth labels, which is shown in Q1.2.\n\n#### __Q4.4 What does it mean non-linear interactions?__\n Yes, the non-linear interactions mean language conjunctions such as ‘if’, ‘but’.\n\nWe thank the reviewers for the recognition of our merits. We hope that our response has addressed the points that are unclear. Thank you!\n\ncomment: We thank the reviewer for the thoughtful suggestions and detailed reviews. We also appreciate your recognition of the significance of our contribution. Here are our response to your concerns.\n\n### Q1. Some technical concepts are not defined or hard to follow.\n#### __Q1.1 What is the concept of 'important'? The notion of 'contribution score' was not defined previously.__\n\nThe 'contribution score' is defined as the target portion of the logit of the particular class we are interested in. It can be calculated in accordance with section 3. The larger the target portion contributed by a node group, the more important it is.\n\n#### __Q1.2 Further, in the experiments, the task is actually to predict whether a node/edge is important or not. This seems to deviate from the definition.__\n\nDEGREE has the ability to calculate the contribution score of any group of nodes. Thus, the contribution score of a node/edge can be defined as the contribution score of a special node group containing only one/two nodes. We define the contribution score of an edge as the contribution score of the group of nodes containing its two vertices. \nTo compare with the baselines methods, we use DEGREE to calculate the contribution score of node/edge and adopt AUC as metric.\n\nDEGREE can further be extended to search for important subgraph via the agglomeration method introduced in Sec 4. We additionally conduct a subgraph level quantitative experiments. We chose one of the decomposition based methods, GNN-LRP and the subgraph-level interpretation search algorithm, SubgraphX [1], to be compared with DEGREE on MUTAG dataset and Graph-SST2 dataset.  Note that we modify our method to only search nodes that boost the score. We employ ACC [2] as the evaluation metric. The ACC is calculated as below:\n- 1.The explanation method outputs the important nodes/walk/subgraph to be identified.\n- 2.Check the consistency between the predictions based on the whole graph and the explanatory subgraphs. If the prediction based on the whole graph and the explanatory subgraphs is the same, then the consistency score is 1.\n- 3.Calculate ACC as the average consistency score over testing samples. \n\nWe depict the relationship between explanation results’ ACC and Sparsity. The Sparsity is the ratio of the size of the explanation subgraph to the original graph. The higher the ACC, the better the interpretation. We can see that DEGREE and SubgraphX have comparable performance. Here are the Anonymous link to the comparison figures.\n\nComparison on MUTAG dataset. [ACC_MUTAG](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_MUTAG.png).\\\nComparison on Graph-SST2 dataset. [ACC_SST](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_SST.png).\n\n[1] Yuan, Hao, et al. \"On explainability of graph neural networks via subgraph explorations.\" arXiv preprint arXiv:2102.05152 (2021).\\\n[2] Liang, Jian, et al. \"Adversarial infidelity learning for model interpretation.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\n### Q2. Qualitative experiments are hard to follow.\n#### __Q2.1 What does it mean 'the process goes from left to right'?__\nFigure 2 shows the process of the agglomeration (introduced in Sec 4) step by step from left to right. The leftmost one shows the contribution score of individual nodes, while the rightmost one shows the contribution score of the whole graph.\n#### __Q2.2 The perturbation-based drawback of unrealistic structures.__\nPerturbation-based methods can introduce artefacts (unrealistic structures) that lead to unreliable interpretations [1] as the perturbed data becomes inconsistent with the original dataset [2]. Take this figure from the MUTAG dataset as an example.\n\nThe Anonymous link to the MUTAG example. (https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/3369.png)\n\nThe original prediction is mutagenic. If we remove node 0 and node 2 from the graph and get a perturbed graph (artefact) that could not exist in nature, the prediction becomes non-mutagenic. However, the node 0 and node 2 are not in N-O or N-H (the ground truth we have). This means that the artefacts introduced affect the quality of the interpretation.\n\n[1] Fong, Ruth C., and Andrea Vedaldi. \"Interpretable explanations of black boxes by meaningful perturbation.\" Proceedings of the IEEE international conference on computer vision. 2017.\\\n[2] Qiu, Luyu, et al. \"Resisting out-of-distribution data problem in perturbation of xai.\" arXiv preprint arXiv:2107.14000 (2021).\n\n\n\n\ncomment: We thank the reviewer for the thoughtful suggestions and detailed reviews. We also appreciate your recognition of the significance of our contribution. Here are our response to your concerns.\n\n### Q1. Will the calculation process have a high time complexity and space complexity (storing sample graphs)?\n\nWe control the sampling process by adjusting its sampling number and the maximum depth. Note that we set the maximum depth to the number of layers of the GNN, as the GNN only aggregates information within the L-hop neighborhood, where L is the number of layers of the GNN. The number of samples taken is within reasonable limits and will not have a high time complexity because the size of  graph in MUTAG and Graph-SST2 is not extremely large. We do not store the intermediate sampling results. We thank the reviewers for pointing out this direction of improvement, as it will help to reduce the sampling time when DEGREE is applied to larger graph datasets than Graph-SST2 and MUTAG.\n\n### Q2. What is the impact of the hyper-parameter q on the subgraph expansion?\n\nHere q is the threshold for node selection, which will affect the 'granularity' of the agglomeration process, i.e. a larger q will allow agglomeration to select fewer nodes for merging in each step. Thus, q will also affect the agglomeration efficiency. The influence of q is shown in Figure 7 in Appendix.\n\n### Q3. The AUC used in the experiments should be explained in detail.\n\nWe thank the reviewers for pointing out our carelessness. A more detailed name for AUC is AUROC (Area Under Receiver Operating Characteristics). It is a performance measure for classification problems and tells how well the model can distinguish between categories. For consistency with the baseline, we used node-level AUC for the synthetic dataset and edge-level AUC for the MUTAG dataset.\n\nHope our response can address your concerns. Thank you.\n\n\n\n\ncomment: We thank the reviewer for the thoughtful suggestions and detailed reviews. We also appreciate your recognition of the significance of our contribution.  Here are our responses to your comments. \n### Q1. Why are relative scores preferred? Also, if they are preferred, why use absolute score in Sec 3?\n\nThe relative score is preferred because a node's score should faithfully reflect its contribution in various situations. It can also be seen as an approximation to the Shapley Value. Take the sentence in the second row of Figure 3 in our paper as an example. The absolute score for the word \"movie\" is 0.13, while its relative score is -0.04. Since \"movie\" itself is a very neutral word, the relative score is better (closer to 0).\n\nThanks to the reviewer for pointing out the unclearness. Sec 3 introduces the principles to decompose GNN layers. We use relative score for all experiments.\n\n### Q2. The exclusion of Graph-SST2 dataset from quantitative evaluation.\n\nTo be consistent with related work, we use the area under the curve (AUC) as a metric. Similar to the accuracy metric in classification tasks, the AUC metric requires ground truth labels. Synthetic and MUTAG datasets naturally have ground truth labels, i.e., which nodes are important. For the synthetic dataset, we consider the nodes within the pattern to be important. For the MUTAG dataset, the \"N-H\" and \"N-O\" edges are important. However, Graph-SST2 is a sentiment graph dataset, and each graph is transformed from sentences using the Biaffine parser. Unfortunately, there are no ground truth labels about which nodes (words) are important. Therefore, we did not include the Graph-SST2 dataset in our quantitative experiments.\n\nTherefore, we conducted another quantitative experiment without ground truth labels. We chose one of the decomposition based methods, GNN-LRP and the subgraph-level interpretation search algorithm, SubgraphX, to be compared with DEGREE on MUTAG dataset and Graph-SST2 dataset.  Note that we modify our method to only search nodes that boost the score. We employ ACC [1] as the evaluation metric. The ACC is calculated as below:\n- 1.The explanation method outputs the important nodes/walk/subgraph to be identified.\n- 2.Check the consistency between the predictions based on the whole graph and the explanatory subgraphs. If the prediction based on the whole graph and the explanatory subgraphs is the same, then the consistency score is 1.\n- 3.Calculate ACC as the average consistency score over testing samples. \n\nWe depict the relationship between explanation results’ ACC and Sparsity. The Sparsity is the ratio of the size of the explanation subgraph to the original graph. The higher the ACC, the better the interpretation. We can see that DEGREE and SubgraphX have comparable performance. Here are the Anonymous link to the comparison figures.\n\nComparison on MUTAG dataset. [ACC_MUTAG](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_MUTAG.png).\\\nComparison on Graph-SST2 dataset. [ACC_SST](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_SST.png).\n\n[1] Liang, Jian, et al. \"Adversarial infidelity learning for model interpretation.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\n### Q3. What could have caused the wrong prediction and how to circumvent them?\n\nThis is a very good question. From our observations of incorrect predictions, we have empirically concluded that there are two reasons for the errors. The first possible reason is shown by the second row in Figure 2. The overhanging sulphuric acid (the blue cross-shaped ion in the middle) shows a strong non-mutagenic effect, which ultimately leads to the wrong prediction. A second possible cause is evidenced by the third row in Figure 2. Most of the non-mutagenic samples contained methyl (CH3), so the model may have incorrectly derived strong evidence for non-mutagenicity. In our appendix, the second row of Fig. 6 shows the incorrect prediction due to cause 1, while the first, third and fourth rows of Figure 6 show the incorrect prediction due to cause 2.\n\nHow to circumvent them is also a good question. The output interpretation (contribution score) is derivable thanks to the feed-forward mechanism of DEGREE. This means that we can calibrate the behavior of the model by adding appropriate regularization terms to inject reasonable (ground truth knowledge). This is an interesting and worthwhile direction to explore in the future. \n\n### Q4. Missing work that should be discussed: On Explainability of Graph Neural Networks via Subgraph Explorations (ICML 2021)\n\nWe add comparison quantitative experiment in Q2.\n\n### Q5. Minor comments\n\nWe will update the color bar and legend as the reviewer suggests. The statistics of the datasets are summarized in Table 2 in the Appendix due to the page limitation. There is only one graph in each synthetic dataset.\n\n\n\ncomment: We thank the reviewer for the detailed reviews. Here are our responses to your comments. \\\n### Q1. How to decompose to the target portion and the background portion for the input features/representations.\nWe agree with the reviewer that it is nontrivial to decompose the target portion and the background portion due to the nonlinearity. As shown in Eq. 10, we adopt the telescoping sum as the linearization technique [1]. It can not completely split the target portion and background portion as it can be viewed as an approximation to Shapley value. The experimental results demonstrated the rationality of our approach.\nOur interpretation process is bottom-up, i.e., the decomposed parts (relevant and irrelevant) in a lower layer flow into the higher layer, and can still be kept as decomposed. If we could distinguish between target and background parts in a lower layer, then we could do the same in a higher layer, towards the output layer. \nWe also conducted a simple forward-looking experiment in the early stages of this work. We modified the MUTAG dataset by selecting half of the graphs in the dataset and picking a random node in each graph and giving it a special feature value, while giving the other nodes a background feature. A 3-layer GCN is trained to predict whether a graph contains special nodes, and DEGREE is used to find special nodes; DEGREE can locate special nodes with 100% accuracy. This simple experiment can show the ability of DEGREE to distinguish the target portion information from the background portion.\n\n[1] Murdoch, W. James, Peter J. Liu, and Bin Yu. \"Beyond word importance: Contextual decomposition to extract interactions from LSTMs.\" arXiv preprint arXiv:1801.05453 (2018).\n\n### Q2. Compare with GNN-LRP and Subgraphx [2].\nWe thank the reviewers for pointing out the existence of a number of decomposition-based interpretation methods. DEGREE differs from these methods in two main ways. Firstly, DEGREE is a forward decomposition method, whereas LRP, Excitation BP and GNN-LRP are backward decomposition methods. Secondly, DEGREE can reveal the importance of a set of nodes, whereas the backward decomposition method can only show the importance of individual nodes. It is worth noting that by using DEGREE, the importance of a group of nodes is not the sum of the importance of the nodes in that group. Therefore, DEGREE also has the ability to explore sub-graph level interpretations.\n\nWe chose one of the decomposition based methods, GNN-LRP and the subgraph-level interpretation search algorithm, SubgraphX, to be compared with DEGREE on MUTAG dataset and Graph-SST2 dataset.  Note that we modify our method to only search nodes that boost the score. We employ ACC [3] as the evaluation metric. The ACC is calculated as below:\n- 1.The explanation method outputs the important nodes/walk/subgraph to be identified.\n- 2.Check the consistency between the predictions based on the whole graph and the explanatory subgraphs. If the prediction based on the whole graph and the explanatory subgraphs is the same, then the consistency score is 1.\n- 3.Calculate ACC as the average consistency score over testing samples. \n\nWe depict the relationship between explanation results’ ACC and Sparsity. The Sparsity is the ratio of the size of the explanation subgraph to the original graph. The higher the ACC, the better the interpretation. We can see that DEGREE and SubgraphX have comparable performance. Here are the Anonymous link to the comparison figures.\n\nComparison on MUTAG dataset. [ACC_MUTAG](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_MUTAG.png).\\\nComparison on Graph-SST2 dataset. [ACC_SST](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/ACC_SST.png).\n\n[2] Yuan, Hao, et al. \"On explainability of graph neural networks via subgraph explorations.\" arXiv preprint arXiv:2102.05152 (2021).\\\n[3] Liang, Jian, et al. \"Adversarial infidelity learning for model interpretation.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n### Q3.  Qualitative evaluation comparison with SubgraphX.\nWe report some qualitative results of SubgraphX. The important edges are in bold. And the important edges selected by SubgraphX fail to include the N-H / N-O (the ground truth label). \n\nTwo SubgraphX explanation examples on MUTAG dataset. [SubgraphX_10](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/subgraphx_10.png).\n[SubgraphX_78](https://anonymous.4open.science/r/ICLR_Rebuttal_FIG-1942/subgraphx_78.png).\n\nWhereas DEGREE can locate them as shown in Figure 2 in our paper.\n\nHope this will address your concerns.\n"
            }
        ]
    },
    {
        "id": "nnU3IUMJmN",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Capturing Structural Locality in Non-parametric Language Models\nKeywords: No keywords\nAbstract: Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improving performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.\n"
            },
            {
                "round2": "Reviewer 1:\nBased on discussion with authors and clarifications in comments and paper itself, I have changed my score to \"marginally above acceptance threshold\" and noted that in my review.\n\nReviewer 2:\nI have gone through the reviews by other reviewers, the responses by the authors and the revised submission.\n\nI thank the authors for implementing the following suggestion made by me in my original review:\n\"It appears that w doesn't matter so much for all non l_0 features. So it would be interesting to set w to 1 for these features and learn only b and for l_0 learn w.\"\nThe experimental study done by the authors in the context of this suggestion seem to confirm my hypothesis. One positive implication of this is that fewer parameters need to be learned while not sacrificing the gains (section A.3). Further, the weights seem to be more interpretable and can provide a heuristic for determining the relative importance of different locality features. \n\nAs noted in my original review, there are several positives in this submission and I continue to be appreciative of them. The submission is well written and the engagement of the authors to the reviews has been positive. The main drawback of this work is that it doesn't provide a principled procedure or heuristics for formulating effective locality features for different datasets. The task of formulating the locality features remain outside the scope of the current work and needs domain knowledge. This limits the potential impact of this work as also noted by other reviewers. All that can be concluded from the experimental study is that the carefully designed locality features seem to give minor improvement over language models that don't use such features. \n\nI'm willing to upgrade my recommendation to marginally above acceptance threshold.\n\n\n\nReviewer 3:\nYes, this would clarify the contribution and differences from the past work on structural locality.\n\nA comment for future work: as couple reviewers have observed, structural locality may have to be defined differently for different domains. It might be interesting to look wider in which domains and what structural locality metrics are useful (i.e. not captured in simple distance metrics) and which ones are already captured in distance. Also what generalizable definitions of structural locality and its metrics can be created. Looking even wider, structural locality should be applicable to non language domains, for example, image retrieval where structural locality could be similar to proposed here (images from single user or single album to be closer vs others) or possibly different. Not something for this paper, but in case there is interest to pursue this further.\n\nThank you.\n\nReviewer 4:\nThank you for your extensive comments.\nRegarding the novelty of the structural locality: I thought that the paper said that the structural locality properties were known. From your response, it seems that you are saying that structural locality was not known or not used before. Is this the case? If you claim novelty of the structural locality concept and/or its expression for Wikipedia and Github domains, then you should make this explicit. If not, then I think my original comment stands.\nRegarding how the structural locality is added to the model, it seems to me that it is straightforward to think about structural locality as an extra distance metric. I agree that learning that additional function is somewhat novel and that comparing to alternative ways of adding it is useful.\n\nConsidering all information, I am OK if this paper is accepted to the conference.\n\nThank you.\n\nReviewer 5:\nThank you for implementing my suggestion to fix all “w” except for w0 to 1 and optimize only w0 and bias terms b_n\". This highlights the relative importance of different locality functions in a much better way. \n\nReviewer 6:\nI thank the authors for responding to my comments. I also thank the authors for including a few more details on the experiments' side. I am convinced that the datasets used by the authors are sufficient enough to conduct a reliable study. My original comment mainly revolved around the fact that there are other interesting use cases, but I do agree that not everything could be done in a limited-page paper.\n\nSince the authors have addressed my comments convincingly, I am tempted to change my recommendation score.\n\nReviewer 7:\nsummary_of_the_paper: This work concerns itself about utilizing structural locality inherent in real-world datasets in improving the effectiveness of non-parametric language models. It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness. It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non-parametric language model with the help of learnable parameters. \n\n\nmain_review: This work concerns itself about utilizing structural locality inherent in real-world datasets in improving the effectiveness of non-parametric language models. It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness. It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non-parametric language model with the help of learnable parameters. \n\n\nPositives:\n\t1. Well stated hypothesis and analysis that shows that structural locality is not implicitly fully captured by the distance metric used in K-nearest neighbour non-parametric language model of Khandelwal et al.\n\t2. Locality features for two datasets - wikipedia and Java projects\n\t3. Incorporation of locality features in non-parametric language models using simple learnable functions of the distance metric\n\t4. Analysis that shows that incorporating locality features leads to improved distance distribution among nearest neighbours\n\nNegatives:\n\t1. Structural locality inherent in datasets need to be captured by a set of custom locality feature functions which requires prior knowledge of the domain of the datasets. \n\t2. Marginal improvements in results\n\t3. No detailed discussion of learned parameters presented in Table 3.\n\t\ta. It appears that w doesn't matter so much for all non l_0 features. So it would be interesting to set w to 1 for these features and learn only b and for l_0 learn w. \n\nIn Eqn 4. y should be w_t?\n\n\n\nsummary_of_the_review: The paper makes an interesting hypothesis and goes about validating the hypothesis. However, the improvements due to the proposed method are marginal.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 8:\nsummary_of_the_paper: The paper is about modelling structural locality in non-parametric language models. The key hypothesis is in modelling not only the co-occurrence characteristics but also structural characteristics such as locality. The paper explains the key claims via case studies conducted on source code data and Wikipedia datasets.\n\nThe model paradigm is based on non-parametric language models. A key difference between the non-parametric model and the parametric counterpart is that in the non-parametric model the model parameters are not only determined by the model architecture but also the underlying data.\n\nStructural locality, which is different from just co-occurrence counts, models the structural relationships between pairs of items, e.g., whether they belong to the same or different directory in the case of source code.\n\nThe optimisation model is presented in Equation 7 where the authors need a small sample set emanating from the same domain to train the model. The authors then conduct experiments to demonstrate that the method improves upon existing works. Both qualitative and quantitative experimental results are shown.\n\nmain_review: The paper models structural information into the non-parametric language models. While the results demonstrate that the method improves upon the existing methods, there are some weaknesses too.\n\nStrengths:\n1. The model or the loss function developed by the authors that incorporate structural information is novel. The authors have also clearly explained the model.\n2. Results demonstrate that the method improves upon existing methods.\n\nWeaknesses:\n1. While there are clear strengths, one weakness is that one may need to define structural properties in different types of datasets that one might use. For instance, it is clear that the model works for source codes and Wikipedia because associated structural information can be mined from the data. It is unclear how does the method generalise across different tasks and datasets, i.e., beyond two datasets. While the authors have addressed these limitations towards the end of the paper, the question is will the work be useful only to a small set of audience, or people from different domains can manually or automatically build such prior knowledge and incorporate it in this model. The key advantages are clear from the paper, this seems to be the weakness that is hard to defend. One possible way to improve the argument so that we could obtain Wikidata-type structure for most datasets is to exploit entity detection and linking including automatically learning their relation (vector) information in a completely unsupervised way. The authors must note that I am simply giving ideas on how to strongly defend this weakness of the model.\n2. In terms of experiments, these can be further improved by conducting some downstream application tasks. Can the model be useful for document classification tasks? Currently, it is very difficult to gauge the usefulness of the model through the limited experiments presented in the paper which mainly revolves around perplexity analysis and table 5 in the appendix has additional token prediction results.\n\n\nsummary_of_the_review: Overall, the paper indeed has some merits. The paper can be made stronger by considering some comments mentioned above.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nReviewer 9:\nsummary_of_the_paper: The authors propose an approach to complement context by adding 'locality' information in examples present in external stores of non-parametric language models. The locality information captures the hierarchical structure, and deems two contexts more similar (or having less distance) if they share common hierarchical structure/attributes. The authors conduct experiments on source code as well as natural language articles. They analyze the results to point out reasons for improvement, and also highlight differences between the domains.\n\nmain_review: + The paper is well written and easy to read\n+ What the paper is suggesting is simple, yet useful. The example depicted in Figure 1 describes their motivation as well as what they are doing effectively\n+The concept of structural locality as used in their paper is defined clearly\n+Results are compared with other state of the art models (Table 2)\n\n- The only weakness that I can identify is that the authors have used one dataset of each domain. Since there is minor improvement in the results, it is difficult to gain confidence that the results will indeed improve on other datasets as well\n\n\n\n\n\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 10:\nsummary_of_the_paper: Authors propose a way to use structural locality in language models. Authors demonstrate that the structural locality information improves results for two domains they experiment with. \n\nmain_review: Strengths:\n- Authors propose a way to include structural locality into kNN-LM models\n- Authors demonstrate improvement on retrieval tasks for Wikipedia and Java source code domains\n\nWeaknesses:\n- It seems that the structural locality and its value are known and not new. The paper's contribution then is adding the structural locality to the non-parametric language models. The way to add structural locality is straightforward. It seems to me that this contribution is not significantly novel.\n- There is no comparison or discussion of other ways to add structural locality to LMs. It seems authors tried just a single approach that they present in the paper.\n- Using locality info in authors' experiments leads to minor improvement. This is not a significant weakness since there might be other tasks where locality will contribute more. However, it would have been interesting to see tasks or domains where improvement is more significant.\n\nBased on discussion with authors and clarifications provided in comments and paper itself, I have increased my score to \"marginally above acceptance threshold\".\n\nsummary_of_the_review: In my opinion, the paper makes a minor contribution by proposing a straightforward way to add structural locality to kNN-LM models. The contribution is possibly useful, but not major. For tested tasks, structural locality information improves results but not significantly. I believe the paper is marginally below acceptance threshold.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: I thank you for the prompt reply and considerations regarding our revision and responses. I am glad that the conversation clears things up and the revisions improved the paper. I am also appreciative of your other comments and will seriously consider them as future work.\n\n\n\ncomment: I thank you for the prompt reply and considerations regarding our revision and responses. I am glad that the conversation clears things up and the revisions improved the paper. I am also appreciative of your other additional suggestions outside the scope of this paper and will seriously consider them as future work.\n\ncomment: Thank you for your feedback. We have incorporated this discussion in the paper in a new footnote (footnote 9) in the “Conclusion” section and direct the reader to more details presented in the newly added Appendix A.4, which explains the relation to previous work and our core novelties in more detail. We hope that these changes improve the quality of the paper. Please let us know of any further suggestions for clarifications that would improve your view of our work.\n\nRegarding your comment on future work: we appreciate your ideas of these future studies. The idea of locality metrics that go beyond modifying distance is quite interesting. This echoes our own position of the paper's implications as well; for instance as mentioned in our \"Limitations\" paragraph at the very end of the paper, we think it would be interesting to see how this work performs on even more domains.\n\n\n\ncomment: Following the discussion with Reviewer 7oBt, we have incorporated a more detailed discussion, which explains the relation to previous work and our core novelties in more detail, in footnote 9 in the “Conclusion” section and direct the reader to more details presented in the newly added Appendix A.4. We hope that these changes improve the quality of the paper. Please let us know of any further suggestions for clarifications that would improve your view of our work.\n\ncomment: Following the discussion with Reviewer 7oBt, we have incorporated a more detailed discussion, which explains the relation to previous work and our core novelties in more detail, in footnote 9 in the “Conclusion” section and direct the reader to more details presented in the newly added Appendix A.4. We hope that these changes improve the quality of the paper.\nPlease let us know of any further suggestions for clarifications that would improve your view of our work.\n\ncomment: Following the discussion with Reviewer 7oBt, we have incorporated a more detailed discussion, which explains the relation to previous work and our core novelties in more detail, in footnote 9 in the “Conclusion” section and direct the reader to more details presented in the newly added Appendix A.4. We hope that these changes improve the quality of the paper.\n\ncomment: Thank you for your feedback. We have incorporated this discussion in the paper in a new footnote (footnote 9) in the “Conclusion” section and direct the reader to more details presented in the newly added Appendix A.4, which explains the relation to previous work and our core novelties in more detail. We hope that these changes improve the quality of the paper. Please let us know of any further suggestions for clarifications that would improve your view of our work.\n\nRegarding your comment on future work: we appreciate your ideas of these future studies. The idea of locality metrics that go beyond modifying distance is quite interesting. This echoes our own position of the paper's implications as well; for instance as mentioned in our \"Limitations\" paragraph at the very end of the paper, we think it would be interesting to see how this work performs on even more domains.\n\n\ncomment: Thank you for the follow-up; we understand the confusion now. Previous work (Hellendoorn and Devanbu, 2017) did make the observation that source code files from the same GitHub repository or subdirectory tend to be relatively similar, but did not include an empirical analysis of this effect; rather, they showed that this is useful for the performance of cache-based (n-gram) language models. We (1) directly examine the internal representations of a neural language model, (2) demonstrate that the internal representations do not sufficiently capture structural locality features, (3) demonstrate ways to compensate for this disconnect, leading to improved language modeling performance, and (4) also demonstrate that this carries over to Wikipedia, which has not been previously examined in this way. This both gives more fine-grained insights and expands the applicability to neural models and new domains. In addition, our work proposes a more generalized formulation for encoding *multiple* localities across multiple domains. Previous work treats locality as strictly nested (e.g. project->subdirectory) while ours can encode more general hierarchies such as the lattice we used in the Wikipedia case (e.g. same category - yes/no combined with same section - yes/no). \n\nIf this explanation seems clear we can revise the paper to more clearly reflect this discussion of the difference between our work and the previous work. Thank you again.\n\n\ncomment: Thank you for the quick reply and for your suggestion. In case you missed it, we have also uploaded the revised paper to incorporate your suggestions in the appendix while mentioning the effects and alternatives in the main text due to page limits. Happy to revise again in case things are not clear in the revised version of the paper.\n\ncomment: I thank you for the prompt reply and considerations regarding our revision and responses. I am glad that the conversation clears things up.\n\ncomment: We thank the reviewer for the comments. We have updated the paper to include some ablation studies regarding alternative ways to learn weights for locality features and expanded discussions with empirical results and analysis in Section 6.2 and Appendix A.3. We will address your concerns below.\n\n\n**Novelty**\n\nWith respect, we strongly disagree with the statement that the insights provided by this paper are already known in the literature. For example, we believe the analysis of the effect of such locality features in language modeling tasks in Figures 2 and 3 are non-trivial, and we personally learned a lot from this, despite being quite familiar with language modeling in general and non-parametric language models in particular. We also feel that the empirical performance comparisons are of significant value. These methods verify that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness, which c) achieves a new state-of-the-art model for code modeling by a large margin. There was a mention in the review that “structural locality and its value are known and not new.” We were not exactly sure which previous results this was referring to, but to contrast with some other related work: Tu et. al. 2014 only utilizes immediate local context before the predicted tokens, not the structural locality we handle in this work, and it is not clear how these results could be directly applied to neural language models. Hellendoorn & Devanbu 2017 apply similar local context to neural language models for code, but again do not discuss structural locality as we define it. In the text domain, some adaptive LMs (Grave et al., 2016; 2017) use previous hidden states to help predict the next token, but they did not explicitly identify and incorporate locality features. Also, none had analyzed their necessity and utility to non-parametric retrieval-based models. We would be happy to elaborate further if the reviewer could provide any references referring to where these insights have been reported previously.\n\n**Discussion of other ways to add structural locality**\n\nThank you for the comment, we agree that the results would be stronger with more complete ablations of the different methods with which to incorporate structural locality. We actually had already performed these experiments before submission and discussed in the paper about several alternative ways we attempted to add locality features into the model (Section 5, contextualizing the weights and biases for locality features on current context vector) but ended up not reporting them in detail as they empirically performed worse than the proposed formulation. We revised the paper to include these results in Appendix A.3 for reference.\n\n\n**Minor improvement**\n\nIn our opinion, a consistent 3-15% relative error reduction over the very strong KNN-LM baseline across multiple metrics over two datasets from different domains is not so minor, but we realize that subjective opinions may differ here. In addition, it is worth mentioning that our results improve the state-of-the-art on the Java Github code modeling corpus by a large margin; retrieval-based language models had not previously been used in that domain.  But perhaps more importantly (and as also mentioned above), these improvements are founded on both a careful analysis of the workings of this model and involve just a few added parameters that are also highly interpretable, and thus the encouraging results here may spur methods that tackle a similar problem in a different way, resulting in further improvements as well.\n\n\n\ncomment: We thank the reviewer for the comments and for the appreciation of our work! We have updated the paper to include some ablation studies regarding alternative ways to learn weights for locality features and expanded discussions with empirical results and analysis in Section 6.2 and Appendix A.3. We will address your concerns below.\n\n**Only one dataset of each domain**\n\nWe understand the potential concern, but at the same time think that consistent results across both English and Java Source Code demonstrate a relatively high level of generality already. We would be happy to have any concrete suggestions for other datasets that would be most appropriate to test on though.\n\n\ncomment: We thank the reviewer for the detailed comments! We have updated the paper to include some ablation studies regarding alternative ways to learn weights for locality features and expanded discussions with empirical results and analysis in Section 6.2 and Appendix A.3. We will address your concerns below.\n\n**The need to define structural properties**\n\nWe would like to first state that both Wikipedia and GitHub are huge and popular datasets already, and the recent GitHub CoPilot/OpenAI Codex is also trained with a plethora of data from Github repositories. In addition, there is a lot of data in the real world that directly follows the same format as Wikipedia (all other Wiki-type/structured documents) and Github (all source code repositories). \n\nFurther, we agree that it would be ideal if a language model was able to learn how to capture structural locality without explicit feature design. However, one major empirical result of our study is that embeddings of current models do *not* perfectly capture structural locality, as Figures 2 and 3 demonstrate. We argue that this is not a weakness of our method, but rather a weakness of the baseline method, and pointing out this weakness is one of the contributions of our paper. The specific method we employed to mitigate this problem in the current study requires custom locality features, but on a higher level we believe our work demonstrates the importance of considering the structural properties of datasets in non-parametric language models, and we hope that it will also spur future work on alternative methods to tackle this problem as well.  \n\nLast but not the least, we greatly appreciate your suggested idea of using Wikidata information to do entity detection for other contexts. We think it’s clever and we would like to consider this as future work, but it might be tough to fit in the current paper given the space limitations. One potential concern is that these methods may be prone to error propagation given that the entity linking may not be perfect in many contexts, but mitigation strategies for this could certainly be devised as well. In this paper, we mainly focus on “user-defined” prior knowledge that could be obtained directly through user input or readily available metadata for datasets, which already covers a large variety of domains.\n\n**Downstream application tasks**\n\nThank you for the comment! We actually did include one downstream task metric, predictive accuracy (at 1 and at 5), which is the standard evaluation metric for code completion [1, 2, 3] and predictive text entry [4]. Upon a second reading of the paper we realize this was not very clear, so we have revised the paper to reflect this. Other downstream tasks such as document classification would also be excellent to add, but as far as we know using these tasks to evaluate non-parametric language models such as KNN-LM is not standard in the literature, and we don’t have a very clear idea how we could do so. Any suggestions on an appropriate experimental setup would be welcome!\n\n[1] Raychev, V., Vechev, M., & Yahav, E. (2014, June). Code completion with statistical language models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (pp. 419-428).\n\n[2] Alon, U., Sadaka, R., Levy, O., & Yahav, E. (2020, November). Structural language models of code. In International Conference on Machine Learning (pp. 245-256). PMLR.\n\n[3] Karampatsis, Rafael-Michael, et al. \"Big code!= big vocabulary: Open-vocabulary models for source code.\" 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEE, 2020.\n\n[4] Stocky, T., Faaborg, A., & Lieberman, H. (2004, April). A commonsense approach to predictive text entry. In CHI'04 Extended Abstracts on Human Factors in Computing Systems (pp. 1163-1166).\n\n\ncomment: We thank the reviewer for the detailed comments! We have updated the paper to include some ablation studies regarding alternative ways to learn weights for locality features and expanded discussions with empirical results and analysis in Section 6.2 and Appendix A.3. We will address your concerns below.\n\n**Structural locality needs to be captured by a set of custom locality feature functions which requires prior knowledge of the domain of the datasets**\n\nWe agree that it would be ideal if a language model was able to learn how to capture structural locality without explicit feature design. However, one major empirical result of our study is that embeddings of current models do *not* perfectly capture structural locality, as Figures 2 and 3 demonstrate. We argue that this is not a weakness of our method, but rather a weakness of the baseline method, and pointing out this weakness is one of the empirical contributions of our paper. The specific method we employ to rectify this weakness in the current study requires custom locality features, but on a higher level we believe our work demonstrates the importance of considering the structural properties of datasets in non-parametric language models, and we hope that it will also spur future work on alternative methods to tackle this problem as well.  \n\n**Marginal improvements in results**\n\nIn our opinion, a consistent 3-15% relative error reduction over the very strong KNN-LM baseline across multiple metrics over two datasets from different domains is not “marginal”, but we realize that subjective opinions may differ here. In addition, it is worth mentioning that our results improve the state-of-the-art on the Java Github code modeling corpus by a large margin; retrieval-based language models have not been previously used in that domain.  But perhaps more importantly (and as also mentioned above), these improvements are founded on both a careful analysis of the workings of this model and involve just a few added parameters that are also highly interpretable. Thus, the encouraging results here may spur methods that tackle a similar problem in a different way, resulting in further improvements as well.\n\n**Discussion of learned parameters presented in Table 3**\n\nThank you for the comment! We agree that a more complete discussion of this is useful.\nIn the original submitted paper, there was already a discussion in Section 6.2 regarding the learned parameters, “the biases vary strongly, as the model tries to “correct” the original distance by emphasizing more local contexts”. The effect of the learned parameters was also illustrated by comparing the “distance” before and after taking into account the weighted locality features, in Figure 3, which we described as “Compared with the original negative distance…, the negative modified distance is more separated between the different locality levels on either dataset, showing the relative importance of different locality more clearly.”\nTo dig a bit deeper than what was included in the originally submitted version, we expanded the discussion in Section 6.2 and Appendix A.3 of the revised paper, specifically following the suggestion to fix all “w” except for w0 to 1 and optimize only w0 and bias terms b_n. We found that for the Wiki dataset, the learned parameters were w0 = 1.1267 and b1 = -0.3851, b2 = -0.4745, b3=-0.7261, which is in line with the discussion made in the paper where the bias varies to modify the “distance” with different levels of locality. The final perplexity on the test set was slightly worse than the full setting in the paper (19.33 vs 19.16).\nFor the Java dataset, we observed a similar effect: the learned parameters were w0 = 0.9017 and b1 = -28.7163, b2 = -55.4277. It’s worth mentioning that these biases are way larger in amplitude than before, to compensate for the small weights learned originally (~0.03). The perplexity is also slightly worse: 2.15 vs. 2.13.\n\n**Typo**\n\nThanks for pointing out the typo in Eq. 4 and we have corrected this in the revised paper.\n\n\ncomment: We thank the reviewers for their insightful feedback and appreciating our work! We have updated the paper to include some ablation studies regarding alternative ways to learn weights for locality features and expanded discussions with empirical results and analysis in Section 6.2 and Appendix A.3.\n\nIn more detail, we first addressed Reviewer PnXs’s comment on an alternative way to formulate the parameter learning for locality features, and included results in Appendix A.3. We found that this led to similar observations with respect to the learned parameters but found that it caused slightly worse model performance.\n\nSecondly, we addressed Reviewer 7oBt’s suggestion regarding having different methods with which to incorporate structural locality by adding complete results of alternatives we tried but ended up not using because they empirically performed worse than the proposed formulation, also in Appendix A.3.\n"
            }
        ]
    },
    {
        "id": "-TSe5o7STVR",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Non-Parallel Text Style Transfer with Self-Parallel Supervision\nKeywords: style transfer, non-parallel corpus, imitation learning, language models, political stance transfer\nAbstract: The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style.\n\nIn this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models."
            },
            {
                "round2": "Reviewer 1:\nThank you for your explanations and added experimental results. They look good. I still two comments based on my previous points:\n1. My first comment in my last review is actually not exactly about low-resource. What I am concerned is that what if there are no or very low percentage of pairs that share similar content in the data distribution? I am wondering whether the authors can add more results on what is the percentage of high-quality pseudo pairs that can be found for each dataset via human evaluation. Especially for the formality dataset, this dataset is actually comprised of parallel pairs so it is very easy to find out pseudo pairs using some unsupervised sentence matching models, however, what if we split the dataset and only use the first half from the formal style and the second half from the informal style? In this case, there will be no pairs existing in the data, and then could we still find out pseudo pairs of good enough quality to make this method effective? \nOverall, my concern of this method is still whether its effectiveness depends on whether there exists high-quality pairs in the original data distribution. If there is very low percentage of high-quality pseudo pairs, would this method fail? If you can resolve this puzzle, I can further raise my point.\n2. In terms of the previous work results replication, I once ran the source code of DualRL and roughly remember that I can obtain the results shown in the paper. So I am not sure what is the reason why you cannot replicate the results in that paper. Have you contacted the original authors for the replication issue? \n\nReviewer 2:\nI'm pleasantly surprised by the few-shot style transfer results, and commend the authors for their rigorous rebuttal to all reviewers. I think the paper will be stronger with results on a dataset with a different content distribution between the two styles (irrespective of size, such as Shakespeare <---> Tweets). I've increased my score to 8 (I've updated main review).\n\nDo you have a sense of why the approach works reasonably in few-shot settings? Is it the case that you have sufficient entity alignments / syntactic alignments (even if there aren't enough full semantic alignments)? Is hallucination more common in the few-shot setting?\n\nNEXUS is certainly a better name, assuming it's an acronym for something related to the technical contribution in this work.\n\nReviewer 3:\nThanks for your reply! As a quick clarification question, what do you mean by \"each s^src has multiple s^tgt as demonstrations?\" Do you mine demonstrations from a pool of only from 1% of the D_tgt corpus, or the entire D_tgt?\n\nReviewer 4:\nThank you for your detailed response! I had one clarification question about the few-shot experiments. There are two ways to construct a 1% training dataset: \n\n1. Take 1% of the unsupervised training corpus (1% for each style). Perform mining on it. Use all mined pairs to train the model.  \n2. Perform mining on the entire unsupervised training corpus. Take 1% of the mined pairs for training the model.\n\nWhich one of the two methods are you using for your few-shot setting?\n\nReviewer 5:\n> \"Informed by several prior work, we believe that GM can serve as a single score with which readers can compare different TST methods easily\"\n\n[1], [3], [4] used references to the [2] as a support for using GM.\n\n[2] provides one very weak comment on it, literally: **\"The G-score is one of the most commonly used \"single number\" measures in Information Retrieval, Natural Language Processing, and Machine Learning.\"**\n\nI strongly advise against using unchecked and unsupported by research metrics as support of your results.\n\nReviewer 6:\nThanks for the quick and detailed update. Part of my concerns was successfully solved; however, I still have some:\n* \"**We also made sure to not claim SotA results on all metrics for all tasks**,\" but you still claim \"LaMer has the best overall performance,\" which should be supported by more solid evaluations, in my opinion.\n* \"**we have replaced perplexity with GM (geometric mean of ACC and BLEU) to measure the overall performance**.\" I don't think it is the right thing to do: while perplexity could be useful as a proxy for fluency, G-score (or H-score) can't be claimed as a universal single number metric without some additional proof and research on the right way to weight its components. The given references lack such explanations or proofs, so the use of G-score is unmotivated here.\n* \"**d_order and d_exist are actually taken into consideration with different weights. For the political stance TST we assign little weight to the order as the sentences are longer and more complicated**\" -- this raises the question of how to pick the correct value for this new hyperparameter.\n\n\nReviewer 7:\nsummary_of_the_paper: This work has proposed a new method to textual style transfer, which is based on the assumption that there exist some pseudo-parallal sentences pairs between two styles. It first construct synthetic parallel corpora by using two similarity measures: semantic similarity based on larger LMs and scene graph similarity. Then it trains the generation model via reinforced imitation learning. \n\nmain_review: Strengths:\n1. The scene graph similarity used to select pseudo parallel pairs is novel and makes sense to me.\n2. The reinforced imitation learning shows better performance compared with baseline of MLE loss for one-to-many mapping.\n3. The empirical results establish the effectiveness of this method.\n\nWeaknesses:\n1. This method is based on the assumption that there are parallel pairs in the original corpora of two styles so that sentences of the same content and different styles can be found out, which limits the use of the method to cases where abundant corpora are existing and there are parallel sentences there in the corpora. In those cases where only small unsupervised corpora are there and there are no sentences pairs sharing the same content, this method would not work.\n2. In the current used three datasets, I would like to know how good are those paired sentences selected out. This kind of quantitative human evaluation of constructed pseudo-parallel corpora is very important to establish the effectiveness of this work, which cannot be lacking.\n3. What is the BLEU score? Is it self-BLEU or ref-BLEU? If ref-BLEU, is it calculated on one human reference or four? I know at least for Sentiment transfer and formality transfer, the test sets contain four references. \n4. In Table 1, according to the Tabl1 of the original DualRL paper: https://arxiv.org/pdf/1905.10060.pdf, the BLEU scores can be 55.2 and 44.9 for Yelp and GYAFC datasets, while the scores reported in this work are significantly lower. I am not sure whether the references are different here. Please explain this gap.\n5. In Table 1, the performance of Ours w/. RD also looks very high, especially for the Political Stance dataset, the difference between w/. RD and w/. S-Emb are very small for BLEU and PPL of w/. RD is even lower, which contradicts to my intuition. Using a pseudo-parallel corpora constructed by random selection should not produce any good performance, should it? This kind of RD baseline even outperforms many strong SOTA baselines, for example of DUAL RL, which is ironic. \n6. In Table 1, it shows that the SAS score is only helpful for one datasets out of three, then why is it so? Why is it only useful in the formality case?\n7. This work is inspired by this work: https://aclanthology.org/D19-1306/, however, it has never been compared with it. \n\nsummary_of_the_review: This work has merits, but its weaknesses are also salient as mentioned above. I am looking forward to author responses for addressing my concerns, or I vote for rejecting this work.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 8:\nsummary_of_the_paper: This paper proposes an approach to addressing text style transfer with non-parallel data. The basic idea is to follow three steps: (1) for a given source sentence, mine some nearly parallel sentences from the target domain; (2) perform an MLE learning; and (3) augmented with an imitation learning. \n\nThe proposed method was evaluated on three text style transfer tasks: (1) sentiment transfer; (2) formality transfer; and (3) a new task called political stance transfer. \n\nmain_review: Strengths\n\n- A step-wise way of training text style transfer models on non-parallel datasets\n- A new dataset for style transfer, collected from the political domain. \n\nWeakness\n\n- First, I am not sure about the novelty of the proposed method. Given the ideas of MLE and RL/IL (specifically, the INFORCE algorithm used in this work) has been used in text generation for a while, I am not sure how much novelty here by simply combining them together. \n- Second, the writing of this work needs to be improved.\n  - In section 2.2, the Scene Alignment Score seems to be an important component of the proposed first step, however, I don't think this work ever (1) explain what is a scene graph and (2) justify the validity of using the Scene Alignment Score for alignment. The numbers in Table 1 may provide some empirical evidence for the second question, but it is not sufficient. \n  - I was confused by the terms used in section 2.4, including \"Reinforced Imitation Learning\", \"Reinforced Policy Gradient\". To be specific, based on the description, I didn't understand the difference between reinforced imitation learning and imitation learning. In addition, I am also not sure what reinforced policy gradient is. It looks like the description in section 2.4 mixes many terms together, without an appropriate explanation. \n  - In Algorithm 1, what is $J_{IL}^{safe}$? \n  - About table 1, I am not sure how the highlights were selected in this table. Apparently, not all the highlights are the best results. \n\nsummary_of_the_review: My major concern of this work is the technical novelty. In addition, the writings of this paper also needs to be improved. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 9:\nsummary_of_the_paper: The paper proposes to transform the popular unsupervised style transfer to a semi supervised task. First the paper proposes simple methods to mine parallel sentences from two unlabelled corpora. Further, they use the mined parallel sentences, in a MLE based training (sequence to sequence framework using BART) and refine it using Contrastive Learning based imitation learning. The paper achieves improvements on preserving the content between the transferred sentences. In addition, the authors also propose a new dataset for political style transfer in a semi-automated way which is a welcome addition to the paper.\n\n\nmain_review: **Strengths**\n\nThe paper tackles an important issue of text style transfer with novel ideas that aims to preserve content in an effective manner. \nThe paper is relatively well written with justifications and reasons for the choices. The authors could have given examples \nThe paper presents a simple idea to use reinforcement learning: imitation learning in particular  to better maintain the content between the two styles. \n\n**Weaknesses**\n\n1. The authors assume that  parallel sentences inherently exist between the source and the target style corpus. However, the existing unsupervised text style transfer methods do not make such assumptions. Since the entire basis of the paper is on such an assumption, providing examples of the inherent parallel sentences is crucial. The paper can be made stronger by  providing  examples (either manually picked or mined).\n\n2. **Scene graph alignment** - What are the scene entities? The authors mention that the use (Wu et al. 2019) paper on Unified Visual Semantic Embeddings. The reviewer skimmed through the other paper as well. However, which parser is used in the current paper?  Wu et al. use a syntactic parser to mine the objects in the sentence, adjectives, nouns, subjects of verbs etc and initialize an embedding for different entities and learn a joint embedding space. Here are my concerns with respect to this.\na) The authors need to be clear on the parser which is used to parse the sentence in this paper. Referring to Wu et al.’s paper without describing what the scene graph entities make it unclear on the reason to include this as a refinement step. \nb) If the above parser to extract objects is used, how is it important for style transfer? For example, the style transfer datasets used in the paper might not share any “visual” entity like a clock, plate etc. Instead they might share some abstract entities like the service provided, the cleanliness of the restaurant etc. \n\n     The reviewer understands that the sentences that talk about similar entities are important, the approach to obtain those entities is unconvincing. Seeing Figure S1 in the appendix, it is clear that the scene entities are words that are salient in the given style or words that carry content. Although the reviewer understands the idea, it would be a good idea to rewrite this to make it clear to the reader.\n\n\n3. **Evaluation Measures**: The used evaluation measures in the paper have known problems. For example, using BLEU does not measure semantic similarity with the input sentence and certain sentences with unnatural structure can still have low perplexity scores. Refer to Kalpesh Krishna et al 2020(https://arxiv.org/abs/2010.05700), which is included as a baseline method to compare against. The adoption of these measures would have made the paper stronger. Although the reviewer agrees that the majority of the text style transfer papers use the metrics used in this paper, the reviewer would appreciate if the authors could evaluate based on the paper from Krishna et al 2020 and add it in the Appendix or supplementary material\n\n4. **Evaluation**: The evaluation section should concentrate on the major contribution of the paper: maintaining the content between the two styles well. For example, with imitation learning we expect the BLEU metrics (and the i-PINC metric)  to do better because imitation learning aims to preserve the content in a better manner between the source and the target style which needs to be the highlight of the evaluation section. The other metrics might not be the focus of the paper and achieving high measures on them is an added bonus. The word “performance” is excessively used in the evaluation section. Instead, the specific measure that the others are talking about should be mentioned. \n\tAlso, Mean and Standard Deviation with multiple experiment runs are missing from the evaluation section.\n\n5. **Conclusion**: The conclusion is a summary of the main findings of the paper. It would be ideal if the authors can provide implications of the current work going forward. Can this be extended to other works that try to control the fine-grained syntax of the sentence etc would be a welcome addition to the conclusion \n\n6. **New Dataset**: More details on the new dataset introduced in the paper is needed. Why is the new dataset more challenging compared to the other two datasets that are mentioned in the paper?\n\n**Minor Typos, Grammatical Errors and Other Readability Issues**\n\n1. There is an extra space on Page 2 Para 2 Line 2 after Figure till (b)\n2. The authors introduce the term “scene graphs” in the introduction on Page 2 Para 2 and Line 4. Readers who are not aware of the computer vision literature might be left wondering on how scene graphs are relevant for Text Style Transfer \n3. The rows showing ablations in Table 1 are confusing “w/” is read as without or with. The authors could instead use “+” or “-” symbol to indicate with or without\n4. Table  1 highlight boxes need to be explained to the reader.  Please mention that you are highlighting the best performing method for that metric. Also some of highlight boxes are missing in the Sentiment dataset\n\n\nsummary_of_the_review: 1. Overall, the paper is easy to read and the ideas to improve content preservation between the source and the target sentence is simple and effective. The ablation studies and parameter sensitivity studies are well performed.  \n2. The reviewer would require more details on the entities that are preserved between the sentences, examples on the inherent parallel sentence between the two corpora in the introduction.\n3.  Problematic evaluation measures used in the literature need to be revised \n4. The evaluation section needs to focus on the main contribution of the paper. Splitting Section 3.2 into more paragraphs might be a starter. \n5. The benchmark table needs to have multiple runs with mean and standard deviation reports.\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 10:\nsummary_of_the_paper: The paper addresses a text-style transfer task based on non-parallel datasets.\n\nThe authors propose a three-step approach:\n- First, to match each source sentence with several sentences from the target style dataset using sentence embedding similarity and scene graph matching.\n- Next, to fine-tune a seq2seq network (BART) in an NMT-like setup with several references.\n- Finally, to use imitation learning to enforce the loss contrast between the best target candidate and all others.\n\nThe authors propose a new dataset for the TST task, called \"political stance transfer,\" and use it for evaluation among the previously known datasets. The results of such a combined approach are competitive with previous works.\n\nmain_review: Strengths:\n\n- The paper uses a novel combination of existing approaches to achieving competitive results.\n\n- The code provided with the paper is clean, structured, and can be useful for developing future models for the same task.\n\n- The paper itself is well written and structured.\n\nWeaknesses and questions:\n\n- The authors claim \"existing methods learn a mapping without considering the self-parallelism, ... they tend to learn the mapping between source and target style by randomly mapping sentence pairs.\" However, no proofs or references for such behavior were given. On the contrary, most of the works mentioned as related have no such problematic behavior by design.\n\n- The first step of the proposed approach (matching sentences from the source and target corpora) implies that both corpora have a similar distribution. However, it is not the case for some definitions of \"styles.\" This assumption possibly limits the method's applicability, but it wasn't addressed in the motivation or discussion sections.\n\n- Moreover, the token-level scene preservation scheme implies that scene entities couldn't be style markers themselves. This assumption also limits possible applicability until the opposite is proven.\n\n- Again, the part of the loss is based on the difference in the orders of scene entities, but in practice, it can be an important part of the style. Consider the example from the 'political stance' dataset presented with the paper:\nStyle1: Mulvaney tapped to lead Trump s budget office.\nStyle2: Trump picks debt warrior Mulvaney to lead White House budget office.\n\n- There is a known inevitable trade-off between content-preservation and style-transfer metrics. Thus, to show the improvement of the new model, one needs to demonstrate Pareto-improvement both of them (in this case, this means the improvement both in terms of ACC and BLEU metrics). However, the reported results show the TSF-DelRetGenLM model is better in terms of one of these two metrics for both sentiment and formality datasets. Thus, the evaluation shows the comparability of the model with the baselines on two standard datasets (and the superiority only on the novel proposed dataset). At the same time, the authors claim \"SOTA results according to many metrics on the three TST tasks,\" which doesn't sound fair. \n\n\n\n\n\n\nsummary_of_the_review: I like the proposed approach and the clarity of the paper and code.\nI'm slightly concerned with the paper's novelty since it presents a combination of previously known methods, but still, I think it can be useful for the community.\nThe main concern is somehow ambiguous motivation and definition of the scope of applicability of the proposed approach (check the questions above). \nI believe this should be clarified.\n\nMy score for the paper is \"marginally above the acceptance threshold\".\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 11:\nsummary_of_the_paper: Style transfer is a text generation task where sentences need to be rewritten in such a way that a particular target attribute is introduced (like formality), but at the same time the content is approximately preserved. Style transfer is usually researched in unsupervised settings --- researchers assume no access to parallel data of sentences differing only in the target attribute. However, most prior works assume access to a corpus of unpaired sentences in each of the target attributes.\n\nThe main technical idea in this paper is searching for (mining) roughly parallel pairs in the unpaired corpora, to create a \"pseudo-parallel dataset\". The authors use two methods to extract pairs - (1) semantic similarity based on RoBERTa embeddings; (2) similarity of a scene-graph built by using an off-the-shelf parser. These pseudo-parallel data is used to fine-tune a large pretrained language model (BART). To improve performance, the authors utilize imitation learning via REINFORCE. The key idea is checking whether the current greedy sample is closer by a margin to the closest mined parallel sentence (positive example), compared to other mined parallel sentences (negative examples). If not, sampling is used to explore the space and the sampled sequence is rewarded / penalized according to the increase / decrease in margin using REINFORCE.\n\nThe authors test their approach on sentiment transfer, formality transfer and politcal stance transfer (which is a new dataset introduced by them), and note improvements over several popular baselines on automatic metrics and human evaluations. The authors also analyze their choice of hyperparameters and conducted several ablation studies to justify their design decisions.\n\nmain_review: *Strengths*\n\n1. I liked the ideas in the paper. As far as I know, scene graphs have not been used previously in style transfer and are a nice way to ensure semantic preservation. I believe parallel data mining has been done before in unsupervised machine translation [1] and pretraining [2] (papers I suggest the authors cite), but not for style transfer. The part of the approach using imitation learning to refine the pseudo parallel data is also new and effective as confirmed by the ablation studies.\n\n2. The authors collect a new dataset for political stance rewriting. This dataset is fairly large (56K pairs) and the task sounds harder than sentiment transfer. This is a valuable contribution and it could be a useful benchmark for future style transfer research. The authors have also carefully curated the dataset to remove hate speech, and used a filtering method (Appendix B) to ensure mined sentences are faithful to the attribute.\n\n3. The authors show improvements in three different text rewriting tasks over several baselines on both automatic metrics and human evaluations. Additionally, the authors provide ablation studies and analyze the choice of their data mining hyperparameters.\n\n*Weaknesses*\n\n1. While I liked the ideas in the paper, I thought the main technical contribution of mining psuedo parallel data from an unpaired corpus is fundamentally limited. First, there's no guarantee the unpaired corpus will have a similar content distribution across attributes (which is required to retrieve roughly parallel sentences). The formality / political stance transfer benchmarks tested were originally parallel datasets, which ensures a shared content distribution. Second, you might need a very large unpaired corpus to ensure this mining is successful, which may not be practical in lower resource settings (this is why recent work is moving towards few-shot setups for style transfer [4, 5]). I suggest the authors try style transfer tasks where the content distributions will be very different across attributes, such as Shakespeare <---> Tweets from [6].\n\n2. Not a very big concern (since your method outperforms baselines on all three metrics), but it will be nice to see an aggregated overall score for style transfer performance (especially on human evaluation). The three style transfer metrics (accuracy, similarity, fluency) often conflict with each other [7], so an overall score is generally more informative. You could do this for both automatic and human evaluations. For human evaluations, since you use 7-point Likert scales you could follow the approach in [8] who calculate the number of instances which got a Likert score of (let's say 5 or more) on all three metrics *simultaneously*. A more general alternative is presented in [4, 6] which can be used for both automatic and human evaluations.\n\n**Minor**\n\n1. The acronym LaMer is not specific to the contributions of this paper (pseudo parallel data, scene graphs, imitation learning). It is also a slang for \"dull\" in informal settings. I suggest changing this acronym to something else.\n\n2. In 2.4, you could alternatively use unlikelihood training [3] on the amateur policies.\n\n3. I suggest reducing focus on the details of the RL methodology and adding more details on the new benchmark dataset proposed in the main body of the paper.\n\n4. Question about human evaluation ---> did you randomly shuffle the order of generations from different systems? That way you would avoid annotator bias from their previous annotations (\"the last generation is always best\")\n\n5. Several links seem to be broken, especially those to the Appendix.\n\n6. I suggest the authors add more qualitative outputs from their system into the Appendix part of the PDF, rather than in an external attachment.\n\n[1] - https://aclanthology.org/P19-1178  \n[2] - https://arxiv.org/abs/2006.15020  \n[3] - https://arxiv.org/abs/1908.04319  \n[4] - https://arxiv.org/abs/2110.07385  \n[5] - https://aclanthology.org/2021.acl-long.293  \n[6] - https://arxiv.org/abs/2010.05700  \n[7] - https://arxiv.org/abs/1910.03747  \n[8] - https://aclanthology.org/N18-1169\n\nsummary_of_the_review: Overall, I'm in favour of acceptance since the paper has several interesting new ideas, a large new dataset for political stance transfer, and automatic + human evaluation comparing the benefits of their approach against baselines. The weakness #1 prevents me from giving the next higher score (of 8).\n\n-----------\n\n**After author response**: I'm pleasantly surprised by the few-shot style transfer results, and commend the authors for their rigorous rebuttal to all reviewers. I think the paper will be stronger with results on a dataset with a different content distribution between the two styles (irrespective of size, such as Shakespeare <---> Tweets). My overall assessment is more like a 7 or 7.5, which means I'll increase my score to 8.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: Thank you for clarifying your questions and for updating your score! \n\n**About Question 1**\n\nIn our few-shot experiment, we actually try to get at what you are suggesting here. We sample the formal and informal sentences separately, not in pairs. This way we are very likely to be left with formal and informal sentences that have different content distribution (since they are shuffled out of sync). Specifically, the process of our few-shot experiments is as follows:\n\n1. We sample 1% of the data points from each of the formal and informal sentences (which corresponds to around 520 formal and 520 informal sentences). Note that we sample the formal and informal sentences separately, meaning that we do not sample formal-informal pairs. Now we have the new $D^{\\textrm{src}}$ (formal) and $D^{\\textrm{tgt}}$ (informal); each with a size of 520.\n2. We run alignment on $D^{\\textrm{src}}$ and $D^{\\textrm{tgt}}$. Each $s^{\\textrm{src}} \\in D^{\\textrm{src}}$ will be mapped to multiple $s^{\\textrm{tgt}} \\in D^{\\textrm{tgt}}$ by the alignment algorithm. The number of these one-to-N pairs is 520 since we have 520 $s^{\\textrm{src}} \\in D^{\\textrm{src}}$.\n3. We then split these one-to-N pairs into one-to-one pairs, resulting in 1800 $s^{\\textrm{src}} - s^{\\textrm{tgt}}$ pairs.\n4. We use the one-to-one pairs for MLE training, and the one-to-N pairs for IL refinement.\n\nWe measure the SAS score of our few-shot dataset before and after we perform alignment. The scores are 0.01 (before alignment, i.e., random alignment), 0.03 (after LM alignment), 0.09 (after LM + SAS alignment). Compared with 0.29 SAS for LM + SAS alignment in full dataset (shown in Table 7), it may partially explain why we see a 40 point $J$-score drop (absolute) when the data is decreased to 1% (see Table 2 in our revised version). However, other baselines show even worse performance as their external modules require training on ample data to work properly. In general, if there is very low percentage of high-quality pseudo pairs (such as 1% few-shot cases), LaMer has relatively robust performance compared to other strong baselines (at least no worse than them), and outperforms them when the full data is available.\n\nWe agree that additional human evaluations to measure the percentage of high-quality pseudo pairs will be an informative addition to the paper. We started running this experiment this morning and though it is somewhat time-consuming, we are confident the experiments can be done in the next week, allowing us to add these results to our camera ready. However, note that absent these experiments, the human reference section of Table 7 actually shows the how much parallel \"human alignment\" can achieve: Since the test sets are written by human (i.e., Human References in Table 7) and they have to be in parallel, the SAS score calculated over these human references actually demonstrates what’s the upper bound of the parallelism shown in human demonstrations. Table 7 shows that LaMer’s SAS alignment manages to reach human-level parallelism, and as discussed in the last paragraph, we see improvements in few-shot cases as well.\n\nFor the TST tasks that source and target style use nearly different forms of language (e.g., ancient to modern English TST), we agree LaMer’s performance could be limited if we still choose scene graph alignment (i.e., SAS alignment) since there are little similarities in surface level among entities. We thus recommend using the (multilingual) LM alignment (our second proposed alignment method) to align the data as an alternative. We have discussed this in the revised section 2.2.\n\n**About Question 2**\n\nThank you for the suggestion. We have indeed contacted the authors of DualRL, and we are looking forward to their response. However, to err on the side of caution, we plan to report their official results (and maybe add our results to appendix) if they don’t get back to us. We will reflect our edits in camera ready version if we have any further updates.\n\nHope our answers can clarify your concerns. We appreciate your kind understanding and professional suggestions!\n\n\ncomment: Thank you for your response and we are grateful that you have updated your recommendation.\n\n1. Thank you for the suggestion regarding experiments on an additional dataset with different content distribution. We agree that such an experiment would strengthen our paper. We plan to run such an experiment on at least one dataset and include the results in the camera ready. \n\n2. We believe our method works reasonably well in few-shot settings mainly because of the imitation learning (IL) refinement. Specifically, though the aligned pairs could be fewer in few-shot settings, the IL refinement will encourage the model to focus more on the really good parallels as the positive demonstration and pay less attention to the bad ones as the negative demonstrations (though there will of course be fewer negative demonstrations in few-shot settings). \n\nWe thank the reviewer again for the thoughtful reviews and great suggestions! \n\n\ncomment: The demonstrations come only from the pool of 1% of the $D^{\\textrm{tgt}}$. Specifically, the whole procedure is as follows:\n\n1. We sample 1% of the data points from each of the formal and informal sentences (which corresponds to around 520 formal and 520 informal sentences). Now we have the new $D^{\\textrm{src}}$ (formal) and $D^{\\textrm{tgt}}$ (informal); each with a size of 520.\n2. We run alignment on $D^{\\textrm{src}}$ and $D^{\\textrm{tgt}}$. Each $s^{\\textrm{src}} \\in D^{\\textrm{src}}$ will be mapped to multiple $s^{\\textrm{tgt}} \\in D^{\\textrm{tgt}}$ by the alignment algorithm. The number of these one-to-N pairs is 520 since we have 520 $s^{\\textrm{src}} \\in D^{\\textrm{src}}$.\n3. We then split these one-to-N pairs into one-to-one pairs, resulting in 1800 $s^{\\textrm{src}} - s^{\\textrm{tgt}}$ pairs.\n4. We use the one-to-one pairs for MLE training, and the one-to-N pairs for IL refinement.\n\nThanks again and please let us know if you have further questions!\n\ncomment: We used the first method. We ran the entire system with the limited data (1% of the 52k formality TST training data), including the mining stage. Mining on this produces around 520 $s^{\\textrm{src}}$, and each $s^{\\textrm{src}}$ has multiple $s^{\\textrm{tgt}}$ as demonstrations --- the number of $s^{\\textrm{tgt}}$ depends on the top-$p$ and -$k$ you set for alignment (the impact of these parameters is shown in Figure 3. We used $k=200, p=0.4$ for formality TST). We then split these one-to-N $s^{\\textrm{src}}$- $s^{\\textrm{tgt}}$ pairs to one-to-one pairs (around 1800) for MLE training, and use the one-on-N version (around 520) for IL refinement. \nFor other baselines we used the same size of data (i.e.,1% of the 52k formality TST training data), which was used to train the core components of those baselines, such as the style classifier used by TSF-DelRetrGen.\n\nWe hope this clarifies our approach to the few-shot setting. \n\ncomment: We thank the reviewer for their fast response (especially on a Monday), allowing us to  comment before the end of the discussion period. On further deliberation on the reviewer’s point, we agree that the use of GM/G-score seems to be based on one paper which then propagated to other papers without much support. Since several of the baselines we are comparing against (DualRL and STRAP) use the GM we will keep it for ease of comparison. However, we have removed the emphasis on it as the one aggregate metric to be looked at. We hope that our comprehensive list of metrics (ACC, BLEU, $i$-PINC,GM, $J$-score, and perplexity), all of which are used by at least one of our baselines, can provide a better picture of our method’s performance with respect to other methods. We made sure that the metrics of all of our baselines are represented in our analysis for more direct comparison.\n \nIn the same way, we have also added a sentence in Section 3.2 to emphasize that GM is only included for comparison purposes and that it is not a well-studied metric for TST.\n \nWe once again thank the reviewer for their time and valuable comments that have strengthened our paper.\n\n\ncomment: Thank you for the response. We are happy our revision and previous response addressed parts of your concerns. We have addressed the new issues raised below and included a revised version of the paper.\n\n#### **SotA Claims** \n\nWe have replaced “best overall performance” to “competitive performance” and have specifically mentioned the metrics in which LaMer outperforms other methods (see Section 3.2 in the revised version). Thank you for pointing this out.\n\n#### **Performance Metrics** \n\nWe have reintroduced perplexity in Section 3.2 with a reference to a new table in the appendix showing our original results (Table 12 in A.9). We decided to remove perplexity from the main table as other reviewers pointed out recent work that shows its shortcomings. For example, [1, 6, 7] find perplexity is actually a poor measure for fluency because “1) it is unbounded and 2) unnatural sentences with common words tend to have low perplexity.” We believe our human evaluations shown in Table 6 (in the main paper) and Tables 9, and 10 (in the Appendix) are a better measure of fluency/readability since they measure it directly, whereas perplexity is a proxy for fluency/readability. However, if the reviewer feels strongly about the inclusion of perplexity in the main paper, we would be happy to do so in Table 1.\n\n\nInformed by several prior work, we believe that GM can serve as a single score with which readers can compare different TST methods easily (though we agree with the reviewer that GM is not a perfect aggregate score). For instance, GM (i.e., G-score) as an aggregate score is used in the following papers (we have cited these papers as the motivation for use of GM in Section 3.2):\n\n- Reformulating Unsupervised Style Transfer as Paraphrase Generation [[1]](https://aclanthology.org/2020.emnlp-main.55.pdf) (see Table 1)\n- Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach [[2]](https://aclanthology.org/P18-1090.pdf) (see Tables 1,2, and 4)\n- Domain Adaptive Text Style Transfer [[3]](https://aclanthology.org/D19-1325.pdf) (see Tables 2 and 6)\n- A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer [[4]](https://www.ijcai.org/proceedings/2019/0711.pdf) (see Tables 1 and 3)\n\nAdditionally, our results in Figure 3 show the trade-off between ACC and BLEU when running the model under different hyperparameters (the $k$ and $p$ in our case). Finally, as suggested by other reviewers, in the revisions we also report additional metrics, such as the $J$-score when we evaluate LaMer in few-show (and full-data) scenarios (see Table 2), which is introduced in [1] as a more advanced overall metric.\n\nWe hope our clarifications and revisions have addressed your concerns regarding the evaluation metrics. \n\n\n#### **Picking the weights for $d_{\\textrm{Order}}$ and $d_{\\textrm{Exist}}$** \n\nWe pick the weights by empirical observation. We run repeated experiments ranging the $\\alpha$ from 0 to 1 by step-size of 0.1, and pick the best $\\alpha$ that can lead to the best performance with respect to GM. We have added this explanation to footnote 2 in Section 2.4 (page 5).\n\n\nWe once again thank you for your time and detailed comments, and we believe your suggestions have made our paper much stronger!\n\n---\nReferences:\n\n[1] [Reformulating Unsupervised Style Transfer as Paraphrase Generation](https://aclanthology.org/2020.emnlp-main.55.pdf)\n\n[2] [Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach](https://aclanthology.org/P18-1090.pdf)\n\n[3] [Domain Adaptive Text Style Transfer](https://aclanthology.org/D19-1325.pdf)\n\n[4] [A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer](https://www.ijcai.org/proceedings/2019/0711.pdf)\n\n[5] [Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization](https://aclanthology.org/2021.acl-long.8.pdf)\n\n[6] [Evaluating Style Transfer for Text](https://aclanthology.org/N19-1049/)\n\n[7] [Text Generation by Learning from Demonstrations](https://openreview.net/pdf?id=RovX-uQ1Hua)\n\n\n\ncomment: We thank the reviewer’s thoughtful comments and constructive suggestions. We are glad that reviewer found strengths in our paper’s novelty, experimental procedure, and contribution to the community. Below we answer your questions:\n\n- **Few-shot cases, and scope of the work.** Thank you for your notes. To explore the few-shot ability of LaMer, we conduct additional experiments in extreme data scarce cases where only 1% of training data is available to train the whole system. As shown in the newly added Table 3, we find LaMer performs well compared with other baselines. The reason could be that the other baselines (except zero-shot GPT-3) all require external systems trained on ample data. For example, TSF-DelRetrGen requires training a classifier to locate style-carrying spans of text. STRAP fine-tunes a GPT-2 model to learn how to paraphrase. IMaT trains a plain Seq2Seq model from scratch as an LM (not pre-trained). All these methods show limited performance in data hungry cases, partially because the compromised external modules would propagate errors to following parts as they are coupled. LaMer instead, built upon pre-trained LMs, requires neither extra data or additional model training, and thus shows superior performance in few-shot cases. We even consider about the two few-shot specific baselines mentioned in your comments [1, 2]; however, none of them release their official implementation. Of note, more unsupervised data will benefit LaMer, but we claim LaMer can do at least as good as many strong LM-based methods in few-shot scenarios. We also include a zero-shot baseline (GPT-3) for reference. Please see revised section 3.2 for more details about these new experiments.  \nFor TST tasks that have different forms of language, though SAS alignment of LaMer relies on entity alignment for mining parallel corpora, other alignment mechanisms, such as (multilingual) LM alignment (which is our second proposed alignment method) can potentially be used for transfer cases where entity alignment might not be possible (such as ancient English v.s. modern English [1], or even multilingual text style transfer [4]). In general, though we agree that LaMer has certain limitations, we believe the ideas conveyed in this paper push forward a simple and efficient TST framework without the need for additional training of external models or extra data (or annotation).\n\n- **About better evaluation metrics, and new human evaluation statistics.**\nThanks for your great suggestions! As shown in revised Table 1, we have replaced perplexity with GM (geometric mean of ACC and BLEU) to measure the overall performance. Also, following your suggestion, we have calculated the ratio of annotators who rated the transfer above a certain point and added this information in Tables 5 and 6.\n\n- **About name: LaMer.** Thank you for your note regarding the name LaMer. We did not change the name in the revision to avoid confusion but we will definitely consider renaming our method for the camera ready (How about NEXUS, which means connection?). :)\n\n- **About presentation.** Thanks for your suggestion. We have moved the details about RL learning to appendix and have fixed the link issues! We have also cited the paper [3] you mentioned in \"Strength 1\".\n\n- **About human evaluation.** Yes. We design the questionnaire with Qualtrics and add a random sampler for shuffling the sentences. The annotators were not aware of which system generates the sentence.\n\n- **About sample generations.** As noted by the reviewer, we have included a text file with examples of sample generation for reviewing purposes. Based on your suggestions, we will add several samples from the file to our appendix for the camera ready (in addition to sample generations already shown in Table 8).  \n\nThanks again for your thoughtful reviews and helpful suggestions!\n\n[1] [Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages](https://arxiv.org/pdf/2110.07385.pdf)\n\n[2] [TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling](https://aclanthology.org/2021.acl-long.293.pdf)\n\n[3] [Pre-training via Paraphrasing](https://arxiv.org/pdf/2006.15020.pdf)\n\n[4] [Paraphrasing for Style](https://aclanthology.org/C12-1177)\n\n\ncomment: We thank the reviewer’s thoughtful comments and suggestions, and we are glad that you found our idea interesting and novel, and could be useful for the community. Below we answer your questions:\n\n- **Regarding related work.** Thank you for pointing this out. We indeed find that most non-LM based methods do not consider self-parallelism within the data by just taking the unsupervised data as the input without any alignment. While some recent works do seem to notice the inherent self-parallelism and propose several strategies to leverage this. LaMer differs from them as it requires neither extra data (such as paraphrases data used by STRAP) nor additional training on external systems (such as style locator used by TSF-DelRetrGen). We have added additional references next to our claims in the revised introduction.\n\n- **Regarding scope of LaMer.** We also added additional discussion in section 2.2 regarding the scope of our SAS alignment and describe how other non-attribute-based alignment methods (such as multilingual LM alignment) can be used for TST scenarios where entities may not be shared across styles (e.g., multilingual TST). In general, though we agree that LaMer has certain limitations, we believe the ideas conveyed in this paper push forward a simple and efficient TST framework without the need for additional training of external models or extra data (or annotation).\n\n- **Regarding scene entities.** We agree that the scene entity can sometimes carry style; however, note that our scene graph alignment only provides weakly parallel samples, the exact mapping relationship is learned by the text2text LM, which means that aligned entities are not a *hard* constraint for learning but provide *soft supervision*. Our experiments demonstrate that learning over such samples can empirically lead to good TST performance. \n\n- **Regarding the token-level scene preservation.** $d_{\\textrm{Order}}$ and $d_{\\textrm{Exist}}$ are actually taken into consideration with different weights. For the political stance TST we assign little weight to the order as the sentences are longer and more complicated. We have updated the equation in Token-level Scene Preservation of section 2.3 to show how we sum up the two weighted distances.\n\n- **About SotA claims.** In the revised Table 1, we have replaced perplexity with GM (geometric mean of ACC and BLEU) to measure the overall performance. We also made sure to not claim SotA results on all metrics for all tasks. Note that after closer examination, we find the BLEU score calculation script used by TSF-DelRetrGen is slightly different from other methods: most existing methods (including ours) measures the average n-gram (n ranges from 1 to 4) matching (e.g., [style transformer](https://github.com/fastnlp/style-transformer/blob/master/evaluator/evaluator.py) uses sentence BLEU, which assigns equal weights for 1-4 gram matching). TSF-DelRetrGen, instead, uses the **max** n-gram (n ranges from 1 to 4) matching to compute BLEU (see line 14 in their [evaluation code](https://github.com/agaralabs/transformer-drg-style-transfer/blob/master/evaluation_scripts/bleu.py)), which could partially explain its relatively high BLEU scores. We re-evaluate their results with the standard scripts to align with other methods. \n\nThanks again for your thoughtful reviews and helpful suggestions! Hope our revision can solve your concerns and questions!\n\n\ncomment: We thank the reviewer’s thoughtful comments and suggestions, and we are glad that you found our idea interesting and novel. Below we answer your questions:\n\n- **Better motivation and samples.** Thanks for your suggestion. We have added an example of SAS mined sentences with corresponding scene entities annotated in Figure 2 (to qualitatively illustrate our assumption). We further justify our assumption through empirical observation on the data: In Table 7 (located in the appendix) we have shown that if NO SAS alignment happens (RD), there would be huge discrepancy on the average SAS score between training data and human references (e.g., in Formality TST, 0.29 with aligned by SAS and 0.01 without alignment). Since the human references are already parallel style transfer sentences, our success on the alignment demonstrates that there is self-parallelism existing in the unsupervised data. Additional human evaluation confirms that our aligned sentences are indeed similar in style-independent content---please take a look at Table 5 and related discussion.\n\n- **Explanation about scene graph lignment.** We use [the parser released by the author of Wu et al.'s paper](https://github.com/vacancy/SceneGraphParser), which outputs mined scenes graphs in *<subject-relation-object>* triplets. In our case, we only consider the end nodes of these triplets (which are *subj.* and *obj.*) as the “scenes entities”. We have made this clear in our revised version (see section 2.2).\n\n- **Better Evaluation Metrics.** Thanks for your great suggestion! We agree that existing metrics have flaws, so in our revised version, besides the ACC and BLEU in Table 1, following your suggestion, we use the new $J$-score framework proposed by [1] in Table 2 to evaluate the few-shot cases (in addition to the full training data experiments). We also include a GM metric which is the geometric mean of ACC and BLEU to represent the overall performance of TST models. We hope these changes can make the evaluation fairer and easier to interpret.\n\n- **More Concentrated Evaluation.** We have heavily revised our evaluation section with reorganized focus on specific perspectives (see section 3.2). We now present the overall and few-shot performances, ablation study, and human evaluation in order, and include more specific conclusions and discussion. For example, as you suggested, we have revised the ablation study section on imitation learning, and highlighted its effectiveness on content preservation (retitled with “IL refinement is crucial for LaMer content preservation”). We have included the SD of results in Table 2 as well. \n\n- **Extended conclusion.** We have revised our conclusion and now it not only summarises our main takeaways and advantages of LaMer but also provides implications for future research. Please take a look (section 5) and let us know if anything can be improved!\n\n- **Explanation about why political stance TST dataset is challenging.** Thank you for your suggestion. We have included more details about the datasets, and discussed the major differences between political stance transfer and the other two TST datasets in the revised Table 7 caption (located in the appendix). In general we find that political stance TST requires transfer which is much longer and contains more entities, which we believe makes it a more challenging TST task.\n\n- **Minor typos, or presentation improvement.** Thanks for the detailed comments! We have fixed the links to the appendix and several typos. In the revised section 2.2 we have explained what the scenes entities are and how we parse them. We have added a visualization sample of scenes entities in parallel sentences as in Figure 2. We have revised Table 1 for better readability and could take your suggestions in our final version. Thanks again!\n\nFinally, we appreciate the suggestions on the paper presentation. We hope our revised version could solve your concerns. Thanks again for your thoughtful reviews!\n\n[1] [Reformulating Unsupervised Style Transfer as Paraphrase Generation](https://arxiv.org/pdf/2010.05700.pdf)\n\n\ncomment: We are glad you found our idea novel, and evaluation results mostly convincing. Below we address your questions:\n\n- **Novelty.** The REINFORCE algorithm we use in our work can be viewed as an optimization procedure, while the core idea or novelty of our method is to use scene graphs (an idea originally from CV) to mine demonstrations from the unsupervised data, and use imitation learning with contrastive loss to learn how to transfer style in an efficient way. The major difference (and novelty) between our method and some recent paraphrase-based TST models is that we don’t need extra supervision from externally trained models (such as paraphraser used by STRAP and style locator used by TSF-DelRetrGen), which will bring additional bias to the training. LaMer, instead, uses the *self-supervision* from the unsupervised data itself with contrastive loss, and learns how to transfer in one-step seq2seq training, which shows superior performance across different settings.\n\n- **About scene entities, and additional human evaluation.** Thanks for your suggestions on the presentation of our paper. We have included an example (Figure 2) to show what scene graph entities look like when we mine using the scene graph parser (see footnote 1 for information about the parser). In revised section 2.2 we also explain how we extract scenes entities from the parser's output. We run additional human evaluations to demonstrate that the SAS-mined pairs share similar style-independent content based on human judgement (newly added Table 5). Please take a look and we are happy to hear further suggestions!\n\n- **About Policy-based RL, REINFORCE, Imitation Learning.** Policy gradient is a common optimization method adopted by many policy-based RL algorithms, and REINFORCE is just one of them. Imitation learning is a branch of RL: Instead of learning from the samples derived from the environment, IL learns from expert demonstrations directly (e.g., learning how to self-driving from human drivers' behavior records rather than vehicles' sensor data), so that it can avoid low sampling efficiency problems in RL and can have fast convergence. Following your suggestion, we have changed the subtitle of section 2.4.1 from “Reinforced Policy Gradient” to “Policy Gradient by REINFORCE”. We also moved the detailed learning algorithm to the appendix, and left the core equation (Eq. 4) there with a more easy-to-understand description in the revised paper. We rephrased the discussion about MLE, IL, and reward-based RL as well for easier understanding. Please let us know if any further improvement can be made!\n\n\n- **What is $J_{\\textrm{IL}}^{\\textrm{safe}}$?** $J_{\\textrm{IL}}^{\\textrm{safe}}$ is a safe-reward threshold. If the greedy policy can already reach this threshold then the policy will not be updated. Empirically, we set $J_{\\textrm{IL}}^{\\textrm{safe}}$ to $\\{0.8, 0.6, 0.4\\}$ for the three TST tasks (sentiment, formality, and political stance), which leads to decent performance. We clarified this in section A.5 of the appendix in the revised paper. \n\n- **Confusion on highlights.** In our revised paper, we have introduced the GM (geometric mean of ACC and BLEU) as an overall performance metric. Now we only color the best results in GM (and $i$-PINC) and underline the second best, to let readers focus on the overall performance in the first part of evaluation. We have made this protocol clear in the caption of Table 1.\n\nFinally, we thank the reviewer for the thoughtful comments and constructive suggestions. We hope our revised paper and additional experiments can provide further clarity!\n\n\ncomment: Thanks for reviewing our paper, and providing your valuable feedback! Below we address the issues raised:\n\n- **About few-shot cases.** To explore the few-shot ability of LaMer, we conduct additional experiments in extreme data scarce cases where only 1% of training data is available to train the whole system. As shown in the newly added Table 3, we find LaMer performs well compared with other baselines. The reason could be that the other baselines (except zero-shot GPT-3) all require external systems trained on ample data. For example, TSF-DelRetGen requires training a classifier to locate style-carrying spans of text. STRAP fine-tunes a GPT-2 model to learn how to paraphrase. IMaT trains a plain Seq2Seq model from scratch as an LM (not pre-trained). All these methods show limited performance in data hungry cases, partially because the compromised external modules would propagate errors to following parts as they are coupled. LaMer instead, built upon pre-trained LMs, requires neither extra data or additional model training, and thus shows superior performance in few-shot cases. Of note, more unsupervised data will benefit LaMer, but we claim LaMer can do at least as good as many SotA methods in few-shot scenarios. We also include a zero-shot baseline (GPT-3) for reference. Please see revised Section 3.2 for more details about these new experiments.  \n\n- **Additional human evaluation.** Thanks for your suggestions. We have added human evaluation results to show how well SAS-aligned pairs are similar in content in newly added Table 5.\n\n- **Which version of BLEU?** We use BLEU-*ref* on multiple references (for sentiment and formality TST). We have added this detail to the paper (see Section 3.2 and footnote 5).\n\n- **Gap between in-house and officially reported results.** We have run their models several times and still cannot reach their reported results. We find we can either approach their reported ACC or BLEU but not at the same time. Similar observations can be found [here](https://github.com/luofuli/DualRL/issues/15), and [one published paper](https://aclanthology.org/2020.inlg-1.25.pdf) (see Table 2, Dual RL, h-BLEU = 17.71, which is BLEU-*ref*) [1]. Finally, we pick the combination with relatively higher acc and lower BLEU (which also leads to the highest GM). We are happy to update this result if you have further suggestions!\n\n- **Why RD has good performance (in ACC, and sometimes in BLEU)?** RD means the source and target sentences are randomly mapped, which means if we train a text2text LM on such data, it can still be aware of opposite styles (good ACC) but has weak supervision to learn how to maintain style-independent content. Our results actually echo this hypothesis: In sentiment and formality TST, LM achieves much higher BLEU than RD. For political stance TST, we find mere LM alignment is not powerful enough---certain entities that appear in scene graphs seem to be very important---LM + SAS brings the most benefit in this case. The reason why RD sometimes can still lead to good BLEU could be the unsupervised data may have many duplicated sentences already, which is especially true in sentiment TST. Please take a look at Table 7 in the appendix: We analyze the SAS before (RD) and after SAS alignment, and compare it with that of human references. In sentiment TST, even though we do not align the data, the SAS of RD (0.55) already approaches that of human references (0.73), which are already pairs in parallel. Training a text2text LM on such \"already weakly parallel\" (though because of duplication) data, and with our IL refinement (upweighting really good parallel but penalizing bad ones) could probably lead to good BLEU. It becomes chanllenging in political stance TST, which RD has inferior BLEU and overall performance.\n\n- **SAS is helpful for content preservation?** In our revised Table 1 we have used GM to measure the overall performance of TST models. Given this new setting, we have updated some records in the table to show the best GM combinations. In this new setting, SAS shows an obvious boost on BLEU. Actually in our original paper, SAS will lead to BLEU increase in two out of three tasks (formality and political, which are the more challenging TST tasks).\n\n- **A new baseline.** Thanks for your suggestion. We have included this work as one of our baselines. Please take a look at revised Table 1, and newly added Tables 2, 3.\n\nFinally, we thank the reviewer for the thoughtful comments and constructive suggestions!\n\n[1] [Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer](https://aclanthology.org/2020.inlg-1.25.pdf)\n\n\ncomment: We thank all the reviewers for their valuable comments and constructive feedback. We are glad that reviewers found strengths in our paper’s novelty, experimental procedure, and contribution to the community. Different from existing methods, LaMer uses scene graphs to align sentences from different styles, requiring neither extra data nor additional systems training. \n\nWe have revised the paper according to the suggestions (highlighted in bronze in the paper). We summarize the highlights from the revision below and address each reviewer’s feedback separately as well. \n\n- **The performance of LaMer in few-shot cases (R1, R5).** To study the performance of LaMer in extreme data hungry cases, we run new experiments in few-shot scenarios (where only 1% of training data is available). We compare LaMer with several LM-based baselines and the GPT-3 based zero-shot TST method. We present the results in Table 2, and discuss the main differences between LaMer and other methods in Table 3. See our revised evaluation part (section 3.2) for the few-shot experiments.\n\n- **The evidence that there is self-parallelism within unsupervised data (R4, R5).** We justify our assumption regarding self-parallelism in certain unsupervised datasets through empirical observation on the data: In Table 7 (located in the appendix) we have shown that if NO SAS alignment happens (RD), there would be huge discrepancy on the average SAS score between training data and human references (e.g., in Formality TST, 0.29 with aligned by SAS and 0.01 without alignment). Since the human references are already parallel style transfer sentences, our success on the alignment demonstrates that there is self-parallelism existing in the unsupervised data. Additional human evaluation confirms that our aligned sentences are indeed similar in style-independent content---please take a look at Table 5 and related discussion.\n\n- **Additional Human judgement about scene graph alignment (R1, R2, R3).** We have added the additional human evaluation results about how well the scene graph based alignment works based on human judgement in Table 5. We also added new statistics about human ratings in Table 6.\n\n- **Qualitative demonstration of scene graph entities, procedure of the parsing, and the scene graph parser we used (R2, R3, R4).**\n We have added a visualization of scene graph alignment (Figure 2), described the parsing procedure (revised section 2.2), and noted the parser that we used (footnote 1). \n\n- **New baseline (R1).** IMaT was added as an additional baseline; We have included its results in Tables 1, 2, and 3.\n\n- **Better evaluation metrics (R3, R4, R5).** Recent TST have pointed out flaws of commonly used evaluation metrics (such as BELU). We evaluate LaMer’s performance with the more advanced $J$-score framework recently proposed by [1] in Table 2, while we still keep the ACC and BLEU in Table 1 for easy comparison with other baselines. We also added GM (geometric mean of ACC and BLEU) as an overall metric to better present our results (shown in revised Table 1).\n\n- **Scope of this work (R4, R5).** We have added additional discussion in section 2.2 regarding the scope of our SAS alignment and describe how other non-attribute-based alignment methods proposed by LaMer (such as multilingual LM alignment) can be used for TST scenarios where entities may not be shared across styles (e.g., multilingual TST, or ancient to modern English TST).\n\n- **Minor fix on presentations (Thanks all reviewers!).** We have fixed the links to the appendix and fixed several typos.\n\nWe hope our revised paper and additional experiments can provide further clarity. Please let us know if any improvement can be made!\n\n\n[1] [Reformulating Unsupervised Style Transfer as Paraphrase Generation](https://arxiv.org/pdf/2010.05700.pdf)\n"
            }
        ]
    },
    {
        "id": "ucASPPD9GKN",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Is Homophily a Necessity for Graph Neural Networks?\nKeywords: No keywords\nAbstract: Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification,  GNNs are widely believed to work well due to the homophily assumption (``like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance.  We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations.  Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding."
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: The authors characterize the conditions and provide supporting theoretical understanding and empirical observations of the conditions that GCNs can achieve strong performance on heterophilous graphs.\n\n\nmain_review: Strength:\n\nThe authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results.\n\nWeakness&Advice:\n\n1. In the last line of page 2, where is $X$ in the equation?\n\n2. The results in table 1 is not consistent with my personal experience even with the hyperparameter tuning range provided in appendix D. Could you please provide more details about the settings for table 1?\n\n3. Kenta’s work does not drop non-linearity in their analysis.\n\n4.  Could you please clarify the significance of theorem 1? What is its relation with heterophily and homophily in definition 1?\n\n5. Why do you use cosine similarity to define inter-class distance? What is its advantages over other metrics?\n\n6. Font size in figure 5 is too small. The dark blue background make it hard to read the value on it.\n\n7. The writing of this paper is not satisfying. I suggest the authors to cut the long paragraphs into smaller pieces, e.g. section 3.3.1, so that it is more reader friendly. It’s better to re-organize the paper, especially for the content after section 3.1.\n\n8. Can your theoretical analysis on CSBM generalize to multiple classes or to more general graphs.\n\nsummary_of_the_review: The novelty and significance are OK but the writing is not satisfactory. I'll consider raising my score if the authors can address my concerns properly.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 2:\nsummary_of_the_paper: The paper revisited the performance of GCN on graph with heterophily and provide negative evidence that heterophily does not always result in the poor performance of GCN, which contradicts with the assumptions/observations of many previous papers. They demonstrated that the GCN embeddings are still label-distinguishable on a special type of graphs with assumptions that the nodes with same labels have the same node feature distribution as well as the same neighborhood label distributions. They theoretically analyzed the case of CSBM model with two classes, and also empirically investigated on synthetic/real-world graphs with multiple classes. The conclusion is that GCNs can achieve good performance on heterophilous graphs under certain conditions. \n\nmain_review: Strength:\n1.\tIt provides a deeper insights into the heterophily problem and help us understand the GCNs’ performance.\n2.\tIt also designs a new cross-class neighborhood similarity metric to help explain the performance of GCN on various graphs (although it is not perfect).\n3.\tThe writing is generally clear.\n\n\nWeakness and Questions:\n1.\tThe analysis is only limited to GCNs, while the paper title is too general (GNNs). Not very \n2.\tOne concern is that the performance of GCN on Chameleon and Squirrel (Table1) differs a lot from the one reported in other papers (e.g. Geom-GCN (Pei et al. 2020), H2GNN (Zhu et al. 2020)). It seems the settings are the same as Pei et al. 2020, why is the result on these two datasets so different (2 to 3 times different)? Generally, I do not think the hyperprameter tunning should impact so much. Could the authors explain more details about it? In fact I also run some experiments on those datasets before and I cannot get those high numbers either.\n3.\tThe cross-class neighborhood similarity metric is intuitive and a good idea. However, it lacks of a direct theoretic connection to GCNs’ performance. Same as the heterophily metric, I do not think it can completely decide the GCNs’ performance, because the node feature distribution is also important here.  In fact, the assumptions in Theorem 1 are quite strong. If the nodes with the same label are sampled from the same feature distribution, it means generally MLP can also have a good performance. When this assumption does not meet, the analysis will become very complex. That is why I think Figure5 may be not enough to explain everything (but it is still interesting to see this empirical result). \n4.\tAlthough Theorem 1 seems correct to me, I have a question here. Assume we have a separate node with 0 neighbors, that means the upper bound here is 0. It is obviously not true. So, how to explain this exception?\n\n\nsummary_of_the_review: The paper provides a new perspective of heterophily and GCNs, but there remains some concerns both in the theoretic part and in the experiments.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: This paper revisits the common belief in prior works, “GCNs require strong homophily assumption.” The paper claims that even under low homophily, if the nodes of each label have distinguishable neighbor label distributions, GCNs can learn distinguishable node representations. To prove this claim, the authors (1) show that GCNs outperform representative baselines on two heterophilous graphs, (2) theoretically analyze the relation between homophily and decision boundary on CSBM with two classes, (3) conduct experiments on synthetic benchmarks created by distinct/random edge addition, and (4) analyze real-world benchmarks qualitatively and quantitatively with the lens of (3).\n\n\nmain_review: This work proposes a novel perspective on learning node representations under various homophily. Considering how recent GNN papers on heterophily treat GCNs (as the weakest baseline), we can say that empirical results by this paper are groundbreaking. However, this paper has several flaws which should be fixed/justified before publication.\n\nFirst, the Table 1 results are not convincing. We now agree that GCNs can learn good representations under some conditions on neighbor label distributions. However, this cannot justify how GCNs outperform other models. Does it not hold for other specialized architectures (H2GCN, CPGNN, GRRGNN)? Or, how about simple other baselines such as GraphSAGE and GAT?\n\nRelated to the first point, is there any reason for GCNs’ supremacy other than hyperparameter tuning? I have read Appendix D.4, and it is a standard procedure with reasonable computational budgets. Does it mean that other related works were doing something wrong? What is the magic here?\n\nSecond, the edge addition algorithm does not control the degree distribution; it monotonically increases the degree of nodes. In Theorem 1, the probability is bounded with singular value, feature dimension, and degree. However, the edge addition algorithm adds edges, and the average degree always increases; thus, whole experiments in Figure 3 do not control the key variate. Does it contribute to getting over-optimistic results for a high degree (or low homophily) regime? \n\nThird, the two-class graph results seem to be insufficient for the main claim. Theorem 2 is about the relation between decision boundary and degree, $p$, and $q$, and this is just one example of a graph with distinguishable neighbor label patterns. Can we say that Theorem 2 *theoretically supports* the paper’s argument? This can work just because predicting one can be reducible to predicting the other in a two-class problem, might not because of the distinguishability. \n\nFourth, the degree distribution $\\mathcal{D}_c$ for edge addition is clearly distinguishable to each other, but only one specific instance has been experimented with (i.e., having neighbors of two other classes other than itself). Is there any specific reason to experiment with one pattern across all datasets? Can we confirm that these empirical results generalize to various neighbor label distributions? For extreme cases, how does the model perform if the node connects to a single different label, or all labels except for itself (with the different number of classes)?\n\n\nsummary_of_the_review: The authors discuss the interesting question (in the title) and their answers in theoretical and empirical ways. The paper has merits but also has the following weaknesses:\n\n- Table 1 and the following description cannot justify how GCNs outperform other models.\n- The edge addition algorithm does not control the degrees, which is the core control variate for the experiments.\n- The two-class graph results seem to be insufficient for the main claim.\n- Only one specific instance of the degree distribution $\\mathcal{D}_c$ for edge addition is used for the experiments.\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: The paper discusses the relationships between the performance of GNNs for semi-supervised node-classification taaks and label-homophily/heterophily in the underlying graph structure. \nThe authors show that the classic CGN architecture can perform well on certain label heterophilic graphs -- which they argue is disputed heavily in the literature -- and provide a theoretical discussion on when a good performance for CGN on such heterophilic graphs is possible. \n\nmain_review: The paper aims to provide a more careful assessment about the role of label-homophily for graph neural network. Given the research activity  in this area recently, I think this is an interesting topic. As the authors outline, there appears to be indeed some \"folklore knowledge\" about label-homophily present in the literature which is probably not entirely accurate. Establishing a more precise understanding on when CGNs can work well, and when not is indeed useful.\n\nHowever, what I find problematic is that the authors claim that the current understanding in the literature is that homophily would be a necessity for CGN to perform well (i.e., a necessary condition for CGNs to work). This feels to much like a straw man argument for me; I think in most cases the argument is that heterophily can be problematic for CGNs -- and indeed the authors confirm this as well: \"bad homophily\" does indeed exist, as they say. The claim that there was some kind of consensus that CGNs can never perform well in label-heterophilic graphs is too far reaching to me. \nThe authors have to be very careful here to not throw out the baby with the bathwater. There is merit in what the authors do: clarifying when CGNs can perform well (even under heterophily); however, that does not mean that there are no issues with heterophilic graphs in general. I strongly suggest the authors reconsider their phrasing in certain areas. \n\nIn a similar vein, I believe the paper would benefit from a more precise treatment of certain aspects of homophily as well. Basically, the authors consider a homophily definition at the level of node-labels; they also assume that there is a tight correlation between the node labels and the underlying node features; so a label-homophily would translate into a feature homophily (or heterophily) as well. I think some more in depth discussion in this context about the relation between the correlation of features of neighbors and labels would be helpful. \n\nAt a high level, CGNs act by aggregating (averaging) the features of the node neighbors. If those features in the neighborhood are correlated (or indeed anti-correlated) to the target node label, then CGNs will be able to pick out the correct relation. This is true for fully homophilic graphs -- in which there is a strong positive correlation between the neighborhood features and the true node label -- and, e.g., for the bipartite example the authors present, in which case there is a strong anti-correlation (which works for prediction as well). However, extending the bipartite examples to multiple classes is already much more complicated: it may well happen that there is no clear label-to-feature correspondence any more that can be exploited. Some aspects of this are already alluded to in the discussion on the embeddings -- but I believe need to be more clearly articulated.\n\nSome further technical comments:\n\nThe CGN architecture described by the authors has no self-embedding component; this is an important point as it leads to \"no mixing\" behavior of the features / labels the authors describe for the toy example. Most convolutional architectures of GNNs have however at least some diagonal weights / self-embeddings -- which will inevitably lead to a stronger mixing between features of nodes of different classes. Again it is the correlation structure between node-labels (across the graph) and node-features (features to labels and features across the graph) that will come into play here and self-loops may help or hinder here.\n\nWhile the probabilistic analysis provided with the SBMs is clearly better than simply assuming constant features; performing the analysis in expectation is in this case basically the same as the deterministic construction discussed at the end of page 3, augmented with a bound on the deviation bound (which is only available here because of the bounded feature assumption it seems)\n\nThere are at least some problematic aspects with this analysis:\n1) the SBM and alike models are not a good model for sparse graphs, as they exhibit a graphon like structure and lead to dense graphs -- which is problematic as we often consider sparse graphs in practise; this limitation needs to be at least discussed I think.\n\n2) the assumption that the every node with the same class label has the same feature distribution (thus providing a perfect probabilistic coupling between labels and features) and the same neighbor label distribution and that each feature is indeed independent seems very unrealistic. This would lead basically to a problem in which a single inspection of the neighborhood suffices -- there is (almost) no graph structure necessary here really. \n\n3) Dropping the nonlinearity, while done in practice, is clearly another strong simplification which would merit some more discussion I think.\n\nFinally, the numerical experiments paint a much richer picture than the theoretical analysis and indeed the introduction of the paper. Some of these aspects could also be discussed in the theoretical parts (neighborhood similarity and correlation structure) -- which would lead to a more balanced picture. As it stands the introduction and theory parts make the paper feel a bit one-sides (to exaggerate: \"heterophily is not a problem\"); whereas in the end the conclusion is much more nuanced. I think such a more nuanced appraisal of heterophily/homophily is useful throughout.\n\nMinor comments\np.2 last line -- X is never defined, instead H and H' are used as input/output. Please correct.\n\n\n\nsummary_of_the_review: Overall I see the paper slightly above the line, provided some of the above points are addressed (I think this is possible).\n\nThe paper should be improved with respect to the following points (see above for more detailed suggestions and comments)\na) provide a more nuanced appraisal of literature and the role of homophily/heterophiliy\nb) discuss the roles of the correlation structure between node labels and features more clearly\nc) discuss the role of self-loops/ego embedding in mixing information from different classes\nd) provide a more rigorous discussion on the assumptions and shortcomings of the theoretical analysis and the SBM model.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: Hi Susheel,\n\nThanks for bringing this related work. We have included and discussed it in our revision. In the KDD'21 work, the authors found that real-world graphs typically have very diverse local assortativity and GNNs generally work better for nodes with high local assortativity. Hence, to break this limit, the authors construct a new computational graph based on structural similarity (following [2]) and the proximity in the original graph. The generated computational graph is shown to have generally higher local assortativity. The authors then proposed a new GNN framework on this graph, which improves the classification performance. Overall, the KDD'21 work aims to design new frameworks to achieve stronger performance. Our work stands from a very different perspective.  We demonstrate the existence of the \"good\" heterophily where GCNs can achieve reasonable performance. Our work carefully characterizes the implications of different heterophily conditions and provides supporting theoretical understanding and empirical observations.\n\nRegards,\nICLR 2022 Conference Paper4711 Authors\n\ncomment: \n--- \nQ6: the assumption that every node with the same class label has the same feature distribution (thus providing a perfect probabilistic coupling between labels and features) and the same neighbor label distribution and that each feature is indeed independent seems very unrealistic. This would lead basically to a problem in which a single inspection of the neighborhood suffices -- there is (almost) no graph structure necessary here really.\n\nA6: Note that our goal in this paper is to characterize the graphs where GCNs can perform well and these assumptions help such analysis. We provide the rationale for each of the motioned three assumptions.  \n\n1) The assumption that \"every node with the same class label has the same feature distribution\" is commonly adopted in GCN/graph literature [1,2,3]. These distributions we assumed are conditional probability distributions, which can be formulated as  $p({\\bf x}|y_c)$ for a given label $y_c$, where ${\\bf x}$ denotes features (for those nodes with label $y_c$). It assumes that there exists some kind of correlation between the label and features, which is typically required for learning classification models. For example, the generative models for classification aims to estimate such conditional probability distribution $p({\\bf x}|y_c)$ for all $y_c$ from the given data.\n\n2) The assumption for neighborhood distribution is directly assumed to characterize those graphs (no matter homophily or heterophily) where the GCNs can perform reasonably well, i.e, this is part of our contribution. \n\n3) The assumption that \"each feature is indeed independent\" is also adopted in some other machine learning methods such as the Naive Bayes classifier. We agree that this assumption is relatively unrealistic. We add a discussion on this limitation in the Section of the Conclusion. \n\nNote that in the classification task, we are only given the features but not the labels as we are predicting the labels. Hence, a single inspection of the neighborhood (if the reviewer means to count the labels of neighboring nodes) is not enough for classification. We need to aggregate the features and combine them for classification, which is what GCN does. The neighborhood information is an important kind of graph structure information. The GCN model indeed captures such neighborhood information. \n\n[1] Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. ICML 2021. \n\n[2] Contextual Stochastic Block Models. NeruIPs 2018. \n\n[3] Adaptive Universal Generalized Pagerank Graph Neural Network. ICLR 2021\n\n---\n\nQ7: Dropping the nonlinearity, while done in practice, is clearly another strong simplification that would merit some more discussion I think.\n\nA7:   In this paper, our analysis mainly focuses on understanding the aggregation part of the GCN model. Hence, we follow some previous works to drop the non-linearity. Furthermore, the empirical results demonstrate that our analysis is likely to be valid even with non-linearity. Nonetheless, we see the dropping of the non-linearity as a limitation of our work and discuss this limitation in the Section of the Conclusion.   \n\n---\n\nQ8: Finally, the numerical experiments paint a much richer picture than the theoretical analysis and indeed the introduction of the paper. Some of these aspects could also be discussed in the theoretical parts (neighborhood similarity and correlation structure) -- which would lead to a more balanced picture. As it stands the introduction and theory parts make the paper feel a bit one-sided (to exaggerate: \"heterophily is not a problem\"); whereas in the end, the conclusion is much more nuanced. I think such a more nuanced appraisal of heterophily/homophily is useful throughout.\n\nA8: Thanks for the suggestion. We have adjusted the introduction to make it more balance. Specifically, we now clearly state that there exist both \"good\" and \"bad\" heterophily in the introduction. We also correspondingly adjusted the theory part to make it more balanced. The modifications are highlighted in blue. \n\n---\n\nQ9: Minor comments p.2 last line -- X is never defined, instead H and H' are used as input/output. Please correct.  \n\nA9: Thanks for pointing this out. We have corrected this typo. \n\n--- \nWe believe that we have responded to and addressed all your concerns with our revisions. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\n\ncomment: A3: Thanks for the insightful comments. We discuss how ego-embedding may affect our analysis. For GCN, the ego-embedding is usually included by weighted average, i.e, the features of the target node are treated equally as all its neighbors. Intuitively, during the aggregation, the importance of the features of a target node $i$ is only $1/(deg(i)+1)$, where $deg(i)$ denotes the degree of node $i$. Hence, the ego-embedding will not affect the analysis very significantly for those nodes with a large degree. It will impact low-degree nodes more. For methods such as GraphSage, which combines ego-embedding and neighborhood embedding through concatenation (and transformation), further analyses needed to be extended from our current results. In this case, ego-embedding is treated equally as the entire neighborhood information. The neighborhood information is typically aggregated by feature average, which is what we discussed in this paper. Hence, our analysis in this paper can be directly utilized for analyzing methods such as GraphSage. As mentioned by the reviewer, combining ego-embedding and neighborhood embedding could be either good or bad. We take different heterophilous kinds of graphs for discussion. For graphs with ``bad heterophily'' (for example, nodes from the same label follow different neighborhood distribution patterns), the neighborhood information is generally not helpful for classification. Then, in this case, methods such as GraphSage may achieve stronger performance as it at least has its own ego-embedding playing an important role in the final features while in the GCN-like method the ego-embedding information is buried in the neighborhood information. We think this is definitely worth more discussion and investigation, which we leave for future work.  \n\n--- \nQ4: While the probabilistic analysis provided with the SBMs is clearly better than simply assuming constant features; performing the analysis in expectation is in this case basically the same as the deterministic construction discussed at the end of page 3, augmented with a bound on the deviation bound (which is only available here because of the bounded feature assumption it seems)\n\nA4: We agree that performing the analysis for CSBM in expectation is similar to the deterministic construction discussed at the end of page 3. However, our analysis is not solely based on expectations. Specifically, Theorem 2 compares the misclassification probability for any node $i$ before and after the GCN operation. This analysis involves the distribution of the output features (not only its expectation). More specifically, the expectations of the two classes are utilized to determine the distance between the two classes (inter-class distance). The larger the inter-class distance, the easier the classification is. The standard deviation of the distribution of the output features determines the intra-class distance. The smaller the standard deviation is, the closer the embeddings for same-labeled nodes are (this can roughly be understood as a stronger correlation). In fact, in the proof of Theorem 2, we demonstrate that, after GCN operation, the inter-class distance is reduced by a factor of $|p-q|/(p+q)$ and the standard deviation for the feature distribution (for features output from GCN) is reduced by a factor of $\\sqrt{deg(i)}$. Roughly speaking, when the effect of reducing standard deviation is more significant than the reduction of inter-class distance, the GCN model can help reduce the misclassification rate. This is how we reached the threshold in Theorem 2. Detailed proof can be found in Appendix B. In summary, the probabilistic analysis with CSBM provided more precise and formal characterization for GCN's performance than those in \"the deterministic construction discussed at the end of page 3\".  \n\n---\n\nQ5: the SBM and alike models are not a good model for sparse graphs, as they exhibit a graphon-like structure and lead to dense graphs -- which is problematic as we often consider sparse graphs in practice; this limitation needs to be at least discussed I think.\n\nA5: We adopt the SBM model to make the analysis more feasible since many of its key properties can be conveniently controlled. Also, it is widely adopted for investigating algorithms on graphs. We agree that SBM model is not very ideal for modeling sparse graphs. We discuss this limitation in the Section of the Conclusion. We also agree that it is important to perform analysis for more general graphs, which will be more complicated and a bit out of the scope of this paper as it is only an initial piece of this research direction. We generally believe our analysis provides a solid step towards deeper analysis on more general graphs.\n\n\ncomment: Q2: In a similar vein, I believe the paper would benefit from a more precise treatment of certain aspects of homophily as well. Basically, the authors consider a homophily definition at the level of node-labels; they also assume that there is a tight correlation between the node labels and the underlying node features; so a label-homophily would translate into a feature homophily (or heterophily) as well. I think some more in depth discussion in this context about the relation between the correlation of features of neighbors and labels would be helpful.\n\nAt a high level, CGNs act by aggregating (averaging) the features of the node neighbors. If those features in the neighborhood are correlated (or indeed anti-correlated) to the target node label, then CGNs will be able to pick out the correct relation. This is true for fully homophilic graphs -- in which there is a strong positive correlation between the neighborhood features and the true node label -- and, e.g., for the bipartite example the authors present, in which case there is a strong anti-correlation (which works for prediction as well). However, extending the bipartite examples to multiple classes is already much more complicated: it may well happen that there is no clear label-to-feature correspondence any more that can be exploited. Some aspects of this are already alluded to in the discussion on the embeddings -- but I believe need to be more clearly articulated.\n\nA2: Thanks for the insightful comments and discussion. We find these comments very helpful, which provides a new perspective to understand our paper. Indeed, the correlation between node features (before or after GCN) and labels are the key for classification performance. In this paper, we generally assume that the original features have some kind of correlation with the labels. However, we do not need to make the assumption that \"features in the neighborhood are correlated (or indeed anti-correlated) to the target node label\", which is not required for our analysis. For example, in the binary CSBM in Section 3.2, we assume the features of nodes with the same label are sampled from a label-specific Gaussian distribution. But we do not assume the correlation between the features of one class to the other class. The argument \" If those features in the neighborhood are correlated (or indeed anti-correlated) to the target node label, then CGNs will be able to pick out the correct relation.\" is not entirely precise. Indeed the correlation between the output features from GCN and the target node label is important for the classification performance. However, such correlation is not due to the correlation between the neighborhood features and the target node label. Instead, it is from the neighborhood distribution. The key idea in our analysis is that when same-labeled nodes follow the same neighborhood distribution, the aggregation process in GCN guarantees the correlation between the output feature from GCN and the target label. If same-labeled nodes do not follow the same neighborhood distribution, such correlation will be broken, i.e, nodes from the same label may be embedded in different areas in the output feature space. We emphasize that the correlation between the output features from GCN and the node label is due to the assumption of having the same neighborhood distribution for same-label nodes. It does not require the correlation between the center node label and the features of neighboring nodes. \n\nNote that our analysis and results are not limited to binary cases (or bipartite cases). They can be generalized to cases with multiple classes. Specifically, our analysis in Theorem 2 can be extended to multiple classes. We include the extended analysis in Appendix F. The empirical results in Section 3.2 are also based on cases with multiple classes. \n\n--- \n\nQ3: The CGN architecture described by the authors has no self-embedding component; this is an important point as it leads to \"no mixing\" behavior of the features / labels the authors describe for the toy example. Most convolutional architectures of GNNs have however at least some diagonal weights / self-embeddings -- which will inevitably lead to a stronger mixing between features of nodes of different classes. Again it is the correlation structure between node-labels (across the graph) and node-features (features to labels and features across the graph) that will come into play here and self-loops may help or hinder here.\n\n\n\ncomment: We thank the reviewer for providing insightful and constructive comments. We also appreciate the reviewer's recognization of the novelty and contribution of our work. We address the reviewer's comments/concerns as follows. \n\n---\n\nQ1: However, what I find problematic is that the authors claim that the current understanding in the literature is that homophily would be a necessity for CGN to perform well (i.e., a necessary condition for CGNs to work). This feels to much like a straw man argument for me; I think in most cases the argument is that heterophily can be problematic for CGNs -- and indeed the authors confirm this as well: \"bad homophily\" does indeed exist, as they say. The claim that there was some kind of consensus that CGNs can never perform well in label-heterophilic graphs is too far reaching to me. The authors have to be very careful here to not throw out the baby with the bathwater. There is merit in what the authors do: clarifying when CGNs can perform well (even under heterophily); however, that does not mean that there are no issues with heterophilic graphs in general. I strongly suggest the authors reconsider their phrasing in certain areas. \n\nA1: We have adjusted the contents in the introduction. More specifically, we change \"existing literature posits that strong homophily of the underlying graph is a necessity for GNNs to achieve good performance on SSNC\" to \"several existing literature posits that many GNNs implicitly assume strong homophily and homophily is critical for GNNs to achieve strong performance on SSNC\". The latter is widely spreading in the community and commonly claimed in existing literature [1,2,3,4,5]. Several other medications are also made in the abstraction and introduction to make our claim more accurate. These modifications are highlighted in blue. \n\n[1] Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. NeurIPs 2021\n\n[2] Graph Neural Networks with Heterophily, AAAI 2021. \n\n[3] Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. NeurIPs 2020. \n\n[4] Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. ECML-PKDD, 2021 \n\n[5] Adaptive Universal Generalized PageRank Graph Neural Network. ICLR, 2021. \n\n---\n\n\ncomment: ---\nQ5: Fourth, the degree distribution for edge addition is clearly distinguishable to each other, but only one specific instance has been experimented with (i.e., having neighbors of two other classes other than itself). Is there any specific reason to experiment with one pattern across all datasets? Can we confirm that these empirical results generalize to various neighbor label distributions? For extreme cases, how does the model perform if the node connects to a single different label, or all labels except for itself (with the different number of classes)?\n\nA5:  Our analysis is for general neighborhood distributions and it is not limited to the patterns we adopt in the paper. We choose such a circulant pattern for both datasets for the convenience of description. Note that it is impractical to enumerate all possible neighborhood distributions. To demonstrate that the analysis holds for other neighborhood distributions patterns, we provide more experiments with other neighborhood distribution patterns. These patterns include two additional neighborhood distribution patterns for both datasets and also the two extreme cases mentioned by the reviewer. A detailed description of the experiments and the results can be found in Appendix H.2 of the revised paper. \n\nIn Section H.2.1, we provide the results for the two additional sets of neighborhood distribution patterns ( check the detailed neighborhood distributions in Section H.2.1). For both Cora and Citeseer, the behaviors for these graphs generated following these new neighborhood distribution patterns are similar to those we observed in Figure 3 (the black curves). More specifically, we can clearly observe the $V$-shape curve under these new settings. Please check the performance of GCNs for graphs generated with these distributions in Section H.2.1 (Figure 13 and Figure 14 for Cora; Figure 15 and Figure 16 for Citeseer).\n\nIn Section H.2.2, we provide the results for the two extreme neighborhood patterns. We provide detailed information about these neighborhood distributions and detailed analyses of GCN's performance in Section H.2.2. We provide a concise summary of the observations and analysis here. In the extreme case of  \"node connects to a single different label\", the neighborhood distributions for different labels are highly distinguishable (no overlap between neighborhood distributions of different labels). Hence, the performance of GCNs is similar to those in Figure 3 (the black curves) and those in Figure 7 in Appendix C.3. Please check the performance of GCNs for graphs generated in this case in Figure 17 (for Cora) and Figure 19 (for Citeseer).  In the extreme case of  \"node connects to all labels except for itself\", we can still observe the $V$-shape curves as in Figure 3. However, we cannot achieve perfect performance even when we add an extremely large number of edges to the graphs (which is possible with other more distinguishable distribution patterns such as the one in Figure 7 in Appendix C.3). This is because, in this case, the neighborhood distributions of different labels are not easily distinguishable (much harder compared with the other extreme case). More specifically, any pair of classes share \"$C-2$ labels\" in their neighborhood distribution patterns with $C$ denoting the total number of labels (please check the distributions provided in Appendix H.2.2 for more details). In this case, the distinguishability of neighborhood distributions for different labels is not high enough (or the \"heterophily\" is not such \"good\"), and thus the performance of GCN is limited. This observation further demonstrates our key argument that the distinguishability of the distributions for different labels is important for GCN's performance. More detailed analysis and discussions can be found in H.2.2. The performance of GCNs for graphs generated in this case can be found in Figure 18 (for Cora) and in Figure 20 (for Citeseer).\n\n---\n\nWe believe that we have carefully responded to and addressed all your concerns with our revisions — in light of this, we hope you consider raising the score. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\n\ncomment: Q4: Third, the two-class graph results seem to be insufficient for the main claim. Theorem 2 is about the relation between decision boundary and degree, $p$ and $q$, and this is just one example of a graph with distinguishable neighbor label patterns. Can we say that Theorem 2 theoretically supports the paper’s argument? This can work just because predicting one can be reducible to predicting the other in a two-class problem, might not because of the distinguishability.\n\nA4: The two-class CSBM is the model we adopt to support our claim since we can conveniently control the distinguishability of the neighborhood distributions for the two classes through $p$ and $q$. Theorem 2 can support the paper's argument for graphs generated from the introduced binary CSBM. Note that Theorem 2 demonstrates when GCN can improve the linear separability based on the distinguishability of neighborhood distributions and the node degree (check detailed discussions in the text following Theorem 2). The GCN works not \"because predicting one can be reducible to predicting the other in a two-class problem\". There are binary classification cases where GCN does not work. The GCN can work only when the distinguishability between the distributions is ensured. For example, as we discussed in the paragraph following Theorem 2, for the CSBM, when $p=q$, the neighborhood distributions are completely indistinguishable. In this case, the GCN cannot help improve the separability at all. This clearly demonstrates that being a \"two-class problem\" is not the reason why it works and the distinguishability is important. Furthermore, the analysis can be extended to CSBM with multiple classes (we provide a proof sketch in Appendix F). We note that CSBM stands for a special type of graph model that is widely adopted in graph research. More efforts are needed to extend the analysis for more general graphs. We believe that our work provides a solid first step towards deeper analysis for more general cases.\n\n\n\ncomment: Q3: Second, the edge addition algorithm does not control the degree distribution; it monotonically increases the degree of nodes. In Theorem 1, the probability is bounded with singular value, feature dimension, and degree. However, the edge addition algorithm adds edges, and the average degree always increases; thus, whole experiments in Figure 3 do not control the key variate. Does it contribute to getting over-optimistic results for a high degree (or low homophily) regime?\n\nA3: Note that the experiment results in Section 3 are not meant to demonstrate Theorem 1. Our goal is to show that GCNs are actually able to achieve reasonable performance for both homophilous **and heterophilous** graphs if they follow certain assumptions as discussed in the two Observations. Furthermore, we also aim to demonstrate that there are both \"good\" heterophily and \"bad\" heterophily; GCNs are able to achieve strong performance for \"good\" heterophily but not for \"bad\" heterophily. Our results are presented to clearly demonstrate these claims. These claims themselves are not related to node degree since we do not aim to tell whether homophily or heterophily is better (in which case, we need to ensure a fair comparison by controlling the degree). No matter how the degree changes, our generated extremely heterophilous graphs (potentially with large degrees) are still heterophilous. So, if GCNs achieve good performance on these heterophilous graphs (specifically see the few right points in the black line in Figure 3), we believe it reasonably demonstrates our first claim that GCNs are able to achieve reasonable performance for some heterophilous graphs. Also, as discussed in Section 3.2.2, if we observe Figure 3 vertically, i.e, by comparing various graphs with the same homophily ratio, we can find the existence of \"good\" and \"bad\" heterophily over multiple graphs with the same average degree (all these graphs have the same number of edges added by definition). For example, on Cora, GCN's performances for graphs with homophily ratio $h=0.25$ varies a lot, ranging from very high (nearly 90\\%) accuracy to very low (nearly 40\\%) accuracy -- this further demonstrates the existence of  \"good\" and \"bad\" heterophily. \n\nWe agree that the degree is an important factor affecting GCN's performance as the degree impacts the intra-class distance. We also discussed this in Theorem 2 for the binary CSBM, which is further extended to multiple classes cases in Appendix F in the revision. Hence, to decouple the impact from the degree, we further conduct additional experiments with a controlled average degree in Appendix H.1. In this set of experiments, when generating new graphs, we control the number of edges to be the same as the corresponding original graphs. Thus, the average degree of the generated graphs will be the same as the original graph since the number of nodes for these graphs is the same. Specifically, while adding $K$ inter-class edges according to Algorithm 1, we correspondingly remove $K$ intra-class edges. We can then generate different graphs by varying the value of $K$. Note that $K$ is at most as large as the total number of intra-class edges in the original graph (in which case the generated graph will have a homophily ratio $0$). We demonstrate the GCN's performance on these graphs with the same degree in Figure 12 (Figure 12(a) for Cora; Figure 12(b) for Citeseer). Clearly, we can still observe the $V$-shape curve for both datasets. However, even when the homophily ratio is $0$, GCN cannot achieve perfect performance, which is different from what we observed in Figure 7 in Appendix C.3. This indeed demonstrates the impact of the degree to GCN's performance -- namely, that GCN is benefited by higher degrees -- and is consistent with the discussion in Observation 2, as well as Theorem 2 and the text following it. \n\n\n\ncomment: We appreciate the reviewer's recognition of the novelty and contribution of this paper. To address the reviewer's concerns/comments, we provide the following detailed responses. \n\n---\nQ1:  First, the Table 1 results are not convincing. We now agree that GCNs can learn good representations under some conditions on neighbor label distributions. However, this cannot justify how GCNs outperform other models. Does it not hold for other specialized architectures (H2GCN, CPGNN, GRRGNN)? Or, how about simple other baselines such as GraphSAGE and GAT?\n\nA1: The main goal of this paper is not to compare GCNs with other architectures. Instead, we aim to demonstrate that GCNs can achieve reasonable performance for some heterophilous graphs under certain conditions. Thus, the analysis in this paper mainly focuses on GCNs. The analysis we developed for the GCN model does not exactly hold for other specialized architectures (H2GCN, CPGNN, GPRGNN) or other simple models such as GraphSage and GAT, as they usually introduce different designs. For example, H2GNN introduces several key designs which make the model more complicated than simple aggregation. In GAT, the attention mechanism is utilized to determine the importance of different neighboring nodes. These designs make them capture different information from GCNs, which leads to different performance compared with GCNs (either better or worse than GCNs). Compared with GCNs, these designs may lead to either better or worse performance depending on the datasets and tasks. On the Chameleon and Squirrel, these other models potentially failed to capture as much useful information as GCNs and thus their performance is not as good as GCNs. We believe deeper investigations/analyses are needed to gain more understandings of these frameworks. However, such analysis requires a significant amount of additional effort and is out of the scope of this paper. We leave it for future work. \n\n--- \nQ2: Related to the first point, is there any reason for GCNs’ supremacy other than hyperparameter tuning? I have read Appendix D.4, and it is a standard procedure with reasonable computational budgets. Does it mean that other related works were doing something wrong? What is the magic here?\n\nA2: We clarify that differences in hyper-parameters may lead to the discrepancy of performances reported in our paper and those in the other related papers such as H2GNN. For example, the hyper-parameter of weight_decay has a significant impact on the GCN's performance. In the Chameleon dataset, when we adopt $5e-04$ as weight_decay  rather than $5e-07$  (the one we currently use), GCN's performance drops from about 68% to around 60% (close to the one reported in H2GNN). Similarly, GCN's performance on Squirrel drops to around 36% when adopting $5e-04$ instead of $5e-07$ as weight_decay. Hence, the search spaces of the hyper-parameters may be the potential reason why our results are different from other related papers.  For reproducing the results in Table 1, we have provided the code with detailed hyper-parameter settings in the following link (https://drive.google.com/file/d/17daB5ZvHEEJgVJawGTb6V-V1udLY445L/view?usp=sharing). We hope this could help clarify the confusion on results in Table 1. \n\n\n\ncomment:  Q3: The cross-class neighborhood similarity metric is intuitive and a good idea. However, it lacks of a direct theoretic connection to GCNs’ performance. Same as the heterophily metric, I do not think it can completely decide the GCNs’ performance, because the node feature distribution is also important here. In fact, the assumptions in Theorem 1 are quite strong. If the nodes with the same label are sampled from the same feature distribution, it means generally MLP can also have a good performance. When this assumption does not meet, the analysis will become very complex. That is why I think Figure 5 may be not enough to explain everything (but it is still interesting to see this empirical result). \n\nA3: Thanks for your recognition of the proposed cross-class neighborhood similarity metric. We agree that this metric cannot completely decide the GCN's performance, and the node feature distribution information also matters. In this paper, we limit our discussion to the case where node features are generally correlated to the node labels (or \"nodes with the same label are sampled from the same feature distribution\"). We clarify a few points about this assumption: 1) The assumption \"the nodes with the same label are sampled from the same feature distribution\" is commonly adopted in GCN/graph literature [1,2,3]. The distributions we assumed are conditional probability distributions, which can be formulated as  $p({\\bf x}|y_c)$ for a given label $y_c$, where ${\\bf x}$ denotes features (for those nodes with label $y_c$). It assumes that there exists some kind of correlation between the label and features, which is typically required for learning classification models. For example, generative models for classification aim to estimate such conditional probability distribution $p({\\bf x}|y_c)$ for all $y_c$ from the given data; and 2) Even when this assumption is satisfied, it does not mean the MLP can always achieve good performance. Specifically, the distinguishability of the distributions for different classes is also important -- if the feature distributions for different classes are not sufficiently different, then samples from different classes will be mixed together in the feature space and the classification performance of MLP will not be good. The inter-class neighborhood similarity empirically describes the distinguishability between the neighborhood label distributions, which directly impact the distinguishability of the distributions for the output features from the GCN model and thus impact GCN's classification performance. \n\nWhen the assumption ``nodes with the same label are sampled from the same feature distribution'' does not hold, the analysis would be much more complicated. In such cases, other necessary assumptions are needed. However, it is a bit out of the scope for the current paper, we leave it for future work.\n\n\n[1] Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. ICML 2021. \n\n[2] Contextual Stochastic Block Models. NeruIPs 2018. \n\n[3] Adaptive Universal Generalized Pagerank Graph Neural Network. ICLR 2021\n\n---\n\nQ4: Although Theorem 1 seems correct to me, I have a question here. Assume we have a separate node with 0 neighbors, that means the upper bound here is 0. It is obviously not true. So, how to explain this exception? \n\nA4: Note that the upper bound in Theorem 1 is $2 \\cdot l\\cdot \\exp \\left(-\\frac{ deg(i) t^{2}}{ 2\\rho^2({\\bf W})  B^2 l}\\right)$. Hence, for a separate node with $0$ neighbors, the upper bound is not $0$; it is $2\\cdot l$, which is the largest value the upper bound can reach. \n\n---\n\nWe believe that we have responded to and addressed all your concerns with our revisions — in light of this, we hope you consider raising the score. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\ncomment: We thank the reviewer for their efforts and time in reviewing our paper. Also, we appreciate the reviewer's perception of our key novelty and contributions. We provided detailed responses to the reviewer's comments/questions as follows. \n\n---\n\nQ1: The analysis is only limited to GCNs, while the paper title is too general (GNNs). Not very\n\nA1: We note that although our analyses are mainly for GCNs, a similar line of analysis may be used for more general message-passing neural networks (MPNNs). For example, the analysis could be directly helpful for analyzing some other GNN methods such as GraphSage. Specifically, for GraphSage, based on the current analysis, we only need to further consider the neighborhood sampling process and the combination of ego and neighbor features. Such extensions are important and valuable. Although our work mainly focuses on GCNs for simplicity in analysis, it provides a solid and important backbone for investigating other GNN methods as well. Such analysis may be nontrivial for other MPNNs which adopt more complicated designs. Hence, we decide to follow the reviewer's suggestion to change the reference to Graph Neural Networks to Graph Convolutional Networks in our title to keep the paper’s title in line with its contributions. However, changing the title of the paper through Openreview is not allowed during the rebuttal. We will update it in later revisions (camera-ready if accepted; re-submissions if rejected). \n\n---\n\nQ2: One concern is that the performance of GCN on Chameleon and Squirrel (Table1) differs a lot from the one reported in other papers (e.g. Geom-GCN (Pei et al. 2020), H2GNN (Zhu et al. 2020)). It seems the settings are the same as Pei et al. 2020, why is the result on these two datasets so different (2 to 3 times different)? Generally, I do not think the hyperprameter tunning should impact so much. Could the authors explain more details about it? In fact, I also run some experiments on those datasets before and I cannot get those high numbers either.\n\nA2: We indeed follow the settings in Geom-GCN (Pei et al. 2020).  Note that GCN's performances reported in the H2GNN paper are 36.89% (Squirrel) and 59.82% (Chameleon), which are already much higher than those reported in the Geom-GCN paper (23.96% and 28.18%, correspondingly). Our results are not extremely far away from those reported in the H2GNN paper (not 2-3 times). Differences in hyper-parameters can potentially lead to the discrepancy of performances reported in our paper and those in the H2GNN paper. For example, the hyper-parameter weight_decay can significantly affect the performance.  In the Chameleon dataset, when we adopt $5e-04$ as weight_decay than $5e-07$ (the one we currently use), GCN's performance drops from 68% to around 60%. Similarly, GCN's performance on Squirrel drops to around 36% when adopting $5e-04$ instead of $5e-07$ as weight_decay. To further clarify the confusion on the results in Table 1, we have provided the code with detailed hyper-parameter settings in the following link (https://drive.google.com/file/d/17daB5ZvHEEJgVJawGTb6V-V1udLY445L/view?usp=sharing) to reproduce the results. We hope this could help clarify the confusion on results in Table 1. \n\n\n\ncomment: Q8: Can your theoretical analysis on CSBM generalize to multiple classes or to more general graphs. \n\nA8: Yes -- we extended the analysis on the CSBM for multiple classes -- please check the detailed description in Appendix F. We also believe that it is possible to generalize the analysis to more general graphs. However, the analysis will be more complicated with more factors to be considered. We think such an extension is valuable, but out of the scope of this paper. Thus, we leave this investigation for future work.\n\n---\n\nWe believe that we have responded to and addressed all your concerns with our revisions — in light of this, we hope you consider raising your score.  Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\ncomment: We appreciate the reviewer's recognition of our novelty and contribution. We respond to the comments/concerns from the reviewer as follows. \n\n---\n\nQ1:  In the last line of page 2, where is ${\\bf X}$ in the equation?\n\nA1: Thanks for pointing this out. We have adjusted it correspondingly in the revised version of the paper. \n\n---\n\nQ2: The results in table 1 is not consistent with my personal experience even with the hyperparameter tuning range provided in appendix D. Could you please provide more details about the settings for table 1?\n\nA2: We have provided the code to reproduce GCN's results in Table 1. Specifically, we provide the code with detailed hyper-parameter settings in the following link (https://drive.google.com/file/d/17daB5ZvHEEJgVJawGTb6V-V1udLY445L/view?usp=sharing). We hope this could help clarify the confusion on results in Table 1. \n\n---\n\nQ3: Kenta’s work does not drop non-linearity in their analysis.\n\nA3:  Thanks for pointing this out. We have adjusted it correspondingly in the revised version of the paper. \n\n---\n\nQ4: Could you please clarify the significance of theorem 1? What is its relation with heterophily and homophily in definition 1? \n\nA4: Theorem 1 helps characterize the conditions for GCNs to achieve reasonable performance and correspondingly helps us understand the performance with heterophily and homophily. Particularly, it characterizes the output embeddings of the GCN model, which in turn impacts GCN's classification performance. More specifically, it bounds the intra-class distance for each class, i.e, the representations of all nodes with the same label are close to the expected representation of this class (demonstrated in Eq.(2)) with high probability (Eq.(3) demonstrates this argument). This is important as it ensures nodes with the same label to be likely classified into the same class. Then, to ensure that the classifier achieves strong performance, the expectations of different classes need to be distant from each other.  With Theorem 1, we can discuss GCN's performance with heterophily and homophily. More specifically, as mentioned in Observation 1, \"In homophilous graphs, the neighborhood distribution of nodes with the same label (w.l.o.g $c$) can be approximately regarded as a highly skewed discrete $\\mathcal{D}_{c}$\", i.e, homophilous graphs always (approximately) satisfy the neighborhood distribution condition in Theorem 1. Furthermore, different labels clearly have distinct distributions. These make GCN models typically achieve strong performance on homophilous graphs. On the other hand, as discussed in Observation 2, for heterophilous graphs satisfying the assumptions in Theorem 1, the GCN model is also able to achieve strong performance given that the distributions of different classes are ensured to be distinguishable. In summary, Theorem 1 helps characterize the existence of ``good'' heterophilous graphs where GCN can achieve strong performance. \n\n---\n\nQ5: Why do you use cosine similarity to define inter-class distance? What is its advantages over other metrics?\n\nA5: We utilize cosine similarity as it provides a simple, effective, and symmetric way to measure the similarity between two vectors. Other metrics can also be adopted. The choice of similarity measure would not affect the results and conclusions significantly. We empirically demonstrate this argument by investigating two other metrics: Euclidean distance and Hellinger distance. The heatmaps based on these two metrics for the Chameleon dataset are shown in Figure 11 in Appendix G. The patterns demonstrated in these two heatmaps are similar to those observed in Figure 5(b), where cosine similarity is adopted.  Note that a larger distance means lower similarity. Hence, the numbers in Figure 11 should be interpreted in the opposite way as those in Figure 5 (b). \n\n--- \n\nQ6: Font size in figure 5 is too small. The dark blue background make it hard to read the value on it.\n\nA6: Thanks for the suggestion. We have enlarged the font size. Also, we change the font color to white for those numbers in the cells with dark blue background. \n\n\n---\n\nQ7: The writing of this paper is not satisfying. I suggest the authors to cut the long paragraphs into smaller pieces, e.g. section 3.3.1, so that it is more reader-friendly. It’s better to re-organize the paper, especially for the content after section 3.1.\n\nA7: Thanks for the suggestion. We have cut Section 3.3.1 into two separated parts. We also reorganized Section 3.2 a bit by separating long paragraphs into pieces.  In both cases, we introduced some subheadings to preface the relevant discussions and break up the text. The updates are highlighted in blue in the revision. \n\n\n"
            }
        ]
    },
    {
        "id": "OM_lYiHXiCL",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis\nKeywords: No keywords\nAbstract: Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples,  which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution;  a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios"
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: This paper proposed an adversarial extreme value analysis (AEVA) framework to detect backdoors in black-box neural networks.\nSpecifically, they first obtained a new (upper bound) backdoor detection formulation by using convex relaxation. With linear model assumption and mean squared error loss, they showed that the mass in the adversarial perturbation would be occupied in the mask area as the backdoor sample size goes to infinity. Hence, a highly skewed distribution in the adversarial map for the infected label is expected.\nBased on the above observation, they designed an Aggregated Global Adversarial Peak (GAP) for detecting the adversarial maps. With the limitation of the black-box setting, a Monte Carlo-based gradient estimation was used in the GAP. \nFinally, they evaluated their methods on several real-world datasets.\n\nmain_review: I have several concerns as follow:\n\n- The relationship between adversarial examples and backdoor samples (e.g., pictures with auxiliary patches) is well known. Therefore, it is reasonable to expect that the adversarial singularity phenomenon may not occur in backdoor-infected DNNs. Will algorithm 1 be efficient under cases where the infected range and uninfected range overlap a lot and the threshold T is large (as discussed in Sec. 3.3)?\n\n- It seems that the black-box setting is somehow contrived since it only poses challenges on computing the gradient ∇φ(x,yt). What is the effect of the sample size used for gradient estimation on the detection accuracy? With exact gradient computations, will the proposed algorithm 1 (AGAP) outperform the existing white-box detection methods?\n\n- The result of Lemma 1 seems to be a natural consequence under the linear model assumption, mean-squared-error loss, and the optimization formulation of Eq. 6. Since this paper targets the adversarial extreme value analysis in DNN, do we have a more formal understanding of the regime of modern neural network architectures?\n\n\nsummary_of_the_review: Overall, I think the paper provides a new perspective of backdoor defense, but it could be made stronger by addressing some critical aspects as listed above.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 2:\nsummary_of_the_paper: The paper propose a new backdoor detection method. The method does not require the original poisoned training data or the parameters\nof the target DNNs. Given an image, the proposed method initiates adversarial attack on it. If the model contain backdoors for predicting the target label, then the adversarial noise will contain singular patterns, which could be detected using extreme value analysis.\n\nmain_review: Strengths:\n1. The method assumes the black-box scenario, which is practical.\n2. Some theoretical analysis is provided to establish the connection between adversary and backdoor attacks.\n3. The idea is smart for using adversarial attack results to detect backdoors.\n\nWeakness:\n1. It seems that it is not difficult to avoid being detected if the backdoor patterns are smartly designed.\n2. There should be some work using interpretation to detect the backdoor. Due to the close relation between adversarial attack and interpretation, I am not sure if the proposed method is still novel from this perspective.\n\nsummary_of_the_review: The paper proposes a good idea for using adversarial attack patterns to diagnose if backdoors exist in models. My only concerns are twofold. First, some adaptive studies could be conducted, analyzing the scenarios where the proposed detection method could be circumvented. This may not be very difficult since this paper assumes that the backdoor patterns are focused patches. Second, the proposed idea seems to be similar to using interpretation (e.g., heat maps) to detect backdoor, since adversarial attack = inversed interpretation. From this perspective, the proposed idea does not look that novel.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: This paper presents a novel approach for the detection of backdoored neural networks.\n\nInspired by the deployment of third-party networks in cloud services, it is argued that this detection task needs to be done in a black-box, hard-label scenario.\nIn this regime, it is assumed that the user/defender only has access to the model through making queries and getting back the labels for those queries.\nTo solve this newly proposed problem, first, a connection between the _detection of backdoors_ and _adversarial example generation with sparse perturbations_ is drawn.\nThen, it is shown that for backdoor models solving this adversarial example generation objective leads to perturbations that are mostly concentrated in the backdoor mask area.\nThis argument is shown both theoretically (for linear classifiers) and empirically (for deep neural networks).\n\nBased on this observation, which the paper calls the _adversarial singularity phenomenon_, a practical black-box backdoor detection is proposed.\nSpecifically, this algorithm first computes the aforementioned adversarial perturbations for some validation data in each class.\nTo this end, a Monte-Carlo gradient estimation is used to make the algorithm suitable for the black-box setting.\nThen, based on the maximum value of these perturbations, it is decided whether the network for this label is infected or not.\nThe effectiveness of this method, named AVEA, is shown through extensive experiments and ablation studies on various datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and attacks (BadNets [[1](https://arxiv.org/abs/1708.06733)], Label-consistent [[2](https://arxiv.org/abs/1912.02771)], Watermark [[3](https://arxiv.org/abs/1712.05526)], and Invisible [[4](https://arxiv.org/abs/1909.02742)] attack).\n\nmain_review: ### Strengths and Weaknesses:\n\n+ The proposed setting and algorithm are novel.\nIn particular, the subtle connection between backdoored networks and sparse adversarial example generation can inspire further research in this direction.\n\n+ The analysis in the paper is logical, and it guides the reader well through the thought-process behind the algorithm.\n\n+ The experimental results are comprehensive.\nAlthough the experiments do not contain the most recent backdoor detection methods as baselines, this reviewer believes that the current comparison baselines (neural cleanse [[5](https://ieeexplore.ieee.org/abstract/document/8835365)] and DL-TND [[6](https://arxiv.org/abs/2007.15802)]) are enough to provide the bigger picture.\nThis is since almost all existing methods are typically designed for the white-box scenario, and as such, they have a huge advantage compared to the current method.\n\n+ The paper is generally well-written, although some parts need proofreading.\nSee the minor comments about this below.\n\n+ There are two gray areas regarding this submission that needs to be clarified further:\n\n\t1. Although the authors talk about the practicality of the black-box scenario in the introduction, this assumption needs to be justified further.\nI strongly suggest providing a concrete example, where the use-case of this scenario is explained.\nIn this example, please specify the training data, the model, the user, and why the user who does not own anything from the training data to the model should be worried about backdoors?\n\n\t2. Another interesting question that is not been explored is the white-box performance of the algorithm.\nCurrently, the only part where the black-box scenario is being dealt with is the adversarial example generation.\nNow, assuming that the user has access to the model parameters, what does the current approach provide in contrast to existing methods?\nAsked the other way around, can someone use the Monte-Carlo gradient estimation in conjunction with existing methods to make them black-box?\nSince being \"black-box\" is considered as the strength of the proposed method compared to existing ones, these questions need to be answered.\n\n### Further Questions:\n\n1. Can you clarify what this sentence below Eq. (3) means: \"Additional difficulty comes from the fact that...\"?\n\n2. At the bottom of page 4 the paper reads: \"Following this, the optimization in Eq. (3) converts to...\" I am guessing that you meant only the first part of Eq. (3), right? Otherwise Eq. (4) is missing a $||m||$ term.\n\n3. For the empirical study of Section 3.2, do you solve Eq. (6) in a white-box setting, or using the Monte-Carlo gradient estimator? How about Figure 6, is this figure generated in the black-box or white-box setting?\n\n4. Does Eq. (7) mean that there are two ways to get the GAP: (1) by generating adversarial perturbations for multiple inputs (as the AVEA does) (2) by generating multiple perturbations for a single input?\n\n5. Does the ablation study on \"the impact of trigger size\" mean that a scattered trigger is more likely to circumvent the detector than a sparse trigger?\n\n6. The results reported in Appendix J indicate that in some cases the performance of the black-box detector is better than the white-box baselines, especially neural cleanse. What do you speculate is the reason behind this?\n\n### Minor Comments/Suggestions:\n\n+ Across the paper, the index \"$i$\" has been used to point to training samples (Eq. (2)), validation samples (Eq. (3)), and class (Algorithm 1).\nConsider using a different index for each one of these to avoid misunderstanding.\n \n+ Consider using different line styles and a bigger plot size for AUROC figures.\n\n+ Consider adding a table of contents and/or explanation of different parts of the Appendix. Right now there are sudden jumps from one section to the next with no explanations.\n\n+ In the second sentence after Eq. (15) $q$ has been used to point out the first and last dimensions of $X$. The second one needs to be changed to $p$.\n\n+ Omit the $'$ in Eq. (16).\n\n+ In Eq. (19) $N_b \\to \\infty$, not $N$.\n\n+ In the explanations that follow Eq. (24), $K$ is used instead of $k$.\n\n+ In Eq. (27) there is a $\\hat{S}$ missing in the second $\\log$.\n\n+ Figure 13 is colliding with the text.\n\nsummary_of_the_review: While there are some gray areas around the proposed method (please see the strengths and weaknesses), I believe that this is a well-written, thought-provoking paper that can be interesting to the community and bring forward fruitful discussions.\nAs such, I vote for borderline acceptance of the paper.\nIf the authors can provide convincing answers to my two questions in the main review, I would be happy to increase my score.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 4: The contributions are significant, and do not exist in prior works.\n\nempirical_novelty_and_significance: 4: The contributions are significant, and do not exist in prior works.\n\nReviewer 4:\nsummary_of_the_paper: This paper proposes the first black-box hard-label backdoor detection where the defender can only access the predicted label of queries. Specifically, the authors first prove that the detection problem is bounded by an `adversarial objective’, based on which they reveal the adversarial singularity phenomenon that the perturbation generated by the adversarial objective is highly skewed distributed. Based on this phenomenon, the authors propose the adversarial extreme value analysis (AEVA) to detect backdoors in a black-box manner. The authors verify the effectiveness of their method in defending against BadNets, Blended Attack, Label-Consistent Attack, and Invisible Attack on CIFAR-10, CIFAR-100, and TinyImageNet dataset.\n\nmain_review: \nPros\n1.\tThis paper is well-written and easy to follow.\n2.\tThe topic is of great significance and sufficient interest to ICLR audiences. In particular, black-box hard-label backdoor detection is probably the hardest problem in backdoor defense and has the highest commercial potential. As such, I think this paper should be encouraged, although it still has some problems. \n3.\tThe authors provide some theoretical supports for their method and open-source their codes, which should be encouraged.\n4.\tI appreciate that the adaptive attacks and potential limitations are also included in the paper. It can prevent readers from being overly optimistic about safety.\n\n\nIn general, I enjoy the reading of this paper and I think the proposed method is also moderately novel. However, I still have some concerns. I will increase my score if the authors can address my concerns. The detailed comments are as follows:\n\n\nMajor Comments\n1.\tMy main concern lies in the novelty of Lemma 2-3. The optimization problem (6) seems to be a classical problem. As such, I have to worry about whether its analysis (i.e., Lemma 2-3) is not new. I will not decrease my score even Lemma 2-3 are not new if the author can provide proper references and illustrations.\n2.\tI would like to see the results of defending against attacks with dynamic and sparse but not compact triggers. \n3.\tPlease analyze the effects of key hyper-parameters (e.g., lambada) involved in the proposed method.\n4.\tPlease provide more details and results in the analysis of potential adaptive attacks.\n\n\nMinor Comments\n1.\tPlease double check the reference format, especially that of conference papers. Please cite the official version of all papers (e.g., BadNets should be IEEE ACCESS, DL-TND should be ECCV). \n\n\nNote: I didn't check all proofs carefully. But the lemma seems reasonable and consistent with the experimental results.\n\n\nPost-rebuttal Comments:\nI would like to thank the authors for their clarifications. Since most of my concerns have been addressed, I increase my score to 8. \nPS: It would be better if you can point out whether there are similar theories in adversarial learning. I think it will help readers to better understand them and it will not reduce your contributions.\n\nsummary_of_the_review: A practical and novel backdoor detection with theoretical guarantees.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: We want to thank all reviewers for the detailed comments and constructive suggestions. We have updated a new version of our submission, which includes additional evaluation on dynamic and non-compact backdoor triggers (Appendix.K), detailed analysis on adaptive attacks, as well as improving the overall presentation of our paper according to the review comments. "
            }
        ]
    },
    {
        "id": "jT1EwXu-4hj",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation\nKeywords: Information retrieval, Learning theory, Causal inference, Missing data, Overlapping, Reweighting, Optimal transport\nAbstract: The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game.\nWe prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system."
            },
            {
                "round2": "Reviewer 1:\n1. The motivation is still not very clear. The authors still do not give a clear motivation of why this problem is important in real-world RL systems.\n\n2. Theorem 1 and Theorem 2 are only the finite sample bounds under $P$ but not the transfer generalization bound w.r.t the target distribution like Proposition 1. It is unclear to me why Theorem 1 and Theorem 2 can resolve the insufficient overlapping problem mentioned for using DA in the IR system. \nWhy does Theorem 2 state that the learned policy (the one that minimizes the transportation-constraint risk) is close to the oracle policy in terms of the target risk? Should the target risk be under $Q$?  I can not find any risk on $Q$ in Theorem 1 and 2.\n\n3. Thanks for your additional experiments and results.\n\nReviewer 2:\nsummary_of_the_paper: In this paper, the authors first point out that the requirement of efficiency of IW- and DA-based methods is the overlapping between the source domain and target domain. Meanwhile, theoretical analysis also shows that insufficient overlapping problems can cause the hardness or impossibility of IW- and DA-based learning methods. In light of this, this paper aims to propose a novel method to optimize recommendation results by solving such insufficient overlapping problems. Specifically, a principled transportation-constraint risk minimization objective function is devised to optimize the recommendation results, which is able to transport the learned patterns from the source domain to the intervention domain better. Then a two-layer adversarial model (GDA) is deployed to optimize the transportation-constraint risk minimization objective function, and sufficient analysis of the GDA is provided. Extensive experiments were conducted on both real datasets and semi-synthetic datasets to show the superiority of the proposed method.    \n\nmain_review: Pros: \n\t1. This paper identifies and tries to tackle an important problem in recommender systems - insufficient overlapping. The theoretical proof is given to show the uncertainty of the results.  \n\t2. Generally speaking, the paper is well-written and well-presented. Though many mathematical theorems are contained in this paper, it is easy to follow for readers.\n\t3. The proposed method is novel and can inspire future research in recommender systems. \n\t4. The experiments are sufficient to validate the claims.\n\nCons:\n\t1. There are some small typos that should be fixed before publication. \n\t\t1. In claim 1, E_Q R(f) should be E_Pw R(f).\n\t\t2. In Table 1, the Hit@10 of DT-MCF on LastFM is 81.81(20). I am confused as 20 is pretty high, and maybe here is (0.20) or (.02).\n\t\t3. In Section 5, we randomly sample m < |I| items and see if i' is among the top-K of these m items. items should be unrelated items. Also, the same questions occurred in other parts. I suggest to double-check that.\n\n\t2. \\mu is also an important hyperparameter, and authors also mentioned that a suitable \\mu will give us the opportunity to better explore the whole feedback data. I recommend that the parameter sensitivity of \\mu should be conducted. \n\t3. The main contribution is to address the insufficient overlapping problem that causes the uncertainty of IW and DA-based methods. However, in the experiment part, IW and DA-based methods are not compared with. I recommend that DT-IW and DT-WA should be included.\n\t4. The motivation is somehow unclear and the presentation of introduction could be further improved. A more detailed explanation about the relationships between intervention and domain adaption, intervention and the proposed method, why the proposed method can address insufficient overlapping problems, etc should be introduced. \n\n\nsummary_of_the_review: The novelty of this paper is high.  However, the motivation is not quite clear, and  more IW- and DA-based methods are expected to compare in the experiment.\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: This paper proposed a new perspective for recommender systems: rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. To optimize the recommendation algorithms in this setting, the authors proposed a transportation-constraint risk minimization objective and convert it to a two-player minimax game. Theoretical and empirical studies demonstrated the effectiveness of the proposed method. \n\nmain_review: The paper views recommender systems from a new perspective, which is similar but different to existing works in causal collaborative filtering. The theoretical analyses are comprehensive. The experimental studies are conducted in synthetic data, offline data and online data, which can well demonstrate the effectiveness of the proposed method.\n\nI have the following questions/concerns about this paper:\n1.\tIt seems to me that the proposed method can outperform IW methods when the overlap between the source and target domains are insufficient. However, there is no measurement about the insufficiency. It will be better to have some theoretical or empirical analysis about how the overlap between different domains can affect the performance.\n2.\tIt is hard to directly compare the bounds derived by the paper, e.g., between Theorem 3 and Proposition 1. This will be important to intuitively understand the advances of the proposed method.\n3.\tThe online evaluation used the Deep and Wide method. But the same method is not evaluated in the offline setting, which seems unreasonable.\n4.\tThe proposed method tries to optimize recommendation algorithms via a minimax game, which could be much more complicated than other baselines. Thus, it would be necessary to analyze the efficiency of the proposed method in the experiments.\n\n\nsummary_of_the_review: In summary, this paper targets at an interesting problem and shows promising theoretical and empirical results, which could be helpful for the community of recommender systems. \n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 4:\nsummary_of_the_paper: The authors propose a novel way to learn new recommendation deterministic policies from feedback collected under different recommendation policies.\nThe authors start by pointing out that the majority of current approaches, either Counterfactual (Inverse Propensity Weighting) either Domain Adaptation-based, \ndo not work well when the support of the current and future recommendation policies do not fully overlap (one of the main causes being that the current recommendation is deterministic and therefore not exploring all actions).\nTo this end, the authors propose a novel way of re-weighting and regularizing empirical risk that encourages the search for policies that have better reward (lower risk) and are still overlapping in evidence with the current policy.\nTo escape the support issue that is present in the existing approaches, the authors switch the regularization parameter to an IPM metric (WASSERSTEIN) that can handle non-overlapping supports.\n The authors provide both theoretical and empirical evidence that the resulting algorithm outperforms existing SOTA methods.\n\nmain_review: Strengths:\n* The subject of the paper is quite relevant and the identified issue of non-overlapping support for RW recommendations serious\n* The application of existing methods from causal inference to deterministic recommendation policies is valuable and the empirical section is well-developed and the results significant \n\nWeaknesses:\n* The motivation for the particular approach is not extremely clear. The authors build the argument around “transportability of patterns”, but I feel the point is never fully explained. If an action is never taken under a certain context in the past, (meaning is in the non-overlapping set), the algorithm will have to extrapolate the outcome. In order for the extrapolation to hold some assumptions need to be made about the reward model, and I find the authors do not really spend time on this.\n* The resulting objective is closely resembling the counterfactual loss proposed in [1], eq. 3. I think the authors should expand on this approach in the related work, and highlight that unlike in the cited work, they can choose the counterfactual world in which they want to act. Furthermore, I think borrowing either the counterfactual notation or the policy learning notation and vocabulary would help quite a lot in explaining the approach.\n\n\n\n\n\nsummary_of_the_review: Overall, I liked the paper and I found the approach interesting. I think the authors should link their approach a lot more to the work presented in [1] and also covered in two of their references [2,3].\nI think by doing so, it clarifies that the main difference in the problem domain is that we are in control of which of the counterfactual worlds we want to compute the risk in. \nOtherwise, all of the vocabulary and the core explanations of the soundness of the approach remain the same.\n\n[1] @inproceedings{shalit2017estimating,\n  title={Estimating individual treatment effect: generalization bounds and algorithms},\n  author={Shalit, Uri and Johansson, Fredrik D and Sontag, David},\n  booktitle={International Conference on Machine Learning},\n  pages={3076--3085},\n  year={2017},\n  organization={PMLR}\n}\n\n[2] F. D. Johansson, D. Sontag, and R. Ranganath. Support and invertibility in domain-invariant representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 527–536. PMLR, 2019. \n[3] F. D. Johansson, U. Shalit, N. Kallus, and D. Sontag. Generalization bounds and representation learning for estimation of potential outcomes and causal effects. arXiv preprint arXiv:2001.07426, 2020. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 5:\nsummary_of_the_paper: In this paper, the authors study the domain transportation issue in information retrieval (IR) systems. The main challenge is the insufficient overlapping problem. The authors propose a new adversarial learning method based on transportation-constraint risk minimization to address this challenge. The main insight behind this proposed method is that the interventional impact of recommendation is not exclusive to observed data, but also interferes with the target domain of interest. The authors first analyze the generalization bound of IW and DA methods, and then further give the consistency, generalization, and excessive risk bounds of the transportation-constraint risk minimization. The simulation, real-data analysis, and online experimentation show the effectiveness of the proposed method.\n\nmain_review: The paper is well written and easy to follow. Generally, the overall framework is novel. The authors give some theoretical analyses to the proposed transportation-constraint risk minimization. Extensive experiments are conducted to show the effectiveness of the proposed methods. \n\nMy major concerns and questions are as follows:\n\n(1) The motivation is not clear to me. One of the claims of the paper is that the proposed algorithm can address the insufficient overlapping problem. However, the authors do not give a clear motivation of why this problem is important in real-world RL systems and why the proposed methods can address this problem.\n\n(2) The theoretical analyses are not clear and lack interpretations.  Proposition 1 is good to show that IW and DA can not address the insufficient overlapping problem. However, Theorem 1 & 2, Claim 1 and Theorem 3 are not clear.  Specifically,  Theorem 1 and Theorem 2 are only the finite sample bounds under $P$ but not the transfer generalization bound w.r.t the target distribution $Q$ like Proposition 1. It is unclear to me why  Theorem 1 and  Theorem 2 can resolve the insufficient overlapping problem mentioned for using DA in the IR system. For Claim 1, I think the left part in the equation is $\\min_{d_{W}(P_{w}, P_{f}) \\leq\\rho}\\mathbb{E}_{P_w} R(f)$ \n\nnot the risk  $\\min_{d_{W}(P_{w}, ~P_{f}) \\leq \\rho}\\mathbb{E}_{Q} R(f)$ under the target $Q$. Thus the Claim 1 is also irrelevant to the target risk.  Theorem 3 indeed gives the generalization bound under the target domain, but it does not clearly show the proposed method can address the insufficient overlapping problem of IW and DA in Proposition 1.  Given above, I think the theoretical part is not well written and it would be great if the authors can provide some interpretations of the theoretical results. The presentation in these parts of the paper could be improved. \n\n\n(3) The literature review is not very good, and the context is missing about how the research work stands in the agenda of each subfield such as Wasserstein Robust Optimization, Unbiased Recommendation, Off-policy Learning and Causal Inference with Missing data. For instance, the authors miss some important related works [1-7] with deficient support in logging policy and unbiased learning in recommendation. The authors also need to discuss the differences between this paper and other Distributionally robust optimization papers with Wasserstein distance in terms of the generalization bounds. This paper does not make a clear connection with the presented theoretical results and contextualize this work in the existing literature. It is difficult to figure out what's new and what's already a part of the existing literature. \n\n(4) The experimental comparison is also not sufficient. This paper lacks some baselines and results on benchmark datasets.  The comparison should also be done with some DA [8] and advanced IW methods [2]. The authors also need to evaluate their methods on Yahoo and Coat datasets which are the benchmark datasets for counterfactual learning in the recommendation.  A more detailed analysis of the time complexity would be desirable.\n\n[1] Off-policy bandits with deficient support. KDD 2020.\n\n[2] Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random. ICML 2019.\n\n[3] A General Offline Reinforcement Learning Framework for Interactive Recommendation. AAAI 2021.\n\n[4] Distributionally Robust Counterfactual Risk Minimization. AAAI 2020.\n\n[5] Asymmetric Tri-training for Debiasing Missing-Not-At-Random Explicit Feedback. SIGIR 2020.\n\n[6] Distributionally Robust Policy Evaluation and Learning in Offline Contextual Bandits. ICML 2020.\n\n[7] A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data. SIGIR 2020.\n\n[8] Causal Embeddings for Recommendation. RecSys 2018.\n\n\nsummary_of_the_review:  I think this paper is potential. The proposed domain transportation strategy for the recommendation is quite interesting. I suggest the authors focus on this part and give more discussions about the motivation, interpretations, and related works. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: We thank the reviewer for the further response that helps to clarify the questions. We believe there are several misconceptions that cause the reviewer's confusion.\n\n1. Our work aims only at the IW- and DA-based IR methods where sufficient overlapping is a crucial presumption. We have made our scope very clear in our manuscript that we study the overlapping issues related to those methods,  and we **do not** claim or suggest that all IR methods will suffer from insufficient overlapping. We agree that it would be an interesting topic for follow-up research, but that is certainly beyond the scope of a single paper.\n\n2. The fundamental idea of our work is to replace a fixed target domain $Q$ with the controllable deployment domain $P_f$, so clearly, there should **not** be any $Q$ in our main theorems. Also, it is **not true** that the target risk does not appear in our theorems. In the generalization error bound in Theorem 2, the \"transported\" target deployment risk under $E_{P_{n,f}}$ is obviously on the RHS of the equation. Intuitively, this bound characterizes a tradeoff between interpolation and extrapolation. The \"transported\" target deployment risk is a consequence of interpolating the observed data and transport (shift) the learnt patterns to $P_f$ for the overlapped part. As for the non-overlapped part, since we make no assumption on the extrapolation performance of the reward model, we pay a price that scales with $\\rho$ -- the maximum discrepancy allowed between the observed and deployment domain. Our generalization bound truthfully reflects this tradeoff, and in practice, we treat $\\rho$ (reflected by $\\lambda$ in our final objective) as the hyperparameter and select the one that gives the best validation performance.\n\nWe hope that we have resolved the reviewer's concerns by clarifying the above misconceptions. Please let us know if there are further questions.\n\ncomment: We thank the reviewer for the careful reading and providing valuable comments on our manuscript. We apologize for the typos and unclear writings. To address the reviewer's concerns, we add several sets of experiments and an extended literature review section as suggested by the reviewer. In the revised manuscript, we mark all the added and modified parts in **RED**). We answer the individual questions as below. \n\n\n### Further discussion on the motivation\n\nOverlapping has long been recognized as a critical presumption for causal inference and domain adaptation methods. On the theoretical side, we derived in Proposition 1 both the upper and lower generalization bounds associated emphasizing on domain discrepancies, and the bounds reveal how insufficient overlapping can cause arbitrarily poor generalization. We also referred to [1], where the authors use a neural tangent kernel argument to rigorously show that over-parameterized models extrapolate poorly beyond the training domain. To corroborate our theoretical arguments, we add an illustrative experiment in **Appendix C.4** using the Movielens data. We use the fact that IW-MF [2] applies a threshold on the estimated propensity score (e.g. 0.001) to \"enforce\" a sufficient overlap. We vary the threshold to generate IW-MF's performances under different \"degrees of overlapping\", and the empirical results are shown in **Figure A.4b** (page 31). It is shown that the performance of IW-MF gets better as we increase the cutoff, which partly suggests insufficient overlapping can cause undesirable performance of recsys.\n\n\n### Interpretation of the theoretical results\n\nWe first apologize for the typo in **Claim 1**: it should be $E_{P_w}$ instead of $E_{P_Q}$ as pointed out by the reviewer. \n\n*Theorem 1* shows that for any policy, the minimal empirical transportation-constraint risk converges to the minimal population transportation-constraint risk at a rate of $1/\\sqrt{n}$, so we can safely consider the empirical version.\n\n*Theorem 2* states that the learned policy (the one that minimizes the transportation-constraint risk) is close to the oracle policy in terms of the target risk: the target risk of the learned policy converges to the target risk of the oracle policy at a rate $O(1/\\sqrt{n} + R_n(\\ell\\circ \\mathcal{F}))$. Compared with *Proposition 1*, the significance of *Theorem1* and *Theorem 2* is that our learning objective gets rid of the domain discrepancy terms that cannot be controlled.\n\n*Theorem 3* reveals that under the ideal bandit feedback setting with complete overlap, the solution of the proposed transportation-constraint risk minimization is consistent with the solution of the classic counterfactual risk minimization. Therefore, Theorem 3 shows a consistency result that is fundamentally different from the generalization results in *Proposition 1*, so they are not meant to be compared.\n\n\n### Extending the related-work section\n\nWe thank the reviewer for pointing out the additional relevant literature. In the original manuscript, we were unable to include all of them due to the space limitation. Therefore, we decide to add an extended literature review section in **Appendix D** (page 34) to address all the related work in more detail, especially those suggested by the reviewer. \n\n\n### Additional experiments and results\n\nWe manage to add all the experiments suggested by the reviewer:\n\n1. advanced IPW method -- doubly robust jointly learning [2] -- denoted by **DR-MF**;\n\n2. domain adaptation method -- causal embedding for recommendation [3] -- denoted by **CE-MF**\n\n3. the **Yahoo!R3** and **Coat** dataset.\n\nThe additional experiment results are shown in **Table A.1, A.2** (page 28), and all the data processing, model configuration, and result interpretation are added to **Appendix C**. We wish to point out that the main reason for not using the Yahoo and Coat data initially is that they do not have timestamps, so the sequential recommendation models cannot be tested. \n\nIn general, DT-MF still outperforms all the other (MCF-based) baseline methods, including DR-MF and CE-MF, in all the added experiments including the Yahoo and Coat data.  This is expected because DR-MF, CE-MF, and many other relevant methods are proposed for either the bandit feedback setting or explicit feedback data. The challenge for deterministic and implicit feedback, which is the focus of our work, often lies beyond the comfort zone of those methods, including non-overlapping, inaccurate propensity estimation, and the fact that not all non-clicked data are genuinely negative. \n\nFinally, we would like to thank the reviewer again for the insightful comments, and we are looking forward to future discussions.\n\n\n[1] Xu K, et al, How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks, ICLR'21\n\n[2]. Wang S, et al, Doubly robust joint learning for recommendation on data missing not at random, ICML'19\n\n[3]. Bonner S, et al, Causal embeddings for recommendation, SIGIR'18\n\ncomment: We thank the reviewer for providing valuable feedback and comments to our manuscript, and we have created an additional literature review section in **Appendix D** (page 34) to include more discussions on the relevant work. In the revised manuscript, we mark all the added and modified parts in **RED**). Here, we address the reviewer's concerns as below. \n\n### Further discussion on the motivation and the issues with extrapolation \n\nThe reviewer made an excellent point that a critical step for learning under non-overlapping is to extrapolate the reward model beyond the observed actions. It is often impossible to guarantee extrapolation performance without making further assumptions, however, our work uses an alternative strategy to resolve this issue as we explain next.\n\nFirstly, our solution makes a major effort to interpolate the observed data via empirical risk minimization. The guarantee for interpolation comes from classical learning theory, with the standard assumption that the actual reward model lies within the $\\ell\\circ\\mathcal{F}$ family we consider.\nThen, the DT constraint critically restricts the future action space where the radius $\\rho$ governs the maximum allowance of extrapolating beyond the observed regime. In what follows, we tune the value of $\\rho$ (reflected by $\\lambda$ in our final objective) to try different extrapolations and select the one that leads to the best empirical validation performance. On the theoretical side, since we make no extra assumption on the extrapolation performance of the reward model, we do pay a price that scales with $\\rho$ as we show in the generalization error bound of Theorem 2. Compared with the bounds for IW and DA in Proposition 1, even though the existence of $\\rho$ means that \"perfect learning\" can not be achieved, what matters here is that we can control $\\rho$ while the discrepancy terms that trouble IW and DA are essentially out of control.\n\nTherefore, an alternative viewpoint of our motivation is finding a balance between interpolation (what we can guarantee) and extrapolation (what we cannot guarantee), and both our implementation and theoretical justifications cling to this motivation. We apologize for not including the above discussions in the manuscript due to the space limit, and we will add them to the appendix.\n\n\n### Extending the discussions on relevant literature.\n\nWe thank the reviewer for suggesting making connections with the counterfactual learning literature. We add a dedicated section in the appendix to include more discussions on the relevant literature (**Appendix D**, page 34), particularly those listed by the reviewer. \n\nFinally, we would like to thank the reviewer again for the comments, and we are looking forward to future discussions.\n\ncomment: We are very thankful for the reviewer's careful reading and providing valuable feedback to our manuscript. We have added experiment results to the revised manuscript as suggested by the reviewer (all the added & modified parts are marked in **RED**), and we address the reviewer's concerns as below.\n\n### Analysis on how insufficient overlap can affect the performance\n\nOverlapping has long been recognized as a critical presumption for IW methods to work correctly for causal inference and domain adaptation problems. On the theoretical side, we derived in Proposition 1 both the upper and lower generalization bounds associated with IW, and the bounds reveal how the domain discrepancies can cause arbitrarily poor generalization. We also referred to [1], where the authors use a neural tangent kernel argument to rigorously show that over-parameterized models extrapolate poorly beyond the training domain. To corroborate our theoretical arguments, we add an illustrative in Appendix C.4 using the Movielens data. We use the fact that IW-MF [2] applies a threshold on the estimated propensity score (e.g. 0.001) to \"enforce\" a sufficient overlap. We then vary the threshold to generate IW-MF's performances under different \"degrees of overlapping\", and the empirical results are shown in **Figure A.4b** (page 31, Appendix). It is shown that the performance of IW-MF gets better as we increase the cutoff, which partly suggests insufficient overlapping can cause undesirable performance.\n\n\n### Comparing the bounds to understand the advance of the proposed approach\n\n*Theorem 1* shows that for any policy, the minimal empirical transportation-constraint risk converges to the minimal population transportation-constraint risk at a rate of $1/\\sqrt{n}$, so we can safely consider the empirical version.\n\n*Theorem 2* states that the learned policy (the one that minimizes the transportation-constraint risk) is close to the oracle policy in terms of the target risk: the target risk of the learned policy converges to the target risk of the oracle policy at a rate $O(1/\\sqrt{n} + R_n(\\ell\\circ \\mathcal{F}))$. Compared with *Proposition 1*, the significance of *Theorem1* and *Theorem 2* is that our learning objective gets rid of the domain discrepancy terms that cannot be controlled.\n\n*Theorem 3* reveals that under the ideal bandit feedback setting with complete overlap, the solution of the proposed transportation-constraint risk minimization is consistent with the solution of the classic counterfactual risk minimization. Therefore, Theorem 3 shows a consistency result that is fundamentally different from the generalization results in Proposition 1, so they are not meant to be compared.\n\n### Offline evaluation of the deep&wide models deployed for online testings\n\nWe apologize to the reviewer that due to the company's privacy codes, we can only report the relative percentage lift against the baseline (control model) for our industrial experiments. Therefore, we provided the online testing outcome, which is more straightforward to reveal the genuine performances of the different approaches. In the revised manuscript, we add the offline evaluations in terms of Recall@20, NDCG@20, and estimated CTR in **Table A.5** (page 33, Appendix). In general, we observed that both IW- and DT-enhanced deep&wide model leads to slightly improved offline performances when compared with the baseline deep&wide model, but the improvement from the DT approach is more significant, which agrees with the online testing results.\n\n\n### Efficiency analysis of the proposed approach\n\nTo answer the reviewer's question, we add another section to the revised manuscript (**Appendix C.5** on page 32) to justify the efficiency of our approach. Despite having a minimax training schema, our approach is very efficient for three reasons:\n\n1. While our DT-X approach has a larger per-epoch training time, the training progress is much more effective than using the original X model thanks to the reweighing and DT regularization. The training time vs. evaluation metric plots in **Figure A.5** (page 33, Appendix) shows that for both the Movielens and Yahoo Music dataset, DT-X catches up and outperforms X in a couple of training epochs even before the X model converges.\n\n2. The update of f, w, and g can be efficiently parallelized, e.g. $w^{(t)}$ is obtained using $f^{(t-1)}$ and $g^{(t-1)}$. Although we do not directly exploit this acceleration in our implementation, we set it as an option for future experiments.\n\n3. Our approach does not change the inference time since only the $f$ component will be used for evaluation and testing, just like when using $f$ alone.\n\nFinally, we would like to thank the reviewer again for the comments, and we are looking forward to future discussions.\n\n[1] Xu K, et al, How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks, ICLR'21\n\n[2]. Saito Y, et al, Unbiased recommender learning from missing-not-at-random implicit feedback, WSDM'20\n\ncomment: We appreciate the reviewer for providing valuable feedback and comments to our manuscript. We have added experiments and arguments to the revised manuscript as suggested by the reviewer (all the added & modified parts are marked in **RED**), and we address the reviewer's concerns as below.\n\n### Sensitivity Analysis on $\\rho$\n\nThe sensitivity analysis on $\\rho$ (which the reviewer referred to as $\\\\mu$) is carried out via $\\lambda$: according to the duality results in Claim 1 and Eq.(3), the role of $\\rho$ (the strength of the constraint) is replaced by $\\lambda$ (the strength of the regularization). In **Figure 2(c)** and **Appendix C.3.1**, we conducted extensive sensitivity analysis on $\\lambda$ to show that there exists a tradeoff in under- and over-exploring the whole feedback data. Within the range of $\\lambda$ we considered, the testing performance experienced moderate variations, and we suggested selecting the hyperparameter via cross-validation. \n\n\n### Adding IPW and domain adaptation (DA) baselines for comparison\n\nIn our original manuscript, we included the propensity-debiased approach [1] as a baseline (denoted by IPW-MF), and in the revised paper, we add two more baselines: \n\n1. the advanced IPW approach -- doubly robust jointly learning [2] -- denoted by DR-MF;\n\n2. the DA-based approach -- causal embedding for recommendation [3] -- denoted by CE-MF.\n\nThe results for the DR-MF and CE-MF methods are provided in **Table A.1 and A.2** (page 28, Appendix). In general, while the IPW- and DA-based solutions are able to improve the performance of MCF, the proposed domain transportation (DT) approach leads to the best outcomes in all the real-world and simulation data that we experimented with. We point out that the existing IPW- and DA-based recommendations are primarily designed for and tested on the bandit feedback or explicit feedback data where the propensities can be effectively obtained or estimated. We focus on the implicit feedback (which is arguably more widespread and applicable), and the experiments demonstrate that our approach significantly improves upon the existing solutions, including those enhanced by IPW and DA.\n\n\n### Further Discussion on the Motivation\n\nAn alternative viewpoint of our motivation is via Proposition 1 in the Preliminary Section: both the bounds we derived for IW and DA contain terms that explode under a large discrepancy between the source and target domain. They imply that neither the IW-based nor DA-based method may generalize well if there lacks a sufficient overlap. Unfortunately, the non-overlapping issue is intrinsic to many IR and recsys applications, particularly when the historical recommender is deterministic or when the logging policy has limited support.\n\nOur solution seeks to take control of the counterfactual worlds where the future policy is searched. Given the key observation that the intervention from recsys both affects the collected data and forms the target domain after deployment, we attempt to devise a policy that transports the learned patterns on the training domain to its intervention domain. Specifically, such a policy should meet two conditions: (i) its deployment domain has sufficient overlap with the training domain; (ii) it attains the minimal empirical risk when mapped to the training domain (via re-weighting). Our proposed transportation-constraint risk minimization then rigorously formulates the above motivations.\n\n\n### Clearing the Typos\n\nWe apologize for the below typos and unclear writings. We have corrected the typos in the revised manuscript.\n\n1. in Claim 1, it should be $E_{P_w}$ instead of $E_{P_Q}$ as pointed out by the reviewer;\n\n2. in experiments, we randomly sample m<|I| **irrelevant** items (so the reviewer's understanding is correct).\n\n\nFinally, we would like to thank the reviewer again for the comments, and we are looking forward to future discussions.\n\n[1]. Saito Y, et al, Unbiased recommender learning from missing-not-at-random implicit feedback, WSDM'20\n\n[2]. Wang S, et al, Doubly robust joint learning for recommendation on data missing not at random, ICML'19\n\n[3]. Bonner S, et al, Causal embeddings for recommendation, SIGIR'18"
            }
        ]
    },
    {
        "id": "PC8u74o7xc2",
        "decision": "Reject",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Embedding models through the lens of Stable Coloring\nKeywords: No keywords\nAbstract: Embedding-based approaches find the semantic meaning of tokens in structured data such as natural language, graphs, and even images. To a great degree, these approaches have developed independently in different domains. However, we find a common principle underlying these formulations, and it is rooted in solutions to the stable coloring problem in graphs (Weisfeiler-Lehman isomorphism test). For instance, we find links between stable coloring, distribution hypothesis in natural language processing, and non-local-means denoising algorithm in image signal processing. We even find that stable coloring has strong connections to a broad class of unsupervised embedding models which is surprising at first since stable coloring is generally applied for combinatorial problems. To establish this connection concretely we define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them. Grounded on this framework, we show that many algorithms ranging across different domains are, in fact, searching for continuous stable coloring solutions of an underlying graph corresponding to the domain.  We show that popular and widely used embedding models such as Word2Vec, AWE, BERT, Node2Vec, and Vis-Transformer can be understood  as instantiations of our general algorithm that solves the problem of continuous stable coloring. These instantiations offer useful insights into the workings of state-of-the-art models like BERT stimulating new research directions."
            },
            {
                "round2": "Reviewer 1:\nI would like to thank the authors for their response. However, most of my concerns were not properly addressed. Therefore, I will keep my current rating.\n\nReviewer 2:\nsummary_of_the_paper: In this paper the authors proposed continuous stable coloring (CSC) as a new framework to unify the understanding of several existing unsupervised learning algorithms, including Word2Vec, BERT, and Node2Vec. The authors show how the original stable coloring algorithm can be understood as optimizing the CSC objective function. The authors also show how to reduce the existing approaches to CSC.\n\nmain_review: This paper provides a novel perspective for understanding unsupervised learning algorithms, which could inspire new research directions. For example, the connection between BERT and stable coloring could help understand the role of different attention layers in BERT. However, the theory of this paper is not proven rigorously and it lacks new method or empirical results.\n\nComments & Concerns:\n - I find the connection between the original 1-WL algorithm defined for the graph isomorphism test and the global GAL formulation a bit over-stretch. In particular, 1-WL is an iterative algorithm that keeps assigning new colors to unseen neighborhood until the overall node partition in a graph by the color is unchanged; 1-WL does not explicitly push the difference between colors in the current iteration to be similar to that of the next iteration. As a result, if one directly use the GAL in equation 9 as the objective function, then there is a trivial minimum where all nodes have the same embedding; therefore, the authors \"proposed\" negative sampling as some sort of regulation. However, negative sampling does not have a correspondence in the original 1-WL algorithm.\n - The connection between local GAL and BERT is also over-stretch. Although there is clear analogous between the multiple attention layers with multiple iterations of 1-WL algorithm. In BERT the weights of multi-head attention in different layers are different. Does this mean that the aggregation function in GAL can be different for different layers? It would be helpful if the authors can provide a rigorous derivation of how the proposed setting leads to the original BERT training mechanism, instead of simply saying it is obvious.\n - In theorem 5.1, the authors claim that the $f_{agg}$ is simply an injective hash function that converts multi-sets to an integer; this is not true since if $f_{agg}$ is injective, then new colors could keep being generated forever, and that the stable coloring algorithm would never converge. It is possible that the authors are referring to a special variant of stable coloring algorithm. In that case I think a more rigorous proof would help understanding.\n - In equation 10, the authors use the coloring function $\\mathcal{C}$, while in equation 11 the embedding $E$ is used. For consistency it is better to use $E$ for both.\n - Theorem 5.2 shows that L-GAL is sufficient to guarantee good solution for G-GAL. However, it may not be necessary. In fact, in 1-WL, the color assigned in current iteration does not need to be the same as the previous iteration for the algorithm to converge, but rather only the partition needs to be the same. I wonder if the author can show the existing algorithms can directly connect to G-GAL.\n\nQuestions:\n - How is the neighborhood $\\mathcal{N}$ in section 5.1 defined?\n - What is the definition of $f$ in equation 8?\n - Are you assuming the kernel function $\\mathcal{S}$ in equation 10 to be strictly < 1?\n - What doe word-pieces ×N in table 1 mean?\n - How is the co-occurance of unigram and bigram defined?\n\nTypos and Grammars:\n - As mentioned before the graph embedding architectures captures capture...\n - $\\mathcal{C}(\\mathcal{N} (u)) = C(\\mathcal{N} (v)$\n - $\\mathcal{C}(\\mathcal{N}_l (u)) = C(\\mathcal{N}_l (v)$\n - Firstly, describe the construction of the graph for a particular domain and sub-graph induction based on the sample in the data.\n - ??? (missing citations)\n\nsummary_of_the_review: the paper is inspiring but major revision is needed\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: The paper tries to relate multiple unsupervised NLP embedding models to the problem of stable coloring. The authors claim that they \"prove equivalence between loss functions of popular NLP, image, and graph embedding models and our proposed constrained L-GAL optimization loss operating on domain-specific graphs\".\n\nmain_review: - The writing of the paper can be improved. In particular, the contribution of this paper is unclear to me. The authors repetitively claim that they \"define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them\", but there is no validating algorithm at all. \n\n- From a theoretical point of view, how could the proposed mathematical framework help us better understand NLP embedding models? \n\n- It seems the authors try to claim that there exists some similarity between graph coloring problems and NLP embedding problems. However it feels like they are simply repeating the so-called distributional hypothesis. Now say the claim is true. How would that be helpful to researchers using those NLP models, including BERT, ViT, Node2Vec? Again the contribution is not clear.\n\n- I do not understand if Theorem 4.1 is a theorem, or a hypothesis or an assumption. Is G_{DH} a weighted graph? Considering that the entries in M are nonnegative integers (counting co-occurrence), what does it mean by adjacency matrix in this case? \n\n- Many Theorem proofs are just one line like \"the proof follows trivially from ...\" E.g., Theorem 4.1. If they are so obvious, is it necessary to present them as \"Theorems\"?\n\n- The derivation from the assumptions in Theorem 4.1 to Definition 2.1 is not so trivial. How do you define neighborhoods in terms of the co-occurrence matrix?\n\n- Section 2.2, description of 1-WL algorithm \"Ci(N(u)) 6= C(N(u)\". This is not properly typed. Why both sides are u? Also, which function C is being used on the RHS? Please clarify. \n\nEditorial comments:\n\n- Extra whitespace: introduction - \"continuous stable coloring (CSC )\", Section 2 - \"Aggregate and Update (GAU )\", Section 2.2 \"(GAU )\".\n\n- Section A.6: \"with the parameters mentioned in the table ??\"\n\n- Definition 2.1: \"C(N(u)) = C(N(v)\"\n\nsummary_of_the_review: At the current stage the contribution of this paper is unclear, both theoretically and experimentally.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: In the paper the authors define a new problem called the continuous stable coloring (CSC). This is an extension of the traditional stable coloring problem, however, instead of having discrete color labels, you have continuous color labels. The authors then provide an objective function that could be minimized to obtain a CSC. They then show that if you tweak the similarity measure $S$ and the aggregator function $f_{agg}$, then the CSC problems various word embedding techniques such as word2vec, AWE, Bert (without non-linearity). \n\nmain_review: Strengths:\n\n1) The main contribution of the paper is the definition of a new problem CSC. As the authors themselves mention most of their Theorems are straightforward consequences of the definition of the problem. \n\nMajor Weaknesses\n\n1) **No new insights**\n\nThe major weakness of the paper, is despite making a connection between CSC and prior word embedding techniques, the paper does not mention anything new about the techniques or present any insights into how the new connections can be used to better design embedding techniques. \n\nSpecifically, the main connection here is that we embed words by tryin to predict the context of the word and that this is the same as embedding a graph by trying to predict the neighbors of the graph. However, this connection (while not presented as CSC) has been known. See [1] for an example that explores the connection to 1WL in detail. \n\nFurther, my main issue with the paper is that the connection between techniques and word2vec has been made at a surface level in function space. That is, it shows that the embeddings that the two techniques are trying to learn are similar. However, it does not prove any properties  about the embeddings. Maybe say something about its generalization. Or show that if the graph satisfies certain properties then the learned embedding satisfies certain properties. \n\nFurther, the more interesting question in deep learning and the usage of neural networks is not the connection in function space, but the connection in parameter space. That is, how do these networks parameterize the function that we are trying to learn/how do optimization techniques learn different representations in the parameter space. \n\n[1] Martin Grohe. 2020. Word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data. In Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS'20). Association for Computing Machinery, New York, NY, USA, 1–16. DOI:https://doi.org/10.1145/3375395.3387641\n\n2) **Missing Details**\n\nI also think the paper is missing details. First, in the problem of CSC, we look at $f_{agg}$ as an *injective* function from the space of all multisubsets of $L$ to $L$. However, the space of *all* multisubsets of $L$ has a strictly bigger cardinality than $L$. (eg. if $L$ is the natural numbers, i.e., is countable, then $N^L$ is uncountable). Hence no injective function can exist. Hence I assume the authors mean that the $f_{agg}$ is defined on the space of all *finite* multisubsets. While this is not too big a deal, it is an issue for a theoretical paper. \n\nThere are also a few vague statement. For example, the authors, say $f_{agg}$ is essentially a hash function in Theorem 5.1. What does this mean exactly? Another is after the box titled research question, the authors say, \"the above mentioned issue of multiplying noisy neighborhoods\". Where are we multiplying neighborhood? What does it mean to multiply neighborhoods?\n\n\n**Minor Details**\n\nThere are typos in the main text. Some of them are\n\n1) Many equations are missing parenthesis. For example, we see $\\mathcal{C}(\\mathcal{N}(u)$ in a few places (def 2.1, top of page 3, last line of 1WL paragraph).\n2) Equation 1, also seems to have a typo, it should be $E^{k-1}(v) : v \\in \\{\\{\\mathcal{N}(u)\\}\\}$. \n3) Theorem 4.2 last line has a typo. \n4) Just before section 3, missing a reference (have ??)\n\n**Improvements**\n\nHere are just a few ideas for improvements. \n\n1) Maybe look into $f_{agg}$ and see how properties of this function effect properties of learned embedding.\n2) Run some experiments. Show empirically, how optimizing for CSC directly using the algorithms presented has similar results to doing the appropriate word embedding technique.\n3) Show if the graph has certain properties then we can say something about the embedding. \n4) Show that using insights from above we can design a new technique. \n\n\n\nsummary_of_the_review: Overall, I think the problem of CSC is a neat problem. However, its connection to word embedding techniques has only been explored at a surface level. Hence I feel there is not enough in the paper to merit acceptance as a theory paper. However, the connection could bear fruit if the authors do provide deeper connections either in parameter space or provide results on the properties of the embedding learned. While I cannot vote for acceptance at this time, I do hope the authors revise with new results and resubmit. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 5:\nsummary_of_the_paper: In this paper, the authors investigate how the solution of the Weisfeiler-Lehman test of isomorphism (WL) is related to embedding algorithms from structured domains. The authors generalize the WL algorithm to the case where the colors assigned to the nodes correspond to feature vectors, and propose a series of optimization problems to solve this problem. Finally, they show that widely used algorithms in natural language processing, computer vision and graph representation learning such as word2vec, BERT, Visual Transformer and node2vec actually solve different instantiations of this common optimization problem.\n\nmain_review: Major comments\n--------------\n- One of the main issues with this paper is that writing is not clear. In fact, the presentation of the paper leaves the reader with a feeling that this was an initial draft of the work. The paper is pretty difficult to read. There are many typos and the structure is disorganized. I suggest the authors work on improving it.\n\n- Overall, the paper seems to be proposing an interesting contribution. The authors establish a connection between the Weisfeiler-Lehman test of isomorphism and embedding algorithms from structured domains. However, in my view the significance of the paper is low. The authors provide no discussion on potential applications of the proposed methodology, while it is not clear whether it can be useful for domains in which there is already a wealth of embedding approaches such as in natural language processing or in graph representation learning. Furthermore, the authors do not experimentally evaluate the proposed methodology.\n\n- It is not clear to me what is the intuition behind the G-GAL and L-GAL formulations, and how exactly the solutions of those formulations are equivalent to the continuous stable coloring. I would suggest the authors make clear why out of all possible formuations, they have proposed those two and not some other formulation. Furthermore, if we assume that $f_{agg}$ is the sum operator, to my understanding, these two problems admit a trivial solution. Setting the representations of all nodes equal to the all-zeros vector leads Equations (7) and (8) to their minimum value (i.e., a value equal to 0). I would like the authors to comment on this point.\n\n- It is not clear to me how embedding algorithms such as word2vec and node2vec are instances of the L-GAL formulation. In the case of those algorithms, a positive instance cooresponds to a pair of words/nodes that co-occur within a fixed-size window in some text or a random walk. In Equation (8), the representation of a node is encouraged to be close not to the representation of each of its neighbors, but to the sum of them (in case $f_{agg}$ denotes the sum operator).\n\n- The authors present in section 5.5 the different optimization algorithms that one could employ to solve the L-GAL problem and its variants. It would strengthen a lot the paper if the authors could apply the proposed methodology to some real-world problem. For instance, the authors could construct a graph from some text corpus and use the proposed approach to embed the different term to some vector space. Then, they could also evaluate the quality of the generated embeddings in standard tasks such as word analogy and word similarity, and compare them againt those generated by other methods.\n\n- Although I am not aware of any previous studies that have established a connection similar to the one presented in this paper, there are previous studies that have proposed to unify embedding approaches into matrix factorization frameworks [1]. Such works are related to the work presented in this paper and should be discussed in the related work section.\n\nMinor comments\n--------------\n- Function $\\mathcal{C}$ which is used for comparing vectors or multisets of vectors seems quite arbitrary. What are other functions that could potentially be employed and what properties do they need to exhibit?\n\n- In p.2, the following sentence \"As mentioned ... information?\" is very long. It is also not clear and does not read well. I suggest the authors rephrase this sentence.\n\n- In p.3, the authors mention \"Chen et al. (2019) in their GIN ...\". In fact, GIN was proposed by Xu et al. and not by Chen et al.\n\nTypos:\n------\np.2: \"architectures captures capture the\" -> \"architectures capture the\"\\\np.3: \"$\\mathcal{C}_( \\mathcal{N}(u)$\" -> \"$\\mathcal{C}_i (\\mathcal{N}(v))$\"\n\np.3: \"examples are given in appendix ??\" -> \"examples are given in appendix B\"\\\np.4: \"surrounding it\" -> \"surrounding them\"\\\np.4: \"provided in appendix\" -> \"provided in appendix.\"\\\np.5: \"which we show this in the\" -> \"which we show in the\"\\\np.5: \"injective $f_{merge}$\" -> \"injective $f_{update}$\"\\\np.6: \"learn-able\" -> \"learnable\"\\\np.8: \"heirarchial\" -> \"hierarchical\"\n\n[1] Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K. and Tang, J., Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the 11th ACM International Conference on Web Search and Data Mining, pp. 459-467, 2018.\n\nsummary_of_the_review: In general, I feel that there is some value in this paper and that the contribution is interesting since,  to my knowledge, no paper so far has established any connection between the Weisfeiler-Lehman test and embedding approaches. However, I have several concerns about this work. The writing is not clear, while the significance of the theoretical results seems to be low. The authors do not perform any experiments, thus there are no empirical results and it is not clear how the proposed approach would work in real-world scenarios. Finally, I am not sure whether some of the claims made by the authors are actually true.\n\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel."
            },
            {
                "round3": "comment: Thank you for your comments. There was a general concern among reviewers that there is a lack of experimental evidence to support the theory or its implications. We do not perform experiments because we believe that there is ample evidence in existing literature that supports the implications of our theory. **We have added a new section in appendix (A.1 in blue)**. In this new section, we do a walkthrough of NLP embedding models and discuss how various improvements in NLP embedding models can be explained via implications of CSC based framework.\n\n\n**Contribution and Utility :**\nWe unify the different conjectures such as - distributional hypothesis, non-local-means , etc (which are the basis of different algorithms in literature) into a single mathematical problem of continuous stable-coloring. This problem leads to formulation of Global GAL framework. We show that current state-of-the-art algorithms in these fields solve different instantiations of  Local GAL problem which gives a solution that is provably close to the solution of Global GAL problem which. This generic framework highlights that art of embedding model design lies in devising good underlying domain graphs and $f_{agg}$ functions.\n\nAgainst this backdrop, we can evaluate the existing algorithms and understand them better. See newly added section A.1 in appendix. For example, “Why does BERT perform better than Word2Vec?” -  Using different layers, BERT can be interpreted to be using higher order tokens in the underlying graph. While word2vec uses 1-gram tokens, BERT with $L$ layers uses upto $L$-gram tokens. Thus, one can expect BERT to perform better as it incorporates more information.\n\nWe can also use this framework to seek further research directions. For example,\n“How can we improve an algorithm like word2vec ?“ - Word2vec improvements can come from a variety of considerations such as \nCan we have a better $f_{agg}$ ? One can see AWE as implementing better $f_{agg}$ over word2vec and we see that AWE beats word2vec\nCan we have a better estimation of chosen $f_{agg}$. Right now we only consider sentences of fixed context. This only gives us an estimation of $f_{agg}(N(u))$ as it samples from the neighbourhood. However, can we have better sampling?\nCan we better model the underlying graph? One can see that BERT considers more informative graph and beats Word2vec\n\n**It is not clear to me what is the intuition behind the G-GAL and L-GAL formulations.**\nGlobal-GAL formulation is exactly the solution to the CSC problem. The term global GAL minimizes is exactly $abs(ln( (S(u,v) / s_N(N(u),N(v)))$  i.e. it tries to make S(u,v) = S_N(N(u), N(v)).\nLocal-GAL formulation decouples the problem while maintaining theoretically provable approximation to the Global-GAL solution.\nTrivial solution\nTrivial solution will not happen when the $f_{agg}$ is injective.. If $f_{agg}$ is not injective (e.g., mean, sum, median), then trivial solutions will exist for the optimization problem.\n\n**It is not clear to me how embedding algorithms such as word2vec and node2vec...**\nWe consider the CBOW model for word2vec and node2vec. We have clarified this now. Skip-gram models can also be explained in this framework.\n\n\ncomment: Thank you for your comments. There was a general concern among reviewers that there is a lack of experimental evidence to support the theory or its implications. We do not perform experiments because we believe that there is ample evidence in existing literature that supports the implications of our theory. **We have added a new section in appendix (A.1 in blue)**. In this new section, we do a walkthrough of NLP embedding models and discuss how various improvements in NLP embedding models can be explained via implications of CSC based framework.\n\n**Contribution and Utility :**\n\nWe unify the different conjectures such as - distributional hypothesis, non-local-means , etc (which are the basis of different algorithms in literature) into a single mathematical problem of continuous stable-coloring. This problem leads to the formulation of the Global GAL framework. We show that current state-of-the-art algorithms in these fields solve different instantiations of  Local GAL problem which gives a solution that is provably close to the solution of Global GAL problem which. This generic framework highlights that art of embedding model design lies in devising good underlying domain graphs and $f_{agg}$ functions.\n\nAgainst this backdrop, we can evaluate the existing algorithms and understand them better.See newly added section A.1 in appendix For example, “Why does BERT perform better than Word2Vec?” -  Using different layers, BERT can be interpreted to be using higher order tokens in the underlying graph. While word2vec uses 1-gram tokens, BERT with L layers uses upto L-gram tokens. Thus, one can expect BERT to perform better as it incorporates more information.\n\nWe can also use this framework to seek further research directions. For example,\n“How can we improve an algorithm like word2vec?” - Word2vec improvements can come from a variety of considerations such as \nCan we have a better $f_{agg}$ ? One can see AWE as implementing better $f_{agg}$ over word2vec and we see that AWE beats word2vec\nCan we have a better estimation of chosen $f_{agg}$. Right now we only consider sentences of fixed context. This only gives us an estimation of $f_{agg}(N(u))$ as it samples from the neighbourhood. However, can we have better sampling?\nCan we better model the underlying graph? One can see that BERT considers more informative graph and beats Word2vec\n\n**On 1-WL and L-GAL**\n\nThe connection that the reviewer highlights between 1-WL and GNN layers is explored well in literature. However, it is only explored only in context of the question \"What architecture should a particular network have w.r.t to two considerations - generalizability and expressivity\".\n\nHowever, this is an entirely orthogonal discussion which, when looked at in context of our paper, tries to answer the question \"what is a good $f_{agg}$\". Contrary to this, in our paper, we connect the solutions of embedding models to stable-colored solutions in continuous domains.\n\n**On other embedding properties**\n\nWhile these are interesting questions, we believe that what we present is the starting point where we look at principles underlying state-of-the art methods. This, for us, is quite interesting. To the best of our knowledge, we do not know of a work that unifies the different conjectures such as - distributional hypothesis, non-local-means, etc (which are the basis of different algorithms in literature) into a single mathematical problem of continuous stable-coloring.\nWe believe that our work can be a solid starting point to incorporate more properties of embeddings and analyse them through our framework.\n\n**Minor questions**\n1) Uncountable number of multi-subsets : We deal with finite graphs and hence, we are looking at finite multi-subsets. We will clarify this in our statement.\n\n\ncomment: Thank you for your comments. There was a general concern among reviewers that there is a lack of experimental evidence to support the theory or its implications. We do not perform experiments because we believe that there is ample evidence in existing literature that supports the implications of our theory. **We have added a new section in appendix (A.1 in blue)**. In this new section, we do a walkthrough of NLP embedding models and discuss how various improvements in NLP embedding models can be explained via implications of CSC based framework.\n\n**Contribution and Utility:**\nWe unify the different conjectures such as - distributional hypothesis, non-local-means , etc (which are the basis of different algorithms in literature) into a single mathematical problem of continuous stable-coloring. This problem leads to the formulation of the Global GAL framework. We show that current state-of-the-art algorithms in these fields solve different instantiations of  Local GAL problem which gives a solution that is provably close to the solution of Global GAL problem which. This generic framework highlights that art of embedding model design lies in devising good underlying domain graphs and $f_{agg}$ functions.\n\nAgainst this backdrop, we can evaluate the existing algorithms and understand them better. See newly added section A.1 in appendix. For example, “Why does BERT perform better than Word2Vec?” -  Using different layers, BERT can be interpreted to be using higher order tokens in the underlying graph. While word2vec uses 1-gram tokens, BERT with L layers uses upto L-gram tokens. Thus, one can expect BERT to perform better as it incorporates more information.\n\nWe can also use this framework to seek further research directions. For example,\n“How can we improve an algorithm like word2vec ?“ - Word2vec improvements can come from a variety of considerations such as \nCan we have a better $f_{agg}$ ? One can see AWE as implementing better $f_{agg}$ over word2vec and we see that AWE beats word2vec\nCan we have a better estimation of chosen $f_{agg}$. Right now we only consider sentences of fixed context. This only gives us an estimation of $f_{agg}(N(u))$ as it samples from the neighbourhood. However, can we have better sampling?\nCan we better model the underlying graph? One can see that BERT considers more informative graph and beats Word2vec\n\n**Other details:**\n\nOn theorem 4.1:\n\nWe have reworded theorem 4.1 and removed the term adjacency matrix.\nFor the construction of the word2vec graph, connect each word to every other word. The label of the edge between word $i$ and word $j$ is the co-occurrence count. Then, theorem 4.1 follows from definition 2.1 for an edge-labelled undirected graph.\n\nOn other theorems :\n\nWhile these statements are important results of the paper, we believe that an avid reader will find the details boring as they naturally follow from the formulation. Undoubtedly, the major contribution is in formulation of CSC and related optimization problems. \n\n**Minor questions:**\n1) neighborhoods in terms of the co-occurrence matrix :  Co-occurrence matrix, in this case, defines the adjacency matrix of the graph . i.e it defines the weighted edges E in G(V,E).\n\n\ncomment: Thank you for your comments. There was a general concern among reviewers that there is a lack of experimental evidence to support the theory or its implications. We do not perform experiments because we believe that there is ample evidence in existing literature that supports the implications of our theory. **We have added a new section in appendix (A.1 in blue)**. In this new section, we do a walkthrough of NLP embedding models and discuss how various improvements in NLP embedding models can be explained via implications of CSC based framework.\n\n\n**Main questions :**\n\nThere seems to be some confusion about the paper connecting the 1-WL algorithm with the iterative (gradient based) solution of G-GAL/L-GAL. This is not the case. We talk about embeddings as being the stable-colored solutions over underlying structured domain graphs. There is no equivalence between 1-WL and solution to L-GAL in an algorithmic sense. While L-GAL is an optimization problem formulation, 1-WL is an iterative combinatorial algorithm. The relationship between L-GAL and 1-WL is subtle in that the solution to L-GAL is an approximate solution to the CSC ( continuous stable coloring problem) while 1-WL solves the (discrete) table coloring problem. We find that most of the confusions arise due to this mis-connection. Please let us know if we misunderstood your questions.\n\n1) On connection between 1-WL and L-GAL. \nAs mentioned above, we do not make this connection.\n\n\n1.1) Trivial solution\nTrivial solution will not happen when the $f_{agg}$ is injective.. If $f_{agg}$ is not injective (e.g., mean, sum, median), then trivial solutions will exist for the optimization problem.\n1.2) On introduction of Negative sampling \nThe introduction of negative sampling helps in injecting more information into the system that is not explicitly present in graph structure.\n\n2) L-GAL and BERT : \nThe connection that the reviewer highlights between 1-WL and GNN ( and hence BERT) layers is explored well in literature. However, it is only explored only in context of the question \"What architecture should a particular network have w.r.t to two considerations - generalizability and expressivity\". This is an orthogonal discussion which, when looked at in the context of our paper, tries to answer the question \"what is a good $f_{agg}$?\" However, our discussion provides a different view of bert, where the complexity of BERT ( and hence its improvement over Word2vec and AWE) stems from considering a more complex underlying graph structure. While Word2vec only considers 1-grams as nodes, BERT considers $k$-grams where $k$ go from 1 to $L$ where $L$ is the number of layers in BERT. This is shown in the proof of theorem 6.2. \n\n2.1) Different aggregation function for different layers  in BERT\nThere is no concept of layers in L-GAL/G-GAL formulation.\n\n\n4) On Theorem 5.1 and 5.2\n\nThe statement is about the stable-coloring solutions and not the algorithms themselves.\n\n**Minor questions:**\n\n1) Neighbourhood definition: $N(u) = {v | (u,v) \\in E}$ for graph $G(V,E)$.\n2) What is the definition of f in equation 8? It was a typo. $f$ has been corrected to $E$.\n3) Co-occurrence of bi-grams and uni-grams : for each time a bi-gram, say u-v,  occurs in the context of uni-gram say w, then this contributes a count of 1 towards the co-occurrence (u-v, w)\n"
            }
        ]
    },
    {
        "id": "bTteFbU99ye",
        "decision": "Accept (Poster)",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Evaluating Distributional Distortion in Neural Language Modeling\nKeywords: No keywords\nAbstract: A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate.  As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy."
            },
            {
                "round2": "Reviewer 1:\nThank you for your detailed answers to my questions and for your additions to the paper and to experiments. I aim raising my score and hope the paper will get accepted.\n\nReviewer 2:\nThank you for your detailed reply and the updates to the manuscript---I raised my score and hope this paper gets in! :)\n\nReviewer 3:\nsummary_of_the_paper: This paper investigates how language models allocate their probability mass, with an emphasis on rare sequences that are part of the 'heavy tail' of the distribution of natural language sequences. The authors use a language model to define a target distribution with access to samples and ground-truth probabilities. A language model is trained on an empirical estimate of the target distribution, and the authors study the gap between the learned model's and target distribution's probability assignments. \n\nUsing this methodology, the authors uncover various interesting phenomena: the model systematically assigns lower probabilities than the target distribution, but assigns unusually high probabilities to unnatural, perturbed sequences (suggesting an explanation for where the probability mass moved to). The authors include several fine-grained analyses with additional interesting findings.\n\nmain_review: **Interesting background**. The connections between productivity, low-probability sequences, and the limitations of perplexity were clearly written and interesting.\n\n**Well-executed experiments and methodology**. The idea of using a language model as a ground-truth distribution was interesting and well-suited for the analysis done here. The experiments were easy to follow and the analysis was clear.\n\n**Interesting findings**. The main findings related to underestimation, comparison of training-set and test-set dynamics during training, and dependence on training data were interesting and not obvious. The finding that perturbed, unnatural sequence received unusually high probabilities (while test samples receive unusually low probabilities) [Figure 4] was especially interesting. \n\n**Clarity**. The paper was well-written and enjoyable to read. The authors articulated how their work fits in with related work, and the concepts (e.g. the LNRE zone), methodology, and results are clearly written. Well done.\n\n**Additional experiments**: A good addition would be measuring the impact of model size with models larger than the target distribution. What would GPT2-XL's or GPT-3's probability assignments look like on a target distribution from GPT2-medium?\n\nsummary_of_the_review: This investigatory paper defines the problem that they are studying, develops a methodology for studying it, and clearly analyzes the results. The investigation yields interesting findings related to language models. This paper would make a great addition to ICLR - accept.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 4: The contributions are significant, and do not exist in prior works.\n\nempirical_novelty_and_significance: 4: The contributions are significant, and do not exist in prior works.\n\nReviewer 4:\nsummary_of_the_paper: The paper proposes to examine the probability mass that a language model trained as usual places on frequent/likely and rare/unlikely sentences by learning from a distribution that is another neural language model itself so instance-level probabilities/NLLs of sentences can be compared. It turns out that all models in all tested circumstances *underestimate* the probabilities of rare events---and an ablation study shows that much of that missing mass can be found in sequences that are also rare and unlikely, but that weren't likely under the origin model.\n\nmain_review: The paper overall is exceptionally well-written and clear and tables and plots are beautiful and clear---multiple times did I go \"ooh that's a nice way of laying it out.\" Kudos! The abstract tells the entire story and the paper substantiates the claims nicely making what is shown feel easy and obvious, which I consider a great feat.\n\nConcerning content, I am equally enthusiastic about the idea and evaluation and agree that this can be a very interesting piece of evidence that nicely connects to a number of past threads of NLP research (as laid out throughout and in the nice related work section, though I don't think I know enough related literature to truly safely say that the paper is as novel as it claims to be). That said, there are three major methodological concerns that I would like to discuss through with the authors before recommending acceptance:\n\n1. We do not know whether neural-LM-defined origin distributions are \"strange\"---or really, structurally different from actual human natural language distributions that we believe exist---in some systematic way that leads to the results in this paper. One potential intuition here is that a neural LM being a very imperfect model makes very erratic choices for what low-frequency/low-likelihood it assigns probability mass to---erratic choices that may be very hard to learn/emulate with the student LM that the authors train. Maybe real natural language distributions have more \"sensible tails\" that are actually not all that hard to learn and maybe not even underestimated. To be clear, this is a fundamental issue that the authors have no way of *resolving* easily under this paradigm and I think the paradigm is still a worthwhile idea! What I am asking is that this limitation is acknowledged and discussed with the same clarity that is afforded to the other content.\n\n2. Relatedly, currently there is no way to know whether the trained models are actually reasonably trained, overall well-working models or not: the paper is missing test ppl/bpc for both the origin model (on English test data) and for the student model (both on the original English data and on test data sampled from the origin model). The authors make a good point that these scores are not as informative as what this paper presents, but the reason these are crucial is so we know whether the effects the authors find are a property of well-trained models or whether these models (through subpar training or another issue) are just poor models that have little connection to the models we actually use in reality. A secondary idea to easy this concern might be to actually use a pretrained GPT model as the origin model to inspire trust that that at least is a good model---I'm not sure why this road wasn't chosen unless it truly did not matter and this \"home-made\" model performs just as well, in which case, again, I would like to see some quantitative substantiation of that claim.\n\n3. The paper is not really clear on how the sampling/teaching procedure and the tempering of the softmax interact: Footnote 2 claims pure (right?) ancestral sampling is used (as opposed to top-k/nucleus?), but then Footnote 4 says the softmaxes are all cooled to T=0.85, and yet Section 4.5 again implies that no tempering took place up to that point in the study. This is not only slightly confusing messaging, but I am worried that it may belie the promise of fair evaluation, especially because it isn't clear to me whether the tempering was accounted for in the calculation of probabilities as defined at the end of section 3.2. Specifically, if the probability of a sentence under the origin model does not take into account this local tempering, then it is no surprise that the student model learns to underestimate rare events because they just weren't sampled thanks to the locally cooled distribution! I would be somewhat reassured if the probability calculation in section 3.2 takes the tempering into account, but my understanding of the paper is that it currently fails to do so. As slight reassurance, Figure 5 does seem to tell us that even without any tempering (yes?) the described effect is visible, so I think this too should not be an issue that sinks the paper, but I do see it as a critical flaw the way it is right now.\n\nBeyond those three, I see some minor concerns that may be worth addressing:\n- \"autoregressive neural language models\" should also cite the Mikolov RNN paper, perhaps in place of one of the two GPT papers\n- not to Schmidhuber, but citing only Melis et al. (2020) for LSTMs looks a little odd (citing Hochreiter & Schmidhuber or Sundermeyer et al. may be less surprising though I understand the desire to cite the *actual* model and training specification used and agree it should be present)\n- concerning citations, Shiran Dudy & Steven Bedrick's work may be nice to also connect to (despite its relatively low visibility)\n- it may be worth pointing out that the perturbations in section 4.4 may individually also make sentences *more* grammatical and likely and that is just a relatively rare outcome (I don't think much more than that needs to be said on that)\n- I did not quite see what was \"[p]erhaps unsurprisin[g]\" in Section 4.5, but that's a minor nitpick \n- Appendix A.1 measures events, okay... but are those words (as the hapax legomena theory usually assumes) or are they sentences (as I think this experiment does)? If they are sentences, we should not be surprised to see very high probabilities of seeing novel events, after all sentences are rarely the same (excluding formulaic language and \"Resumption of the session\" style sentences). I also would have liked to see some Poisson distributions or other distributions that were more interesting than uniform n-sided dice in Figure 6, but that is more wish than necessity.\n\nsummary_of_the_review: Before author response:\n\nMy comparatively low score is a consequence of my concerns and very much given as a temporary score until we can discuss the three major concerns I have---if they are addressed satisfactorily, I anticipate raising my score significantly to champion the paper if needed as I found this paper a joy to read and thought-provoking in a good way (even if that lead to criticism).\n\n\nAfter author response:\n\nThank you for all the clarifications and edits, they are greatly appreciated. I would strongly like to see this paper accepted.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 5:\nsummary_of_the_paper: The paper conducts experiments to evaluate whether neural sequence models such as LSTMs and Transformers are able to correctly assess the probability of rare sentences, which collectively constitute a large probability mass in natural language productions (heavy-tail phenomenon). In order to do so, it performs experiments in a controlled synthetic environment where a first language model $P_L$ is trained on a corpus of natural sentences, and a second model $P_M$ is trained to emulate the first model. The authors observe that the second model systematically tends to underestimate the probability of rare $P_L$ sentences, the more so the rarer such sentences are, and show that some artificially corrupted sentences tend to receive higher probability from $P_M$ than from $P_L$, partly explaining where the missing probability mass over rare well-formed sentences went.\n\n\n\nmain_review: * Strengths: A very well written and clear paper, addressing a focussed question through a well-designed set of experiments on synthetic data allowing to precisely control experimental conditions.\n\n* Weaknesses: No major weakness, but some mild issues and areas for improvement, see the comments and questions below.\n\n* Comments and Questions:\n\n    * (Minor Comment) I did not find the description of the LNRE Zone in section 2 to bring much to the discussion. In particular, characterizing this zone through the condition that $\\mathcal{P}_N$ is not __null__ is almost empty of content for natural language, because it would in particular mean that we could precisely delimit what constitutes a possible \"natural\" sentence, and that the LNRE Zone should cover all such sentences. Here the formal description as given does not clarify anything.\n    * (Question) In some of your experiments, you train a $P_M$ GPT2-medium model on 30M sentences sampled from the $P_L$ GPT2-medium model, that is, $P_M$ here uses the same architecture as $P_L$ (if I understand correctly). However, the $P_M$ model still underestimates the probabilities relative to $P_L$. Do you think this is a symptom of 30M sentences being too few, or more a symptom that $P_M$ is stuck in some local minimum during the optimization ? After all, (again, if I understand the setup correctly), using the same architecture, $P_M$ would make it possible, in principle, to just use the same parameters as $P_L$, and then the mismatch could not occur. Perhaps this would be worth a note in the text?\n    * (More important Question/Comment) In section 4.4, you perform some perturbations, and observe that they lead to higher $P_M$ than $P_L$. First, I think it would be worth defining these perturbations in more detail than what you do. Second, you do not discuss your choice for these perturbations nor analyze why they have (relatively) high probability relative to $P_M$. However, the fact that, for instance, swapping two words or deleting a word in a natural sentence is not severely penalized by $P_M$ is an interesting but specific fact, which may have to do with a special tolerance of $P_M$ for such operations, rather than for others. To give one example, if the perturbations consisted solely in replacing a word in the natural sentence by a _rare_ word in the training set, would you observe such a difference between $P_M$ and $P_L$? Overall, could you discuss this aspect of your experiments?\n\n\n\nsummary_of_the_review: A nice experimental paper addressing the difficulty of neural LMs to approximate the probability of rare events from an underlying \"teacher\" LM. The paper could be improved by more analysis and discussion of some of the results.\n\n*After authors' response*: thank you for your detailed answers to my questions and for your additions to the paper and to experiments. I aim raising my score and hope the paper will get accepted.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: Dear Reviewers,\n\nWe just wanted to ask if you had any further questions for us. We’d be happy to address any lingering concerns. Thanks!\n\ncomment: Dear Reviewer, \n\nWe have just updated the manuscript with the finalized version of the experiments using a pretrained ground-truth model. Please see Section A.2. We fine-tuned a pretrained GPT2-medium model on 1.5M sequences sampled from the OpenWebText corpus, and trained a randomly-initialized GPT2-medium model on generations from this pretrained model. We find that the student language model underestimates the probability of the majority of test sequences. Furthermore, this underestimation is more severe for less probable sequences. Therefore, these results are similar to those we saw for non-pretrained ground-truth distributions. In fact, the underestimation seems to be more severe in this case. \n\ncomment: Thank you for the kind and thoughtful comments. Please see our comment below.\n\n**Additional experiments with models larger than the target distribution**: Thank you for this suggestion. We have deployed training runs on larger models and will include these results in the final version of the paper.\n\ncomment: Thank you for the thoughtful and useful comments. Please see our comments below and the revised version of the manuscript. All significant changes are in blue.\n\n**Comment on LNRE zone discussion**: We appreciate this point. The formal description was simply meant to make explicit the connection between the massive amount novelty that language models have to deal with and the productivity of language. We have added a sentence clarifying this point to the paper.\n\n**Question about local minimum and increasing data**: This is a very good point. Our results highlight that underestimation asymptotes within 30M sequences. This result suggests that larger amounts of training data have relatively minimal effects on underestimation error. Nonetheless, it is still likely that $P_M$ is stuck in a local minimum during optimization. One possible way to investigate whether this is true is to train a larger model on sentences sampled from $P_L$. If the larger model provides a better fit, then perhaps there are optimization issues (related work indicates that overparameterization may be more beneficial than increasing the amount of data [1]). We are currently training a $P_M$ GPT2-large model on sentences sampled from the $P_L$ GPT2-medium model to see whether the large model provides a better fit. Our initial results (after 15M sequences) suggest that GPT2-large provides a marginally better fit ($P_M$ GPT2-med obtains perplexity of 22.4411 after 15M sequences, whereas $P_M$ GPT2-large obtains 22.3180) but still suffers from underestimation. We will add these considerations in the main text once our experiments are completed. Another experiment which may help elucidate whether the model is in a local minimum would be to deploy multiple training runs with different random seeds. If all training runs converge to the same test PPL score, then we can take this evidence that we are not in a single local minimum.\n\n**Comment/Question on perturbation experiments**: This is a good point. It could be the case that the specific perturbations we chose may be particularly tolerated by our student models. However, since our teacher and student models are in the same model class, then we might expect sequences perturbed using these edits to also be assigned relatively high probability by the teacher, which they are not.\n\nThe goal of the perturbations experiments was to examine the probability of sequences in Σ* outside of high-probability subspaces under $P_L$ . Based on your comment, we have added an additional experiment to the Appendix which examines the probability of randomly sampled sequences from Σ* under $P_L$  and $P_M$ . Specifically, we sample a sequence length l from a Poisson distribution, then uniformly sample tokens from the model’s vocabulary. These experiments show that $P_M$ tends to overestimate the probability of these sequences as well. This indicates that the overestimation behavior of $P_M$  is likely attributable to a general tendency to spread probability mass more uniformly on Σ* rather than a consequence of our specific edit operations.\n\nNevertheless, we agree that examining other edit operations would also be interesting and we would like to hear if you have any suggestions for further approaches to this problem. In particular, we would like to add the edit you suggested, but need more clarification regarding how this perturbation differs from the substitution perturbation defined in Table 3. Finally, we have added more information in the paper about how our perturbed sentences are constructed.\n\n\nReference: [1] https://arxiv.org/pdf/2002.11794.pdf\n\ncomment: Thank you for the thoughtful and useful comments. Please see our comments below and the revised version of the manuscript. All significant changes are in blue.\n\n**Major concern 1) The properties of the tails in the ground-truth distribution**: This is a very good point. We agree that our lack of knowledge of the properties of the ground-truth distribution should be acknowledged and discussed. To this end, we added a paragraph at the end of the conclusion in our revision. We state it is likely that the distribution defined by our ground-truth models is less structured in the tail than the distribution of a natural language. We expect this to be the case since the main takeaway of our paper---that neural-LMs spread mass too uniformly over Σ*--- holds for our ground-truth models as well. That being said, we expect this lack of structure to make it easier for our student LMs to recover the distribution; our experiments in Section 4.5 indicate that underestimation increases when decreasing the entropy of the target distribution. Therefore, it is not unreasonable to expect student neural-LMs to underestimate more severely when trained on distributions with more explicit structure. Possible ways to test this claim include the use of grammars (e.g., a PCFG) as ground-truth distributions. However, something like a PCFG typically provides poor coverage of natural languages, and scoring sequences under these models can be costly. As a middle ground, one could guide the sampling of sentences from a neural LM such that they have high probability under an external grammar. We leave these experiments to future work. \n\n**Major Concern 2) Performance of ground-truth models**: Using a pre-trained GPT is an excellent suggestion. We have added an additional experiment to the Appendix which uses a pretrained GPT2-medium model as the ground-truth distribution. Specifically, we fine-tune a pretrained GPT2-medium model on 1.5M sequences sampled from the OpenWebText corpus. Training a randomly-initialized GPT2-medium model on generations from this pretrained model, we find that the student language model underestimates the probability of the majority of test sequences, and does so more severely for less probable sequences. Thus, these results are similar to those we saw for non-pretrained ground-truth distributions. Finally, we have also added the perplexity values for the models used in the paper to the Appendix. \n\n**Major concern 3) Sampling and scoring information**: Thank you for letting us know that our sampling procedure was not clear. We do indeed account for tempering in the calculation of the ground-truth probabilities. Additionally, all experiments prior to Section 4.5 are conducted for distributions induced by an ancestral sampling scheme with softmax $T=0.85$, as suggested by footnote 4. Note that we tempered the ground-truth distribution in an attempt to define an artificial language with more realistic entropy. Given the findings of our paper, it is likely that the distribution defined by our ground-truth models spreads mass more uniformly over Σ* than a natural language does. By tempering the distribution, we attempt to lessen this discrepancy. We have added clarifications in Sections 3.1, 3.2 and 4.5 to make these points clearer. \n\nAs you noticed, the underestimation effect occurs whether the distribution is tempered or not. As further evidence of this, we have added a plot to the Appendix which shows GPT2-small and LSTM underestimation behaviour when fitting an untempered ($T=1.00$) ground-truth distribution. The trends are analogous to those found for $T=0.85$. For conceptual simplicity and clarity, we are also re-running all experiments prior to section 4.5 without any tempering. For completeness, in the final version of the paper, we will add results for all experiments when setting $T=1$.\n\n**Minor concerns**: Thanks for the great suggestions. We have added the relevant citations, and have added a mention of Shiran Dudy & Steven Bedrick's work in our related works section. We have also added a sentence mentioning that our perturbations may in rare cases produce more grammatical sentences, and have added an additional experiment to the Appendix which computes model estimation on random sequences in Σ*. We have removed the “perhaps unsurprisingly” from Section 4.5. Finally, the Appendix productivity measures are for sentences; and we agree that this is not a surprising fact, but we think it is nevertheless important in the context of language modeling. We will add distributions other than uniform dice, along with measures for words in the final version of the paper.\n"
            }
        ]
    },
    {
        "id": "14kbUbOaZUc",
        "decision": "Reject",
        "source": "ICLR 2022",
        "statements": [
            {
                "round1": "Title: Metric Learning on Temporal Graphs via Few-Shot Examples\nKeywords: Metric Learning, Few-Shot Learning, Temporal Graph\nAbstract: Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs; Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we wish to learn a distance metric only over fewer temporal graphs, which metric could not only help accurately categorize seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to describe temporal graphs on different time scales. Then we propose the MetaTag framework: 1) to learn the metric over a limited number of streaming-snapshot modeled temporal graphs, 2) and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art algorithms for temporal graph classification problems."
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: This paper introduces a methodology of graph learning for dynamic graphs, where the dynamics are encoded in the representation to obtain improved results on graph classification tasks. This framework includes a temporal graph encoder that uses attention mechanisms to generate representations, as well as a meta-learning component that ensures easy knowledge transfer. Experiments are carried out on two temporal graph datasets to show strong performance in graph classification. \n\nmain_review: **Strengths:**\n\n- The idea of enriching a graph representation using knowledge of its temporal dynamics is quite nice. The authors split the time attention into three parts: node-level, intra-snapshot, and inter-snapshot. The node-level attention mechanism supplies temporal sensitivity to the node representations, the intra-snapshot mechanism adds in a graph autoencoder to learn how to reconstruct the adjacency matrix of a snapshot, and the inter-snapshot attention mechanism weights different snapshots according to discriminative power. The modular design is helpful in understanding how different aspects of the pipeline can be improved in future work.\n\n- The additional meta-learning module is a nice addition, and it is useful to see how to interface this module with learning dynamic graph representations.\n\n\n**Weaknesses/points to clarify:**\n\n\n- The use of $t_e \\in T_e$ is unclear from Figure 2. Could the authors please clarify how $t_e$ is obtained? Currently it seems that we have a set of edges at $t_s=0$ with $max_{t_e} = 2$, and that the new edges added at $t_s = 1$ have $t_e=3$, i.e. incremented by 1. Similarly, at $t_s=2$, the new edges are labeled with $t_e=4$. So the figure doesn't shed light into the differences between the two timescales $t_e$ and $t_s$ beyond \"increment by 1\". Section A.2 contains further mention of $t_e$ but without explanation, so I still don't fully understand how $t_e$ is generated. I think this issue can be fixed with a few lines of clarifying language.\n\n- In Section 4.1.1, where is the attention in the \"intra-snapshot time attention\" module? It seems that here we just learn how to reconstruct the adjacency matrix at each snapshot. Perhaps this can be clarified in the author response.\n\n- Possibly more seriously, I'm concerned about the **novelty** of the proposed framework. Consider references [A],[B] and the further references contained therein to attention-based models for learning dynamic graph representations. These do not appear as baselines in the current work, and in fact do not even appear as references. It seems that the meta-learning portion in the current work is new, but is that the only source of novelty? I hope the authors will be able to clarify these connections in their response.\n\n[A0] Sankar, A., Wu, Y., Gou, L., Zhang, W., & Yang, H. (2018). Dynamic graph representation learning via self-attention networks. arXiv preprint arXiv:1812.09430.\n\n[A1] Sankar, A., Wu, Y., Gou, L., Zhang, W., & Yang, H. (2020, January). Dysat: Deep neural representation learning on dynamic graphs via self-attention networks. In Proceedings of the 13th International Conference on Web Search and Data Mining (pp. 519-527).\n\n- (A1 is an expanded version of A0)\n\n[B] Rossi, E., Chamberlain, B., Frasca, F., Eynard, D., Monti, F., & Bronstein, M. (2020). Temporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637.\n\nsummary_of_the_review: I think the paper has good experimental results and that the incorporation of the meta-learning framework is useful. However, I am concerned that the authors have left out important references, and would welcome a thorough evaluation of these references (and ideally other important references contained therein) and how they compare to the current work. Currently I will lean toward reject, but welcome a discussion with the authors on the novelty of their work. \n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 2:\nsummary_of_the_paper: This authors present a novel method for learning representations for time-varying graphs which allows for incorporating information at different time-scales using their streaming-snapshot model. The streaming-snapshot model has the following parts:\n* Each snapshot $S^{(t_{s})} = (V^{(t_{s})}, E^{(t_{s})})$ has edges of the form $(v_i, v_j, t_e) \\in E^{(t_{s})}$ where $t_e$ denotes the time at which edge was formed (and is present since then).\n* The snapshots $S^{(t_{s})}$ are at a different time-scale ($t_e$ and $t_s$ are not comparable) with the overall learning representation being ${\\cal S} \\to \\Re^f$.\n* Learning this representation is used for downstream few-shot classification task (for dynamic graphs) and is evaluated on two scenarios - time-varying biological (protein-protein interaction) networks and time-varying social networks.\n\nThe MetaTag architecture has the following components:\n\n* _Time-aware node representation_ The edge creation time $t_e$ is used to learn a time-aware node representation ${\\bf u}^t_e$ using attention-based weighting of neighbouring nodes features concatenated with a learnable time kernel (Algorithm 1).\n- The snapshot feature matrix $U^{(t_{s})}$ takes the node representation by consider the latest edge for node $u$ and using attention mechanism above to get influence of earlier edges.\n* _Intra-snapshot representation_ This is constructed using standard representation loss using a GCN-based encoder-decoder architecture followed by permutation-invariant readout to obtain vector representation for snapshot.\n* _Overall representation_ The overall representation for the time-varying graph is  weighted average using attention pooling (learnt parameter) of different snapshot representations. \n\nThis representation is used downstream for classification task (classification-head) based on prototypical approach [Snell+, 2017] resulting in overall end-to-end differentiable model with weighted average of reconstruction loss and classification loss. Further, the model allows adaptation to new tasks (with different classification labels) by fine-tuning on small test set (few-shot learning). \n\nExperiments are shown on biological and social network datasets (in appendix) showing efficacy of the approach compared to static graph representation methods as well as tdGraphEmbed (doc2vec style method for embedding temporal graphs) including augmentation with ProtoNet for few-shot learning comparison. \n\n\nmain_review: \n# Strengths\n- First attempt to provide an end-to-end differentiable model for handling time-varying graphs \n- Addresses separate time-scales (using streaming/snapshot model)\n- Adapts and uses existing methodology where appropriate (GCNs/Readout) for snapshot representation, ProtoNets for few-shot learning, learnable time-kernels and attention for aggregating the time-aware influence of neighbouring node features into node representation. \n\n# Weaknesses\n1. The paper needs some rewriting starting with separate notions of time (streaming/snapshot) - classic stochastic theory handles this by consider $t_e$ as discrete time and ${\\cal S}^{(t)} |_{t=t_s}$ is the observation event (when we see the interactions).  As described by the authors, the snapshots model episodic, slow-changing, and periodical patterns. However, the model does not correspond to \"time elapsed\" between consecutive snapshots $(s-1), s, (s+1)$ beyond ordering. Further, it is not clear how to relate $t_s$ to $t_e$ (which is the 'real time').  I would recommend the authors call it \"k\" snapshots and not call it snapshot 'time' for clarity. \n\n2. The attention weighting across snapshots is agnostic to \"elapsed time\" meaning that few-shot learning results will work well if the time between consecutive snapshots is same across original task and new task (see point 3 below as well). \n\n\n3. The biological dataset [DPPIN, Fu and He, 2021] is not 12 separate time-varying graphs rather one large graph and a _single_ time-varying gene expression dataset from which dynamics are inferred over twelve different subgraphs. This means in the fine-tuning setting, the learnt weighting (equation 7) benefits from the fact that the temporal dynamics are representing the same underlying time-scale (snapshots in test and train correspond to real gene expression values in the same experiment at the same instant) which is why the attention weighting works well. I would be very interested in seeing how few-shot learning works when considering two different gene expression arrays for deriving the underlying dynamics.\n\n4. Would it make sense to re-order Algorithm 1 (lines 1-2) in terms of latest edge connecting to a node for a snapshot, i.e., to generate ${\\bf U}^{(t_s)}$, for each node $v$ select latest edge $(v, v', t')$ and use time-aware attention mechanism to get influence of pre-existing edges $(v, v'', t''), t'' \\le t'$  into its node representation. From a computational perspective, is only ${\\bf u}^{t'}_v$ computed or all ${\\bf u}^{t''}_v$ computed first and only the latest one selected in  ${\\bf U}^{(t_s)}$. \n\n5. (a) When computing ${\\bf U}^{(t_s)}$, suppose you have only the following edges $(v_1, v_2, 0)$, $(v_2, v_3, 1)$, $(v_3, v_4, 2)$ would the final ${\\bf U} = [ u_1^0, u_2^1, u_3^2, u_4^2]$? \n    (b) In Algorithm 1. Should $t < t_e$ be $t \\le t_e$ otherwise consider two edges $(x, y, t_e=3)$  and $(x, y', t_e=3)$ - then the influence of the other edge is missed. Further, how is this addressed in ${\\bf U}^{(t_s)}$.\n    An example in the text would clarify here.  \n\n6. In the appendix A.2, the lines \"For each temporal graph, 36 edge timestamps together describe three consecutive metabolic cycles. In each graph, we take a subgraph by extracting a interval of 5 edge timestamps every 3 edge timestamps. The subgraph shares the same class label with its original entire graph. Therefore, we have 11 temporal subgraphs per class.\" are directly copied from https://arxiv.org/pdf/2107.02168.pdf  \"in each graph, we take a subgraph by extracting the time interval of five timestamps every three timestamps. The subgraph shares the same class label with its original entire graph. Therefore, we have eleven temporal subgraphs per class.\" It is not clear to me how eleven (and not twelve) temporal subgraphs are obtained. Further, there is a mismatch in the Table 1 (#classes shown as 1) and section 5.2 Line 1 (given the 12 classes). The description of the dataset should be expanded upon and made consistent. \n\n# Minor corrections\n- Equation 1. ${\\bf y}_j$ should be ${\\bf x}_j$\n- In section 3, the authors should define a time-stamped edge $(v_i, v_j, t_e, t_s)$. My understanding is that this means an edge between nodes $v_i$ and $v_j$ that formed at \"microscopic\" time scale $t_e$ and is present whenever the $t_s^{th}$ snapshot was taken (clearly sometime after $\\max \\{ t_e | (v_i, v_j, t_e, t_s) \\in S^{(t_s)} \\}$).  \n\n- Page 9, Line 1: \"and fine tune a few times on $\\tilde{G}^{train}\\_{support}$  and report the accuracy on  $\\tilde{G}^{train}\\_{query}$\". Should this be \"fine tune a few times on $\\tilde{G}^{test}\\_{support}$ and report the accuracy on  $\\tilde{G}^{test}\\_{query}$\"? \n- On page 3, it should be made clear whether ${\\bf A}^{(t_s)}$ the snapshot adjacency matrix is just the presence of an edge at that instant (regardless of when it was created) i.e., $(i, j, t_e) \\in E^{(t_{s})} \\Rightarrow A_{ij}^{(t_s)}=1$.\n\n\n\nsummary_of_the_review: The current paper addresses an important problem using a smart approach but requires significant rewriting as well as better empirical evaluation. Results on social network data are only marginally better than a much simpler approach (GL2Vec+ProtoNet) while the biological time-varying graph dataset is not actually different tasks since the underlying dynamics for all the 12 networks are learnt from a single gene-expression dataset which means train and test settings share the same underlying biological process and time-scale. I do not believe the paper is ready for publication in the present form. \n\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 3:\nsummary_of_the_paper: This paper considers the graph metric learning problem where the underlying graphs are temporal. The key idea of obtaining a higher classification accuracy is to use a bi-level meta-learning paradigm. It essential contains two parts: 1) prototypical temporal graph encoder where the model uses multi-scale time attention to capture temporal information; and 2) meta-learner where it uses bi-level paradigm proposed in Finn et al., 2017. The authors apply the proposed method to the task of graph classification on two real-world datasets. Compared with other baseline methods, MetaTag achieves better performance in terms of classification accuracy.\n\nmain_review: \nThe strengths are:\n\n1. The problem considered in this paper is interesting and new. Specifically, the authors consider the graph metric learning but in the temporal setting.\n\n2. The authors proposed a new method, which includes two parts. To capture the temporal information, the key of the proposed is based on three different attentions. Previous works such as Xu et al, 2020 and Yang et. al., 2021 have demonstrated the power of time-related attention layers.\n\n3. The temporal graph classification experiments on two datasets demonstrate that the proposed method learns much better classifiers than other baseline methods.\n\nThe weaknesses are:\n\n1. The contribution of the streaming-snapshot model is not critically novel. The essential idea of the streaming-snapshot model is to combine the discrete-time dynamic graph (DTDG) and continue the time dynamic graph (CTDG) model together (see more details in [1]). The authors use both two models but may have different time granularity.\n\n2. The technical contributions are limited. The whole framework may look interesting and new but it is based on Finn et. al., 2017. The time attention layers are commonly used in temporal graphs which are also proposed in previous work.\n\n3. The experiments are limited. First of all, the run time analysis is missing from both theoretical and empirical perspectives. More explanations are needed on the results of GL2Vec + ProtoNet. The experimental setup and parameter settings are unclear.\n\n---\nMinors:\n1. It is a little bit unfair to directly use CAW and TGAT under the graph metric learning setting. What is the specific loss used for these two?\n2. Table 3, MetaTag → \\textsc{MetaTag}\n3. Why directly adding ProtoNet onto CAW and TGAT be a reasonable thing?\n4. I expect detailed experimental setups like parameter tuning in the appendix. Could you list the experimental details in the appendix?\n5. What is the scalability of this method compared with two strong baselines: 1) GL2Vec + ProtoNet and 2) tdGraphEmbed + ProtoNet? It seems that GL2Vec + ProtoNet is competitive on the social network datasets but bad on the bio-based datasets. Why is this case?\n\n[1] Kazemi SM, Goel R, Jain K, Kobyzev I, Sethi A, Forsyth P, Poupart P. Representation Learning for Dynamic Graphs: A Survey. J. Mach. Learn. Res.. 2020 Jan 1;21(70):1-73.\n\n\n\nsummary_of_the_review: In general, it is an interesting paper. The experimental results look promising. However, the overall quality is not strong enough for acceptance. There is space for improvement in experiments. For example, the comparison between the proposed method and baseline methods. More discussion on the experimental results is also needed.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: The paper proposed a method for metric learning for few-short examples, where each example is a temporal graph modeled with two timescales. \n\nmain_review: The definition and example of the temporal graph are confusing. I could see that that a temporal graph consists of snapshots. I could not see why in snapshot, there are edge timestamps? Do you assume the node set fixed? Do you mean that a snapshot is also a temporal graph with edge appear and disappear at different times (a)? If yes, then I could not see it in the Figure 2. I could not see the meaning of t_e=0/1/2 in the figure. \n\nI'd be assuming (a), then the statements like \"previous methods only focus on one time scale and ignore the whole lifetime evolution representation\" are difficult to understand. I don't see why methods for temporal graphs do not take all the times of the graphs into account, i.e. do not take the whole graphs into account. \n\nThe method used in the paper is a combination of different standard methods available. There is nothing wrong with that, but it would be useful to prove the usefulness for each of these choices. \n\nOverall, I could see that the proposed method is supposed to be general with multi-scale timestamps and claim that, due to its generality, it is better than all other methods available out there. This kind of messages, in my opinion, needs to be more specific on when/why.\n\nsummary_of_the_review: The problem is not clearly defined and hard to read. The method is general and claimed to be better than others due to its generality. I do not see too much novelty in the method. The results look good.  \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "No author response found. For ICLR 2022, the author response format might have changed or the authors may not have provided a response."
            }
        ]
    },
    {
        "id": "N3kGYG3ZcTi",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Suppression helps: Lateral Inhibition-inspired Convolutional Neural Network for Image Classification\nKeywords: Lateral Inhibition, Convolutional Neural Networks\nAbstract: Convolutional neural networks (CNNs) have become powerful and popular tools since deep learning emerged for image classification in the computer vision field. For better recognition, the dimensions of depth and width have been explored, leading to convolutional neural networks with more layers and more channels. In addition to these factors, neurobiology also suggests the widely existing lateral inhibition (e.g., Mach band effect), which increases the contrast of nearby neuron excitation in the lateral direction, to help recognition. However, such an important mechanism has not been well explored in modern convolutional neural networks. In this paper, we explicitly explore the filter dimension in the lateral direction and propose our lateral inhibition-inspired (LI) design. Our naive design incorporates the low-pass filter, while eliminating the central weight to mimic the inhibition strength decay. The inhibition value is computed from the filtering result of the input, with a simple learnable weight parameter per channel for multiplication to decide the strength. Then the inhibition value is subtracted from the input as suppression, which could increase the contrast to help recognition. We also suggest an alternative using depthwise convolution, as a general form. Our design could work on both the plain convolution and the convolutional block with residual connection, while being compatible with existing modules. Without any channel attention along the channel dimension, the preliminary results demonstrate an absolute improvement of 3.68\\% and 0.69\\% over AlexNet and ResNet-18, respectively, in the ImageNet data set, with little increase in parameters, indicating the merits of our design to help feature learning for image classification."
            },
            {
                "round2": "Reviewer 1:\n\nI am very grateful to the authors for their thoughtful responses to all reviewers. This is very commendable. I really admire that.\n\nAs I said in the comments, the exploratory spirit of this paper, specifically the exploratory spirit of exploring neural networks that conform to human cognition, is worthy of recognition. I think this is a very important direction. I look forward to and believe that the authors can make outstanding contributions in this direction.\n\nAlthough the authors may have rushed to submit this paper, the spirit of the authors is worthy of recognition. I look forward to the authors doing impactful work.\n\n\n\nReviewer 2:\nDear Authors,\n\nThank you so much for preparing the response to our initial comments in such a short time! I think we are on the same page about the potential of this method, and the work needed to be done to improve the overall quality of the paper to meet the conference standard. Regarding the results, the lack of large-scale experiments is still concerning given that the proposed method adds complexity to networks. I encourage the authors to further improve the depth and the value of the method, and polish the paper quality to fully release the potential of the idea. I will not recommend acceptance at the current stage but would love to see an improved revision coming out. Thanks again!\n\nCheers\n\nReviewer 3:\nDear Authors,\n\nI thank you very much for carefully considering comments in my review and adding meaningful baselines to the comparisons. I appreciate you highlighting the differences between the proposed work and Hasani et al [2019]. However, I don't see this comparison being made in the new set of results in your response above. I thank you for adding DivisiveNorm and WeightedNorm comparisons to the response. However, I still think the results are quite preliminary and aren't sufficient in my opinion to be accepted at ICLR at this moment. \n\nI hope the authors please work on the following two broad directions to add more value to their submission: (1) Understanding Lateral Inhibition / its relevance to biology -- I see that authors show  improved performance on ImageNet with the AlexNet architecture. Why does LI cause this improved performance? What is different about the features learned by AlexNet with LI vs without LI? I find the explanation related to Mach Band effect interesting but the authors need to demonstrate if it is indeed what is happening in the network; (2) Relevance to deep learning -- If the authors intend to make this submission relevant to deep learning audience, they must significantly strengthen empirical results and show across multiple networks and (datasets or tasks) that the proposed LI module produces gains with error bars across multiple random initializations.\n\nAt this stage, I retain my score and do not recommend acceptance. If the authors were to work on either or both of the above directions, I'm sure this valuable contribution will be further strengthened and will eventually be of interest to machine and/or biological vision audience. Once again, thank you very much to the authors for engaging in the rebuttal phase and good luck on enhancing this submission. \n\nReviewer 4:\nsummary_of_the_paper: The authors propose to add a biologically inspired lateral inhibition mechanism into deep convolutional networks for image recognition. When incorporated into AlexNets and ResNets, LI seems to improve performance on ImageNet classification without increasing trainable parameters. The authors examine the LI filter weights and find a biologically-resemblant center-surround pattern of inhibition.\n\nstrength_and_weaknesses: Strengths:\n1) There are considerable gains on AlexNet and ResNet while using LI, however, I would like to note that these are preliminary results as also highlighted by the authors.\n\nWeaknesses:\n1) The proposed work lacks novelty, several methods in the past have tried to apply a very similar lateral or divisive inhibition mechanism to deep convolutional networks and have reported gains in image classification performance (particularly when added to AlexNet). See [1] [2] and [3] for example. The proposed work is almost exactly similar to [1].\n2) The evaluation is very preliminary and lacks comparison to suitable baselines (other kinds of normalization such as BatchNorm, LayerNorm, etc.) or other normalization techniques such as [1, 2] which are very relevant.\n\nReferences:\n1. Hasani, H., Soleymani, M., & Aghajan, H. (2019). Surround modulation: A bio-inspired connectivity structure for convolutional neural networks. Advances in neural information processing systems, 32.\n2. Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\n3. Pan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. bioRxiv.\n\n\nclarity,_quality,_novelty_and_reproducibility: The proposed work is not novel and evaluations are preliminary. The authors don't discuss sharing code or trained models, which is troublesome to reproduce the presented results.\n\nsummary_of_the_review: The proposed work is preliminary and lacks the novelty and quality of work expected at ICLR. I do not recommend accepting this paper at this stage. I suggest the authors to please consider a suitable workshop for this work and a significant extension of this work could be suitable for the ICLR audience with wider evaluation using suitable baselines and tasks.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nReviewer 5:\nsummary_of_the_paper: This paper incorporates the lateral inhibition of neuron into the Deep network for image classification. Results show the effectiveness of the proposed mechanism. The lateral inhibition is not novel but the idea to explore the usability of the lateral inhibition into artificial neural network is interesting.  The key weakness of this paper is that there are no any explanations why their design improves the performance.  \n\n\nstrength_and_weaknesses: The lateral inhibition based idea to explore the usability of the lateral inhibition into artificial neural network is interesting and useful. Such an approach should be strongly supported. \n\nThe key weakness of this paper is that there are no any explanations why their design improves the performance.  \n\nclarity,_quality,_novelty_and_reproducibility: 1\\ Why the low-pass filter with eliminating the central weight could model the LATERAL INHIBITION is not clear from this paper. The design is not a typical Difference of Gaussian function which is generally adopt in image processing [1].  Eq (5) seems an OFF-receptive field ?\n\n2\\ The inhibition computation by subtracting a computed value from the previous one is commonly used in many previous models [1][2][3]. The traditional models simulated visual mechanisms should be clearly mentioned and discussed [1][2][3]. \n\n3\\ The description of related work is very limited as the reason shown in above. \n\n4\\ The learned weight W plays the similar function of channel attention. Hence, what’s the difference between channel attention and the inhibition computation?  \n\n5\\ Eq (5) seems an OFF-receptive field ? Author should visualize the learned kernels as many examples. \n\n[1] IEEE Transactions on Image Processing. 2015;24(8):2565{2578.\n[2] Brain-inspired weighted normalization for CNN image classification.\n[3] IEEE transactions on pattern analysis and machine intelligence. 2015;37(10):1973{1985.\n\nMinors:\n\nSection 3.3 INHIBITION PLACE\n\nwe place it right after that convolution, as shown in Fig.\n\n\nsummary_of_the_review: It can be seen that the work is not complete with only five pages as the main text. Many important issues such as explaining why the design of lateral inhibition improves the performance. Computationally, what’s the difference between channel attention and the inhibition computation?  In summary, the technique contribution is simple.  \n\nI encourage authors to enhance their work by addressing 1-2 key questions mentioned above, adding content and depth to the paper.\n\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 6:\nsummary_of_the_paper: \nSummary:\nThis article starts from the perspective of neuroscience, hoping to design a neural network that conforms to human cognition. The original intention of the article is good, but there is nothing special about the method of the article. Regarding the writing, methodology, and experiments, this article is far below the acceptance criteria for a good conference like ICLR. Reviewers put a lot of effort into reviewing each article, so hopefully, authors will write each article well before submitting it.\n\n\n\nstrength_and_weaknesses: \n(Positive) Although this article is very poor, its exploratory spirit, specifically the exploratory spirit of exploring neural networks that conform to human cognition, is worthy of recognition.\n\n\n(Negative) The authors claim some gain in their approach. But as we all know, after introducing attention or dynamic mechanism, a general neural network can have some improvement. In particular, the worse the network, the more obvious the improvement.\n\n(Negative) In fact, filtering is not a new thing in neural networks.\n\n\n(Negative) A very working method should be validated on large-scale tasks. In addition to classification, there is detection, segmentation, and so on.\n\n\n(Negative) The writing of this article is very poor. There are multiple copies of the text in the article. For example, there are multiple repetitions of text in the abstract and introduction. A good article should have a better way of expressing it.\n\n\n(Negative) Gaussian filtering and bilateral filtering are not new in neural networks. The method proposed in the article is equivalent to a bilateral filtering non-local neural network.\n\n(Negative) The article associates its method with depth-wise convolution. This is very imprecise and arrogant. If this is true, is MLP-Mixer also a special case of this article?\n\n(Negative) As I said before, adding attention or dynamics to the neural network, such as senet, sknet, acnet, dynamic nets, will bring gains. There are too many such examples. So the results in Table 2 are not dazzling at all.\n\n(Negative) The numbers in Equation 5 are so intuitive and natural.\n\n\n\nclarity,_quality,_novelty_and_reproducibility: \nThe quality, clarity, and the originality are poor.\n\n\n\nsummary_of_the_review: \nSee \"Summary Of The Paper.\" Regarding the writing, methodology, and experiments, this article is far below the acceptance criteria for a good conference like ICLR.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nReviewer 7:\nsummary_of_the_paper: This paper proposes a type of neural networks that uses lateral inhibition inspired layers. Lateral inhibition in this paper refers to the effect where the contrast of nearby neuron excitation in the lateral direction is increased for recognition. Inspired by this, the paper proposes to use a Gaussian low-pass filter with a zero center weight along with a learnable channel weight to mimic this effect. A flexible alternative would be a depthwise convolution. Then this output is used to subtract from the input tensor to form the final output. This lateral inhibition inspired design is tested with AlexNet and ResNet18 for image classification on ImageNet, and shows improvements at little additional cost.\n\nstrength_and_weaknesses: This paper is motivated by a neurobiological effect named lateral inhibition. It is always fascinating to see methods that are inspired in this way because it may reveal some important angles that the academic community has overlooked.\n\nBut this paper seems to be less prepared for this conference. The method is interesting but very simple: it adds a couple of convolutional layers to existing architectures. The baseline architectures are not from recent works, and the model scales in the test are also small (e.g. ResNet18, AlexNet). I also didn’t find the visualizations mentioned in the paper. I hope that the authors could add more experiments to the paper, show larger experiment settings, and polish the paper a bit more.\n\nclarity,_quality,_novelty_and_reproducibility: The paper is clear to read. But its quality is below the conference standard. The method is simple and the novelty is limited. The reproducibility seems good.\n\nsummary_of_the_review: This is a paper inspired by the lateral inhibition effect. The motivation is great, but the method is too simple, and the paper does not show the strength of the method through large-scale and comprehensive experiments. Overall, I think the paper is below the conference standard.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel."
            },
            {
                "round3": "comment: Thank you for your valuable time and review. Just a little bit more information for your concerns:\n- **I appreciate you highlighting the differences between the proposed work and Hasani et al [2019]. However, I don't see this comparison being made in the new set of results in your response above.**\n\n    For Hasani et al [2019] on ImageNet dataset with AlexNet (the original paper only using a subset of 100 categories in reduced resolution of 160x160, with 500 instances for training and 100 instances for testing per category, on a small convolutional network), when applying the fixed difference of Gaussian (DoG) for the all the initial convolutional channels, the Top-1 accuracy is 51.46%, when applying to all channels of all convolutional layers, the Top-1 accuracy is 51.39%. Results indicate reduced accuracy by adding their filter to AlexNet on the original size and resolution of ImageNet dataset, which could be caused by lacking flexibility of the filter (as the original paper says applying to half of the initial channels is better than applying to all channels, but both improve the accuracy). And applying to all channels at all convolutional layers further decreases the accuracy. While our method applying to all layers significantly improves the accuracy to 60.39%.\n\n- **Why does LI cause this improved performance?**\n\n    The main idea of our lateral inhibition inspired design is to calculate lateral inhibition value from neighbors at each location for each channel, then subtracts that value from the original value at each location. With learnable weight W, it is flexible (could be zero, positive or negative) to model different lateral interaction (zero for none, positive for lateral inhibition, negative for lateral enhancement).\n\n    Our guassian inhibition inspired design follows common lateral inhibition description that an excited neuron can reduce the activity of its neighbors, by subtracting inhibition caused by neighbors from the original value. It can be viewed a special difference of Gaussian (DoG), with two gaussian filters of size 1x1 and 3x3, but central weight eliminated for 3x3 filter. For the case of one activate neuro with no activate neighbors, it receives zero inhibition from neighbors and remains the same value in our design, but decreases a lot with central weight.\n\n    Our LI design **could increase the contrast and sharpness along the boundary** in the same channel (consider Mach band effect). **When calculating, on one side near the boundary, strong neighbors become weak to have reduced inhibition, the remaining value gets sudden increase. While on the other side near the boundary, remaining value gets sudden decrease. This could make larger contrast than without it** (see the the illustration of Mach bands in our paper, with sudden curves towards two different directions along both sides of the boundary, to make the boundary more prominent).\n\n    **Our learnable weight W is flexible to (can be zero, positive or negative) to fit various lateral interactions.** For our method, by manually checking the learnable W at all layers (in total 5 layers with only one convolution per layer for AlexNet), it shows that most of the channels have positive weight for that channel from layer 1 ~ layer 3, meaning lateral inhibition happens (original value minus the multiplication value of positive weight and positive RELU activation value from neighbors) for these early layers in this network, which could increase the contrast and sharpness along the boundary. Then for layer 4 ~ layer 5, most of the channels have negative weight for that channel, meaning neighbors could enhance the activation (in biological findings, there are lateral interactions which neighbors inhibit or strengthen the signal). It makes sense that for early layers the network needs more spatial-accurate signals, but for high level layers, the network needs the signals (larger parts, concept) in large receptive field to combine all of them for recognition.\n\n\n\ncomment: Thank you for your time and valuable comments. Idea comes shortly before the initial submission, and we try to present the preliminary results for other researchers. Our paper is about lateral inhibition inspired structure. We have performed comparison with several normalization methods, according to some reviewers' opinion. We have obtained the code from authors of [1] (ICLR 2022), and outperform all of them by a large margin. We will address your concerns accordingly.\n\n - **The method is interesting but very simple: it adds a couple of convolutional layers to existing architectures.**\n\n    Yes, however, using a filter to be more brain-similar (only need to consider our naive design with guassian filter, it has learnable weight W for the guassian filter, to compare with related methods) from lateral inhibition perspective within the same channel to increase the contrast and sharpness along the boundary (while making homogeneous area less prominent, consider Mach band effect) is rarely explored. We have a learnable weight W for our guassian filter, which is flexible (could be zero, positive or negative) to fit various lateral interactions. *The convolution result will be subtracted from the original feature map.*\n\n\n- **The baseline architectures are not from recent works, and the model scales in the test are also small (e.g. ResNet18, AlexNet).**\n\n  Given limited time and resources, feature learning research may be only verified using small network on ImageNet dataset for image classification, as some brain-inspired methods [1][2] (ICLR 2022, 2021) using only AlexNet. We will do the best we can.\n\n- **I hope that the authors could add more experiments to the paper, show larger experiment settings, and polish the paper a bit more.**\n  \n  Yes, we will. We have requested and obtained the code from [2] (and fix their missing padding for the initial convolution) for comparison with full size AlexNet (initially our method uses Pytorch build-in official model for AlexNet, which comes from [3] with less channels). AlexNet is the only model used in [1][2] (ICLR 2022, 2021) on ImageNet dataset. With a simple scaling factor (to be learned) per channel, our method already outperforms all of them; with shift mean or bias (to be learned) per channel makes further improvement. Results on ImageNet dataset (best model for 90 epochs as [1], while 100 epochs show no much difference) for Top-1 Acc with 4 GPU setting:\n\n    - AlexNet (baseline) 52.82%\n    - GroupNorm    57.76%\n    - LayerNorm    58.81%\n    - BatchNorm    59.04%\n    - WeightedNorm-c (center)  59.41%\n    - DivisiveNorm  59.54%\n    - Ours (scale) 59.87%\n    - Ours (shift-scale) 60.26%\n    - Ours (scale-bias) **60.39%**\n\n[1] Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\n\n[2] Pan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. In International Conference on Learning Representations.\n\n[3] One weird trick for parallelizing convolutional neural networks. Alex Krizhevsky, 2014.\n\n\ncomment: Thank you for your time and valuable comments. Idea comes shortly before the initial submission, and we try to present the preliminary results for other researchers. Our paper is about lateral inhibition inspired structure. We have obtained the code from authors of [1] (ICLR 2022), according to some reviewers' opinion, and outperform related methods by a large margin.\n\n- **The authors claim some gain in their approach. But as we all know, after introducing attention or dynamic mechanism, a general neural network can have some improvement. In particular, the worse the network, the more obvious the improvement.**\n\n   Yes, we agree. However, the main goal of our paper is to present the effectiveness of the structure which could increase the contrast and sharpness along the boundary in a flexible manner per channel from lateral inhibition perspective, with very few parameters.\n\n- **In fact, filtering is not a new thing in neural networks.**\n\n  Yes, however, using a filter from lateral inhibition perspective (consider Mach band effect) is rarely explored.\n\n- **A very working method should be validated on large-scale tasks. In addition to classification, there is detection, segmentation, and so on.**\n\n  Yes, we agree. Given limited time and resources, feature learning research may be only verified using small network on ImageNet dataset for image classification, as brain-inspired methods [1][2] (ICLR 2021, 2022). We will do the best we can.\n\n- **The writing of this article is very poor. There are multiple copies of the text in the article. For example, there are multiple repetitions of text in the abstract and introduction. A good article should have a better way of expressing it.**\n\n  Yes, given the limited time before submission. We will modify the paper and incorporate related normalization methods as well as other parts before the deadline.\n\n- **Gaussian filtering and bilateral filtering are not new in neural networks. The method proposed in the article is equivalent to a bilateral filtering non-local neural network.**\n\n  Well, we have a learnable weight W for our guassian filter, which is flexible (could be zero, positive or negative) to fit various lateral interactions). *The convolution result will be subtracted from the original feature map.* \n\n- **The article associates its method with depth-wise convolution. This is very imprecise and arrogant. If this is true, is MLP-Mixer also a special case of this article?**\n\n  Please ignore the depth-wise design. Our naive guassian design could increase the contrast and sharpness along the boundary in a flexible manner, while making homogeneous area less prominent (consider Mach band effect). *The convolution result from lateral inhibition perspective will be subtracted from the original feature map.*\n\n- **As I said before, adding attention or dynamics to the neural network, such as senet, sknet, acnet, dynamic nets, will bring gains. There are too many such examples. So the results in Table 2 are not dazzling at all.**\n\n  Well, design from lateral inhibition perspective (consider Mach band effect) for spatial adjustment to increase the contrast and sharpness along the boundary (in a flexible manner) is rarely explored. It is more brain-similar and may provide some value for researchers.\n\n- **Comparison.**\n\n  We have requested and obtained the code from [2] (and fix their missing padding for the initial convolution) for comparison with full size AlexNet (initially our method uses Pytorch build-in official model for AlexNet, which comes from [3] with less channels). AlexNet is the only model used in [1][2] (ICLR 2021, 2022) on ImageNet dataset. Normalization usually takes the form with a scaling factor, shift mean, and bias. With a simple scaling factor (to be learned) per channel, our method already outperforms all comparison methods; with shift mean or bias (to be learned) per channel makes further improvement. Results on ImageNet dataset (best model for 90 epochs as [1], while 100 epochs show no much difference) for Top-1 Acc with 4 GPU setting:\n\n    - AlexNet (baseline) 52.82%\n    - GroupNorm    57.76%\n    - LayerNorm    58.81%\n    - BatchNorm    59.04%\n    - WeightedNorm-c (center)  59.41%\n    - DivisiveNorm  59.54%\n    - Ours (scale) 59.87%\n    - Ours (shift-scale) 60.26%\n    - Ours (scale-bias) **60.39%**\n\n[1] Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\n\n[2] Pan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. In International Conference on Learning Representations.\n\n[3] One weird trick for parallelizing convolutional neural networks. Alex Krizhevsky, 2014.\n\n\ncomment: Thank you for your time and valuable comments, as well as related models [1][2][3]. We will address your concerns accordingly.\n\n- **The key weakness of this paper is that there are no any explanations why their design improves the performance.**\n\n  It could increase the contrast and sharpness along the boundary in the same channel (consider Mach band effect). On one side near the boundary, strong  neighbors become weak to have reduced inhibition, the remaining value gets sudden increase. While on the other side near the boundary, remaining value gets sudden decrease.  Also we have a learnable weight W for our guassian filter, which is flexible (could be zero, positive or negative) to fit various lateral interactions.\n\n- **Why the low-pass filter with eliminating the central weight could model the LATERAL INHIBITION.**\n\n  We model lateral inhibition from neurobiology findings, that inhibition strength decreases with increased distance. Gussian (low-pass) filter could reflects the decay regarding the distance and is widely used (e.g., smoothing in image processing, multi-scale representation in Scale-space theory), as well as for lateral inhibition influence in space (the forms often assumed in theoretical calculations in [5]. \n\n  Our guassian inhibition design follows common lateral inhibition description that an excited neuron can reduce the activity of its neighbors, by taking inhibition from neighbors out of the original value. It can be viewed a special difference of Gaussian (DoG), with two gaussian filters of size 1x1 and 3x3, but central weight eliminated for 3x3 filter. For the case of one activate neuro with no activate neighbors, it receives zero inhibition from neighbors and remains the same value in our design, but decreases a lot with central weight.\n\n\n- **Eq (5) seems an OFF-receptive field?**\n\n  The convolution result needs to be subtracted from the original feature map in our method, so finally it is On-receptive field. For comparison purpose, we only focus on our guassian filter (naive design) with weight W. When W is positive, finally it is On-receptive field.\n\n\n- **The inhibition computation by subtracting a computed value from the previous one is commonly used in many previous models [1][2][3]. The traditional models simulated visual mechanisms should be clearly mentioned and discussed. The description of related work is very limited as the reason shown in above.**\n\n  Yes, they will be mentioned and discussed. Our method could increase the contrast and sharpness along the boundary in a flexible manner (while making homogeneous area less prominent for suppression), then the following convolution in the original AlexNet could combine different channels to learn different antagonistic response as in [1][3]. \n\n  For [2] (ICLR 2021), it does normalization along the channel dimension for their WNc (WeightedNorm-center) method. Our spatial lateral inhibition method outperform it. They also add spatial normalization after channel normalization, leading to decreased performance on ImageNet as stated in [2].\n\n- **The learned weight W plays the similar function of channel attention. Hence, what’s the difference between channel attention and the inhibition computation?**\n\n  The learned weight W decides the amplitude of our guassian filter and final value after spatial adjustment (before subtraction from original value). Channel attention aims to scale one entire channel with one value (by multiplication), which is learned using MLP on the average value of all channels. Channel attention has no spatial adjustment within that channel, which we do for each location individually (consider Mach band effect).\n\n- **Eq (5) seems an OFF-receptive field?  Author should visualize the learned kernels as many examples.**\n\n  We focus on our naive design of guassian filter (central weight eliminated), with learnable weight W for comparison.\n\n- **Comparison.**\n\n  Normalization usually takes the form with a scaling factor, shift mean, and bias. For comparison, with a simple scaling factor (to be learned) per channel, our method already outperforms all comparison methods; with shift mean or bias (to be learned) per channel makes further improvement. Results on ImageNet dataset (best model for 90 epochs as [2], while 100 epochs show no much difference) for Top-1 Acc with 4 GPU setting:\n\n    - AlexNet (baseline) 52.82%\n    - GroupNorm    57.76%\n    - LayerNorm    58.81%\n    - BatchNorm    59.04%\n    - WeightedNorm-c  59.41%\n    - DivisiveNorm  59.54%\n    - Ours (scale) 59.87%\n    - Ours (shift-scale) 60.26%\n    - Ours (scale-bias) **60.39%**\n\n[4] Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\n\n[5] On Tuning and Amplification by Lateral Inhibition. Proceedings of the National Academy of Sciences of the United States of America,\nVol. 62, No. 3 (Mar. 15, 1969), pp. 733-740.\n\n\ncomment: Thank you for your time and valuable comments, especially recent brain-inspired methods [3] (ICLR 2021), [2] (ICLR 2022). Idea comes shortly before the initial submission, and we try to present the preliminary results for other researchers. We will address your concerns accordingly. Paper will be revised to highlight our contribution and incorporate the related methods as well as comparison results.\n\n- **Novelty.**\n\n    (i) Most comparison methods in [2], [3] are normalization across different 2D channels at the same 2D spatial location (x, y), without spatial adjustment exploring neighbors within the same channel, which our method does (could increase the contrast and sharpness along the boundary in a flexible manner, while making homogeneous area less prominent, consider Mach band effect) to be more brain-similar. The only spatial normalization part (called surround norm) in WeightedNorm [3] is applied after channel normalization (called WeightedNorm-c, center norm). This combined design has the name of WeightedNorm-cs (center surround norm), and it has even decreased performance (than without spatial normalization) on ImageNet dataset according to [3]. \n\n\n    (ii) Things need to be noticed from [1] (SM, surround modulation) (NIPS 2019) and our method:\n\n      a) [1] applies two gaussian filters (5x5) with 1:1 weight (fixed) for the difference of Gaussian (DoG) function. While we use apply one gaussian filter (3x3, central value as 0), with the convolution result subtracted from the original feature map (could be viewed as two filters with size 1x1 and 3x3), to replace the two-gaussian-filter design. Our method has 1:w weight, where w is learnable (flexible) per channel for the amplitude of our (3x3) gaussian filter (can be zero, positive or negative) to fit various lateral interactions.\n\n      b) [1] only applies SM filter on half of the initial channels. Applying it on full channels leads to reduced performance, which indicates its limitation of flexibility.\n\n- **Comparison.**\n\n  We have requested and obtained the code from [2] (and fix their missing padding for the initial convolution) for comparison with full size AlexNet (initially our method uses Pytorch build-in official model for AlexNet, which comes from [4] with less channels). Normalization usually takes the form with a scaling factor, shift mean, and bias. For comparison, with a simple scaling factor (to be learned) per channel, our method already outperforms all comparison methods; with shift mean or bias (to be learned) per channel makes further improvement. Results on ImageNet dataset (best model for 90 epochs as [2], while 100 epochs show no much difference) for Top-1 Acc with 4 GPU setting:\n\n    - AlexNet (baseline) 52.82%\n    - GroupNorm    57.76%\n    - LayerNorm    58.81%\n    - BatchNorm    59.04%\n    - WeightedNorm-c  59.41%\n    - DivisiveNorm  59.54%\n    - Ours (scale) 59.87%\n    - Ours (shift-scale) 60.26%\n    - Ours (scale-bias) **60.39%**\n\n[4] One weird trick for parallelizing convolutional neural networks. Alex Krizhevsky, 2014."
            }
        ]
    },
    {
        "id": "xfqDe72zh41",
        "decision": "Accept: poster",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Actionable Neural Representations: Grid Cells from Minimal Constraints\nKeywords: Grid Cells, Representation Theory, Theoretical Neuroscience, Normative Models\nAbstract: To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an  `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode."
            },
            {
                "round2": "Reviewer 1:\nI would like to thank the authors for their response to my comments. This response and revision improve several aspects of the work. \n\n(1) I am still unconvinced about the argument of 2-by-2 blocks, and the claim that going from Eq (3) to Eq (6) follows from the deduction. The authors seem to take something (it is that \"something\" I'm trying to figure out) for granted. Why the representation should be compact? And why should it be two-d? Why should it be flat? The grid cell system embeds the 2-d space into a high-dimensional representation. It is not at all clear why the embedding dimension should be 2D, or why the representation should consist of 2D subspaces as the authors are using. To me, these are all assumptions.  I'd be thankful if the authors could clarify these issues further.\n\n(2)  It is also unclear to me why the model proposed here really cast the problem as a path integration problem. It appears that the only thing needed from the path integration argument is Eq (6). After that point, everything becomes a coding argument. Thus the claim of this theory being a path integration theory seems to be a bit misleading to me. \n\nReviewer 2:\nsummary_of_the_paper: In this paper, the authors provided a solution of how to parametrically hard code grid cells with learnable parameters in order to be functional, actionable, and biological by minimizing the objective function. Under these three constraints, they showed that hexagonal grid cells are the optimal representation of locations in 1D angular and 2D euclidean space.\n\nstrength_and_weaknesses: Strength:\n- To-date, most computational models on grid cell emergence have focused on training a neural network to solve path-integration tasks and selecting grid cells based on their known neuroscientific properties. This creates a possibility, which also applies to other EC-HPC representations, that the neural representations are simply due to correlation with different behavioral variables (i.e. “you see what you’re looking for”). In this paper, the authors only imposed normative constraints rather than assumptions about known properties of grid cells, and still observed grid cells, which fills the gap in the literature on why EC-HPC representations exist in the first place.\n\nMajor weaknesses:\n- Though it provides a good model for grid cell emergence, this framework is somewhat isolated in that it doesn’t consider the interactions between MEC (where grid cells are observed) and other regions in the EC-HPC circuit. For example, under this framework, do grid cells still help explain place cell remapping (Whittington et al., 2020)?\n- The metric described in this paper (i.e. functional, biological, actionable) still does not account for whether the grid cells representations are useful for behavior, which in my opinion is what “functional” should mean. I think it would make the paper even stronger if you could show the connection between your model and path integration behavior, or some other behavioral task, eg. train model on path integration task and do lesion study.\n- As raised in Schaeffer et al., 2022, grid cell emergence depends on hyperparameters and implementation choices. Is that also the case here? Eg. What happens when you increase the number of neurons? Or other hyperparameters listed in section B.3? It would be interesting to see whether the claim from Schaeffer et al. still holds when grid cells emerge from not path-integration but mathematical optimization.\n- One can also see grid cells in non-spatial settings (Whittington et al., 2020), where a different set of constraints may apply (eg. the resolution of representation does not necessarily follow a Goldilocks of frequency). And grid cells have been shown to represent other behaviourally relevant variables besides location, such as time and distance [(Kraus et al., 2015)](https://www.cell.com/neuron/fulltext/S0896-6273(15)00820-X?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS089662731500820X%3Fshowall%3Dtrue) . How can your model account for this? Or is it only applicable to spatial location?\n\n\nMinor weaknesses:\n- Please remake all hand drawn figures, including the ones in appendix, with computer softwares\n- Page 5 first paragraph: “... and $\\theta$ dependent parts (equation 6; Figure 3A)”: 1) Adding a hyphen, i.e. $\\theta$-dependent, might be helpful for readers. 2) Do you mean equation 5?\n- Page 5 first paragraph: “...This effect is limited by the firing rate bound: $||a_0||^2 - 2 L_0 = N$...\"\nDo you mean $1/2 L_0$ ?\n\n\n\nclarity,_quality,_novelty_and_reproducibility: Among the EC-HPC models, the paper is quite novel in terms of technicality. The ideas are clearly communicated, and as long as the simulation codes are released it should be reproducible.\n\nsummary_of_the_review: I’d recommend accepting this paper for ICLR, as it provides a normative explanation for why the multi-module representation of grid cells is the optimal representation of space. The paper is well-written and clearly communicated, but also please remake your hand-drawn figures.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 4: The contributions are significant, and do not exist in prior works.\n\nReviewer 3:\nsummary_of_the_paper: This paper seeks to explain the properties of grid cells. Linear group representation consideration leads to the superposition of sine and cosine plane waves. A specially designed loss function is proposed to explain the emergence of hexagon grid patterns under the constraints of non-negative and bounded firing rate. The paper makes several predictions based on the proposed theory. \n\nstrength_and_weaknesses: Strengths: \n\n(1) The proposed loss function is new and reasonable. \n\n(2) The explanation of the emergence of hexagon grid patterns is sound. \n\n(3) The predictions seem to be supported by empirical evidence. \n\nWeaknesses: \n\n(1) The mathematical analysis in the main text is not very rigorous, although it is quite insightful and intuitive. \n\n(2) The superposition of sine and cosine waves seems too restrictive, and may not model biological neurons. \n\n(3) The proposed loss function, although reasonable, is not simple enough to serve as a first principle. \n\n(4) The linear transformation model does not cover the continuous attractor network. \n\n\n\n\nclarity,_quality,_novelty_and_reproducibility: About novelty, the group representation perspective has been previously investigated by Gao et al. (2021) and earlier Gao et al. (Learning grid cells as vector representation of self-position coupled with matrix representation of self-motion. ICLR 2019). \n\nAbout actionable, any RNN that transforms the hidden vector based on the input action is an actionable representation, including the state space model in control, as well as latent space model-based RL, such as the dreamer model. The hidden vectors in most RNNs are actionable if you interpret the input as an action. As long as the transformation depends on the action, it does not need to be linear. In fact, the linear transformation or matrix group is not entirely biologically plausible because the recurrent connection weight matrix depends on the action. \n\nThe paper is clearly written, and the results and predictions are clearly explained. \n\nThe mathematical reasoning is good, but not very rigorous, at least in the main text. \n\nAssuming Fourier plane waves is too specific and restrictive, with little biological plausibility. The continuous attractor network seems more biologically plausible. \n\nAbout nonnegativity constraint, it is biologically true. But it is unclear if it is entirely necessary theoretically. Gao et al (2019, 2021) learned hexagon patterns without this constraint. \n\nsummary_of_the_review: The paper provides an interesting explanation of the emergence of hexagon grid patterns of the grid cells. However, the loss function is not minimalistic and the superposition of Fourier plane waves is not generic enough. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 4:\nsummary_of_the_paper: This paper develops a theory to understand the grid cell responses by combining ingredients from existing studies. They articulate three properties that are needed for the grid responses to appear when representing space: actionable, functional, and biological.\nThis paper contains some quite interesting ideas. But at the same time,  it is also highly problematic in multiple ways.  These concerns greatly diminished my enthusiasm for this paper. \n\nstrength_and_weaknesses: Strengths:\n\nThe attempt to unify a somewhat messy literature is ambitious and should be applauded.\nThe proposed three key requirements for the grid patterns, if true, would be a useful finding.\nThe argument in Section 3.1 seems to be quite insightful.\n\n\nWeaknesses:\n\n** The idea of “actionable representation” and the use of representation theory and group theory to study the grid cell system is not a new one (e.g., see a recent line of research in Gao et al, ICLR, 2019; Gao et al., NeurIPS, 2021). The current paper did refer to Gao et al 2021 in the Introduction and had a detailed comparison to that paper in the SI. But it did not acknowledge that using group theory to study the grid cell was a key idea in that line of work. \n\n** The math presented in the paper is not rigorous. Eq. (5) actually is a strong assumption. The paper seems to say that it is a necessary consequence of Eq. (3), thus misleading. Assuming matrix T consists of 1-by-1 and 2-by-2 blocks is a critical assumption that greatly simplified the problem.\n\n** The writing needs to be improved. Various statements need to be toned down. The math in the SI needs to be better organized. Phrases such as “big result” should probably be removed.\n\n** The comparison to the data is rather preliminary and unconvincing.\n\n** Eq. (1) needs to be better motivated. The second and third terms together lead to a band-pass filter— this seems to be rather post-hoc. Previously, it was known that band-pass filtering can lead to grid patterns (Sorscher, Mel, et al, NeurIPS 2019). Some previous studies used the different-of-Gaussian place fields as inputs to the grid cell network, leading to band-pass filtering. This connection needs to be explained.  How the present theory is different from Sorscher, Mel, et al, NeurIPS 2019 also needs to be better explained. \n\n** The study will benefit by treating neural noise more carefully.  It is unclear what assumption the model makes about the neural noise.\n\n** how are Eq (3) and the positivity constraint compatible theoretically? The author seems to impose positivity after using Eq (3). If that is the case, Eq. (3) is no longer satisfied. I’d appreciate further clarification on this point.\n\n\n---Other comments:\n**It is unclear what the next paragraph really means:\n “To motivate our current rabbit hole further, this is exactly the result we hinted towards in Section 2. We discussed T(∆θ), and, in 2-dimensions, argued it was, up to a linear transform, the 2-dimensional rotation matrix. Further, we discussed how every extra two neurons allowed you to add another frequency, i.e. another rotation matrix. These rotation matrices are the irreps of the rotation group, and adding two neurons allows you to create a larger T (∆θ) by stacking rotation matrices on top of one another. Now including the invertible linear transform, S, we can state the 4-dimensional version of equation 4.” \nI interpreted that the authors were making an important assumption here according to their math, but from the text,  it read like there is no assumption involved and everything just follows from the rule of deduction.\n\n**The following might be a useful paper to discuss. While the authors assume that the grid cells are representing one space, there is work suggesting that grid cells may represent multiple spaces.\nSpalla, Davide, et al. \"Can grid cell ensembles represent multiple spaces?.\" Neural Computation 31.12 (2019): 2324-2347.\n\n\nclarity,_quality,_novelty_and_reproducibility: The clarity and quality would need some major improvement.  The novelty and significance were over-stated in the current version.\nOverall, it feels that the work is a bit incremental. \n\nsummary_of_the_review: An ambitious attempt to unify grid coding theory with several major concerns. Various improvements could be made to strengthen the paper.\n\n==========\nThe revision addressed some of concerns and improved the paper, although some of my conerns remain. In my view, the revision has turned this into a borderline paper. I've increased my score from 3 to 5. \n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: Hi!\n\nThanks for updating your evaluation of our work, we really appreciate your help improving this manuscript.\n\nWe remain open to discussing confusing aspects of our work and answering any questions you have.\n\ncomment: > (2) It is also unclear to me why the model proposed here really cast the problem as a path integration problem. It appears that the only thing needed from the path integration argument is Eq (6). After that point, everything becomes a coding argument. Thus the claim of this theory being a path integration theory seems to be a bit misleading to me.\n\nLet’s define path-integration as the following: given a start position and a series of movements, you are able to say which position you end up in. Seems reasonable?\n\nThis requires two things, that we have conveniently decomposed into actionability and functionality. You must be able to sum up the series of movements and work out their net effect (actionability). Then, once you’ve worked out where you’ve moved to in your representation, you have to be able to say which position you’ve ended up at (functionality). Actionability ensures you can add up movements and work out their consequences (by ensuring the representation transition rules mirror the variable’s transition rules). Functionality ensures you can tell which position you are at (by making the representation of all the points you might visit different from one another).\n\nSo, by this argument, the whole paper is a path-integrating theory! We really are trying to get the core ideas necessary for a representation to path-integrate, and find out what that implies for a representation, that is all. Sorry if this was not made clear.\n\nOne final comment: the fact that actionability leads us to a very simple criterion (equation (6)) is, in our view, something to be celebrated! To us it is concise, beautiful, and useful. That does not mean it is unimportant! In fact, it is a (perhaps the?) key piece of our work. Without actionability grids are not optimal (see ablation study, appendix I). Further, it is the novel theoretical development that will hopefully generalise. So while, yes, actionability simply gets us to eqn. (6), that it does belies its fundamental importance to our work!\n\n\ncomment: Thanks for the response! We look forward to hashing this out!\n\n> (1) I am still unconvinced about the argument of 2-by-2 blocks, and the claim that going from Eq (3) to Eq (6) follows from the deduction. The authors seem to take something (it is that \"something\" I'm trying to figure out) for granted. Why the representation should be compact? And why should it be two-d? Why should it be flat? The grid cell system embeds the 2-d space into a high-dimensional representation. It is not at all clear why the embedding dimension should be 2D, or why the representation should consist of 2D subspaces as the authors are using. To me, these are all assumptions. I'd be thankful if the authors could clarify these issues further.\n\nSo, let’s first establish that the 2-by-2 blocks, and the fact that $g(x)$ is a 2D manifold, are separate.\n\nOur starting premise is that we want to understand how the brain represents periodic 2D position, i.e. the vector of neural firing rates $g(x)$. $x$ is a 2-dimensional compact variable, therefore $g(x)$ is necessarily 2-dimensional and compact. Every position on the manifold of all possible $g(x)$ is specified by a position in periodic 2D space, $x$, no assumption there! Let's call the set of all possible $g(x)$ the activity manifold.\n\nFurther, there is no assumption that the activity manifold is flat, it can be, and is, very curvy! None of our representations, $g(x)$, can be understood as activity lying in a flat 2D plane, they are all curvy.\n\nSo, we’re dealing with a curvy 2-dimensional activity manifold in a high dimensional neural space, and we’d like to choose this manifold so that it is the best, by our definition of best. Great. Now let’s return to the 2-by-2 blocks, the irreps.\n\nThe fact that both the irreps and the activity manifold are 2-dimensional is a numerological accident! Generally they won’t have the same dimensionality. If you are considering the representation of an angle then $g(\\theta)$ is a curvy 1-dimensional manifold (since the $\\theta$ is 1-dimensional) but the irreps are still 2-by-2. A more extreme example is the representation of a position on a sphere; there again the position is 2-dimensional, hence $g(x)$ is again a curvy 2-dimensional manifold, but the irrep blocks can take any odd-numbered dimensionality (i.e. 3-by-3 blocks, or 9-by-9 blocks).\n\nOkay, so we’ve established that the dimensionality of the representation and the irreps are independent quantities that both happen to be 2 for 2D space. Now let’s go through one last section on what these 2-by-2 spaces are.\n\nThe only assumption we make is actionability: we want there to exist matrices for all movements, $T(\\Delta x)$, that consistently update the representation $T(\\Delta x)g(x) = g(x + \\Delta x)$. This is core to our programme, it makes the representation interesting, allowing it to perform clever things like zero-shot inference of next position given an action, as discussed in the main paper. To reiterate, that is our only additional constraint!\n\nIt is then a deduction that uses the Peter-Weyl theorem and a couple of lines of logic to say that $g(x)$ has the form shown in equation (6). And it is this deduction that we invest effort to try to make intuitive (though it seems we have failed!).\n\nThe game is the following: find a set of matrices $T(\\Delta x)$ that when you multiply them together using matrix multiplication they obey the same rules that adding up 2D translations obey on a periodic 2D space. Things like $T(\\Delta x_1) T(\\Delta x_2) = T(\\Delta x_1 + \\Delta x_2)$, $T(-\\Delta x) = T(\\Delta x)^{-1}$, $T(0) = $Identity Matrix. It turns out any real matrix that satisfies all these rules (and is therefore a representation of the group) can, up to a linear transform, be made from 2-by-2 blocks each of which is rotation at some frequency; a consequence of the Peter-Weyl theorem. Our efforts to make this intuitive highlight the fact that rotation matrices satisfy these properties, and that stackings of rotation matrices also satisfy these properties. We do not have an intuitive answer for why these are the only possible matrices, but they are nonetheless!\n\nTo briefly elaborate on one point: the only constraint that this implies on $g(x)$ is shown in equation 6. This does not make it flat. It is true and useful that if you apply an appropriate linear transform to $g(x)$ you can arrange things so that in each 2-dimensional subspace the activity loops round a circle, but importantly (A) that required a linear transform, without which this is not true, (B) even ignoring (A), the fact that you can project it onto 2D subspaces with easy transition rules does not mean the activity manifold is flat! The activity manifold is the sum of all these different components, hence can have complex curvy properties in the higher-dimensional neural space.\n\nDoes that make any more sense? Please continue to ask questions, there’s some misunderstanding here that it would be great to hash out!\n\n\ncomment: Hi!\n\nAs the end of the discussion period is fast approaching, do let us know if you have any further comments or concerns to which we can respond. We'd be interested to hear if our comments and changes to the manuscript helped answer your questions, especially concerning the use of representation theory! Is the justification for our logic now clearer?\n\ncomment: Hi!\n\nAs the end of the discussion period is fast approaching, do let us know if you have any further comments or concerns to which we can respond. We'd be interested to hear if our comments and changes to the manuscript helped answer your questions, especially concerning the minimality of our loss and the restrictiveness of sines and cosines!\n\ncomment: Hi! \n\nAs the end of the discussion period is fast approaching, do let us know if you have any further comments or concerns to which we can respond. We'd be interested to hear if our comments and changes to the manuscript helped answer your questions, especially about the parameter dependence of our solutions!\n\n\n\ncomment: Hi All,\n\nWe'd like to share an anonymous github repo with all our code here:\n\nhttps://anonymous.4open.science/r/ICLR_Actionable_Reps-67B5/readme.md\n\nFurther, as the end of the discussion period is fast approaching, we would appreciate hearing any feedback or lingering questions that you may have!\n\ncomment: We minorly revised our submission one final time to include the results of a very recent preprint (released this week!) \n\nOtherwise the submission is unchanged.\n\nWe look forward to discussion with the reviewers!\n\ncomment: We would like to thank all the reviewers for their time and comments. We appreciated their feedback and their comments have greatly improved the manuscript. \n\nWe have sent detailed responses to each of your comments, and have made numerous edits to the manuscript according to your comments and to address parts of the paper that were not stated clearly the first time around. In the submission we have highlighted the larger changes in a sort of plum colour. Below we summarise major changes we made to the manuscript, we look forward to discussing more in the coming weeks!\n\n__Summary of larger changes to the Manuscript:__\n\n1) In section 2 and Appendix A we have tried to outline our use of representation theory more formally. In particular, our reliance on approximating a small region of flat 2D space arbitrarily well with a small region of a very large 2D periodic space, and the pivotal role the Peter-Weyl theorem plays in our arguments.\n2) In Appendix B (B.3 and B.4) we have added a much more extensive numerical exploration of the role of the key parameters that define our loss (l, L, sigma, N) on the optimal representations.\n3) In the Related work section and Appendix H we have tried to highlight the relationship of our work to related ideas in Gao et al. (2018, 2021). Further, we have added a section highlighting the similarities and differences between our work and that of Sorscher, Mel et al. 2019.\n\n\ncomment: > __Comment 1:__ It is unclear what the next paragraph really means: “To motivate our current rabbit hole further, this is exactly the result we hinted towards in Section 2. We discussed T(∆θ), and, in 2-dimensions, argued it was, up to a linear transform, the 2-dimensional rotation matrix. Further, we discussed how every extra two neurons allowed you to add another frequency, i.e. another rotation matrix. These rotation matrices are the irreps of the rotation group, and adding two neurons allows you to create a larger T (∆θ) by stacking rotation matrices on top of one another. Now including the invertible linear transform, S, we can state the 4-dimensional version of equation 4.” I interpreted that the authors were making an important assumption here according to their math, but from the text, it read like there is no assumption involved and everything just follows from the rule of deduction.\n\nSorry for the lack of clarity. Hopefully the above responses mean that this paragraph makes more sense. In brief - it is not an important assumption, rather, it is trying to provide intuition for the Peter-Weyl theorem, and how that result relates to the intuition we provided in section 2 of the main paper. We hope our paper will be read by neuroscientists, and so aimed for maximal understandability. We have left this paragraph as is for now, hoping that our preceding discussion will make it intelligible. Please let us know if it is still opaque.\n\n> __Weakness 5:__ Eq. (1) needs to be better motivated. \n\nMany thanks for the comment. To understand a space an animal/agent must 1) have different representations for different locations otherwise it would confuse two different locations. 2) it cannot understand locations exactly as that would require infinite neurons. 3) There is no point trying to represent locations you never go to. Eq 1 is a loss that characterises these three things: The functional loss itself consists of three parts - 1) making sure each location is coded differently, 2) caring about this difference up to a certain resolution, and 3) only caring about locations actually visited. \n\n> The second and third terms together lead to a band-pass filter— this seems to be rather post-hoc. Previously, it was known that band-pass filtering can lead to grid patterns (Sorscher, Mel, et al, NeurIPS 2019). Some previous studies used the different-of-Gaussian place fields as inputs to the grid cell network, leading to band-pass filtering. This connection needs to be explained. How the present theory is different from Sorscher, Mel, et al, NeurIPS 2019 also needs to be better explained.\n\nAs above, these terms are not post-hoc. We appreciate your comment on the  relationship to Sorscher, Mel et al 2019. However, their rationale for hexagons is very different to ours. Sorcsher, Mel et al. show that non-negativity leads to triplet interactions that produce hexagonal grids. This argument however is dependent on some specific (and likely implausible - Schaeffer et al 2022) aspects of the place cell code. Our argument is that non-negativity leads to a lattice of frequencies, and that a high pass (from p(x)) and low pass filter (from chi, i.e.  only resolving space up to a certain scale) selects a hexagonal lattice due to optimal packing. These are two very different arguments.\n\nBeyond these technical differences, our work captures key aspects of the grid code that are not present in Sorscher, Mel et al.’s description. For example we can explain in our framework why grid axes align. Further, we provide a normative derivation of multiple modules, which we believe only arise in Sorscher, Mel et al. due to quantization of the position (Figure 3). But most crucially, our predictions come as a result of our normative framework for understanding generic representations.\n\nWe have added a section, Appendix H.2, that talks through these points.\n\n> __Comment 2:__ The following might be a useful paper to discuss. While the authors assume that the grid cells are representing one space, there is work suggesting that grid cells may represent multiple spaces. Spalla, Davide, et al. \"Can grid cell ensembles represent multiple spaces?.\" Neural Computation 31.12 (2019): 2324-2347.\n\nThank you for pointing us to this work, which we were not aware of previously. Our work does indeed assume that at any one time the grid cells represent one space, but at different times, depending on the shape of the environment, the grid cells may choose to map that to real space in different ways, e.g. by stretching and warping the grid. This underlies the analysis of how the different room shapes affect the optimal solution, and the suggestion that the grid cells change their coding to reflect the optimal solution in that room, a suggestion that is supported by data (Stensola et al. 2015). This could be implemented mechanistically by up or down-weighting the velocity input to a continuous attractor network in different environments.\n\n\ncomment: > __Weakness 2 Cont.__: Eq. (5) actually is a strong assumption. The paper seems to say that it is a necessary consequence of Eq. (3), thus misleading. Assuming matrix T consists of 1-by-1 and 2-by-2 blocks is a critical assumption that greatly simplified the problem.\n\nWe thank the reviewer for highlighting a confusing aspect of our work that was not explained well enough. The crucial point is that Eq. 5 & 6 follow exactly from Eq. 3 for compact topological groups (e.g. the transformation group for variables on a circle, 2D and 3D torus, or sphere). This is because the Peter-Weyl theorem guarantees the existence of a set of irreducible representations, and that any representation of the group is a direct product of the irreducible representations of the group. This is what we used to arrive at the matrix T, made from 1-by-1 and 2-by-2 blocks. Hence, it is this theorem, and a couple of lines of simple logic shown in Appendix A.3, that means Eq. (3) to Eq. (6) is a deduction, not an assumption.\n\nWe believe the confusion might have arisen because (A) we wrote the appendix to be intuitive, rather than formal, and (B) we did not highlight enough that at all times we are considering compact groups (i.e. the 2D variable is periodic) - rather than a non-periodic 2D space. In section 2 of the main paper we had mistakenly said variable x lived in $\\mathbb{R}^2$, but we had meant the variable lived in $\\mathbb{T}^2$, i.e. on a torus. We apologise for this, and have amended the text accordingly.\n\nWe have re-written the paragraph that leads up to equation 6 in order to highlight the key step: our (arbitrarily good) approximation of a region of flat 2D space with an equivalently sized region of periodic 2D space. Again, apologies for not doing so earlier. The text now reads:\n\n*This argument comes from an area of maths called Representation Theory (a different meaning of representation!) that places constraints on the matrices $\\bf{T}$ for variables whose transformations form a mathematical object called a group. This includes many of interest, such as position on a circle, torus, or sphere. These constraints on matrices can be translated into constraints on an actionable neural code just like we did for $\\bf{g}(\\theta)$ (see Appendix A). When generalising the above example to 2D space (a torus), we must consider a few things: First, the space is two-dimensional, so compared to our previous equation 5, the frequencies, denoted $\\bf{k}_d$, are now two dimensional. Second, to approximate a finite region of flat 2D space, we consider a similarly sized region of a torus. As the radius of the torus grows this approximation becomes arbitrarily good (see Appendix A.4 for discussion). Periodicity constrains the frequencies in equation 5 to be $\\frac{n}{R}$ for integer $n$ and ring radius $R$. As the loop (torus in 2D) becomes very large these permitted frequencies become arbitrarily close, so we drop the integer constraint,*\n\nAs a note, you are correct that if we had been considering a non-compact group such as non-periodic 2D space, with representations like this:\n$$\\bf{T}(\\bf{\\Delta x}) = \\begin{pmatrix}1, 0, \\Delta x\\\\\\\\ 0, 1, \\Delta y \\\\\\\\ 0, 0, 1\\end{pmatrix} $$\nthen we could not go from eq 3 to eq 5 or 6. We have edited Appendix A.2, and added an extensive discussion of this point in appendix A.4.\n\n> __Weakness 7:__ how are Eq (3) and the positivity constraint compatible theoretically? The author seems to impose positivity after using Eq (3). If that is the case, Eq. (3) is no longer satisfied. I’d appreciate further clarification on this point.\n\nWe agree that for the non-compact example above then non-negativity and eq. 3 are not compatible. However, Eq (3) and non-negativity are compatible theoretically: e.g. $\\bf{g}(\\bf{x})$ = vector of 1s, $\\bf{T}(\\bf{\\Delta x})$ = identity matrix. Equally, take the example representation of a 1D variable in figure 3A. It is positive (since the orange curve lies in the positive orthant) and satisfies the 1D version of equation (3), since there is a matrix which rotates the activity around the orange curve. \n\n> __Weakness 3:__ The writing needs to be improved. Various statements need to be toned down. The math in the SI needs to be better organized. Phrases such as “big result” should probably be removed.\n\nApologies for this. We have endeavoured to improve our writing. For example we have changed “big result” to relate to the Peter-Weyl theorem, which is indeed a big result for the development of our ideas - all our results rest on it! We hope all our changes have helped with readability.\n\n\n\ncomment: >  The attempt to unify a somewhat messy literature is ambitious and should be applauded. The proposed three key requirements for the grid patterns, if true, would be a useful finding. The argument in Section 3.1 seems to be quite insightful.\n\nWe thank the reviewer for the enthusiasm for our aims. We hope that our responses clarify comments and are convincing that we have made inroads into achieving these aims.\n\nWe really appreciate the detailed reading and suggestions regarding our submission - your comments have helped us create a better manuscript. We hope we have successfully answered your concerns below, and look forward to discussing further.\n\n>  __Weakness 1:__ The idea of “actionable representation” and the use of representation theory and group theory to study the grid cell system is not a new one (e.g., see a recent line of research in Gao et al, ICLR, 2019; Gao et al., NeurIPS, 2021). The current paper did refer to Gao et al 2021 in the Introduction and had a detailed comparison to that paper in the SI. But it did not acknowledge that using group theory to study the grid cell was a key idea in that line of work.\n\nApologies, we had not meant to downplay the links to the group theory used in the work of Gao et al., and this was also highlighted by another reviewer. We have rewritten the SI section to highlight the use of group theory in this work, and the differences between our two approaches. The key difference is that we use group and representations theory to provide analytic justification for why grid cells emerge, whereas Gao et al. provide numerical simulations only. We also explain multiple modules, introduce biological constraints, and make neural predictions.\n\nWe added this paragraph to Appendix H.1:\n\n*Theoretically, Gao et al. develop a rich group theory view for studying grid cells. We add to this work using Representation theory to make clear what constraints linear actionability implies (equation (5)). Given this starting point we are able to explain intuitively and with analytic insight why this loss leads to grid cells, extend to multiple modules, and make neural predictions.*\n\nAnd this sentence to the literature review:\n\n*Our dive into Representation Theory builds on the group theory ideas in Gao et al. (2018, 2021)*\n\n> __Weakness 4:__ The comparison to the data is rather preliminary and unconvincing.\n\nWe certainly agree with you that these results are preliminary. We would love to test these claims more quantitatively, but we did the best comparisons possible to data we found online. We are now in the process of getting data to test these claims.\n\nThat said, we believe the presented data is favourable and worth sharing. For example, no-one has (to our knowledge) noticed or explained the consistently small angles between grid modules, nor given a normative explanation of why some grids squash and others maintain their lengthscale when the room changes.\n\n> __Weakness 6__: The study will benefit by treating neural noise more carefully. It is unclear what assumption the model makes about the neural noise.\n\nThe reviewer is right: we do not explicitly consider neural noise and it would be interesting to do so. Implicitly the loss (equation (1)) could be interpreted as minimising the overlap between noisy neural firing where the noise is gaussian with a lengthscale of order sigma. Adding noise in a more systematic or biological way seems like an interesting direction for future work.\n\n> __Weakness 2:__ The math presented in the paper is not rigorous. \n\nMany thanks for the comment. We understand your frustration that the maths does not appear rigorous in the main text. This was a choice of ours to appeal to both experimental and theoretical neuroscientists. We kept the rigorous maths to the appendix (Appendix A: the step from actionability to sines and cosines, Appendix C: the analysis of the simple functional loss, Appendix D: the high and low frequency biases, Appendix E: the harmonic penalty, Appendix L: how the loss changes in different rooms) and feel it is best for it to stay there to the paper is accessible to a wider audience. \n\ncomment: > __Weakness 4:__ The linear transformation model does not cover the continuous attractor network.\n__And:__ Assuming Fourier plane waves is too specific and restrictive, with little biological plausibility. The continuous attractor network seems more biologically plausible.\n__And:__ About actionable, any RNN that transforms the hidden vector based on the input action is an actionable representation, including the state space model in control, as well as latent space model-based RL, such as the dreamer model. The hidden vectors in most RNNs are actionable if you interpret the input as an action. As long as the transformation depends on the action, it does not need to be linear. In fact, the linear transformation or matrix group is not entirely biologically plausible because the recurrent connection weight matrix depends on the action.\n\nThis is a great point. We agree that it is likely that the brain uses a continuous attractor network and we agree that our theory does not cover the continuous attractor network. However, we note that our theory does not try to either. Instead we propose a normative theory of what spatial representations in the brain should look like, rather than considering the particular machinery that the brain may use. We are not trying to understand mechanism, we are trying to understand representational principles.\n\nWe do think that it would be possible to approach the same problem from the constraints that a continuous attractor network would impose, and in particular when the activation function is non-negative. There are already some insights by Sorscher, Mel et al 2019 on this, but we suspect it may be possible to incorporate some of our analysis in too. However, we leave this for future work.\n\nIt’s true that any RNN that appropriately transforms a hidden vector based on an input action has an actionable representation. The crucial thing that we do though is translate actionable into a theoretical framework, where it can be analysed with maths and we can learn something about the neural representations of an actionable representation.\n\n> __Comment 1:__ About novelty, the group representation perspective has been previously investigated by Gao et al. (2021) and earlier Gao et al. (Learning grid cells as vector representation of self-position coupled with matrix representation of self-motion. ICLR 2019).\n\nThis is true, and though we tried, we should have done a better job of highlighting the similarities and differences of our work from Gao et al.’s.. We have added the following sentence to the literature review:\n\n*Our dive into Representation Theory builds on the group theory ideas in Gao et al. (2018, 2021)*\n\nAnd added this paragraph to Appendix H.1:\n\n*Theoretically, Gao et al. develop a rich group theory view for studying grid cells. We add to this work using Representation theory to make clear what constraints linear actionability implies (equation (5)). Given this starting point we are able to explain intuitively and with analytic insight why this loss leads to grid cells, extend to multiple modules, and make neural predictions.*\n\n> __Comment 2:__ About the nonnegativity constraint, it is biologically true. But it is unclear if it is entirely necessary theoretically. Gao et al (2019, 2021) learned hexagon patterns without this constraint.\n\nThis is a good point. Gao et al. do indeed learn hexagons without non-negativity, but with an additional isotropy term in the loss. We are very interested in analysing how this emerges. Regardless, it seems you have to choose either non-negativity or isotropy. Considering that non-negativity is an intrinsic biological constraint, while isotropy is not, we believe non-negativity is sensible to include.\n\n\n\n\ncomment: > This paper seeks to explain the properties of grid cells. Linear group representation consideration leads to the superposition of sine and cosine plane waves. A specially designed loss function is proposed to explain the emergence of hexagon grid patterns under the constraints of non-negative and bounded firing rate. The paper makes several predictions based on the proposed theory.\n\nWe thank you for your detailed and useful comments on our work. We have made changes accordingly, and hope that these changes, along with our response here, address your concerns. We look forward to continued discussion in the coming weeks.\n\n>  __Strengths:__\n> 1) The proposed loss function is new and reasonable.\n> 2) The explanation of the emergence of hexagon grid patterns is sound.\n> 3) The predictions seem to be supported by empirical evidence.\n\nMany thanks!\n\n>  __Weakness 1:__ The mathematical analysis in the main text is not very rigorous, although it is quite insightful and intuitive. \n> __And:__ The mathematical reasoning is good, but not very rigorous, at least in the main text.\n\nMany thanks for the comment. We understand your frustration that the maths does not appear rigorous in the main text. This was a choice of ours to appeal to both experimental and theoretical neuroscientists. We kept the rigorous maths to the appendix (Appendix A: the step from actionability to sines and cosines, Appendix C: the analysis of the simple functional loss, Appendix D: the high and low frequency biases, Appendix E: the harmonic penalty, Appendix L: how the loss changes in different rooms) and feel it is best for it to stay there to the paper is accessible to a wider audience. \n\n>  __Weakness 2:__ The superposition of sine and cosine waves seems too restrictive, and may not model biological neurons.\n\nThis is fair. We agree that our approach is simple, and we see that as a positive since the approach is general to any group representation, and the simplicity still captures many aspects of the grid cell code and makes numerous neural predictions.\n\nIn general, however, a superposition of sines and cosines is very unrestrictive and can account for any smooth function as per Fourier. Our case is more restrictive than that since we only allow a certain number of frequencies present (with more frequencies allowed with more neurons), rather than the potentially infinite frequencies as per Fourier. Thus our representation derived from group theory, is neither unrestrictive nor too restrictive - the number of neurons controls this restrictiveness.\n\nOverall, we agree that our theory is not a perfect model of biology, and there are cases, highlighted in the discussion, where our model will be wrong. However, the test of whether it is a useful model will come down to whether it allows new phenomena to be understood, or provides new interpretations. We are hopeful that this is the case, and have presented preliminary evidence for this. We are in the process of more quantitatively testing our theory’s predictions in data, though this will not be ready for many months.\n\n>  __Weakness 3:__ The proposed loss function, although reasonable, is not simple enough to serve as a first principle.\n\nMany thanks for the comment. We hope that it is clear from our theory and ablation studies that each component - actionability, functional, and biological - are each crucially important. Without any one of these, then we do not observe grid cells, but with all three we do, thus these three are minimal. Perhaps you are concerned that the functional loss is not simple enough? In this case we hope that we are able to convince you here: the functional loss itself consists of three parts - 1) making sure each location is coded differently, 2) caring about this difference up to a certain resolution, and 3) only caring about locations actually visited. This is the minimal set that an agent/animal has to understand: 1) it needs to have different representations for different locations otherwise it would confuse two different locations. 2) it cannot understand locations exactly as that would require infinite neurons. 3) There is no point trying to represent locations you never go to.\n\nWe are unsure of how to answer this further though, as it is not clear why you think the proposed loss function is not simple enough. Nevertheless, we hope we have convinced you that it is in our above argument.\n\n\ncomment: > As raised in Schaeffer et al., 2022, grid cell emergence depends on hyperparameters and implementation choices. Is that also the case here? Eg. What happens when you increase the number of neurons? Or other hyperparameters listed in section B.3? It would be interesting to see whether the claim from Schaeffer et al. still holds when grid cells emerge from not path-integration but mathematical optimization.\n\nThis is a very reasonable concern that we did not address adequately in our previous submission. We have now extensively updated appendices B.3 and B.4 to include a thorough discussion of the parameters, and our solution's robustness.\n\nWe focus on 4 parameters that together define key parts of the problem: the two spatial lengthscales, l and L, the neural lengthscale, sigma, and the number of neurons, N. Since the position units are arbitrary we set L = 1, leaving us with three parameters to explore (The neural space units are not arbitrary due to the firing rate constraint). \n\nOur theory actually makes predictions on how these parameters change the representation. As discussed in appendix E.3, we expect the number of modules to depend on the ratio N:sigma. A small ratio means few modules, and a large ratio means many modules. The role of l is simpler: if it is large we should get hexagons, if it is sufficiently small it will have no effect and we should get arbitrary grids.\nWe have now added simulations that verify these claims:\n1) For fixed N and sigma, for a range of l one module of hexagons is optimal, figure 8. When l is too small, hexagons are no longer optimal.\n2) For fixed N and l, decreasing sigma transition from a single module to multiple modules. The smaller sigma the more modules, figure 9.\n3) You can vary the N, and find qualitatively similar behaviours at all population sizes tested.\n\nOur results do not produce hexagonal grid cells for all possible parameters, but they do for a reasonable range. Thus, we are open to the possibility that there are additional constraints the brain faces that we have not considered here. We have added the following sentence on this:\n\n*Future work could usefully explore whether more constraints are needed to robustly generate many hexagonal modules, or whether more neurons is enough as it was in figure 8.*\n\n> One can also see grid cells in non-spatial settings (Whittington et al., 2020), where a different set of constraints may apply (eg. the resolution of representation does not necessarily follow a Goldilocks of frequency).\n\nMany thanks for the comment. In our work the goldilocks frequency preference came from two lengthscales: the agent explores a limited range so you want frequencies high enough to oscillate in that range, and there is a limited resolution with which you want to encode the world that penalises high frequencies. We think the two of these forces are still at work in TEM. There is a limited map to encode, which enforces a low frequency cutoff, and all the points are separated by at least a lattice lengthscale (i.e. there is no continuity of points) and so there is a high-frequency cutoff. Frequencies that oscillate faster than the lattice lengthscale are not useful. As such, even in TEM, there is a push towards Goldilocks frequencies that may explain the emergence of hexagons. We have added this discussion to Appendix H, where we discuss links to Whittington et al. 2020 and other works.\n\n_In particular, the goldilocks frequency bounds (section 3.2) can be thought of as coming from the finite map the agent explores (high frequency bias) and the discrete step-size in the environment, which creates a low frequency bias_\n\n>  And grid cells have been shown to represent other behaviourally relevant variables besides location, such as time and distance (Kraus et al., 2015) . How can your model account for this? Or is it only applicable to spatial location?\n\nMany thanks for the comment. Our theory, like TEM, applies to situations where there is a common set of actions that act on some space, and so if that variable is structured like physical space then we can build analogous representations for such variables. Unlike TEM which only operates in discrete graph worlds, our theory applies to both discrete and continuous spaces. In Appendix M we applied our framework to 1D, 2D, 3D, and spheres, Appendix M. Both distance and time are 1D variables so a naive prediction would also be modules of grids in 1D, but we are actively working on applying our ideas more broadly in other work.\n\n> Please remake all hand drawn figures, including the ones in appendix, with computer softwares\n\nDone!\n\n> Page 5 first paragraph: “... and θ dependent parts (equation 6; Figure 3A)”: 1) Adding a hyphen, i.e. θ-dependent, might be helpful for readers. 2) Do you mean equation 5?\n\nMany thanks! Good spot. We have changed these accordingly.\n\n\n> Page 5 first paragraph: “...This effect is limited by the firing rate bound: ||a0||2−2L0=N...\" Do you mean 1/2L0?\n\nDone! Many thanks!\n\n\n\n\ncomment: > In this paper, the authors provided a solution of how to parametrically hard code grid cells with learnable parameters in order to be functional, actionable, and biological by minimizing the objective function. Under these three constraints, they showed that hexagonal grid cells are the optimal representation of locations in 1D angular and 2D euclidean space.\n\nWe would like to thank you for carefully reading and commenting on our work. We appreciate your comments, which, as we detail below, we have tried to address, and which have helped us to improve the manuscript. We’d also like to point out that we applied our framework to spherical and 3D spaces too! (deep in Appendix M ;) )We look forward to discussing any remaining concerns further.\n\n> To-date, most computational models on grid cell emergence have focused on training a neural network to solve path-integration tasks and selecting grid cells based on their known neuroscientific properties. This creates a possibility, which also applies to other EC-HPC representations, that the neural representations are simply due to correlation with different behavioral variables (i.e. “you see what you’re looking for”). In this paper, the authors only imposed normative constraints rather than assumptions about known properties of grid cells, and still observed grid cells, which fills the gap in the literature on why EC-HPC representations exist in the first place.\n\nMany thanks!\n\n> Though it provides a good model for grid cell emergence, this framework is somewhat isolated in that it doesn’t consider the interactions between MEC (where grid cells are observed) and other regions in the EC-HPC circuit. For example, under this framework, do grid cells still help explain place cell remapping (Whittington et al., 2020)?\n\nThis is a fair comment: our model is not a model of how the EC and HPC interact. Our work however does extract the EC component from Whittington et al. 2020’s Tolman-Eichenbaum machine (TEM) and explains why it produces grid cells - we understand the constraints of the grid code. However, should we use a conjunctive representation of the hippocampus (as in Whittington et al. 2020) between our grid cells and sensory observations then we would see a similar remapping phenomena to Whittington et al. 2020. In fact, any model with griddy entorhinal representations and a TEM-like EC-HPC conjunctive scheme would lead to the same remapping results.\n\n\n> The metric described in this paper (i.e. functional, biological, actionable) still does not account for whether the grid cells representations are useful for behavior, which in my opinion is what “functional” should mean. I think it would make the paper even stronger if you could show the connection between your model and path integration behavior, or some other behavioral task, eg. train model on path integration task and do lesion study.\n\nWe agree that ‘functional’ and ‘actionable’ should bear relevance for behaviour, and we have designed them exactly for this purpose. Together these constraints mean the representation path-integrates, and path integration is critical for behaviour as without it you cannot plan etc.  In particular, 1) An actionable representation gets updated by an action consistent with how actions work in the underlying space (i.e. the representation obeys the same rules that space does - north+east+south+west takes both the actionable representation, and the real world, back to the same place). 2) Being functional means each location can be uniquely decoded up to some resolution, thus you can understand different parts of the world differently - a requirement for successful behaviour. These two components are equivalent to path-integration since path integration is updating a representation on the basis of an action to track real location. We have added the following paragraph in Appendix H (due to space constraints)and referred to it in the main text:\n\n_Our theory is a theory of path-integrating representations. Actionability means you understand the rules of space (a representation should be unchanged after taking a north then east then south then west), but it does not mean you understand where you are, or how fine-grained this understanding is (a constant representation satisfies the actionable rules for example). Our functional constraint rectifies this, as it requires all relevant locations to be represented differently, up to a certain spatial scale. Understanding rules, and representing locations differently is the same as path-integration - now a representation can be updated according to the rules, and it is different in different locations meaning the underlying spatial variables can be decoded._\n\n\nFurthermore, in Appendix I we show the effect of lesioning various constraints on the representations. With no functional term, the representation still follows rules but it does not successfully map the whole space; while with no actionable term it encodes space well but without rules.\n"
            }
        ]
    },
    {
        "id": "rwetAifrs16",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Incremental Predictive Coding: A Parallel and Fully Automatic Learning Algorithm\nKeywords: Cognitive Science, deep learning, predictive coding\nAbstract: Neuroscience-inspired models, such as predictive coding, have the potential to play an important role in the future of machine intelligence. However, they are not yet used in industrial applications due to some limitations, such as efficiency. In this work, we propose incremental predictive coding (iPC), a variation of the original model derived from the incremental expectation maximization algorithm,  where every operation can be performed in parallel without external control. We show both theoretically and empirically that iPC is more efficient than the original algorithm by Rao and Ballard, with performances comparable to those of backpropagation in image classification tasks. This work impacts several areas, as it has general applications in computational neuroscience and machine learning, and specific applications in scenarios where automatization and parallelization are important, such as distributed computing and implementations of deep learning models on analog and neuromorphic chips. "
            },
            {
                "round2": "Reviewer 1:\nThanks to the authors for their response.  I agree with many of the points in the response, particularly that this approach is a more biologically plausible algorithm for PC as opposed to Rao and Ballard, and that the simplicity of the approach here is a positive feature (potentially enhancing its biological plausibility).  I agree that this observation can be considered novel, particularly from a neuroscience perspective, and I'd be interested to see further work along these lines.\n\nReviewer 2:\nThank you for the response. I think the paper is much stronger with these updates. Maybe the table of runtimes can be turned into a plot instead, but anyway the updates to the text are definitely improvements. \n\nI think the reviews are converging to the interpretation that: detached from the neuroscience influence, this is just incremental EM. Given that the neuroscience part is not really a bonus in the ICLR criteria, maybe the paper would be received better at another venue. I hope the authors are not discouraged by the lukewarm response overall, because (for me at least) the story and the \"mix of two existing techniques\" is still interesting. \n\nReviewer 3:\nAs the authors themselves point out, the difference is between EM and incremental EM. In one case you first let the neural activity settle and then you update the weights, and in the other you do them together. This I find is too small a contribution (noting further that this has been known in the EM/incremental EM liturature).\n\nReviewer 4:\nsummary_of_the_paper: This paper describes a variant of predictive coding, named incremental predictive coding (iPC), based on incremental EM, which it is argued should be considered a biologically plausible approach to learning in the brain.  The complexity of iPC is considered in relation to back-propagation (BP), and a CPU implementation is provided.  Further, the generalization performance is investigated on a number of datasets, and the algorithm is shown to perform well in comparison to BP and PC.\n\nstrength_and_weaknesses: Strengths:\n\n-  The biological plausibility argument is interesting, and in general the argument is convincing that some form of 'localized EM' algorithm is more plausible than BP or PC alternatives, while retaining convergence and generalization properties.\n-  The experimentation convincingly demonstrates that iPC should be considered a viable alternative to BP generally, at least for simple architectures and specialized hardware.\n\nWeaknesses:\n\n- I'm mainly concerned about the paper's novelty - essentially, iPC is equivalent to iEM applied to a hierarchical Gaussian model.  The theoretical properties are described elsewhere (e.g. Karimi 2019) and the biological plausibility argument is hard to evaluate, although likely to be worth pursuing further.\n- There is little theoretical novelty, since the time-complexity analysis (Theorem 1) essentially follows simply by definition.  As discussed by the authors, the comparison of training-loss convergence rates in terms of 'non-parallel matrix multiplications' is an interesting result, but this is investigated solely empirically (Fig. 2 right).\n\nclarity,_quality,_novelty_and_reproducibility: The clarity, quality and reproducibility are mainly good (I spotted a few typos - for instance, in Eq. 4, the conditional in the second expression should read 'p(x^(l-1) | x^(l))', and the Gaussian formulation in the third expression should include the prior and the x's).  As noted above, the novelty is an issue for me.\n\nsummary_of_the_review: An interesting investigation of an algorithm that may have relevance in neuroscience, and deserves further attention.  Potentially, the paper may be of interest to those working in neuroscience and optimization.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 5:\nsummary_of_the_paper: This paper proposes an incremental predictive coding method that performs faster on larger models than BP.\n\nstrength_and_weaknesses: I do like the idea and the demonstrated performance of the proposed method. That being said, there are a few weaknesses I wish that the author could address:\n* 3.2 on CPU implementation, I do not fully understand what is the overhead for iPC. Besides, the time complexity is related to L, then why does the improvement over BP also scales with hidden dimension (is this the amount of neurons in hidden layers)?\n* For the experimental results reported in table 1, is the PC trained with the same amount of epochs as iPC, so that it behaves worse on AlexNet as it is not yet converges? The comparison with PC states that the iPC converges faster, but my understanding is that he iPC is a more efficient way to approximate PC, I do not understand why it failed to scale with model complexity. Besides, is it possible to test on some more complex model or image data set to back the idea that iPC preforms better on complex image related tasks?\n\n\n\nsummary_of_the_review: Overall, this is an interesting work. Some clarification on test results would make it more convincing. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 6:\nsummary_of_the_paper: This paper points out that the predictive coding implementation from Rao and Ballard is essentially expectation maximization, and therefore can be sped up by using the incremental (aka partial) variant of EM instead of the full version. The paper also argues that this is more efficient than backpropagation, at least for large networks and full-batch learning, and has better uncertainty calibration than models trained with backprop. \n\n\nstrength_and_weaknesses: I have some issues with the claims and descriptions made in this paper. I want to acknowledge early that some of my issues may be misunderstandings, because I am still learning about predictive coding here. \n\nThe high-level issue for me is: the background seems to be \"predictive coding vs backpropagation\", where predictive coding is essentially equivalent to expectation-maximization, but this is not the way I think about predictive coding. This sentence matches my understanding better: \"PC is based on the assumption that brains implement an internal generative model of the world, needed to predict incoming stimuli (or data).\" Expectation-maximization is one way to optimize a predictive coding model, and backpropagation is another, and there are more still, like Hebbian learning -- these are all optimization techniques, and choices here are orthogonal to the issue of predictive inference vs non-predictive inference. \n\n\nclarity,_quality,_novelty_and_reproducibility: > [main limitation of predictive coding being] \"lack of efficiency\"\n\nI'm not sure about this. I think it's more like: predictive coding models are harder to train and do not work as well as feedforward alternatives. \n\n> \"can match backpropagation in its most important property: generalization capabilities\"\n\nI'm not sure about this. I think the the common view is that the most important property of backpropagation is that it can manage credit assignment across long chains of modules/neurons, and enable learning of large nonlinear systems.\n\nNotation-wise, it seems like almost every variable in the paper has a bar over it. Maybe all bars can be removed, to make things simpler. \n\nIn Eq. 1, ybar is defined as \"the generated vector\", but also p(ybar, \\thetabar) is described as \"likelihood of the data given the causes\", suggesting ybar is actually the data. Can this be clarified please?\n\nSome citation issues -- parenthetical and non-parenthetical citations are mixed up, making  it hard to read. (e.g., \"intractable Friston (2003)\", and \"Following (Rao ...),\".\n\noptimizingthe -> optimizing the\ngradient descend -> gradient descent\n – the slow inference phase –there  -> ---the slow inference phase---there \n\n> \"Note that this increased speed does not harm the final performance, as the iEM algorithm has been proven to converge to a stationary point of the loss function, the same as EM or backpropagation\" \n\nThis is not very convincing. I think what we really want to know is whether or not this works in practice. I think even incremental EM (without the predictive coding interpretation) is typically less stable than EM. \n\n> \"This algorithm is able to obtain performance competitive to BP on image classification\ntasks.\"\n\nThis is a problematic claim. There are many algorithmic components involved in applying BP on image classification tasks. I don't think such a general claim can be made accurately. \n\n> \"we provide enough evidence that the speed up against BP in full batch training is theoretically possible using iPC\"\n\nI'm not sure -- what makes it \"enough\"? \n\n> \"the concept of inference and iterations are not present in BP\"\n\nI'm not sure what is meant by this. These concepts are certainly present in BP-based models. Why not simply count the number of gradient steps?\n\n> \"we first prove this formally using the number of non-parallel matrix multiplications needed to perform a weight update as a metric\"\n\nI don't know what to do with this \"proof\". Is this useful? What exactly is meant by \"full batch\" anyway? Does this mean putting the whole dataset into memory? To me this seems totally impractical. \n\n> \"This is still below the result of Theorem 1 due to the large overhead introduced in our implementation.\"\n\nOK but is there any feasible way forward on this? To me it seems more like the theorem is not very useful. Also, I would like to know: What are the actual runtimes here? I understand the ratio is somewhat helpful for a comparison, but it obscures the actual values in play here. \n\nThe descriptions in \"setup of experiments\" and \"change of width\" were a bit confusing. Why is everything sequential (\"first we trained this, then we trained that\")? Why does the order matter? Are you training the model, then adding more parameters, then continuing training?  (I expect not...)\n\nInstead of Theorem 1 and the proof and so on, why not simply count the actual number of matrix multiplications? \n\nI am not sure about the comparison between BP and iPC in the experiments. Is the network architecture exactly the same? Normally with BP we have a feedforward architecture, without a generative interpretation. \n\nSometimes the term \"Z-IL\" appears but I think it was never defined. It appears more in the discussion. What is this? \n\nsummary_of_the_review: I enjoyed reading the paper and I think the topic is important, and a great fit for ICLR. I pointed out a variety of claims that appear questionable to me, which maybe stem from thinking about predictive coding as a model formulation (as I do, which I think is standard) vs. thinking about predictive coding as an optimization technique (as the paper does). \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 7:\nsummary_of_the_paper: This paper is basically an implementation of the prospective configuration algoithm (Yuhang Song, et al, Inferring neural activity before plasticity: A foundation for learning beyond backpropagation) where each layer of a heirarchical Gaussian generative model (with the further assumption that the covariance matrix of the gaussians is identity), is updated in parallel. Convergence guarantees to some local optima come from Neal and Hinton's incremental EM paper. \n\nstrength_and_weaknesses: Strength: Empirical results show faster convergence\nWeekness: There is nothing novel in this paper. Neither does this paper claim to introduce the prospective configuration algo (I am guessing it is done in another submission), nor does it need to show convergence (which follows from Neal and Hinton). It is therefore just an implementation paper. Had the experiments been large and comprehensive, there would be reason to consider this paper.\n\nclarity,_quality,_novelty_and_reproducibility: The paper is very clearly written. But it lacks novelty\n\nsummary_of_the_review: The paper has very limited novelty. It is just an implementation of ideas found in other papers.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: We thank the author for the feedback on our work. \n>  This paper is basically an implementation of the prospective configuration algoithm (Yuhang Song, et al, Inferring neural activity before plasticity: A foundation for learning beyond backpropagation) where each layer of a heirarchical Gaussian generative model (with the further assumption that the covariance matrix of the gaussians is identity), is updated in parallel. \n\nThe statement that our work is an implementation of the prospective configuration algorithm is incorrect.  The paper that you cite uses the original implementation of predictive coding, and does NOT update the weights in parallel with the neural activities: it first updates the value nodes until convergence, and THEN performs a weight update. This is shown in Figs.1b and 2c of the paper you cite:\n\n https://www.biorxiv.org/content/10.1101/2022.05.17.492325v2.full.pdf\n\nIn detail, the description of Fig. 1c states the following:\n\n“In prospective configuration, neural activity settles into a new configuration (purple of different intensity) before weight modification (left).”\n\nThis clearly states that FIRST the neural activities converge, and THEN a weight update is performed. Something similar is stated in the description of Fig. 2b:\n\n“Here, it is revealed that the relaxation infers the prospective neural activity, towards which the weights are THEN modified”\n\n\nAgain, this means that FIRST the neural activities converge, and THEN a weight update is performed. The implementation details of this work further confirm that the algorithm of the paper that you cite is NOT equivalent to iPC. In fact, the “Methods” of the paper actually provide the actual equations, and the pseudocode of the algorithm. Particularly, we refer to Eq. (4). Right above Eq. (4), the authors state the following (line 770):\n\n“Then, relaxation is run until convergence, after which the weights are updated using the activity at convergence to further decrease the energy.”\n\nAgain, this is also visible in the pseudocode that the authors provide on page 26. Here, the value nodes are updated until convergence (lines 6-13). The weights are only updated at lines 15-17 when the relaxation process is complete.  Hence, our work is NOT an implementation of the prospective configuration algorithm, as that is a theoretical study of the properties of predictive coding, while our work is the proposal of a faster and better optimization techniques for predictive coding networks. All in all, there is absolutely NO intersection in the contributions of our work and the one you have cited, as each layer of a hierarchical Gaussian generative model  is NOT updated in parallel there.\n\nOn the weaknesses: \n\n> Neither does this paper claim to introduce the prospective configuration algorithm…\n\nAgain, the prospective configuration algorithm is an independent line of work, which we cannot claim, as it is equivalent to the predictive coding algorithm introduced by Rao and Ballard from 1999. In our work, we propose an improvement over that update rule, which may benefit researchers working in this field. There is absolutely no intersection between the contributions of the paper that you have cited and ours.\n\n> nor does it need to show convergence (which follows from Neal and Hinton).\n\nAs already stated in the paragraph about novelty that we have provided, this is actually an advantage of the method that we have proposed, and not a limitation:  When an algorithm is based on a well-studied mathematical field, it is often possible to derive specific results ‘for free’, as they are probably derivations of already well-studied problems. The fact that we can use the theory of statistical inference for free to prove results is a strength of predictive coding (which is a well-worth result on its own), used in this work to provide a faster and better performing algorithm with theoretical guarantees. \n\n\ncomment: \n|   $L$ |   Hidden Dim. |   Seed |   iPC (ms) |      BP (ms) |\n|------:|--------------:|-------:|-----------:|-------------:|\n|    16 |             8 |   5341 |   16.9411  |     1.18208  |\n|    16 |             8 |   7472 |   19.0938  |     1.08862  |\n|    16 |             8 |   9273 |   10.7536  |     1.01233  |\n|    16 |            16 |   5341 |   14.2412  |     0.998735 |\n|    16 |            16 |   7472 |   13.3479  |     1.64127  |\n|    16 |            16 |   9273 |   13.8955  |     1.00088  |\n|    16 |            32 |   5341 |   13.4552  |     1.90115  |\n|    16 |            32 |   7472 |   14.5161  |     1.6222   |\n|    16 |            32 |   9273 |   12.8353  |     1.41335  |\n|    16 |            64 |   5341 |   18.4188  |     3.74508  |\n|    16 |            64 |   7472 |   14.9627  |     2.6381   |\n|    16 |            64 |   9273 |   18.5626  |     3.79133  |\n|    16 |           128 |   5341 |   35.1024  |    30.864    |\n|    16 |           128 |   7472 |   35.4571  |    33.3576   |\n|    16 |           128 |   9273 |   31.0454  |    30.7331   |\n|    16 |           256 |   5341 |   55.4535  |   131.252    |\n|    16 |           256 |   7472 |   46.2825  |   134.253    |\n|    16 |           256 |   9273 |   46.9806  |   155.415    |\n|    16 |           512 |   5341 |  307.729   |  2071.97     |\n|    16 |           512 |   7472 |  355.52    |  2037.23     |\n|    16 |           512 |   9273 |  319.8     |  2073.53     |\n|    16 |          1024 |   5341 | 1327.06    |  9836.65     |\n|    16 |          1024 |   7472 | 1486.89    | 10146.4      |\n|    16 |          1024 |   9273 | 1421.91    | 10326.6      |\n|    32 |             8 |   5341 |   17.4241  |     3.14283  |\n|    32 |             8 |   7472 |   34.1778  |     3.09277  |\n|    32 |             8 |   9273 |   18.239   |     1.89447  |\n|    32 |            16 |   5341 |   22.4276  |     2.50888  |\n|    32 |            16 |   7472 |   18.6222  |     2.55203  |\n|    32 |            16 |   9273 |   25.3308  |     1.94812  |\n|    32 |            32 |   5341 |   24.7233  |     3.09539  |\n|    32 |            32 |   7472 |   20.9057  |     3.2227   |\n|    32 |            32 |   9273 |   21.1201  |     2.91514  |\n|    32 |            64 |   5341 |   20.0055  |     6.16384  |\n|    32 |            64 |   7472 |   25.9078  |     7.68113  |\n|    32 |            64 |   9273 |   21.2889  |     5.53656  |\n|    32 |           128 |   5341 |   60.7922  |    67.327    |\n|    32 |           128 |   7472 |   60.6296  |    66.6113   |\n|    32 |           128 |   9273 |   68.9101  |    91.809    |\n|    32 |           256 |   5341 |   77.8816  |   263.214    |\n|    32 |           256 |   7472 |   74.8813  |   289.155    |\n|    32 |           256 |   9273 |  110.585   |   252.253    |\n|    32 |           512 |   5341 |  558.599   |  4911.16     |\n|    32 |           512 |   7472 |  535.059   |  4991.77     |\n|    32 |           512 |   9273 |  628.008   |  4483.46     |\n|    32 |          1024 |   5341 | 2076.82    | 20333.9      |\n|    32 |          1024 |   7472 | 2112.99    | 20711.6      |\n|    32 |          1024 |   9273 | 2147.04    | 20581        |\n\ncomment: Here is the table of the official running times in milliseconds of all the experiments provided in Figure 3 of the paper:\n\n|   $L$ |   Hidden Dim. |   Seed |   iPC (ms) |      BP (ms) |\n|------:|--------------:|-------:|-----------:|-------------:|\n|     2 |             8 |   5341 |   19.8402  |     0.341654 |\n|     2 |             8 |   7472 |    2.98715 |     0.141382 |\n|     2 |             8 |   9273 |    3.9506  |     0.220537 |\n|     2 |            16 |   5341 |    3.48687 |     0.144958 |\n|     2 |            16 |   7472 |    3.92389 |     0.206947 |\n|     2 |            16 |   9273 |    2.72107 |     0.147343 |\n|     2 |            32 |   5341 |    4.7071  |     0.337601 |\n|     2 |            32 |   7472 |    4.64439 |     0.257015 |\n|     2 |            32 |   9273 |    4.23813 |     0.293732 |\n|     2 |            64 |   5341 |    5.14531 |     0.521898 |\n|     2 |            64 |   7472 |    3.50261 |     0.433445 |\n|     2 |            64 |   9273 |    4.74596 |     0.578642 |\n|     2 |           128 |   5341 |   10.8678  |     4.48537  |\n|     2 |           128 |   7472 |   11.2545  |     4.80199  |\n|     2 |           128 |   9273 |   10.3552  |     3.94964  |\n|     2 |           256 |   5341 |   16.7456  |    16.1965   |\n|     2 |           256 |   7472 |   17.9191  |    16.8622   |\n|     2 |           256 |   9273 |   18.4166  |    16.2866   |\n|     2 |           512 |   5341 |  160.162   |   251.189    |\n|     2 |           512 |   7472 |  150.585   |   262.268    |\n|     2 |           512 |   9273 |  160.136   |   262.373    |\n|     2 |          1024 |   5341 |  595.119   |   997.341    |\n|     2 |          1024 |   7472 |  597.221   |  1021.49     |\n|     2 |          1024 |   9273 |  632.622   |  1147.89     |\n|     4 |             8 |   5341 |    7.59339 |     0.28944  |\n|     4 |             8 |   7472 |    6.31738 |     0.362158 |\n|     4 |             8 |   9273 |    4.81153 |     0.312805 |\n|     4 |            16 |   5341 |    3.72839 |     0.343561 |\n|     4 |            16 |   7472 |    4.91095 |     0.319958 |\n|     4 |            16 |   9273 |    3.79443 |     0.286818 |\n|     4 |            32 |   5341 |    5.84936 |     0.500441 |\n|     4 |            32 |   7472 |    6.40368 |     0.5126   |\n|     4 |            32 |   9273 |    6.74725 |     0.51403  |\n|     4 |            64 |   5341 |    5.96905 |     0.788212 |\n|     4 |            64 |   7472 |    8.39949 |     0.929832 |\n|     4 |            64 |   9273 |    7.04503 |     0.873327 |\n|     4 |           128 |   5341 |   14.2429  |     8.55374  |\n|     4 |           128 |   7472 |   16.2199  |     8.93378  |\n|     4 |           128 |   9273 |   15.1129  |     8.7893   |\n|     4 |           256 |   5341 |   23.9842  |    33.7701   |\n|     4 |           256 |   7472 |   20.5846  |    33.2019   |\n|     4 |           256 |   9273 |   36.7255  |    35.2798   |\n|     4 |           512 |   5341 |  158.126   |   517.919    |\n|     4 |           512 |   7472 |  167.869   |   497        |\n|     4 |           512 |   9273 |  164.187   |   522.093    |\n|     4 |          1024 |   5341 |  678.133   |  2015.67     |\n|     4 |          1024 |   7472 |  664.752   |  2066.1      |\n|     4 |          1024 |   9273 |  622.046   |  1997.78     |\n|     8 |             8 |   5341 |    9.25779 |     0.61059  |\n|     8 |             8 |   7472 |   10.5295  |     1.0767   |\n|     8 |             8 |   9273 |    8.0502  |     0.564337 |\n|     8 |            16 |   5341 |    7.69091 |     0.540972 |\n|     8 |            16 |   7472 |   13.0358  |     0.863791 |\n|     8 |            16 |   9273 |    8.44145 |     0.617743 |\n|     8 |            32 |   5341 |    9.35507 |     1.15371  |\n|     8 |            32 |   7472 |   10.0608  |     0.883102 |\n|     8 |            32 |   9273 |   10.4787  |     1.03331  |\n|     8 |            64 |   5341 |   10.4144  |     1.69182  |\n|     8 |            64 |   7472 |   10.9556  |     1.62578  |\n|     8 |            64 |   9273 |    7.75647 |     1.48463  |\n|     8 |           128 |   5341 |   19.7504  |    15.8749   |\n|     8 |           128 |   7472 |   15.4333  |    17.0178   |\n|     8 |           128 |   9273 |   24.1373  |    19.2568   |\n|     8 |           256 |   5341 |   35.5456  |    63.5219   |\n|     8 |           256 |   7472 |   31.1813  |    63.3771   |\n|     8 |           256 |   9273 |   32.109   |    62.4328   |\n|     8 |           512 |   5341 |  231.559   |  1025.62     |\n|     8 |           512 |   7472 |  266.099   |   982.408    |\n|     8 |           512 |   9273 |  240.943   |  1085.49     |\n|     8 |          1024 |   5341 |  870.819   |  4124.99     |\n|     8 |          1024 |   7472 |  825.8     |  3995.9      |\n|     8 |          1024 |   9273 |  804.621   |  4100.12     |\n\ncomment: We thank the reviewer for the detailed feedback. We have addressed all the sentences pointed out by the reviewer in the updated version of the manuscript. \n\n\n\n> What does it mean that the concepts of steps and iterations are not present in BP? These concepts are certainly present in BP-based models. Why not simply count the number of gradient steps?\n\nThe problem with counting the number of gradient steps is that a single gradient step performed by BP uses different operations than a step of PC, with a complexity that scales linearly with the number of layers. This is not the case with PCs, where the complexity is constant with respect to the number of layers, as every operation can be performed in parallel. Because of this, comparing PC and BP in terms of “steps” would be wrong. Hence, we needed a fair metric to compare the complexity of the two models, which is the number of simultaneous matrix multiplications. We have better specified this in the updated manuscript.\n\n> \"we first prove this formally using the number of non-parallel matrix multiplications needed to perform a weight update as a metric\"I don't know what to do with this \"proof\". Is this useful? What exactly is meant by \"full batch\" anyway? Does this mean putting the whole dataset into memory? \n\nIt is useful, as it shows the much improved efficiency of iPC over the original formulation of PC, and also an advantage over BP in a very restricted case: full batch training. We agree that it is mostly impractical, as most of large-scale applications would not fit a whole dataset into memory. However, we still believe it is an interesting result and hence included it in the paper, as it could be useful in cases where we deal with small datasets of low-dimensional datapoints, such as continual learning, and we need to quickly memorize them before being faced with the next data.  \n\n\n\n> \"This is still below the result of Theorem 1 due to the large overhead introduced in our implementation.\"\nOK but is there any feasible way forward on this? To me it seems more like the theorem is not very useful. Also, I would like to know: What are the actual runtimes here? I understand the ratio is somewhat helpful for a comparison, but it obscures the actual values in play here. \n\n\nThank you for the suggestion, we have provided a table with the actual runtimes of every experiment in the following reply. However, note that the actual runtimes are not interesting per se, as every experiment was run on a CPU.\n\n\n> The descriptions in \"setup of experiments\" and \"change of width\" were a bit confusing. Why is everything sequential (\"first we trained this, then we trained that\")? Why does the order matter? Are you training the model, then adding more parameters, then continuing training? (I expect not...)\n\nThank you for the pointer, we have reworded the paragraph. (The order does not matter, and we do not add more parameters during training!)\n\n\n> Instead of Theorem 1 and the proof and so on, why not simply count the actual number of matrix multiplications? \n\nThe proof is basically obtained by counting the number of non-parallel multiplications. We have decided to summarize the result in a theorem, to make it clearer and more accessible, while leaving the annoying details on counting the single operations by hand to the supplementary material.\n\n> I am not sure about the comparison between BP and iPC in the experiments. Is the network architecture exactly the same? Normally with BP we have a feedforward architecture, without a generative interpretation. \n\nYes, the architecture is always exactly the same in all the tasks that we have compared iPC against BP. When using generative models, we only compare PC against iPC, as shown in Fig. 2 (a,b). \n\n\n> Sometimes the term \"Z-IL\" appears but I think it was never defined. It appears more in the discussion. What is this?\n\nAddressed; thanks. Z-Il is a variation of PC that is able to exactly replicate the weight update of BP. We will clarify this in the revised version.\n\n> thinking about predictive coding as a model formulation (as I do, which I think is standard) vs. thinking about predictive coding as an optimization technique (as the paper does).\n\nThis is a good point. We also think about predictive coding as a model formulation. However, iPC is indeed an optimization technique (i.e., a learning algorithm), specific for predictive coding networks. The efficiency results, for example, are obtained by training predictive coding networks using iPC. To make this concept clearer, we will refer to the models as “Predictive coding networks (PCNs)”, and to the update rule as PC or iPC. We have had a second read of the paper, and tried to make the difference among the two as clear as possible. \n\n\ncomment: We thank the reviewer for his time.\n\n> why does the improvement over BP also scales with hidden dimension (is this the amount of neurons in hidden layers)?\n\n\nThis is because of a computational overhead introduced by our implementation of iPC on CPU, and is hence merely an implementation drawback that should not be present in an ideal implementation of the algorithm. In that case, the improvement should scale only with the number of layers. We will make this more clear in the revised version.\nIn our implementation, the overhead is given by the time needed to create multiple threads, where each thread runs the computation of one layer, and also the time of communication with these threads. The improvement over BP then scales with the hidden dimension (again, only for the considered implementation), as the overhead introduced by iPC is relatively fixed, and hence has a large impact when the hidden dimension is small, and a smaller one when the hidden dimension is large, as most of the time needed to perform the update is taken by the multiplication of two large tensors.\nFor example, let us assume that the time needed to start L threads is 1 second, and the time needed to compute one multiplication in a small network is 0.01 seconds, iPC / BP is then (0.01+1)/(0.01*L)>1, so iPC is worse than BP, because of this fixed starting thread overhead. However, if we consider much larger layers, where the time needed to compute one operation is 1 second, the overhead becomes negligible, and we have that iPC / BP < 1.\n\n\n\n> is the PC trained with the same amount of epochs as iPC, so that it behaves worse on AlexNet as it is not yet converged?\n\nAll the experiments in the table are performed using the same number of epochs. However, we have carefully checked whether the energy/loss of every model had converged, and this was indeed the case. Hence, the worse performance of PC on Alexnet is probably due to scaling properties of PC, rather than a non-converged network. This is a problem that we have not experienced using iPC, able to well scale to larger architectures.\nTo conclude, note that iPC does not approximate PC. The convergence result states that both iPC and PC provably converge to a stationary point of the variational free energy, but it does not provide any approximation result as to the dynamics during training. Our extensive experiments, however, show that iPC tends to converge to better minima than PC. This is a solid result, as it has been tested on both generative and discriminative models on hundreds of different parametrizations (reported in both the supplementary material and main text). This is also reflected in the final test accuracies, that show that iPC tends to perform better than PC. \n\n\ncomment: We thank the author for the feedback on our work. \n\nRegarding the weaknesses of the paper:\n\n> I'm mainly concerned about the paper's novelty - essentially, iPC is equivalent to iEM applied to a hierarchical Gaussian model. \n\nWe agree that iPC is obtained by applying incremental EM to the original formulation of PC, and it is hence a mix of two existing techniques. This, however, does not affect the novelty of the proposed algorithm, as it addresses a fundamental problem in the field. The first work introducing the weight update of PC is Rao and Ballard’s, in 1999. The incremental EM is from Neil & Hinton, in 1996. From Rao and Ballard’s paper to today, countless works use the original rule to update the weights, that is much slower than ours, and less performing in the tasks we have proposed. The authors performing the experiments have certainly experienced the bad efficiency of PC models. Hence, as obvious as the proposed algorithm can be, it certainly addresses an existing, practical, problem that a large community has had for years. The fact that it empirically converges to a better minimum (shown in a large number of experiments in this paper) is also a big plus. \n\nAs your main concern is the novelty of this work, we point to the ‘On Novelty” paragraph that we have added as an answer to all the reviewers. \n\n\n> The comparison of training-loss convergence rates in terms of 'non-parallel matrix multiplications' is an interesting result, but this is investigated solely empirically\n\nNote that in a classification task, it is possible to initialize all the internal variables of the model to have zero error. In this case, at t=0, the total energy of the model would be concentrated in the output layer, and hence be equivalent to the train loss. This has been shown to lead to a good performance empirically. Starting from this assumption, we can use the iEM algorithm to minimize the total energy of the network, equivalent in this case to the  the train loss.  \n\n> Typos\nThank you for the pointers, we have addressed them.\n"
            }
        ]
    },
    {
        "id": "GX0uI5T8kd",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Self-Supervised Off-Policy Ranking via Crowd Layer\nKeywords: off-policy ranking, policy representation learning, reinforcement learning\nAbstract: Off-policy evaluation (OPE) aims to estimate the online performance of target policies given dataset collected by some behavioral policies. OPE is crucial in many applications where online policy evaluation is expensive. However, existing OPE methods are far from reliable. Fortunately, in many real-world scenarios, we care only about the ranking of the evaluating policies, rather than their exact online performance. Existing works on off-policy ranking (OPR) adopt a supervised training paradigm, which assumes that there are plenty of deployed policies and the labels of their performance are available. However, this assumption does not apply to most OPE scenarios because collecting such training data might be highly expensive. In this paper, we propose a novel OPR framework called SOCCER, where the existing OPE methods are modeled as workers in a crowdsourcing system. SOCCER can be trained in a self-supervised way as it does not require any ground-truth labels of policies. Moreover, in order to capture the relative discrepancies between policies, we propose a novel transformer-based architecture to learn effective pairwise policy representations. Experimental results show that SOCCER achieves significantly high accuracy in a variety of OPR tasks. Surprisingly, SOCCER even performs better than baselines trained in a supervised way using additional labeled data, which further demonstrates the superiority of SOCCER in OPR tasks."
            },
            {
                "round2": "Reviewer 1:\nLet me first share some references that should help the authors understand what we care about in OPE's theoretical analysis:\n\n1. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning: http://proceedings.mlr.press/v48/jiang16.pdf\n2. Doubly Robust Policy Evaluation and Learning: https://arxiv.org/pdf/1103.4601.pdf\n3. DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections: https://proceedings.neurips.cc/paper/2019/file/cf9a242b70f45317ffd281241fa66502-Paper.pdf\n\nBasically, OPE aims to evaluate the return/value of a policy, and thus we should analyze the bias and variance/MSE of the evaluation of an OPE estimator. Then, in this work, since a new principle named OPR is proposed, what should be analyzed is the quality of policy ranking. Using your own experiments as example/evidence, rank correlation and regret were treated as the main metric, because the proposed method is designed to rank/select the best policy.  \n\nAgain, directly borrowing proof from learning from crowdsourced labels does not work, as these two problems care distinct criteria.\n\nRegarding point 2, I encourage the authors to consider the setting in my earlier post: hint - consider whether the two classifiers can still make independent mistakes.\n\nRegarding point 3, I am afraid we just cannot assume how an OPE estimator's bias distributes, otherwise all the references I provided would be meaningless. \n\n\n\nReviewer 2:\nI am afraid this time I cannot agree with most of the follow-up explanations and believe they are incorrect.\n\n1. The intuition is correct, i.e., the lower the empirical risk of learning from crowdsourced labels is, the more accurate OPR is. But there is no direct translation from empirical risk to OPR in Theorem 2. For example, how do we relate the regret of OPR selection, i.e., whether the best policy is identified, with the empirical risk or Theorem 2? I believe in both OPR and OPE, we care whether we are choosing the best policy.\n2. Independent OPE training does not lead to independent OPE results across different estimators, which is the key in Theorem 2. Let me be more specific. First, to give you a naive example: for a given training set for a classification problem, we can independently train a SVM classifier and a logistic regression classifier; but their classification results are not necessarily independent from each other, as they are trained on the same set of instances. It is the same issue here for a set of OPEs. Second, in Theorem 2, the summation over $M$ OPEs already assumes they independent. This is the common assumption in crowdsourcing, on which your proof is based. \n3. The distribution of biases of an OPE clearly depends on specific OPE methods. For example, IPS is known to be unbiased if ground-truth propensity score is known; otherwise, its bias totally depends on the estimated propensity. I cannot understand why/how we can assume how the bias is distributed.   \n\nReviewer 3:\nFinally I got some time to digest the newly added proofs and here are my understandings. \n\nThe proofs are basically applying what's known for analyzing crowdsourcing onto the proposed crowdsourcing-based off-policy ranking problem. As a result, the Theorem 2 shows we can optimize the upper bound of the empirical risk of a learner over a set of OPEs. Such a result is fine for learning from crowdsourced labels, where we care about the quality of the learnt classifier. But in OPE, we care about policy evaluation, or more precisely policy ranking in this paper. I do not think Theorem 2 answers this question. For example, how does Theorem 2 suggest the probability that the proposed method finds the truly best policy?  \n\nTheorem 1 is also problematic: it is also borrowed from crowdsourcing field, where people often assume independent annotators and each annotator makes independent mistakes. But as annotators become specific OPE methods in this work, they are not necessarily independent from each other, e.g., they might be different types of model-based or IPS estimators. And the assumption that within a particular OPE method biases towards different policies are uniformly random is also very strong. \n\nAs a result, I do not think the provided proofs address the real need of theoretical analysis in this paper. \n\nReviewer 4:\nI really appreciate the authors' effort in providing the new theoretical analysis about the proposed algorithm. \n\nAs I was out of town during the weekend, I need sometime to carefully digest the new results. In the meanwhile, I understand the discussion phase is coming to an end today, and thus I will try my best to follow up as soon as possible (within 24 hours). \n\nJust a few quick clarification questions that help me better understand the results: \n1. what is a leaner $g\\in\\mathcal{G}$ in Theorem 2? $g$ in the paper is defined as a mapping from logged trajectory to $n$-dimensional representation. \n2. how do we know the bias of each OPE method? \n3. Theorem 1 is about each OPE's bias and Theorem 2 is about the quality of $g$ learning. How are these two related? \n\nThanks!\n\nReviewer 5:\nI would like to thank the authors for providing the additional results supporting the paper's claims. \n\n- I find it informative that average score and majority voting schemes do not work well in this setting, and that using architecture without attention mechanism results in the large drop in performance. \n- The generalisation results are quite interesting and might require further discussion. Why in some cases it is better to train on a different set of policies? In which cases training on a different set of policies does not work very well? It might be a useful finding for practical considerations. \n- Regarding the computational costs, I still think that the authors should still discuss this in the paper. Even through the method relies on existing OPE techniques, it still means that for any particular problem, the practitioner needs to train *all* the OPE methods instead of just one of them before following the proposed methodology. \n- Related to this, other reviewers raised concerns about the hyperparameters of the method and I agree that this should be clearly explained.\n\nIn the light of new empirical results, I update my score as my biggest concerns were addressed. \n\nReviewer 6:\nI am afraid I cannot be convinced that because both OPE and crowd sourcing have theoretical results, the proposed solution will have theoretical results. For example, the references on OPE mentioned in the paper and rebuttals are all about off-policy evaluation problem, i.e., evaluating the value of a target policy. Since this paper is about the rank of policies, I do not see an easy connection between those existing solutions to the proposed one in this paper. It would be helpful if the authors could shed light on it.\n\n\n\nReviewer 7:\nThank you authors for the clarifications. \n\nA1: Thanks for the additional experimental result with an MLP. There seems to be a significant drop in performance. This does show the significance of using a transformer, although it would be more interesting to compare with a simpler sequence model like CNN/RNN, and compare the performance/run-time trade-off. Transforms can be costly in terms of inference time, a simple CNN/RNN might be faster. A suggestion for a future paper maybe :) \n\nA2: Thanks for your comments. I agree that the baseline SOPR-T can be considered an LTR method. \n\nA4: Thanks for the clarification. \n\nReviewer 8:\nsummary_of_the_paper: This paper proposes a new method for ranking of offline RL policies with off-policy evaluation (OPE). The ranking is produced with a model that 1) learns a pairwise policy representation with a transformer architecture, 2) uses a crowd layer to aggregate OPE scores of other methods. In the experimental results the authors show that their method is able to outperform the other baselines. The ablation studies show the importance of various components of the proposed method.\n\nstrength_and_weaknesses: Strengths:\n\n- The paper is well written and easy to follow. The details of the method and the experiments are clearly explained. The figures are informative and well explained.\n- The idea of using pairwise policy representation sounds interesting and novel.\n- The experimental results where the proposed method outperformed the other baseline is very encouraging.\n- The experimental results studies the problem with different settings.\n\nWeaknesses:\n\n- To my mind, the experimental results are lacking an adequate baseline that is comparable with the proposed method. A baseline would be comparable if it also uses other existing OPE methods to aggregate the results. For example, I can imagine several easy baselines in this case: 1) take the average OPE scores of all methods and produce a ranking out of them, 2) use a majority voting scheme to aggregate the rankings, 3) there are many rank aggregation methods that could be considered, for example, [1]. \n- As the proposed method aggregates the ranks from the existing policies, the computational cost for it is much higher than for any other method and it includes the costs of all other methods. This should also be discussed. \n- Another limitation in the current experiments is that as far as I understand the method needs to be trained for every new set of policies and the environment from scratch. Then, it is tested on its own training set (no validation or test set). Do I understand the setting correctly? Would the policy representations generalize across different sets of policies? Suppose a new policy is added to the set of policies, can the previous results be re-used? To me, the method would be useful in practice if it can show signs of such generalization.\n- I do not understand why transformers are the best architecture in the given policy representation design. As the states (equation 1) are chosen as just a set (not ordered), what is the advantage of using a transformer which is known to be the best suited for sequential data? Did the authors consider other architectures (possibly simpler, e.g., MLP) here?\n\nOther comments:\n- Several times the authors mention that in practice finding the best policy is the main objective. In that case, it would be more logical to consider off-policy policy selection (OPS) problem formulation and as the quality metric measure the regret @1. How would the method perform in that case? \n- I still do not understand the role and training of the \"aggregation token\" very well, maybe this could be explained further.\n- In the first part of section 4.2 the authors say that they \"show how the policy ranking can be reduced to binary classification\". I think this is a common way to approach the ranking problem (but the text sounds now like this is one of the contributions). Some work could be references here, for example, [2].\n\n[1] Fast and Accurate Inference of Plackett–Luce Models. Lucas Maystre, Matthias Grossglauser. NIPS 2015.\n\n[2] Preference Learning with Gaussian Processes. Wei Chu, Zoubin Ghahramani. ICML 2005.\n\n\nclarity,_quality,_novelty_and_reproducibility: Clarity: good.\n\nQuality: good.\n\nNovelty: the paper combines several existing components and aggregates the results of the existing methods, but the idea of the aggregation is reasonably novel. Also, using pairwise instead of direct policy representation sounds novel to me.\n\nReproducibility: the paper provides sufficient details on the methodology as the space permits. Are the authors planning to open source the code?\n\nsummary_of_the_review: I am leaning toward rejecting this paper mainly because I find the experiments lacking comparable baselines that would benefit from aggregating the results of the existing methods in the same way as the proposed method. Also, I would also like to see some generalization of the method or policy representation to the unseen policies that would make the method scalable to real world problems.\n\n---\nUpdated my score after rebuttal in the light of new empirical results.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 9:\nsummary_of_the_paper: To fulfill OPE task, this paper simplifies OPE into OPE and proposes an end-to-end method, namely SOCCER. SOCCER is compared with several baselines in two environments. Experimental results show that the proposed method achieves high accuracy with certain hyper-parameters. An ablation experiment is also performed.\n\nstrength_and_weaknesses: Strength:\n1. This paper simplifies the OPE into OPR tasks.\n2. This paper proposes an end-to-end method to solve the OPR.\n3. The proposed method is relatively technical sound.\n\nWeaknesses:\n1. The experiment settings lack some explanations, e.g. the reason for the specific value of hyper-parameters.\n2. The figures need to be improved, e.g. sub-figures in Fig.4 are not aligned.\n3. Ablation experiments as well as baselines need to be considered more carefully. From my point of view, the essence of the proposed method is similar to a label aggregation method with deep learning. Thus, some other structure-like method should also be added, e.g. SpeeLFC. The ablation experiment introduces an extra strategy to obtain ‘truth’ to train the method. This setting lowers the convincing performance of the proposed method.\n\n\nclarity,_quality,_novelty_and_reproducibility: The overall quality of this paper as well as the reproducibility of the proposed method is fair. The proposed method is technically sound. However, the novelty is relatively limited. From my point of view, the proposed method is like a combination of several existing methods in a simplified manner.\n\nsummary_of_the_review: This paper simplifies the OPE task into an OPR task and proposes an end-to-end method, namely SOCCER, to tackle the problem. SOCCER contains policy representation (pairwise), a feature extractor (MLP and MSA), and a training strategy (Crowd Layer). SOCCER is compared with several baselines in two environments with three modes. The process of the experiment is relatively complete. An ablation experiment is also performed, while some settings make the results not very convincing. Experimental results show that the proposed method can achieve good performance in some conditions. Although this work lacks theoretical innovation and the representation of this paper has some flaws, the entire work is relatively complete and the OPE problem is relatively solved.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nReviewer 10:\nsummary_of_the_paper: This paper studies the problem of offline policy evaluation. The key idea is that instead of predicting the value of the target policy, the authors proposed to evaluate the relative rank of the policy, among a set of candidate policies. When there is no sufficient historical observations of different behavior policies, the authors proposed to leverage the idea of crowd-sourcing to integrate prediction from multiple offline policy evaluators. The final ranking/scoring of each target policy is obtained by aggregating the pairwise comparisons among all candidate policies. Experiments on a set of simulated offline trajectory data were performed against a rich collection of offline policy evaluation methods. \n\nstrength_and_weaknesses: Strength:\n+ The idea of off-policy rank evaluation is reasonable and valid, which to my best knowledge is first time discussed in literature. Although only high-level intuitive arguments are provided regarding the advantage of this new objective, it is still nice to observe its effectiveness in the reported empirical studies. \n+ The reported empirical studies provide a comprehensive picture about the comparison between the proposed solution and a rich set of baseline methods.  \n\nWeakness:\n- The proposed method lacks necessary theoretical justification, with unknown properties about the provided estimations, e.g., there is no way to quantify its bias and variance. \n- The solution boils down to a binary classification problem, under which the crowd-sourcing idea is a natural extension when only noisy labels exist. Following this logic, methods for learning from noisy labels can also be leveraged to address the problem. \n- The empirical nature of the proposed solution creates a large set of hyper-parameters, which make the tuning and comparison hard to exhaust. For example, presumably the number of states where we execute the policies to obtain their actions, the number of pairwise policy comparisons, and the number of OPE workers are important for the performance of the proposed solution. But there is no experiment evaluating the impact from such hyper-parameters.   \n\nQuestions:\n- In Section 5.2, it is mentioned that in each epoch, 5 OPE workers are randomly selected from the baseline models. I am not sure why we should sample different OPE workers every epoch. Shouldn’t they be the same throughout the training and testing stages for the crowd layer to learn the (equivalent) confusion matrix?  \n- What’s the principle to determine the number of states where we execute the policies to obtain their actions, the number of pairwise policy comparisons, and the number of OPE workers in practice? Are they the more the better?\n- Why not have a dedicate policy encoder to represent each policy and then compare their embeddings for binary classification? Or we can simply follow the way we use transformer to encode two sentences into one embedding to embed the two policies. The conventional position embedding in transformer can help us realize the corresponding positions are actions from two policies but under the same states.\n- My understanding about $e_\\alpha$ and $e_\\beta$ is that they are fixed one-hot vectors. But Section 5.2 described them as random vectors at initialization. Clarification is necessary here.  \n\n\nclarity,_quality,_novelty_and_reproducibility: The paper is generally well written and easy to follow, especially its core idea of converting off-policy evaluation into a binary classification problem. Every step in the proposed solution is standard, e.g., the crowd-layer for integrating different off-policy evaluators, and transformer to encode a sequence of actions. Hence, it should not be difficult to reproduce the algorithm pipeline. The authors also provided details about most of the hyper-parameter settings in the paper, which should help ensure the reproducibility of the reported results.  \n\nsummary_of_the_review: The idea of studying offline policy ranking, instead of evaluation, is an interesting and practical idea. But the proposed solution is overly simplified and lacks necessary theoretical justification or analysis. It is hard to know when the algorithm would work better than standard off-policy evaluation methods.  \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 11:\nsummary_of_the_paper: The authors present an Off-Policy Evaluation (OPE) method geared toward ranking policies w.r.t their estimated performance, as compared to the standard task of predicting the estimated performance absolutely. It is more aligned with the OPE end goal, i.e. ranking policies among a set of candidates. \n\nPrevious work on off-policy ranking SOPR-T (Jin et al., 2022) used supervised learning for ranking policies, but it assumes access to multiple deployed policies with reward signals available. It is argued by the authors that this assumption is impractical, as it assumes multiple deployed policies along with their rewards in the log data. \n\nAuthors instead propose a self-supervised learning method, based on the 'learning from crowd' paradigm, where they assume different OPE methods as workers in a crowd-sourcing setting, with each worker generating noisy labels. A previous work, 'Crowd Layer (CL)' from learning via crowdsourcing literature is used to learn with the noisy label setting. \n\nstrength_and_weaknesses: Strengths of the paper:\n\n- The problem of OPE via policy ranking is very practical and bypasses the requirement for estimating the absolute policy value (expected reward), which is known to be a difficult problem. \n- In practical settings, like recommender systems, ultimately we need a ranked list of policies, according to their estimated 'online' performance. \n- The method can be used in a setting where access to multiple deployed policies with corresponding rewards is not feasible, for ex: in a clinical trial. \n- Existing work from crowd-sourcing is used to learn from existing OPE methods while assuming they generate noisy labels, which is a fair assumption. \n\nSome questions to the authors: \n- How is a transformer-part important to the off-policy ranking pipeline? For ranking policies, have you compared with existing Learning-to-rank methods like LambdaMART on top of features extracted from trajectories of the policies, two-tower network (like Neural Matrix Factorization) with say RNN based feature extraction for both heads? For a given trajectory, a listwise method like lamdaMART can rank all the policies at a time, instead of a pairwise comparison. \n- A clarification question (sorry if I missed something obvious): What is the dimensionality of $x_k$s (Eq. 2)? Is it a scaler, or a vector? Since you are adding a 2-d $e_\\alpha$ vector to it, I am assuming it's a 2-D vector?  \n- In a setting like a recommender system/search, where you have access to a large log data with multiple policies and their rewards, how does this method compare with the baseline \"SOPR-T\"?\n\nclarity,_quality,_novelty_and_reproducibility: - The paper is very well-written, and it's very easy to understand. \n- The method proposed is novel and very practical, especially in a setting where access to deployed logging policies is limited, like clinical trials. \n\n\nsummary_of_the_review: - Authors propose an off-policy ranking method, which directly predicts the ranking between two policies, using a ranking method. \n- Assuming a setting where access to multiple logging policies is restricted, the proposed method can leverage existing OPE methods in a pseudo-crowd-sourcing setup, with each OPE method as a worker in the crowd, and learning from the workers in a noisy label setting. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: 1. Theorem 2 only gives an formal upper bound on the empirical risk. Given the intuition that the lower the empirical risk, the more accurate the classifier, we are expected to find the best policy with a high probability.  The reviewer seems to ask us to reveal the connection between the risk and the accuracy, which we think is unnecessary. Taking the simple SVM as example, we do not care the exact relations between risk the accuracy, instead, we just minimize the risk to improve the accuracy.\n\n2. We define the independence of two workers as the independence of their prediction results (as well as the incured loss). In fact, one worker's predictions depend only on the inputs and its own loss function, which are irrelevant of other workers' prediction results. In other words, changing other workers' predictions (or hyper-parameters, loss functions) does not influence the target worker's predictions and losses, no matter whether they are using the same set of training data.\n\n3. We agree that different OPE methods have different biases. Since there might be many different OPE methods, we cannot use one theorem to cover them all. As we have explaned, we only provided a special case where the biases are uniformly distributed. Theorem 1 might be in different forms under different assumptions on the distribution of the biases.\n\nWe do appreciate the reviewer's efforts in checking our results. We sincerely hope that our responses help to solve the issues.\n\ncomment: We thank the reviewer for your time. However, we feel that you might have some misunderstandings on our theorems.\n\n1. **Relations between emperical risk and policy ranking.** The logic is: the lower the emperical risk, the more accurate the classifier (which predicts whether a policy is better than the other). If the classifier is accurate, we can easily select the best policy out of a set of policies. In other words, the OPR task is reduced to learning an accurate classifier. \n\n2. **The workers are independent from each other.** When we judge whether two OPE methods are independent from each other, we only need to judge whether the learning process of one worker depends on the other. Actually, although they share the same set of training data, the learning process of the workders are indeed independent, no matther whether the basic learning methods are similar.\n\n3. **The distribution of biases.** There are many ways to describe the distribution of biases, among which the uniform distribution is the simplest version. If we assume other kinds of distributions of biases, we can still get close-form representations or upper bounds on the biases. We will leave it for future work. \n\nWe hope that the above replies address the reviewer's concerns. Thank you!\n\ncomment: We really appreciate that you will spend your precious time in checking our results. Please find the clarifications bellow.\n\n1. The learner $g$ in the proof actually represents the model (including the Crowd Layer) that maps trajectories $\\tau_{i,j}$ to labels $y_{i,j}$. The notation is slightly abused. We will explain more clear in the final version of the paper.\n\n2. It is hard to accurately quantify the biases of the OPE methods. However, existing works have provided upper bounds on the biases [1,2], which we denote as $B_r$ in Theorem 1.\n\n3. Theorem 2 provides an upper bound on the emperical risk of learner $g$. We can see from the results that this bound depends on the averaged worker quality $\\widetilde \\eta$. Recall that Theorem 1 suggests that we can select workers with small biases to improve the worker qualities. Combine them together we can conclude that OPE workers with small biases lead to a tighter upper bound on the emperical risk, thus a better learning performance in OPR task. \n\nWe will be ready to address your further concerns. Please feel free to ask us if clarifications are needed. Thanks again!\n\n[1] Batch policy learning under constraints. In ICML, 2019.\n\n[2] DualDICE: behavior-agnostic estimation of discounted stationary distribution corrections. In NeurIPS, 2019.\n\ncomment: 1. We agree that there would be more interesting findings from more extensive experiments regarding to the generalisation ability.\n\n2. Yes we will discuss about the computational cost and the choice of hyperparameters in the next version of our paper. Thank you for your advise!\n\ncomment: **Theorem 1** suggests that we can select high-quality OPE workers who have small biases. Now we show how the worker qualities influence the final performance of SOCCER.\n\nRecall that our input data can be represented by $\\tau_{i,j}=<(s_1,a_1^i),(s_1,a_1^j),...,(s_K,a_K^i),(s_K,a_K^j)>$, which contains the different actions generate by $\\pi_i$ and $\\pi_j$ at the same set of states. We denote by $y_{i,j}$ the ground-truth label indicating whether $\\pi_i$ performs better than $\\pi_j$. We also denote by $y^1_{i,j},\\cdots,y^M_{i,j}$ the labels provided by $M$ OPE workers. Then the ground-truth dataset (which we cannot access in practice) and the noised dataset can be represented as $\\mathcal{D} =$ {$(\\tau_{i,j}, y_{i,j})$} and $\\mathcal{\\widetilde D} =$ {$(\\tau_{i,j}, y^1_{i,j},\\cdots,y^M_{i,j})$}, respectively.\n\nGiven an loss function $\\ell$, we define the empirical risk for the given learner $\\hat g \\in \\mathcal{G}$ learning with ground truth labels as $R_{\\mathcal{D}}(\\hat g) = \\mathbb{E_{\\mathcal{D}}} \\Big[\\ell(\\hat g(\\tau_{i,j}),y_{i,j})\\Big]$, and the empirical risk learning with crowd layer as: $R_{\\mathcal{\\widetilde D}}(\\hat g) = \\frac{1}{M}  \\sum_{r=1}^M \\mathbb{E_{\\mathcal{ D}^r}}\\Big[\\ell(\\hat g(\\tau_{i,j}), y^r_{i,j})\\Big] $. The following theorem gives an upper bound on the difference between $R_{\\mathcal{D}}(\\hat g)$ and $R_{\\mathcal{\\widetilde D}}(\\hat g)$.\n\n**Theorem 2** (Upper bound on emperical risk.)  Assume that for a given dataset $D$ and any $g \\in \\mathcal{G}$, $R_{\\mathcal{D}}(g)$ is upper bounded by $\\overline \\ell$ and lower bounded by $\\underline \\ell$. Denote by $\\widetilde \\eta =\\frac{1}{M}  \\sum_{r=1}^M \\eta^r$. Then for any given learner $\\hat g$, we have\n\n$$\\qquad \\qquad \\qquad R_{\\mathcal{D}}(\\hat g) - R_{\\mathcal{\\widetilde D}}(\\hat g) \\le (1-\\widetilde \\eta)\\cdot (\\overline \\ell - \\underline \\ell).$$\n\nMoving $R_{\\mathcal{\\widetilde D}}(\\hat g)$ to the right side we would have an upper bound on $R_{\\mathcal{D}}(\\hat g)$. **Although we cannot directly minimize  $R_{\\mathcal{D}}(\\hat g)$ due to the lack of ground-truth labels, this theorem suggests that we can minimize its upper bound instead. Moreover, choosing high-quality workers leads to a higher $\\widetilde \\eta$ and a tighter upper bound of $R_{\\mathcal{D}}(\\hat g)$.**\n\n**Proof of Theorem 2.** We start by expanding $R_{\\mathcal{D}}(\\hat g) - R_{\\mathcal{\\widetilde D}}(\\hat g)$:\n\n\\begin{equation}\n  \\qquad \\quad \\quad R_{\\mathcal{D}}(\\hat g) - R_{\\mathcal{\\widetilde D}}( \\hat g) = \\mathbb{E_{\\mathcal{D}} }\\Big[\\ell(\\hat g(\\tau_{i,j}),y_{i,j})  \\Big] - \\frac{1}{M}  \\sum_{r=1}^M\\mathbb{E_{\\mathcal{D^r}}} \\Big[\\ell (\\hat g(\\tau_{i,j}),y^r_{i,j})  \\Big]\n\\end{equation}\n$$\\qquad \\qquad \\qquad \\quad = \\mathbb{E_{\\mathcal{D}}} \\Big[\\ell(\\hat g(\\tau_{i,j}),y_{i,j})  \\Big]- \\frac{1}{M}  \\sum_{r=1}^M \\mathbb{E_{\\mathcal{D^r}}} \\Big[\\ell(\\hat g(\\tau_{i,j}), y^r_{i,j})\\Big]$$\n$$\\qquad \\qquad \\qquad \\quad = \\frac{1}{M} \\sum_{r=1}^M \\Bigg[\\mathbb{E_{\\mathcal{D}}}\\Big[ \\ell(\\hat g(\\tau_{i,j}),y_{i,j}) \\Big] -  \\mathbb{E_{\\mathcal{D^r}}}\\Big[\\ell(\\hat g(\\tau_{i,j}), y^r_{i,j})\\Big]\\Bigg]$$\n$$\\qquad \\qquad \\qquad \\quad \\le \\max_{g \\in \\mathcal{G}}\\frac{1}{M} \\sum_{r=1}^M \\Bigg|\\mathbb{E_{\\mathcal{D}}}\\Big[ \\ell( g(\\tau_{i,j}),y_{i,j}) \\Big] -  \\mathbb{E_{\\mathcal{D^r}}} \\Big[\\ell( g(\\tau_{i,j}), y^r_{i,j})\\Big]\\Bigg|$$\n$$\\qquad \\qquad \\qquad \\quad =  \\max_{g \\in \\mathcal{G}}\\frac{1}{M} \\sum_{r=1}^M \\Bigg|\\mathbb{E_{\\mathcal{D}}}\\Big[ \\ell( g(\\tau_{i,j}),y_{i,j}) \\Big] -  \\mathbb{E_{(\\pi_i,\\pi_j)\\sim \\mathcal{ D^r},y^r_{i,j}=y_{i,j}}} \\Big[\\ell( g(\\tau_{i,j}), y_{i,j})\\Big]$$ \n$$\\quad \\qquad \\qquad \\qquad \\quad-\\mathbb{E_{(\\pi_i,\\pi_j)\\sim \\mathcal{ D^r},y^r_{i,j}=1-y_{i,j}}} \\Big[\\ell( g(\\tau_{i,j}),1-y_{i,j})\\Big] \\Bigg|$$\n$$\\qquad \\qquad \\qquad \\quad =  \\max_{g \\in \\mathcal{G}}\\frac{1}{M} \\sum_{r=1}^M \\Bigg|\\mathbb{E_{\\mathcal{D}}}\\Big[ \\ell( g(\\tau_{i,j}),y_{i,j}) \\Big] -  \\mathbb{E_{\\mathcal{D}}} \\Big[\\eta^r\\cdot\\ell( g(\\tau_{i,j}), y_{i,j})\\Big]$$\n$$\\qquad \\qquad \\qquad \\quad \\quad-  \\mathbb{E_{\\mathcal{D}}} \\Big[(1-\\eta^r)\\cdot\\ell( g(\\tau_{i,j}),1-y_{i,j})\\Big] \\Bigg|$$\n$$\\qquad \\qquad \\qquad \\quad =  \\max_{g \\in \\mathcal{G}}\\frac{1}{M} \\sum_{r=1}^M \\Bigg|\\mathbb{E_{\\mathcal{D}}}\\Big[(1-\\eta^r) (\\ell( g(\\tau_{i,j}),y_{i,j}) - \\ell( g(\\tau_{i,j}),1-y_{i,j}))\\Big]  \\Bigg|$$\n$$\\qquad \\qquad \\qquad \\quad \\le (1-\\widetilde \\eta)\\cdot( \\mathcal{\\overline \\ell} -  \\underline \\ell)$$\n\ncomment: We are sorry for the late reply. We thank the reviewer for your useful suggestion. Following the reviewer's suggestion, we provide the following theoretical analysis to futher demonstrate the superiority of our method. \n\nIntuitively, if an OPE method could accurately predict two policies' online performances, it would be regarded as a high-quality worker in the crowdsourced OPR task. We formally define the worker quality as follows.\n\n**Definition 1** (OPE worker quality)\nGiven two policies $\\pi_i$ and $\\pi_j$, we denote by $\\delta_i$ and $\\delta_j$ their real expected returns. For an OPE worker $r$, we denote by  $b^r_i$ and $b^r_j$ the prediction biases of $r$. The quality $\\eta^r$ of the OPE worker $r$ is defined as the probability that $ y^r_{i,j}$ equals to $ y_{i,j}$:\n\n$$\\qquad \\qquad \\qquad \\eta^r =  P( y_{i,j} = y^r_{i,j})$$\n\nwhere $y_{i,j} = \\mathbb{1_{\\delta_i > \\delta_j}}$ denotes the ground-truth label of whether $\\pi_i$ performs better than $\\pi_j$ and $ y^r_{i,j} = \\mathbb{1_{\\delta_i +b^r_i > \\delta_j +b^r_j}}$ denotes the label predicted by worker $r$.\n\nFrom this definition we can see that the worker quality is actually determined by the biases. We derive the following theorem to reveal the connection between biases and worker quality.\n\n**Theorem 1** (Close-form representation of worker quality.) Assume that the bias of worker $r$ is bounded: $|b^r| \\le B_r$, and $b^r$ subjects to a uniform distribution $U(-B_r, B_r)$. We denote by $\\Delta = |\\delta_i - \\delta_j|$ the distance between the real expected returns of two policies. Then we have \n\\begin{equation}\n   \\qquad \\qquad \\qquad \\eta^r = \\frac{1}{2} +\\frac{\\Delta}{4B_r} \\quad if \\quad \\Delta <2B_r \\quad and \\quad \\eta^r =1 \\quad otherwise.\n\\end{equation}\n\nIn fact, $\\Delta$ is a fixed number if two policies are given. **Therefore, this theorem indicates that the smaller the bias bound $B_r$, the higher the worker quality $\\eta^r$.**\n\n\n**Proof of Theorem 1.** Given $\\pi_i$ and $\\pi_j$, recall that $\\eta^r = P( y_{i,j} = y^r_{i,j})$, we have:\n\n$$\\qquad \\qquad \\qquad \\eta^r =  P( y_{i,j} = y^r_{i,j}) = P(\\mathbb{1_{\\delta_i > \\delta_j}} = \\mathbb{1_{\\delta_i +b^r_i > \\delta_j +b^r_j}})$$\n$$\\qquad \\qquad \\qquad \\quad = P[(\\delta_i - \\delta_j)(\\delta_i +b^r_i - \\delta_j -b^r_j)>0]$$\n$$\\qquad \\qquad \\qquad \\quad= P[(\\delta_i - \\delta_j)>0] \\cdot P[(b^r_i-b^r_j)>-(\\delta_i-\\delta_j)]$$\n$$\\qquad \\qquad \\qquad \\quad\\quad + P[(\\delta_i - \\delta_j)<0] \\cdot P[(b^r_i-b^r_j)<-(\\delta_i-\\delta_j)]$$\n\n\nLet $\\zeta = b^r_i-b^r_j$, we have $\\eta^r = P(\\zeta>-\\Delta)$ if $\\delta_i - \\delta_j >0$, and $\\eta^r = P(\\zeta<\\Delta)$ if $\\delta_i - \\delta_j < 0$. \nRecall that $b^r \\sim U(-B_r,B_r)$ and $b_i^r$, $b_j^r$ are independent from each other, then random variable $\\zeta$ follows a triangular distribution whose probability density function can be written as:\n\n$$\\qquad \\qquad \\qquad \\quad f(\\zeta) = \\frac{1}{2B_r} + \\frac{1}{4B^{2}_r}\\cdot \\zeta \\qquad if  \\quad -2B_r \\le \\zeta < 0$$\n$$\\qquad \\qquad \\qquad \\quad f(\\zeta) = \\frac{1}{2B_r} - \\frac{1}{4B^{2}_r}\\cdot \\zeta \\qquad if \\quad 0 \\le \\zeta \\le 2B_r$$\n$$\\qquad \\qquad \\qquad \\quad f(\\zeta) = 0 \\qquad otherwise $$\n\nWith this close-formed density function,  the worker quality can be calculated as $\\eta^r = \\int_{-\\Delta}^{+\\infty}f(\\zeta)d\\zeta$ if $\\delta_i - \\delta_j >0$, and $\\eta^r = \\int_{-\\infty}^{\\Delta}f(\\zeta)d\\zeta$ if $\\delta_i - \\delta_j <0$. As it is easy to see that $\\int_{-\\Delta}^{+\\infty}f(\\zeta)d\\zeta = \\int_{-\\infty}^{\\Delta}f(\\zeta)d\\zeta$, we finally have\n\n   $$\\qquad \\qquad \\qquad \\quad  \\eta^r = \\frac{1}{2} +\\frac{\\Delta}{4B_r}  \\qquad if \\quad \\Delta <2B_r$$\n   $$\\qquad \\qquad \\qquad \\quad  \\eta^r = 1 \\qquad \\qquad \\qquad if \\quad \\Delta \\ge 2B_r$$\n\n\n\ncomment: Thank you for your kind suggestion! \n\nWe will definitely consider to develop more efficient architectures for learning policy representations. \n\ncomment: We thank the reviewer for your constructive comments. \n\n$\\\\textbf{Q1}$: The proposed method lacks necessary theoretical justification, with unknown properties about the provided estimations, e.g., there is no way to quantify its bias and variance. \n\n$\\\\textbf{A1}$: Since our framework combines the existing OPE methods and crowdsourcing techniques, we also enjoy the theoretical gurantees of these two fields of research. Intuitively, we can choose high-quality OPE workers whose performance have been demonstrated emperically or theoretically.\n\nFor example, DualDICE [1] and FQE [2] present that the error associated with their estimated policy performance can be theoretically bounded under some mild assumptions. And doubly-robust method [3] proves that its estimated results enjoy reduced variance when the value estimator is accurate. These theoretical analysis provide guidlines on selecting high-quality OPE workers. On the other hand, some works from the crowdsourcing field provide theoretical guarantees on the learning performance. For example, [4] proves that as long as the worker qualities exceed a threshold, the generalization error of the trained model is bounded. This result builds the connection between worker quality and the final learning performance. We agree with the reviewer that a unified theoretical quarantee would further improve the reliability of our framework. However, we want to emphasize that our work provides the first practical crowdsourcing based framework to address the off-policy ranking problem. \n\n$\\\\textbf{Q2}$: The solution boils down to a binary classification problem, under which the crowd-sourcing idea is a natural extension when only noisy labels exist. Following this logic, methods for learning from noisy labels can also be leveraged to address the problem. \n\n$\\\\textbf{A2}$: We agree with the reviewer that other learning from crowd (noisy labels) methods can also be leveraged to address the problem. We regard it as a merit because any future advances in the field of crowdsourcing could benefit the problem of off-policy ranking. In this work, we choose to use the Crowd Layer because it is naturally compatible with deep learning approaches and can be trained in an end-to-end way. We also add some experiments to compare CL with another two label aggregation methods, namely \"Average Score\" and \"Majority Voting\". We report their performance on rank correlations in the following table. We can see from the following table that our method domindates these two baselines in all of the six environements.\n\n$$\\\\begin{array}{|c|c|c|c|}\n\\\\hline\\text{Environment} &\\text{Avg. Score}&\\text{Major Voting}&\\text{Ours}\\\\\\\\\n\\\\hline HalfCheetah-expert &-0.34\t&-0.27\t&\\textbf{0.71}\\\\\\\\\n\\\\hline HalfCheetah-full-replay &0.24&\t0.31&\t\\textbf{0.74}\\\\\\\\\n\\\\hline HalfCheetah-medium &0.32&\t0.57&\t\\textbf{0.81}\\\\\\\\\n\\\\hline Walker2d-expert &0.53\t&0.23&\t\\textbf{0.85}\\\\\\\\\n\\\\hline Walker2d-full-replay &0.41&\t0.31&\t\\textbf{0.82}\\\\\\\\\n\\\\hline Walker2d-medium &0.21&\t0.29&\t\\textbf{0.80}\t\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q3}$: Why not have a dedicate policy encoder to represent each policy and then compare their embeddings for binary classification? Or we can simply follow the way we use transformer to encode two sentences into one embedding to embed the two policies. The conventional position embedding in transformer can help us realize the corresponding positions are actions from two policies but under the same states.\n\n$\\\\textbf{A3}$: The idea of using a dedicate policy encoder is used in SOPR-T [5], which serves as the main baseline to our method. Moreover, if we understand correctly, the method that the reviewer describes exactly matches our design of the Policy Comparison Transformer. In PCT, we utilize the self-attention mechanism of transformer to directly learn pairwise policy representations, which is proved to be more effective than SOPR-T. Detailed experimental results can be found in Figure 4 of our paper. \n\n$\\\\textbf{Q4}$: My understanding about $e_{\\\\alpha}$ and $e_{\\\\beta}$ is that they are fixed one-hot vectors. But Section 5.2 described them as random vectors at initialization. Clarification is necessary here.\n\n$\\\\textbf{A4}$: $e_{\\\\alpha}$ and $e_{\\\\beta}$ represent one-hot vectors that indicate the orders of two policies. In practice, $e_{\\\\alpha}$ and $e_{\\\\beta}$ will be further mapped to learnable embeddings whose dimensions match the input tokens (recall that positioned encodings are added to input tokens). We will state more clearly in the next version of our paper.\n\n[1] DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections. In NeurIPS, 2019.\n\n[2] Batch policy learning under constraints. In ICML, 2019.\n\n[3] Doubly robust off-policy value evaluation for reinforcement learning. In ICML, 2016.\n\n[4] Learning from noisy singly-labeled data. In ICLR, 2018.\n\n[5] Supervised off-policy ranking. In ICML, 2022.\n\ncomment: $\\\\textbf{Q5}$: Why we should sample different OPE workers every epoch. Shouldn’t they be the same throughout the training and testing stages for the crowd layer to learn the (equivalent) confusion matrix? \n\n$\\\\textbf{A5}$: In implementation, we would equip each worker with an independent crowd layer (2 parameters, the scale factor and the bias). In each epoch, only the crowd layers corresponding to the sampled workers would be trained. In the testing stage, all crowd layers would be removed, and the policy comparison can be directly predicted by the rest architecture of our model. \n\n$\\\\textbf{Q6}$: The empirical nature of the proposed solution creates a large set of hyper-parameters, which make the tuning and comparison hard to exhaust.\n\n$\\\\textbf{A6}$: In our framework, most hyper-parameters are brought by the existing OPE methods. However, many existing OPE methods are open-sourced and well-tuned. Therefore, it is convenient to reproduce them in practice. \n\nIn addition, our framework includes two types of hyper-parameters, we will show how to select these hyper-parameters as follows.\n\n(1) Hyper-parameters of the model architecture (such as the depth of transformer and the learning rate). For these hyper-parameters, we actually use grid-search to determine the best choices of them in our experiments. We will state more clearly in the paper.\n\n(2) The batch size of state-action pairs. During training, we feed a batch of state-action pairs to the Transformer in order to compute the approximated policy representations. To balance the computational cost and the performance, we chose the number 256 as the batch size. This choice is supported by the experimental results bellow, which show the averaged rank correlations of our model with the batch size growing. We can find that when the batch size is larger than 256, the performance of our model tends to be stable.\n\n$$\\\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\\\hline\\text{Batch Size} &8\t&16\t&32\t&64&\t128\t&256&\t512&\t1024&\t2048\\\\\\\\\n\\\\hline \\text{Avg. Rank Correlation} &0.21&\t0.27&\t0.37&\t0.39&\t0.56&\t0.65\t&0.64&\t0.66\t&0.65\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n\ncomment: We thank reviewer for your helpful comments and kind suggestions.\n\n$\\\\textbf{Q1}$: The experimental results are lacking an adequate baseline that is comparable with the proposed method.\n\n$\\\\textbf{A1}$: The reviewer suggests to compare the Crowd Layer (CL) with other label aggregation methods such as \"Average Score\" and \"Majority Voting\". Actually, the superiority of CL over these baslines has been demonstrated in [1]. However, we agree with the reviewer that it is still valuable to reproduce this superiority in the context of off-policy ranking. Specifically, we use the methods of \"Average\" and \"Majority Voting\"  to replace the CL in SOCCER and report the rank correlations in the following table. We can see from the first four columns that our method domindates these two baselines in all of the six environements. Note that the framework of SOCCER can also combine with more advanced crowdsourcing methods other than CL.\n\nWe note that the reviewer mentioned Rank Aggregation (RA), which is a line of works that aggregate a set of pairwise comparisons into a ranking list. Since RA aggregates pairwise comparisons instead of labels,  we cannot directly compare it with CL. In fact, SOCCER also incorperates a simple averaged ranking aggregation procedure, as described in Equation (6) in the paper. Following the reviewer's suggestion, we use a more recent and simple RA method [2] to replace Equation (6). From the last two columns we can see that the method in [2] indeed further improves SOCCER in some of the test evironments. However, this does not contradicts to our main contribution: modeling the off-policy ranking problem from the perspective of crowdsourcing.\n$$\nscore_i = \\frac{1}{N}\\sum_{j\\\\neq i}\\hat{y}_{i,j}, \\\\,\\\\, i=1,...,N \\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,(6)\n$$  \n\n$$\\\\begin{array}{|c|c|c|c|}\n\\\\hline\\text{Environment} &\\text{Avg. Score}&\\text{Major Voting}&\\text{Ours}&\\text{Ours with RA}\\\\\\\\\n\\\\hline HalfCheetah-expert &-0.34\t&-0.27\t&0.71&\t\\textbf{0.72}\\\\\\\\\n\\\\hline HalfCheetah-full-replay &0.24&\t0.31&\t\\textbf{0.74}&\t0.73\\\\\\\\\n\\\\hline HalfCheetah-medium &0.32&\t0.57&\t\\textbf{0.81}&\t0.80\\\\\\\\\n\\\\hline Walker2d-expert &0.53\t&0.23&\t\\textbf{0.85}&\t0.83\\\\\\\\\n\\\\hline Walker2d-full-replay &0.41&\t0.31&\t\\textbf{0.82}&\t0.75\\\\\\\\\n\\\\hline Walker2d-medium &0.21&\t0.29&\t0.80\t&\\textbf{0.87}\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q2}$: As the proposed method aggregates the ranks from the existing policies, the computational cost for it is much higher than for any other method and it includes the costs of all other methods. It should be discussed.\n\n$\\\\textbf{A2}$: Since our method is built upon existing OPE methods, we do need to implement these methods. However, we don't think it is a big problem due to three facts. (1) Existing OPE methods can be implemented in parallel. (2) The computational cost of our proposed Policy Comparison Transformer and Crowd Layer depends on the amount of training data, and it is usually worth to incorporate more data if they are available. (3) Our model, once trained, can be used to inference the rankings of arbitray number of policies. By contrast, existing OPE methods such as FQE and model-based approaches, need to train their models from scratch for every new policy. Therefore, from this perspective, the averaged computational cost of our method is much lower than existing methods.\n\n$\\\\textbf{Q3}$: The generalization ability of our model.\n\n$\\\\textbf{A3}$: We thank the reviewer for pointing out this issue. The generalization ability is actually an advantage of our method, due to the strong representational ability of our proposed Policy Comparison Transformer. To test the generalization ability, we added a new set of experiments where our model is trained on a given policy set and tested on other sets. We report the rank correlations in the following table. Set 1-5 represent Halfcheetah-expert set I, Halfcheetah-expert set II, Halfcheetah-full-replay set I, Halfcheetah-full-replay set II, Halfcheetah-medium set I, respectively. Specifically, the rows indiate the policy sets we used for training and the columns indicate the policy sets we used for test. We can see that our method generalizes well in most cases. For example, the model trained using Set 1 achieves 0.52 correlation when tested on Set 2 (see row 1, colunm 2), which even outperforms the model trained using Set 2 itself. \n\n$$\\\\begin{array}{|c|c|c|c|c|}\n\\\\hline\\text{ } &\\text{Set 1}&\\text{Set 2}&\\text{Set 3}&\\text{Set 4}&\\text{Set 5}\\\\\\\\\n\\\\hline \\text{Set 1} &\\textbf{0.71}\t&\\textbf{0.52}&\t0.65&\t0.32&\t0.65\\\\\\\\\n\\\\hline \\text{Set 2} &0.51&\t0.42&\t0.52\t&0.53&\t0.76\\\\\\\\\n\\\\hline \\text{Set 3} &0.66\t&0.43&\t\\textbf{0.74}&\t\\textbf{0.65}&\t0.77\\\\\\\\\n\\\\hline \\text{Set 4} &0.43\t&0.42\t&0.54&\t0.41\t&0.50\\\\\\\\\n\\\\hline \\text{Set 5} &0.65\t&0.32&\t0.67\t&0.40&\t\\textbf{0.81}\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n[1] Deep learning from crowds. In AAAI. 2018.\n\n[2] Just sort it! A simple and effective approach to active preference learning. In ICML. 2017.\n\ncomment: $\\\\textbf{Q4}$: Why transformers are the best architectures in the given policy representation design.\n\n$\\\\textbf{A4}$: Transformers are powerful due to its self-attention mechanism which can capture the correlations between input tokens. Moreover, the design of positional encoding makes Transformers work well on sequential data. In our Policy Comparision Transformer (PCT), the self-attention machanism successfully captures the correlation between state-action pairs, which is key to learn effective and generalizable policy representations. Since we want to learn pairwise policy representations, we design a novel positional encoding which reflects the $\\\\textbf{order of two policies}$, so that the learned pairwise policy representations can be directly used to predict the order of them. To show the advantage of Transformers, we did additional ablation experiments by replacing the PCT with an MLP which has the same number of parameters. We present the average rank correlation of PCT and MLP of the 6-policy set in HalfCheetah-v2 environment as follows. We can see that PCT indeed performs much better MLP.\n$$\\\\begin{array}{|c|c|}\n\\\\hline\\text{ } &\\text{Rank Correlation}\\\\\\\\\n\\\\hline \\text{PCT} &0.65\\\\\\\\\n\\\\hline \\text{MLP} &0.32\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q5}$: The role and training of the \"aggregation token”.\n\n$\\\\textbf{A5}$: The aggregation token is similar to the \"cls\" token which is widely used in other transformer architectures, e.g. ViT and BERT. In our case, this token aggregates information from state-action pairs and will be used in predicting the order of two policies. \n\n\ncomment: We thank the reviewer for the helpful and constructive comments.\n\n$\\\\textbf{Q1}$: The experiment settings lack some explanations, e.g. the reason for the specific value of hyper-parameters.\n\n$\\\\textbf{A1}$: There are mainly two types of hyper-parameters in our work. \n\n(1) Hyper-parameters of the model architecture (such as the depth of transformer and the learning rate). For these hyper-parameters, we actually use grid-search to determine the best choices of them in our experiments. We will state more clearly in the paper.\n\n(2) The batch size of state-action pairs. During training, we feed a batch of state-action pairs to the Transformer in order to compute the approximated policy representations. To balance the computational cost and the performance, we chose the number 256 as the batch size. This choice is supported by the experimental results bellow, which show the averaged rank correlations of our model with the batch size growing. We can find that when the batch size is larger than 256, the performance of our model tends to be stable.\n\n$$\\\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\\\hline\\text{Batch Size} &8\t&16\t&32\t&64&\t128\t&256&\t512&\t1024&\t2048\\\\\\\\\n\\\\hline \\text{Avg. Rank Correlation} &0.21&\t0.27&\t0.37&\t0.39&\t0.56&\t0.65\t&0.64&\t0.66\t&0.65\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q2}$: Some other label aggregation methods should also be added.\n\n$\\\\textbf{A2}$: We agree with the reviewer that other label aggregation methods should be compared. We add two simple label aggregation methods, namely \"Average Score\" and \"Majority Voting\", and compare their rank correlations with the Crowd Layer (CL). We can see from the following table that our method domindates these two baselines in all of the six environements. However, we agree that CL may not be the best method for learning from crowd, since this line of research is evolving itself. In fact, our contribution is approaching the off-policy ranking problem from a crowdsourcing perspective, therefore any advances in crowdsourcing or label aggregation could be combined with our proposed SOCCER framework and benefit the OPR task. \n\n$$\\\\begin{array}{|c|c|c|c|}\n\\\\hline\\text{Environment} &\\text{Avg. Score}&\\text{Major Voting}&\\text{Ours}\\\\\\\\\n\\\\hline HalfCheetah-expert &-0.34\t&-0.27\t&\\textbf{0.71}\\\\\\\\\n\\\\hline HalfCheetah-full-replay &0.24&\t0.31&\t\\textbf{0.74}\\\\\\\\\n\\\\hline HalfCheetah-medium &0.32&\t0.57&\t\\textbf{0.81}\\\\\\\\\n\\\\hline Walker2d-expert &0.53\t&0.23&\t\\textbf{0.85}\\\\\\\\\n\\\\hline Walker2d-full-replay &0.41&\t0.31&\t\\textbf{0.82}\\\\\\\\\n\\\\hline Walker2d-medium &0.21&\t0.29&\t\\textbf{0.80}\t\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q3}$: The ablation experiment introduces an extra strategy to obtain ‘truth’ to train the method. This setting lowers the convincing performance of the proposed method.\n\n$\\\\textbf{A3}$: It seems that the reviewer has some misunderstandings to our design of the ablation study. We will state more clearly in the next version of our paper. In fact, by comparing SOCCER with SOCCER-without-CL (trained using ground-truth labels), we demonstrate the superiority of SOCCER since it can achieve comparable performance with SOCCER-without-CL, but does not require the ground-truth labels. Moreover, to demonstrate the effectiveness of our proposed Policy Comparison Transformer, we train SOCCER-without-CL and SOPR-T using the same set of ground-truth labels for a fair comparison. Therefore, introducing ground-truth labels are actually necessary in our ablation studies.\n\n\ncomment: We thank the reivewer for your valuable comments and questions. \n\n$\\\\textbf{Q1}$: How is a transformer-part important to the off-policy ranking pipeline? \n\n$\\\\textbf{A1}$: Transformers are powerful due to its self-attention mechanism which can capture the correlations between input tokens. Moreover, the design of positional encoding makes Transformers work well on sequential data. In our Policy Comparision Transformer (PCT), the self-attention machanism successfully captures the correlation between state-action pairs, which is key to learn effective and generalizable policy representations. Since we want to learn pairwise policy representations, we design a novel positional encoding which reflects the order of two policies, so that the learned pairwise policy representations can be directly used to predict the order of them. To show the advantage of Transformers, we did additional ablation experiments by replacing the PCT with an MLP which has the same number of parameters. We present the average rank correlation of PCT and MLP of the 6-policy set in HalfCheetah-v2 environment as follows. We can see that PCT indeed performs much better MLP.\n$$\\\\begin{array}{|c|c|}\n\\\\hline\\text{ } &\\text{Rank Correlation}\\\\\\\\\n\\\\hline \\text{PCT} &0.65\\\\\\\\\n\\\\hline \\text{MLP} &0.32\\\\\\\\\n\\\\hline\n\\\\end{array}$$\n\n$\\\\textbf{Q2}$: For ranking policies, have you compared with existing Learning-to-rank methods?\n\n$\\\\textbf{A2}$: Actually, SOPR-T [1] can be regarded as a learning-to-rank method, which extracts features of two policies seperately and uses a loss function similar to RankNet to train the model.  SOPR-T is also the main baseline to our work (see Figure 4 in the paper for detailed comparisons). \n\nWe agree with the reviewer that learning-to-rank is another way to approach the problem of off-policy ranking. To our knowledge, most learning-to-rank methods focus on designing appropriate loss function to learn good ranking results. However, in the problem of off-policy ranking, the policy representation plays an important role. Learning policy representations separately (as is done in SOPR-T and other learning-to-rank methods) does not capture the core discrepancy between two policies. Therefore, in this work we focus on learning pairwise policy representations to improve the policy ranking. There might be some oportunities for future work to combine pairwise representations with learning-to-rank methods.\n\n \n$\\\\textbf{Q3}$: A clarification question (sorry if I missed something obvious): What is the dimensionality of $x_k$ (Eq. 2)?\n\n$\\\\textbf{A3}$: Thanks for your question. Input token $x_k$ is a d-dimensional vector, where d is a hyper-parameter. One-hot vectors $e_{\\\\alpha}$ and $e_{\\\\beta}$ would also be mapped to d-dimensional vectors (positional encoding), so that they can be added to $x_k$. We will state more clearly in the paper.\n\n$\\\\textbf{Q4}$: Do you have access to a large log data with multiple policies and their rewards?\n\n$\\\\textbf{A4}$: Thanks for your advice. We actually have considered to test our method in recommender systems. Unfortunately, there are no public datasets that contains enough recommendation policies and their online performance. We note that SOPR-T does not include experiments on recommender systems either. We think that the access to this kind of data is the main obstacle. This is also the main motivation of our work: to develop solutions to off-policy ranking problems without ground-truth labels.\n\n\n[1] Supervised off-policy ranking. In ICML, 2022.\n"
            }
        ]
    },
    {
        "id": "kL67fyKb6A",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Online black-box adaptation to label-shift in the presence of conditional-shift\nKeywords: label-shift, online, black-box, adaptation, Bayesian\nAbstract: We consider an out-of-distribution setting where trained predictive models are deployed online in new locations (inducing conditional-shift), such that these locations are also associated with differently skewed target distributions (label-shift). While approaches for online adaptation to label-shift have recently been discussed by Wu et al. (2021), the potential presence of concurrent conditional-shift has not been considered in the literature, although one might anticipate such distributional shifts in realistic deployments. In this paper, we empirically explore the effectiveness of online adaptation methods in such situations on three synthetic and two realistic datasets, comprising both classification and regression problems. We show that it is possible to improve performance in these settings by learning additional hyper-parameters to account for the presence of conditional-shift by using appropriate validation sets. "
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: It is well known that the performance of machine learning models is highly dependent on the distribution of the data on which it is evaluated: model performance deteriorates when tested on data generated from a distribution shifted with respect to the training data generating process. Identifying and mitigating the effects of distribution shifts is a major open challenge for machine learning practitioners, as distribution shifts are ubiquitous in an ever-changing world. In the supervised learning context, evaluating test performance and mitigating it usually require labelled testing data, which is often difficult or impossible to obtain. \n\nLittle can be done about arbitrary distribution shifts – generalization from training to test data is only possible if the shift leaves some structure in the data unchanged. Label shift is a basic example of such a distribution shift, where the conditional probability P(X|Y) remains fixed, and only P(Y) changes. Here X are the covariates, Y the label, and P(X,Y) = P(X|Y)P(Y) is their joint distribution. Recent years saw much progress with the analysis of label shift, and methods have been developed to mitigate its impact on black box models – with deep learning a primary application -- in both offline and online settings. Essentially, these methods rely on re-weighting model predictions using the distribution of predicted (pseudo-)labels, and thus do not require true labels for the test data. \n\nThe current paper follows three goals related to label shift adaptation:\n\n1. The paper’s main effort focuses on examining how previously proposed label shift mitigation methods perform on shifted distributions do not satisfy the label shift condition – a scenario highly relevant to real-world applications, where often as pure label shifts are rare. In an online learning setting, albeit one in which the distribution does not shift continuously, the paper examines empirically how recently proposed algorithms for online adaptation to label shift perform on a few synthetic and realistic datasets that exemplify different kinds of “non-label” distribution shift. The empirical investigation also considers a couple of heuristically-motivated extensions to these algorithms, most notably performing model selection on an OOD validation set which is shifter with respect to both the training and test sets. The findings of these investigations are not clear cut, but suggest that in some cases, the proposed algorithms provide an improved adaptation to the distribution shift. The takeaway is that label shift adaptation methods (or some heuristic generalization thereof) might sometimes be useful to mitigate general distribution shifts, even if this practice has no known theoretical justification.\n\nThe paper considers two further issues related to label shift adaptation:\n\n2. Past work on label shift adaptation has mostly focused on classification problems. The paper proposes an algorithm for label shift adaptation in regression settings, and studies it empirically.\n3. Past algorithms for online label shift adaptation require the inversion of an empirically measured confusion matrix. The paper suggests a heuristic fix for the case when this matrix is non-invertible and studies it empirically. \n\nThe latter two issues are discussed briefly (compared to the main topic of the paper), and here too the investigations do not provide clear cut conclusions on the efficacy of the proposed methods, but in some cases these methods perform better than the baseline.\n\n\nstrength_and_weaknesses: Major Strengths: \n\n1. The problem investigated is well motivated. Distribution shifts are indeed a big and relevant problem when machine learning models are deployed in the real world. Much of the work to date has focused on idealized types of shifts, like label or covariate shift. It is natural to wonder how much methods developed for idealized shifts might be useful in more realistic settings. Furthermore, if label shift adaptation methods generalize to realistic shift scenarios, they are attractive from a practical standpoint, as they do not require labelled test data. \n2. Empirical results are, for the most part (except for some comments below), clearly presented: I could understand what was done and believe I have enough information to attempt to reproduce the results. \n3. The paper is quite honest about the inconclusive nature of much of the results, and does not try to oversell the proposed methods. \n\nMajor Weaknesses: \n\n1. A systematic or principled approach to the types of distribution shifts considered is missing. Distributions can shift in many ways and for many reasons. Adding conditional shift to label shift is tantamount to considering general distribution shifts. Indeed, the paper considers two examples with no label shift (P(Y) is not changed in the synthetic MNIST and COCO-on-Places datasets), an example with covariate shift (Mixture of Gaussians), and two general distribution shifts (from the WILDS dataset). Framing the issue as “label shift in the presence of conditional shift” might give a wrong impression that the conditional shift is a perturbation of the label shift condition. I find it clearer to state that general distribution shifts are considered. \nLittle can be said about distribution shifts in general, without focusing on particular types or characteristics of the shifts, such as label/covariate shift, subpopulation shift [6], or shifts where the data generating process has a fixed known causal structure [3]-[5]. Since experiments in the paper do not belong to a particular type of shift, it is hard to compare results or to generalize from them to general shifts. \nThe lack of a systematic approach to general label shifts is reflected also in the absence of discussion of relevant work on this issue, including refs [1]—[6].\n\n2. Given the vast scope of possible distribution shifts, with no systemic understanding of how they relate to or differ from label shift, and with heuristic methods lacking a theoretical foundation – given these, a major and comprehensive empirical study is necessary in order to ascertain the usefulness of the proposed methods. The paper offers modest experiments, in terms of types and strengths of shift, types of data, and alternative baselines/methods. This severely limits the usefulness of the results, as it is unclear when the suggested methods can be expected to improve upon baselines, and how good such improvement are compared to alternative methods. As it stands, few generalizable insights can be drawn from the empirical scope of the paper. The paper itself is honest about the modest and tentative nature of the findings, when it concludes that the experiments are “suggestive” that the proposed methods show “promising trends for the most part” in the limited scope in which they were tested. \nConcretely, for the experiments performed, here are some suggestions of baselines/methods that might provide a wider context for obtained results: \n  a. An optimal fixed classifier, as considered by Wu et al. (2021).\n  b. Results obtained from offline domain adaptation methods (Garg et al., 2020).\n  c. Results obtained from known domain generalization methods such as those mentioned in the related works section of the paper, or the ones surveyed by Gulrajani & Lopez-Paz (2020). In particular, if I understand correctly, CORAL was used for the two WILDS datasets considered in the paper, but not the others. It might be more informative to test all datasets with and without CORALS (and/or other domain adaptation methods). \n  d. The paper emphasizes the importance of the use of an OOD validation set. It would thus be useful to test the effect of this OOD validation set on test performance by considering the effect of different validation sets, preferably with different characteristics. For example, for the synthetic colored MNIST dataset, one could use validation sets that are more or less correlated with the test sets. \n\n3. Goals 2+3 above are not explored in detail in the paper. No references are given to prior work on regression label shift / domain adaptation (e.g., [7]-[8] below), nor to the discussion in Lipton et al. (2018, section 7) about remedies to non-invertible empirical confusion matrices. The corresponding experiments provide only an initial investigation into them. The paper provides some interesting but embryonic discussion/exploration of both. Their inclusion in the current form of the paper \n\n4. Some key definitions and explanations are lacking in the paper, making it difficult to understand some sections of it.\n  a. “Conditional shift” is not defined. While it is a term used in the literature and whose meaning might be intuitive, many other terms are used in the literature as well. To guarantee that there are no misunderstandings regarding this central concept, its definition should be provided. \n  b. Method FTH-H-B and FTH-H-B (R) are never clearly defined (what is the “pseudo-count hyper-parameter” mentioned? I did not understand).\n  c. In equation (3), the definition of the expected error rate \\ell^{\\test{new}} is only given in words, not in a formula. \n  d. In section 4.1, what are a, b, kappa, and mu?   \n  e. In appendix A, none of the notation is defined, and in fact no information is given about the context and goal of the derivation there. \n\nFurther comments\n\n1. Online vs offline methods. The scope of label shifts considered in this paper is more limited than those considered by Wu et al.: here only constant shifts are considered (test data is drawn from a fixed shifted distribution), whereas Wu et al considered distributions that keep changing throughout training. An important strength of online methods are their ability to deal with continual changes. Considering only constant changes reduces (but does not invalidate) the usefulness of online methods compared to offline ones. The decision to focus on online methods should be motivated in the paper. \n\n2. OOD validation: the concept of OOD validation is introduced in Heuristic 1 without being properly defined/explained. As this is a central tenet in the proposed methods, the idea and procedure should have a clear and detailed explanation. Furthermore, in Heuristic 1 it is written that OOD validation is a standard practice of model selection, with a reference to Gularjani & Lopez-Paz (2020). As far as I can tell, this reference (which emphasizes the importance of validation set details in the context of domain generalization) does not advocate the use of validation on a separate OOD set. Rather, it attributes this method to Krueger et al. (2020), who indeed mention it in an appendix. \nRegarding the method itself OOD validation itself: why should it work? I can understand that it might be useful when the shifts in the validation and test sets are somehow related (like the Skewed-MNIST example where test is a more severe shift of the same type as validation), but why would it help in examples like the mixture of Gaussians, or the WILDS datasets? Looking at the experiment results, it indeed seems to me that OOD validation is helpful only for the skewed-MNIST example. If my reading is correct, this should be stated clearly, and the appropriate qualifications should be made in the conclusions about the merits of OOD validation. Currently, section 5.3 states that “Using OOD validation sets … improves results on the whole” – but for S-COCO-on-Places and iWildCam (Avg) I do not see any improvement more significant than the noise level, and for iWildCam (F1) there is a small deterioration (which is also consistent with noise). \nFrom a practical perspective, performing OOD validation is not always possible as it requires more labelled data – it would be useful to emphasize this fact. Technically – what are all the optimization steps performed on this validation set? I.e., which hyper-parameters are calculated on this validation set, other than the confusion matrix? \n\n3. Non-invertible confusion matrices. The methods proposed in Heuristic 3 surely generate invertible matrices, but why would they be expected to work for label shift and general distribution shift adaptation? They seem to me ad-hoc and unmotivated. What would be their merit compared to using a pseudo inverse, or the soft probability matrix suggested by Lipton et al. (2018)? \n\n4. Section 4: The role of this Bayesian discussion is not clear to me. What insights are gained from this Bayesian perspective? Are these insights relevant also to cases of pure-label shift, or only general distribution shifts? I found the discussion around equations (11)-(14) confusing on first reading. The notation in equations (11)-(12) is confusing, perhaps Y|\\phi ~ Cat(\\alpha) and \\phi ~ Dir(\\alpha). The notation in equation (13)-(14) – P_t(\\phi), P^{new}_{t+1} is not defined anywhere. \nI found the whole of section 4.1 confusing. How is the discussion related to label shifts in regression problems? What are the takeaways or results of this section? Are the results valid only for the Gaussian example with a conjugate prior, or more generally applicable? What kind of calibration is performed in this section, and why is it useful?\n\n5. Experiment details. Right before section 5.1: \n  a. It would be worthwhile to provide the details of “the surrogate loss implementation of Wu et al.” \n  b. What are the details of the grid search used for the parameter of OGD? On which validation set is it taking place.\n  c. Skewed-MNIST should reference the inspiration from color MNIST of Arjovsky et al. (2019). A table with the makeup (number of digits of each color) of each of the train/val/test datasets would be useful. It is stated that “Since the overall class frequencies are balanced … we drop the P(Y)”. Drop it from where (same comment for skewed COCO on Places)? Appendix C.1 describes how digits were split into two sets – was there a precise protocol for this? How is the “tend(ency) to be confused” measured context? What was the optimizer used for training  - SGD? \n  d. WILDS-iWildCam: it is stated that “We use Heuristic 3 for evaluating methods on this dataset. Heuristic 3 mentions several approaches: adding a tunable scalar to the diagonal? Using the identity matrix? Using a “pseudo-count”? \n  e. Table 2: How are the error estimates estimated relevant to all tables)? Why are the error estimates here +- 0? Are the quantities really measured to perfect accuracy? \n\nMinor comments\n\n1. Before equation (4): “where e is a one hot vector for the predicted category” – the description and notation there can be clarified: it was initially unclear to me which predicted category is referred to, and only after reading Wu et. al (2021) did I understand that these are calculated for each step I separately. \n\n2. After equation (4), it is stated that calculating the gradients is tricky. Why is it so? For self-containedess, the statement should be explained. Similarly, before equation (7) it is stated that FTH is more efficient than OGD – efficient in which sense? Compute time? Memory? Data complexity? \n\n3. Right before 5.1.3, it is mentioned that “test-sets are smaller”. Smaller than what? \n\n4. Typos: \n- Heuristic 1, line 2: shiftis -> shift is\n- Two lines below equation (19): minimum -> minima\n- Last line of page 6: there’s a superfluous ). \n- 5.2.2, last line of first paragraph, should read “in neither training nor validation sets for OOD test.”\n- The reference to Sun and Saenko (2016) is missing bibliographic info (journal name).\n- Appendix A: equations (23) and (24) seem to be the same\n\n\nReferences \n[1] Storkey, When training and test sets are different, in:Quinonero Candela et al., Dataset Shift in Machine Learning, 2009\n[2] Moreno-Torresa et al., A unifying view on dataset shift in classification  (2012)\n[3] Schoelkopf et al., On Causal and Anticausal Learning (2012)\n[4] Zhang et al., Domain adaptation under target and conditional shift (2012) \n[5] Kull and Flach, Patterns of dataset shift (2014)\n[6] Breeds: Benchmarks for subpopulation shift, Santurkar et al. (2020)\n[7] Cortes and Mohri, Domain Adaptation in Regression (2011)\n[8] Cortes and Mohri, Domain adaptation and sample bias correction theory and algorithm for regression (2014)\n\n\nclarity,_quality,_novelty_and_reproducibility: The paper, while being short and concise, is for the most part easily readable. Some sections that I found to be more difficult to understand are listed above. \n\nExperiments are described clearly and seem reproducible. Some minor misunderstandings that I had regarding experimental protocols are listed above. \n\nAs far as I can tell, the paper's examination of online labels+conditional shift adaptation of neural networks is novel, as are the experiments performed here. \n\nAs detailed above, the quality of the paper can in my opinion be greatly improved if more context was provided about the distribution shifts considered, a more thorough empirical investigation was conducted, and the unclear/undefined terms and sections are clarified. \n\nsummary_of_the_review: The work presented in this paper is novel, seems technically correct, and addresses a key problem to many real-world scenarios. I believe that the work in its current state with some corrections/improvements could and should merit publication in some venue. However, with the flaws described above, I do not believe this paper is ready for publication. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 2:\nsummary_of_the_paper: This draft considers the problem of online learning with label shift in the presence of additional conditional shift. In addition to the change in the class-priors $\\Pr[y]$, the posterior probability $\\Pr[x|y]$ can also change over time. Based on the previous work of Wu et al. (2021), the authors propose three heuristics to improve its empirical performance for both classification and regression tasks when the additional conditional shift appears. In their empirical studies, the authors suggest 1) using the OOD validation set instead of the ID validation set when estimating confusion matrices (a key component in the work of Wu et al. (2021)), and 2) adding scaling hyperparameters to the original loss function to improve the performance. \n\nstrength_and_weaknesses: Strength:\n+ This work studies a well-motivated problem, which covers many real-world applications.\n\nWeakness:\n- The authors introduce three heuristics to improve the empirical performance of the algorithm proposed by Wu et al. (2021) when an additional conditional shift appears. There are, however, no significant or distinguishable improvements over the original method based on their empirical studies. For example, in Table 1, the proposed algorithm's performance on S-COCO-ON-PLACES and IWILDCAM (Avg.) is nearly the same compared with the original algorithm, and even worse than the original one in IWILDCAM (F1).\n- Heuristic 3 (adding a tunable scalar to the diagonal and renormalizing rows to the confusion matrix) is a common practice to avoid the non-invertible problem.\n- A discussion of the intuition behind the heuristics is suggested.\n\nclarity,_quality,_novelty_and_reproducibility: Clarity and Quality:\nIt is easy to follow this draft. The proposed idea is easy to understand.\n\nNovelty:\nThe novelty of the proposed method is limited. There is a lack of discussion of the intuition behind the proposed heuristics. In the related works, heuristic 3 is a common practice.\n\nReproducibility:\nThe proposed heuristic is not difficult to implement. The reproducibility is acceptable.\n\nsummary_of_the_review: This draft considers the problem of online learning with label shift. The authors propose three heuristics to handle the additional conditional shift based on the previous work of Wu et al. (2021). However, the empirical studies do not show significant improvements over the original approach. The proposed approach lacks theoretical support, and its proposed heuristics lack intuitive support, as well.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 3:\nsummary_of_the_paper: This paper studies how to adapt a black box to the testing distribution in an online fashion under the label shift condition. The main contributions are to propose several heuristics to improve the algorithm proposed by Wu et al., (2021) when the label-shift assumption is broken, or the confusion matrix is non-invertible. Empirical studies are conducted to validate the effectiveness of the proposed methods.\n\n\nstrength_and_weaknesses: ### strength\n+ This paper considers an interesting problem on how to relax the label shift assumption made in the previous work.\n+ This paper extends the online label shift problem to the regression setting.\n### weaknesses:\n- About the written quality: Although this paper is well-organized, some parts of the presentations, particularly the algorithm design part, are not totally clear. The unclear parts are listed as follows.\n\t- about the background: the notation $f$ in Eq. (2) and Eq. (4) are very confusing. In Eq. (2), $f$ is used for an underlying base model, but Eq. (4)  uses $f$ for the model to be learned. \n\t- about Heuristic 1: it is unclear to me how the validation set is collected. Does the learner collect the validation data once at the beginning of the testing online test stage, or do the data just appear in an online fashion? The former one seems less promising in the online adaption problem since the underlying distribution $P_t$ could be different for every iteration.\n\t- about the Bayesian methods: the notation $\\hat{y}_\\tau$ in Eq. (9) and $y_t$ in Eq.(13) is not defined, though I can guess they are the pseudo-label and the true label. It is unclear to me why Eq. (13) updates with the true label while Eq. (9) updates with the pseudo-label. It is a very strong requirement to obtain the true label for each iteration.\n\n- About the soundness of the proposed heuristics:\n\t- about the validation set: the main difference between this work and Wu et al., 2020 is that the latter does not require the validation set. When a validate set sampled from the testing distribution is available, a strong baseline is that we can just estimate $\\hat{\\mathbf{q}}^{\\mathrm{new}}$ by the labeled validate set. I think it would be necessary to compare the proposed method with such a baseline.\n\t- about Heuristics 1: Heuristics 1 aims to solve the problem when the label-shift condition is broken. But the proposed methods are still based on the reweighted classifier Eq (2). Such a kind of reweighting mechanism still crucially relies on the label shift assumption. In this sense, I believe only the adjustment on the estimation of the confusion matrix is not sufficient.\n\t- about Heuristic 3: it seems that the use of an identity matrix instead of the confusion matrix will sacrifice the unbiasedness of the gradient estimator. I am not sure whether such a method can perform well when the confusion matrix is invertible. \n\n\nclarity,_quality,_novelty_and_reproducibility: Quality: the paper lacks a comparison with the baseline, which learns directly with the validation dataset, and the proposed heuristics are somewhat unconvincing. (Please see the second point of the weaknesses for more details.)\n\nClarity: This paper is well-structured, but some notations are abused, which makes the background part hard to follow. Besides, the difference between this work and the previous one [Wu et al., 2021] on the problem setup is not clearly discussed. It is unclear to me how the validation dataset is collected, which plays an important role in the algorithms design of this paper. \n\nOriginality: this paper is an extension of Wu et al., [2020]. Although some heuristics are proposed to improve the previous work, some of them are less convincing to me.\n\nReproducibility: codes for the experiments are not provided.\n\n\nsummary_of_the_review: This paper considers how to improve the previous work [Wu, et al., 2021] to learn beyond the label shift assumption. This is an interesting problem, but the proposed method is somewhat unconvincing to me as it requires an additional validation dataset and the reweighed classifier still implicitly relies on the label shift assumption. Besides, the background, problem setup, and method parts of the paper are not clearly written. Given the above concerns, I tend to reject this paper.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: Using standard label shift handling techniques/equations, it is proposed to handle additional manifestation-shift (p(x/y) shift) by simple heuristics based on additional hyperparameters (like in (8)) or using Bayesian models over labels etc. (19). Empirically it is shown that the proposed heuristics improve over those in Wu et.al.'21.\n\n\nstrength_and_weaknesses: Weakness:\n1. The writeup does not seem to be self contained and the reader may have to refer to Wu et.al.'21 to understand some background.\n2. Some parts may require more details. For example, how is p estimated in (8) ? Do the additional hyperparameters effect p's estimation?\n3. While the Bayesian model equations are standard, the connection of it for the problem at hand in section 4 is not clear to me. Perhaps some re-writing might help here.\n4. Apart for some cryptic justification, none of the propositions seem to have theoretical justifications. This makes it very hard for me to evaluate the work.\n\nclarity,_quality,_novelty_and_reproducibility: Clarity:\nThe paper does not seem to be self contained and some necessary details seem to be explained at a very high level. this makes it very hard to understand the methodology, motivation etc.\n\nNovelty and QUality:\nlimited novelty and technical contribution - essentially the heuristics\n\nReproducibility:\nBecause of the high level intuitions, and lack of details, one may not be able to reproduce the results.\n\nsummary_of_the_review: This seems to be a work in preliminary form with many missing details and justifications. Hence I tend to not accept the paper.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nempirical_novelty_and_significance: 1: The contributions are neither significant nor novel."
            },
            {
                "round3": "comment: Thanks for the review! \n\n> 1. Writing not self-contained; reader might have to consult background literature\n\nWe apologize for the lack of self-containedness. We have tried to emphasize some of the key missing background information suggested by the reviews.\n\n> 2. Estimation of p\n\np is estimated by the OGD and FTH methods described in Section 2.1. The additional hyper-parameters do not influence the FTH estimation of p, but they can technically be included in OGD’s estimation of the class-wise error-rate. However, in early experiments when we attempted including the revised learning rule within OGD’s p-estimation step, we found it to hurt performance. We hypothesize that this might be because our hyper-parameters are learned in an online setting on a specific validation set, while OGD’s estimation of the class-wise error rate is performed in aggregate (on potentially different validation set, as in our C-IID experiments). While more refined alternatives might be possible, in practice, our heuristic for now is to only apply the additional hyper-parameters at the predictive step, with p estimated as in Wu et al. \n\n> 3. Relevance of the Bayesian connection\n\nThe Bayesian interpretation of FTH leads us to the FTH-H-B method for classification problems, and the generality of the update equations helps us easily derive analogous updates for regression. The model follows from Section 3.2, where we approximate FTH by using an identity confusion matrix, and observe in Section 4 that using a smoothing vector is equivalent to the update equations 13 and 14. We have rewritten parts of 3.2 to make the transition clearer.\n\n> 4. Lack of theoretical justification\n\nWe agree that we do not provide any theoretical justifications behind our proposed heuristics. We view our work as more of an empirical study at this point, but a more principled view would definitely be an improvement.\n\ncomment: Thanks for the review! \n\n> Writing:\n\n – We apologize for the confusions; we inherit all of our Background discussion from past work, where f is treated generally as a classifier at first, but then, due to the output-adjustment approach, the classifier f gets eventually replaced by the classifier g in Eq. 5, with the learnable parameters being p and not those of f anymore, which remains a black-box classifier.\n \n–  The validation set is collected at the beginning, but the processing is sequential when selecting hyper-parameters. For the purposes of validation, we do not think it is unrealistic to collect an entire set up front for development, as long as the test-set is strictly sequential. Also, we do not consider any temporal label-drift in our problems, only that due to different deployment locations, where the models are deployed separately.\n\n – We apologize for not being clear; in practice, since we do not have access to the true label, we do in fact use the pseudo-label in Eq. 13 as well. We have updated the draft to make this clearer.\n\n> Soundness:\n\n – We agree that the required access to an additional validation set can be a weakness. However, we note that Wu et al. also requires a large hold-out validation set, both to estimate the confusion matrix as well as to estimate the gradient for OGD. We agree with a baseline where we are aware of the true distribution, and have added two such oracle rows to Table 1, OFC and OFC-H.\n\n – We agree that we are still approximating the predictive rule based on the label-shift assumption. Our take is that as long as an approximation works in practice, it’s worth reporting.\n\n – When the confusion matrix is invertible, an identity approximation ought to under-perform (as long as the matrix is estimated to a sufficiently high degree of robustness, which is why the literature always involves holding out a significantly large sized portion of data to estimate this). Our point is that such robust estimation might be difficult to achieve in practice when working with long-tailed problems in post-hoc settings. \n\ncomment: Thanks for the review! \n\n – We agree that the improvements from the modifications and new methods we discussed does not lead to particularly strong improvements, but we believe that our discussion in this submission serves relevant objectives of (a) kicking-off a discussion in the black-box label-shift literature, by pointing to the practical implications of some of the strong assumptions typically made in experimentation, and (b) providing practitioners with some encouragement about the fact that such methods can continue to be useful even when assumptions are violated (with simplifications when techniques don’t work out of the box), and potentially improved by taking advantage of extra degrees of freedom.\n\n – To our knowledge, adding a tunable scalar and renormalizing is not common practice in the label-shift community when facing degenerate matrices. The widely-adopted practice so far in this literature has been to avoid such matrices altogether, mostly by making hold-out validation sets large enough, and working on well-balanced problems unlike the long-tailed dataset we use in iWildCam. The typical recommendation in the label-shift literature when encountering such confusion matrices has been to either use a pseudo-inverse or a soft-confusion matrix. We initially found these alternatives to be detrimental to downstream accuracies in initial experiments (we have added a set of results in Appendix E), which is why we opted for a “mixing” of the original confusion matrix with an identity matrix. However, we agree with reviewer feedback that this heuristic is somewhat ad-hoc, and has the limitation of adding an extra hyper-parameter. Furthermore, most often, we found the tuning process to pick the largest scalar, suggesting an asymptote towards an identity approximation. Therefore, we have revised our heuristic to simply use the identity, which we also believe makes for a smoother narrative, since our Bayesian view of FTH comes from this approximation.\n\n – Our key intuition is that learning additional degrees of freedom for the “strength” of logit adjustment on an OOD validation set is more likely to reflect the level of correction that works best once a classifier is “out there”. When in-distribution, classifiers that rely on spurious correlations are high-performant, and learning scaling parameters on this set is more likely to generalize poorly in OOD settings. We have added results in Appendix D, Table 6, where we show how methods fare when hyper-parameters are selected using in-distribution/out-of-distribution/oracle validation. These numbers suggest that OOD validation is most often beneficial over in-distribution validation (and oracle validation of course works best).\n\ncomment: > Further comments\n\n> 1. Motivate online vs. offline methods. Discussion restricted to constant changes.\n\nWe agree with these points; our motivations for this work were primarily based on the use of cloud based APIs in real-time deployment in several locations, necessitating the black-box, post-hoc, online setup. Such deployments are often across several clients in diverse locations with unique label-distributions, as captured in our choice of WILDS datasets. Such deployments often come with additional resource-constraints, since ideally one would perform client-based adaptation on-device, which is a good use-case for cheap approaches based on output-distribution-adjustment. While temporal drift would indeed be a natural extension for such online applications, in this submission we restrict our scope to exploring constant-shift.\n\n> 2. Further discussion of intuitions behind heuristics required. What hyper-parameters are picked on the validation set?\n\nWe agree that OOD validation should not be expected to help when the nature of the distributional shift is significantly different between validation and test-time settings. Model-selection is quite a fundamental problem in OOD settings with no obvious solution. Regarding the specific methods we consider, our intuition is that learning additional degrees of freedom for the “strength” of logit adjustment on an OOD validation set is more likely to reflect the level of correction that works best once a classifier is “out there”. When in-distribution, classifiers that rely on spurious correlations are high-performant, and learning scaling parameters on this set is less likely to generalize OOD. To illustrate this, we have added results showing the performances when performing validation using IID/OOD/Oracle sets in Table 6 in the Appendix. \n\nThe learning rate for OGD-based methods, and the scaling hyper-parameters for FTH-H and OGD-H (and also the pseudo-count for FTH-H-B) are picked using the validation sets.\n\n> 3. Ad-hoc heuristic for generating invertible confusion matrix.\n\nPlease see above, response to Major Weaknesses, point 3.\n\n> 4. Role of the Bayesian discussion, notation, discussion on regression unclear.\n\nThe Bayesian interpretation of FTH leads us to the FTH-H-B method for classification problems, and the generality of the update equations helps us easily derive analogous updates for regression. Such insights ought to be equally applicable to pure-label shift. We agree our notations were awkward in (11-12), we’ve made changes. Section 4.1 tells us how the output distribution in a regression problem might be similarly “reweighted” to account for label-shift in a new location. The equations are generally applicable, however, one ought to use conjugate priors for whatever likelihood model one picks, otherwise the integral in Eq. 14 becomes intractable (although one might also attempt approximations here). By “calibration” we simply meant the scaling hyper-parameters, playing a role equivalent to those in the classification problems, and they are useful similarly. \n\n> 5. Miscellaneous comments and clarifying questions (a)-(e).\n\nWe apologize for not being clearer. (a) We have added a brief description for the surrogate loss implementation; (b) The grid search was conducted for the learning rate in OGD from 1e-8 to 10.0 in steps of x10; (c) We’ve added these details. Re. P(Y) being dropped, we drop it from the decision rule, since weighting by a uniform distribution does not change the rule. Re. the protocol for the S-MNIST dataset, we trained the same base network the same way as in the experiments (SGD with weight decay) for 200 iterations, and then looked at the confusion matrix on the test set. This tells us which digits get confused the most, and we used this partition the digits such that each split contains 5 digits; (d) Heuristic 3 does not involve a pseudo-count, it only deals with the approximation for the confusion matrix. We used the tunable-scalar variant in our submission, but we have changed this to simply be the identity matrix, as described above; (e) In Table 2 for PovertyMap, the standard deviations are over multiple re-orderings of the test-sets in each location (since we are evaluating an online method). The error estimates are very low because the methods we evaluate are generally quite robust to random re-orderings. In other cases, when we do not split up results by folds or over separate trainings of the base network, the variation comes to a large extent from the variation in the training of the base networks (as can be partially inferred from the standard deviations for the Base results). We observed all methods to be very robust to random re-orderings of the test sets, except for S-COCO-on-Places, which is why we aggregated results over 20 trials for each seed.\n\n> Minor suggestions for writing/typos.\n\nWe have updated the draft fixing these.\n\nOnce again, thank you for the very thoughtful, well-written, and constructive review!\n\ncomment: Thanks for the review!\n\n> Major Weaknesses:\n\n> 1. Nature of distribution shifts; characterization as ‘general distribution shift’ more appropriate\n\nWe agree that we are fundamentally dealing with general distribution shifts. In an earlier draft, we used the more common phrase in the domain adaptation literature, “generalized target shift” (GeTARS) from [4] (Zhang et al.) to refer to our problem setting. However, at the time, we received feedback that this term was confusing to readers, and that we ought to use a title and description that makes it clear that our adaptation is explicitly for the output-distribution-adjusting label-shift problem. Re. our examples with “no label shift”, we do perform adaptation separately in the test environments before aggregating accuracy, and in each of these environments, there is both label-shift and conditional shift (by design in the synthetic experiments, and as analyzed/suggested in Koh et al. for iWildCam and PovertyMap).\n\n> 2. More comprehensive experimentation would allow for clearer conclusions.\n\nWe agree that conditional-shift can manifest in a variety of ways (correlation shifts, more-extreme covariate shifts such as domain shifts, subpopulation shifts), and we have not explored the full range. We also agree that for a really thorough empirical demonstration, we ought to explore the range of distribution-shift types at test-time as well as a range of base models likely to be encountered in reality, and also ranges/types of distributional-shift in validation sets. We do believe that our discussion in this submission serves relevant objectives of (a) kicking-off a discussion in the black-box label-shift literature, by pointing to the practical implications of some of the strong assumptions typically made in experimentation, and (b) providing practitioners with some encouragement about the fact that such methods can continue to be useful even when assumptions are violated (with simplifications when techniques don’t work out of the box), and potentially improved by taking advantage of extra degrees of freedom.\n\nWe have added results in Appendix D, Table 6, where we compare hyper-parameters selection with IID/OOD/oracle validation. These numbers suggest that OOD validation is most often beneficial over IID validation (and oracle validation of course works best). As suggested, we have added the Optimal Fixed Classifier (OFC) in Table 1, and a variant OFC-H, where we learn scaling hyper-parameters on top of the fixed p.\n\n> 3. Discussion about degenerate confusion matrices embryonic; advantages unclear over existing alternatives.\n\nWe agree our discussion of confusion matrices was not very detailed. As further background, we developed Heuristic 3 when we found use of both the pseudo-inverse as well as the soft-confusion matrix to result in poor performance on the iWildCam dataset. We found that as we added larger values to the diagonal of the confusion matrix and renormalized (i.e. converged to the identity matrix), downstream validation performance recovered. We tuned the additive scalar on an average of downstream OOD validation accuracy for FTH and OGD. However, we note that most often, the largest value was picked through this scheme, suggesting that the identity-approximation is usually better. Given overall reviewer sentiment about the ad-hoc nature, and the limitations induced by adding yet another hyper-parameter to the system, we are rolling this back to simply using the identity matrix itself as an approximation. This also makes for a smoother narrative, since our Bayesian re-interpretation for FTH is derived from the identity approximation. We have added a discussion in Appendix E (Tables 7 and 8).\n\nRegression problems have indeed been considered generally in the literature on adaptation, and we are adding in the suggested citations. To our knowledge, a specific discussion for online black-box label-shift has not been discussed in the context of regression, analogous to the discussion for classification problems in Wu et al. (2021). We have clarified this in the draft.\n\n> 4. Lack of clarity in discussion about conditional shift, notation, and other points.\n\nIn this submission, we generally consider the “conditional shift” that occurs due to changes in location; the implication is that the semantic features conditioned on labels do not change significantly, but that background features can, due to the environmental shift. Our notations and discussion in Section 4 are heavily based on the literature, in particular that in our citations of DeGroot (2004) and Murphy (2007). We apologize for not adding descriptions around the derivation in Appendix A; the goal is to show why (and under what conditions) the Bayesian posterior-update equation is valid (it is valid when the samples in a new deployment location are drawn independently in that location). We have edited our draft to reflect these clarifications."
            }
        ]
    },
    {
        "id": "WHlt5tLz12T",
        "decision": "Accept: poster",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: LiftedCL: Lifting Contrastive Learning for Human-Centric Perception\nKeywords: contrastive learning, human-centric perception\nAbstract: Human-centric perception targets for understanding human body pose, shape and segmentation. Pre-training the model on large-scale datasets and fine-tuning it on specific tasks has become a well-established paradigm in human-centric perception. Recently, self-supervised learning methods have re-investigated contrastive learning to achieve superior performance on various downstream tasks. When handling human-centric perception, there still remains untapped potential since 3D human structure information is neglected during the task-agnostic pre-training. In this paper, we propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. In particular, to induce the learning process, a set of 3D skeletons is randomly sampled by resorting to 3D human kinematic prior. With this set of generic 3D samples, 3D human structure information can be learned into 3D-aware representations through adversarial learning. Empirical results demonstrate that LiftedCL outperforms state-of-the-art self-supervised methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing."
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: This paper proposes the Lifting Contrastive Learning (LiftedCL) workflow to train robust human-centric representations that are useful for downstream 2D and 3D human tasks such as pose estimation, shape prediction, and parsing. There are two core components. The contrastive learning part generates features that are invariant and equivariant (under inverse transforms). The \"lifting\" part generates 3D skeletons from the human-centric representations and is trained with a discriminator along with randomly sampled real 3D skeletons. Experiments show that the proposed training produces more useful features for downstream tasks than competitive alternatives such as MoCo-v2 and PixPro. \n\nstrength_and_weaknesses: ### Strength\n- The intuition of learning a robust representation that encodes 3D human information makes a lot of sense to me.\n\n- The designed pipeline is straightforward. It could inspire further development in this direction.\n\n- The invariance and equivariance training is novel in the context of 3D human feature learning tasks. \n\n- I like the idea of having unpaired randomly sampled 3D skeletons as auxiliary training data.\n\n- Experiments show the solid performance of the proposed method when trained on ImageNet + COCO: downstream tasks have significant improvement. That validates the idea of encoding 3D human structure information is useful for human-centric tasks.\n\n### Weakness\n- I think the paper could use a strong baseline in which all the 3D elements are replaced by 2D counterparts. For example, instead of lifting 2D features to 3D skeletons, the network will output 2D skeletons and a discriminator will compare those with randomly sampled real 2D skeletons. Other parts of the network will be kept as much as possible. If the proposed LiftedCL outperforms this 2D alternative on 2D human tasks such as pose estimation and parsing, then it is clear that 3D is essential to both 2D and 3D human tasks. I agree with the authors that 3D is important but it will be more convincing with such an experiment.\n\n- I might have missed it, but is there an ablation study on the effectiveness of the KCS layer?\n\n\nclarity,_quality,_novelty_and_reproducibility: ### Clarity\nThe paper is straightforward and easy to follow. \n\n### Quality\nThe quality of the work is good.\n\n### Novelty\nI think the idea of encoding 3D-aware representations in human-centric tasks via lifting-to-3D contrastive learning is novel and it can inspire future studies in this direction.\n\n### Reproducibility\nThere seems to be a sufficient amount of detail in the paper for reproducibility. It will be better if the authors plan to release their code. It is not currently mentioned in the submission.\n\nsummary_of_the_review: Overall, I like this paper because it studies an interesting and important problem of encoding 3D-aware representations in human-centric tasks and provides a viable solution. The design components in this workflow could inspire future research in this direction. The experiment results are solid on multiple human-centric tasks. The paper could be further strengthened by adding the 2D baseline mentioned above, but it is a good contribution to the community in its current form.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 2:\nsummary_of_the_paper: The authors propose, Lifting Contrastive Learning (LiftedCL), a feature representation learning approach based on contrastive learning with primary focus on the task of 2D/3D human pose estimation from a single image. Authors further use kinematic chain space (KCS) layer and a discriminator to regularise the skeleton. The key idea is to decompose the information into two, invariant and equivariant features. \n\nAuthors provide key experiments on MSCOCO, MPII and H36m datasets. Ablation study shows that the unsupervised pre-training with invariant and equivariant features improves performance over vanilla supervised training.\n\nstrength_and_weaknesses: Strengths:\n+ Contrastive learning for 3D tasks is quite useful because acquiring annotated 3D data is very hard for for a lot of tasks. Unsupervised feature learning approaches make a lot of sense.\n+ I like the ablation study to motivate the choice of using invariant and equivariant feature learning.\n+ The approach is straightforward and easy to follow.\n\nWeakness:\n- The idea of using equivariance and invariance for feature learning has been proposed in Cheng et al. ICCV'21 (not cited). Eg: They also use contrastive learning on augmented images and use that augmentation of the same image are positive pairs and different images serve as negative pairs. How is the proposed formulation different?\n\n- How is KCS + adversarial learning (Sec 3.3) different than Critic Network in RepNet? Eq. 4,5,6,7 here are same as Eq. 3,4,5,6,7 in RepNet.\n\n- Sec 3.2: To learn equivariant features, since the inverse transformation is known, why do we need to use contrastive learning? We have direct supervision that features of positive pairs should be the same after the inverse transform. Can the authors comment on this?\n\n- Datasets like H36m are a bit saturated for pose estimation as they are a bit synthetic and controlled. 3DPW is more relevant dataset these days. Authors use it for shape recovery but not pose estimation evaluation. Can the authors comment on this?\n \nClarifications and minor suggestions:\n- Sec 3.2: What augmentations are used?\n- Add citation to Wandt et al. ECCVW'18 and RepNet, CVPR'19 in Sec 3.3. Authors mention this in related work but it is better to add the citation here as well, otherwise KCS looks like a contribution of this work.\n- PixPro is just an unsupervised feature representation method. How do the authors use it for 2D pose prediction in Sec 4.2? Can the authors provide more details?\n- Add a bullet list of key contributions. It clearly distinguishes the contributions of this work from the other works. This is quite useful as the final approach often uses components from other works.\n- The ideas of equivariance and invariance are quite well known in the 3D vision community. Authors might be interested in checking some of these works out. eg: ART, Zhou et al. 3DV'22.\n\nclarity,_quality,_novelty_and_reproducibility: The paper is well written and easy to follow. I have concerns about originality as the key contributions of this work seem a bit derivative of existing works, equivariance and invariance  formulation is very similar to Cheng et al. ICCV'21 and lifting to 3D using KCS is similar to RepNet, CVPR'19. Can the author clarify why the current approach is not a combination of these two?\n\nsummary_of_the_review: I'm currently on the fence for this work. The key ideas seem to be coming from other works (see weakness section) and I've seen better numbers on 2D/3D pose estimation task. Although these SOTA works (including the best performing version of SimplePose, whose smaller version authors use as baseline) use much bigger networks than Res50 so they are not directly comparable.\nI don't see a very compelling reason to accept this work. I'm open to changing my mind if authors can clarify the limited technical novelty aspect.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\ndetails_of_ethics_concerns: I do not see immediate ethical concerns for this work.\n\nReviewer 3:\nsummary_of_the_paper: Paper proposed a novel architecture for self-learning of human-centric perception problems: 2D/3D human pose estimation, human shape recovery and human parsing. The proposed method is simple and effective -- exceeding SOTA for several benchmarks. The encoder subnetwork uses Contrastive Learning and Lifting network to generate the 3D skeleton of the input; this is then combined with Adversarial loss from Discriminator sub-network to improve the encoder. \n\n\n\nstrength_and_weaknesses: Strengths\n1. Paper's motivations and methods are well presented and supported by experimental results.\n2. Experimental setups are comprehensive and strongly supports the core claim of the paper: self-supervised learning network with contrastive approach to generate 3D representation of human images is superior to other self-supervised approaches.\n3. Ablation studies are thorough and well-done.\n\nWeaknesses\n1. Incremental novelty as proposed method is similar to other approaches for CV tasks, including image classification, object detection, semantic segmentation, etc\n2. Experimental results only compare with other self-supervised methods. \n\nclarity,_quality,_novelty_and_reproducibility: 1. Clarity is excellent. Paper is well-written and clearly presented.\n2. Quality is good. All sections are well organized. Figures, Tables etc. are also properly labelled and easy to understand.\n3. Novelty is incremental. See Weaknesses above.\n4. Reproducibility is high as many details are given. However, it's not clear if source code will be shared after acceptance.\n\nsummary_of_the_review: Overall, paper is a good piece of work which should be accepted for ICLR. There are aspects of the work which are incremental, e.g. using of Contrastive Loss, 3D structure representation of human. However, the scientific contributions of it are still sufficiently significant, especially in view of its strong empirical experimental results.\n\n\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: We sincerely appreciate your careful and thoughtful comments and time. We will explain your concerns point by point.\n\n**Q1: Novelty**\n\n**A1:** The core idea of our work is to learn 3D-aware human-centric representations which can be transferred to various downstream tasks. However, the learned representations using previous self-supervised learning methods are not 3D-aware [1, 2, 3], or the pre-training process requires multi-view data or multi-modality data [4, 5]. Our main contribution is that we show a feasible way to learn 3D-aware representations for human-centric tasks via lifting and adversarial training only using single-view images. Moreover, our work could also inspire other works, e.g., learning 3D-aware representations for object detection.\n\nWe believe \"a new use of an existing technique with different goals\" is meaningful when the new use is effective and brings a new perspective. Although invariance and equivariance [3, 6, 7], lifting [8] and KCS [9, 10] have been proposed in previous works, we first use them for 3D-aware human-centric representation learning and show promising results. Our work focus on pre-training the backbone and we hope our work can inspire others when learning 3D-aware representations.\n\nMore specifically, In our work, invariant and equivariant contrastive learning is to help learn generic representations, experimental results in Table 8 also show that this part is essential for the final 3D-aware representation learning. However, how to encode 3D human information into the representations is not solved. Inspired by previous 3D pose estimation works [8, 10], we lift learned representations to 3D skeleton format and use adversarial learning to regularize them, thus encoding 3D human information into the learned representations, making them 3D-aware. The main difference between our LiftedCL and RepNet is that we focus on learning 3D-aware representations for human-centric pre-training, while RepNet focuses on fine-tuning stage.\n\n**Q2. Why do we need to use contrastive learning to learn equivariant features**  \n\n**A2:** We follow previous works [3, 11] to conduct contrastive learning when learning equivariant features. To this end, our proposed LiftedCL can be compared with other SSL methods in a fair way.\n\n**Q3. 3DPW dataset not used for pose estimation**  \n\n**A3:** When comparing our proposed LiftedCL with other SSL methods on 3D human pose estimation, we choose [12] to fine-tune as this model uses a ResNet-50 backbone which can be pre-trained using our and other SSL methods. We directly refer to its official codes for convenience, however, it only conducts experiments on Human3.6M dataset, so we do not test our LiftedCL in 3DPW 3D pose estimation.\n\ncomment: **Q4: Clarifications**\n\n**A4:** \n\n**1. Sec 3.2: What augmentations are used?** \n\nWe mention this in Appendix C, implementation details of LiftedCL, \"The data augmentation pipeline consists of 256×256-pixel random resized cropping, random color jittering, random gray-scale conversion, gaussian blurring and random horizontal flip following (Chen et al., 2020b). Rotation is not used for fair comparison with previous self-supervised learning methods.\" \n\n**2. citations**\n\nWe will add citation of KCS [9] and RepNet [10] in the method section to avoid confusion. We will also add citation of \"Cheng et al. ICCV'21\" [6] and \"Feige et al. ICML'19\" [7] in the method section.\n\n**3. PixPro is just an unsupervised feature representation method. How do the authors use it for 2D pose prediction in Sec 4.2?**\n\nHere is detailed explanation, our LiftedCL and compared SSL methods focus on pre-training the backbone, e.g., ResNet-50. When evaluating one SSL method, we first use this method to pre-train the backbone, and then we use the pre-trained weights to initialize the the backbone of specific downstream task model. In the end, the downstream task model is fine-tuned on its downstream task dataset.\n\n**4. Add a bullet list of key contributions.** \n\nOur main contributions are summarized as follows:\n\n1. We propose the Lifting Contrastive Learning (LiftedCL) for human-centric pre-training in a simple yet effective way.\n\n2. We demonstrate a feasible approach to learn 3D-aware representations via lifting and adversarial learning only using single-view images.\n\n3. LiftedCL significantly outperforms state-of-the-art self-supervised learning methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing.\n\nWe will add this in our final version.\n\n\n**References**\n\n[1] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729–9738, 2020.\n\n[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020.\n\n[3] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684–16693, 2021.\n\n[4] Hou Ji, Saining Xie, Benjamin Graham, Angela Dai, and Matthias Nießner. Pri3d: Can 3d priors help 2d representation learning?. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5693-5702, 2021.\n\n[5] Fangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu. Versatile Multi-Modal Pre-Training for Human-Centric Perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16156-16166, 2022.\n\n[6] Zezhou Cheng, Jong-Chyi Su, and Subhransu Maji. On equivariant and invariant learning of object landmark representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9897-9906, 2021.\n\n[7] Ilya Feige. Invariant-equivariant representation learning for multi-class data. In International Conference on Machine Learning, pages 1882-1891, 2019.\n\n[8] Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2640–2649, 2017.\n\n[9] Bastian Wandt, Hanno Ackermann, and Bodo Rosenhahn. A kinematic chain space for monocular motion capture. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0-0, 2018.\n\n[10] Bastian Wandt, and Bodo Rosenhahn. Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7782-7791, 2019.\n\n[11] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024–3033, 2021.\n\n[12] Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu. Human pose regression with residual log-likelihood estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11025-11034, 2021.\n\ncomment: We sincerely appreciate your careful and thoughtful comments and time. We will explain your concerns point by point.\n\n**Q1: Incremental novelty**\n\n**A1:** We appreciate your objective comment on our novelty. We believe \"a new use of an existing technique with different goals\" is meaningful when the new use is effective and brings a new perspective. To the best of our knowledge, we are the first learn 3D-aware human-centric representations only using single-view images, while previous works may need multi-view data [1] or multi-modality data [2]. Experimental results show the effectiveness of our LiftedCL. Moreover, our work could be beneficial to the general community as it shows the feasibility of learning 3D-aware representations via lifting and adversarial learning. Our idea has the potential to be applied in other tasks, e.g., object detection. We hope our work can inspire others when learning 3D-aware representations.\n\n**Q2: Experimental results only compare with other self-supervised methods.**\n\n**A2:** We follow previous works, MoCo [3], SimCLR [4], DenseCL [5] and PixPro [6], to compare our method only with other self-supervised learning methods. We all focus on pre-training stage, so we pre-train the backbone (e.g., ResNet-50) using different SSL methods and fine-tune using the same downstream task model. The used downstream task model can be chosen in a wide range of existing SOTA models. We are not sure if our explanation could solve your concerns, if not, we may have to bother you to comment and then we shall know.\n\n**Q3: Reproducibility and codes**\n\n**A3:** Training codes and our pre-trained models will be released in GitHub.\n\n**References**\n\n[1] Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, and Matthias Nießner. Pri3d: Can 3d priors help 2d representation learning?. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5693-5702, 2021.\n\n[2] Fangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu. Versatile Multi-Modal Pre-Training for Human-Centric Perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16156-16166, 2022.\n\n[3] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729–9738, 2020.\n\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020.\n\n[5] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024–3033, 2021.\n\n[6] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684–16693, 2021.\n\ncomment: We sincerely appreciate your careful and thoughtful comments and time, especially your approval for our idea of encoding 3D information into representations, which really means a lot for us. We will explain your concerns point by point.\n\n**Q1: A strong baseline in which all the 3D elements are replaced by 2D counterparts**\n\n**A1:** We conduct an ablative experiment on Human3.6M 2D pose estimation using 2D lifting. To build the 2D lifting baseline, we first remove the depth channel of 3D skeleton labels, and then change the output dimension to fit that. Other parts of the framework and hyper parameters are all kept unchanged. We use two-stage pre-training \"IN+CC\". The result is 87.6% JDR, 0.4% JDR below 3D lifting, showing that 2D lifting is inferior to 3D lifting on Human3.6M 2D pose estimation. We also evaluate it on MPII 2D pose estimation, and it achieves 88.7% PCKh\\@0.5, which is 0.2% PCKh\\@0.5 below that using 3D lifting.\n\n**Q2: An ablation study on the effectiveness of the KCS layer**  \n\n**A2:** To conduct this ablative experiment, we remove the KCS branch in the Discriminator. The result on Human3.6M 2D pose estimation is 87.3% JDR, 0.7% JDR below full branch. The result is in accordance with our intention that KCS can help \"better detect properties of human skeletons such as kinematic chains, bone lengths and joint angle limits\" (in Sec 3.3). \n\n**Q3: Reproducibility and codes**  \n\n**A3:** We appreciate your approval for our work's reproducibility. Training codes and pre-trained models will be released in GitHub for future work. Moreover, what we expect more is that our LiftedCL can inspire future works when learning 3D-aware representations for other tasks. For instance, learning 3D-aware representations for object detection. We have shown the feasibility to learn via lifting and adversarial learning in human centric tasks, however, how to do it for other tasks is not very clear right now and it could be a valuable research subject.\n"
            }
        ]
    },
    {
        "id": "IQM-3_Tzldw",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning\nKeywords: Federated learning\nAbstract: Federated learning (FL) emerged as a novel machine learning setting that enables collaboratively training deep models on decentralized private data. Due to the heterogeneity (non-iidness) of the decentralized data, FL methods (e.g. FedAvg) suffers from unstable and slow convergence. Recent works explain the non-iid problem in FL as the client drift, and deal with it by enforcing regularization at local updates. However, these works neglect the heterogeneity among different communication rounds: the data of sampled candidates at different communication rounds are also of non-iid distribution, and we term it as period drift, which as well as client drift can lead to aggregation bias that degrade convergence. To deal with it, we propose a novel aggregation strategy, named FedPA, that uses a Parameterized Aggregator, as an alternative of averaging. We frame FedPA within a meta-learning setting, and formulates the aggregator as a meta-learner, to learn to aggregate the model parameters of clients. FedPA can directly learn the aggregation bias and well calibrate and control the direction of aggregated parameters to a better direction towards the optimum. Experiments show that FedPA can achieve competitive performances compared with conventional baselines."
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: The paper presents a learnable aggregation scheme in the context of federated learning. The paper achieves this using meta-learning to generalize the parameters of the aggregator with a proxy dataset. The paper identifies 'period drift' in the current federated learning setup and presents the meta-learning-based aggregator as a way to overcome this issue. The paper follows up with experimental results showing increased accuracy for different methods and heterogeneity rates across two datasets.\n\nstrength_and_weaknesses: Strengths\n1. The paper identifies a possible source of client drift\n2. The paper proposes a novel aggregation scheme.\n\nWeaknesses\n1. The paper does not do enough to discriminate between regular client drift and the so called period drift either theoretically or through experiments.\n2. The aggregation strategy uses a proxy dataset which limits use cases. Also, it is very similar to other knowledge distillation-based techniques like FedET[1] and DS-FL[2]. A comparison of performance with these methods should be shown to justify its usefulness.\n3. There is no ablation study showing the effect of the data distribution in the proxy data on model performance.\n4. The experimental settings are not strong. The datasets and models are too simple. I suggest including results on CIFAR-100 and Stack Overflow datasets.\n\n\n[1] Cho, Y. J., Manoel, A., Joshi, G., Sim, R., & Dimitriadis, D. (2022). Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning. arXiv preprint arXiv:2204.12703.\n\n[2] Itahara, S., Nishio, T., Koda, Y., Morikura, M., & Yamamoto, K. (2020). Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data. arXiv preprint arXiv:2008.06180.\n\n\nclarity,_quality,_novelty_and_reproducibility: Clarity: The paper is not very well written and has some grammatical mistakes.\nQuality: The paper quality needs to be improved. The axes font in the figures is too small to read and overall the writing needs to be updated.\nNovelty: The paper has limited novelty.\nReproducibility: No code was given.\n\nsummary_of_the_review: The paper proposes a meta-learning-based aggregation scheme. However, it does not show enough theoretical or experimental justification to highlight the effectiveness of the algorithm. Additionally, the paper lacks enough ablation studies on the different aspects of the algorithm like the data distribution of proxy data, the influence of the size of the aggregator model, etc.  Furthermore, the paper's concept of 'period drift' is not well defined despite being a key motivation of the algorithm.\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 1: The contributions are neither significant nor novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 2:\nsummary_of_the_paper: This paper proposed a method called FedPA that deals with client and period drift problems. The period drift problem is caused by the asynchronized updates of each client, leading to extra bias in model aggregation. The authors proposed a learning-based aggregation strategy, that parameterizes the aggregation function using neural network models. The models are trained under a meta-learning framework, which treats the global model as a meta-learner and each client as a specific task. Experimental results have shown that FedPA can account for the additional bias induced by both client and period drift and therefore demonstrate superior performance over other FL baselines in various tasks.\n\nstrength_and_weaknesses: Strength:\n1. Drifts in FL arise in time and space, while most existing works only address the heterogeneity of client data distributions. This paper has discovered this practically important problem and proposed the notion of period drift that can facilitate further research.\n2. The authors have conducted comprehensive experiments in different settings. Results have shown that FedPA has a superior advantage over baselines in different categories.\n\nWeakness:\n1. This paper lacks an in-depth discussion on why meta-learning frameworks are particularly suited for the period drift problem. It seems like both client and period drift influence the model performance by introducing extra bias in model aggregation. In that case, why not use a regularization-based approach incorporated with a temporal dimension? Moreover, it seems like this paper [1] have studied a similar problem, the authors could consider comparing FedPA with their work as an additional baseline.\n2. The dynamic system analogy seems useless in section 3. The authors are not using significant knowledge from this area. I would recommend adding more discussions or simply removing this part to avoid confusion.\n3. From my understanding, FedPA accounts for additional bias via controlling $\\Delta w_t^k$ through $u_t^k$, then why do we need two separate neural networks for both $w$ and $\\Delta w$? The authors need to be more specific on the choice of NN architectures.\n\nMinor:\n1. Please add a discussion on FedDL in section 2.\n2. Please move the definition of $n_k$ and $n$ to the beginning of section 3.\n\n[1] Jothimurugesan et al., Federated Learning under Distributed Concept Drift, 2022\n\nclarity,_quality,_novelty_and_reproducibility: This paper is well-motivated and easy to follow. The technical novelty is ok but not much. It is also ambiguous whether the proposed method is the best solution for this problem (please see weakness).\n\nsummary_of_the_review: This work can benefit from adding more in-depth discussion on the unique advantages of the proposed method and further polishing the writing. I am leaning toward rejecting this paper at this time.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: This paper proposed a method named FedPA to learn aggregators in federated learning. When aggregating model updates from clients, instead of uniform or weighted by number of examples as in the popular FedAvg, FedPA will feed both the global model and the client model updates to a neural network before “aggregating”/averaging. The aggregator is trained on the server with a proxy dataset. Experiments on EMNIST and MovieLens show the advantage of FedPA. \n\n\nstrength_and_weaknesses: The general idea of “meta learning” and “learning to aggregate” makes sense. \n\nHowever, as the authors commented, though interesting, having a proxy dataset is a strong assumption in problem setting.  \n\nIn addition, the server seems to have access to the model updates of each individual client, which makes it hard to be consistent with FL privacy principles, and other privacy techniques like SecAgg and differential privacy [Federated Learning and Privacy https://queue.acm.org/detail.cfm?id=3501293]\n\nMy major concern is that the proposed method seems to be ad-hoc. It is hard for me to connect the motivation of “period shift” to the proposed FedPA method. Instead of learning neural networks for aggregation, I am wondering if there are easier approaches to use the proxy data. For example, we can simply “learn” a scalar weight to do weighted aggregating/averaging of client model updates. I would strongly suggest some more ablation studies of the proposed FedPA method.\n\nA second major concern is the experiment performance. The accuracy on FEMNIST seems to be lower than expected. For example, [Adaptive Federated Optimization https://arxiv.org/abs/2003.00295] reports >80% for natural user non-IID. \n\nI would also appreciate some more comments on hyperparameter tuning. For example, how are 100 communication rounds, 5 epochs, learning rate \\eta_l=0.01 chosen? How are training epochs (5 for MovieLens, 30 for FEMNIST) and learning reate \\eta_g chosen?\n\n\n\nclarity,_quality,_novelty_and_reproducibility: AFAIK, the idea is novel.\n\nNeed improvement and clarification: the intra/inter-communication arguments look inaccurate to me. The global model and control variates are shared “inter” rounds, for example, in SCAFFOLD [Karimireddy et al. 2021]. Some previous work also assume all clients can participate in training, and I would strongly encourage the authors to clarify the source of “intra-round heterogeneity”\n\nCould you clarify “since many companies like Google, Facebook remains previous data at the turning point of legislation for privacy” for motivating the proxy dataset?\n\nI may have missed it, is the code open sourced? The authors mention they implement the algorithms in PyTorch. Using a FL framework, or based on previous released code can significantly help reproducibility. \n\nMinor issue:\nThe citation format does not seem to be consistent. I would suggest the authors carefully consider the usage of `\\citep` and `\\citet`. \nI cannot understand why Kairouz et al 2021 is cited for Figure 1. \nSome grammatical errors might need to be corrected. \n\n\nsummary_of_the_review: The idea is interesting, but the draft itself needs improvement. Ablation study and experimental performance are my main concerns. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: 1. The paper tackles the problem of client drift (locally) in federated learning (FL) due to heterogeneous client data distributions. Besides, they target the period drift (globally), which is the inter-communication heterogeneity of data distributions.\n2. They propose a learning based parameterized aggregator called FEDPA, debiasing model aggregation under client drift and period drift in a unified framework. Their key approach is to learn an adaptive calibration parameter to approximate the global objective.\n3. The input of the framework includes the intra-communication client parameters.\n\n\nstrength_and_weaknesses: Strength\n1. The 2 levels of drift they tackle is indeed important for FL.\n2. They can achieve better training performance on the MovieLens dataset and FEMNIST dataset.\nWeaknesses\n1. Some small typos, grammar issues, inconsistency issues as stated below.\n2. The key idea should be agnostic to the dataset and types of tasks. It would be good to show the performance on other LEAF datasets.\n3. It would be good if the overhead on the server can be quantified. If the method can not be applied at scale (equation 7 seems to be iterative but more clarification would be good), it is not a perfect match for FL.\n\n\nclarity,_quality,_novelty_and_reproducibility: 0.Could you explain how the drift issue in FL (stated in Figure 1) is different from similar convergence issues in non-FL training pipelines? Explaining this is important for people to understand that this is indeed a new problem.\n1. At the beginning of section 2, “Federated learning with non-iid data Federated Learning with non-iid Data” is duplicated.\n2. “Implementation” does not end with “.” in section 4.1.\n3. “We compare FEDPA with the baselines that using proxy dataset” should be “We compare FEDPA with the baselines that use proxy datasets” in section 4.2.\n4. The idea is inspired by the control theory and dynamic systems. They add a regularization term, which is determined by the client weight updates, weight matrix and the parameters of the learnable aggregator.\n5. Could you explain Equation 7 regarding how the dense() operator actually functions?\n6. Plan for open source code?\n\n\nsummary_of_the_review: The introduction and related work part is in good shape and I enjoyed reading it. But the quality for writing could be improved. (See the above session for more details)\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "No author response found. For ICLR 2023, the author response format might have changed or the authors may not have provided a response."
            }
        ]
    },
    {
        "id": "8XfHh4XSQ0Q",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Adaptive Block-wise Learning for Knowledge Distillation\nKeywords: Knowledge distillation, Local error signals, Bilevel optimization\nAbstract: Knowledge distillation allows the student network to improve its performance under the supervision of transferred knowledge. Existing knowledge distillation methods are implemented under the  implicit hypothesis that knowledge from teacher and student contributes to each layer of the student network to the same extent. In this work, we argue that there should be different contributions of knowledge from the teacher and the student during training for each layer. Experimental results evidence this argument. To the end, we propose a novel Adaptive Block-wise Learning~(ABL) for Knowledge Distillation to automatically balance teacher-guided knowledge between self-knowledge in each block. Specifically, to solve the problem that the error backpropagation algorithm cannot assign weights to each block of the student network independently, we leverage the local error signals to approximate the global error signals on student objectives. Moreover, we utilize a set of meta variables to control the contribution of the student knowledge and teacher knowledge to each block during the training process. Finally, the extensive experiments prove the effectiveness of our method. Meanwhile, ABL provides an insightful view that in the shallow blocks, the weight of teacher guidance is greater, while in the deep blocks, student knowledge has more influence."
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: The manuscript observes the problem of fixed contributions of ground truth knowledge and teacher knowledge at different blocks of the student networks during knowledge distillation training. The author proposes a bi-level optimization scheme to balance the knowledge on the lower level and update the network based on optimized balance at the higher level.\n\nstrength_and_weaknesses: Strength:\n+ Paper is well written\n+ Experiments on common benchmark datasets for KD\n+ A number of KD schemes are tested with the proposed scheme in the experimental results\n\nWeaknesses:\n1. The problem might not be well justified. The main argument of the paper is to use the performance of the proposed scheme to claim the solving of the fixed contributions of the two types of knowledge in different blocks. It would be more interesting to see what part of the network goes wrong with the fixed contributions scheme. \n\n2. Whether the improvement of the network comes from the auxiliary network? Please consider adding an experiment by removing the bi-level optimization but still keeping all the auxiliary networks. Consider tuning the fixed parameters between ground truth knowledge and teacher knowledge in this scheme. \n\n3. The increase in performance might be the result of solving the gradient vanishing not solving the balance between ground truth information and the teacher at the different blocks. The addition of an auxiliary could also be interpreted as a shortcut to the last layer. Consider checking the gradient vanishing between the standard KD scheme and the proposed KD scheme. Or further, can we just add a shortcut from each layer to the final layer and have different losses corresponding to each shortcut?\n\n4. For some datasets, the utilization of teacher knowledge only could also achieve comparable or even better results compared to the utilization of both teacher knowledge and ground truth knowledge. The authors should consider adding a KD scheme with only teacher knowledge guidance as one of the baselines. So that the scheme of balancing between teacher knowledge and ground truth knowledge would be more meaningful.\n\n5. How does the proposed scheme training time increase compared to the standard KD schemes? The main concern about the bi-level optimization is that it always takes too much time, while the achieved improvement in the manuscript seems to be not too significant (only around 0.5%, and poorer in some cases). This might be the strongest challenge to apply the proposed scheme to practical applications.\n\n6. In table 3, Is that normal where the standard KD performs poorer than supervised training?\n\n7. The authors should details out how each parameter is fixed in standard KD schemes in all experiments.\n\nsummary_of_the_review: The paper might be helpful to the research community. However, the concern about training time might reduce the chances of its practical applications. While considering about the fundamental contributions, the authors should justify the problem clearly as well as put more analyses on the proposed schemes. \n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 2:\nsummary_of_the_paper: The authors argue that contributions of knowledge from the teacher to the student network should be layer dependent.  Adaptive block-wise learning automatically balances the contribution of knowledge between the student and teacher for each block.\n\n\nstrength_and_weaknesses: The paper talks in generalities and employs a general abstract notation. There is no experimental specificity.\n\n\nclarity,_quality,_novelty_and_reproducibility: There is minimal novelty and no technical depth to the paper.  \n\n+ There is no description of the teacher, student or auxiliary network. \n+ It would have been nice if the loss functions $\\mathcal L_S$ and ${\\mathcal L}_{KD} $ could have been defined. \n+ There is no description of how the data was divided between training and validation. \n+ What were the $\\gamma_1^{(l)}$ and $\\gamma_2^{(l)}$ set to in the experiments? \n\n+ There was no analysis of the experimental improvements.  Are the improvements statistically significant?\n\n\nsummary_of_the_review: The paper is incrementally novel, but it has extensive experiments.\nThe paper talks in generalities without any technical or experimental details.  A below average ML paper.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 3:\nsummary_of_the_paper: This paper proposes Adaptive Block-wise Learning, a method that adapts the amount of knowledge distillation from a teacher to a student at each layer of the network. A set of auxiliary networks is used for this purpose. The method uses local error signals to control how much teacher or student knowledge should each layer use. This is accompanied by a set of metavariables to control these contributions, which are optimized using bilevel optimization. The authors report experimental results on several SOTA knowledge distillation methods, showing that this more selective knowledge distillation strategy can benefit both homogeneous and heterogeneous knowledge distillation problems.\n\nstrength_and_weaknesses: Strengths:\n- The paper presents a novel idea of using local errors to determine the contribution of teacher and student networks during knowledge distillation.\n- The paper is well written and well organised. \n- The paper reports relevant findings for the knowledge distillation area, in particular that deep blocks of the network benefit more from student knowledge, while shallow blocks of the network benefit from more teacher knowledge.\n\nWeaknesses:\n- Some terms are used that are never defined. This makes the paper difficult to understand for a researcher not in this area. For example, what are homogeneous and heterogeneous knowledge distillation? Please provide a definition or cite a relevant definition.\n- In the experiments, it is never mentioned how big is the teacher and how big is the student for each of the datasets. Furthermore, how relevant is this for the proposed method? \n- I would like to see how the findings regarding the contribution of teacher and student at different layers in the knowledge distillation setting are connected to previous similar findings in transfer learning in general (see for example [1])\n\nReferences: \n[1] Neyshabur, B., Sedghi, H., & Zhang, C. (2020). What is being transferred in transfer learning?. Advances in neural information processing systems, 33, 512-523.\n\nclarity,_quality,_novelty_and_reproducibility: The paper is clearly written and appears technically sound. The quality of the proposed method and the experimental results is high. Although local error signals have been studied in the past, the proposed approach seems novel, although I am not fully familiar with knowledge distillation literature. The reproducibility is enough for a researcher in the area.\n\nsummary_of_the_review: The paper presents a method for knowledge distillation that considers the contribution of teacher and students distinctly at different layers of the network. This approach seems novel and useful for the area. The conclusions around the different degrees of contribution of teacher and students at different depths of the network seem to be an important contribution (although I am not fully aware of the knowledge distillation literature, so I may be convinced otherwise based on other reviews). Based on this, my recommendation at this stage is for acceptance of the paper.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 4:\nsummary_of_the_paper: This paper attempts to improve knowledge distillation (KD) research from the perspective of allocating adaptive weighting factors to update layer-wise learnable parameters of the student model during back-propagation. The authors conjecture that the knowledge from the teacher  should be different to the student layers from shallow to deep. Based on this, they propose a new KD method called adaptive block-wise learning (ABL), which uses auxiliary networks for learning to dynamically generate meta weighting factors for gradients propagation at different blocks of the student network. In designs, these auxiliary networks are added to intermediate student layers, and leverage the local error signals to approximate the global error signals on student objectives. In implementation, ABL uses a greedy bi-level (two-stage) optimization. The effectiveness of ABL is validated on image classification datasets CIFAR100 and ImageNet.     \n\nstrength_and_weaknesses: Strengths.\n\n+ Making the knowledge from teacher to layers of the student network to be different is interesting.\n\n+ The proposed ABL sometimes shows improvements (but actually are not fair) to different KD methods.  \n\n+ Experimental comparisons are conducted on both CIFAR100 and ImageNet datasets with different teacher-student network pairs.\n\nWeaknesses.\n\n- The method.\n\nThe core ideas of the proposed ABL are in two aspects: 1) adding auxiliary networks to some intermediate student layers to ease the training; 2) in optimization, auxiliary networks are used to approximate the global error signals from the pre-trained teacher, learning to dynamically generate meta weighting factors for gradients propagation at different blocks of the student network. However, there already exist many KD works that explore the use of auxiliary networks to improve KD process. Some representative works are DKS [1], BYOT [2], DCM [3] , MetaDistiller [4], to name a few. Besides, MetaDistiller and some other works also explore the use of meta learning. Unfortunately, these works are completely missed by the authors. A comprehensive comparison of ABL with them, both in methodology and performance is necessary. \n\n[1] Deeply-Supervised Knowledge Synergy, CVPR 2019.\n\n[2] Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation, ICCV 2019.\n\n[3] Knowledge Transfer via Dense Cross-layer Mutual-distillation, ECCV 2020. \n\n[4] MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation, ECCV 2020.\n\n- The motivation.\n\nIn the paper, the authors argue that different layers of the student network have different emphases on the knowledge learned through the one-hot labels and the knowledge distilled by the teacher. Furthermore, the authors claim a new contribution that the deep and abstract representation inclines to learn from student knowledge, while the shallow and less abstract representation tends to be guided by teacher knowledge. However, the analysis and experiments provided in the paper are not convincing enough. Could the authors provide more evidence or richer experiments to support these claims? E.g., how about the role of auxiliary networks? how to design them? the effects of  different auxiliary classifiers? how about the performance of adding low-quality/shallow auxiliary networks? what will happen if smoothing the predication of the pre-trained teacher model?\n\n- The experiments.\n\nExperimental comparisons are problematic and misleading: (1) For experimental comparisons (Table 1 and Table 2) on CIFAR100, it seems that all results for counterpart baseline methods (denoted as 'Stan.') are directly copied from the paper of CRD (mostly), and the paper of DKD. As a result, the authors merely tested the combination of ABL and each of them, but not tested each corresponding counterpart with the same training machine and training code settings. As a result, all reported \\delta values are totally misleading; (2) For experimental comparisons (Table 3 and Table 4) on ImageNet, the authors also use such an unfair comparison; (3) This even applies to some ablations. In a nutshell, the authors did not actually run any experiments for counterpart baseline methods (denoted as 'Stan.') at all. Therefore, main experiments need to re-design and re-implement. Even w.r.t. the current results, the improvement from ABL is mostly marginal.\n\nA comprehensive comparison of ABL with closely related methods such as DKS [1], BYOT [2], DCM [3] and MetaDistiller [4] is necessary. \n\nAs auxiliary classifiers play a key role in ABL, a deep analysis of them is also necessary, please see my comments in 'The motivation' for details. \n\nHow about the training cost of ABL compared to counterpart baselines methods?\n\n**----Update----**\n\nI keep my original score as my major concerns are not well addressed.\n\nclarity,_quality,_novelty_and_reproducibility: The basic ideas of this paper are easy to understand, but the presentation is not good enough. This paper has serious issues in novelty, experiments and claims. Code is not provided.\n\nPlease refer to my comments in 'Strength And Weaknesses' for details.\n\nsummary_of_the_review: This paper is below the acceptance bar of ICLR.\n\nPlease refer to my comments in 'Strength And Weaknesses' for details.\n\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "No author response found. For ICLR 2023, the author response format might have changed or the authors may not have provided a response."
            }
        ]
    },
    {
        "id": "yKbprarjc5B",
        "decision": "Accept: poster",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Leveraging Large Language Models for Multiple Choice Question Answering\nKeywords: NLP, language models, multiple choice question answering, symbol binding, GPT-3, Codex\nAbstract: While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., “A”) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated."
            },
            {
                "round2": "Reviewer 1:\nThanks to the authors for the detailed response. While I still don't see technical novelty as a strength of this paper, I see that it is a thorough paper and the empirical analysis itself has some novelty and is insightful.\n\nReviewer 2:\nThanks for the author's detailed responses, however, the novelty is still my big concern.\n\nReviewer 3:\nsummary_of_the_paper: This paper addresses the difference between multiple-choice prompting and standard prompting (so called cloze prompting), clarifying major reasons why LLM underperforms on Multiple Choice Question Answering (MCQA) problems. First, what LLM tries to predict in terms of “more likely” does not always mean “more correctly”. This conflation often happens when the tokens in the answer sequence is less common or less grammatical. Second, LLM must rely on normalization schemes to compare candidate answers with different lengths or different frequencies. But this yields additional dependency on tokenizer. Third, standard prompting compares different options only indirectly via the (normalized) likelihood without direct comparison. Obviously, such standard prompting is expensive comparing to generating one option token.\n\nTo make LLM solve MCQA problems with order-invariance, the authors propose Multiple Choice Symbol Binding (MCSB) capability that could be model-agnostically testable by recording the answer with the highest probability for each ordering of question (so called PPA). The experimental results show that training on code data (especially by multi-staging) is useful for MCSB. Providing more shots as few-shot examples also help boosting the performance.\n\n\nstrength_and_weaknesses: (Strengths)\n1) Reveals problematic ingredients for likelihood-based answering.\n2) Introduce the concept of MCSB and measure it by PPA.\n3) Concentrated results that significantly improves QA performance by using multiple-choice prompting.\n\n\n(Weaknesses)\n1) Individual problematic ingredients are neither being theoretically-proven nor empirically-proven.\n2) No novel/brand new ideas. Mostly empirical analysis based on OpenAI playground.\n3) Some major arguments are less supported.\n\n\nclarity,_quality,_novelty_and_reproducibility: The submission is an analysis paper rather than finding something new.\n\nsummary_of_the_review: (Major concerns)\nHow to make sure Codex model clearly outperforms Instruct model? This is a critical question as the authors measures the main experiments (Table 2) that compare Multiple Chocie Prompting (MCP) and Cloze Prompting (CP) only with Codex model. \n\n0) The capability to perform MCSB could be due to human feedback alignment by Reinforcement Learning rather than other points indicated by the authors.\n\n1) Are the PPA difference between Codex and Instruct (in Figure 2) statistically significant? While no statistical test has been provided, it seems not easy to decline null hypothesis that says the difference is a random effect.\n\n2) Only Codex tested on OpenBookQA shows strong performance gain when using MCP, whereas Instruct outperforms Codex on the other two tasks in Table 1. More detailed experiments are necessary to convince how Codex achieve such higher accuracy.\n\n\n(Minor concerns)\n1) Any reason to choose OpenBookQA which also matters the performance of retriever?\n\n2) Do you know how Codex model is exactly trained? Codex model that you used could be first based on Instruct, then being further trained on code data. Equally likely, Codex model might perform it's own alignment similar to Instruct but based on the preference of generated codes.\n\n\n\n\ncorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 4:\nsummary_of_the_paper: The authors find that the MCQA ability of LLMs has been underestimated, and they propose multiple choice prompting (MCP), in which a question and its symbol-enumerated candidate answers are all passed to an LLM as a single prompt. Surprisingly, the performance of LLMs equipped with MCP dramatically improves, approaching or even surpassing SOTA. This demonstrates that the power of LLMs can be used broadly in the future.\n\nstrength_and_weaknesses: Strength:\n\n1. This paper is well-written and easy to understand.\n\n2. The authors propose a simple but effective prompting method, which outperforms previous CP methods, approaching or even surpassing SOTA performance.\n\n3. Experimental results show that the MCQA ability of LLMs has been previously underestimated. And there is a better way to prompt a single LLM. The potential of multiple choices prompts can be further tapped. Future work include prompt engineering is still promising. \n\nWeakness:\n\n1. The novelty of this paper is limited. Multiple choices prompting (MCP) has been used in other QA tasks, such as TruthfulQA and RACE.\n\n2. Although the experimental results prove that the proposed multiple choices prompting (MCP) methods can outperform existing cloze prompting (CP) methods, the reasons behind it are still unclear. Since the authors have listed several problems within CP methods, I'm curious about whether these problems are all solved or avoided by their MCP methods. More analysis is needed to show this.\n\n\nclarity,_quality,_novelty_and_reproducibility: Clarity\nGood. But it would be better to include more analysis.\n\nQuality\nGood. A simple but effective prompting method.\n\nNovelty\nNovelty is limited. Details can be found in the weakness part.\n\nReproducibility\nGood. The code is attached by the authors.\n\n\nsummary_of_the_review: Interesting paper but not good enough. It would be better to include more analysis on whether MCP can deal with these several problems that CP faces.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 5:\nsummary_of_the_paper: The authors identify a better method to prompt LLMs for multiple-choice question answering. Instead of (the usual) comparing the probability of producing each answer, they present all options to the model and then identify the right option by producing just the letter that identifies the answer.\n\n\nstrength_and_weaknesses: Strengths: The authors explain their approach well. They also discuss the (somewhat surprising) variance between different models in their ability to separate the letter from the answer. (They call this Multiple Choice Symbol Binding.) The approach is evaluated on a wide range of (20) datasets.\n\nWeaknesses: The approach is not new, just discussed and evaluated. The authors differentiate their suggested prompting from “prompt engineering”, which they seem to define as fine-tuning of prompts to increase model performance. However, I’m not convinced that these are fundamentally different, and would include research such as theirs in the general domain of prompt engineering.\n\nclarity,_quality,_novelty_and_reproducibility: The paper is well written and I believe the experiments are verifiable with the given information, i.e. it should be possible to reproduce them.\n\nRegarding novelty, I am less convinced. The authors mention others having used the MCP approach. So the main addition here is the systematic discussion and wide range of experiments.\n\n\nsummary_of_the_review: The authors discuss an alternative (but not novel) way to prompt LLMs for better results on multiple-choice tasks. The prompt is well-motivated and thoroughly discussed with a good range of experiments that support the author's arguments. However, it is not novel: it is a fairly obvious way to prompt and has been tried before.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 6:\nsummary_of_the_paper: The paper tackles the problem of using Large Pre-trained Language Models (LLMs) for multiple-choice question-answering. Instead of using the standard cloze formulation, the paper suggests presenting the question and answer choices to the model and have the model output the answer symbol (e.g., A, B,C, ...etc.). The authors conduct a thorough study on multiple datasets and using different LLMs and show that the new formulation improves over variants of the standard cloze prompting technique. In addition, they measure the effect of shuffling the order of answers on the performance and note which LLMs show better invariance to the order of the answers. \n\nstrength_and_weaknesses: Strengths:\nThe paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings). \n\nWeaknesses:\nThere is no contribution/novelty from the modeling/methods side. The paper is mainly an empirical analysis of a different way of formulating an existing problem. \n\nclarity,_quality,_novelty_and_reproducibility: The paper is quite well written and easy to follow and the code will be made available which facilitates predictability. \nThe paper presents an interesting and thorough empirical analysis but there is not much novelty from the methods/modeling side.\n\nsummary_of_the_review: Overall, while the paper doesn't have modeling/methods novelty, it presents a thorough and interesting analysis of how a different formulation of Multiple-Choice Question-Answering affects the performance of LLMs on the task. The analysis includes multiple datasets and several LLMs and looked at issues affecting the performance such as order invariance which are sometimes ignored in other studies.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
            },
            {
                "round3": "comment: Thank you for the encouraging review! With respect to your comments about the novelty of the paper, please see our response in the shared response to all reviewers.\n\ncomment: Thank you for your insights and helpful feedback. We address your concerns below:\n\n> The approach is not new, just discussed and evaluated.\n> Regarding novelty, I am less convinced. The authors mention others having used the MCP approach. So the main addition here is the systematic discussion and wide range of experiments.\n> It is a fairly obvious way to prompt and has been tried before.\n\nPlease see our response to this concern in the shared response to all reviewers.\n\n> The authors differentiate their suggested prompting from “prompt engineering”, which they seem to define as fine-tuning of prompts to increase model performance. However, I’m not convinced that these are fundamentally different, and would include research such as theirs in the general domain of prompt engineering.\n\nThanks for pointing this out. We agree that this research falls in the general domain of prompt engineering and have made changes to the paper to reflect this. In particular, we’ve renamed the “Prompt Engineering” section to “Prompt Phrasing” and reworded it. References to prompt engineering in the Introduction and Conclusion sections have also been changed.\n\ncomment: Thank you for the thoughtful review and valuable recommendations. We address your concerns below:\n\n> The novelty of this paper is limited. Multiple choices prompting (MCP) has been used in other QA tasks, such as TruthfulQA and RACE.\n\nPlease see our response to this concern in the shared response to all reviewers.\n\n> Although the experimental results prove that the proposed multiple choices prompting (MCP) methods can outperform existing cloze prompting (CP) methods, the reasons behind it are still unclear. Since the authors have listed several problems within CP methods, I'm curious about whether these problems are all solved or avoided by their MCP methods. More analysis is needed to show this.\n\nThanks for the recommendation. We agree with the assessment and have substantively expanded the paper to address it. Please see the Results section for the majority of the relevant changes. In the revised version of this section we discuss how MCP avoids the issues outlined in Section 3. MCP is less expensive (we now quantify this by pointing out that on average the MCP columns in Table 2 required 4.3x less API calls (or equivalently forward passes with batch size 1) than did the CP columns). MCP clearly doesn’t require any normalization strategies for good performance (as supported by Table 2). To address the other two points from Section 3 (“No direct comparison between answers” and “Conflation of likelihood as answer and likelihood as natural language”) we evaluate accuracy for a diverse set of 3 datasets under different corruptions of the answer options. We show that answer choice corruptions like randomly adding spaces or changing casing result in 10+% accuracy loss for SCP methods but less than 2% loss for MCP.\n\n> Clarity Good. But it would be better to include more analysis.\n\nWe assume this is in reference to the same concern addressed above. Please let us know if this is in reference to something else that could be improved.\n\ncomment: Thank you for your time and for your valuable feedback on how the paper might be improved. We address your concerns below:\n\n> Individual problematic ingredients are neither being theoretically-proven nor empirically-proven.\n\nWe assume this is in reference to the same concerns you outline under the “Summary of the Review” section (which we address below). Please let us know if this is in reference to something else that could be improved.\n\n> No novel/brand new ideas.\n\nPlease see our response to this concern in the shared response to all reviewers.\n\n> Some major arguments are less supported.\n\nWe assume this is in reference to the same concerns you outline under the “Summary of the Review” section (which we address below). Please let us know if this is in reference to something else that could be improved.\n\n> How to make sure Codex model clearly outperforms Instruct model? This is a critical question as the authors measures the main experiments (Table 2) that compare Multiple Chocie Prompting (MCP) and Cloze Prompting (CP) only with Codex model. (0) The capability to perform MCSB could be due to human feedback alignment by Reinforcement Learning rather than other points indicated by the authors. (1) Are the PPA difference between Codex and Instruct (in Figure 2) statistically significant? While no statistical test has been provided, it seems not easy to decline null hypothesis that says the difference is a random effect. (2) Only Codex tested on OpenBookQA shows strong performance gain when using MCP, whereas Instruct outperforms Codex on the other two tasks in Table 1. More detailed experiments are necessary to convince how Codex achieve such higher accuracy.\n\nThanks for bringing this to our attention. It was not our intention in the paper to claim that Codex is empirically stronger than Instruct, but we failed to convey that in our writing. In our revised version of the paper we state clearly in Section 5.1, “It is evident that both Codex and Instruct have high multiple choice symbol binding ability and can effectively leverage MCP prompts across tasks. We make no argument that one is empirically stronger than the other.” We then outline our reasons for choosing Codex: “For all our further experiments we choose to use Codex (Davinci) because it is the least expensive (since it is currently in free beta), and because Codex should not add any dataset leakage issues that were not already present in GPT-3 since it was exclusively fine-tuned on Python files.” To expand a little more on why cost was an important factor consider the AG News dataset, which has a test set of size 7,600. To calculate accuracy in the few-shot MCP setting with Instruct would cost COST_PER_TOKEN x NUM_INSTANCES x TOKENS_PER_INSTANCE = (0.02/1000) x 7600 x ~4,000 = ~608 dollars. In the non-MCP setting it would cost that multiplied by 8 (4 answer choices and 2 forward passes for each). So just for the few-shot setting of AG News we would need to pay ~$5,000-5,500. And that is for one setting of one dataset. That Codex is currently free made presenting a large array of results possible.\n\n> Any reason to choose OpenBookQA which also matters the performance of retriever?\n\nThis is a great question, and we have made clarifying changes to the paper to address it (see “We evaluate on the OpenBookQA…available to the models.” in Section 4). It is true that the original OpenBookQA offers a “book” component (a list of elementary science facts from the WorldTree corpus), but we follow the practice from GPT-3 and later language model papers of ignoring the retrieval aspect of OpenBookQA. We choose OpenBookQA because it is widely used as a benchmark in large language model papers and because it is an archetypal multiple choice dataset.\n\n> Do you know how Codex model is exactly trained? Codex model that you used could be first based on Instruct, then being further trained on code data. Equally likely, Codex model might perform it's own alignment similar to Instruct but based on the preference of generated codes.\n\nWe agree that exactly how the Codex model was trained was left somewhat ambiguous in the paper in which it was presented. Luckily, we were able to get an answer from the authors. Codex was trained from the original (non-Instruct) GPT-3. We include this information in the paper in Section 4. We also express in that section that further training (whether it be through human feedback alignment or through training on code) seems essential for good MCSB ability because Codex and Instruct both outperform GPT-3, which they were fine-tuned from, in terms of MCSB ability.\n\ncomment: We thank all reviewers for the substantive and thoughtful reviews.\n\nBased on the helpful feedback we received we have made a number of changes to the paper and posted a revised version. These changes include better justifying our model and dataset choices, correcting the way we talk about our method with respect to prompt engineering, and adding additional quantitative analysis and an experiment with answer choice corruption to the Results section in order to demonstrate how MCP avoids the issues faced by SC that we enumerate in Section 3. We appreciate the feedback, which has helped us to meaningfully improve the paper.\n\nA common reviewer concern was that our method is not sufficiently novel. This concern is understandable, particularly as we include references to other works that use MCP in an effort to make our cited work as complete as possible. However, though the MCP formulation has been used before, we believe that our work brings enough novel and important contributions to justify its publication at ICLR 2023. Our reasons are below:\n\nFirst, existing work with MCP and large language models is limited, and we believe our work could meaningfully increase community adoption. As we state in our paper, the only LLM papers that use MCP are Gopher and followup Chinchilla. That is, MCP is not used for evaluation in many LLM papers (e.g., GPT-3, GLAM, Megatron-LM, PaLM, etc.). In the PaLM paper, which was published after Gopher and Chinchilla, the authors specifically mention that Gopher and Chinchilla use a different setup for RACE-h/m and thus don’t include the results (see Table 4 caption). Given that in our work we show that MCP greatly outperforms CP across most of 20 diverse datasets and that Codex+MCP achieves a new SOTA on 9 datasets, we believe future LLM paper authors will be more inclined to use MCP instead of sticking with the GPT-3 style prompts which systematically underestimate the performance of LLMs with high MCSB performance on MCQA tasks. To the best of our knowledge the only real prior discussion of MCP is found in the appendix of the Gopher paper and it is treated peripherally. Given the ~10% average accuracy gain of CP to MCP that we show across 20 datasets, we believe MCP deserves more than a peripheral treatment.\n\nSecond, our investigation on the importance of MCP may provide researchers with better explanations of observed phenomena. For example, in the Gopher paper the authors point out that Gopher outperforms GPT-3 by a notable 17%. They use MCP for Gopher for this task. The authors note, “Smaller models from the Gopher family do not perform as well on these tasks, which suggests that data alone does not explain the performance difference — the combination of scale and data is crucial.” It seems very possible that use of MCP is responsible for both some of the gain in accuracy and for the lower performance of smaller models (due to no emergence of MCSB ability).\n\nFinally, many aspects of our paper are, to our knowledge, completely novel. A few of these are:\n* We identify challenges faced by the CP approach - conflation of likelihood as answer and likelihood as natural language, reliance on normalization procedures, no direct comparison between answers, and computational expense. To our knowledge, previous work has cited none of these as reasons for using MCP.\n* We show that MCP avoids the above issues, including showing that using MCP requires less forward passes/API calls, and that MCP is robust to simple corruptions of answer choices (like changing character caps or adding spaces) where CP is not.\n* We show that normalization procedures are not necessary to get high performance on MCQA tasks when using LLMs.\n* For the first time, we introduce the concept of MCSB ability and systematically demonstrate that different models have different degrees of this ability (we agree with Reviewer jqnn that this result is “somewhat surprising” and believe other researchers will also find it to be a surprising and useful result).\n* To show the variance in MCSB ability between models we introduce a novel way of measuring the ability - PPA. We hope it will be useful to researchers in the future who would like to measure the MCSB ability of their models.\n* For the first time, we identify failure to use multiple choice prompting as a reason for why large language models often lag behind the state of the art on multiple choice question answering tasks. We systematically support this novel attribution with extensive experiments across 20 diverse datasets.\n\nThanks again for your reviews and consideration."
            }
        ]
    },
    {
        "id": "8foynpwwRb",
        "decision": "Reject",
        "source": "ICLR 2023",
        "statements": [
            {
                "round1": "Title: Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning\nKeywords: Optimization, Sharpness-aware Training, Computation Efficiency.\nAbstract: By driving optimizers to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose an efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functions. Further, we extend the RST to a general framework (G-RST), where we can adjust regularization degree on sharpness freely for any scheduling function. We show that G-RST can outperform SAM in most cases while saving 50\\% extra computation cost.\n"
            },
            {
                "round2": "Reviewer 1:\nsummary_of_the_paper: This paper presents new training methods called RST and G-RST to extend the geometry inspired training method SAM and improve its computational efficiency.\nBased on randomized gradient boosting RST randomly selects between SGD and SAM with probability based on parameterized Bernoulli distribution.\nThe authors explore different parameterization schemes for such a scheduling function and analyze their effect on computations and performance trade-off.\nThe authors also develop RST’s convergence properties for non-convex stochastic cases where the classes of objective functions are smooth and strongly convex.\nThe paper evaluates RST and G-RST on multiple image classification tasks showing that the proposed methods can save 50% computations while performing on-par or even better than the original SAM.\n\n\nstrength_and_weaknesses: - The paper is written in a very clear and professional way.\n- The paper is very solid with balanced views and analysis.\n- The paper provides convergence analysis which somehow lacks in SAM literature.\n- The resulting algorithm is simple and effective.\n- I literally didn’t find any flaws in the paper but thought adding more experiments on large scale and different domains could make the paper even stronger.\n\n\nclarity,_quality,_novelty_and_reproducibility: This paper is very well written overall. The paper is also original to some extent in the sense that although there exist SAM variants attempting to improve on SAM’s computational aspect, unlike most of these works this work develops based on matured techniques and optimisation characteristics and provides very well thought-out and reliable results.\n\n\nsummary_of_the_review: Highly recommended for interested readers on SAM literature.\n\ncorrectness: 4: All of the claims and statements are well-supported and correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nReviewer 2:\nsummary_of_the_paper: Recently, sharpness-aware training such as SAM has drawn large attention, because of its provably guarantee its significant performance improvement. However, SAM requires a huge additional computation cost, and it is not easy to adopt SAM on a large-scale model or a real-time analysis system. This paper provides an efficient computation for SAM, RST, and G-RST. They adopt randomized sharpness-aware training. The idea is simple, and it works well.\n\nstrength_and_weaknesses: Strength points\n1. This paper aims to improve the efficiency of SAM, the important problems.\n2. This paper raises interesting ideas such as randomized sharpness and its general extension G-RST that adjust regularization degree freely for any scheduling function.\n\nWeakness points\n1. The concept of this paper is similar to the other papers [1]. This paper should cite and discuss the difference between this paper and another paper.\n2. This paper is necessary to include diverse related works and efficient computation of SAM. For example, there are three research works, [2], [3], [4]. This paper only cites and discusses the [4]. However, there is no experimental comparison with [4]. I suggest that this paper should discuss and compare the performance with [2], [3], [4].\n\n[1] Zhao, Yang, Hao Zhang, and Xiuyuan Hu. \"SS-SAM: Stochastic Scheduled Sharpness-Aware Minimization for Efficiently Training Deep Neural Networks.\" arXiv preprint arXiv:2203.09962 (2022).\n[2] Du, Jiawei, et al. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).\n[3] Liu, Yong, et al. \"Towards efficient and scalable sharpness-aware minimization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[4] Du, Jiawei, et al. \"Efficient sharpness-aware minimization for improved training of neural networks.\" arXiv preprint arXiv:2110.03141 (2021).\n\nclarity,_quality,_novelty_and_reproducibility: This paper is well-written, and it is easy to follow the contents.\n\nThere is an implementation code, and it is easy to reproduce.\n\nsummary_of_the_review: This paper raises an interesting concept, but similar ideas were already suggested in other venues. \n\nDiscussion and additional experimental comparisons are necessary.\n\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 3:\nsummary_of_the_paper: Sharpness-aware minimisation (SAM) has been shown powerful to train high-performance deep learning models, but it also incurs at least double computational cost due to the extra back-propagation for the sharpness estimation. To improve the efficiency of the vanilla SAM, this paper proposes an training scheme, dubbed Randomized Sharpness-Aware Training (RST). RST performs a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and SAM. The probabilities of Bernoulli trials at each time are  determined by a predefined scheduling function $p(t)$. The average extra time is reduced from 1 to $\\sum p(t) / T$.\n\nstrength_and_weaknesses: Strength:\n- The proposed RST can reduce the extra computational cost of the vanilla SAM from 1 to $\\sum p(t) / T$ in average, and preserves similarly good performances.\n- The paper is easy to follow.\n\nWeakness:\n- The novelty of this paper is my only concern. The idea of applying SAM to a subset of parameters or iterations has been explored in several papers [Mi et al. 2022, Liu et al. 2022, Du et al. 2022]. For example, Liu et al. 2022 propose to only periodically calculate the inner gradient ascent across the training iterations; Liu et al. 2022 and Du et al. 2022 propose to select a subset of parameters to calculate the inner gradients in each iterations. Over expectations, randomly selecting a subset of parameters in iterations is same as randomly select interactions. And the later can be considered as a special case of the former --- alternatively masking out all parameters. \n\n\nMi et al. 2022, Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach\n\nDu et al. 2022, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks\n\nLiu et al. 2022, Towards Efficient and Scalable Sharpness-Aware Minimization\n\nsummary_of_the_review: The paper is easy to follow. The empirical experiments are enough to support the arguments. However, the novelty of this paper is the concern. The idea of selecting a subset of parameters or iterations for implementing SAM has been explored in several recent works.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nReviewer 4:\nsummary_of_the_paper: The paper targets at improving the efficiency of sharpness aware optimizer (SAM) by assigning a probability where a normal SGD optimizer is used. The decision is based on Bernoulli trial. The authors further propose a general framework to make the proposed optimizer schedule usable for different architectures. The idea is intuitive and clearly expressed. The proposed method is simple. This would not be an issue if it is extremely effective. However, the paper has missed many efficient SAM baselines such as GSAM [1] and SAF [2]. Besides, the experiments on large dataset ImageNet is not solid. The selected baseline models are different from the experiments on CIFAR-10 and the are not representative enough.\n\n\n[1] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training.\n\n[2] Du, Jiawei, et al. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).\n\nstrength_and_weaknesses: Strength:\n1. The motivation of the proposed method is clear. SAM is indeed suffering from low computational efficiency.\n2. The proposed method is clearly presented. The reproducibility is not an issue.\n\nWeakness:\n1. The proposed algorithm is intuitive and not novel enough. It is adding a probability to replace some of the SAM steps with normal SGD optimization steps. This would not be an issue if the proposed method is verified to be super-effective. However,\n2. The experiments do not show the superiority of the proposed method. The paper has missed some relevant baselines as mentioned in the summary section. Also, I was confused that why the selected architectures are different on CIFAR and ImageNet dataset?\n\nclarity,_quality,_novelty_and_reproducibility: The paper is well written. The proposed method is clearly presented. However, the proposed method is not novel enough and the experiments are not solid enough.\n\nsummary_of_the_review: In summary, the proposed method is not well supported by the presented experiments mainly due to two points:\n\n1. The missing baselines on efficient SAM algorithms and \n2. The missing architectures on ImageNet dataset.\n\ncorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\ntechnical_novelty_and_significance: 2: The contributions are only marginally significant or novel.\n\nempirical_novelty_and_significance: 2: The contributions are only marginally significant or novel."
            },
            {
                "round3": "comment: \nSecondly, \"Sharpness-Aware Training for Free\" (SAF), Nips 2022. SAF proposes to minimize the KL-divergence between the output distributions yielded by the current model and the moving average of past models. It should be particularly noted that unlike SAM, SAF follows another way, where no explicit regularization is imposed within the neighborhood region during the whole training process. In fact, this makes SAF seem more like the idea of knowledge distillation than sharpness-aware learning, where the teacher model is the moving average of past models and the student model is the current model. Besides, just as the results reported in their paper, the superiority of SAF over SAM variants still requires further demonstrations. Technically, SAF is not solving the computation issue that exists in SAM, because it completely discards the basic idea of SAM. So it is somehow not well appropriate to directly compare with SAF in the current topic in terms of the computational efficiency. We have added discussions in the revised paper.\n\nHere, we have also discussed with some typical works that targets to improve the computational efficiency in SAM. Importantly, we would like to show that our RST does not conflict with these efficient methods, which means that we do not have to compare with these corresponding efficient techniques. We could easily notice that when selecting performing SAM algorithm in RST, we would not impose any operation on SAM algorithm. In fact, we could adopt these efficient methods when selecting SAM algorithm to further improve the computational efficiency of performing SAM algorithm. The following table only shows the results of ResNet18. And for detailed results, the reviewer could check the appendix B.6 in the newly uploaded version, including the results regarding VGG16BN and WideResNet28-10.\n\nLookSAM \"Liu et al. 2022, Towards Efficient and Scalable Sharpness-Aware Minimization, CVPR2022\"\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|15.6|4.48|20.79|\n|LookSAM|+5.6|4.06|20.30|\n|LookSAM + G-RST[50%]|+3.0|4.18|20.44|\n|LookSAM + G-RST[75%]|+4.4|3.94|20.11|\n\nESAM \"Du et al. 2022, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks, ICLR2022\"\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|ESAM|+18.6|4.05|20.28|\n|ESAM + G-RST[50%]|+9.5|4.41|20.72|\n|ESAM + G-RST[75%]|+14.0|4.08|20.21|\n\nSSAM \"Mi et al. 2022, Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach, Nips2022\"\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|SSAM|+21.0|3.89|20.17|\n|SSAM + G-RST[50%]|+10.7|4.03|20.41|\n|SSAM + G-RST[75%]|+15.8|3.83|20.19|\n\nIt should be pointed that here, ESAM and SSAM are both implemented on the git repository https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization, where one A100 GPU is used.As we could see in the table, for all these efficient techniques, our RST could naturally improve the computational efficiency further since all these methods could have the choice to selecting performing base learning algorithm. However, if the selecting probability in RST is relatively low (0.5 in the table), it may harm the mixing effect, where the models may not be well trained. On the other hand, as properly raising the selecting probability (0.75 in the table), it is possible to acquire comparable results with these efficient techniques. We thank again for the review time and valuable comment, and the reviewer could check Appendix for more details.\n\n`Q: Also, I was confused that why the selected architectures are different on CIFAR and ImageNet dataset?`\n\nA: We would like to thank the reviewer for the valuable comment. Actually, the validation here strictly follows the convention in the typical contemporary literatures regarding SAM related works (such as SAM, ESAM, ASAM, gradient norm regularization etc.). As the reviewer could see in the related papers, they all use different architectures for Cifar and ImageNet. We suppose this may partly because that some network architectures are not well suitable for training on ImageNet because of the computation cost and the similarity with ResNet architectures.\n\nWe thank the reviewer for the valuable time and comments.\n\n\ncomment: `Q: The proposed algorithm is intuitive and not novel enough. It is adding a probability to replace some of the SAM steps with normal SGD optimization steps. This would not be an issue if the proposed method is verified to be super-effective.`\n\nWe would like to thank the reviewer for the helpful comment. We understand the reviewer's concern. In deep learning, we generally want the method to be as simple as possible because we could easy to implement in practice. And on the other side, we should also give such simple method a comprehensive analysis to show how and why it works. The reviewer could find that we have provided a comprehensive demonstration about the meaning of the proposed RSL and theoretically proved the convergence of such randomized policy, not just in an intuitive manner. Based on the interpretation from the perspective of gradient norm regularization, it is possible for us to manually control the randomized effect in RST. Further, we have also presented a detailed empirical study of how should one schedule the probability in practical training. The presented results have shown the effectiveness of our methods.\n\n`Q: However, the paper has missed many efficient SAM baselines such as GSAM [1] and SAF [2]..`\n\nWe would like to thank the reviewer for the valuable comment. Firstly, GSAM, ICLR2022. GSAM is an equivalent variant of SAM, which targets to contribute to the the convergence of SAM not its computational efficiency. Compared to SAM, GSAM still requires two steps i.e. two forward-backward propagations at each training iterations. For the two steps, GSAM keep the ascent step unchanged, and additionally subtract the orthogonal direction of gradient as the decent gradient. \n\nSince GSAM is not targeting to reduce the computational overhead in SAM, it is therefore not necessary to compare with GSAM. Instead, we could actually apply our randomized policy on these SAM variants like GSAM to boost their computational efficiency. The following table only shows the results of ResNet18. And for detailed results, the reviewer could check the appendix B.5 in the newly uploaded version, including the results regarding VGG16BN and WideResNet28-10. \n\n\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|15.6|4.48|20.79|\n|ASAM|+15.8|3.77|20.02|\n|ASAM + RST[50%]|+8.0|3.91|20.31|\n|ASAM + G-RST[50%]|+7.9|3.65|19.95|\n|GSAM|+17.4|3.81|19.91|\n|GSAM + RST[50%]|+8.9|3.99|20.43|\n|GSAM + G-RST[50%]|+8.9|3.70|20.10|\n\n\nAs we could see in the table, when using RST on ASAM and GSAM, we could obtain a similar results as using RST on SAM. Specifically, since RST and G-RST randomly selecting between sharpness-aware learning algorithm and the base learning algorithm, the computational efficiency could be largely improved for both ASAM and GSAM. And based on our demonstrations, RST would weaken the regularization effect, so we could see that the corresponding performance would be relatively lower than the standard sharpness-aware training. When doubling the regularization effect in G-RST, we could get comparable results with the standard sharpness-aware training, which confirms the effectiveness of our method.\n\n\n\ncomment: `Q: The concept of this paper is similar to the other papers [1]. This paper should cite and discuss the difference between this paper and another paper.`\n\nWe thank the reviewer for the helpful comment. This mentioned paper is obviously an unfinished work. And here we publicly promise no violation of terms in regards to plagiarism and ethics. \n\n`Q: This paper is necessary to include diverse related works and efficient computation of SAM. For example, there are three research works, [2], [3], [4]. This paper only cites and discusses the [4]. However, there is no experimental comparison with [4]. I suggest that this paper should discuss and compare the performance with [2], [3], [4].`\n\nWe thank the reviewer for the valuable comment. We have discussed all these methods in the newly uploaded paper. And importantly, we would like to show that our RST does not conflict with these efficient methods, which means that we do not have to compare with these corresponding efficient techniques. We could easily notice that when selecting performing SAM algorithm in RST, we would not impose any operation on SAM algorithm. In fact, we could adopt these efficient methods when selecting SAM algorithm to further improve the computational efficiency of performing SAM algorithm. The following table only shows the results of ResNet18. And for detailed results, the reviewer could check the appendix B.6 in the newly uploaded version, including the results regarding VGG16BN and WideResNet28-10.\n\nLookSAM[3]\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|15.6|4.48|20.79|\n|LookSAM|+5.6|4.06|20.30|\n|LookSAM + G-RST[50%]|+3.0|4.18|20.44|\n|LookSAM + G-RST[75%]|+4.4|3.94|20.11|\n\nESAM[4]\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|ESAM|+18.6|4.05|20.28|\n|ESAM + G-RST[50%]|+9.5|4.41|20.72|\n|ESAM + G-RST[75%]|+14.0|4.08|20.21|\n\nSSAM\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|SSAM|+21.0|3.89|20.17|\n|SSAM + G-RST[50%]|+10.7|4.03|20.41|\n|SSAM + G-RST[75%]|+15.8|3.83|20.19|\n\nIt should be pointed that here, ESAM and SSAM are both implemented on the git repository https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization, where one A100 GPU is used. As we could see in the table, for all these efficient techniques, our RST could naturally improve the computational efficiency further since all these methods could have the choice to selecting performing base learning algorithm. However, if the selecting probability in RST is relatively low (0.5 in the table), it may harm the mixing effect, where the models may not be well trained. On the other hand, as properly raising the selecting probability (0.75 in the table), it is possible to acquire comparable results with these efficient techniques. We thank again for the review time and valuable comment, and the reviewer could check Appendix for more details.\n\nBesides, we would like to particularly discuss the paper \"Sharpness-Aware Training for Free\" (SAF) [2], Nips 2022. SAF proposes to minimize the KL-divergence between the output distributions yielded by the current model and the moving average of past models. It should be particularly noted that unlike SAM, SAF follows another way, where no explicit regularization is imposed within the neighborhood region during the whole training process. In fact, this makes SAF seem more like the idea of knowledge distillation than sharpness-aware learning, where the teacher model is the moving average of past models and the student model is the current model. Besides, just as the results reported in their paper, the superiority of SAF over SAM variants still requires further demonstrations. Technically, SAF is not solving the computation issue that exists in SAM, because it completely discards the basic idea of SAM. So it is somehow not well appropriate to directly compare with SAF in the current topic in terms of the computational efficiency. We have added discussions in the revised paper.\n\nWe thank the reviewer for the valuable time and comments.\n\n\ncomment: We thank the reviewer for the valuable and constructive comments. We are very gratified that the reviewer admits our work. We totally agree with the reviewer's point that adding diverse experimental results could complete our work. Therefore, we start from trying to check our method on the noisy-label experiments. The following table shows the results of ResNet18 trained with different percentage of label corruptions on Cifar10. We could see that using G-RST could give comparable effect with SAM in the tasks of label corruption. And we would add these results and more results in the final paper. \n\n|Method|Corruption|C-10 Error|\n|--|:--:|:--:|\n|SGD|0.2|8.84|\n|SAM|0.2|7.25|\n|G-RST|0.2|7.10|\n\n|Method|Corruption|C-10 Error|\n|--|:--:|:--:|\n|SGD|0.4|12.28|\n|SAM|0.4|9.57|\n|G-RST|0.4|9.66|\n\n|Method|Corruption|C-10 Error|\n|--|:--:|:--:|\n|SGD|0.6|17.64|\n|SAM|0.6|13.59|\n|G-RST|0.6|13.23|\n\n|Method|Corruption|C-10 Error|\n|--|:--:|:--:|\n|SGD|0.8|32.09|\n|SAM|0.8|30.81|\n|G-RST|0.8|30.94|\n\nBesides, we have additionally performed detailed experimental results regarding mixing with other efficient techniques and SAM variants in the Appendix to further show the effectiveness of our method. The reviewer could check if interested. Again thanks for the valuable time and comments.\n\ncomment: We thank all the reviewers for their constructive and valuable comments. We have carefully investigated all the comments one by one. According to these comments, we have made many improvements to the work and have uploaded the revised paper. \n\nThe core or maybe the only concern could be summarized as the comparisons with other recent new contemporary works that target to improve the computational efficiency in SAM. We have added discussions in the Introduction. And we have provided a detailed investigation to show that our RST does not conflict with these efficient methods. Appendix B.6 (Page 18-20) shows the mixing effect of our method with other related techniques. \n\nAdditionally, we have further investigated the effectiveness of our RST on SAM variants. Appendix B.5 shows the corresponding results of our RST on ASAM and GSAM.\n\nWe thank the reviewer for their review time.\n\ncomment: `Q: The novelty of this paper is my only concern. The idea of applying SAM to a subset of parameters or iterations has been explored in several papers [Mi et al. 2022, Liu et al. 2022, Du et al. 2022]. For example, Liu et al. 2022 propose to only periodically calculate the inner gradient ascent across the training iterations; Liu et al. 2022 and Du et al. 2022 propose to select a subset of parameters to calculate the inner gradients in each iterations. Over expectations, randomly selecting a subset of parameters in iterations is same as randomly select interactions. And the later can be considered as a special case of the former --- alternatively masking out all parameters.`\n\n\n\nWe would like to thank the reviewer for the valuable comment. Firstly, we would like to discuss the difference between the weight masking method and our randomized policy. We understand the reviewer's point in regards to the mentioned \"expectation\" intuition. However, the two strategies have very different effect in terms of the computational efficiency. The reviewer could find that in ESAM paper, the ESAM's authors have explained why the improvement of computation efficiency is limited when using such masking strategy. In a word, due to the chain rule in BP, computing the gradients of weights in shallower layers requires computing the gradients in deeper layers. Therefore, gradients of many masked-out weights still need to be computed despite the fact that they have been masked out. Further, the reviewer could also find that the empirical results in ESAM confirm this demonstrations, where the computational efficiency is improved by only 10\\% even the majority of the weights are not selected. Meanwhile, Sparse SAM follows the very similar idea of masking weights as ESAM. Although the Sparse SAM's authors show that their method theoretically requires much less FLOPs, yet still due to the impact of the chain rule in BP in practice, the actual training wall time is pretty close to the standard SAM training, even if we have set a very high sparse rate in Sparse SAM. The reviewer could give it a try https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization, and we also have uploaded the corresponding training logs in our repo for your reference. Besides, from the perspective of practical implementations, randomly selecting to records the gradients is not quite friendly to current deep learning frameworks because we may frequently alter the computational graph. For JAX, this may probability cause a re-complication in auto-differential framework, which could instead largely burden the computational efficiency. As for our method, we do not need to worry about all these concerns. Besides, we have also provided a comprehensive analysis to show how and why our method works.\n\nNext, more importantly, we would like to show that our RST does not conflict with these efficient methods, which means that we do not have to compare with these corresponding efficient techniques. We could easily notice that when selecting performing SAM algorithm in RST, we would not impose any operation on SAM algorithm. In fact, we could adopt these efficient methods when selecting SAM algorithm to further improve the computational efficiency of performing SAM algorithm. The following table only shows the results of ResNet18. And for detailed results, the reviewer could check the appendix B.6 in the newly uploaded version, including the results regarding VGG16BN and WideResNet28-10.\n\nLookSAM\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|15.6|4.48|20.79|\n|LookSAM|+5.6|4.06|20.30|\n|LookSAM + G-RST[50%]|+3.0|4.18|20.44|\n|LookSAM + G-RST[75%]|+4.4|3.94|20.11|\n\nESAM\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|ESAM|+18.6|4.05|20.28|\n|ESAM + G-RST[50%]|+9.5|4.41|20.72|\n|ESAM + G-RST[75%]|+14.0|4.08|20.21|\n\nSSAM\n|Method|Time|C-10 Error|C-100 Error|\n|--|:--:|:--:|:--:|\n|SGD|24.4|4.66|20.98|\n|SSAM|+21.0|3.89|20.17|\n|SSAM + G-RST[50%]|+10.7|4.03|20.41|\n|SSAM + G-RST[75%]|+15.8|3.83|20.19|\n\nIt should be pointed that here, ESAM and SSAM are both implemented on the git repository https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization, where one A100 GPU is used.As we could see in the table, for all these efficient techniques, our RST could naturally improve the computational efficiency further since all these methods could have the choice to selecting performing base learning algorithm. However, if the selecting probability in RST is relatively low (0.5 in the table), it may harm the mixing effect, where the models may not be well trained. On the other hand, as properly raising the selecting probability (0.75 in the table), it is possible to acquire comparable results with these efficient techniques.\n\nBesides, we have also added extra results of our RST on SAM variants like ASAM and GSAM at Appendix B.5. The reviewer could check if interested. We thank the reviewer for the valuable time and comments.\n"
            }
        ]
    }
]