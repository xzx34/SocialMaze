{
  "summary": {
    "model": "gemma-2-27B",
    "total_papers": 100,
    "correct_predictions": 75,
    "accuracy": 75.0,
    "confusion_matrix": [
      [
        70,
        6
      ],
      [
        19,
        5
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 92.10526315789474,
        "precision": 78.65168539325843
      },
      "Accept": {
        "total": 24,
        "recall": 20.833333333333336,
        "precision": 45.45454545454545
      }
    }
  },
  "results": [
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – adapting the computational path for each domain in unsupervised domain adaptation. The motivation is sound: different domains may require different levels of processing to achieve a common feature representation. The core concept of a multi-branch network with learnable weights for each branch (flow) is novel in its application to domain adaptation, even if adaptive computation itself isn’t entirely new. The writing is clear and the paper is well-structured.\n\nHowever, the reviewers raise significant concerns, and I largely agree with them. The biggest weakness is the insufficient experimental validation. The comparison to state-of-the-art UDA methods is lacking. Comparing against DANN and RPT is insufficient; methods like CDAN, which focus on distribution alignment, *must* be included to establish whether the proposed adaptive parameter sharing offers a genuine advantage. Reviewer 3 rightly points out that the proposed method addresses a different aspect of UDA than many existing approaches, making a direct comparison crucial.\n\nFurthermore, the ablation studies are minimal. The choice of K (number of flows) is not justified, and the potential benefits of more than two flows are unexplored. The lack of investigation into different grouping configurations of computational units is also a significant omission. The reviewers also correctly question the behavior observed in Figure 4, where parameter sharing appears absent, and the authors need to address this counterintuitive result.\n\nReviewer 2’s point about the intuition of varying computational complexity not being fully realized (all domains having the same number of layers) is also valid. While not a fatal flaw, it weakens the argument for the method’s flexibility.\n\nThe minor comments from Reviewer 1 regarding naming conventions, references, and code availability are valid but addressable with minor revisions. The fact that Reviewer 2 is satisfied after the discussion period is a positive sign, indicating that some initial concerns were addressed.\n\nOverall, the paper presents a promising idea, but the lack of comprehensive experimentation and analysis prevents it from reaching the standard expected of a top-tier conference. The novelty is somewhat diminished by the lack of comparison to current state-of-the-art methods, and the potential of the approach is not fully explored. While the reviewers acknowledge the potential, the current presentation is not strong enough to warrant acceptance. More rigorous experimentation and a more thorough analysis are needed to demonstrate the method’s effectiveness and justify its contribution to the field.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper is, unfortunately, not suitable for publication at a top-tier conference. The reviewer comments are overwhelmingly negative and consistently point to fundamental flaws in the paper's presentation, clarity, and academic rigor. Let's break down the issues:\n\n* **Writing Quality:** All three reviewers highlight the extremely poor writing quality. Reviewers 2 and 3 explicitly state they struggled to understand the paper's core ideas, and Reviewer 2 even admits rarely rejecting papers solely on writing, but makes an exception here. This is a critical issue; even brilliant ideas are useless if they cannot be communicated effectively to the research community. The grammatical errors and incomprehensible sentences render the paper inaccessible.\n* **Lack of Related Work:** The complete absence of citations (as noted by Reviewer 1 and 3) is a major red flag. Any research paper, especially in a well-established field like reinforcement learning and MCTS, *must* situate itself within the existing literature. Ignoring prior work suggests a lack of understanding of the field or a deliberate attempt to avoid comparison with existing methods.\n* **Lack of Clarity & Justification:** Reviewer 2 points out inaccurate and unjustified statements, and the abstract is unintelligible. Reviewer 3 notes undefined mathematical symbols and a general lack of clarity regarding the paper's intent. This suggests the authors haven't clearly formulated their contribution or haven't adequately explained it.\n* **Presentation Issues:** Reviewer 3 mentions padding (blank space filling pages) and poorly formatted algorithm boxes. While presentation isn't the primary concern, it contributes to the overall impression of a hastily prepared and unprofessional submission.\n* **Conceptual Clarity:** Even attempting to decipher the core idea is difficult. The concept of using MCTS to control episode generation *could* be interesting, but the paper fails to articulate it in a coherent and convincing manner.\n\nThe reviewers' consensus is clear: this paper is not ready for publication in *any* venue, let alone a top-tier conference. The issues are not minor stylistic concerns; they are fundamental problems with the paper's writing, content, and presentation. While the authors might have a potentially interesting idea, it is currently buried under layers of poor execution. A complete rewrite, with significant attention to clarity, rigor, and proper citation of related work, would be necessary before resubmission.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to provide theoretical guarantees for the convergence of SGD in training WGANs, specifically focusing on the case of a one-layer generator. While the theoretical analysis appears sound and complete, as noted by Reviewer 2, the core issue lies in the extreme simplification of the problem setting. All three reviewers converge on this point: the use of linear and quadratic discriminators significantly deviates from the spirit and practical application of WGANs.\n\nReviewer 1 is particularly critical, highlighting the disconnect between the analysis and actual WGAN implementations. The reviewer questions the validity of treating the quadratic discriminator as a true WGAN discriminator and points out the lack of explanation regarding the Lipschitz constraint. The author responses, according to Reviewer 1, failed to address these concerns adequately.\n\nReviewer 2, while initially more positive, ultimately agrees that the simplified discriminator is a weakness. They acknowledge the contribution of analyzing gradient descent-ascent but argue that results with standard neural network discriminators would be more impactful.\n\nReviewer 3 echoes these concerns, stating that learning joint distributions of only two coordinates is significantly weaker than recovering the full data distribution and that the analysis is unlikely to generalize to more complex discriminators due to error propagation.\n\nThe authors' justification for using these simplified discriminators – that they are designed in tandem with the generator and that more complex discriminators would worsen sample complexity – is not entirely convincing. While sample complexity is important, the resulting model is so far removed from practical WGANs that the theoretical guarantees have limited real-world relevance. The claim that SGD learns \"one-layer networks in WGANs\" is misleading, as the \"WGAN\" component is substantially altered by the choice of discriminator. The analysis essentially becomes a study of optimization in a highly constrained, simplified setting, rather than a genuine insight into WGAN training dynamics.\n\nThe paper's contribution, therefore, is limited to a specific, artificial scenario. It doesn't offer substantial insights into why WGANs work in practice with the more complex neural network discriminators they typically employ. The lack of a revision addressing Reviewer 1's concerns further weakens the submission. A top-tier conference prioritizes work with broader impact and relevance to the field, and this paper falls short on both counts.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles a relevant and challenging problem – coordinating multiple agents for complex manipulation tasks. The core idea of learning diverse, adaptable skills and then coordinating them via a meta-policy is sound and aligns with how humans approach similar problems. The empirical results demonstrate the method's effectiveness, outperforming the provided baseline in several environments. The inclusion of a website with videos and code is a significant positive.\n\nHowever, the paper isn't without its weaknesses, as highlighted by the reviewers. Let's break down the concerns:\n\n**Reviewer 1** raises valid points about notation, the potentially primitive-action-like behavior due to `T_low = 1`, high variance in training curves, and the need for more in-depth discussion of recent multi-agent methods. The authors addressed these concerns in their response (not provided, but implied by the reviewer's willingness to increase their score). The variance issue is particularly concerning for a top-tier conference; robust results are crucial.\n\n**Reviewer 2** poses fundamental questions about the necessity of treating temporal abstraction differently in multi-agent settings and the justification for using DIAYN specifically. These are insightful questions that challenge the core motivation of the work. The authors need to convincingly articulate *why* their approach is necessary and novel compared to existing single-agent temporal abstraction methods and other multi-agent skill discovery techniques. The questions regarding the continuous latent variable and the fixed `T_low` are also legitimate and require clear answers.\n\n**Reviewer 3** identifies a critical limitation: the reliance on pre-defined subtasks. This significantly restricts the method's generality. The reviewer's suggestion to explore DIAYN alone as a baseline is excellent and would provide valuable insight into the contribution of the pre-defined subtasks. The missing information about the latent skill embedding size is a minor but important detail.\n\nConsidering these points, the paper demonstrates a promising approach but falls short of the standards expected for a top-tier conference in its current form. The core novelty isn't entirely convincing, and the limitations regarding pre-defined subtasks and the need for more robust results (lower variance) are significant. While the authors seem responsive to feedback, the fundamental questions raised by Reviewer 2 need to be thoroughly addressed to establish the theoretical grounding and necessity of the proposed method. The addition of the DIAYN-only baseline suggested by Reviewer 3 is crucial. Without these improvements, the paper feels more like a proof-of-concept than a substantial contribution to the field.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Reasoning\n\nThis paper presents a novel and intriguing connection between the expressivity of ReLU networks and Sharkovsky's Theorem from dynamical systems. The core idea – leveraging the properties of periodic points and chaos theory to understand depth-width trade-offs – is genuinely creative and addresses an open question raised by Telgarsky. The reviewers are overwhelmingly positive.\n\nReviewer 2 is particularly enthusiastic, stating the paper is a \"delight to read\" and \"cleanly answers the open question\" posed by Telgarsky. They commend the clear exposition and correctness of the proofs. Reviewer 1, while already at the maximum score, acknowledges the thoroughness of the author's response and the inclusion of a synthetic dataset. Reviewer 3, while suggesting a restructuring of the paper for better flow, also recognizes the clever application of Sharkovsky's theorem and the paper's contribution to understanding the benefits of deep networks.\n\nThe reviewers raise valid points for improvement. Reviewer 2 asks about the impact of bias terms and requests intuition for the sharpness of the lower bound, potentially through empirical validation. Reviewer 3 suggests restructuring the paper to prioritize the main result and questions the practical applicability of the period-dependent depth lower bound, as well as requesting more intuition for definitions.\n\nHowever, these are largely constructive criticisms and do not fundamentally undermine the paper's contribution. The theoretical result itself is strong, and the reviewers agree on its correctness. The questions raised by reviewers 2 and 3 are reasonable avenues for future work, but not prerequisites for publication in a top-tier conference, especially given the novelty and theoretical significance of the findings. The author's response to the reviewers (mentioned by Reviewer 1) suggests they are engaged and receptive to feedback. The inclusion of a synthetic dataset, as noted by Reviewer 1, is a step towards addressing the need for empirical contextualization.\n\nThe paper's strength lies in its theoretical contribution and the innovative application of a mathematical tool (Sharkovsky's Theorem) to a central problem in deep learning. While practical applicability is a concern raised by Reviewer 3, many theoretical papers in top-tier conferences lay the groundwork for future applied research. The paper clearly articulates its limitations and focuses on a specific class of functions, which is a reasonable scope for a theoretical investigation.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles a genuinely novel and challenging problem: singing voice generation without scores or lyrics. The authors explore three interesting settings – free singer, accompanied singer, and solo singer – and propose a pipeline involving data preparation, GANs, and custom metrics. However, despite the ambition and effort, the paper falls short of the standards expected for a top-tier conference, primarily due to significant weaknesses in evaluation and motivation.\n\nThe reviewers consistently highlight the lack of robust evaluation. The proposed metrics – vocalness and average pitch – are deemed insufficient to capture the nuances of *singing*. They don't adequately assess harmonic consistency, melodic quality, or rhythmic accuracy, all crucial aspects of singing. The absence of a baseline comparison is a major flaw. As Reviewer 2 points out, even a relatively simple baseline using Automatic Music Transcription (AMT) followed by a score-to-singing voice model would provide valuable context. The MOS study in Table 2 suffers from the same issue – lacking a reference point for judging success. The audio samples, while demonstrating some vocal-like qualities, are not convincingly evaluated.\n\nBeyond evaluation, the motivation for the three task settings is weak. The reviewers question why these specific settings are interesting and what insights can be gained from comparing their performance. The paper doesn't clearly articulate the potential applications or benefits of unconditional singing voice generation.\n\nSeveral reviewers also point out issues with the experimental setup and data. The description of the training data is vague, making it difficult to assess the validity of the results. Concerns are raised about the source separation process and the potential for introducing artifacts. The reviewers also identify minor technical inaccuracies and areas where the literature review could be improved (e.g., the omission of MelNet, misinterpretation of BEGAN, and inappropriate use of CREPE).\n\nWhile the authors have made an effort to address reviewer comments, the fundamental issues regarding evaluation and motivation remain unresolved. The paper presents an interesting exploration of a new problem, but it doesn't provide sufficient evidence to demonstrate the effectiveness of the proposed approach or its significance to the field. The lack of comparison to existing methods, combined with the weak evaluation metrics, makes it difficult to assess the quality of the generated singing voices. The paper feels more like a preliminary investigation than a polished, impactful contribution suitable for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting approach to improving disentanglement in VAEs by combining progressive learning with hierarchical representation learning via a Variational Ladder Autoencoder (VLAE). The core idea – starting with abstract representations and progressively adding layers to capture finer details – is intuitively appealing and aligns with principles observed in biological systems. The authors also introduce a new disentanglement metric (MIG-sup), though its practicality is questioned by reviewers.\n\nThe initial reviews were quite critical. Reviewer 2, in particular, raised significant concerns about the clarity of the paper’s purpose, the convincingness of the qualitative results, the novelty of the proposed metric, and the lack of ablation studies. Reviewer 1 focused on the need for more detailed analysis of information flow within the layers, particularly in comparison to VLAE. Reviewer 4 requested ablation studies on the \"fade-out\" mechanism.\n\nHowever, the authors’ rebuttal and subsequent revisions appear to have addressed many of these concerns. Reviewer 2, after the rebuttal, significantly upgraded their score to a “Weak Accept,” citing improved quantitative and qualitative results, and demonstrating that the first layer learns the intended representations. Reviewer 1’s concerns about information flow were also addressed with new experiments. Reviewer 3 was already positive, praising the novelty and thoroughness of the experiments. Reviewer 4’s suggestions for ablation studies, while not explicitly stated as being fully addressed, are less critical given the overall positive shift in sentiment.\n\nThe remaining weaknesses are relatively minor. The practicality of MIG-sup remains questionable, but the authors acknowledge its limitations. The connection between hierarchical representation and disentanglement, while not fully elucidated, is supported by the experimental results. The paper still leans heavily on disentanglement metrics for evaluation, which, as noted by reviewers, can be sensitive and potentially misleading. A more thorough discussion of the limitations of these metrics would be beneficial.\n\nDespite these minor shortcomings, the paper presents a novel and well-executed approach. The combination of progressive learning and hierarchical VAEs is a valuable contribution, and the experimental results, particularly after the rebuttal, are convincing. The paper demonstrates a clear improvement over the baseline VLAE, and the authors have responded effectively to the reviewers’ concerns. While not a groundbreaking advance, it represents a solid contribution to the field and is worthy of publication in a top-tier conference. The \"Weak Accept\" from Reviewer 2 is a strong indicator, and the overall consensus leans towards acceptance.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in extreme classification – the computational cost of softmax. The proposed adversarial sampling method is a reasonable approach to improve the signal-to-noise ratio in negative sampling, and the theoretical justification (minimizing gradient variance) is a strong point. Reviewer 1 correctly identifies the paper as well-written and educative, even for those not deeply familiar with the field. The claimed speedup is also significant.\n\nHowever, the reviewers raise several valid concerns that, taken together, significantly weaken the paper's case for acceptance at a *top-tier* conference.\n\nReviewer 2 points out a critical omission: a lack of thorough comparison with related work. While the authors acknowledge negative sampling, they don't adequately contextualize their approach within the existing landscape of \"hard negative\" sampling techniques, specifically mentioning several recent papers ([1-4]) that offer similar logarithmic complexity solutions. This omission is a major flaw.\n\nReviewer 3’s critique is even more damning. The experimental evaluation is incomplete and unconvincing. The lack of comparison to state-of-the-art methods like Slice [1] and DiSMEC [2] is a serious oversight. The reviewer rightly questions the choice of baselines (OVE being suboptimal) and highlights the importance of understanding the approximation error introduced by negative sampling (comparing to a full softmax, even on smaller datasets). Furthermore, the reviewer raises a crucial point about the fat-tailed distribution of labels in extreme classification and the need to evaluate performance on tail labels using appropriate metrics. The limited dataset selection also weakens the conclusions. The lack of consideration for multi-label extensions is another missed opportunity.\n\nThe authors' response to these criticisms will be crucial. However, even assuming a strong rebuttal addressing the missing baselines and contextualization of related work, the limited experimental scope remains a concern. A top-tier conference expects a comprehensive evaluation, and this paper falls short. The minor typos mentioned by Reviewer 2 are also indicative of a lack of thoroughness.\n\nWhile the core idea is sound and the theoretical contribution is valuable, the paper's weaknesses in experimental validation and related work analysis are too significant to ignore. The paper feels incomplete and doesn't demonstrate a clear advantage over existing methods in a rigorous manner. It reads more like a preliminary study with promising results than a fully-fledged contribution ready for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper is, unfortunately, not suitable for publication at a top-tier conference. The reviewer comments are overwhelmingly negative and consistently point to fundamental flaws in the paper's presentation, clarity, and academic rigor. Let's break down the issues:\n\n* **Writing Quality:** All three reviewers highlight the extremely poor writing quality. Reviewers 2 and 3 explicitly state they struggled to understand the paper's core ideas, and Reviewer 2 even admits rarely rejecting papers solely on writing but makes an exception here. This is a critical issue; a paper must be clearly communicated to be evaluated on its merits. The abstract is described as incomprehensible.\n* **Lack of Related Work:** The complete absence of citations (as noted by Reviewer 1 and 3) is a major red flag. Any research paper, especially in a well-established field like reinforcement learning and MCTS, *must* situate itself within the existing literature. Ignoring prior work suggests a lack of awareness of the field or a deliberate attempt to avoid comparison.\n* **Lack of Clarity & Justification:** Reviewer 2 points out \"inaccurate and unjustified statements,\" and Reviewer 3 notes undefined mathematical symbols and a general lack of decipherable intent. This suggests the authors haven't adequately explained their methodology or provided sufficient justification for their approach.\n* **Presentation Issues:** Reviewer 3 mentions formatting problems (blank space, difficult-to-follow algorithm boxes) contributing to the overall poor impression. While presentation isn't the primary concern, it exacerbates the issues with clarity and readability.\n* **Scope & Novelty:** While the abstract hints at an interesting idea – using MCTS to coordinate episode generation – the reviewers' inability to understand the paper makes it impossible to assess the novelty or significance of the proposed approach. The description of applying this to Tic-Tac-Toe, while a valid testbed, doesn't inherently suggest a breakthrough.\n\nThe reviewers' consensus is clear: this paper is not ready for publication in *any* venue, let alone a top-tier conference. The issues are not minor stylistic concerns; they are fundamental problems with clarity, rigor, and presentation. The suggestion from Reviewer 1 to resubmit after significant revisions (starting with related work and references) is appropriate, but the current version falls far short of acceptable standards.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and challenging problem – modeling continuous time-evolving graphs. The core idea of combining temporal point processes with deep learning, specifically LSTMs, is promising and aligns well with current research trends. The authors attempt to address several key aspects of this problem, including multi-resolution modeling and selective neighbor aggregation. However, despite the potential, the paper suffers from significant weaknesses that, in my assessment, prevent it from reaching the standard required for a top-tier conference like ICLR.\n\nThe primary issue is a lack of novelty and insufficient justification for the proposed contributions. As highlighted by both Reviewer 1 and Reviewer 2, the individual components – LSTM with time gates, stacked LSTMs, and attention mechanisms – are not novel in themselves. The combination is presented as the main contribution, but the paper fails to convincingly demonstrate *why* this specific combination is effective or superior to existing approaches. The analysis of these components is superficial. The authors mention capturing multiple resolutions with stacked LSTMs but provide no empirical evidence or analysis to support this claim. Similarly, the attention mechanism's benefits are asserted rather than demonstrated.\n\nThe ablation study, as pointed out by Reviewer 1, is inadequate. Simply switching components on and off doesn't provide meaningful insights into their individual contributions or interactions. A more granular ablation, exploring different parameter settings (K values, scaling parameters, alpha) and architectural variations (single-layer LSTM, residual connections) is crucial.\n\nThe comparison to baselines is also problematic. The omission of JODIE (as flagged by Reviewer 1) is a significant oversight, given its relevance to the problem. Furthermore, the justification for excluding new nodes in the interaction prediction task is unconvincing. The lack of comparison with temporal baselines for the classification task is also a weakness.\n\nThe technical details also raise concerns. Reviewer 2 correctly points out a potential error in the log-likelihood function. Reviewer 3 raises valid questions about the handling of graph growth/shrinkage and the computational cost of the proposed method. These issues suggest a lack of thoroughness in the technical validation of the approach.\n\nThe writing and presentation could also be improved. The notation is inconsistent, and the paper lacks a clear overview diagram to illustrate the complex interactions between the different components. The discussion of results is descriptive rather than analytical, failing to provide insights into *why* the proposed method performs well.\n\n\n\nIn summary, while the paper addresses an important problem and presents a potentially interesting approach, the lack of novelty, insufficient justification, inadequate ablation study, missing baseline comparisons, and technical concerns outweigh its strengths. The paper feels more like an engineering effort combining existing techniques rather than a significant research contribution.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – out-of-distribution (OOD) detection – under a challenging constraint: no access to OOD data for validation and no model retraining. The core idea of leveraging normalized compression distance on feature maps is interesting and potentially valuable. However, the reviewers raise significant concerns that, taken together, lead me to believe the paper is not currently ready for publication in a top-tier conference.\n\nReviewer 1 provides the most substantial critique, focusing on the fairness of the comparison with prior work. The inability to replicate the performance of the Mahalanobis detector, coupled with the fact that adversarial samples *can* be used for validation within the constraints of that method, casts doubt on the validity of the comparison. The authors’ response doesn’t fully address this, and simply stating a failure to replicate isn’t sufficient; they need to present both their replication results *and* the original reported numbers. The reviewer rightly points out that any performance gain needs to be demonstrated under equivalent conditions. The fact that all new results are relegated to the appendix is also a red flag, suggesting the authors themselves may not be fully confident in their presentation.\n\nReviewer 2 highlights a crucial weakness: the justification for the strict constraints. The paper doesn't adequately explain *why* these constraints are necessary or common in real-world scenarios. Without a compelling justification, the work feels somewhat contrived. The reviewer also points out grammatical issues, indicating a lack of polish.\n\nReviewer 3 raises a valid point about the limited complexity of the experimental setting. While the paper demonstrates performance on standard benchmarks, it doesn't explore more realistic and challenging scenarios, such as those found in safety-critical applications. The lack of theoretical guarantees is also noted, which is a common expectation for papers in this field.\n\nThe authors’ responses to the reviewers are generally weak. They acknowledge the issues raised but don’t offer convincing solutions or demonstrate significant improvements. The post-rebuttal statements from both Reviewer 1 and 2 indicate that their initial assessments haven’t changed, which is a strong signal.\n\nWhile the idea is novel, the flawed comparison, lack of justification for the constraints, limited experimental scope, and weak rebuttal collectively undermine the paper’s contribution. A top-tier conference demands rigorous methodology, clear justification, and compelling results, none of which are fully present here. The paper has potential, but requires substantial revisions and further experimentation to address the reviewers’ concerns.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and open question in deep learning theory: understanding the impact of activation functions on the training dynamics of overparameterized neural networks. The authors provide a theoretical analysis focusing on the eigenvalues of the “G-matrix” (related to the Neural Tangent Kernel), linking these eigenvalues to the convergence rate of training. The core contribution lies in differentiating between non-smooth and smooth activation functions, providing conditions under which each performs well or poorly.\n\nThe strengths of the paper are significant. The analysis is generally sound, covering a broad range of activation functions. The separation of non-smooth and smooth cases is a valuable insight. The authors acknowledge and discuss discrepancies between theory and experiments (specifically on CIFAR10), which demonstrates intellectual honesty. The inclusion of extensive proofs in the appendix, while lengthy, suggests a rigorous approach. Reviewer 2 correctly identifies the thoroughness of the theoretical study and the successful case made for the differences between activation types.\n\nHowever, the paper isn’t without weaknesses. The length and organization are problematic, as both reviewers point out. A long appendix, while containing valuable details, can obscure the main contributions and make the paper less accessible. The reviewer comments highlight specific areas where clarity could be improved (e.g., the meaning of \"DZXP setting,\" the explanation of the M matrix, and awkward phrasing). Reviewer 1’s point about the unclear extensions in the appendix is valid; a clearer roadmap of the appendix content would be beneficial. The gap between theory and experiment on CIFAR10, while acknowledged, could be explored further – is it a limitation of the theoretical model, or a characteristic of real-world data?\n\nThe reviewers’ opinions are also informative. Reviewer 1 provides detailed, constructive criticism, focusing on clarity and organization. Reviewer 2 is strongly in favor of acceptance, praising the theoretical rigor and overall contribution. The suggestions from both reviewers are reasonable and addressable.\n\nConsidering the strengths and weaknesses, and the positive assessment from Reviewer 2, I believe the paper has the potential to be a valuable contribution to the field. The core theoretical results are solid, and the identified weaknesses are primarily related to presentation and organization, rather than fundamental flaws in the research. Addressing the reviewers’ comments – streamlining the paper, clarifying explanations, and better organizing the appendix – would significantly enhance its impact. While the paper might not be *perfect* as is, it meets the threshold for acceptance at a top-tier conference, particularly given the challenging nature of the problem and the theoretical depth of the analysis. The authors have identified a key aspect of neural network training and provided a rigorous, albeit complex, analysis.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to address the problem of training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. While the problem is relevant, and the authors provide a TensorFlow implementation, the paper suffers from several significant weaknesses that, in my assessment, preclude its acceptance at a top-tier conference like ICLR.\n\nThe core issue is that the contributions feel incremental and lack sufficient novelty to justify publication. Reviewer 1 rightly points out the authors’ apparent lack of awareness of existing work in online EM, and the proposed approach feels “ad-hoc” in comparison. The “novel” ideas – the max-component approximation, the regularizer, and the subspace approach – are either well-known techniques (log-sum-exp trick, natural parameterization of multinoulli) or lack strong justification and analysis.\n\nSpecifically:\n\n* **Max-Component Approximation:** Reviewer 3 correctly notes that a standard, numerically stable approach to computing the log-sum-exp exists, diminishing the novelty of the proposed max-component method. The authors don't convincingly demonstrate why their approach is superior.\n* **Regularizer:** While the regularizer appears to be the most substantial contribution, its justification is weak, and the grid-based smoothing feels arbitrary. The lack of theoretical analysis regarding its properties is concerning.\n* **Subspace Approach:** The computational cost of recomputing the QR decomposition for the subspace is a valid concern raised by Reviewer 1, and the authors don’t adequately address it.\n* **Experimental Evaluation:** The experimental evaluation is a major weakness. The datasets used (28x28 images) are not truly “high-dimensional” as claimed in the abstract. The results are not compelling, and the comparisons to EM are incomplete (missing runtime comparisons, unclear log-likelihood definitions, and repeated text). The image selection criteria are also questionable.\n* **Theoretical Guarantees:** The lack of any convergence guarantees or performance bounds, as highlighted by Reviewer 2, is a significant drawback, especially when compared to more recent methods based on moment matching.\n* **Minor Issues:** Numerous minor issues were raised by all reviewers, including incorrect statements, unclear definitions, and mathematical errors (e.g., the expression for τ). These suggest a lack of thoroughness in the paper's preparation.\n\nThe author responses (not provided, but inferred from the reviewers' comments) don't seem to adequately address these concerns. The authors appear to defend their choices without providing strong justifications or acknowledging the limitations of their work.\n\nWhile the paper is well-written and the authors demonstrate a good understanding of the underlying concepts, it doesn't present a significant advance over existing methods. The incremental nature of the contributions, combined with the weak experimental evaluation and lack of theoretical analysis, make it unsuitable for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles a relevant and challenging problem – coordinating multiple agents for complex manipulation tasks. The core idea of learning diverse, adaptable skills and then coordinating them via a meta-policy is sound and aligns with how humans approach similar problems. The empirical results demonstrate the method’s effectiveness, outperforming the provided baseline in several environments. The inclusion of a website with videos and code is a significant positive.\n\nHowever, the paper isn’t without its weaknesses, as highlighted by the reviewers. Let's break down the concerns:\n\n**Reviewer 1** raises valid points about notation, the potentially primitive-action-like nature of skill selection (T_low = 1), high variance in training curves, and the presentation of results (success rate vs. episode reward). The authors addressed these concerns in their response (not provided here, but implied by the reviewer's willingness to increase their score). Addressing these would significantly strengthen the paper.\n\n**Reviewer 2** poses fundamental questions about the necessity of treating temporal abstraction differently in multi-agent settings and the justification for using DIAYN specifically. These are *critical* questions that go to the heart of the paper’s contribution. The authors need to clearly articulate why their approach is novel and necessary compared to existing single-agent temporal abstraction methods and other multi-agent skill discovery techniques. The questions about approximating p(z|s) and the fixed T_low are also important technical points that require thorough explanation.\n\n**Reviewer 3** identifies a key limitation: the reliance on pre-defined subtasks. This significantly restricts the method’s generality. The reviewer’s suggestion to explore DIAYN alone or with a larger latent space is a valuable one. The missing information about the latent skill embedding size is a minor but important detail.\n\nConsidering the reviewers’ comments and the implied author responses, the paper has potential but falls short of the standards expected for a top-tier conference. While the empirical results are promising, the theoretical justification and discussion of related work need substantial improvement. The limitation of requiring pre-defined subtasks is a significant drawback. The authors need to more convincingly demonstrate the novelty and advantages of their approach over existing methods, and address the concerns about the fixed T_low and the approximation of p(z|s). The variance in training curves also needs to be addressed with more robust statistical analysis.\n\nThe paper is not currently at a level where it would significantly advance the field. It feels more like a proof-of-concept than a fully-fledged contribution. A rejection with encouragement to address the reviewers’ concerns and resubmit would be the most appropriate course of action.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in continual learning – catastrophic forgetting – with a novel approach inspired by Hebbian learning and the complementary learning systems (CLS) theory. The core idea of differentiable Hebbian consolidation, using fast and slow weights, is conceptually sound and well-motivated. The authors clearly articulate the connection to neurobiological principles, which is a strength. The implementation appears straightforward and computationally efficient, and the authors promise code release, which is positive.\n\nHowever, the reviewers raise significant concerns regarding the empirical evaluation and the overall impact of the proposed method. Let's break down the issues:\n\n* **Weak Empirical Performance:** The most consistent criticism is the method's underwhelming performance, particularly when used in isolation. Reviewers 1 and 4 explicitly state that DHP Softmax doesn’t perform well on its own and often lags behind simpler baselines like fine-tuning or synaptic intelligence (SI). While the combination with synaptic consolidation methods yields better results, it's unclear if the improvement is attributable to DHP Softmax or simply the synaptic consolidation itself.\n* **Insufficient Comparison to Related Work:** Reviewer 3 points out the lack of comparison to other CLS-based approaches. The authors dismiss these approaches as focusing on simpler tasks, but this justification feels weak without empirical evidence. A direct comparison is crucial to establish the novelty and advantage of their method.\n* **Lack of Scalability Demonstration:** Reviewer 4 rightly questions the scalability of the method, suggesting evaluation on larger datasets like CIFAR-100 or CORe50. The current experiments are limited to MNIST variants and a small vision dataset, which doesn't convincingly demonstrate the method's applicability to more complex, real-world scenarios.\n* **Clarity and Explanation:** Reviewer 2 identifies issues with the clarity of the method's description, specifically regarding the mathematical notation and the motivation for applying the Hebbian strategy only to the softmax layer. These clarity issues hinder understanding and evaluation.\n* **Imbalanced Dataset:** While the introduction of an imbalanced Permuted MNIST is a positive contribution, it doesn't fully address the broader concerns about the method's generalizability.\n\nThe author responses (not provided, but implied to be limited by time) haven't sufficiently addressed these concerns. The reviewers are not convinced that the proposed method offers a significant advancement over existing techniques. The paper feels more like a proof-of-concept than a substantial contribution to the field. While the idea is interesting, the empirical evidence is not strong enough to warrant acceptance at a top-tier conference. The paper needs more rigorous evaluation, clearer explanations, and a more compelling demonstration of its advantages over existing methods.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an incremental improvement to the Defense-GAN framework for adversarial defense, focusing on reducing computational cost through a more efficient latent space search. While the idea is sound and the reviewers acknowledge a performance benefit in terms of speed, the paper suffers from several significant weaknesses that, in my assessment, preclude acceptance at a top-tier conference.\n\nHere's a breakdown of the issues, drawing from the reviewer comments:\n\n**Strengths:**\n\n* **Computational Efficiency:** The primary contribution – reducing the computational cost of Defense-GAN – is valuable. Reviewers 1 and 3 confirm this benefit.\n* **Comparable Performance:** The method achieves comparable, and sometimes better, performance than Defense-GAN, particularly given the reduced computational burden.\n\n**Weaknesses:**\n\n* **Incremental Contribution:** Reviewer 2 rightly points out that the contribution is too incremental. It's a refinement of an existing method, not a fundamentally new approach. Top-tier conferences prioritize novelty.\n* **Lack of Strong Baselines:** The comparison lacks crucial baselines. Reviewers 2 and 4 both highlight the absence of state-of-the-art adversarial training methods like PGD and TRADES. Without these comparisons, it's difficult to assess the true effectiveness of the proposed method.\n* **Limited Dataset Evaluation:** The experiments are limited to MNIST and Fashion-MNIST. Reviewers 3 and 4 emphasize the need for evaluation on more complex datasets like CIFAR-10 to demonstrate generalizability and robustness. The reliance on simple datasets raises concerns about whether the method will scale to real-world scenarios.\n* **Writing Quality & Clarity:** Reviewers 2 and 4 consistently criticize the writing quality. The paper is described as \"hastily written,\" \"hard to follow,\" and containing numerous typos. Clarity is paramount in academic publications.\n* **Theoretical Concerns:** Reviewer 3 raises valid theoretical concerns about the reliance on the autoencoder's detection ability and the potential for susceptibility to white-box attacks. The lack of assurance regarding the similarity of latent distributions is also a concern.\n* **Unclear Implementation Details:** Reviewer 4 questions the practical implementation of the encoder-assisted search process, highlighting a lack of clarity in the description.\n* **Missing Analysis:** Reviewer 2 points out the lack of analysis regarding false alarms in the detector (Fig. 4).\n\n\n\nThe author responses (not provided, but implicitly considered in the reviewers' final assessments) don't appear to have adequately addressed these concerns. The core issue is that the paper presents a relatively minor improvement to an existing technique without demonstrating its superiority over established, stronger baselines on challenging datasets. The writing quality further detracts from the paper's potential. While the computational efficiency gain is positive, it's not sufficient to overcome the other significant shortcomings.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper explores the application of recurrent neural networks, specifically bidirectional LSTMs, to predict DNA folding patterns using epigenetic data. The core idea – leveraging sequential information in the genome for predicting 3D structure – is reasonable and aligns with current trends in computational biology. The authors demonstrate that their LSTM model outperforms several baseline methods on a *Drosophila melanogaster* Hi-C dataset. However, despite this positive result, the paper suffers from several significant weaknesses that, in my assessment, preclude its acceptance at a top-tier conference.\n\nReviewer 3’s assessment is particularly strong and accurately identifies the core issues: lack of novelty, unclear methodology, and insufficient evaluation. The paper *is* similar to existing work, and the authors fail to adequately cite and differentiate their contribution. Simply applying a known technique (LSTM) to a new dataset, while potentially useful, doesn’t constitute a significant advance without substantial methodological innovation or a particularly compelling biological insight.\n\nThe methodological clarity is indeed lacking. Reviewer 2 highlights several specific points (equation clarity, reference [5], awkward phrasing) that contribute to this. More fundamentally, the description of feature representation is vague (Reviewer 3, point 2). How are the ChIP-seq signals converted into inputs for the model? This is crucial for reproducibility and understanding the model’s behavior. The justification for the biLSTM architecture is weak and doesn't convincingly connect to the biological context (Reviewer 3, point 3). The claim about DNA being double-stranded doesn't logically support the choice of a biLSTM, and a comparison to non-recurrent models is essential.\n\nThe evaluation is also problematic. Reviewer 1 correctly points out the critical flaw: the improvement might be due to the custom weighted MSE loss function rather than the LSTM architecture itself. A fair comparison *must* include other models trained with the same loss function, or at least a vanilla LSTM. The lack of additional evaluation metrics (MSE, R^2) as suggested by Reviewer 3 further weakens the evaluation. The analysis of feature importance is also limited, relying on linear regression weights which may not translate to the LSTM’s internal representations (Reviewer 3, point 5). The unexplained behavior of the training errors (Reviewer 2, point g and Reviewer 3, point 6) is concerning and requires further investigation. The hyperparameter tuning process is also not described.\n\nThe authors’ responses to these criticisms would need to be substantial to address these concerns. Simply clarifying minor points won’t be enough. They need to demonstrate the true contribution of the LSTM architecture, provide a more rigorous evaluation, and improve the clarity and reproducibility of their methods. The specific question of generalizability (Reviewer 1) is also important – the authors need to discuss the limitations of their approach and potential avenues for applying it to other datasets.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles a well-established problem – Mahalanobis metric learning – and proposes an approximation algorithm based on linear programming techniques. The core idea of formulating the problem as minimizing violated constraints and then leveraging LP approximation schemes is reasonable. However, the reviewers raise significant concerns that, taken together, suggest the paper, in its current form, doesn't meet the bar for a top-tier conference.\n\n**Strengths:**\n\n* **Sound Theoretical Foundation:** Reviewer 1 and 2 acknowledge the method is based on sound theory. The connection to LP approximation algorithms is a potentially valuable contribution.\n* **Potential for Parallelization:** The algorithm's parallelizability is a positive aspect, though not a groundbreaking one.\n* **Experimental Validation:** The authors provide experimental results, demonstrating performance comparable to or better than existing methods.\n\n**Weaknesses (and why they are critical):**\n\n* **Novelty Concerns (Major):** This is the most significant issue. Reviewer 3 directly states a lack of understanding of the main novelty, and the other reviewers echo this sentiment. Simply formulating a problem as an LP doesn’t automatically constitute a novel contribution, especially in a field like metric learning where such transformations are not uncommon. The paper needs to *clearly* articulate what new insight or technique it brings to the table.\n* **Lack of Context & Comparison (Major):** The paper fails to adequately position itself within the existing literature. Reviewers 1 and 2 specifically request discussion of recent, relevant work (Verma & Branson, Ye et al.).  A thorough comparison, highlighting the advantages and disadvantages of the proposed approach relative to state-of-the-art methods, is crucial. The paper mentions ITML and LMNN, but doesn't sufficiently contextualize them.\n* **Technical Clarity & Rigor (Moderate):** Several technical questions are raised by the reviewers (Lemma 2.1 justification, Figure 4 anomaly, definitions of \"accuracy\" and \"combinatorial dimension\"). These indicate a lack of clarity in the presentation and potentially gaps in the technical arguments. Addressing these questions is essential for establishing the correctness and validity of the approach.\n* **Missing Conclusion:** The absence of a conclusion is a significant oversight.\n* **Ambiguous FPTAS Assumption:** The assumption of constant dimension 'd' for the FPTAS needs to be explicitly stated.\n* **Motivation for Approximation:** Reviewer 3 asks a crucial question: \"Why is the approximation needed?\". The paper doesn't adequately justify the need for an approximation algorithm.\n\n**Author Responses (Not Provided):** The absence of author responses makes it impossible to assess whether the authors have addressed the reviewers' concerns. However, given the severity of the issues raised, substantial revisions would be required to address them adequately.\n\n\n\nConsidering the significant concerns regarding novelty, context, and technical clarity, and the lack of author responses to demonstrate a plan for addressing these issues, the paper is not currently suitable for publication in a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – learning reusable skills from demonstrations without requiring detailed, time-aligned labels. The weakly-supervised approach, inspired by multiple instance learning, is a reasonable direction to explore. However, the reviewers raise significant concerns that, taken together, lead me to believe the paper is not currently suitable for acceptance at a top-tier conference.\n\n**Major Concerns:**\n\n* **Motivation vs. Results Disconnect:** Reviewer 1 highlights a critical flaw: the paper *frames* itself as a method for learning reusable skills, but the presented method only provides per-timestep labeling. This labeling, in itself, doesn’t demonstrate any capability for learning skills that can be *used* in downstream tasks. The suggested experiment of training behavior cloning agents conditioned on the predicted skills is crucial and missing. Without it, the core claim of learning reusable skills is unsubstantiated.\n* **Limited Novelty:** Reviewer 2 correctly points out the limited novelty of the log-sum-exp pooling approach, as it has been successfully used in other domains. While applying it to this problem is valid, it doesn't represent a significant algorithmic contribution.\n* **Underwhelming Results & Lack of Baselines:** The reported accuracy (35-60%) is low, and the lack of comparison to a fully-supervised oracle baseline (as suggested by both Reviewer 2 and 3) makes it difficult to assess the effectiveness of the approach. The behavioral cloning results are also weak. The absence of comparisons to unsupervised clustering methods (Reviewer 3) further weakens the evaluation.\n* **Experimental Rigor & Clarity:** Reviewer 1 raises valid concerns about the lack of experimental details (task setup, hyperparameters, architectural choices). The potential for overfitting, particularly in the video examples, is also a concern. The lack of analysis regarding the consistency of predictions over time is a significant weakness.\n* **Theoretical Limitations:** Reviewer 3 identifies a fundamental limitation: the method's feasibility depends on specific data characteristics (skill co-occurrence). The paper doesn't address this limitation or provide any theoretical bounds on when the approach might succeed. The reviewer’s example of the skill sequence dataset is compelling.\n* **Insufficient Evaluation Metrics:** Reviewer 3 rightly points out the inadequacy of accuracy alone. Confusion matrices or precision/recall metrics are needed to understand the nature of the errors. Qualitative results, like trajectory classification visualizations, are also missing.\n\n**Author Responses (Not Provided):** While author responses are missing, the severity of the reviewers’ concerns suggests that addressing them would require substantial revisions, potentially bordering on a complete overhaul of the experimental evaluation and a refocusing of the paper’s claims.\n\n\n\nThe reviewers are consistent in their assessment. While the idea is interesting, the paper falls short in terms of novelty, experimental validation, and theoretical grounding. The disconnect between the stated motivation and the actual results is a particularly damaging flaw.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper explores the application of recurrent neural networks, specifically bidirectional LSTMs, to predict DNA folding patterns using epigenetic data. The core idea – leveraging sequential information in DNA for predicting chromatin structure – is reasonable and aligns with current trends in applying machine learning to genomics. The authors demonstrate that their LSTM-based model outperforms several baseline methods on a *Drosophila melanogaster* Hi-C dataset. However, despite this positive result, significant concerns raised by the reviewers, particularly Reviewer 3, lead me to believe the paper, in its current form, is not suitable for a top-tier conference.\n\nThe most critical issue is the lack of methodological novelty. Reviewer 3 rightly points out the similarity to existing work, and the authors haven’t adequately addressed this in their responses (not provided here, but implied by the reviewers’ comments). Simply applying a known technique (LSTM) to a new dataset, even with some performance improvement, isn't sufficient for a top-tier venue. The paper needs to demonstrate a *significant* advance in methodology or a particularly insightful application that justifies publication.\n\nFurther weakening the paper is the lack of clarity in the methods section. Reviewer 2 highlights issues with equation presentation and referencing, while Reviewer 3 questions the feature representation and handling of genomic data. These ambiguities make it difficult to assess the validity and reproducibility of the results. The description of the data and how it was processed is insufficiently detailed.\n\nThe experimental design also suffers from several flaws. Reviewer 1 correctly identifies the need for a fairer comparison. The improvement attributed to the LSTM might be due to the modified weighted MSE loss function, and the authors haven’t convincingly ruled this out by comparing other models with the same loss function or a vanilla LSTM. The justification for the specific parameters used in the loss function (e.g., the '11' in the weighted MSE) is also weak.\n\nReviewer 3’s points regarding the biLSTM motivation and comparison to non-recurrent models are also valid. The connection between the biLSTM architecture and the double-stranded nature of DNA is tenuous, and a comparison to other neural network architectures (e.g., CNNs) is necessary to demonstrate the benefits of using a recurrent model.\n\nFinally, the analysis of feature importance is limited. While the authors use linear regression weights for feature importance, they don’t provide a comparable analysis for the LSTM model. The unexplained negative errors in training (pointed out by Reviewer 2) are also concerning and require further investigation.\n\nWhile the paper presents a potentially interesting application and shows promising results, the methodological weaknesses, lack of novelty, and insufficient experimental rigor prevent it from meeting the standards of a top-tier conference. Addressing these concerns would require substantial revisions, including a more thorough literature review, a clearer description of the methods, a more rigorous experimental design, and a more in-depth analysis of the results.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents NORML, a meta-learning framework utilizing an LSTM to modulate gradient updates for a learner network. The core idea – learning neuron-wise optimization – is interesting and addresses the scalability concerns of some existing meta-learning approaches. However, the reviewers raise significant concerns that, in my assessment, outweigh the potential contributions.\n\n**Strengths:**\n\n* **Addresses a relevant problem:** Scalability of meta-learners is a known challenge, and the paper attempts to tackle it.\n* **Novelty (to a degree):** While building on existing LSTM-based meta-learners, the neuron-wise optimization approach is a distinct variation.\n\n**Weaknesses (and why they are critical):**\n\n* **Incorrect Claims & Misrepresentation of Prior Work:** This is the most damaging issue. Reviewer 1 and 3 both point out inaccuracies in the paper’s description of Ravi & Larochelle and Andrychowicz et al. Specifically, the claim that these prior works require an enormous number of parameters for the meta-learner is demonstrably false. This casts doubt on the authors’ understanding of the field and the validity of their contribution. The claim of using a single meta-learner is also contradicted by the equations presented.\n* **Lack of Fair Comparison:** The paper doesn’t provide a direct comparison to Ravi & Larochelle with comparable experimental settings. Reviewer 1 rightly points out that this is crucial to validate the benefit of the increased parameter count in NORML. Furthermore, Reviewer 3 highlights the use of different base learners and pre-training strategies when comparing to prior work, making the results difficult to interpret.\n* **Insufficient Experimental Validation:** The experimental results are not compelling. The authors fail to include results from Sun et al. (the method they build upon), and the reported results are not superior. The choice of a fully-connected network for the Omniglot experiment, while simplifying the setup, limits the generalizability of the findings. The lack of confidence intervals in Table 2 is also a significant omission.\n* **Unsubstantiated Claims:** The claim that NORML can handle a large number of inner loop updates due to improved gradient flow is not supported by the experiments, which use a small number of steps.\n* **Poor Writing Quality:** Reviewer 2 identifies significant issues with the paper’s writing, including grammatical errors, incorrect citations, and a lack of clarity. While writing quality isn’t always a deciding factor, it significantly hinders understanding and evaluation.\n\n**Overall Assessment:**\n\nThe paper proposes an interesting idea, but it is severely hampered by inaccuracies, a lack of rigorous comparison, and insufficient experimental validation. The misrepresentation of prior work is particularly concerning. While the authors respond to the reviewers (responses not provided here, but assumed to be standard), the core issues identified by the reviewers remain unaddressed. The paper, as it stands, does not meet the standards of a top-tier conference. It requires substantial revisions, including a thorough re-evaluation of related work, a more rigorous experimental setup with fair comparisons, and a significant improvement in writing quality.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – extending GQN to learn object-oriented 3D scene representations. The core concept of decomposing scenes into individual objects and representing them in a viewpoint-invariant 3D manner is promising and aligns with goals in representation learning and 3D understanding. The qualitative results, as highlighted by reviewers, *are* impressive, demonstrating the ability to manipulate and recompose scenes.\n\nHowever, the paper suffers from significant weaknesses that, even after revision, prevent it from reaching the standard of a top-tier conference like ICLR. The reviewers consistently point to issues with clarity, experimental rigor, and positioning within the existing literature.\n\n**Clarity & Presentation:** The initial reviews were scathing regarding the writing quality, with numerous typos and confusing explanations. While the authors have clearly made substantial revisions, Reviewer 4’s post-rebuttal comment indicates the changes are so extensive that assessing the impact of the revisions is difficult. More importantly, the core issue isn’t just about fixing typos; it’s about presenting a complex method in a way that is accessible and understandable. The reviewers’ struggles with understanding the notation, the roles of different components (e.g., f3d->2d, STN-INV), and the flow of information suggest the method remains difficult to grasp.\n\n**Experimental Rigor:** This is the most significant weakness. The experiments, even after revision, are insufficient to support the claims made. \n* **Lack of Ablation Studies:** The absence of ablation studies is a critical flaw. It’s impossible to determine which components of ROOTS contribute to the observed performance gains. Are the 3D representations truly essential, or is the improvement due to other modifications to the GQN framework?\n* **Weak Comparison to GQN:** The comparison to GQN is not convincing. The reported improvements are often small, and the authors haven't convincingly demonstrated that GQN *fails* in a way that ROOTS overcomes. The fact that ROOTS sometimes performs *worse* in generalization, as noted by Reviewer 4, is particularly concerning.\n* **Missing Baselines:** The lack of comparison to more recent and relevant work in 3D scene understanding and neural rendering is a major oversight. The reviewers rightly point out the need to compare to methods like 3D-R2N2, DeepVoxels, and others.\n* **Evaluation Metrics:** The choice of evaluation metrics (MSE, NLL) is not fully justified, and the lack of standard metrics like precision-recall curves raises questions about the validity of the quantitative results.\n\n**Related Work:** The initial reviews highlighted a significant gap in the related work section. While the authors likely added citations in the revision, the reviewers’ concerns about the paper’s positioning within the broader field remain valid. The paper needs to more clearly articulate its contributions in relation to existing work and acknowledge the limitations of its approach.\n\n**Novelty:** The claim of being the \"first unsupervised model that can identify objects in a 3D scene\" was demonstrably false and has been corrected. However, the remaining claims of novelty need to be more carefully substantiated.\n\n\n\nThe AC's post-rebuttal update reflects the core issue: the reviewers are divided. Some are willing to accept the paper given the improvements in clarity, while others remain unconvinced by the experimental weaknesses. Given the high standards of a top-tier conference, the lack of consensus and the significant experimental shortcomings are fatal.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in multi-task learning – how to effectively transfer knowledge between tasks, especially when those tasks aren't obviously related. The core idea of modeling task dependencies at multiple levels (general and data-specific) using attention is potentially valuable. However, the paper suffers from significant weaknesses in presentation, clarity, and experimental validation, as highlighted by all four reviewers.\n\nLet's break down the issues:\n\n**Strengths:**\n\n* **Novelty (potentially):** Reviewer 2 identifies the idea of sample-dependent task relatedness as potentially new. This is the strongest argument for acceptance.\n* **Relevant Problem:** Addressing the limitations of traditional multi-task learning assumptions is a worthwhile research direction.\n\n**Weaknesses – and they are substantial:**\n\n* **Poor Writing & Clarity:**  Reviewers 1 & 2 repeatedly point to grammatical errors and a generally difficult-to-follow presentation. This hinders understanding of the core contributions.\n* **Lack of Clear Differentiation:** Reviewer 1 questions the significance of the distinction between \"general\" and \"data\" dependency, suggesting they are intertwined. The paper doesn't convincingly demonstrate the independent contribution of each level.\n* **Insufficient Motivation:** Reviewer 2 requests a stronger motivation for the sample-dependent task relatedness. The initial argument isn't compelling.\n* **Experimental Deficiencies:** This is a major issue.\n    * **Dataset Concerns:** Reviewer 2 raises a valid point about potential data leakage in the multi-label learning setup.\n    * **Missing Baselines:**  The most critical flaw. Reviewers 1, 2, and 4 all emphasize the need for comparison to relevant baselines, specifically \"End-to-end multi-task learning with attention\" and a simplified version of their method without attention.  The rebuttal doesn't adequately address this.\n    * **Lack of Comparison to Related Work:** Reviewer 3 correctly points out the lack of comparison to existing attention-based multi-task learning methods.\n* **Prior Work Ignored:** Reviewer 4 delivers a devastating critique, highlighting the paper's apparent unawareness of \"Taskonomy\" and other related work. This is a serious oversight and suggests a lack of thorough literature review. The rebuttal doesn't convincingly demonstrate how this work differs from existing approaches.\n* **Technical Issues:** Reviewer 4 identifies numerous technical issues with the method's presentation, including confusing notation, missing details, and inconsistencies.\n* **Unconvincing Justification:** The hierarchical justification offered in the rebuttal (\"text and graph data are hierarchical\") feels weak and doesn't fully address the concerns raised.\n\n**Rebuttal Analysis:**\n\nThe rebuttal acknowledges some of the issues but doesn't provide sufficient solutions. While the authors claim improvements, the reviewers remain unconvinced, particularly regarding the experimental validation and comparison to related work. The rebuttal doesn't demonstrate a clear understanding of *why* their approach is superior to existing methods.\n\n\n\nGiven the severity and number of issues, particularly the lack of adequate experimental validation and the failure to address the concerns about prior work, this paper is not ready for publication in a top-tier conference. The core idea has potential, but it needs significant refinement, clearer presentation, and a much more rigorous experimental evaluation.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper, MuPPET, proposes a precision-switching strategy for quantised fixed-point training of CNNs. The core idea is to dynamically adjust the precision during training based on gradient diversity, aiming to accelerate training while maintaining accuracy. While the idea is interesting and addresses a relevant problem – the high computational cost of training large CNNs – the paper suffers from several significant weaknesses that, in my assessment, prevent it from reaching the standards of a top-tier conference like ICLR.\n\n**Strengths:**\n\n* **Relevance:** Reducing training time for large CNNs is a crucial research area.\n* **Novelty (Potential):** The combination of multiple fixed-point precisions with a dynamic switching mechanism is a potentially novel approach.\n* **Experimental Validation:** The authors evaluate their method on standard datasets (ImageNet) and architectures (AlexNet, ResNet18, GoogLeNet).\n\n**Weaknesses (Significant):**\n\n* **Lack of Clear Motivation & Heuristic Nature:** Reviewer 1 rightly points out that the reasoning behind the precision-switching mechanism is not well explained. The connection between gradient co-alignment and lack of learning progress is not intuitive and requires stronger justification. The reliance on a heuristic borrowed from another work without sufficient explanation weakens the contribution.\n* **Limited Performance Gains:** Reviewer 2 highlights a critical issue: the performance gains are not substantial enough. A 1.28x speedup, while positive, is not a breakthrough, especially considering the added complexity of the proposed method. The paper fails to convincingly demonstrate a clear advantage over existing state-of-the-art techniques like mixed-precision training (Micikevicius et al., 2018).\n* **Conceptual Issues & Clarity:** Reviewer 2 raises several valid concerns regarding the clarity of the method. The description of the precision switching policy is confusing, notations are unclear (p, y, gamma), and there are inconsistencies between the description and the figures (Figure 2). The claim that the method \"decides the most appropriate quantization level\" is misleading, as it only determines *when* to increase precision, not *which* level to choose.\n* **Reproducibility Concerns:** Reviewer 3 identifies several issues hindering reproducibility, including unclear details about the quantization scheme, scaling factors, and the meaning of certain notations.\n* **Insufficient Analysis:** The lack of training and validation curves (as noted by Reviewer 2) is a significant omission. These curves would provide valuable insights into the training dynamics and the effectiveness of the precision switching policy.\n* **Ad-hoc Justifications:** The explanation for the observed behavior with AlexNet (switching at the \"wrong\" time being beneficial) feels like post-hoc rationalization rather than a principled explanation.\n* **Minor Issues:** The reviewers also point out typos, grammatical errors, and readability issues (small font size in figures).\n\n\n\n**Author Responses:**\n\nWithout access to the author responses, it's difficult to assess whether they adequately address these concerns. However, the severity and number of issues suggest that substantial revisions would be required to bring the paper up to the standards of a top-tier conference. Even with strong responses, the fundamental limitations in performance gains and clarity remain problematic.\n\n\n\nIn conclusion, while the paper explores a potentially interesting direction, the lack of clear justification, limited performance improvements, conceptual issues, and reproducibility concerns outweigh its strengths. The paper feels underdeveloped and lacks the rigor expected of a publication at a highly selective conference.\n\nFinal Deci"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in certified robustness: the gap between certification bounds and empirical robustness. The core idea – using regularizers to encourage tighter convex relaxations – is conceptually sound and potentially valuable. However, the paper suffers from significant clarity issues in its theoretical analysis and lacks sufficient experimental rigor to convincingly demonstrate its contribution at the level expected of a top-tier conference.\n\nLet's break down the issues raised by the reviewers:\n\n**Theoretical Clarity (Reviewers 1 & 2):** The most significant weakness is the convoluted and often confusing analysis section (Section 4). Reviewers 1 and 2 both pinpoint substantial problems with the authors’ understanding and explanation of the convex relaxation, its optimality, and the implications of their regularizers. Specifically:\n\n* **Misunderstanding of Optimality:** The authors repeatedly mischaracterize Eq. (O) and the relationship between the relaxed problem (C) and the original non-convex problem. They seem to struggle with articulating *when* the relaxation is exact and the conditions under which their regularizers improve upon existing methods.\n* **Unclear Motivation:** Reviewer 2 rightly questions *why* the proposed relaxation (which appears similar to Fast-Lin) would be better than an optimal convex relaxation. The paper doesn't adequately address this, leaving the reader wondering if they are intentionally using a looser relaxation for a specific purpose, and if so, what that purpose is.\n* **Proposition 1 & Assumptions:** The assumptions underlying Proposition 1 are extremely strong and unrealistic, as highlighted by Reviewer 1. The proposition essentially requires a highly linear network with no ambiguous ReLUs, which severely limits its applicability.\n* **Lack of Connection to Prior Work:** While the paper cites relevant work, it doesn't clearly position its contribution within the existing landscape. The connection to ReLU stability (Xiao et al.) mentioned by Reviewer 1 is a potentially valuable avenue for discussion that is not fully explored.\n\n**Experimental Rigor (Reviewers 1 & 2):** The experimental evaluation, while showing some improvement, is not compelling enough.\n\n* **Missing Baselines:** Reviewer 1 correctly points out the absence of crucial baselines like Gowal et al. (2020). Including these results would provide essential context and allow for a more meaningful comparison.\n* **Confounding Factors:** The improvement observed when comparing to CROWN-IBP seems to be largely attributable to *using* CROWN-IBP's training method, rather than the proposed regularizers themselves. This needs to be disentangled.\n* **Limited Datasets:** Reviewer 3 raises a valid point about the limited dataset (MNIST). While MNIST can be a useful starting point, a more comprehensive evaluation on more challenging datasets is necessary for a top-tier conference.\n* **Clarification Needed:** Reviewer 2 asks for clarification on whether existing methods used the tighter or looser convex relaxation when reporting results, which is a critical detail for interpreting the experimental findings.\n\n**Minor Issues:** The reviewers also identify minor issues like typos and citation formatting errors, which, while not major, contribute to the overall impression of a paper that needs further polishing.\n\n**Reviewer 3's Opinion:** Reviewer 3's \"Weak Accept\" is the most optimistic assessment, but even this acknowledges \"some presentation issues\" and the need for more datasets.\n\n\n\nIn summary, the paper presents an interesting idea but is hampered by significant weaknesses in its theoretical analysis, experimental evaluation, and clarity of presentation. While the empirical results are promising, they are not strong enough to overcome the substantial concerns raised by the reviewers. The paper requires a major revision to address these issues before it would be suitable for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in reinforcement learning – effective exploration – and proposes a method, Deep Coherent Exploration (DCE), to bridge the gap between step-based and trajectory-based exploration in deep RL. The core idea of modeling the last layer parameters as latent variables and using recursive inference is clever and addresses a key limitation of scaling previous work (van Hoof et al., 2017) to deep networks.\n\nThe reviewers are largely positive. Reviewers 1, 2, and 3 acknowledge the novelty and potential contribution of the work, particularly regarding the scalability aspect. Reviewer 1 tentatively accepts, while reviewers 2 and 3 lean towards acceptance with minor revisions. Reviewer 4 is more critical, pointing out the close relationship to prior work and the limited success with SAC.\n\nThe strengths of the paper are clear: a well-written presentation, strong empirical results for A2C and PPO, detailed mathematical derivations, and a readily reproducible implementation (as noted in the appendix). The ablation studies are also a significant positive, demonstrating a good understanding of the method’s components.\n\nHowever, several concerns raised by the reviewers need careful consideration. The most consistent critique revolves around the performance with SAC. The authors acknowledge this limitation, but the reviewers rightly question *why* DCE doesn’t translate well to SAC. The explanation provided (heuristic optimization, restrictive generative model) feels somewhat insufficient. The lack of a thorough investigation into hyperparameter sensitivity for SAC is also a weakness.\n\nReviewer 2’s point about the significance being limited due to SAC’s dominance is valid. If the method doesn’t improve upon the state-of-the-art (SAC), its practical impact is diminished. However, the strong performance on A2C and PPO still represents a valuable contribution, especially given the challenges of exploration in on-policy methods.\n\nReviewer 3’s comments regarding clarity in Section 3 are helpful and should be addressed. Simplifying the presentation and moving some details to the appendix would improve readability. The minor issue with figure font size is easily fixable.\n\nReviewer 4’s concerns about the lack of experiments on pure exploration or sparse reward tasks are also valid. While the Mujoco tasks are standard benchmarks, demonstrating the method’s effectiveness in more challenging exploration scenarios would strengthen the paper.\n\nThe suggestion from Reviewer 1 to reframe the paper as an on-policy method and explicitly acknowledge the off-policy case as a limitation is a reasonable one. This would allow the authors to focus on the areas where DCE demonstrably excels and avoid overstating its generality.\n\nOverall, the paper presents a solid contribution to the field. The strong results with A2C and PPO, combined with the theoretical justification and detailed implementation, outweigh the weaknesses, particularly the limited success with SAC. Addressing the reviewers’ concerns regarding SAC, clarity, and the scope of experiments would further strengthen the paper.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts a valuable and important goal: bridging the gap between meta-learning theory and practice. The motivation is sound – meta-learning algorithms often lack theoretical justification, and incorporating theoretical insights could lead to improved performance and understanding. However, the reviewers raise significant concerns that, taken together, lead me to believe the paper, in its current form, is not suitable for a top-tier conference.\n\nThe core issue is a lack of novelty and a questionable connection between theory and implementation. Reviewers 1 and 4 correctly point out that the proposed regularization terms are closely related to existing techniques (spectral normalization and weight decay). While the authors frame these as enforcing theoretical assumptions, the reviewers argue (and convincingly so) that these assumptions are often already addressed by standard practices. The claim of being the \"first contribution that puts the most recent learning bounds of meta-learning theory into practice\" feels overstated given this overlap.\n\nReviewer 3 raises a crucial philosophical point about \"enforcing\" assumptions. The assumptions are properties of the *problem*, not something algorithms can dictate. \"Respecting\" the assumptions is a more accurate framing, and the lack of exploration into scenarios where the assumptions *don't* hold weakens the paper. The suggestion of synthetic experiments to investigate this is excellent and currently missing.\n\nSeveral reviewers also highlight weaknesses in the experimental evaluation. Reviewer 1 notes that improvements are modest and could be due to simple training tricks. Reviewer 3 points out that improvements are often at the level of standard deviation, casting doubt on their significance. Reviewer 4 correctly identifies a lack of sufficient baselines. The omission of baseline values in Table 1 (as noted by Reviewer 2) is a significant oversight.\n\nThe concerns regarding the consistency between the theoretical setting (no validation set in Eq. 1) and the few-shot learning setting (Reviewer 2) are also valid and require further discussion. The differences in the meta-learning loss function (Eq. 4) compared to MAML (Reviewer 2) need clarification. Finally, the missing details regarding the calculation of subgradients (Reviewer 4) is a practical concern that impacts reproducibility.\n\nThe author responses are not provided, so I cannot assess whether they adequately address these concerns. However, based on the severity and number of criticisms, it's unlikely that revisions could fully resolve these issues to meet the standards of a top-tier conference. The paper feels more like a preliminary investigation or a blog post exploring an interesting idea, rather than a substantial contribution to the field.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Reasoning\n\nThis paper tackles an interesting and important problem – integrating language and vision for referring expression comprehension, specifically image segmentation. The core idea of modulating bottom-up visual processing with language, rather than solely relying on top-down attention, is a reasonable hypothesis. The paper is generally well-written and the experimental setup seems sound.\n\nHowever, the reviews reveal significant concerns that, despite the author rebuttals, remain largely unaddressed. The central issue is a lack of convincing evidence for a *significant* contribution. While the paper reports improvements, these are often marginal, inconsistent across datasets (as highlighted by Reviewer 2), and lack rigorous statistical validation (emphasized by Reviewer 4). The claim of state-of-the-art performance is also contested, with the paper struggling to match existing works on certain metrics.\n\nReviewer 4’s critique is particularly strong and resonates with several points raised by others. The lack of a clear conceptual motivation for the model, beyond a general intuition about cognitive plausibility, is a major weakness. The paper doesn’t adequately explain *why* language should influence low-level visual features, nor does it provide sufficient analysis to understand *how* it does so. The requested visualizations and error analyses (e.g., similar to Hu et al. and Hui et al.) are crucial for building confidence in the model’s behavior and are missing.\n\nThe rebuttal doesn’t fully address these concerns. The authors’ defense relies heavily on citing a baseline paper (Mei et al., 2018) that wasn't originally included in the paper, raising questions about the completeness of the related work and the novelty of the approach. The statistical concerns raised by Reviewer 4 regarding the IoU calculations and the need for significance testing are also not adequately addressed. The authors' claim about the overall IoU is difficult to verify given the inconsistencies in the reported results.\n\nReviewer 1 points to a lack of exploration of different language modeling techniques, suggesting the approach is not thoroughly investigated. Reviewer 2 raises concerns about potential overfitting to the validation set. Reviewer 3, while initially positive, doesn’t offer a sufficiently critical assessment to outweigh the concerns raised by the other reviewers.\n\nWhile the idea is interesting, the paper falls short of the standards expected for a top-tier conference. The improvements are not substantial enough, the conceptual motivation is weak, and the analysis is insufficient to justify the claims made. The lack of statistical rigor and the unresolved inconsistencies in the results further weaken the paper.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and increasingly relevant problem: the biological implausibility of the training regimes required for state-of-the-art deep neural networks that achieve impressive performance on benchmarks like BrainScore. The authors explore strategies to reduce the number of supervised updates needed to reach a reasonable level of performance, focusing on weight initialization and selective training. The core idea – that we can achieve significant performance with far fewer updates – is compelling and potentially impactful.\n\nHowever, the paper is plagued by a central, repeatedly raised concern by multiple reviewers: the interpretation of “match” to the ventral stream as a meaningful biological proxy. The authors rely heavily on BrainScore, and while acknowledging its limitations, don't adequately address the fundamental question of *what* a BrainScore of 0.8 actually *means* in terms of representational similarity. The reviewers rightly point out that 80% of a score could be meaningless if the score itself isn’t strongly correlated with biological plausibility. The authors’ attempts to address this in revisions are insufficient, falling short of making readers “extremely aware” of the distinction between score-based match and actual representational similarity.\n\nBeyond this core issue, several other concerns, echoed across multiple reviews, weaken the paper. The analogy to biological development is stretched thin, particularly the assumption of primarily supervised learning. The novelty of the weight compression technique is questionable, and its advantages over existing methods aren’t convincingly demonstrated. The rationale for selecting “critical” layers for training feels arbitrary. The discussion of the kernel visualization (Fig. 5c) is weak. Minor numerical inconsistencies and ambiguous phrasing further detract from the paper’s rigor.\n\nReviewer 4 provides a strong endorsement, highlighting the thoroughness of the experiments and the well-written nature of the paper. However, the weight of the criticisms, particularly the fundamental issue of interpreting BrainScore, is substantial. While the empirical results are interesting, the paper doesn’t provide enough justification for its central claims about biological plausibility. The transfer learning experiment is a positive aspect, but doesn’t fully compensate for the other weaknesses.\n\nThe paper is borderline, as acknowledged by multiple reviewers. It presents a worthwhile exploration of a relevant problem, but its conclusions are overstated and its interpretations are insufficiently cautious. A major revision addressing the concerns about BrainScore, strengthening the biological justification for the methods, and clarifying the novelty of the contributions would be necessary for acceptance. As it stands, the paper risks misleading readers into believing it has achieved a greater degree of biological insight than is warranted.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents a novel and promising approach to complex query answering over incomplete knowledge graphs. The core idea – framing query answering as an optimization problem leveraging pre-trained neural link predictors – is elegant and addresses a significant challenge in the field. The authors demonstrate strong empirical results, outperforming state-of-the-art methods with significantly less training data. The inclusion of explainability aspects is also a valuable contribution.\n\nHowever, the paper isn't without its weaknesses, as highlighted by the reviewers. Several reviewers raise valid concerns regarding the technical details and experimental setup. Specifically:\n\n* **Comparison to GQE/Q2B:** Reviewer 3 rightly points out the similarities between the continuous optimization method and existing approaches like GQE and Q2B, questioning the novelty of the formulation beyond the use of ComplEx and t-norms. The authors need to more clearly articulate the distinctions and justify the improvements.\n* **Computational Complexity:** The exponential time complexity of the combinatorial optimization method (CQD-CO) is a significant concern, as noted by Reviewer 3 and Reviewer 4. The authors need to provide a more thorough analysis of the runtime performance and scalability, and a clear comparison to existing methods.\n* **Neural Link Predictor Calibration & Ablation:** Reviewers 1, 3, and 4 all request more details on the calibration of the neural link predictor output and suggest ablation studies with different embedding models (e.g., TransE). This is crucial for understanding the contribution of the neural link predictor itself versus the proposed optimization framework.\n* **Experimental Clarity:** Several reviewers (3, 4, 5) point to ambiguities in the experimental setup, particularly regarding the amount of training data used and the comparison of inference times. The authors need to provide a more precise and transparent presentation of their experimental results.\n* **Notational Issues:** The minor notational inconsistencies identified by Reviewer 3 should be addressed for clarity.\n\nThe author responses (not provided here, but referenced by Reviewer 5) are crucial. If the authors have adequately addressed these concerns in their response, demonstrating a willingness to clarify and improve the paper, it strengthens the case for acceptance.\n\nDespite these concerns, the overall quality of the work is high. The problem is important, the proposed solution is innovative, and the results are compelling. The reviewers are largely positive, with Reviewer 5 explicitly recommending acceptance even after considering other reviews. The concerns raised are largely about refinement and clarification, rather than fundamental flaws. A top-tier conference often accepts papers with minor weaknesses if the core contribution is significant and the authors demonstrate a commitment to addressing reviewer feedback.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting approach to disentangling representations within pre-trained language models by learning binary masks over weights or activations. The core idea – identifying subnetworks responsible for specific factors of variation without fine-tuning the base model – is novel and potentially valuable. The results, while not overwhelmingly conclusive, demonstrate competitive performance against several baselines, including finetuned BERT models. However, several concerns raised by the reviewers highlight significant weaknesses that prevent this paper from reaching the standard of a top-tier conference *at this time*.\n\n**Strengths:**\n\n* **Novelty:** The masking approach to disentanglement is a fresh perspective compared to VAEs or adversarial training.\n* **Efficiency:** Avoiding full fine-tuning is a potential advantage, though the reviewers rightly question the actual speedup given the mask size.\n* **Clear Writing:** The paper is generally well-written and easy to understand.\n* **Competitive Performance:** The method achieves comparable or better results than some baselines, demonstrating its potential.\n\n**Weaknesses (and how they impact the paper):**\n\n* **Generality:** This is the most significant concern. The experiments are limited to movie reviews (Drama/Horror) and a somewhat artificial syntax/semantics disentanglement task. Reviewers 1, 4, and 5 all rightly point out the lack of evidence for generalizability. A top-tier conference expects work to demonstrate broader applicability.\n* **Limited Improvement & Benchmarking:** While competitive, the paper doesn't demonstrate *significant* improvement over strong baselines on standard benchmarks. Reviewer 2 correctly notes the lack of improvement on benchmarks and questions the impact of the work. The claim of outperforming VAEs is not substantiated in the main text.\n* **Parameter Overhead:** Reviewer 3 highlights the substantial increase in parameters due to the masks, which undermines the efficiency argument.\n* **Triplet Loss Limitation:** The triplet loss formulation, as noted by Reviewer 3, restricts disentanglement to only two factors.\n* **Leakage & Probe Strength:** Reviewer 3 also points out the potential for attribute leakage and the weakness of the probe used to measure it.\n* **Missing Discussion:** The comparison between weight masking and hidden unit masking (Reviewer 4) is missing, and the results in Figure 4 are not convincingly better than finetuning.\n* **Related Work Gaps:** Reviewers point out missing citations of relevant work (dropout-based confidence modeling and HUBERT).\n\n**Author Responses:**\n\nWhile we don't have the author responses in detail, the initial reviewer comments suggest that the authors have attempted to address some concerns. However, the core issues of generality and the lack of substantial improvement on benchmarks remain.\n\n\n\n**Overall Assessment:**\n\nThe paper presents a promising idea, but the limited scope of the experiments, the lack of compelling evidence for generalizability, and the weaknesses in the experimental setup and analysis prevent it from being accepted at a top-tier conference in its current form. The reviewers' concerns are valid and collectively indicate that the work requires substantial revision and further experimentation to demonstrate its true potential. A stronger focus on broader applicability, more rigorous benchmarking, and a more thorough analysis of the results are necessary.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to generalize the Maximum Mean Discrepancy (MMD) to discrete distributions, proposing a \"Generalized Probability Kernel\" (GPK). While the idea of extending kernel methods to directly compare distributions rather than samples is interesting, the execution, as highlighted by all four reviewers, is severely lacking. The paper suffers from significant issues in clarity, rigor, novelty, and completeness.\n\nHere's a breakdown of the major concerns, consolidating the reviewers' points:\n\n* **Lack of Clarity and Rigor:** The reviewers consistently point to numerous typos, grammatical errors, and imprecise definitions. The notation is inconsistent and often undefined, making it difficult to follow the mathematical derivations. Several definitions are questionable (e.g., the definition of a \"kernel\" in Definition 2 being more akin to a distance, the meaning of the Stein operator in a discrete setting).\n* **Questionable Novelty:** The paper claims to generalize MMD and KSD, but the reviewers question the practical benefit of this generalization. The theoretical contributions are described as \"elementary manipulations\" and largely follow existing literature. The connection to Bernstein polynomials, mentioned in the introduction, is not developed.\n* **Incompleteness:** Crucially, the paper lacks a simulation section. Without experimental validation, it's impossible to assess the performance or utility of the proposed GPK. The proof of Theorem 5 is missing. Several sections are identified as trivial or containing known results and should be removed.\n* **Unclear Application to Two-Sample Testing:** The application of the GPK to two-sample testing is poorly explained. The reviewers correctly point out the difficulty of applying the kernel when the distributions *p* and *q* are unknown, which is the standard scenario in two-sample testing. The paper doesn't adequately address how to estimate the kernel in this setting. The advantages of the symmetric KDSD over existing methods are not justified.\n* **Fundamental Conceptual Issues:** Reviewer 3 raises a valid point about the relevance of an RKHS setting for discrete variables with labels in R^k. The motivation for this setting is weak, and the paper doesn't provide compelling examples where this approach is beneficial.\n\nThe authors' responses (not provided, but inferred from the severity of the reviewers' criticisms) are unlikely to address these fundamental flaws adequately. The issues are not merely superficial; they strike at the core of the paper's theoretical foundation and practical applicability.  A \"major work-over\" as suggested by Reviewer 2 is a significant understatement; the paper requires substantial revision and likely a rethinking of its core concepts.\n\nGiven the severity and breadth of the criticisms, and the lack of experimental validation, this paper is not suitable for publication in a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents GG-GAN, a novel approach to graph generation leveraging a geometric interpretation and Wasserstein GANs. The core idea – associating nodes with spatial positions and defining edges based on similarity – is interesting and potentially valuable. The authors attempt to address key challenges in geometric graph generation, such as handling isomorphic graphs and exploiting the latent distribution. The theoretical grounding, with propositions and corollaries, demonstrates a level of rigor often lacking in purely empirical work.\n\nHowever, the paper suffers from several significant weaknesses that, in my assessment, prevent it from reaching the standards of a top-tier conference. The most substantial concerns revolve around the experimental evaluation.\n\n* **Limited Datasets:** The datasets used are extremely small (9 and 20 vertices). As highlighted by Reviewer 1, this raises concerns about the impact of isomorphic graphs and the feasibility of simply enumerating all non-isomorphic graphs. The claim of scalability is therefore not convincingly demonstrated. Testing on larger, more complex graphs is crucial.\n* **Weak Baselines:** The comparison to baselines is inadequate. While graphRNN is included, NetGAN (which the reviewers consistently point out) is missing, despite being a relevant and competitive method. The inclusion of simpler baselines (Watts-Strogatz, etc.) would also strengthen the evaluation. The single MMD result favoring GG-GAN is not sufficient evidence of superiority.\n* **Questionable Claims & Theoretical Weaknesses:** Several reviewers raise valid concerns about the claims made in the paper. Reviewer 2 questions the handling of mode collapse and the modeling of edge dependencies (pointing out the independent Bernoulli sampling). Reviewer 4 correctly identifies that the theoretical contributions feel weakly connected to the proposed method. The corollary regarding the existence of a distribution is considered trivial.\n* **Ad-hoc Features & Implementation Details:** The use of hand-crafted node features (Reviewer 2 & 4) is a weakness, potentially limiting the model's ability to learn meaningful representations. Several reviewers also point to unclear implementation details (e.g., the role of 'phi', collision avoidance) that require further clarification.\n* **Lack of Ablation Studies:** The authors don't provide ablation studies to justify design choices, such as the unique sampling of 'z' for each node (Reviewer 2 & 5).\n\nWhile the reviewers acknowledge the paper's clarity and the novelty of the approach (Reviewers 3 & 5), the overwhelming consensus is that the experimental validation is insufficient and the claims are not adequately supported. The concerns raised by multiple reviewers regarding the datasets, baselines, and theoretical connections are substantial. Addressing these issues would require significant revisions and additional experimentation. The current version does not demonstrate a clear and compelling advantage over existing methods.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles a crucial problem in deep learning: reliable uncertainty estimation, particularly on out-of-distribution (OOD) data. The core idea – calibrating predictions based on exposure to corrupted data – is intuitive and potentially valuable. The experimental results, while not universally compelling, demonstrate improvements over existing methods on benchmark datasets. However, several significant concerns, raised consistently by multiple reviewers, prevent this paper from reaching the standard of a top-tier conference.\n\nThe most critical flaw, highlighted by Reviewer 1, is the fundamental assumption that the set of possible novel distributions is *known*. This drastically simplifies the problem and distinguishes it from the more general OOD detection/calibration challenge. The authors essentially shift the problem from identifying truly unseen distributions to selecting from a pre-defined set of corruptions. While they acknowledge this, they don't adequately address the limitations this imposes on real-world applicability. The rebuttal doesn't fully alleviate this concern; simply stating the approach works *given* this knowledge doesn’t make the assumption less restrictive.\n\nReviewer 2 points out a lack of clarity regarding the motivation for the \"multiple image method\" and suggests alternative approaches. This indicates a potential weakness in the theoretical justification of the method. The reviewer also rightly calls for evaluation on more realistic OOD datasets beyond artificially corrupted images.\n\nReviewer 3 raises valid concerns about the paper's clarity, specifically regarding undefined notation ($p_{max}$) and the rationale behind choosing \"contrast\" corruption for calibration. The lack of explanation for generalizing beyond contrast corruption is a significant weakness.\n\nReviewer 4, while increasing their rating after the rebuttal, still expresses a need for more empirical/theoretical analysis to understand *why* the method works, particularly when applied to genuinely OOD data.\n\nThe authors’ responses to the reviewers address some minor points, but they fail to adequately address the core criticisms regarding the assumption of known distributions and the lack of generalization to truly OOD scenarios. The additional experiments requested by reviewers are partially addressed, but the analysis remains superficial. The paper also suffers from a lack of thorough comparison to relevant baselines, as noted by Reviewer 1 and 2. While the authors mention Ovadia et al. (2019), they don’t fully explore the applicability of other calibration-under-distribution-shift techniques.\n\nThe numerous minor points regarding notation, clarity, and missing details (calibration set sizes, equation derivations) further detract from the paper's quality. While not individually fatal, they contribute to an overall impression of insufficient rigor and polish.\n\nIn summary, the paper presents an interesting idea, but its limitations, particularly the strong assumption about known OOD distributions, and the lack of comprehensive evaluation and theoretical justification, prevent it from meeting the standards of a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents $\\alpha$VIL, a dynamic task weighting method for multitask learning. The core idea – leveraging parameter updates to estimate task importance – is intuitively appealing. However, the paper falls short of the standards expected for a top-tier conference publication due to a combination of theoretical weaknesses, limited empirical validation, and concerns raised by all three reviewers.\n\n**Strengths:** The paper is well-written and clearly explains the proposed method. The motivation for dynamic task weighting is sound, and the approach is relatively straightforward to understand. The experimental setup covers both computer vision and NLP domains.\n\n**Weaknesses:**\n\n* **Lack of Theoretical Justification:** This is the most significant weakness, highlighted by Reviewer 1. The method is presented as largely *ad hoc*, without a strong theoretical foundation. Why should parameter differences correlate with task importance? A formal analysis or even a compelling argument would significantly strengthen the paper.\n* **Limited Empirical Evidence:** The experimental results are not convincing. As pointed out by all reviewers, the improvements over baselines are often marginal and not statistically significant. The choice of baselines is also limited, particularly the \"standard multitask learning\" baseline which lacks clear definition (addressed by Reviewer 2). The results on NLU tasks are particularly mixed.\n* **Novelty Concerns:** Reviewer 2 correctly points out that dynamic task weighting is not entirely novel and that the approach bears resemblance to meta-learning techniques like MAML. The paper needs to more clearly articulate its unique contribution and differentiate itself from existing work.\n* **Methodological Questions:** Reviewer 3 raises a valid point about the relationship between $\\alpha$ and task-specific weights, questioning whether $\\alpha$ is truly necessary. The arbitrary subtraction of 1 in Algorithm 1 also lacks justification.\n* **Experimental Details & Analysis:** Several reviewers requested more detailed experimental analysis. Specifically, statistical significance tests are missing, the evolution of task weights over training is not shown, and the choice of Multi-MNIST with only one auxiliary task is questionable. The inconsistency in reporting development vs. test results (average vs. ensemble) is also a concern.\n* **Minor Issues:** Reviewer 2 also notes several stylistic issues and requests clarifications on specific implementation details. While minor, these contribute to the overall impression of a paper that needs further refinement.\n\n**Reviewer Consensus:** All three reviewers express concerns about the paper. Reviewer 1 suggests improvement is possible but highlights both theoretical and empirical shortcomings. Reviewer 2 is skeptical about the novelty and justification of the method. Reviewer 3 recommends rejection due to the lack of strong results and a need for more thorough discussion.\n\n\n\nGiven the significant theoretical gaps, the unconvincing empirical results, and the concerns raised by all reviewers, the paper, in its current form, does not meet the standards for acceptance at a top-tier conference. While the idea is interesting, it requires substantial further development and validation.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents Block Minifloat (BM), a novel floating-point representation designed for efficient DNN training, particularly targeting low-precision regimes (4-8 bits). The core idea – leveraging a shared exponent bias to reduce exponent bitwidth and enable more efficient hardware – is sound and addresses a relevant problem in the field of efficient deep learning. The authors demonstrate promising results, showing significant improvements in area and power consumption compared to FP8 while maintaining accuracy, particularly with their 6 and 8-bit formats. The hardware evaluation, including RTL synthesis, is a strong point, setting it apart from many quantization papers.\n\nHowever, the paper isn’t without its weaknesses, as highlighted by the reviewers. The novelty of simply using a shared exponent bias is limited, and the authors need to more clearly articulate *why* reducing the exponent bits is so crucial for Kulisch accumulation. While they touch on this, a more detailed explanation is needed. The initial concerns regarding the need for multiple formats (different exponent/mantissa configurations for forward/backward passes) were significant, potentially leading to substantial hardware overhead. The authors’ clarification that the hardware area results *do* account for supporting multiple formats is crucial and addresses this major concern.\n\nReviewer 2’s point about the comparison to existing work like Flexpoint is valid and should be addressed. The paper doesn’t sufficiently contextualize its contribution within the broader landscape of adaptive numerical formats. The confusion surrounding the tables and notation (particularly Table 2 and Equation 6) is also a concern, indicating a lack of clarity in presentation. Reviewer 3’s comments regarding the explanation of the exponent bias calculation and Equation 6 are also well-taken; these aspects require more detailed mathematical justification and intuition.\n\nThe emulation methodology, relying on FP32 GEMM libraries, raises questions about the validity of the accumulator settings. While the authors likely considered this, a more thorough discussion of potential convergence impacts is needed. The lack of larger network evaluations (e.g., ResNet50) is a minor weakness, but given the results on ResNet-18 and the thorough exploration of the design space, it’s not a deal-breaker.\n\nThe fact that Reviewer 2, initially hesitant, raised their score to a 7 (accept) after clarification is a positive signal. The hardware results are compelling, and the authors have responded to concerns, albeit with room for improvement in clarity and explanation. The paper's strengths – the hardware evaluation, the exploration of the design space, and the potential for significant efficiency gains – outweigh its weaknesses, especially given the iterative nature of research and the potential for further refinement in a revised version.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting application of deep learning to the ecological inference problem. The core idea – approximating the loss function to enable the use of deep learning models – is sound and potentially valuable, particularly given the importance of ecological inference in areas like political science and redistricting. The use of a real-world dataset (Maryland 2018 elections) is a significant strength.\n\nHowever, the reviewers raise substantial concerns that, taken together, lead me to believe the paper is not ready for publication in a top-tier conference *at this time*. The primary issues revolve around clarity, completeness of experimentation, and lack of convincing evidence of significant improvement over simpler baselines.\n\nHere's a breakdown of the concerns:\n\n* **Clarity and Presentation:** Reviewers 1 and 4 consistently point to a lack of clarity in the writing and organization. The paper is difficult to follow, with typos and unclear explanations of key concepts (like the input data and evaluation tasks). This is a major issue for a top-tier conference where papers are expected to be rigorously presented and easily understood by a broad audience.\n* **Incomplete Experimentation:** Reviewer 2 highlights a critical flaw: the lack of comparison to a standard multi-level model (MLM). Without this baseline, it's impossible to determine whether the improvements observed are due to the deep learning components or simply inherent in the MLM framework itself. The claim of improved crossover voting estimates also lacks sufficient justification and analysis. The experiments feel incomplete and the analysis doesn't go deep enough to support the claims made.\n* **Reproducibility:** Reviewer 2 raises valid concerns about reproducibility. Crucial experimental details (train/test splits, network architectures, latent feature definitions) are missing, making it difficult for others to verify the results.\n* **Baseline Performance:** The fact that simple linear regression outperforms the more complex deep models on the primary held-out performance metric (R^2) is a significant issue that the authors haven't adequately addressed. This needs a thorough explanation.\n* **Related Work:** Reviewer 4 correctly points out a lack of adequate citation of related work, particularly in the area of learning with label proportions (LLP). This weakens the paper's contribution and places it less firmly within the existing literature.\n* **Minor Issues:** While less critical, the numerous typos and minor writing problems detract from the paper's overall quality.\n\nReviewer 3 offers a more positive assessment, but even they suggest additional work (optimizing hyperparameters, synthetic data analysis, comparison to state-of-the-art methods) that would strengthen the paper.\n\nWhile the idea is promising, the paper feels like a preliminary exploration rather than a fully developed research contribution. The authors need to address the concerns raised by the reviewers, particularly regarding the completeness of the experiments, clarity of presentation, and justification of the claimed improvements. A more thorough comparison to relevant baselines and a more rigorous analysis of the results are essential.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to present a novel neuron reduction algorithm for deep ReLU networks, grounded in theoretical work (some published, some not). However, the reviews paint a consistent picture: the paper is fundamentally flawed in its presentation, lacks sufficient experimental validation, and relies too heavily on unpublished work, hindering reproducibility and verification.\n\nHere's a breakdown of the key issues, synthesized from the reviewer comments:\n\n* **Clarity and Presentation:** This is a pervasive problem. Reviewers 1, 2, 4, 5, and 6 all highlight significant issues with clarity, organization, and writing quality. Concepts are poorly defined, important details are relegated to footnotes, and the flow is difficult to follow. The frequent references to \"unpublished work\" exacerbate this, as reviewers cannot independently verify the theoretical foundations.\n* **Novelty and Positioning:** While the idea of reducing neurons isn't new, the authors claim novelty in *how* they do it. However, reviewers 3 and 6 point out similarities to existing architectures (highway networks, residual networks) and pruning techniques. The paper fails to adequately position itself within the existing literature and demonstrate a clear advantage over established methods.\n* **Experimental Validation:** The experiments are woefully inadequate. Reviewers 2, 3, and 5 all criticize the limited scope – toy dataset and MNIST. There's a complete lack of comparison to state-of-the-art pruning techniques (Reviewers 1, 2, 4), and the results presented aren't statistically significant (Reviewer 3). The hyperparameters aren't explored (Reviewer 5). The choice of a bottleneck architecture is also not justified (Reviewer 4).\n* **Theoretical Rigor:** While Reviewer 1 initially had concerns about the mathematical correctness, they were addressed by the authors. However, other reviewers (3, 5) raise concerns about the reliance on unpublished work and the lack of derivation for key equations. Reviewer 5 specifically points out potential errors in the equations. The connection between the theory and the algorithm isn't clearly established (Reviewer 6).\n* **Technical Issues:** Numerous typos and grammatical errors are noted across multiple reviews (1, 4, 5, 6), further detracting from the paper's credibility.\n\nThe authors responded to some reviewer comments, but the core issues remain. The paper still relies heavily on unverifiable claims, lacks a compelling experimental evaluation, and suffers from significant presentation problems. While the underlying idea *might* have potential, it's not demonstrated convincingly in this manuscript. A top-tier conference demands a higher level of rigor, clarity, and validation.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging model updates to dynamically adjust task weights in multi-task learning. The core concept of $\\alpha$VIL is intuitive and the authors clearly articulate the motivation. However, the paper falls short of the standards expected for a top-tier conference publication due to a combination of theoretical weaknesses, unconvincing empirical results, and a lack of thoroughness in addressing reviewer concerns.\n\n**Strengths:**\n\n* **Clear Writing:** The paper is well-written and easy to understand. The methodology is explained in a straightforward manner.\n* **Novelty (Potential):** The idea of using parameter updates to inform task weighting is a novel direction, although, as pointed out by reviewers, its distinctiveness from existing meta-learning approaches is questionable.\n* **Empirical Evaluation:** The authors evaluate their method on both computer vision and NLP tasks, demonstrating some breadth in their experimentation.\n\n**Weaknesses:**\n\n* **Lack of Theoretical Justification:** This is the most significant weakness. Reviewer 1 rightly points out the need for a theoretical analysis. Without it, the method feels ad-hoc and lacks a solid foundation. The reviewers question *why* this approach should work, and the paper doesn't provide a satisfying answer.\n* **Unconvincing Empirical Results:** The experimental results are not strong enough to support the claims made in the abstract. The improvements over baselines are often marginal and not statistically significant. Reviewer 2 highlights that results are often within the standard deviation of baselines. The choice of baselines is also limited, as noted by Reviewer 1.\n* **Questionable Novelty:** Reviewer 2 correctly identifies the similarity to existing meta-learning techniques (MAML, etc.). The paper doesn’t adequately differentiate itself from these established methods.\n* **Unaddressed Concerns:** The authors' response to the reviewers (not provided, but inferred from the comments) appears insufficient. Several key questions raised by the reviewers remain unanswered. For example, the intuition behind subtracting 1 from weights (Reviewer 2) and the relationship between $\\alpha$ and task-specific weights (Reviewer 3) are not adequately addressed.\n* **Experimental Design Issues:** The experimental setup, particularly the use of only one auxiliary task in Multi-MNIST, is questionable. Reviewer 3 points out that this doesn't align with the core motivation of multi-task learning. The inconsistency in reporting statistics (standard deviations for MNIST but not NLU, ensemble only for test set) also raises concerns.\n* **Minor Issues:** The style notes from Reviewer 2 highlight minor issues that, while not critical, detract from the overall polish of the paper.\n\n\n\nThe reviewers are largely aligned in their assessment. All three reviewers express concerns about the lack of theoretical justification and the weak empirical results. Reviewer 3 explicitly recommends rejection. While Reviewer 1 suggests improvement is possible, the extent of the required revisions is substantial. Reviewer 2’s comments are particularly damning, questioning the novelty and ad-hoc nature of the approach.\n\n\n\nGiven these weaknesses, the paper, in its current form, does not meet the standards for acceptance at a top-tier conference. It requires significant theoretical development, more compelling empirical validation, and a more thorough response to reviewer feedback.\n\nFinal Deci"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting and potentially impactful hypothesis: that the cerebellum functions as a decoupled neural interface (DNI), resolving the credit assignment problem in the brain. The core idea – linking cerebellar forward and inverse models to forward and backward locking in deep learning – is novel and could bridge neuroscience and AI. The authors demonstrate the feasibility of their cortico-cerebellar-DNI (CC-DNI) model across a range of tasks, including motor control and cognitive tasks like caption generation, and highlight the benefits of sparsity.\n\nHowever, despite the intriguing premise, the paper falls short of the standards expected for a top-tier conference publication. The reviewers raise valid and consistent concerns that significantly weaken the paper’s impact.\n\n**Strengths:**\n\n* **Novelty:** The central hypothesis is genuinely novel and offers a fresh perspective on cerebellar function.\n* **Broad Applicability:** The model is applied to diverse tasks, suggesting potential generality.\n* **Clear Writing:** The paper is generally well-written and accessible.\n* **Neuroscience Predictions:** The authors attempt to generate neuroscience predictions, which is a positive step.\n\n**Weaknesses (and responses to reviewer comments):**\n\n* **Lack of Comparative Analysis with DNI:** Reviewer 1 rightly points out the lack of a direct comparison between CC-DNI and the original DNI architecture. Establishing whether CC-DNI offers improvements *over* DNI is crucial to demonstrating its value as a deep learning approach. The authors’ response to this in the supplementary material doesn’t adequately address this.\n* **Insufficient Connection to Experimental Data:** This is the most significant weakness, highlighted by all three reviewers. While the authors incorporate architectural features of the cerebellum, they make limited attempts to rigorously compare their model’s predictions with experimental findings. The predictions offered are vague and lack specificity (Reviewer 3). Simply stating that the model “opens the door to explicit comparison” is insufficient.\n* **Unaddressed Biological Plausibility Concerns:** Reviewer 3 raises a critical point regarding the biological plausibility of gradient computation and transmission in the brain. The authors dismiss this as outside the scope of the paper, but it’s a fundamental issue that undermines the core premise. How can the cerebellum learn to predict gradients for a vastly larger number of cortical neurons with a limited number of Purkinje cells?\n* **Conceptual Confusion & Clarification Needed:** Reviewer 1 correctly identifies confusion arising from the presentation of the model in both feedforward and recurrent settings. The paper needs to clearly delineate these contexts and their associated locking problems. The bootstrapping aspect of the synthesiser also needs clearer explanation.\n* **Minor Errors & Inaccuracies:** The reviewers identify several inaccuracies in references and figure labeling, indicating a lack of thoroughness.\n* **Incremental Contribution (Reviewer 2):** Reviewer 2’s assessment is harsh but contains a kernel of truth. The paper largely *juxtaposes* existing knowledge rather than offering genuinely new insights. While the combination is interesting, it doesn’t necessarily warrant publication in a top-tier venue.\n\n\n\nThe authors' responses to the reviewers, while acknowledging the concerns, do not offer sufficient revisions to address the fundamental weaknesses. The paper remains largely descriptive and lacks the rigorous validation and comparative analysis needed for a high-impact publication. The lack of concrete, testable predictions and the dismissal of key biological plausibility issues are particularly problematic.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting and potentially valuable approach to achieving ensemble-like benefits within a single neural network, leveraging a multi-input multi-output (MIMO) configuration. The core idea – training independent subnetworks within a single model – is clever and the empirical results demonstrate improvements in robustness, accuracy, and calibration across several datasets. However, several concerns raised by the reviewers, and my own assessment, suggest the paper, in its current form, falls short of the standards for a top-tier conference.\n\n**Strengths:**\n\n* **Novelty:** The idea of implicitly creating an ensemble within a single network via MIMO is a novel contribution. While multi-branch architectures exist, the specific application to training independent subnetworks for robustness with minimal overhead is a worthwhile exploration.\n* **Empirical Results:** The experimental results are thorough and convincing. The authors demonstrate improvements on standard benchmarks (CIFAR10, CIFAR100, ImageNet) and their out-of-distribution variants. The investigation into subnetwork independence (Section 3) is particularly strong, as highlighted by Reviewer 2.\n* **Efficiency:** The claim of achieving ensemble benefits with minimal computational cost is appealing. The single forward pass is a significant advantage over traditional ensembles.\n* **Clarity:** The paper is generally well-written and easy to follow, as noted by Reviewer 2 and Reviewer 4.\n\n**Weaknesses & Concerns:**\n\n* **\"For Free\" Claim:** The claim of achieving benefits \"for free\" is misleading. As pointed out by Reviewer 1, there *is* a parameter increase (even if small). More importantly, Reviewer 4 correctly identifies a deceptive element in the efficiency argument. Reducing batch size by replicating the same input doesn't equate to free computation; it trades off throughput for latency. The paper needs to acknowledge this trade-off and provide a more honest comparison of computational costs (MACs, FLOPS).\n* **Related Work:** The lack of thorough discussion of related work is a significant weakness. Reviewer 2 and Reviewer 4 both highlight the omission of relevant papers on multi-input/multi-output architectures (e.g., BatchEnsemble) and multi-branch networks used for robustness and knowledge distillation. This omission diminishes the perceived originality of the work.\n* **Theoretical Understanding:** Reviewers 3 and 2 correctly point out the lack of theoretical analysis. While a full theoretical treatment might be beyond the scope of a single paper, some attempt to explain *why* independence emerges from the architecture would significantly strengthen the work. The current explanation relies heavily on empirical observation.\n* **Testing Methodology:** Reviewer 3 raises a valid concern about the discrepancy between training (M independent examples) and testing (M identical copies). This difference warrants further investigation and justification.\n* **Generalizability & Scalability:** The reviewers question the scalability of the approach. The capacity of a single network is limited, and the effectiveness of the method might diminish with larger values of M or more complex datasets (e.g., OpenImages, as suggested by Reviewer 1).\n* **Minor Issues:** Reviewer 4 notes some formatting inconsistencies, which, while minor, detract from the overall polish of the paper.\n\n\n\n**Overall Assessment:**\n\nThe paper presents a promising idea with strong empirical support. However, the misleading claims about efficiency, the insufficient discussion of related work, and the lack of theoretical grounding prevent it from reaching the standards of a top-tier conference. The reviewers’ concerns are valid and collectively paint a picture of a paper that needs substantial revision before it can be considered for publication. Addressing the concerns regarding computational cost, related work, and theoretical explanation are crucial.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents $\\alpha$VIL, a dynamic task weighting method for multitask learning. The core idea – leveraging parameter updates to estimate task importance – is intuitively appealing. However, the paper falls short of the standards expected for a top-tier conference publication due to a combination of theoretical weaknesses, unconvincing empirical results, and a lack of thorough analysis.\n\n**Strengths:**\n\n* **Clear Writing:** The paper is well-written and easy to understand. The methodology is explained clearly.\n* **Novelty (Potential):** The idea of directly using parameter updates for task weighting is a potentially novel contribution.\n* **Empirical Evaluation:** The authors evaluate on both computer vision and NLP tasks, demonstrating some breadth.\n\n**Weaknesses (and responses to reviewer concerns):**\n\n* **Lack of Theoretical Justification:** This is the most significant weakness, repeatedly highlighted by all three reviewers. The method feels *ad hoc* without a formal analysis explaining *why* it should work. Reviewer 1 explicitly requests theoretical justification, and Reviewer 2 points out the lack of justification for specific design choices (e.g., subtracting 1 from weights). The authors' response to this is not provided, making it impossible to assess if they addressed this critical point.\n* **Unconvincing Empirical Results:** The experimental results are not strong enough to support the claims. As Reviewer 1 notes, the method doesn’t consistently outperform baselines. Reviewer 2 finds the results “mixed” and not convincing, and Reviewer 3 states the improvements are “minor and sometimes lower.” The small differences, especially on MNIST, are often within the margin of error (standard deviations are reported, but not used for statistical significance testing as requested by Reviewer 3).\n* **Limited Baselines:** Reviewer 1 correctly points out that the baselines are somewhat limited. While standard multitask learning and single-task training are included, the comparison to DIW is insufficient to demonstrate a clear advantage.\n* **Connection to Existing Work:** Reviewer 2 raises a valid point about the connection to meta-learning approaches like MAML. The paper doesn't adequately differentiate itself from existing work in dynamic task weighting or meta-learning.\n* **Experimental Design Concerns:** Several specific concerns are raised about the experimental setup: task balancing in the NLU tasks (Reviewer 2), the choice of MultiMNIST with only one auxiliary task (Reviewer 3), and inconsistencies in reporting (development vs. test set results – Reviewer 3).\n* **Algorithm Clarity:** Reviewer 3 questions the distinction between $\\alpha$ and task-specific weights, suggesting a simplification of the algorithm. This indicates a potential lack of clarity in the method's design.\n* **Missing Ablation Studies:** Reviewers 2 and 3 both request ablation studies to understand the impact of different components of the algorithm and to analyze the learned task weights. The absence of these studies weakens the analysis.\n\n**Overall Assessment:**\n\nThe paper presents an interesting idea, but it is not sufficiently developed to warrant acceptance at a top-tier conference. The lack of theoretical grounding, combined with the weak and inconclusive empirical results, makes the contribution questionable. The reviewers’ concerns are valid and highlight significant shortcomings in the paper. While the authors might be able to address some of these issues with further work, the current version is not ready for publication.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents Shoot Tree Search (STS), a modification to MCTS that expands multiple nodes per simulation, aiming to balance exploration and exploitation more explicitly. The core idea is sound and potentially valuable, and the experimental results demonstrate improvements over baseline methods in challenging domains like Sokoban and Google Research Football. However, the paper faces significant challenges regarding novelty, presentation, and experimental rigor, as highlighted by the reviewers.\n\n**Strengths:**\n\n* **Interesting Idea:** The concept of controlling the bias-variance trade-off in tree search through multi-step expansion is intriguing and potentially impactful.\n* **Empirical Evaluation:** The authors provide a reasonably thorough empirical evaluation across multiple domains.\n* **Ablation Studies:** The inclusion of ablation studies is a positive aspect, demonstrating an attempt to understand the contribution of different components.\n\n**Weaknesses (and how they impact the paper):**\n\n* **Novelty Concerns (Major):** This is the most significant issue. Several reviewers (1, 4, and 5) rightly point out that multi-step expansion isn't entirely novel. Prior work, like Soemers et al. (2016) and Coulom (2006/2007), has explored similar concepts. The authors need to *clearly* articulate how STS differs from and improves upon existing approaches, particularly in the context of DNN-based value/policy function approximation. The argument that the DNN changes the trade-offs is a good starting point, but needs to be fleshed out.\n* **Presentation Quality (Major):** Reviewer 1 identifies significant issues with the manuscript's clarity and organization. Algorithm 3 & 4 are unnecessary in the main text, definitions are missing (CALCULATE_TARGET), and algorithmic details are questionable (e.g., the greedy action selection in SELECT, the weighting in UPDATE). These issues detract from the paper's readability and make it difficult to assess the technical soundness of the approach.\n* **Experimental Rigor (Moderate):** Several reviewers raise concerns about the experimental setup. Reviewer 1 requests more thorough ablation studies, specifically comparing STS and MCTS with rollout policy evaluation. Reviewer 2 points out issues with the empirical details in the appendix (incomplete runs). Reviewer 3 questions the scalability of STS compared to MCTS with a large number of simulations. Reviewer 5 highlights a potential inconsistency in Table 1 (N_p > C). These concerns suggest that the experimental validation isn't as conclusive as it could be.\n* **Lack of Intuition (Moderate):** Reviewer 2 correctly notes the lack of intuitive explanation for *why* STS should be better than standard MCTS. The paper needs to provide a more compelling justification for the proposed approach.\n* **Missing Related Work:** Reviewers 2 and 3 point out missing related work, further weakening the novelty argument.\n\n**Author Responses:**\n\nWhile the author responses aren't provided in full, the \"After Discussion\" section from Reviewer 5 indicates that some concerns were addressed, but not entirely resolved. The reviewer still recommends clarification on several points. This suggests that the revisions, while helpful, haven't fully addressed the core weaknesses.\n\n\n\n**Overall Assessment:**\n\nThe paper presents a potentially interesting idea, but it is hampered by concerns about novelty, presentation, and experimental rigor. While the authors have attempted to address some of the reviewers' concerns, significant issues remain. The paper, in its current form, doesn't meet the standards of a top-tier conference. A major revision, addressing the novelty concerns with a more thorough comparison to related work, improving the clarity and organization of the manuscript, and strengthening the experimental validation, would be necessary for reconsideration.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging Max-Q entropy search for efficient exploration in continuous RL, explicitly addressing both epistemic and aleatoric uncertainty. However, the reviewers consistently point to significant weaknesses in clarity, presentation, experimental validation, and theoretical justification.\n\n**Strengths:**\n\n* **Novelty:** The core idea of applying an information-theoretic principle (Max-Q entropy) to continuous RL exploration, specifically disentangling epistemic and aleatoric uncertainty, is novel and potentially valuable.\n* **Theoretical Foundation:** The paper attempts to ground the method in a theoretical framework, linking it to mutual information maximization.\n* **Initial Empirical Results:** The initial results on Mujoco environments show promise, with MQES outperforming baselines in some cases.\n\n**Weaknesses (and why they are critical for a top-tier conference):**\n\n* **Clarity and Presentation:** This is the most significant and consistently raised concern. Multiple reviewers (1, 2, 4, and 5) find the paper difficult to follow due to unclear notation, poorly explained concepts, and a lack of context for equations. The writing is described as \"haphazard\" and \"vague.\" A top-tier conference demands exceptional clarity.\n* **Insufficient Experimental Validation:** The experiments are preliminary and lack crucial details. Reviewers 1, 2, and 3 all highlight this. There's a lack of hyperparameter ablation, limited baselines (missing OAC and VIME, as pointed out by reviewers 4 and 5), and a questionable choice of horizon (100 vs. the standard 1000). The small number of seeds (5) also raises concerns about statistical significance.\n* **Theoretical Gaps and Justifications:** Several reviewers (1, 3, and 5) identify issues with the theoretical derivations and justifications.  Specific concerns include the update rule for the policy (keeping covariance unchanged), the meaning of mutual information in a deterministic environment, and inconsistencies in notation.\n* **Limited Generalizability:** Reviewer 1 questions the method's applicability as a general module for existing entropy-regularized algorithms like SAC or TD3.\n* **Conceptual Concerns:** Reviewer 5 raises a valid point about the reliance on Q* for exploration, questioning its effectiveness in sparse-reward settings.\n* **Minor Errors:** Numerous minor errors and inconsistencies are pointed out across the reviews (e.g., equations, notation, terminology).\n\n**Author Responses (Not Provided):** While author responses could potentially address some of these concerns, the sheer number and severity of the issues suggest that a substantial revision would be required. Without seeing the responses, it's difficult to assess the likelihood of successful mitigation.\n\n**Overall Assessment:**\n\nThe paper presents a potentially interesting idea, but it is currently underdeveloped and suffers from significant flaws in clarity, experimental validation, and theoretical justification. These flaws are particularly problematic for a top-tier conference, where papers are expected to be rigorously presented, thoroughly validated, and clearly explained. The reviewers' comments are overwhelmingly negative, and the concerns raised are substantial. While the idea has merit, the current manuscript is not ready for publication.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to present a novel neuron reduction algorithm for deep ReLU networks, grounded in theoretical work (some published, some not). However, the reviews paint a consistent picture: the paper is fundamentally flawed in its presentation, lacks sufficient experimental validation, and relies too heavily on unpublished work, hindering reproducibility and verification.\n\nHere's a breakdown of the key issues, synthesized from the reviewers' comments:\n\n* **Lack of Clarity and Rigor:** Multiple reviewers (1, 2, 4, 5, 6) highlight significant issues with clarity, organization, and mathematical rigor. Definitions are missing, explanations are vague, and important concepts are relegated to footnotes. The core theoretical result (Section 4.1) isn't presented as a rigorous theorem.\n* **Over-Reliance on Unpublished Work:** The paper heavily references \"unpublished work,\" making it impossible for reviewers to independently verify the foundational theory. This is a major red flag for a top-tier conference. Reviewer 5 rightly points out that the paper is effectively asking readers to trust the authors without sufficient evidence.\n* **Insufficient Experimental Validation:** The experiments are weak. They are limited to a toy dataset and MNIST, which are insufficient to demonstrate the algorithm's effectiveness on more complex, real-world problems. Crucially, there's *no* comparison to state-of-the-art pruning techniques (Reviewers 1, 2, 4, 5). This makes it impossible to assess the contribution of the proposed method.\n* **Incremental Contribution & Similarity to Existing Work:** Reviewer 3 notes the similarity to highway networks, and Reviewer 6 points out that the initial pruning steps (removing dead neurons, combining always-active neurons) are not novel. The overall contribution appears incremental at best.\n* **Technical Issues & Typos:** Numerous typos and grammatical errors (identified by multiple reviewers) further detract from the paper's credibility. Some reviewers also identified potential errors in the equations themselves (Reviewer 5).\n* **Inconsistencies & Confusing Claims:** Reviewer 7 points out inconsistencies in the claims made in the introduction versus the methods used in the experiments. The claim of \"exact preservation\" is undermined by the use of approximation methods.\n\nWhile Reviewer 1 acknowledges some improvement after author responses, the fundamental issues remain. The authors have addressed some mathematical concerns, but the broader problems of clarity, validation, and reliance on unpublished work are not adequately resolved. The potential theoretical interest, as noted by Reviewer 1, is overshadowed by the significant shortcomings in presentation and experimental evaluation.\n\nGiven the severity and consistency of the criticisms, this paper is not suitable for publication in a top-tier conference. It requires substantial revisions, including a more rigorous theoretical presentation, a comprehensive experimental evaluation with comparisons to existing methods, and a reduction in reliance on unpublished work.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents PERIL, a method combining meta-RL and imitation learning to improve sample efficiency and adaptation in sparse reward environments. The core idea – leveraging demonstrations to guide meta-RL – is sound and addresses a known limitation of many meta-RL algorithms. The experimental results demonstrate promising performance gains over strong baselines, particularly in multi-task settings. However, the paper suffers from significant weaknesses in clarity, presentation, and justification of design choices, ultimately hindering its impact and reproducibility.\n\n**Strengths:**\n\n* **Relevant Problem:** Combining meta-RL and IL is a valuable research direction, addressing the challenges of sparse rewards and sample efficiency.\n* **Promising Results:** The empirical results show PERIL outperforms baselines, suggesting the approach has merit. The multi-task generalization results are particularly encouraging.\n* **Well-Documented Implementation:** The authors provide details on hyperparameters and reward functions, which aids reproducibility (though the clarity issues complicate this).\n\n**Weaknesses (and why they are critical):**\n\n* **Clarity and Notation:** This is the most significant issue. Reviewers 1, 2, and 4 all highlight substantial confusion regarding the mathematical formulation, loss functions, and the interplay between different components. The mixing of problem setting and approach (as pointed out by Reviewer 4) is a fundamental flaw in presentation. The inconsistent notation (k/K for demonstrations/rollouts, VE/VAE) further exacerbates this. A top-tier conference demands exceptional clarity.\n* **Justification of Complexity:** PERIL is a complex method with numerous components and loss terms. The paper doesn't adequately justify *why* each component is necessary. The ablations provided (primarily on the auxiliary loss) are insufficient to understand the contribution of each element. Reviewer 1’s question about whether a simpler approach would suffice is a critical one that the paper doesn’t answer.\n* **Strong Assumption about Expert Data:** The assumption that the expert policy can condition on the learned latent variable *z* is highly restrictive and unrealistic. This significantly limits the applicability of the method.\n* **Incomplete Related Work:** The related work section is lacking, particularly regarding context-based meta-RL and prior work combining meta-RL and IL (as highlighted by Reviewers 2 and 3). Failing to adequately position the work within the existing literature diminishes its perceived originality.\n* **Experimental Limitations:** The experiments, while promising, have limitations. The tasks used don't fully demonstrate the need for sophisticated exploration, and the noisy BC baseline is questionable. The lack of statistical significance testing and multiple seeds is a serious concern.\n* **Unaddressed Reviewer Concerns:** The reviewers raised numerous specific questions and suggestions for improvement. A strong rebuttal would address these points comprehensively. The fact that so many concerns remain unaddressed suggests a lack of responsiveness to critical feedback.\n\n\n\n**Reviewer Consensus:**\n\nThere is a strong consensus among the reviewers that the paper has potential but is not ready for publication in its current form. All reviewers identify significant weaknesses, particularly regarding clarity and justification. Reviewer 3's recommendation of rejection is the strongest, but even reviewers who see more promise (1, 2, and 4) express serious reservations.\n\n\n\nGiven the severity of the clarity issues, the questionable assumption about expert data, the incomplete related work, and the lack of a compelling justification for the method's complexity, I believe the paper falls short of the standards for a top-tier conference. While the idea is promising, the current presentation and analysis are insufficient to convince the community of its value.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles a relevant and important problem in distributed machine learning – distributed mean estimation – and offers a novel approach leveraging lattice theory to improve upon existing bounds, particularly in scenarios where the input vectors are clustered around a non-zero mean. The core idea of using lattice quantization to reduce communication cost while maintaining accuracy is sound and well-motivated. The authors also provide lower bounds, demonstrating the optimality of their approach, and a practical extension using cubic lattices.\n\nThe reviewers generally agree on the paper's strengths: the problem is important, the idea is interesting, and the work is well-written. However, there are also several points of concern raised across the reviews.\n\n**Strengths:**\n\n* **Novelty:** The focus on input variance rather than input norm is a significant contribution, addressing a limitation of previous work.\n* **Theoretical Rigor:** The provision of both upper and lower bounds strengthens the theoretical foundation of the work.\n* **Practicality:** The extension to cubic lattices makes the approach more accessible and potentially useful in real-world applications.\n* **Clear Presentation:** Reviewers 3 and 4 specifically commend the paper's clarity and motivation.\n\n**Weaknesses & Concerns:**\n\n* **Incremental Nature (Reviewer 2):** While the result is \"clean,\" the individual techniques used (lattice quantization, hashing, reduction to l-infinity) are not entirely new. The value lies in the combination and application to this specific problem. This isn't necessarily a fatal flaw, but it does suggest the paper might be better suited for a workshop than a top-tier conference if the improvement isn't substantial enough.\n* **Alternative Approaches (Reviewer 1):** The suggestion of iterative refinement (subtracting the current estimate) is a valid point. The authors should address this in their response, explaining why their approach is superior or under what conditions it is preferable.\n* **Computational Complexity of Lattices (Reviewer 2):** The reliance on computationally intractable random lattices is a concern. The cubic lattice extension mitigates this, but the logarithmic factor loss should be clearly acknowledged and discussed.\n* **Minor Technical Issues:** Numerous minor errors and typos were identified across the reviews (e.g., missing \"\\n\", \"thanto\", incorrect notation, unclear explanations). While individually small, these suggest a lack of thoroughness in the final polishing of the manuscript.\n* **Limited Experiments (Reviewer 3):** The experiments are limited to n=2, which is insufficient to demonstrate the scalability and practical benefits of the approach.\n* **Algorithm-Bound Discrepancy (Reviewer 4):** The fact that the practical algorithm doesn't achieve the optimal bounds is a significant drawback. The authors need to explain this discrepancy and its implications.\n* **Definition Clarity (Reviewer 4):** The ambiguity in the definition of the packing radius needs to be addressed.\n\n**Author Responses:**\n\nThe author responses are not provided, which makes a complete assessment difficult. However, the quality of the responses will be crucial. Addressing the concerns raised by the reviewers, particularly regarding the iterative refinement approach, the limitations of the experiments, and the discrepancy between theoretical bounds and the practical algorithm, will be essential for acceptance.\n\n\n\n**Overall Assessment:**\n\nThe paper presents a solid contribution to the field of distributed mean estimation. The theoretical results are strong, and the practical extension is a valuable addition. However, the concerns raised by the reviewers, particularly regarding the incremental nature of the work, the limited experiments, and the discrepancy between theory and practice, cannot be ignored.\n\nGiven the high standards of a top-tier conference, the paper, as it stands, falls slightly short. A strong revision addressing the reviewers' concerns, with a focus on more extensive experiments and a clear explanation of the algorithm-bound discrepancy, could elevate the paper to an acceptable level. Without evidence of such a revision, the paper is better suited for a workshop or a journal publication.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles a challenging and relevant problem – automatic music accompaniment – using a modern deep learning approach (CycleGANs). The idea of framing the problem as an image-to-image translation task using Mel-spectrograms is reasonable and builds upon existing work in MIR. The authors attempt to address the lack of paired data by leveraging CycleGANs, and the inclusion of both objective and subjective evaluation is commendable. However, the paper suffers from several significant weaknesses that, in my assessment, prevent it from reaching the standard required for a top-tier conference.\n\n**Strengths:**\n\n* **Relevant Problem:** Automatic accompaniment is a valuable goal with potential applications for musicians and music producers.\n* **Sound Approach:** Using CycleGANs on Mel-spectrograms is a logical starting point, leveraging established techniques.\n* **Mixed Evaluation:** The attempt to combine objective metrics with human evaluation is a positive aspect.\n* **Clear Writing (Generally):** The paper is mostly well-written and easy to follow, despite the technical details missing.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Rigorous Experimentation:** The most significant flaw is the insufficient experimental rigor. The reviewers consistently point out missing details and crucial control experiments. Specifically:\n    * **Demucs Impact:** The reliance on Demucs for source separation without a thorough evaluation of its performance *on the FMA dataset* is a major issue. The potential for bleed-through and artifacts significantly impacts the validity of the results. A comparison to training solely on MusDB is essential.\n    * **Reproducibility:** The lack of details regarding model architecture, hyperparameters, training procedures, and the exact dataset composition (including song IDs) hinders reproducibility.\n    * **Evaluation Justification:** The choice of evaluation metrics (STOI, FID, and the human-derived attributes) lacks sufficient justification. Why are these metrics appropriate for evaluating *musical* quality?\n* **Missing Technical Details:** The paper omits critical technical details, such as the phase retrieval method from Mel-spectrograms. This omission makes it difficult to understand the quality limitations and potential sources of artifacts. The reviewers rightly point out the audible phase distortion.\n* **Overstated Claims:** The claim of being the first to treat music audio as images is demonstrably false. The authors downplay the extensive prior work in MIR that utilizes spectrograms as image-like representations.\n* **Limited Scope & Generalizability:** The focus on only bass, drums, and vocals limits the scope and generalizability of the findings. While a focused approach is acceptable, the authors don't adequately address this limitation.\n* **Weak Justification for Design Choices:** Several design choices, such as the quantization of spectrograms and the binarization of human ratings, are not adequately justified.\n* **Unclear Motivation:** The motivation for the approach, particularly the claim about the superiority of raw audio representation for artistic purposes, is not convincingly presented. The counterarguments about symbolic music representation are not adequately addressed.\n\n\n\nThe reviewers’ comments are largely consistent in identifying these weaknesses. Reviewer 1 provides a particularly thorough critique, highlighting the lack of motivation, missing details, and questionable claims. Reviewer 2 offers constructive suggestions for improvement, while Reviewer 3 focuses on the importance of evaluating the impact of Demucs and addressing the phase retrieval issue.\n\nGiven these substantial shortcomings, the paper, in its current form, does not meet the standards for acceptance at a top-tier conference. While the idea is promising, the lack of rigorous experimentation, missing technical details, and overstated claims undermine the validity and impact of the work. A major revision addressing these issues would be necessary for reconsideration.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting approach to learning layout representations using a large-scale dataset of presentation slides and a Transformer-based pre-training strategy. The creation of a dataset with over 1 million slides is a significant contribution, addressing a key bottleneck in the field – the scarcity of labeled layout data. The idea of adapting self-supervised learning techniques from NLP to the graphic design domain is also promising. However, the paper suffers from several weaknesses that, in my assessment, prevent it from reaching the standards of a top-tier conference.\n\n**Strengths:**\n\n* **Large-Scale Dataset:** The dataset is a valuable resource for the community.\n* **Novel Application of Transformers:** Applying Transformers to layout representation is a novel idea.\n* **Sound Core Idea:** The pre-training approach, inspired by BERT, is conceptually sound.\n\n**Weaknesses (and why they are critical):**\n\n* **Insufficient Dataset Details:** Reviewers 1 and 2 rightly point out the lack of crucial dataset statistics. Knowing the distribution of elements per slide, completeness of properties, and parsing methodology is essential for understanding the dataset's characteristics and the model's performance.\n* **Weak Evaluation:** This is the most significant weakness. The evaluation relies heavily on tasks that feel like byproducts of the pre-training process rather than independent, challenging layout understanding problems. The baseline (decision tree) is too weak to provide meaningful insights. The layout retrieval evaluation is subjective, and the auto-completion task is overly simplistic.\n* **Missing Baselines:** The lack of comparison to relevant prior work (Neural Design Network, LayoutGAN, Cao et al. (SIGGRAPH 2019), READ, and GNNs) is a major omission. Without these comparisons, it’s impossible to assess the true contribution of CanvasEmb. Reviewers 2 and 3 emphasize this point strongly.\n* **Lack of Ablation Studies/Analysis:** There's no exploration of *why* the Transformer architecture is beneficial. A comparison to simpler models or ablation studies to understand the contribution of different components would be valuable. The reviewers question the necessity of such a complex model given the task.\n* **Clarity Issues:** Reviewer 2 identifies several clarity issues, including the confusing use of \"pre-trains\" and the lack of a visual explanation of the Transformer architecture.\n* **Questionable Task Definition:** The \"image captioning\" task is essentially binary classification, which doesn't fully leverage the model's potential for layout understanding.\n* **Related Work Deficiencies:** The related work section lacks a clear positioning of the current work relative to existing literature. It jumps between references without adequately explaining the contributions and differences.\n\n\n\nThe authors respond to the reviewers, but the responses don't fully address the core concerns. They acknowledge the need for more detailed dataset statistics and baseline comparisons, but don't commit to including them in a revised version. They also defend their evaluation tasks, but don't offer a convincing argument for why they are sufficient to demonstrate the model's capabilities.\n\n\n\nIn conclusion, while the paper presents a promising idea and a valuable dataset, the weak evaluation, missing baselines, and lack of detailed analysis prevent it from meeting the high standards of a top-tier conference. The paper feels incomplete and requires substantial revisions to demonstrate its true contribution to the field.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – the limitations of GNNs on disassortative graphs – and proposes a global attention mechanism based on spectral filters to address it. The core idea is sound and well-motivated by the shortcomings of local aggregation schemes. The experimental results are promising, showing strong performance on disassortative graphs and competitive performance on assortative ones. However, several significant concerns raised by the reviewers, particularly regarding novelty and computational complexity, lead me to believe this paper does not meet the bar for a top-tier conference like ICLR.\n\n**Strengths:**\n\n* **Relevant Problem:** Addressing the limitations of GNNs on disassortative graphs is a crucial area of research.\n* **Clear Motivation:** The paper clearly articulates the problem and the rationale behind the proposed solution.\n* **Strong Empirical Results:** The model demonstrates superior performance on disassortative graphs compared to existing methods.\n* **Lucid Exposition:** Reviewer 2 highlights the paper's clarity, which is a significant positive.\n\n**Weaknesses & Concerns:**\n\n* **Limited Novelty (Major Concern):** This is the most significant issue. Reviewers 1 and 4 both strongly question the novelty of the approach. The idea of using attention to capture long-range dependencies is not new (as pointed out with references to Non-Local GNNs and ChevNet). The authors' attempts to differentiate their work from existing approaches (e.g., arguing ChevNet suffers from over-parameterization rather than expressiveness) are not entirely convincing to the reviewers. The contribution of the spectral filter-based global attention seems incremental at best.\n* **Computational Complexity (Major Concern):** Reviewers 1 and 4 raise serious concerns about the claimed computational complexity. The need to compute a full matrix (phi) in Eq. (3) inherently leads to a complexity of at least O(V^2), which contradicts the authors' claims of achieving linear complexity. The post-discussion update from Reviewer 4 reinforces this concern, questioning the feasibility of the claimed complexity and the clarity of alternative implementations. This is a critical flaw, as scalability is paramount for GNNs.\n* **Small Datasets:** Reviewer 3 and 4 point out the use of relatively small datasets for evaluation, particularly for disassortative graphs. This limits the generalizability of the findings and makes it difficult to assess the model's performance on larger, more realistic graphs.\n* **Lack of Theoretical Analysis:** Reviewer 2 correctly notes the absence of theoretical results. While not always essential, theoretical grounding can significantly strengthen a paper's contribution.\n* **Limited Evaluation Tasks:** The evaluation is limited to node classification. Exploring other tasks like link prediction or graph reconstruction could provide a more comprehensive assessment of the model's capabilities.\n\n\n\nThe author responses, while detailed, do not fully address the core concerns raised by the reviewers. The rebuttal doesn't convincingly demonstrate a significant reduction in computational complexity or a substantial novelty beyond existing approaches. The arguments regarding ChevNet's limitations are not universally accepted by the reviewers.\n\n\n\nConsidering the significant concerns regarding novelty and computational complexity, coupled with the limited scale of the experiments, the paper, in its current form, does not meet the high standards required for acceptance at a top-tier conference like ICLR.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting application of Fourier convolutional networks to the challenging problem of 3D snapshot microscopy and lensless imaging. The core idea – leveraging global convolutions in the Fourier domain to handle the wide point spread functions inherent in these techniques – is sound and well-motivated. The authors demonstrate promising results, particularly in simulation, showing that their FourierNet architecture outperforms UNets in optimizing optical encoders for 3D snapshot microscopy. They also show competitive performance on a lensless imaging dataset.\n\nHowever, the paper faces several significant criticisms from the reviewers, centering around novelty and the strength of the empirical validation. A recurring theme is the connection to prior work, specifically Rippel et al. (2015), which already explored learning multiplicative weights in the Fourier domain. While the authors extend this idea to *global* convolutions, the reviewers rightly question whether this extension is sufficiently novel to warrant publication in a top-tier conference like ICLR. Several reviewers (4, 7) explicitly point this out.\n\nThe empirical evaluation is another area of concern. The reliance on simulated data for the 3D snapshot microscopy experiments is acknowledged as a limitation by the authors, but it significantly weakens the impact of the work. Real-world optical systems introduce aberrations and calibration errors that are not captured in simulation, and the performance gains observed in simulation may not translate to practical applications. The lensless imaging experiment provides some real-world validation, but reviewers (4, 5) raise valid concerns about the fairness of the comparison and the resource requirements. The inconsistencies in the speed comparisons (Table 3) also raise questions about the thoroughness of the evaluation.\n\nThe reviewer comments reveal a spectrum of opinions. Reviewers 1, 2, and 3 are cautiously optimistic, improving their ratings after the rebuttal. Reviewer 6 is generally positive but identifies some major issues that need addressing. Reviewers 4, 5, and 7 are more critical, highlighting the lack of novelty and the limitations of the empirical evaluation. Reviewer 3’s comment that the paper would “shine in an application-specific conference or journal” is particularly insightful.\n\nConsidering the balance of these reviews, the paper falls short of the high bar for acceptance at a top-tier conference like ICLR. While the technical work is solid and the application is interesting, the limited novelty and the lack of compelling real-world validation are significant drawbacks. The paper feels more like a successful application of existing techniques to a new problem than a groundbreaking contribution to machine learning.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents a regularized inversion approach to Target Propagation (TP), aiming to make it more practical for training recurrent neural networks. While the idea is interesting and the authors attempt to bridge the gap between the original TP concept and modern differentiable programming frameworks, the paper falls short of the standards expected for a top-tier conference publication.\n\nThe primary weakness, consistently highlighted by all reviewers, is the lack of compelling evidence for the method’s superiority. Reviewers 1, 3, and 4 all point to insufficient or unconvincing experimental results. Reviewer 3 specifically notes the low accuracy on image datasets, questioning the generalizability of the approach. Reviewer 4 emphasizes the need for more extensive experimentation and comparison with existing RNN optimization techniques. Even Reviewer 2, the most positive reviewer, acknowledges that the numerical results are \"a bit confusing.\"\n\nBeyond the empirical shortcomings, the theoretical contributions are also questioned. Reviewer 1 raises concerns about the claimed novelty and the algorithm's limitations (e.g., requiring weight transport, not allowing decoupled updates). They also challenge the authors' clarification of the 0-th order approximation. Reviewer 4 delves deeper into the theoretical aspects, pointing out a loose bound in Lemma 3.1 and a lack of thorough exploration of the relationship to Gauss-Newton methods. The questions raised regarding the regularization and its connection to Gauss-Newton are particularly insightful.\n\nThe authors respond to reviewer comments, but the responses don't fully address the core concerns. While they provide clarifications and additional details, they don't present new results or significantly strengthen the arguments for the method's advantages. The paper feels like a preliminary exploration of an idea rather than a fully developed and rigorously evaluated contribution.\n\nThe novelty is also marginal. While the regularized inversion approach is a specific implementation choice, it doesn't fundamentally alter the core challenges associated with TP. The reviewers consistently rate the novelty as low to moderate (mostly 2s and some 3s).\n\nThe paper is generally well-written and clear, as noted by Reviewers 2 and 4, and the correctness is generally acceptable (mostly 3s and a 4). However, clarity and correctness alone are insufficient for acceptance at a top-tier conference. The lack of strong theoretical justification and compelling empirical evidence outweigh the positive aspects.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – applying boosting techniques from supervised learning to reinforcement learning, specifically aiming for state-space independent sample complexity. The core concept of leveraging weak learners to iteratively improve policy accuracy is promising. However, the execution and presentation fall significantly short of the standards expected for a top-tier conference.\n\nThe reviewers consistently highlight issues with clarity, readability, and notation. Reviewers 4, 5, and 6 are particularly scathing in their assessment of the paper's density and lack of explanation. The sheer number of undefined notations and the terse writing style make it extremely difficult to follow the authors' reasoning and understand the contributions. Even reviewers who found the core idea interesting (1, 3, 6) acknowledge the need for substantial rewriting and clarification.\n\nThe novelty is also questioned. Reviewer 2 points out the similarity to Hazan and Singh (2021) and criticizes the lack of discussion regarding the differences between boosted contextual bandits and boosted RL. While the authors claim state-space independence, the significance of this achievement isn't adequately contextualized within existing RL literature, particularly concerning state compression techniques (as noted by Reviewer 2).\n\nThe lack of empirical validation is a significant weakness. While the paper is primarily theoretical, the absence of even preliminary experiments, as suggested by Reviewers 2 and 5, hinders the ability to assess the practical relevance and effectiveness of the proposed algorithm. The comparison to Hazan and Singh (2021), which *does* include experiments, further emphasizes this gap.\n\nReviewer 3 offers a \"weak accept,\" but even this is contingent on addressing the identified weaknesses. The other reviewers are far more critical. The emergency review status of some reviews (4 & 5) suggests the paper was flagged for potential issues early on.\n\nThe authors have responded to reviewer comments (Reviewer 1 notes improvement after responses), but the fundamental issues of clarity and presentation remain pervasive based on the subsequent reviews. A rewrite focusing on a subset of the problem, as suggested by Reviewer 1, might be a viable path forward, but it's unlikely that a single revision cycle could address all the concerns raised. The paper, in its current form, is simply too difficult to understand and lacks sufficient justification for its contributions to warrant acceptance at a top-tier conference. The numerous minor comments from Reviewer 6, while individually small, collectively paint a picture of a paper that hasn't been carefully polished and vetted.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The core contribution lies in the platform itself, aiming to provide a user-friendly and efficient tool for researchers and practitioners. However, the reviewers consistently raise concerns about the novelty and significance of the methodological contributions, particularly the Believer-Penalizer (BP) strategy.\n\nHere's a breakdown of the key points from the reviews:\n\n* **Novelty:** The majority of reviewers (1, 2, 3, and 4) agree that the novelty is limited. While the platform is well-engineered and potentially useful, it largely assembles existing techniques. The BP strategy is considered a relatively straightforward idea, lacking strong theoretical justification. Reviewer 2 explicitly states it's more of a \"software design doc and manual\" than a scientific paper.\n* **Empirical Validation:** The empirical results are also a point of contention. Reviewer 4 strongly criticizes the overstated claims regarding BP's performance, pointing out inconsistencies between the reported results and the figures. Reviewer 1 also notes a \"mixed picture\" in Figure 5. While Reviewer 5 finds the empirical support adequate, the other reviewers are less convinced.\n* **Correctness & Clarity:** The paper is generally well-written and clear, with minor issues identified by Reviewer 5. Reviewer 2 points out a misleading statement regarding the platform's applicability to different numbers of objectives.\n* **Significance:** The reviewers are divided on the significance. Reviewer 5 views the problem as well-motivated and the platform as a valuable contribution. However, the others emphasize the lack of substantial methodological advancement. Reviewer 2 explicitly states it's not suitable for a top-tier conference like ICLR.\n\nThe author responses haven't fundamentally addressed the core concerns regarding novelty and empirical validation. While they may have clarified some points, they haven't provided compelling evidence to demonstrate a significant methodological contribution beyond the platform itself. The platform's value is acknowledged, but a top-tier conference prioritizes novel and impactful research contributions. The BP strategy, as presented, doesn't meet that threshold. The concerns about overstated empirical results also weaken the paper's case.\n\nThe existence of a similar, earlier work (Tian et al., arXiv:2104.05959) cited by Reviewer 2 further diminishes the novelty. While not a direct replication, it highlights that the core ideas are not entirely new.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – accelerating sampling from diffusion models – and proposes a novel approach, DDSS, combined with a flexible sampler family, GGDP. The core idea of differentiating through sample quality to optimize the sampler is intriguing and demonstrably effective, as evidenced by the strong FID scores achieved with a small number of steps. The improvements over DDPM and DDIM baselines are significant, particularly in the low-step regime.\n\nHowever, the paper isn't without its weaknesses. A recurring theme in the reviews is a lack of theoretical grounding and clarity. While Theorem 1 addresses some initial concerns about the validity of the approach, reviewers rightly point out the need for a deeper understanding of *why* it works. Is the improvement solely due to the optimization process, the non-Markovian structure, or a combination of both? The paper doesn’t fully disentangle these factors. The reliance on a perceptual loss, while empirically successful, raises questions about the statistical properties of the generated samples and whether they truly reflect the underlying DDPM distribution.\n\nThe reviewers also raise valid points about the limited scope of the experiments. While the results on CIFAR-10 and ImageNet 64x64 are compelling, the lack of evaluation on larger, more challenging datasets (LSUN, CelebA, full-resolution ImageNet) weakens the claim of generalizability. The appendix samples on ImageNet 64x64 also raise concerns about robustness.\n\nThe rebuttal responses appear to have addressed some of the reviewers’ concerns, particularly regarding the justification for using a perceptual loss and the validity of the approach. Reviewer scores have generally increased post-rebuttal, indicating a positive impact from the authors’ responses. However, the fundamental concerns about theoretical justification and the limited experimental scope remain.\n\nConsidering the strengths and weaknesses, this paper falls into a difficult category. It presents a novel and effective technique, but lacks the theoretical depth and comprehensive evaluation expected of a top-tier conference publication. The empirical results are strong enough to warrant publication, but the lack of a more thorough theoretical analysis and broader experimental validation prevent it from being a clear \"accept.\" The paper is a good contribution, but not a groundbreaking one.\n\nGiven the improvements made in response to the reviews and the significant empirical results, I believe the paper is now at a level where its potential benefits outweigh its remaining shortcomings. However, the authors should be strongly encouraged to address the theoretical gaps and expand the experimental evaluation in the camera-ready version.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in multi-modal learning – the tendency of models to become overly reliant on a single modality. The core idea of identifying and mitigating this \"greedy\" behavior is conceptually sound and potentially valuable. However, the reviewers consistently raise concerns about the strength of the evidence supporting the claims, the limited scope of experimentation, and the lack of thorough comparison to existing work.\n\nHere's a breakdown of the key issues:\n\n* **Weak Validation of Claims:** The central claim that greedy learning negatively impacts generalization isn't convincingly demonstrated. Reviewers 1, 2, and 4 all point out that the experimental results, particularly on ModelNet40 and NVGesture, don't show statistically significant improvements with the proposed method. The reliance on Colored MNIST, a potentially biased dataset, as a primary source of support is also criticized (Reviewer 4).\n* **Insufficient Theoretical Grounding:** Several reviewers (6, 4) question the theoretical justification for the proposed method. The connection between conditional learning speed and conditional utilization rate is not fully convincing, and the reasoning behind the re-balancing step is unclear. The lack of a theoretical analysis explaining *why* greedy learning occurs and *why* the proposed method addresses it is a significant weakness.\n* **Limited Experimental Scope:** The experiments are largely confined to visual-only datasets. Reviewers 3 and 6 rightly point out the need for evaluation on more natural multi-modal datasets (e.g., audio-visual) to demonstrate the generalizability of the approach.\n* **Inadequate Comparison to Related Work:** The paper doesn't adequately position itself within the existing literature. Reviewers 6 and 7 highlight the omission of relevant work on modality bias and imbalance, and the lack of comparison to state-of-the-art methods.\n* **Implementation Details & Parameter Sensitivity:** Concerns are raised about the clarity of the re-balancing step (Reviewer 5) and the sensitivity of the method to the hyperparameter α (Reviewer 7). The lack of an ablation study for this parameter is a notable omission.\n* **Inconsistencies in Results:** Reviewer 2 points out discrepancies in the reported results between the initial submission and the revision, raising concerns about the rigor of the experimental process.\n\nThe author responses address some of the reviewers' concerns, but they don't fully resolve the fundamental issues. The revisions improve the writing and readability, but the core weaknesses in the validation and theoretical justification remain. While the idea is promising, the paper, in its current form, doesn't meet the standards of a top-tier conference. The evidence presented is not strong enough to convince the research community of the validity and significance of the proposed approach.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents a significant theoretical contribution to understanding the optimization landscape of two-layer ReLU networks with L2 regularization. The core result – a complete characterization of global optima via a convex program – is a strong advancement over prior work (specifically Pilanci & Ergen 2020), which only identified a single optimal solution. The additional results regarding stationary points, optimality checking, continuous paths to optima, and the absence of spurious valleys further strengthen the paper's impact.\n\nThe reviewers largely agree on the paper's strengths. Reviewers 2 and 4 are particularly enthusiastic, recommending acceptance and highlighting the novelty and significance of the work. Reviewer 1 acknowledges the novelty and importance of the theoretical results, while Reviewer 3, though more critical, ultimately maintains a positive assessment after considering the rebuttal.\n\nThe weaknesses identified are relatively minor. The limitation to two-layer networks is acknowledged by the authors and reviewers as a natural starting point for this line of inquiry. The typos and minor technical issues raised by Reviewers 1 and 3 are typical of pre-publication drafts and are easily addressable. The questions about extending the results to other activation functions and deeper networks (raised by Reviewers 2 and 3) are valid directions for future research, but do not detract from the current contribution. The missing references pointed out by Reviewer 2 are also easily rectified.\n\nThe fact that the authors don't rely on duality-based approaches, as highlighted by Reviewer 4, is a notable strength, offering a complementary perspective to existing convex relaxations of neural network training. The polynomial-time algorithm for checking global optimality is a practical contribution, even if limited in scope.\n\nThe reviewers' scores consistently place the paper in the acceptance range. The average correctness score is 3.5, and the average technical novelty and significance score is 3.25. While not exceptionally high, these scores are indicative of a strong, impactful paper. The rebuttal addressed the concerns of Reviewer 3, further solidifying the paper's quality.\n\nConsidering the strong positive reviews, the significance of the theoretical results, and the relatively minor nature of the identified weaknesses, this paper is well-suited for publication in a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents a theoretical analysis of the Feedback Alignment (FA) algorithm in deep linear networks, focusing on convergence guarantees and implicit regularization. The reviewers are largely in agreement that the paper is technically correct and well-written. However, there's significant concern regarding the practical significance of the results due to the strong assumptions, particularly the reliance on spectral initialization.\n\nHere's a breakdown of the key points from the reviews:\n\n* **Strengths:** The paper rigorously analyzes FA under specific conditions, proving convergence and identifying an interesting \"implicit anti-regularization\" phenomenon. Reviewer 3 particularly highlights the novelty of this observation. The numerical experiments, while limited, support the theoretical findings.\n* **Weaknesses:** The most significant weakness is the strong assumption of spectral initialization. Reviewers 2 and 4 point out that this initialization is rarely used in practice and may not generalize to more common initialization schemes. The analysis simplifies the problem considerably, potentially oversimplifying the dynamics and limiting the broader applicability of the results. Reviewer 1 questions whether the one-dimensional dynamics adequately capture the behavior of higher-dimensional systems. The connection to practical advantages over Gradient Descent (GD) remains unclear.\n* **Novelty:** While the theoretical results are novel within the constrained setting, the reviewers disagree on the overall significance. Reviewer 3 believes the novelty is substantial, while Reviewers 1, 2, and 4 consider it marginal. The novelty is heavily tied to the specific conditions under which the results hold.\n* **Reviewer Disagreement:** There's a clear split in opinion. Reviewer 3 strongly recommends acceptance, while Reviewer 4 leans towards rejection. Reviewers 1 and 2 are more neutral, acknowledging the technical correctness but questioning the significance.\n\nConsidering these points, the paper falls into a difficult category. It presents solid theoretical work, but its relevance to the broader machine learning community is questionable. The authors have identified an interesting phenomenon, but the strong assumptions limit its practical impact. The paper doesn't convincingly demonstrate that the insights gained from this simplified setting translate to more realistic scenarios. The authors' response to the reviewers would be crucial. If they can convincingly address the concerns about spectral initialization and provide some evidence (even preliminary) that the observed phenomena might hold under more common initialization schemes, the paper's value would increase significantly. However, based on the current state of the paper and the reviewers' comments, the limitations outweigh the contributions for a top-tier conference. The paper feels more suited for a workshop or a more specialized theoretical venue.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles a fundamental question in deep learning: the interplay between architectural bias and optimization (gradient descent) in generalization. The authors attempt to disentangle these effects using tools from PAC-Bayes theory, NNGP analysis, and empirical evaluation. While the paper demonstrates technical sophistication and raises interesting points, it ultimately falls short of the standards expected for a top-tier conference like ICLR.\n\nHere's a breakdown of the strengths and weaknesses, synthesizing the reviewer feedback:\n\n**Strengths:**\n\n* **Interesting Question:** The core question addressed – the relative contributions of architecture and optimization – is highly relevant and important.\n* **Novel Technical Contributions:** The development of an analytical unbiased estimator for NNGP generalization error and the PAC-Bayes bound are genuine technical contributions. The consistency result for Monte Carlo estimation is also valuable.\n* **Clear Writing:** The paper is generally well-written and the authors have made an effort to explain complex concepts.\n\n**Weaknesses (and why they are critical):**\n\n* **Limited Scope & Generalizability:** The theoretical results are largely confined to MLPs. Extending these results to more realistic architectures (CNNs, Transformers) is a significant challenge that the authors don't adequately address. The reviewers repeatedly point this out. The claim of understanding architectural bias feels limited by this restriction.\n* **\"Nero\" Optimizer:** The reliance on the \"Nero\" optimizer is a major concern. It's not clear whether the observed benefits are due to gradient descent *per se* or specific properties of Nero. The lack of ablation studies with standard optimizers (SGD, Adam) weakens the conclusions about GD's implicit bias. Several reviewers highlighted this.\n* **Margin Discussion:** The discussion around margin and its connection to generalization is not rigorous enough. The reviewers (especially 2, 5, and 7) raise valid concerns about the assumptions and interpretations. The analogy between NNGP and GD in this context is questionable, particularly given the behavior of NTK in CNNs.\n* **Loose Bounds & Gap to Empirical Results:** The theoretical bounds are loose and don't provide a tight explanation for the observed generalization performance. The authors attribute this gap to GD's implicit bias, but this explanation is not fully convincing, and the reviewers question whether the bound's limitations are a more significant factor.\n* **Incremental Contributions:** While the technical contributions are novel, they are often incremental improvements over existing work (e.g., Valle-Pérez et al., 2019). The paper doesn't present a paradigm shift in understanding generalization.\n* **Lack of Comparison to Prior Work:** The paper doesn't sufficiently engage with related work, particularly in the areas of Neural Tangent Kernels and trajectory-based analyses of GD.\n\n\n\nThe reviewers are largely aligned in their assessment. Most recommend a weak reject or borderline acceptance with low confidence. The consistent criticisms regarding the limited scope, the Nero optimizer, and the margin discussion are particularly damning. While the authors have responded to reviewer comments, they haven't fully addressed these fundamental concerns. The paper feels premature and overpromises on its analysis of both architectural and optimization biases.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents TimeVAE, a VAE-based approach for multivariate time series generation. While the idea is reasonable and the paper is well-written, the reviewers raise significant concerns that, taken together, lead me to recommend rejection.\n\nHere's a breakdown of the issues:\n\n**Novelty & Significance:** The core criticism across multiple reviews (1, 2, and 3) is a lack of substantial novelty. Reviewer 2 explicitly states the method doesn't present originality beyond applying a standard VAE to time series. Reviewer 3 points out the lack of context within the broader literature of time series modeling (state-space models, probabilistic autoregressive models, etc.), making the technical contribution unclear. Even reviewer 4, the most positive, acknowledges the limited contribution to the field. The addition of trend and seasonality components, while potentially useful, doesn't appear to be a groundbreaking innovation.\n\n**Empirical Evaluation:** The empirical evaluation is a major weakness. Reviewer 1 highlights the lack of hyperparameter tuning and ablation studies for the decoder building blocks. This is crucial – without demonstrating how these components are best utilized, their value is questionable. Reviewer 3 criticizes the use of novel, unavailable datasets, hindering comparison to existing work. The next-step prediction evaluation, with its reliance on a post-hoc LSTM, is also questioned (Reviewer 3). The counterintuitive results in Table 1 (Reviewer 1) raise concerns about the experimental setup or interpretation. Furthermore, the promised evaluation of interpretability is entirely missing (Reviewers 1 & 4).\n\n**Technical Issues & Clarity:** Reviewer 3 identifies several imprecise claims and a non-standard framing of VAEs. The categorization of generative models is confusing, and the description of autoregressive models is inaccurate. These issues, while not fatal on their own, contribute to a sense of lacking rigor. Reviewer 2 points out formatting issues with equations.\n\n**Author Responses (Not Provided):** While author responses aren't included, the severity of the reviewers' concerns suggests that addressing them adequately would require substantial revisions, potentially bordering on a complete overhaul of the experimental setup and a more thorough literature review.\n\n\n\nIn summary, the paper proposes a reasonable idea but lacks the novelty, rigorous evaluation, and contextualization necessary for acceptance at a top-tier conference. The reviewers consistently point to weaknesses in the empirical validation and a lack of demonstrable benefit over existing methods. The claims of interpretability are unsupported by evidence.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting application of meta-learning to adversarial training via sample reweighting. The core idea – learning to reweight samples based on a multi-class margin – is conceptually sound and the experimental results demonstrate improvements over several baselines, particularly under strong attacks like AutoAttack. However, a significant number of reviewers raise valid concerns that, taken together, suggest the paper isn’t quite ready for a top-tier conference.\n\nThe most critical concern revolves around the lack of evaluation against adaptive attacks. Reviewer 1 and others rightly point out that introducing a learned reweighting module *changes* the game, and an attacker should be able to exploit this. Dismissing this as outside the scope of the paper is not justifiable, especially given the claim of improving robustness. The authors’ defense – that the reweighting module isn’t used at test time – doesn’t hold water; an independently learned reweighting head could be used to craft adaptive attacks.\n\nBeyond the adaptive attack issue, several reviewers question the novelty of the approach. While applying MAML to adversarial training is novel *in this context*, it’s largely an adaptation of an existing technique. The reviewers consistently ask for justification of the multi-class margin as an input to the meta-network and for ablation studies to decouple the contributions of the bilevel framework and the specific input encoding. The authors’ responses to these questions are largely unsatisfying, failing to provide convincing evidence that the multi-class margin is superior to other potential inputs.\n\nFurthermore, concerns are raised about the marginal improvements observed, the reliance on TRADES for strong performance, and the lack of results on CIFAR-100. The reviewers also point out minor issues with the experimental setup (e.g., outdated datasets, small CNNs) and inconsistencies in the presentation. While these are less critical, they contribute to the overall impression of a paper that could benefit from further refinement.\n\nReviewer 4 and 7, and 8 and 10 advocate for acceptance, citing the positive results and clear presentation. However, the weight of the concerns raised by the other reviewers, particularly regarding the adaptive attack evaluation and the lack of strong justification for the design choices, is substantial. The paper feels incomplete and lacks the rigor expected of a top-tier conference publication. While the work has merit and could be a good contribution to the field, it needs further development and a more thorough evaluation before it reaches the necessary standard.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting problem – investigating compositional learning through meta-referential games – but falls significantly short of the standards expected for a top-tier conference publication. The consensus across all four reviewers is overwhelmingly negative, with all assigning low scores for novelty and significance (mostly 2s) and questioning the correctness of claims.\n\nHere's a breakdown of the key issues, synthesizing the reviewers' concerns:\n\n**1. Lack of Clarity and Motivation:** The paper is difficult to follow. Reviewers 1, 2, and 4 all highlight issues with clarity, repetitive phrasing, and a lack of clear motivation for the proposed approach. The introduction is convoluted and doesn't clearly establish the problem's importance. The benchmark itself isn't adequately explained – its structure, differences from existing benchmarks, and what it measures are all insufficiently detailed.\n\n**2. Questionable Novelty and Significance of SCS:** The core contribution, the Symbolic Continuous Stimulus (SCS) representation, is heavily criticized. Reviewers 3 and 4 question its utility beyond the specific experimental setup. The justification for SCS over other representations (like one-hot encoding or fully continuous representations) is weak. In fact, the initial experimental results (Figure 2, as noted by Reviewer 2) suggest OHE performs *better* than SCS, undermining the core claim. The limitations of SCS – inability to adapt to new dimensions, potential for ambiguity – are also raised.\n\n**3. Insufficient Experimental Evaluation:** The experiments are inadequate to support the claims. Reviewers 1, 2, and 4 all point to a lack of ablation studies, insufficient hyperparameter tuning (DNC performing worse than LSTM is a red flag), and a limited number of seeds (only 2 for DNC+Rec and LSTM+Rec). The results themselves are weak – barely above chance performance – and don't provide a compelling endorsement of the proposed method. The lack of comparison to existing compositional generalization benchmarks (SCAN, gSCAN) is a significant omission.\n\n**4. Technical Issues and Missing Details:** Numerous technical details are missing or unclear. Reviewers 3 and 4 highlight missing information about model architectures, training regimes, vocabulary size, and the specifics of the communication process. There are also grammatical errors and inconsistencies throughout the paper.\n\n**5. Limited Generalizability:** The reviewers consistently express concern that the proposed method and benchmark are too specific and lack generalizability. The applicability of SCS outside the narrow experimental setting is questionable.\n\n\n\nThe authors' response to the reviewers will be crucial, but the issues are so fundamental that a simple clarification or minor addition is unlikely to be sufficient for acceptance. A major revision addressing the clarity, motivation, experimental design, and justification for SCS would be necessary, and even then, the paper's ultimate viability is uncertain. Given the current state, the paper does not meet the standards for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The core contribution lies in the platform itself, offering a modular framework and a GUI for ease of use, alongside a novel batch optimization strategy called Believer-Penalizer (BP). However, the reviewers raise significant concerns regarding the novelty and empirical validation of the work, leading me to conclude that the paper, in its current form, is not suitable for a top-tier conference.\n\nHere's a breakdown of the issues:\n\n**Novelty:** The reviewers consistently question the novelty of the contributions. While the platform is a valuable engineering effort, it largely assembles existing techniques. The BP strategy, while a reasonable idea, is described as \"naive\" and lacks strong theoretical justification. Reviewers 3 and 4 specifically highlight this lack of theoretical grounding. Reviewer 2 points out the platform's limitations to 2-3 objectives, questioning its generality. The existence of a prior arXiv preprint (Tian et al., 2021) further diminishes the perceived novelty.\n\n**Empirical Validation:** The empirical results are a major weakness. Reviewer 4 convincingly argues that the claims of superior performance are overstated and not consistently supported by the figures. The comparison to baselines is not always clear-cut, and the real-world benchmark appears \"cooked up\" and lacks a robust comparison (only against random search). Reviewer 1 also expresses skepticism about the overall performance shown in Figure 5. While Reviewer 5 finds the empirical results supportive, the other reviewers' concerns are more substantial.\n\n**Correctness & Clarity:** While generally well-written, minor issues regarding correctness and clarity are noted by multiple reviewers (e.g., terminology, abbreviations, misleading statements about objective function count). These, while not major, contribute to a sense of lack of rigor.\n\n**Reviewer Disagreement:** There's a range of opinions among the reviewers. Reviewer 5 is the most positive, while Reviewer 2 is the most critical. Reviewers 1, 3, and 4 fall somewhere in between, expressing appreciation for the platform but significant reservations about its scientific contribution. The consistent questioning of novelty and empirical validation across multiple reviewers is particularly concerning.\n\n**Conference Standards:** Top-tier conferences prioritize significant methodological contributions and rigorous empirical validation. AutoOED, as presented, leans more towards a software engineering project with limited novel theoretical insights and questionable empirical support for its claims. While a useful tool, it doesn't meet the high bar for acceptance at a conference like ICLR.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting observation – that minimizing a weighted mean can be linked to optimizing higher-order moments of the loss distribution, potentially improving robustness. However, the reviewers raise significant concerns that, taken together, lead me to believe the paper is not ready for a top-tier conference like ICLR.\n\n**Major Concerns:**\n\n* **Algorithm Validity (Reviewers 2 & 3):** The core algorithm (Algorithm 1) is heavily criticized for *not* being a proper gradient descent of the intended objective function. Reviewers 2 and 3 correctly point out the missing term in the gradient update rule. This is a fundamental flaw. If the algorithm isn't optimizing what the authors claim it is, the entire empirical validation is questionable. The authors' response doesn't adequately address this.\n* **Theoretical Weakness & Coherence (Reviewers 4 & 5):** The theoretical contributions are described as \"basic computations\" and \"simplistic.\" The introduction of negative weights ($\\lambda < 0$) creates a significant incoherence. The initial motivation is rooted in variance penalization (positive $\\lambda$), but the paper then pivots to exploring negative weights, seemingly allowing for arbitrary behavior without a clear justification. Reviewer 4 aptly notes this feels like \"doing one thing and its opposite.\" Reviewer 5 highlights the dependence of weights on $\\theta$ and questions the validity of Lemma 2 under this dependence.\n* **Lack of Novelty & Impact (Reviewers 4, 5, & 6):** Several reviewers (4, 5, and 6) find the contributions to be marginal. The paper doesn't convincingly demonstrate how this weighted mean trick offers a substantial advantage over existing robust loss functions or techniques like MoM minimization (mentioned by Reviewer 4). The connection to related work is superficial.\n* **Unclear Explanation & Technical Issues (Reviewers 6 & 5):** Reviewer 6 points out several unclear statements and technical issues, including assumptions about the existence of moments and the meaning of certain notations. Reviewer 5 also notes a lack of clarity and questions the theoretical implications of the approach.\n\n**Positive Aspects:**\n\n* **Reviewer 7's Positive Assessment:** Reviewer 7 sees value in the observation and votes to accept. This is a counterpoint, but the weight of the other reviewers' concerns is stronger.\n* **Improved Revisions (Reviewer 1):** Reviewer 1 acknowledges improvements from the authors' response. However, this doesn't address the fundamental flaws in the algorithm and theoretical justification.\n\n**Overall:**\n\nWhile the idea behind the paper is potentially interesting, the significant flaws in the algorithm, the lack of a strong theoretical foundation, the incoherence introduced by negative weights, and the limited novelty outweigh the positive aspects. The paper, in its current form, does not meet the standards of a top-tier conference like ICLR. The authors need to address the algorithmic validity concerns, provide a more coherent theoretical framework, and demonstrate a clear advantage over existing methods.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to provide a frequency-domain perspective on adversarial robustness, challenging the common assumption that adversarial examples are purely high-frequency noise. While the idea is interesting and potentially valuable, the reviews reveal significant weaknesses that, in my assessment, prevent it from meeting the standards of a top-tier conference.\n\nHere's a breakdown of the issues:\n\n**Major Concerns:**\n\n* **Lack of Novelty:** The most consistent criticism revolves around the lack of genuinely novel contributions. Several reviewers (2, 3, 5) point out that the core finding – that adversarial examples are dataset-dependent and not strictly high-frequency – has been previously observed and discussed in the literature (Tsuzuku & Sato 2019, Yin et al. 2020, Bernhard et al. 2021, and others). The paper is largely framed as \"additional evidence\" rather than a groundbreaking new understanding.\n* **Unclear Contributions:** Reviewers 2 and 5 struggle to identify the unique contributions of the paper. The proposed methods (frequency-constrained adversarial training and the frequency-based trade-off control) are described as unclear and lacking demonstrable benefits. Reviewer 2 specifically asks for evidence of improved performance and questions the difference between the proposed method and existing techniques like TRADES.\n* **Insufficient Validation:** Reviewer 3 rightly points out the limited scope of the datasets used (CIFAR-10 and ImageNet-derived). The claim of dataset dependence needs broader validation, including simpler datasets like MNIST and others like CIFAR-100, SVHN, and Fashion-MNIST. Furthermore, the reliance on PGD attacks alone is a limitation, as other attack methods might reveal different frequency characteristics.\n* **Weak Justification & Argumentation:** The paper's motivation, based on a \"common misconception,\" is undermined by the fact that this misconception has already been questioned in prior work. The authors don't adequately address this in their rebuttal, and the arguments for the novelty of their approach are not convincing.\n\n**Minor Concerns:**\n\n* **Presentation:** Reviewer 4 notes minor grammatical errors and suggests improving the figure presentation. While not critical, these detract from the overall quality.\n* **Methodological Justification:** Reviewer 3 raises valid questions about the averaging of noise gradients and the imperceptibility of low-frequency attacks. These points require further justification.\n\n**Positive Aspects:**\n\n* **Interesting Observations:** Several reviewers (3, 4, 6) acknowledge that the experiments and observations are interesting and potentially valuable to the community.\n* **Well-Written & Extensive Experiments:** Reviewer 6 praises the paper's writing quality and the thoroughness of the experiments.\n* **Potential for Insight:** The frequency-domain analysis *could* offer valuable insights, but the paper doesn't fully capitalize on this potential.\n\n\n\n**Overall Assessment:**\n\nThe paper presents an interesting line of inquiry, but it falls short of the novelty, clarity, and rigor expected of a top-tier conference publication. The core finding is not new, the contributions are unclear and lack demonstrable impact, and the validation is insufficient. While the experiments are extensive, they don't provide enough compelling evidence to overcome the fundamental weaknesses. The reviewers are largely aligned in their concerns, with a significant number expressing doubt about the paper's contribution.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an interesting and important problem: the convergence of SGD with momentum under covariate shift, a scenario frequently encountered in real-world machine learning applications. The core idea – framing the dynamics as a parametric oscillator and identifying resonance phenomena – is insightful and potentially impactful. However, the reviewers raise several valid concerns that collectively weaken the paper's case for acceptance at a top-tier conference.\n\n**Strengths:**\n\n* **Novelty of the core idea:** The analogy to a parametric oscillator is a clever way to analyze the problem and provides a new perspective on the divergence behavior of SGDm.\n* **Thoroughness:** The authors attempt to bridge the gap between theory and practice by conducting experiments under increasingly relaxed assumptions.\n* **Clear Presentation:** Reviewers 4 and 6 commend the paper's writing quality and systematic approach.\n* **Identified Important Problem:** The issue of non-iid data is crucial, and understanding its impact on optimization algorithms is valuable.\n\n**Weaknesses (and responses to reviewer concerns):**\n\n* **Limited Technical Novelty:** This is the most consistent criticism (Reviewers 2 & 3). While the application of oscillator theory to this problem is novel, the underlying mathematical tools are largely borrowed from existing literature (numerical analysis, Floquet theory). The authors claim \"technical novelty\" but reviewers argue they are primarily *applying* existing results rather than developing new ones. The response to this criticism doesn't fully address the concern.\n* **Weak Connection Between Theory and Experiment:** Several reviewers (2, 3, 5, & 6) question the direct relevance of the theoretical results to practical scenarios. The theory relies on strong assumptions (linear regression, periodic covariate shift, expected gradients) that are often violated in real-world applications. The experiments, while extensive, don't convincingly demonstrate that the observed phenomena in neural networks are *caused* by the same resonance mechanism.\n* **Analysis on Expected Gradients:** Reviewer 4 and 3 rightly point out that analyzing the expected gradients effectively removes the \"S\" from SGD, making it questionable whether the results truly apply to stochastic gradient descent. This is a significant limitation.\n* **Lack of a Compelling Empirical Example:** Reviewer 6 highlights the absence of a real-world dataset exhibiting the clear oscillatory behavior predicted by the theory. This weakens the argument for the practical relevance of the findings.\n* **Presentation Issues:** Reviewers 3 and 5 point out areas where the presentation could be improved, particularly regarding the explanation of resonance behavior and the connection between theoretical parameters and observed phenomena.\n\n**Overall Assessment:**\n\nThe paper presents an interesting idea, but the concerns raised by the reviewers are substantial. The limited technical novelty, the weak connection between theory and practice, and the questionable relevance to true SGD are significant drawbacks. While the authors have attempted to address these concerns, their responses are not entirely convincing. The paper feels more like an insightful application of existing tools to a new problem than a groundbreaking contribution to the field. For a top-tier conference, the bar for novelty and impact is higher.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and increasingly relevant question: how do standard reinforcement learning algorithms behave in the overparameterized regime, a setting common in modern deep RL. The authors attempt to provide theoretical grounding for observed phenomena and develop practical improvements. However, the reviews reveal significant concerns that, taken together, outweigh the paper’s contributions.\n\n**Strengths:**\n\n* **Relevant Topic:** The investigation of overparameterization in RL is timely and potentially impactful.\n* **Unified Interpretation:** The attempt to unify the behavior of TD, FVI, and RM through a constrained optimization lens is a valuable effort.\n* **Initial Positive Response:** Several reviewers initially found the paper interesting and increased their scores after the rebuttal, suggesting the authors addressed some concerns.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Novelty:** A recurring theme across multiple reviews (7, 10, 9) is the lack of significant novelty. Key results (e.g., Theorem 2, Theorem 4) are presented as new but are closely related to, or directly derived from, existing work without sufficient attribution. Reviewer 7 is particularly scathing on this point, highlighting errors in the presentation of known results.\n* **Technical Errors:** Several reviewers (3, 6, 9) point out specific technical errors in the proofs and derivations. These aren't minor typos; they relate to fundamental aspects of the mathematical arguments. Reviewer 6's detailed critique of the matrix decomposition and Reviewer 9's errors regarding equations 38, 42, and the Hoeffding inequality are particularly damaging.\n* **Limited Connection to Deep RL:** A major criticism (Reviewer 7, 10) is the weak connection to deep reinforcement learning. The analysis is largely confined to overparameterized *linear* representations, which, as several reviewers point out, is not representative of the architectures used in practice. The authors fail to convincingly demonstrate the transferability of their findings to deep neural networks.\n* **Questionable Setting & Generalization:** Reviewer 8 raises a valid concern about the IID assumption in the theoretical analysis, which doesn't hold in typical offline RL scenarios.\n* **Weak Empirical Evaluation:** The experiments are described as \"toyish\" and lacking strong motivation (Reviewer 7).\n* **Insufficient Comparison to Underparameterized Case:** Reviewer 9 correctly points out the lack of a clear comparison between over- and underparameterized scenarios, hindering the understanding of the benefits of overparameterization.\n* **Writing Quality:** Reviewer 9 notes grammatical issues and a lack of precision in the writing.\n\n**Rebuttal Impact:**\n\nWhile the authors addressed some reviewer concerns in the rebuttal, the fundamental issues regarding novelty, technical correctness, and relevance to deep RL remain. The fact that multiple reviewers, even after the rebuttal, maintain relatively low scores indicates that the core problems were not adequately resolved.\n\n\n\nGiven the significant technical flaws, the lack of substantial novelty, and the limited connection to practical deep RL applications, this paper does not meet the standards of a top-tier conference. The reviewers' concerns, particularly regarding the unacknowledged reliance on existing work and the presence of errors, are too serious to overlook.\n\nFinal Deci"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents a practical approach to improving face detection accuracy and efficiency, specifically targeting the challenges of small faces in low-resolution images. The core idea – jointly optimizing computation allocation and sample distribution – is sound and demonstrably effective, as evidenced by the strong results on the WIDER FACE dataset. The authors clearly articulate the motivation and provide a reasonably thorough ablation study. The code release is also a significant positive.\n\nHowever, the paper isn’t without its weaknesses. A recurring theme in the reviews (R4, R6, R7) is the relative simplicity and lack of novelty in the search strategies employed. While the authors achieve strong results, the methods used to *find* those results (random search within a defined space) are not particularly innovative. Several reviewers rightly point out the need for comparison with more sophisticated Neural Architecture Search (NAS) techniques (R4, R6). The fact that the improvements are achieved through a relatively straightforward search raises questions about whether the gains are primarily due to the search process itself or inherent properties of the chosen architecture (TinaFace) and the specific redistribution strategies.\n\nFurthermore, some reviewers (R7) identify issues with clarity and completeness. The lack of a framework overview figure and algorithmic description for the computation redistribution method hinders understanding. The details regarding sample distribution are also lacking. The cross-dataset evaluation is insufficient to claim SOTA performance without a direct comparison to other top performers on those datasets.\n\nThe reviewer scores are mixed. While correctness is generally rated highly (mostly 3 or 4), technical novelty consistently receives lower scores (2 or 3). Empirical novelty is rated more favorably (3 or 4), reflecting the positive experimental results. The fact that several reviewers \"addressed their questions\" or found the paper \"helpful\" suggests the core ideas are understandable and potentially valuable.\n\nConsidering the strengths and weaknesses, this paper falls into a gray area. It's not a groundbreaking contribution to NAS, but it *is* a solid engineering effort that delivers state-of-the-art results on a challenging benchmark. For a top-tier conference, the lack of significant technical novelty is a concern. However, the strong empirical results and practical relevance are compelling. A rejection would be justified if the conference prioritizes highly novel theoretical contributions. However, given the practical impact and the positive feedback regarding the results, a conditional acceptance with requests for major revisions addressing the concerns about novelty, clarity, and completeness seems appropriate. Specifically, the authors should:\n\n1. **Compare against more advanced NAS methods:** Include a comparison with at least one evolutionary or gradient-based NAS technique.\n2. **Provide a clearer explanation of the search strategies:** Elaborate on the rationale behind the search space design and discuss the limitations of the random search approach.\n3. **Improve clarity and completeness:** Add a framework overview figure, an algorithm for computation redistribution, and more details on the sample distribution process.\n4. **Strengthen cross-dataset evaluation:** Provide a more comprehensive comparison with SOTA methods on AFW, Pascal, and FDDB.\n\n\n\nFinal Decision: Accept (with majo"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents a compelling and well-executed approach to interpreting deep visual features using natural language descriptions. The core idea – MILAN – is innovative and addresses a significant challenge in the field of interpretability: moving beyond simple category labels to richer, compositional descriptions of neuron behavior. The applications demonstrated (analysis, auditing, editing) are insightful and showcase the potential impact of this work.\n\nThe reviewers are largely positive. Reviewer 2 provides a thorough and balanced assessment, highlighting both strengths and weaknesses. Reviewer 3 is enthusiastic and sees the work as inspirational. Reviewer 4, while acknowledging a lack of CV expertise, identifies valuable points for improvement, particularly regarding generalization and comparison to existing language models. Reviewer 1's lack of comment isn't necessarily negative; it suggests no major flaws were immediately apparent.\n\nSeveral key strengths stand out:\n\n* **Novelty:** While building on existing techniques (mutual information, language modeling), the combination and application to neuron-level interpretation are novel. The creation of the MILANNOTATIONS dataset is a significant contribution.\n* **Technical Soundness:** The method is clearly explained, and the authors provide sufficient detail for reproducibility.\n* **Empirical Results:** The results demonstrate that MILAN generates descriptions that align well with human intuition and outperform baselines. The generalization across architectures, datasets, and tasks is encouraging.\n* **Practical Applications:** The demonstrated applications are not just theoretical exercises; they address real-world concerns like bias detection and robustness improvement.\n\nHowever, the reviewers raise legitimate concerns that need to be addressed. The most prominent are:\n\n* **Inter-Annotator Agreement:** The relatively low inter-annotator agreement in the MILANNOTATIONS dataset is a valid concern. The authors need to clarify how this was handled during training and whether additional quality control measures were taken.\n* **Scalability & Leveraging Existing Data:** The suggestion to leverage existing large multimodal datasets (GQA, Visual Genome, CLIP/ALIGN) is a strong one. This could significantly improve the scalability and robustness of the approach.\n* **Generalization:** Reviewer 4 rightly questions the generalizability of the results beyond image models and similar datasets. Demonstrating transfer to other modalities or tasks would strengthen the paper considerably.\n* **Comparison to Image Captioning Models:** A direct comparison to image captioning models as baselines is crucial to establish the unique value of MILAN. The authors' hypothesis that captioning models operate at a higher level of abstraction needs empirical validation.\n* **Description Characteristics:** More detailed analysis of the generated descriptions (length, vocabulary, diversity) would provide further insights into the method's capabilities.\n\n\n\nThe authors' response to these concerns will be critical. However, even without those responses, the core contribution of this work is substantial enough to warrant acceptance. The paper tackles an important problem, proposes a novel and well-executed solution, and demonstrates its potential through compelling applications. The identified weaknesses, while important, are not fatal and could be addressed in future work. The potential impact of this research on the field of interpretability is significant.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper proposes a contrastive learning objective to mitigate text degeneration in language models. While the idea is intuitively appealing and the paper is well-written and clear, the consensus among the reviewers points towards a rejection. Here's a breakdown of the issues:\n\n**Novelty:** This is the most significant weakness. Reviewers 1 and 3 explicitly state the lack of novelty. The core idea builds upon unlikelihood training (UL), and reviewer 1 *strongly* disputes the authors' claim that UL doesn't consider positive token probabilities. The authors attempt to differentiate their approach by focusing solely on penalizing *negative* tokens (previously generated ones) rather than all unlikely tokens, but this difference isn't convincingly demonstrated as substantial. Reviewer 4 also questions the importance of using only the M previous tokens versus all previous tokens as done in UL.\n\n**Experimental Evaluation:** The experiments are considered insufficient by multiple reviewers (1 & 2). The use of GPT-2 small is a major concern, as larger models often exhibit less degeneration, diminishing the relevance of the proposed technique. Reviewer 2 rightly points out that the metrics used (perplexity, dist-1) are either unreliable or interpreted in a non-standard way. The lack of evaluation on tasks beyond language modeling (e.g., summarization, translation, captioning, as suggested by the authors themselves) further weakens the empirical contribution. The human evaluation (Table 3) lacks statistical significance according to reviewer 4.\n\n**Technical Correctness:** Reviewer 1 identifies a factual inaccuracy regarding the UL paper's formulation. While minor, this casts doubt on the authors' understanding of related work. Reviewer 4 also asks for clarification on a specific claim regarding irrelevant tokens.\n\n**Author Response (Not Provided):** The absence of author responses prevents assessing whether the authors addressed these concerns. However, given the strength and consistency of the reviewers' criticisms, it's unlikely a rebuttal could fully resolve them.\n\n**Overall:** The paper presents an incremental improvement over existing techniques, but the novelty is questionable, the experimental validation is weak, and there are concerns about technical correctness. These shortcomings fall short of the standards expected for a top-tier conference like ICLR. While the idea is sound and the writing is clear, the paper doesn't offer a compelling enough contribution to warrant acceptance.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – accelerating sampling from diffusion models – and proposes a novel approach, DDSS, coupled with a new sampler family, GGDP. The core idea of differentiating through sample quality to optimize the sampling process is intriguing and the empirical results are compelling, particularly the significant FID score improvements with a reduced number of steps. However, the paper isn’t without its weaknesses, as highlighted by the reviewers.\n\n**Strengths:**\n\n* **Strong Empirical Results:** The reported FID scores, especially on LSUN church, demonstrate a substantial improvement over existing methods with fewer sampling steps. This is the strongest aspect of the paper.\n* **Novelty:** The concept of directly optimizing the sampler via a differentiable loss based on sample quality is a novel contribution.\n* **Generality:** The method is claimed to be applicable to any pre-trained diffusion model, which is a significant advantage.\n* **Addressing Practical Concerns:** The use of gradient rematerialization addresses a key practical challenge in implementing this approach.\n\n**Weaknesses & Concerns (addressed to varying degrees by the rebuttal):**\n\n* **Theoretical Justification:** The initial lack of strong theoretical grounding was a major concern raised by multiple reviewers (1, 2, and 4). While Theorem 1 provides *some* justification, it doesn't fully address the question of why minimizing KID is effective or what the statistical properties of the generated samples are. The reviewers correctly point out that the method feels somewhat \"hand-crafted\" without a deeper theoretical understanding.\n* **Motivation for Non-Markovian Structure:** The benefit of the non-Markovian structure of GGDP beyond enabling DDSS isn't clearly articulated. Reviewer 3 rightly questions whether GGDP has value on its own.\n* **Clarity & Presentation:** Several reviewers (3, 5) noted issues with clarity, particularly regarding notation and the explanation of GGDP.\n* **Dataset Scope:** The limited range of datasets, particularly the lack of results on higher-resolution images (LSUN, ImageNet, CelebA), is a valid criticism. While the authors claim the method is applicable to larger models, demonstrating this empirically would significantly strengthen the paper.\n* **Potential for Overfitting:** Reviewer 4 raises a valid point about the potential for overfitting to the evaluation metrics (FID/IS) when optimizing perceptual loss.\n\n**Reviewer Analysis:**\n\n* **Reviewer 1:** Initially skeptical, but significantly improved their score after the rebuttal and Theorem 1. Still raises concerns about interpretation and significance.\n* **Reviewer 2:** Initially negative, but raised their score after the rebuttal. Remains concerned about the motivation and theoretical underpinnings.\n* **Reviewer 3:** Positive overall, but questions the independent value of GGDP. Remains cautiously optimistic.\n* **Reviewer 4:** Critical of the lack of theoretical guarantees and potential for overfitting. Remains skeptical.\n* **Reviewer 5:** Positive, but highlights clarity issues and the need for more extensive experiments.\n\n**Overall Assessment:**\n\nThe paper has improved significantly due to the authors' responses to the reviewers. The addition of Theorem 1 addresses some of the initial concerns about theoretical justification. The empirical results are strong and demonstrate a clear benefit in terms of sampling speed and quality. However, the lingering concerns about theoretical grounding, clarity, and the limited scope of experiments prevent me from giving it a strong acceptance. The paper feels like a promising first step, but further work is needed to fully understand the underlying mechanisms and demonstrate its scalability and robustness. The reviewers' scores are also quite varied, indicating a lack of consensus.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper proposes a Variational Neural Cellular Automata (VNCA) as a generative model. The core idea – combining the generative power of VAEs with the self-organizing principles of NCAs – is interesting. However, the reviews paint a consistent picture: while the paper is well-written and the idea is sound, the empirical results are underwhelming and the novelty is questionable.\n\nHere's a breakdown of the key points from the reviews:\n\n* **Weak Empirical Results:** Multiple reviewers (1, 4, 7) highlight the underwhelming performance, particularly in terms of sample quality and likelihoods on datasets beyond MNIST. Reviewer 4 specifically notes the results are significantly behind state-of-the-art.\n* **Limited Novelty:** Several reviewers (2, 6) point out that the proposed architecture can be viewed as a specific type of deep convolutional network, and much of the observed behavior isn't surprising from that perspective. Reviewer 6 argues the novelty lies in the *interpretation* as an NCA, not the architecture itself, and questions what is gained from this interpretation. Reviewer 2 correctly identifies connections to prior work on ResNets with tied weights and Neural ODEs.\n* **Lack of Thorough Comparison:** Reviewers 4 and 7 call for more rigorous comparison to existing methods, particularly for tasks like inpainting and super-resolution. They suggest quantifying the benefits of the VNCA approach against standard VAEs with upsampling.\n* **Positive Aspects:** The paper is well-written (4, 5, 7), and the robustness to perturbations (demonstrated in Figure 10) is considered a positive aspect (7). Reviewer 3 is strongly in favor of acceptance after the rebuttal, indicating the authors addressed their specific concerns. Reviewer 5 also moved to an acceptance recommendation after the rebuttal.\n\nThe rebuttal period seems to have addressed some concerns, particularly regarding clarity and explanations of the architecture. However, the core issue of underwhelming empirical results and limited novelty remains. While the authors addressed reviewer concerns, they didn't present substantially improved results. The reviewers who initially leaned towards rejection (1, 6) did not change their stance significantly.\n\nThe fact that multiple reviewers independently identify the lack of strong empirical evidence and question the novelty is a significant concern for a top-tier conference. While the paper might be a valuable contribution to a more specialized workshop or a less competitive venue, it doesn't meet the high bar for acceptance at a top-tier conference like ICLR. The reviewers' comments suggest the work is more of an interesting exploration of a connection between existing concepts than a breakthrough contribution.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper, MCTransformer, proposes combining Transformers and Monte-Carlo Tree Search for offline reinforcement learning. While the core idea of leveraging Transformers for sequence modeling within an MCTS framework is interesting, the paper suffers from significant weaknesses that, in my assessment, preclude its acceptance at a top-tier conference in its current form.\n\nThe primary concern, echoed by all three reviewers, is the **lack of novelty**. Reviewers 2 and 3 are particularly strong in their criticism, correctly pointing out the similarity to AlphaGo/AlphaZero, essentially replacing a ResNet with a Transformer. This isn't inherently a flaw, but the paper doesn't sufficiently justify *why* a Transformer is a superior choice or offers any novel architectural or algorithmic contributions beyond this substitution. The authors need to demonstrate a clear advantage of using Transformers in this context, which is currently missing.\n\nThe **empirical evaluation is extremely limited**. The choice of SameGame, while a useful testbed, is considered \"toy-ish\" by Reviewer 1 and lacks the breadth and established benchmarks (MiniGo, Atari) expected of a top-tier conference submission. The comparisons are also weak. The lack of comparison to Decision Transformers, Online DTs, and other MCTS-based methods *with* neural networks (like AlphaZero) is a major oversight. The comparisons to a random player are uninformative. Reviewer 3 rightly points out the need for comparison to other offline RL and imitation learning methods. The experimental setup, as highlighted by Reviewer 2, also appears potentially unfair regarding simulation counts.\n\n**Clarity and presentation** are also significant issues. Reviewer 1 notes the writing is unclear and requires multiple readings. Specific issues like the unclear figure captions and unexplained components (rtg_\\pi(s_t)) detract from the paper's accessibility. While Reviewer 3 finds the presentation easy to follow *despite* typos and grammatical errors, these errors still impact the overall quality.\n\nThe reviewers' scores reflect these concerns. The average scores for technical and empirical novelty are low (around 2 or 1), and correctness is only marginally better. While the idea has some merit (as indicated by the novelty scores being above 1), the execution and validation are insufficient.\n\nThe author responses are not provided, so I cannot assess whether the authors have addressed these concerns. However, based on the severity and consistency of the reviewers' criticisms, a substantial revision addressing the novelty, empirical evaluation, and clarity issues would be required for reconsideration. Even with revisions, the paper faces an uphill battle to demonstrate sufficient impact for acceptance at a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting theoretical framework for neural network compression based on tropical geometry and Hausdorff distance of tropical zonotopes. The core idea is novel, and the mathematical analysis appears solid, as highlighted by multiple reviewers. The potential for inspiring further theoretical work is also a significant strength. However, the paper suffers from several weaknesses, primarily related to the experimental evaluation and clarity of claims.\n\nHere's a breakdown of the issues:\n\n**Strengths:**\n\n* **Novelty:** The application of tropical geometry to neural network compression is a relatively unexplored area.\n* **Theoretical Foundation:** The paper provides a mathematically grounded approach with a theorem relating approximation error to Hausdorff distance.\n* **Potential Impact:** The work could stimulate further research in the intersection of tropical geometry and neural networks.\n\n**Weaknesses:**\n\n* **Limited Empirical Evaluation:** The experiments are conducted on small, outdated architectures (LeNet, VGG) and relatively small datasets (MNIST, Fashion-MNIST). The lack of evaluation on modern architectures (ResNets, Transformers) significantly limits the practical relevance and generalizability of the findings. Reviewers 1, 3, and 4 all point this out.\n* **Overstated Claims:** The abstract and paper make claims about outperforming \"modern pruning techniques\" which are not adequately supported by the experimental results. The comparison to ThiNet (2017) is insufficient to demonstrate superiority over state-of-the-art methods. Reviewer 1 is particularly critical of this.\n* **Unclear Error Bounds:** Reviewer 3 raises a valid concern about the decreasing error bounds as pruning increases, questioning their practical utility.\n* **Lack of Practical Details:** The paper lacks information about computational complexity, runtime, and code availability, hindering reproducibility and practical assessment. Reviewer 4 specifically requests pseudo-code.\n* **Scope Limitation & Clarity:** The algorithm is explicitly for fully-connected layers, yet experiments are performed on CNNs. The extent to which the method is applied to fully-connected parts within CNNs needs clarification. Reviewer 1 highlights this.\n* **Minor Issues:** Several minor issues are raised regarding table captions, unexplained terminology (\"deepNN\"), and missing comparisons to relevant work (Alfarra et al., 2020).\n\n\n\n**Author Responses:**\n\nThe author responses, as evidenced by Reviewer 2's update, acknowledge the limitations of the empirical evaluation, characterizing it as \"proof-of-concept.\" While this is helpful, it doesn't fully address the concerns about overstated claims and the lack of evaluation on more challenging benchmarks. The authors don't seem to commit to addressing the limitations in a substantial way.\n\n**Overall Assessment:**\n\nWhile the theoretical contribution is valuable, the paper's weaknesses in the experimental section and the overstated claims significantly detract from its overall impact. A top-tier conference demands both theoretical rigor *and* compelling empirical validation. This paper falls short on the latter. The reviewers are generally positive about the theoretical aspects but consistently raise concerns about the limited scope and impact of the experimental results. The paper, as it stands, feels more like a preliminary investigation with promising theoretical foundations than a fully-fledged contribution ready for publication in a highly competitive venue.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an interesting and relevant problem – continual learning of hierarchical motor control in RNNs, drawing inspiration from neuroscience. The core idea of incorporating a thalamocortical-inspired “preparatory module” to improve motif chaining is potentially valuable. However, the reviewers raise significant concerns that, taken together, lead me to believe the paper is not currently suitable for acceptance at a top-tier conference.\n\nHere's a breakdown of the issues:\n\n**Major Concerns:**\n\n* **Lack of Rigorous Evaluation of Alternatives:** The most consistent and damning criticism comes from Reviewers 3 and 5. The authors prematurely jump to an inductive bias solution without thoroughly exploring whether learning-based approaches could address the problem. Reviewer 3 specifically outlines clear experiments (incorporating end states into training, training for consistent return points) that should have been conducted to demonstrate the necessity of the proposed bias. The authors’ dismissal of these alternatives feels defensive rather than scientifically justified. This is a critical flaw, as a core tenet of strong research is to exhaustively explore simpler solutions before resorting to more complex ones.\n* **Small Scale and Limited Generalizability:** Reviewer 3 rightly points out the extremely small scale of the experiments (300 units, 10 motifs). This raises serious questions about the generalizability of the findings. While not every paper *needs* to be on ImageNet scale, the small size makes it difficult to believe the observed benefits are robust or will translate to more complex systems.\n* **Presentation and Clarity:** Reviewer 5 highlights significant issues with the paper's writing and organization. The lack of a clear formalization of the task, scattered information, and confusing presentation hinder understanding and make it difficult to assess the validity of the claims. This is a substantial issue for a top-tier conference where clarity and reproducibility are paramount.\n* **Potential Trivial Solution:** Reviewer 5 also raises a valid point about the possibility of a trivial solution (hard resetting). The authors need to convincingly demonstrate that their approach offers something beyond this simple fix.\n\n**Minor Concerns:**\n\n* **Biological Justification:** Reviewer 2 questions the computational and biological plausibility of the specific resetting mechanism. While inspiration from neuroscience is valuable, the connection isn't fully convincing.\n* **Novelty:** Multiple reviewers (2 & 3) rate the novelty as marginal. The proposed architecture builds on existing continual learning techniques and the thalamocortical inspiration isn’t entirely novel in the context of RNNs.\n* **Correctness:** While not a pervasive issue, several reviewers (2, 3, & 5) identify minor inaccuracies or unsupported claims.\n\n**Reviewer Scores:** The scores are consistently low across the board, with no reviewer giving high marks for novelty or significance. Reviewer 4 is the most positive, but even their enthusiasm is tempered by uncertainty about the analytical work in the appendices.\n\n**Author Responses:** The provided information doesn't include author responses, which would be crucial for assessing whether the authors have adequately addressed the reviewers' concerns. However, Reviewer 1 explicitly states that the authors' efforts haven't changed their initial assessment, suggesting limited improvement.\n\n\n\nIn conclusion, while the paper explores a potentially interesting direction, the significant methodological flaws (lack of alternative evaluation, small scale), combined with the clarity issues and concerns about novelty, outweigh the potential contributions. The paper needs substantial revisions, including a more rigorous experimental evaluation and a clearer presentation, before it would be suitable for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper investigates epoch-wise double descent in a linear teacher-student model, providing analytical insights into the phenomenon. The reviewers largely agree on the paper's strengths – a clear presentation, well-motivated theory, and a qualitative match with empirical observations in a deep neural network. However, significant concerns are raised regarding novelty, connections to existing work, and technical details/correctness.\n\n**Strengths:** The core idea of attributing epoch-wise double descent to learning features at different scales is insightful. The derivation of closed-form expressions is a valuable contribution, and the qualitative agreement with ResNet18 results strengthens the argument for generalizability. Reviewer 1 highlights the value of demonstrating this behavior in a tractable model.\n\n**Weaknesses & Concerns:**\n\n* **Novelty:** This is the biggest concern. Reviewer 2 points out the connection to Tibshirani's work on implicit regularization of GD/SGD, suggesting the observed double descent might be a consequence of Ridge-like regularization induced by the optimization process. Reviewer 3 also questions the novelty, asking about connections to Heckel & Yilmiz (2020) and Stephenson & Lee (2021). The authors need to thoroughly address these connections and clearly articulate what *new* insight their work provides beyond these existing results. Simply observing double descent in another setting isn't enough for a top-tier conference.\n* **Technical Correctness & Clarity:** Reviewer 3 identifies a *substantial* number of typos and inconsistencies in the equations and notation. This is a major red flag. While not necessarily invalidating the core results, it severely impacts the readability and trustworthiness of the work. The questions raised about the mathematical derivations (e.g., averaging over datasets, the meaning of certain variables, assumptions about orthogonality) need to be addressed carefully.\n* **Connection to Deep Learning:** While the qualitative match with ResNet18 is encouraging, the connection to realistic deep learning scenarios feels somewhat weak. Reviewer 3 suggests exploring how label noise might create the necessary feature scale separation. A more detailed discussion of this link would be beneficial.\n* **Random Matrix Theory:** The abstract mentions random matrix theory, but Reviewer 3 notes its absence in the main text. This is misleading and needs clarification.\n\n**Reviewer Disagreement:** The reviewers are relatively consistent in their assessment of correctness (mostly 3/4) and novelty (mostly 2/3). The main difference lies in the level of detail in their critiques. Reviewer 3 provides the most thorough and critical assessment, highlighting numerous technical issues.\n\n**Author Response:** The authors *must* provide a detailed response to all reviewer comments, especially those concerning novelty and technical correctness. They need to:\n\n*   Clearly position their work within the existing literature, explicitly addressing the connections raised by Reviewer 2 and 3.\n*   Thoroughly revise the manuscript to correct all typos and inconsistencies.\n*   Provide clear explanations for all mathematical derivations and assumptions.\n*   Strengthen the connection to realistic deep learning scenarios.\n*   Clarify the role of random matrix theory (or remove the mention from the abstract if it's not used).\n\n\n\nGiven the significant concerns regarding novelty and technical correctness, and the substantial number of errors identified by Reviewer 3, the paper, in its current form, does not meet the standards for acceptance at a top-tier conference. A major revision addressing these points is necessary.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper, PEARL, presents a novel approach to differentially private data synthesis using a one-shot privacy mechanism and adversarial learning. The core idea – privatizing embeddings upfront and then training a generative model – is a sound one, addressing a key limitation of iterative gradient-based DP-SGD methods. The reviewers generally agree on the paper's strengths: a well-motivated approach, clear mathematical formulation, and promising initial empirical results. However, significant concerns regarding the thoroughness of the evaluation and clarity of certain technical details prevent a strong recommendation for acceptance at a top-tier conference.\n\nLet's break down the issues:\n\n**Strengths:** The novelty lies in the combination of techniques and the one-shot privacy approach. Avoiding the compounding privacy loss of iterative methods is a valuable contribution. The writing is generally good, and the theoretical grounding is present.\n\n**Weaknesses (and severity):**\n\n* **Insufficient Benchmarking (High Severity):** This is the most significant weakness, highlighted by all three reviewers. The comparison to DP-MERF and DP-GAN is a good start, but lacks breadth. Crucially missing are comparisons to PATE-GAN, CTGAN (for tabular data), and potentially PrivBayes/PGM/PrivSyn as suggested by Reviewer 3. The limited dataset evaluation (only a few datasets mentioned) further exacerbates this issue. A top-tier conference demands a comprehensive evaluation against state-of-the-art methods.\n* **Technical Clarity (Medium Severity):** Reviewer 2 raises valid points about the auxiliary information release and sensitivity calculation (the 'k' factor). These need to be clarified. The lack of detail regarding the one-shot sampling strategy and its privacy guarantees is also concerning, as pointed out by Reviewer 3.\n* **Related Work (Medium Severity):** Reviewer 3 correctly identifies missing relevant work (Privsyn, DPT, and the DP-MRF paper). A more thorough literature review is essential to properly position the contribution and acknowledge existing approaches.\n* **Lack of Complexity Analysis (Low-Medium Severity):** While not a deal-breaker, a complexity analysis would strengthen the paper and provide insights into its scalability.\n* **Saturation of Performance (Low Severity):** Reviewer 2's observation about performance saturation with increasing epsilon is worth investigating, though not a critical flaw.\n* **Editorial Quality (Low Severity):** Reviewer 3 notes inconsistencies and imprecise descriptions, which can be addressed with careful editing.\n\n**Author Responses (Not Provided):** The absence of author responses is a significant drawback. It's impossible to assess whether the authors are willing and able to address the reviewers' concerns.\n\n\n\n**Overall Assessment:**\n\nThe paper has potential, but in its current form, it falls short of the standards expected for a top-tier conference. The lack of comprehensive benchmarking and the technical ambiguities are major concerns. While the novelty is appreciated, it needs to be demonstrated convincingly through rigorous evaluation and clear explanation. The authors need to significantly expand the experimental section, address the technical questions raised by the reviewers, and strengthen the related work discussion.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging the inherent conservatism of on-policy algorithms for offline RL. The core argument, that on-policy methods can naturally address the overestimation issue in offline RL, is thought-provoking. However, the paper suffers from several weaknesses that, despite the author responses, prevent it from reaching the standard of a top-tier conference.\n\n**Strengths:**\n\n* **Novelty (to a degree):** The framing of applying on-policy methods directly to offline RL, and the theoretical justification, is a novel perspective.\n* **Simplicity:** BPPO is indeed a simple modification of PPO, which is appealing.\n* **Empirical Performance:** The experimental results on D4RL are generally positive, showing competitive or superior performance compared to some baselines.\n\n**Weaknesses (and issues not fully addressed by responses):**\n\n* **Theoretical Gaps & Soundness Concerns:** Several reviewers (1, 2, 5) raise valid concerns about the theoretical foundations. The connection to Generalized Proximal Policy Optimization (GePPO) highlighted by Reviewer 2 significantly diminishes the novelty of the theoretical contributions. The approximation in Eq. 13 and its implications for policy improvement are not convincingly justified. The assumptions, particularly Assumption 1, are strong and potentially unrealistic.\n* **Experimental Limitations:** The D4RL benchmark, while standard, is becoming saturated. The improvements shown are often marginal, and the paper lacks a compelling demonstration of BPPO solving a problem where other methods fail. The lack of analysis on dataset optimality (as pointed out by Reviewer 4) is a significant omission. The experiments don't adequately address the overestimation problem that motivates the work.\n* **Clarity and Presentation:** Reviewer 1 correctly points out sections that are unclear and colloquial. While the author responses likely improved clarity, the initial presentation was a weakness.\n* **Comparison to Baselines:** The comparison to Onestep BPPO is not convincing, and the lack of ablation studies on key hyperparameters (clip coefficient decay, asymmetric advantage coefficient) weakens the robustness analysis. The claim of simplicity is also debatable when considering the tuning of these hyperparameters.\n* **Off-Policy Nature:** The argument that BPPO is discovering on-policy methods can solve offline RL is misleading. As Reviewer 1 points out, BPPO is effectively an off-policy version of PPO, relaxing the on-policy constraint. This is a subtle but important distinction.\n\n**Rebuttal Analysis:**\n\nThe author responses address some concerns, particularly regarding experimental details and providing additional results. However, they do not fundamentally resolve the core theoretical concerns or the limitations of the experimental evaluation. The added results, while helpful, don't demonstrate a significant advantage over existing methods.\n\n\n\n**Overall:**\n\nWhile the paper presents an interesting idea and achieves reasonable empirical results, the theoretical weaknesses, limited experimental analysis, and questionable novelty prevent it from being accepted at a top-tier conference. The concerns raised by multiple reviewers regarding the soundness of the method and the lack of a compelling practical advantage are significant. The connection to existing work (GePPO) further reduces the contribution.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – enriching online knowledge distillation with a specialist ensemble created via label prior shift and importance sampling. However, the reviews consistently point to significant weaknesses that, in my opinion, preclude acceptance at a top-tier conference.\n\n**Strengths:** The paper is generally well-written and clear (as noted by reviewers 3, 4, and 5). The idea of formally framing teacher diversification within online KD is potentially valuable, and the empirical analysis is reasonably thorough. Figure 1 is highlighted as a good summary.\n\n**Weaknesses – and why they are critical:**\n\n* **Novelty Concerns (Major):** This is the most significant issue. Reviewers 1, 2, 4, and 5 all express concerns about the novelty of the work. The approach largely combines existing techniques (label prior shift, PC-Softmax, importance sampling) without a compelling justification for *why* this specific combination is superior or offers a fundamentally new insight. The reviewers repeatedly ask for a clearer explanation of *why* these choices were made and how they differ from existing methods.\n* **Insufficient Baselines & Comparison (Major):** Several reviewers (2, 4, and 5) point out that the paper lacks comparison to more recent and relevant baselines. The performance improvements reported are often marginal, and the lack of comparison to stronger methods casts doubt on the significance of the results. Reviewer 4 specifically mentions several missing baselines (CGL, PCL, L-MCL) and reviewer 5 highlights MULTI-VIEW CONTRASTIVE LEARNING.\n* **Ablation Study Gaps (Significant):** Reviewer 1 rightly points out the need for more detailed ablation studies. Specifically, the paper needs to demonstrate the individual contribution of each component (importance sampling vs. direct sampling, the benefit of PC-Softmax, etc.).\n* **Limited Generalizability:** Reviewer 1 questions the specialization to online distillation and peer networks, suggesting the technique might be applicable more broadly. The authors haven’t addressed this adequately, failing to demonstrate why their approach is particularly well-suited to this specific setup.\n* **Minor Issues:** Reviewer 3 notes several typos, which, while not a major issue, detract from the overall polish of the paper.\n* **Reproducibility Concerns:** Reviewer 2 raises a serious concern about the reproducibility of results, noting inconsistencies with published results and code for a baseline method (MVCL).\n\n**Reviewer Disagreement:** While there's consensus on the weaknesses, there's some variation in the severity of the assessments. Reviewer 3 is the most positive, giving high scores for correctness, while reviewer 5 is the most critical, questioning the core claims. This discrepancy further highlights the need for substantial revisions.\n\n**Overall Assessment:** The paper presents an incremental contribution at best. It combines existing techniques in a specific setting without providing a strong justification for its design choices or demonstrating a significant advantage over the state-of-the-art. The lack of comparison to recent baselines and the concerns about reproducibility further weaken the paper. While the writing is clear, the lack of novelty and empirical strength are fatal flaws for a top-tier conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging graph structures derived from object detection to improve out-of-distribution (OOD) detection. The core intuition of representing images as networks of related features is plausible and potentially valuable, particularly for interpretability. However, the paper suffers from significant weaknesses that, in my assessment, preclude its acceptance at a top-tier conference in its current form.\n\n**Strengths:** The idea itself is a positive. The authors demonstrate competitive AUROC scores on the LSUN dataset, suggesting the approach *can* work. The comparison of different graph embedding algorithms is also a positive aspect, providing some insight into the effectiveness of different representation learning techniques within their framework.\n\n**Weaknesses – and they are substantial:**\n\n* **Lack of Rigor and Clarity:** The most consistent criticism across all reviewers is the lack of clarity and rigor. Terms like \"common sense\" are ill-defined. The connection between the introductory claims and the actual implementation is not clearly established. The writing quality is poor, with grammatical errors and missing details hindering understanding.\n* **Insufficient Novelty:** While the combination of object detection and graph-based representations is not entirely trivial, reviewers 3 and 4 correctly point out that the method largely combines existing techniques. The novelty is incremental rather than groundbreaking.\n* **Weak Experimental Validation:** This is a critical flaw. The paper relies on reporting performance comparable to SOTA *without reproducing those results for a controlled comparison*. This is unacceptable for a top-tier conference. The evaluation is limited to the LSUN dataset, which is not extensive enough. Crucially, the paper lacks comparisons to other feature-based OOD detection methods (as highlighted by reviewer 4) and a thorough ablation study.\n* **Dependence on Object Detection Accuracy:** The method's performance is heavily reliant on the accuracy of the pre-trained object detection model. The authors acknowledge this but don't provide a sufficient analysis of how inaccuracies in object detection impact OOD detection performance. This is a significant limitation, especially considering OOD data can challenge object detectors. Reviewer 1 rightly points this out.\n* **Poor Related Work:** Reviewer 2 and 4 both criticize the inadequate discussion of related work, particularly in the context of established OOD detection literature.\n* **Reproducibility Concerns:** The lack of detailed descriptions and missing information makes reproducing the results difficult.\n\n**Reviewer Consensus:** The reviewers are largely aligned in their assessment. Reviewer 2 provides a \"strong reject\" recommendation, while the others suggest revisions. However, the extent of revisions required to address the identified weaknesses is substantial. Even with significant revisions, the fundamental issue of lacking a rigorous comparison to established OOD detection methods remains. The paper's reliance on a pre-trained object detector with potentially limited coverage of in-distribution data is also a major concern.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper proposes a new adversarial training algorithm (ARoW) with a theoretically motivated regularizer. The core idea is to apply more regularization to samples vulnerable to adversarial attacks. The reviewers are largely split, with a significant contingent expressing serious concerns about the theoretical contributions and the marginal empirical improvements.\n\n**Strengths:** Several reviewers (6, 7, 13, 14, 15) acknowledge the clear writing, reasonable approach, and empirical support for the method. The experimental results consistently show *some* improvement over baselines, even if small. The inclusion of ablation studies is also a positive.\n\n**Weaknesses - The Dominant Theme:** The most significant and recurring criticism revolves around the theoretical novelty. Reviewers 5, 9, 10, 11, 12, and 16 all raise substantial doubts about the claimed \"new theorem\" (Theorem 1). The core argument is that the theorem is either a trivial rewriting of existing results (TRADES) for the binary classification case, or a loose and arguably meaningless bound for the multi-classification case. The reviewers highlight inconsistencies in the mathematical definitions and question the justification for the inequalities used. Several reviewers specifically point out that the theoretical justification doesn't seem to strongly support the practical method.\n\nFurthermore, several reviewers (8, 15) point out that the empirical improvements are often marginal, and the comparisons to state-of-the-art baselines, particularly when using extra data, are not entirely convincing. Reviewer 8 specifically notes that HAT performs similarly to ARoW with extra data, and that ARoW doesn't outperform HAT in that setting. Reviewer 15 notes discrepancies between the reported baseline results and those published on RobustBench.\n\n**Rebuttal Impact:** The rebuttal period led to some positive movement. Reviewers 2 and 6 increased their scores after seeing additional experiments. However, the fundamental concerns about the theoretical contribution remain largely unaddressed. The authors defend their theorem as a non-trivial extension to the multi-classification case, but the reviewers remain unconvinced. The authors acknowledge the looseness of the bound but argue it's the \"tightest\" available. This doesn't address the core issue of whether the bound is meaningful or provides genuine insight.\n\n**Overall Assessment:** While the paper is well-written and presents a reasonable approach, the concerns about the theoretical novelty are too significant to ignore. A top-tier conference requires substantial theoretical contributions, and the reviewers overwhelmingly agree that this paper falls short in that regard. The marginal empirical improvements, coupled with the questionable theoretical foundation, do not justify acceptance. The paper feels like an incremental improvement with a somewhat overstated theoretical justification. The extensive debate among reviewers regarding the validity of the theorem is a strong indicator that it doesn't meet the standards for publication in a highly selective conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in reinforcement learning – the lack of diversity in policies learned through self-play. The core idea of explicitly maintaining a population of diverse, high-skill agents using a bi-objective optimization approach is reasonable and potentially valuable. The experiments in Pong and Justice Online demonstrate promising results, showing that the proposed method can achieve competitive or superior performance compared to some baselines.\n\nHowever, the paper suffers from several significant weaknesses that, in my opinion, prevent it from reaching the standards of a top-tier conference. The reviewers consistently point to a lack of engagement with relevant literature. Specifically, the omission of Lanctot et al.'s work on multi-agent reinforcement learning and the failure to compare against quality-diversity algorithms (like MAP-Elites) are major shortcomings. These algorithms represent established approaches to maintaining diversity in agent populations and provide crucial context for evaluating the novelty and significance of the proposed method. Reviewer 3 rightly points out that combining multi-objective optimization with RL is not novel in itself, and the paper doesn't adequately position itself within the existing body of work.\n\nThe experimental design also raises concerns. Reviewer 1 correctly identifies the potential bias introduced by the uneven sampling of opponents during evaluation, which could artificially inflate the performance of the proposed method. While the authors provide sufficient detail for reproducibility, the lack of confidence intervals in the reported results (as noted by Reviewer 2) weakens the statistical significance of the findings. The limited number of domains (Pong being particularly simple) also limits the generalizability of the conclusions. The questions raised by Reviewer 3 regarding scaling to higher-dimensional style spaces and the reasons for performance differences in Justice Online further highlight areas needing more investigation.\n\nWhile the paper is generally well-written and clear, the identified weaknesses in literature review, experimental design, and comparison to related work outweigh the strengths. The contributions, while potentially interesting, are ultimately *marginal* given the existing landscape of research in this area. A major revision addressing these concerns – including a thorough literature review, a more robust experimental setup with appropriate baselines and statistical analysis, and a discussion of the limitations and potential scalability issues – would be necessary for reconsideration.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting empirical observation – swapping the order of Batch Normalization and bounded activation functions (BN after activation) can lead to improved performance. The core idea, and the subsequent analysis attempting to explain *why* this happens through asymmetric saturation and sparsity, is potentially valuable. However, the paper suffers from several significant weaknesses that, in my assessment, prevent it from reaching the standard required for a top-tier conference.\n\n**Strengths:**\n\n* **Novel Observation:** The empirical finding that the swapped order performs better with bounded activations is a worthwhile contribution. It challenges the conventional wisdom and prompts further investigation.\n* **Interesting Hypothesis:** The connection drawn between asymmetric saturation, sparsity, and performance is intriguing and provides a direction for future research.\n* **Reproducibility:** The authors claim to provide sufficient details and code for reproducibility, which is a positive aspect.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Rigorous Explanation:** The reviewers consistently point out a lack of clarity and rigor in the explanations. The connection between asymmetric saturation, sparsity, and generalization is not convincingly established. The arguments often feel speculative rather than data-driven. Specifically, the explanation of *why* the skewness leads to higher sparsity is weak, and the reviewers rightly question the correlation vs. causation.\n* **Limited Generalizability:** The analysis is restricted to architectures *without* residual connections, significantly limiting its applicability. The justification for this limitation (Tanh and residual connections) is also questioned by reviewers and doesn't seem fully convincing. The paper doesn't adequately address how the findings might translate to more common architectures.\n* **Contradictory/Unclear Claims:** Several reviewers highlight inconsistencies and unclear statements within the paper. For example, the relationship between saturation and sparsity is presented in a confusing manner. The definition of \"center of the function\" is vague.\n* **Weak Comparison to ReLU:** The fact that the swapped models with bounded activations still underperform conventional models with ReLU is a major drawback. This undermines the practical significance of the findings. The paper doesn't adequately address this limitation.\n* **Insufficient Addressing of Reviewer Concerns:** While the authors provided responses to reviewers, the revisions don't fully address the core concerns raised. The request for skewness figures for VGG16_11, for example, wasn't adequately fulfilled.\n* **Minor Technical Issues:** Numerous typos and undefined acronyms (NWDBN) detract from the paper's polish.\n\n**Reviewer Consensus:**\n\nThe reviewers are largely aligned in their assessment. While acknowledging the interesting observation, they consistently express concerns about the lack of rigor, limited generalizability, and weak arguments. The scores for correctness, novelty, and empirical significance are generally moderate to low, reflecting these concerns.\n\n\n\nGiven these weaknesses, and the high standards of a top-tier conference, I believe the paper is not ready for publication in its current form. It requires substantial revisions to address the reviewers' concerns, provide more rigorous explanations, and demonstrate broader applicability.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting and potentially impactful idea – Functional Risk Minimization (FRM) – challenging the standard assumption of a single underlying function with additive noise. The core concept of modeling each data point as originating from its own function is novel and could offer a more flexible framework for handling real-world data variations. However, the paper suffers from significant weaknesses in clarity, theoretical grounding, and scalability, as highlighted by the reviewers.\n\nReviewer 1 provides the most comprehensive and critical assessment. Their points regarding the disconnect between the theoretical presentation and the implementable algorithm (Equations 7 & 8 lacking derivation), the scalability issues due to Hessian calculations, and the absence of theoretical guarantees (low loss, approximation accuracy, generalization) are all substantial concerns. The reviewer also correctly points out the confusing use of terminology (\"ERM\" and \"FRM\") and the lack of a clear definition of the FRM learning objective.\n\nReviewer 2 raises valid questions about the relationship between FRM and hierarchical Bayesian models, and the inconsistency between the training and testing assumptions. The lack of experimental evaluation for the functional generative model is also a notable omission.\n\nReviewer 3 is the most positive, but even they acknowledge the high computational cost and suggest the authors could better justify the necessity of FRM over modifications to existing ERM approaches.\n\nThe authors' abstract claims \"better performance in small experiments,\" but the reviewers don't indicate these experiments are particularly compelling or demonstrate a significant advantage over existing methods. The small scale of the experiments, as noted by Reviewer 1, further weakens this claim.\n\nWhile the novelty is high (scores of 4 from two reviewers), the lack of theoretical support, clarity issues, and scalability concerns are critical flaws for a top-tier conference. The paper feels more like a promising initial exploration of an idea than a fully developed, rigorously justified contribution. Addressing the reviewers' concerns would require substantial revisions, including a more formal derivation of the algorithm, theoretical analysis, clarification of terminology, and potentially, exploration of more scalable approximations. Without these revisions, the paper falls short of the standards expected for acceptance at a prestigious conference.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper attempts to improve PPI prediction by incorporating pre-trained protein structure embeddings from OmegaFold into a GNN framework. While the motivation – leveraging structural information for PPI prediction – is sound, the paper suffers from significant weaknesses that, in my assessment, disqualify it from acceptance at a top-tier conference like ICLR.\n\nThe core issue is a lack of novelty and a failure to convincingly demonstrate the benefits of the proposed approach. Reviewers consistently point out that the method largely consists of applying existing techniques (OmegaFold embeddings + GNNs) without substantial modification or innovation. The authors essentially adopt a pre-trained embedding model and feed its output into a standard GNN architecture. This is not the kind of novel contribution expected at a top conference.\n\nFurthermore, the empirical evaluation is weak. The improvements reported are marginal, and the baseline comparisons are inadequate. Reviewers 3 and 4 rightly criticize the choice of baselines (ResNet, GraphSAGE/GAT) as being too weak and not representative of the state-of-the-art in PPI prediction. The absence of comparison to AlphaFold-Multimer, a highly relevant and powerful method, is a major omission. The lack of confidence intervals in the results also weakens the conclusions.\n\nSeveral specific concerns raised by the reviewers are also compelling. The inconsistency in results between tables (Reviewer 1), the unclear setup of the binary classification problem and AUC measurement (Reviewer 3), and the questionable necessity of the GNN layer given the pairwise nature of the PPI prediction task (Reviewer 4) all point to a lack of rigor and clarity in the methodology. The mean pooling strategy for handling variable-length embeddings is also flagged as a potential limitation.\n\nThe author responses, while addressing some minor points, do not fundamentally address the core criticisms regarding novelty, baseline comparisons, and the justification for the GNN architecture. The authors seem to defend their choices without adequately explaining *why* these choices were made and *how* they contribute to a significant advancement in the field.\n\nThe paper also suffers from clarity and presentation issues, as noted by multiple reviewers. The writing could be improved, and the tables could be presented more effectively. While these are minor issues, they contribute to the overall impression of a paper that is not polished and ready for publication in a highly competitive venue.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – understanding the convergence of gradient descent in deep linear networks – and presents technically sound results. However, the reviewers raise significant concerns regarding novelty and the depth of discussion, which ultimately weigh against acceptance in its current form.\n\nHere's a breakdown of the key points:\n\n**Strengths:**\n\n* **Technical Correctness:** The majority of reviewers (4, 5, and 1 after updates) agree the paper is technically correct and the proofs appear rigorous.\n* **Clear Presentation:** Reviewers 4 and 1 commend the paper's clarity and readability.\n* **Intriguing Trajectory Result (Theorem 3.3):** Reviewer 3 highlights this as a potentially novel and insightful contribution, suggesting the optimization trajectory mirrors that of the convex counterpart.\n* **Generalization:** The paper extends results to more general loss functions and varying layer widths.\n\n**Weaknesses & Concerns:**\n\n* **Limited Novelty (Major Concern):** This is the most consistent criticism. Reviewers 3, 5, and 1 all question the novelty of the core convergence results (Theorems B.1, B.2, and B.3). They point out that similar rates were achieved in prior work (Du & Hu 2019, Hu et al. 2020), and the extension to varying layer widths seems incremental. Reviewer 1 specifically questions the novelty of extending the loss function.\n* **Lack of Discussion & Insight (Reviewer 6):** This is a critical point. Reviewer 6 argues the paper lacks sufficient discussion around *why* the results hold, particularly regarding the importance of overparameterization and how it avoids issues like vanishing Hessians. They also point out overclaims in the title and abstract.\n* **Fairness of Comparison (Reviewer 1):** The comparison to Du & Hu (2019) regarding the operator P(t) is deemed unfair, suggesting the paper obscures the underlying similarities in technique.\n* **Minor Technical Issues:** Reviewers 3 and 6 raise several minor technical points regarding equations, definitions, and the presentation of results. These are less critical than the novelty and discussion concerns but contribute to the overall impression of a paper that could be more polished.\n* **Local Convergence (Reviewer 4):** The analysis is limited to local convergence, which is a constraint.\n\n**Author Responses:**\n\nThe author responses address some of the reviewer concerns, particularly regarding Section 4. However, they don't fully resolve the core issue of novelty. The authors need to more convincingly demonstrate the significance of their extensions beyond simply stating they generalize previous results.\n\n\n\n**Overall Assessment:**\n\nWhile the paper is technically sound and presents a clear analysis, the concerns about novelty and the lack of in-depth discussion are substantial. A top-tier conference demands significant contributions, and this paper, in its current form, falls short of that standard. The trajectory result (Theorem 3.3) is promising, but it's not enough to outweigh the other weaknesses. The paper would benefit significantly from a more thorough discussion of the implications of the results, a more nuanced comparison to prior work, and a more careful framing of the contributions.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting approach to protein sequence and structure co-design, addressing a significant challenge in bioengineering. The core idea of iterative translation with equivariant layers is novel and appears to offer substantial improvements in inference speed compared to existing methods like diffusion models. The experimental results demonstrate strong performance across multiple tasks, often outperforming state-of-the-art baselines.\n\nHowever, the reviews reveal several concerns that need careful consideration. A recurring theme is the degree of novelty. While the combination of techniques is presented as novel, several reviewers point out similarities to AlphaFold2 and existing co-design methods (like Jin's work). The authors need to more carefully position their work within the existing landscape and clearly articulate the specific contributions beyond simply applying existing concepts to a new domain. The claim of being the \"first protein sequence-structure co-design\" is strongly contested and needs to be toned down.\n\nAnother significant concern is the clarity and completeness of the method description. Several reviewers (3, 6) specifically request a clear equation for SeqIPA and more detailed explanations of certain algorithmic components (Algorithm 1, equations 7-8, 11). The lack of hyperparameter details and training specifics (Reviewer 5) is also a weakness, hindering reproducibility. The discrepancy in RMSD scores compared to published results for Diffusion models (Reviewer 5) needs to be addressed.\n\nFurthermore, the functional relevance of the evaluation metrics is questioned (Reviewer 5, 7). While PPL, RMSD, and AAR are useful, they don't necessarily correlate with protein functionality. Including functional-oriented evaluations would strengthen the paper. The low AAR scores across all methods raise concerns about the quality of the designs.\n\nDespite these weaknesses, the overall sentiment from the reviewers is positive. Reviewers 1, 7, and 8 explicitly recommend acceptance. The authors have engaged in good communication with the reviewers, addressing many of their initial concerns. Reviewer 1 even increased their score after the rebuttal. The improvements in inference speed are a significant practical benefit.\n\nConsidering the balance between the strengths and weaknesses, and the positive feedback from multiple reviewers, I believe the paper is suitable for publication *with minor revisions*. The authors must address the remaining concerns regarding clarity, completeness of the method description, and the novelty claims. A more nuanced discussion of the limitations and a stronger emphasis on the practical benefits of the speed improvements are also necessary.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging the complementary strengths of different adversarial training methods through a collaborative framework (CAT). The core observation that different defenses fail on different instances is reasonable and forms a plausible motivation. The experimental results, particularly achieving SOTA on CIFAR-10 under AutoAttack, are encouraging. However, the paper falls short of the standards expected for a top-tier conference due to several significant weaknesses highlighted by the reviewers.\n\n**Novelty:** The biggest concern is the limited novelty. All four reviewers point out that the core idea of combining defenses isn’t new. Ensemble methods for adversarial robustness are well-established (as pointed out by Reviewer 1 with citations), and related work exists on knowledge fusion for robustness (Reviewer 2 cites several papers). While CAT presents a specific *way* to combine defenses, it doesn’t fundamentally introduce a new concept. The authors need to more thoroughly position their work within the existing literature and clearly articulate what distinguishes CAT from prior approaches.\n\n**Theoretical Analysis:** The lack of theoretical justification is a major drawback (Reviewer 1). The paper doesn’t explain *why* this collaborative training leads to improved robustness. Is it simply averaging predictions? Is it encouraging the models to learn more diverse features? Without a theoretical understanding, the method feels somewhat ad-hoc.\n\n**Experimental Evaluation:** The experimental evaluation, while extensive on CIFAR-10/100, is insufficient. Reviewers 3 and 4 correctly point out the need for evaluation on larger datasets like ImageNet and with more diverse network architectures (VGG, MobileNet). The absence of comparisons to stronger baselines, such as those incorporating AWP (Adversarial Weight Perturbation) as suggested by Reviewer 4, weakens the claims of SOTA performance. Furthermore, the reviewers request crucial ablations (TRADES-TRADES, more attackers, etc.) that would help understand the method's behavior and contributions. The confusion matrix analysis suggested by Reviewer 4 would also be highly valuable in motivating the approach.\n\n**Efficiency:** The increased computational cost (training two models) is a significant practical concern, and the authors don't adequately address this. The reviewers rightly question whether the benefits justify the added expense.\n\n**Clarity & Correctness:** While generally well-written, the paper contains typos (Reviewer 1) and lacks clarity in certain areas (e.g., the unexplained abbreviation \"ALP\" - Reviewer 2).\n\n\n\nIn summary, while the idea is interesting and the initial results are promising, the paper lacks sufficient novelty, theoretical grounding, and comprehensive experimental validation to warrant acceptance at a top-tier conference. The reviewers consistently raise valid concerns that need to be addressed through significant revisions.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting empirical observation – swapping the order of Batch Normalization and bounded activation functions (BN after activation) can lead to improved performance. The authors attempt to explain this phenomenon through the lens of asymmetric saturation and resulting sparsity. However, the paper suffers from several significant weaknesses that, despite the novelty of the observation, ultimately outweigh its strengths.\n\n**Strengths:**\n\n* **Novel Observation:** The core finding – that swapping BN and activation order matters for bounded activations – is a potentially valuable contribution. This challenges the common practice of placing BN before activations.\n* **Interesting Hypothesis:** The connection made between asymmetric saturation, sparsity, and performance is intriguing and motivates further investigation.\n* **Reproducibility:** The authors claim to provide sufficient details and code for reproducibility, which is a positive aspect.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Rigorous Explanation:** The reviewers consistently point out a lack of clarity and rigor in the explanations. The connection between asymmetric saturation, sparsity, and generalization is not convincingly demonstrated. The authors often state correlations without establishing causality. Several reviewers question the logic behind claims, particularly regarding the relationship between saturation and sparsity (contradictory statements are noted).\n* **Limited Generalizability:** The analysis is limited to architectures *without* residual connections, significantly restricting its applicability. The justification for this limitation (Tanh and residual connections) is also questioned by reviewers. The paper doesn't convincingly demonstrate that the findings extend beyond the specific architectures tested.\n* **Performance Compared to ReLU:** A major drawback is that the \"Swap\" models with bounded activations consistently underperform compared to \"Conventional\" models with ReLU. This severely diminishes the practical significance of the findings. The reviewers rightly question the value of a performance boost with a less commonly used activation function if it still lags behind the standard.\n* **Unclear Definitions & Typos:** Several reviewers highlight issues with definitions (\"center of the function\") and technical errors (typos in LeCun Tanh). These issues detract from the paper's credibility.\n* **Incomplete Analysis & Unaddressed Concerns:** Reviewers raise valid points about the behavior of saturation in deeper layers, the impact of different hyperparameters (weight decay), and the potential for accelerated training rather than improved generalization. The authors' responses, as reflected in the reviewer comments, do not fully address these concerns.\n* **Metric Issues:** The saturation metric itself is questioned by reviewers, with concerns about its sensitivity to centering and potential for misleading results.\n\n**Reviewer Consensus:**\n\nThe reviewers are largely aligned in their assessment. While acknowledging the interesting observation, they consistently express concerns about the lack of rigorous justification, limited generalizability, and the practical relevance of the findings. The scores for correctness, technical novelty, and empirical significance are generally moderate to low, reflecting these concerns.\n\n\n\nGiven the significant weaknesses and the lack of a compelling argument for the practical importance of the findings, the paper, in its current form, does not meet the standards of a top-tier conference. While the observation is interesting, it requires substantially more rigorous analysis, broader experimentation, and clearer explanations to be considered a significant contribution.\n\nFinal Deci"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and timely problem: data leakage in federated learning applied to tabular data. The motivation is strong, as tabular data is prevalent in high-stakes applications where privacy is paramount. The authors identify key challenges specific to tabular data – mixed discrete-continuous features, high variance, and difficulty in assessing reconstruction quality – and propose a method, TabLeak, to address them.\n\nThe reviewers offer a mixed assessment. Reviewer 1 highlights the poor writing quality and lack of novelty, while Reviewer 2 echoes the novelty concerns and points out the use of standard techniques (softmax relaxation and ensembling). However, Reviewer 3 provides a much more positive evaluation, praising the simplicity, experimental demonstration, and the entropy-based heuristic. They also acknowledge the novelty of applying gradient attacks to tabular data.\n\nI agree with Reviewer 3's overall assessment. While the individual components of TabLeak (softmax, ensembling, entropy) are not groundbreaking in isolation, their *combination* and adaptation to the specific challenges of tabular data *is* a valuable contribution. The softmax relaxation cleverly addresses the mixed discrete-continuous optimization problem, and the pooled ensembling and entropy measure appear to be effective in mitigating variance and assessing reconstruction quality, respectively. The 10% improvement over baselines, particularly at a practically relevant batch size of 32, is a meaningful result.\n\nThe criticisms regarding writing quality (Reviewer 1) are valid and need to be addressed. Clarity is crucial for a top-tier conference. However, this is a fixable issue. The concerns about novelty (Reviewer 1 & 2) are less severe. While the techniques aren't entirely new, their application and combination in this context *are* novel, and the experimental results demonstrate their effectiveness. Reviewer 2's suggestions for alternative optimization techniques (Gumbel trick, branch-and-bound) are valid points for future work, but don't invalidate the current contribution. Reviewer 3’s detailed comments regarding justifications and studies of pooling, and the entropy measure are insightful and should be addressed in a revision.\n\nThe availability of source code and the reproducibility of experiments (as noted by Reviewer 2) are significant positives. The detailed experimental analysis, including ablation studies, is also commendable. The minor typos and comments raised by Reviewer 3 are easily addressable.\n\nOverall, the paper presents a solid contribution to the field of federated learning security. The identified problem is important, the proposed solution is reasonable and effective, and the experimental results are convincing. The writing quality needs improvement, and addressing the detailed comments from Reviewer 3 would strengthen the paper further. However, the core contribution is sufficient to warrant acceptance, contingent on a thorough revision addressing the reviewers’ concerns.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "## Detailed Reasoning\n\nThis paper tackles the important problem of label differential privacy for regression, extending concepts from classification. The core idea – a “randomized response on bins” – appears novel and the claim of optimality under a given loss function is significant. The experimental results demonstrate efficacy compared to baseline DP methods. However, a careful review of the reviewer comments reveals several points of concern that, while not necessarily fatal, raise questions about the paper’s readiness for a top-tier conference.\n\n**Strengths:**\n\n* **Clear Writing:** Multiple reviewers (1, 4, 5, 6) commend the paper's clarity and ease of understanding. This is a significant positive.\n* **Novelty (to a degree):** Reviewers 3, 4, and 6 acknowledge the novelty of the approach, though some (5) point out its relationship to prior work (Ghazi et al., 2021). The extension to general regression losses is a positive contribution.\n* **Empirical Validation:** The experiments on real-world datasets provide evidence supporting the proposed method's effectiveness.\n\n**Weaknesses & Concerns:**\n\n* **Optimality Questioned:** Reviewers 4 and 5 raise a critical point regarding the optimality claim. The optimality is defined with respect to a specific term in Equation (1), but doesn't account for the impact on the overall Bayes optimal error (specifically, the variance of predictions). This is a substantial theoretical limitation that needs to be addressed.\n* **Comparison to Related Work:** Reviewer 3 correctly points out the lack of comparison to Ghazi et al. (2021), a relevant work in label DP. While the authors address this in their response, the comparison is relegated to the appendix and acknowledged as not entirely fair. This suggests a potential reluctance to directly compare against a strong baseline.\n* **Practical Considerations:** Reviewer 5 highlights a practical limitation: the label party often has access to feature data. This isn't a flaw in the method itself, but it limits its applicability in certain real-world scenarios and should be discussed more thoroughly.\n* **Binning Issue:** Reviewer 6 raises a concern about mapping continuous label spaces to finite bins, which is a potential simplification that could impact performance.\n* **Minor Issues:** Reviewer 6 also notes typos, indicating a need for careful proofreading.\n* **Novelty Assessment:** Reviewer 5 explicitly states the method is \"not technically novel\" compared to Ghazi et al. (2021), which weakens the argument for acceptance at a *top-tier* conference. The extension to regression losses, while valuable, might not be enough to overcome this.\n\n**Overall Assessment:**\n\nThe paper presents a reasonable approach to label DP for regression. The writing is strong, and the experimental results are promising. However, the concerns raised by reviewers 4 and 5 regarding the optimality claim and the limited novelty are significant. The lack of a strong comparison to Ghazi et al. (2021) further weakens the paper. While the authors responded to reviewer comments, the core theoretical concerns remain unaddressed. A top-tier conference demands a higher level of theoretical rigor and novelty.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an interesting and relevant problem – identifying phase transitions in permuted linear regression. The theoretical analysis using message passing is a reasonable approach. However, the reviewers consistently point to significant issues with clarity, rigor, and novelty that, in my assessment, outweigh the potential contributions.\n\nHere's a breakdown of the key concerns, synthesizing the reviewer feedback:\n\n**1. Clarity and Writing:** This is the most pervasive issue. Multiple reviewers (1, 2, 3, 5) highlight the paper's poor writing, heavy notation, and lack of self-containment. Reviewer 5 is particularly scathing, stating the writing is so poor it's inaccessible to a general ICLR reader. The reliance on external references (Semerjian et al., Mezard & Montanari) without sufficient explanation within the paper creates a steep barrier to understanding. The numerous typos and grammatical errors further detract from clarity.\n\n**2. Novelty:** While the authors claim to provide a \"precise identification\" of phase transition thresholds, reviewers question the incremental nature of the contribution. Reviewer 1 notes the novelty lies in an approximation, while Reviewer 2 suggests the oracle case results are largely a corollary of existing work. Reviewer 4 acknowledges novelty in the precise characterization of the SNR threshold but notes the connection to prior work on branching random walks. Reviewer 5 is skeptical of claims of \"first framework\" or \"generalization\" without sufficient justification.\n\n**3. Rigor and Correctness:** Several reviewers raise concerns about the mathematical rigor. Reviewer 2 points out inconsistencies and a lack of rigor in the theoretical results. Reviewer 5 identifies specific inaccuracies in definitions and equations. The contradiction between claiming \"precise\" thresholds and then using approximations (Section 4.2) is also problematic.\n\n**4. Reproducibility:** The lack of publicly available code hinders reproducibility, as noted by Reviewer 4.\n\n**5. Practical Relevance:** Reviewer 2 questions the practical meaningfulness of the oracle case and the motivation for partial permutation recovery. The claim about empirical results aligning with theoretical predictions is weakened by the fact that the theoretical results are asymptotic and the empirical results are for finite samples.\n\n**Overall Assessment:**\n\nThe paper attempts to address a challenging problem, but it falls short of the standards expected for a top-tier conference like ICLR. The significant issues with clarity, rigor, and the questionable incremental novelty make it unsuitable for publication in its current form. While the underlying ideas might have merit, they are obscured by the presentation. A substantial rewrite, addressing the reviewers' concerns, would be necessary before reconsideration. The reviewers are largely aligned in their negative assessment, with even the more positive reviews (3, 4) highlighting significant weaknesses.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents the RuDar dataset, a new weather radar dataset for precipitation nowcasting, covering a geographically and climatically diverse region of Russia. The dataset incorporates 3D radar echo observations and orographic information, addressing a gap in existing datasets that often focus on single-level or aggregated data. The authors also provide baseline evaluations using several models and explore uncertainty quantification.\n\nThe reviewers largely agree on the strengths and weaknesses of the paper. The primary strength is the dataset itself – its size, dimensionality, geographical coverage, and publicly available nature are all positive aspects. Reviewers 1 and 2 specifically highlight the dataset's potential usefulness. Reviewer 3 emphasizes the richness of the data, including altitude levels and additional fields.\n\nHowever, the reviewers consistently point to a lack of novelty as a major concern. Reviewer 3 is particularly blunt, stating the dataset is \"too similar to others\" for a top-tier conference like ICLR. Reviewers 1 and 2 also assign low scores to technical and empirical novelty. The baseline methods used are considered \"naïve\" (Reviewer 2) and \"low-cost\" (Reviewer 1), and there's a call for exploring more sophisticated models like transformers. Reviewer 2 also points to a lack of ablation studies and suggests experiments to validate claims about the dataset's applicability to other ML tasks. Reviewer 3 questions the dataset's resolution, suggesting it might be limiting for precipitation forecasting.\n\nThe evaluation metrics are also criticized (Reviewer 2), with MSE being insufficient to reflect nowcasting accuracy. The lack of comparison to previous methods *on* the proposed dataset is also noted. While the authors claim correctness and reproducibility, the limited novelty and the shortcomings in the experimental setup raise concerns about the paper's suitability for a highly competitive conference.\n\nThe authors respond to the reviewers (responses not provided here, but implied by the review scores), but the core issue of limited novelty remains. While a valuable resource for the community, the paper, as presented, feels more like a dataset release paper suitable for a journal focused on datasets rather than a conference emphasizing novel algorithms or significant methodological advancements. The empirical contribution, while present, doesn't appear substantial enough to overcome the lack of novelty in the methods.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and challenging problem: moving beyond simply proving convergence of no-regret learning dynamics to analyzing their *performance* – specifically, the quality of the equilibria they reach. The motivation is sound, and the investigation of average-case performance is a natural next step. However, the reviewers consistently point to significant limitations that, in my assessment, prevent this paper from reaching the standards of a top-tier conference.\n\nThe core issue is the limited scope of the results. While the authors claim a “systematic comparison” and a “framework for initiating…comparison,” the formal results are largely confined to symmetric 2x2 coordination games. This severely restricts the generalizability and impact of the work. Reviewers 3 and 4 explicitly highlight this limitation, with Reviewer 4 noting it's a \"rather restricted case.\"\n\nFurthermore, the novelty of the work is heavily questioned, particularly by Reviewer 2, who correctly points out the significant overlap with prior work by PP16. The authors acknowledge this in their response, but the reviewers feel the paper initially downplayed the contributions of PP16. While the authors expanded the discussion of PP16 post-review, the fundamental concern remains: the paper builds upon existing work with incremental improvements rather than presenting a truly novel breakthrough. Reviewers 1, 2, and 3 all assign low scores to novelty.\n\nSeveral minor issues, consistently raised by multiple reviewers, further detract from the paper's quality. These include imprecise language (e.g., \"whenever\" vs. \"if and only if,\" monotonicity of stable manifolds), notational overloading, an inaccurate abstract, and formatting inconsistencies. While these are fixable, they indicate a lack of polish expected at a top-tier conference.\n\nThe empirical results, while potentially promising, are presented as preliminary and lack rigorous justification for generalization beyond the 2x2 case. The reliance on plots without formal proofs (as noted by Reviewer 3) weakens the conclusions.\n\nReviewer 4 initially recommended acceptance but lowered their assessment after discussion, acknowledging the limitations in scope and contribution. The consensus among the reviewers, even after rebuttal, leans towards a weaker evaluation of novelty and significance. While the paper is technically correct and well-written, these factors are insufficient to overcome the substantial limitations in scope, novelty, and generalizability. The paper feels more appropriate for a workshop or a more specialized conference than a top-tier venue.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important and practically relevant problem: adapting machine learning models in online settings where both label shift and conditional shift are present. The motivation is sound, and the exploration of how existing label shift adaptation methods perform under more realistic distributional shifts is valuable. However, the paper suffers from several significant weaknesses that, in my assessment, preclude its acceptance at a top-tier conference in its current form.\n\n**Strengths:**\n\n* **Relevant Problem:** The problem of distribution shift, especially in online learning scenarios, is a crucial area of research.\n* **Empirical Investigation:** The authors perform experiments on several datasets, including both synthetic and real-world examples.\n* **Honest Assessment:** The authors are appropriately cautious in their claims and acknowledge the inconclusive nature of some results.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Systematic Approach:** The most significant weakness is the lack of a systematic framework for analyzing the types of distribution shifts considered. Simply adding \"conditional shift\" to \"label shift\" doesn't provide a clear understanding of the underlying problem. The reviewers rightly point out the broadness of \"general distribution shift\" and the need for a more focused analysis.\n* **Limited Empirical Evaluation:** While experiments are conducted, they are not comprehensive enough to draw strong conclusions. The reviewers suggest several important baselines (offline domain adaptation, domain generalization methods) that are missing. The limited scope makes it difficult to generalize the findings.\n* **Heuristic Nature & Lack of Theoretical Justification:** The proposed heuristics are largely ad-hoc and lack a strong theoretical foundation. While empirical validation is important, the absence of theoretical grounding weakens the contribution.\n* **Clarity and Completeness:** Several reviewers highlight issues with clarity, notation, and missing details. The explanations of the heuristics, particularly the Bayesian approach, are confusing. The lack of code availability further hinders reproducibility.\n* **Modest Improvements:** The empirical results, as pointed out by multiple reviewers, show only marginal improvements over existing methods. The gains are not substantial enough to justify publication in a highly competitive venue.\n* **OOD Validation Concerns:** The use of an OOD validation set is a central tenet of the paper, but its effectiveness is not consistently demonstrated across all datasets. The reviewers raise valid concerns about its applicability and the conditions under which it might be beneficial.\n\n**Reviewer Consensus:**\n\nAll four reviewers express significant reservations about the paper. While they acknowledge the relevance of the problem, they consistently criticize the lack of theoretical grounding, limited empirical evaluation, and clarity issues. The scores for novelty and significance are consistently low.\n\n**Overall Assessment:**\n\nThe paper presents an initial exploration of an interesting problem, but it falls short of the standards required for a top-tier conference. The lack of a systematic approach, limited empirical evaluation, and weak theoretical justification are major drawbacks. While the authors are honest about the limitations of their work, the paper needs substantial revisions to address these concerns before it can be considered for publication. The current version feels more like a preliminary study or a work-in-progress.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem – the trade-off between training data size and validation set quality – and proposes a novel approach, the Proximal Validation Protocol (PVP). The core idea of constructing a validation set via data augmentation and distributional consistency is interesting and potentially valuable, especially given the practical challenges of validation set construction in real-world ML deployments. However, the paper suffers from significant weaknesses in clarity, experimental thoroughness, and theoretical justification, leading to a consensus among reviewers leaning towards rejection.\n\nHere's a breakdown of the issues, categorized by severity:\n\n**Major Concerns (requiring substantial revision):**\n\n* **Lack of Model Selection Focus:** The most critical weakness, highlighted by Reviewer 1, is the paper's exclusive focus on evaluating a *fixed* model. The purpose of a validation set is fundamentally to *select* the best model or hyperparameters. Demonstrating that PVP improves validation accuracy for a pre-defined model is insufficient; the paper needs to show that it leads to better model selection outcomes.\n* **Unclear Experiments & Metrics:** Reviewer 1 correctly points out significant clarity issues in the experimental section, particularly regarding the definition and interpretation of the \"F1\" score and the meaning of the \"deterioration\" values in Tables 3 & 4. The lack of clarity hinders proper evaluation of the results.\n* **Limited Dataset Scale & Generalizability:** Reviewer 2 raises a valid concern about the small size of the datasets used in the experiments. The results on CIFAR-LT and CIFAR-100-LT, while potentially indicative, don't provide strong evidence of generalizability to larger, more complex datasets like ImageNet.\n* **Missing Theoretical Foundation:** Reviewers 3 & 4 both emphasize the lack of theoretical justification for PVP. While the authors acknowledge this as a limitation, the absence of any theoretical analysis weakens the paper's contribution. Why *should* a validation set constructed in this way be representative of the true data distribution?\n* **Ablation Studies & Robustness:** Reviewer 3 and 1 request more ablation studies to assess the sensitivity of PVP to choices like data augmentation techniques and weighting schemes. Demonstrating robustness to these choices is crucial.\n* **Limited Scope:** Reviewer 3 and 4 point out the method seems tailored to classification and its applicability to other learning paradigms (regression, unsupervised learning) is unclear.\n\n**Minor Concerns (addressable with revisions):**\n\n* **Writing Style:** Reviewer 1 notes some unusual language choices that detract from the paper's professionalism.\n* **Statistical Significance:** Reviewer 3 requests statistical significance testing, which is essential for a rigorous evaluation.\n* **Alternative Approaches:** Reviewer 3 suggests a baseline comparison involving merging the proximal validation set with the training data, which could provide valuable insights.\n\n\n\n**Reviewer Consensus:**\n\nThe reviewers are split, but the majority (3/4) express significant reservations. Reviewer 4 recommends a clear rejection, while the others suggest a \"weak reject\" or \"borderline\" decision. The common thread is that the paper, while promising in its idea, lacks the rigor and thoroughness expected of a top-tier conference publication.\n\n\n\nGiven these concerns, the paper, in its current form, does not meet the standards for acceptance at a prestigious academic conference. The core idea is interesting, but the lack of a focus on model selection, the unclear experiments, the limited dataset scale, and the absence of theoretical justification are substantial weaknesses that need to be addressed.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper tackles an important problem in Federated Learning – the impact of data heterogeneity on convergence. The authors identify “period drift” as a contributing factor alongside the well-known “client drift” and propose FedPA, a meta-learning based aggregation strategy, to mitigate these issues. However, despite identifying a potentially valuable problem, the paper suffers from several significant weaknesses that, in my assessment, prevent it from reaching the standard required for a top-tier conference.\n\n**Strengths:** The identification of “period drift” is a potentially valuable contribution, as it highlights a less-explored aspect of FL challenges. The idea of learning an aggregation strategy is also reasonable. Reviewers 2 and 4 acknowledge the importance of the problem and the potential of the approach.\n\n**Weaknesses:** The most significant weakness is the lack of convincing evidence that FedPA demonstrably addresses *both* client and period drift better than existing methods. Reviewer 1 rightly points out the lack of discrimination between the two drifts, both theoretically and experimentally. It’s unclear whether the observed improvements are due to addressing period drift specifically, or simply a consequence of the meta-learning framework acting as a more sophisticated regularizer.\n\nThe reliance on a proxy dataset is a major limitation. As highlighted by multiple reviewers (1 & 3), this limits the applicability of the method and introduces a strong assumption. The justification for the proxy dataset is also weak and somewhat concerning (\"since many companies...\"). Furthermore, the similarity to existing knowledge distillation techniques (FedET, DS-FL) is not adequately addressed. A thorough comparison with these methods is crucial to justify the novelty and usefulness of FedPA.\n\nThe experimental evaluation is also insufficient. The datasets (EMNIST, MovieLens) and models are considered too simple by Reviewer 1, and the performance on EMNIST is questioned by Reviewer 3. Expanding the experiments to more challenging datasets like CIFAR-100 and Stack Overflow is necessary. The lack of ablation studies, as noted by multiple reviewers, is a significant omission. Understanding the impact of the proxy data distribution and the aggregator model size is crucial.\n\nFinally, the clarity and quality of the writing need substantial improvement. Multiple reviewers point out grammatical errors, inconsistencies, and a lack of clarity in explanations (e.g., Equation 7, the dynamic system analogy). The lack of code availability hinders reproducibility.\n\n**Reviewer Consensus:** The reviewers are generally leaning towards rejection. While some acknowledge the potential of the idea, the concerns regarding novelty, experimental validation, and clarity are consistently raised. The scores for correctness, technical novelty, and empirical novelty are all relatively low, further supporting a rejection decision.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents an interesting idea – leveraging different label representations, specifically binary labels, within a multi-task learning framework to improve image classification performance. The core concept of encouraging the model to learn relationships between classes through this binary representation is intuitively appealing. However, the paper falls short of the standards expected for a top-tier conference like ICLR due to a combination of weaknesses identified by the reviewers.\n\n**Strengths:** The novelty of the approach is acknowledged by multiple reviewers (R3, R4). The idea is relatively simple to implement and shows some performance gains across several datasets. The potential for improved data efficiency is also a positive aspect.\n\n**Weaknesses (and why they are critical):**\n\n* **Lack of Rigorous Analysis:** The most consistent criticism across all reviewers (R1, R2, R4) is the lack of in-depth analysis. The paper doesn't adequately explain *why* this method works, or under what conditions it's most effective. The reviewers rightly point out the need to explore factors like dataset characteristics (fine-grained vs. coarse-grained labels), task weighting, and the impact of different binary label generation strategies.\n* **Weak Experimental Evaluation:** The experimental setup is insufficient. Reviewers (R1, R2, R4) highlight the lack of comparison to strong baselines (SOTA methods), the absence of standard deviation reporting, and the limited scope of experiments (no ImageNet, no multi-label classification). The data efficiency metric is also questioned (R4). The reported improvements are often small, making a robust evaluation even more crucial.\n* **Clarity and Reproducibility Issues:** Several reviewers (R1, R2, R4) identify clarity issues in the paper, including undefined terms (\"dense labels,\" \"Metabalance\"), ambiguous descriptions of the method, and inconsistencies in calculations. Reproducibility is also a concern due to missing implementation details.\n* **Related Work:** The paper doesn’t adequately address related work, particularly in areas like label smoothing, knowledge distillation, and multi-class label coding (as pointed out by R2 and R3). This omission weakens the novelty claim.\n* **Motivation & Intuition:** Reviewer 4 succinctly points out a critical flaw: the paper doesn't clearly articulate *what problem* the binary labels are solving or *what properties* they model that one-hot labels don't. This lack of a strong theoretical grounding undermines the contribution.\n\n**Reviewer Disagreement:** While there's general consensus on the weaknesses, there's some variation in the severity of the assessments. Reviewer 3 is the most positive, while Reviewer 2 is the most critical. However, the majority of reviewers (1, 2, 4) express significant reservations.\n\n**Overall Assessment:** The paper presents a potentially interesting idea, but it's presented in a preliminary and underdeveloped state. The lack of rigorous analysis, weak experimental evaluation, and clarity issues prevent it from meeting the high standards of a top-tier conference. The reviewers' comments collectively indicate that substantial revisions are needed to address these shortcomings. The paper, as it stands, feels more like a proof-of-concept than a fully-fledged research contribution.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper, \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning,\" attempts to address the important problem of privacy in Machine Learning as a Service (MLaaS). However, based on the reviewer comments, the paper suffers from significant flaws – both technical and empirical – that preclude its acceptance at a top-tier conference like ICLR.\n\n**Major Concerns:**\n\n* **Fundamental Technical Flaws (Reviewer 2):** The most damning critique comes from Reviewer 2, who identifies a critical flaw in the proposed approach. The server can reconstruct a \"surrogate\" encoder and essentially recover the client's data through model inversion, rendering the privacy guarantees illusory. This fundamentally undermines the core contribution of the paper. The reviewer also correctly points out the inconsistency between the stated goal of privacy-preserving *inference* and the *training* requirement of the encoder, which exposes the client's labels and class centers to the server.\n* **Weak Threat Model (Reviewer 2):** The assumption that the server will honestly implement the center loss is unrealistic and naive. A malicious server could simply disable it and reconstruct the data.\n* **Limited Empirical Evaluation (Reviewers 1 & 2):** The experimental evaluation is weak. The datasets are simplistic (MNIST, CelebA), and the comparison to baselines is insufficient. Reviewer 1 specifically calls for comparisons to Fully Homomorphic Encryption (FHE) approaches, while Reviewer 2 suggests more challenging tasks and modern architectures (ResNets, Vision Transformers). The claim of privacy preservation is made based on a limited and potentially misleading scenario (expression recognition from gender recognition).\n* **Lack of Novelty (Reviewers 1 & 2):** The core idea of splitting a network for privacy is not novel. The paper doesn't adequately differentiate itself from existing work like split learning, failing to acknowledge related research.\n* **Clarity and Correctness Issues (Reviewers 1 & 3):** All three reviewers point out issues with clarity, grammatical errors, and unsupported claims. Reviewer 3 highlights the lack of a defined security model and the potential for information leakage even from the encoded features.\n\n**Minor Concerns:**\n\n* **Ambiguous Statements (Reviewer 2):** Several ambiguous statements detract from the paper's clarity.\n* **Missing Details (Reviewer 2):** Important experimental details like optimizers and training epochs are missing.\n* **Unclear Privacy Guarantee (Reviewer 3):** The mechanism by which the model guarantees privacy is not clearly explained.\n\n\n\nThe authors' response to these criticisms would be crucial, but given the severity of the technical flaws identified by Reviewer 2, it is unlikely that the paper can be sufficiently revised to meet the standards of a top-tier conference. The fundamental issues with the threat model and the server's ability to reconstruct the data are particularly problematic.\n\n\n\nFinal Deci"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "## Detailed Reasoning\n\nThis paper presents DeepSeaProbLog, an extension of DeepProbLog to handle continuous random variables alongside discrete ones. The core idea – integrating neural networks with probabilistic logic programming in a mixed discrete-continuous domain – is a valuable and potentially impactful direction for neuro-symbolic AI. However, the reviewers raise several consistent concerns that, taken together, lead me to believe the paper is not quite ready for acceptance at a top-tier conference.\n\n**Strengths:**\n\n* **Novelty:** The extension to continuous domains *is* a genuine contribution, addressing a limitation of existing NPLP systems. The use of weighted model integration is a clever approach to tackle the mixed domain challenge.\n* **Technical Soundness:** Reviewers 4, 5, and 7 all indicate the technical aspects appear correct, and reviewer 3 specifically highlights the novelty of the WMI approach.\n* **Clarity (Generally):** Most reviewers find the paper well-written, though some point to areas needing clarification (see weaknesses).\n\n**Weaknesses (and why they are significant):**\n\n* **Limited Empirical Evaluation:** This is the most significant and consistently raised concern. The two \"toy\" applications (as described by Reviewer 2) and the MNIST subtraction task are insufficient to demonstrate the broad applicability and robustness of DeepSeaProbLog. The lack of comparison to other approaches on standard benchmarks is a major drawback. Reviewers 2 and 3 specifically request this.\n* **Insufficient Baselines & Ablation Studies:** The MNIST subtraction task, while illustrative, uses auxiliary tasks (location supervision, curriculum learning) that raise questions about the fairness of comparison with baselines (Reviewer 4). The lack of ablation studies (Reviewer 3) weakens the claim that DeepSeaProbLog is *responsible* for the observed improvements.\n* **Missing Related Work:** Reviewers 3 points out missing references to recent work on WMI solvers. This is a significant omission, as the choice of solver could heavily impact performance.\n* **Lack of Detail on Key Aspects:** Several reviewers (6, 7) point to a lack of detail regarding the handling of discrete variables, the semantics of the language, and the differentiation process through WMI.  Deferring these details to another paper is unacceptable for a conference submission.\n* **Incremental Nature:** Reviewer 7 rightly points out the incremental nature of the work. While extending DeepProbLog is valuable, the paper doesn't fully capitalize on the potential of the continuous domain extension.\n* **Efficiency Concerns:** The potential performance overhead of sampling-based WMI compared to exact solvers (Reviewer 4) is not adequately addressed.\n\n**Reviewer Disagreement:**\n\nThere's a range of opinions among the reviewers. Some are more enthusiastic (Reviewers 1, 4), while others are more critical (Reviewers 5, 6). This disagreement, combined with the consistent concerns about empirical evaluation and missing details, suggests the paper needs further refinement.\n\n\n\nIn conclusion, while the core idea is promising and the technical work appears sound, the limited empirical evaluation, lack of detailed explanations, and incremental nature of the contribution prevent this paper from meeting the standards of a top-tier conference. More extensive experimentation, a more thorough comparison to existing methods, and a more detailed presentation of the technical aspects are necessary.\n\nFinal Deci"
    }
  ]
}