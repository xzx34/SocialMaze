{
  "summary": {
    "model": "llama-3.3-70B",
    "total_papers": 100,
    "correct_predictions": 87,
    "accuracy": 87.0,
    "confusion_matrix": [
      [
        69,
        7
      ],
      [
        6,
        18
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 90.78947368421053,
        "precision": 92.0
      },
      "Accept": {
        "total": 24,
        "recall": 75.0,
        "precision": 72.0
      }
    }
  },
  "results": [
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"SGD Learns One-Layer Networks in WGANs\" presents an analysis of the convergence of stochastic gradient descent-ascent for training Wasserstein Generative Adversarial Networks (WGANs) with a one-layer generator network and simple discriminators (linear and quadratic). The authors provide theoretical results showing that, under certain conditions, stochastic gradient descent-ascent can learn the target distribution in polynomial time and sample complexity.\n\nHowever, the reviewers have raised several concerns that need to be addressed:\n\n1. **Simplistic discriminators**: All reviewers agree that the use of linear and quadratic discriminators is overly simplistic and does not capture the true nature of WGAN training, which typically involves more complex neural networks. This simplification may not be representative of real-world WGAN training scenarios.\n2. **Limited applicability**: The results obtained in the paper may not be easily extendable to more complex discriminators or generators, which is a significant limitation. The analysis relies heavily on the simplicity of the discriminators, and it is unclear how the results would generalize to more realistic settings.\n3. **Misleading claims**: Some reviewers feel that the paper's title and claims are misleading, as the results are specific to very simple discriminators and generators, which may not be representative of the broader WGAN literature.\n4. **Lack of experimental validation**: While the paper provides some experimental results, they are not comprehensive, and more experiments are needed to validate the theoretical findings, especially in more realistic settings.\n\nDespite these concerns, the paper does make some contributions to the understanding of WGAN training, particularly in the context of simple discriminators and generators. The theoretical analysis is well-done, and the results provide insight into the convergence of stochastic gradient descent-ascent in these simplified settings.\n\nHowever, considering the top-tier conference standards, the paper's limitations and simplifications may not be sufficient to warrant acceptance. The conference typically expects papers to present more significant, generalizable, and impactful results that advance the state-of-the-art in the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"NORML: Nodal Optimization for Recurrent Meta-Learning\" proposes a novel meta-learning framework that utilizes an LSTM-based meta-learner to perform neuron-wise optimization on a learner for efficient task learning. The paper's main contribution is the introduction of a meta-learner that can scale to learner networks with a large number of parameters.\n\nHowever, upon analyzing the reviewer comments, several concerns and issues with the paper are raised. Reviewer 1 points out that the paper's comparison to previous work, specifically Ravi & Larochelle, is inaccurate. The reviewer argues that Ravi & Larochelle's approach does not require an enormous number of parameters, contrary to the paper's claim. Reviewer 1 also questions the paper's experimental results, stating that the results are not convincing and that the authors' claims about the benefits of their approach are not supported.\n\nReviewer 2 criticizes the paper's writing style, stating that it is beyond the level required by a scientific manuscript and contains numerous grammatical mistakes. The reviewer also questions the paper's technical novelty, stating that it is minimal.\n\nReviewer 3 raises several concerns, including the limited novelty of the paper, missing baselines, and limited applicability. The reviewer argues that the paper's approach is a fairly incremental variation compared to previous work and that the authors do not provide a fair comparison with other methods. The reviewer also points out that the paper's assumptions about the base learner being a fully connected network limit the applicability of the method.\n\nIn addition to these concerns, the reviewers also point out several minor issues, including inaccuracies in the description of previous work, missing references, and formatting errors.\n\nGiven the numerous concerns and issues raised by the reviewers, it is clear that the paper requires significant revisions to address these issues. The paper's limited novelty, missing baselines, and limited applicability are major concerns that need to be addressed. Additionally, the paper's writing style and formatting need to be improved to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning hierarchical representations using a variational autoencoder (VAE) with a progressive learning strategy. The authors propose a method to incrementally learn independent hierarchical representations from high- to low-levels of abstractions, which improves disentanglement in comparison to existing works.\n\nThe paper is well-motivated, and the results look impressive, with the learned hierarchy and latent traversals being convincing. The authors introduce a new metric, MIG-sup, to capture the one-to-one mapping of the ground truth factors to the latent dimensions, which is a valuable contribution to the field.\n\nHowever, the reviewers have raised several concerns that need to be addressed. Reviewer 1 suggests adding an experiment to compare mutual information of each layer with VLAE, which the authors have addressed in their rebuttal. Reviewer 2 has several concerns, including the lack of clarity in the purpose of the paper, the relationship between hierarchical representation and disentangled representation, and the similarity to existing work. The authors have addressed these concerns in their rebuttal, and Reviewer 2 has increased their score to a Weak Accept.\n\nReviewer 3 finds the idea fresh and well-explained, with sufficiently thorough experiments. They suggest adding quantitative experiments on information flow and formalizing the concept of \"z from different abstraction\". Reviewer 4 finds the paper well-motivated and easy to read, with impressive results. They suggest adding ablation studies to compare the effect of incremental learning and fade-out.\n\nAfter carefully considering the reviewers' comments and the authors' rebuttal, I believe that the paper has addressed the major concerns raised by the reviewers. The authors have provided additional experiments and clarifications, which have improved the paper's quality and addressed the reviewers' concerns.\n\nThe paper's contributions, including the novel approach to learning hierarchical representations and the introduction of the MIG-sup metric, are significant and valuable to the field. The results are impressive, and the learned hierarchy and latent traversals are convincing.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The research paper in question has been evaluated based on the provided title, abstract, reviewer comments, and author responses. The analysis of these components reveals several critical issues that significantly impact the paper's suitability for publication in a top-tier conference.\n\n1. **Lack of Clarity and Poor Writing**: All three reviewers have highlighted the poor writing quality of the paper, noting grammatical issues, incomprehensible passages, and an overall lack of clarity. This makes it extremely difficult for reviewers to understand the paper's contributions, methodology, and conclusions. Clear and professional writing is essential for academic communication, and the paper's current state falls short of these standards.\n\n2. **Absence of References**: The paper does not cite any previous research, which is a fundamental aspect of academic writing. References are crucial for situating the research within the broader context of the field, acknowledging prior work, and demonstrating how the current research contributes to the existing body of knowledge. The lack of references suggests a disconnect from the academic community and the field's literature.\n\n3. **Methodological and Conceptual Issues**: Reviewers have struggled to discern the paper's methodology and the specific improvements it proposes to Monte Carlo Tree Search (MCTS) agents. The abstract's mention of focusing on the \"coordination of episode generation\" is unclear, and without a detailed explanation of the approach and its novelty, it's challenging to assess the paper's contributions.\n\n4. **Insufficient Evidence and Analysis**: While the abstract mentions experiments with a small problem showing \"robust performance\" compared to Alpha Zero, there is no detailed analysis or presentation of results that would allow for an evaluation of these claims. In academic research, especially in a top-tier conference, it is essential to provide comprehensive evidence and analysis to support any assertions of improvement or innovation.\n\n5. **Professionalism and Academic Standards**: The reviewers' comments collectively suggest that the paper does not meet basic standards of academic professionalism and rigor. The inclusion of undefined mathematical symbols, difficult-to-follow algorithm boxes, and the overall structure of the paper contribute to this assessment.\n\nGiven these considerations, the paper in its current form does not meet the standards expected for publication in a top-tier conference. The issues identified are fundamental and would require significant revisions to address. While the idea of refining MCTS agents using MCTS itself is potentially interesting, the execution and presentation of this idea in the paper are severely lacking.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of how different activation functions impact the training of overparametrized neural networks. The authors provide a comprehensive study that separates activation functions into two cases: non-smooth activations (e.g., ReLU, SELU, ELU) and smooth activations (e.g., tanh, swish, polynomial). They show that non-smooth activations have large minimum eigenvalues of the associated Gram matrix, leading to faster training, whereas smooth activations can have small minimum eigenvalues, resulting in slower training, especially when the data has a low-dimensional structure.\n\nThe strengths of the paper include:\n\n1. **Theoretical contributions**: The paper provides novel theoretical results that advance our understanding of the impact of activation functions on the training of overparametrized neural networks.\n2. **Comprehensive analysis**: The authors consider a wide range of activation functions and provide a detailed analysis of their properties and how they affect the training process.\n3. **Empirical validation**: The paper includes empirical experiments that validate the theoretical findings, demonstrating the practical relevance of the results.\n4. **Well-organized and self-contained**: The paper is well-written, and the authors provide a clear and concise presentation of their results, making it easy to follow.\n\nHowever, there are also some weaknesses and areas for improvement:\n\n1. **Length and organization**: The paper is lengthy, and the appendix is extensive. While the authors provide a proof sketch of the main results, some reviewers found it challenging to navigate the paper and suggested reorganizing the appendix to emphasize the main techniques and ideas.\n2. **Clarity and notation**: Some reviewers pointed out minor issues with notation and clarity, such as the explanation of the M matrix and the wording of certain statements.\n3. **References and context**: The paper could benefit from more context and references to related work, particularly in the introduction and discussion sections.\n\nAfter carefully considering the reviewer comments and the paper's content, I conclude that the paper's strengths outweigh its weaknesses. The theoretical contributions, comprehensive analysis, and empirical validation demonstrate the paper's significance and relevance to the field.\n\nAddressing the reviewer comments and suggestions will further improve the paper's clarity, organization, and overall quality. The authors should consider reorganizing the appendix, providing more context and references, and addressing minor issues with notation and clarity.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Differentiable Hebbian Consolidation for Continual Learning\" proposes a novel approach to address the problem of catastrophic forgetting in neural networks. The method, called Differentiable Hebbian Consolidation, combines a Differentiable Hebbian Plasticity (DHP) Softmax layer with task-specific synaptic consolidation methods to retain learned representations over a longer timescale.\n\nThe paper is well-written, and the method is well-motivated by cognitive principles and neurobiological substrates. The authors provide a clear explanation of the proposed approach and demonstrate its effectiveness on several benchmarks, including Permuted MNIST, Split MNIST, and Vision Datasets Mixture. The results show that the proposed method outperforms comparable baselines and reduces forgetting.\n\nHowever, the reviewer comments raise several concerns that need to be addressed. Reviewer 1 suggests that the empirical evaluation is not strong enough, and the results on Split CIFAR-10/100 are not convincing. Reviewer 2 has trouble following the details of the DHP Softmax and suggests that a figure and equations for the standard Softmax would be helpful. Reviewer 3 points out that the experimental section is limited and suggests that the authors should compare their method to other CLS-based approaches and try more experiments on different datasets. Reviewer 4 considers the results on simpler continual learning settings to be weak and suggests that the authors should evaluate their method on more challenging datasets.\n\nDespite these concerns, the paper has several strengths. The proposed method is simple, easy to implement, and does not require additional hyperparameters. The authors demonstrate the flexibility of their method by integrating it with well-known task-specific synaptic consolidation methods. The results show significant improvements over simple baselines, and the method can complement other task-specific Hebbian-based learning paradigms.\n\nTo address the concerns raised by the reviewers, the authors could provide additional experiments on more challenging datasets, such as CIFAR-100 or ILSVRC, and compare their method to other CLS-based approaches. They could also provide more details on the DHP Softmax, including equations and figures, to make the paper more accessible to readers who are not familiar with Hebbian learning.\n\nIn conclusion, while the paper has several strengths, the concerns raised by the reviewers need to be addressed. The authors should provide additional experiments and comparisons to other methods to demonstrate the effectiveness and scalability of their approach.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Extreme Classification via Adversarial Softmax Approximation\" presents a novel approach to improve the efficiency of training softmax classifiers in extreme classification settings, where the number of classes is very large. The authors propose an adversarial sampling mechanism that generates negative samples from an auxiliary model, which mimics the data distribution, to enhance the gradient signal and reduce the computational cost.\n\nThe paper has several strengths. Firstly, the proposed approach is technically sound and well-motivated. The authors provide a clear explanation of the problem, the proposed solution, and the theoretical foundations of their method. The experimental results demonstrate the effectiveness of the proposed approach in reducing the training time and improving the convergence rate compared to several competitive baselines.\n\nHowever, the paper also has some weaknesses. One of the main concerns is the lack of comprehensive comparison with existing state-of-the-art methods. Reviewer 2 points out that the authors gloss over many relevant prior works and fail to put their results in the right context. Specifically, the authors do not compare their approach with recent methods such as Slice, Adaptive Sampled Softmax, and Sampled Softmax with Random Fourier Features, which also address the problem of efficient softmax approximation.\n\nReviewer 3 raises several concerns about the experimental evaluation, including the lack of comparison with recent state-of-the-art methods, the omission of important baselines such as DiSMEC, and the limited evaluation on only two datasets. The reviewer also suggests that the authors should investigate the performance of their approach on more datasets, including smaller ones, and evaluate its extension to the multi-label setting.\n\nAdditionally, Reviewer 2 and Reviewer 3 point out several minor errors and typos in the paper, which should be corrected.\n\nIn conclusion, while the paper presents a novel and technically sound approach to extreme classification, the lack of comprehensive comparison with existing state-of-the-art methods and the limited experimental evaluation are significant weaknesses. To strengthen the paper, the authors should address the concerns raised by the reviewers, including comparing their approach with recent state-of-the-art methods, evaluating their approach on more datasets, and investigating its extension to the multi-label setting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns. The authors propose using a bidirectional LSTM RNN model, which outperforms other traditional machine learning models. However, after carefully analyzing the paper, reviewer comments, and author responses, several concerns and limitations have been identified.\n\nFirstly, the paper lacks methodological novelty, as pointed out by Reviewer 3. The use of bidirectional LSTM RNNs for predicting DNA folding patterns is not a new concept, and the paper does not provide significant improvements or new insights. The authors do not clearly explain how their approach differs from previously published works, which is a major concern.\n\nSecondly, the description of the data and methods is not clear enough. Reviewer 3 points out that it is unclear which features were used as inputs, how they were represented, and how genomic segments of different chromosomes were treated. This lack of clarity makes it difficult to understand and reproduce the results.\n\nThirdly, the evaluation of the models is limited. Reviewer 1 suggests that the improvement in performance may be due to the proposed new metric, rather than the LSTM model itself. Reviewer 3 also points out that the authors should use the same loss function for training all models and use additional evaluation metrics such as MSE and R^2 score.\n\nFourthly, the paper lacks generalizability. Reviewer 1 points out that the approach is specific to Hi-C data and ChIP-seq features, and it is unclear if it can be applied to other datasets or features. The authors do not provide any evidence that their approach can be generalized to other organisms or datasets.\n\nLastly, there are several technical issues with the paper. Reviewer 2 points out that there is only one equation in the paper, and it is not clearly explained. Reviewer 3 suggests that the authors should compare recurrent models to non-recurrent models and use additional evaluation metrics.\n\nConsidering these concerns and limitations, it is clear that the paper requires significant revisions and improvements before it can be considered for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Interaction Processes for Time-Evolving Graphs\" proposes a novel approach to modeling continuous time-evolving graphs using a temporal point process framework. The authors introduce several key contributions, including the use of a time gate in the LSTM to handle temporal dependencies, a stacked LSTM approach to capture multiple time resolutions, and an attention mechanism to select relevant nodes.\n\nThe reviewers have raised several concerns and questions about the paper, which can be summarized into several key areas:\n\n1. **Novelty and justification of contributions**: Reviewer 1 and Reviewer 2 question the novelty of the proposed approach, arguing that the individual components are not particularly new. Reviewer 1 also requests more analysis and justification for the contributions, such as empirical demonstrations of the effectiveness of the stacked LSTM approach and the attention mechanism.\n2. **Ablation study and analysis**: Reviewer 1 suggests that the ablation study is not adequate and requests more detailed analysis of the architecture, such as the effect of different hyperparameters and the importance of each component.\n3. **Comparison to baselines**: Reviewer 1 and Reviewer 2 point out that the paper misses a comparison to a recently proposed baseline, JODIE, and request more empirical comparisons to state-of-the-art methods.\n4. **Modeling and mathematical formulation**: Reviewer 2 questions the correctness of the log-likelihood function in Section 3.6.1 and requests more clarification on the mathematical formulation of the model.\n5. **Readability and clarity**: Reviewer 3 finds the paper difficult to understand and requests more diagrams, examples, and explanations to improve readability.\n\nDespite these concerns, the reviewers also acknowledge the paper's strengths, including its promising approach to modeling temporal and dynamic graphs, its improved empirical performance on multiple datasets, and its potential applications in various fields.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper requires significant revisions to address the concerns raised by the reviewers. The authors need to provide more analysis and justification for their contributions, improve the ablation study and analysis, compare their approach to more baselines, clarify the mathematical formulation of the model, and improve the readability and clarity of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning to Transfer via Modelling Multi-level Task Dependency\" proposes a novel multi-task learning framework that constructs attention-based dependency relationships among different tasks. The authors argue that most existing multi-task learning methods assume that tasks are closely related, which may not be true in real-world scenarios. The proposed framework aims to address this limitation by modeling task dependencies at multiple levels.\n\nThe reviewers' comments highlight several strengths and weaknesses of the paper. Reviewer 1 appreciates the paper's idea but criticizes the writing quality, grammar, and clarity. Reviewer 2 finds the idea of making the amount of transfer between tasks dependent on the particular sample at hand to be new and potentially acceptable with improved presentation and additional evaluation. Reviewer 3 questions the claim that most multi-task learning models rely on the assumption of highly correlated tasks and points out that the proposed network has a large number of model parameters. Reviewer 4 raises significant concerns about the paper's lack of awareness of prior work, missing critical specifics, and unnecessary mix of different concepts.\n\nThe authors' responses to the reviewers' comments attempt to address some of the concerns, but the reviewers remain unconvinced. The main issues with the paper are:\n\n1. **Lack of clarity and writing quality**: The paper's writing is poor, with numerous grammatical mistakes, errors, and unclear explanations.\n2. **Unawareness of prior work**: The authors seem to be unaware of critically related prior work, such as \"Taskonomy: Disentangling task transfer learning\", which has similar concepts and formulations.\n3. **Missing critical specifics**: The paper lacks clear explanations and details about the proposed method, including the \"general task dependency\" matrix, the \"Transfer Block\", and the \"Point-wise Mutual Attention Mechanism\".\n4. **Unclear contributions**: The reviewers are unsure about the novelty and significance of the proposed method, and the authors' responses do not provide convincing justifications.\n\nConsidering these issues, I believe that the paper requires significant revisions to address the reviewers' concerns. While the idea of modeling multi-level task dependency is interesting, the paper's current state is not suitable for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Domain Adaptive Multibranch Networks\" presents a novel approach to unsupervised domain adaptation by introducing a deep learning framework that allows each domain to undergo a different sequence of operations. This approach is motivated by the fact that different domains may require different processing to arrive at a common feature representation effective for recognition.\n\nThe reviewers' comments provide valuable insights into the strengths and weaknesses of the paper. Reviewer 1 finds the paper to be well-motivated, with sufficient related work and convincing experimental results. However, they suggest that the paper could be improved by linking it to more transfer learning techniques outside of the deep learning domain and avoiding the use of arxiv references.\n\nReviewer 2 finds the idea of adaptive computation graphs for domain adaptation to be novel and technically sound. However, they point out that the paper does not experiment with different amounts of computation per domain, which is a key intuition behind the method. They also suggest that the paper should report state-of-the-art performance for domain adaptation methods and provide more discussion on the experimental study.\n\nReviewer 3 appreciates the novelty of the idea but finds the experimental support to be insufficient. They suggest that the paper should compare with other state-of-the-art unsupervised domain adaptation (UDA) methods, such as CDAN, and provide more ablation studies and hyperparameter sensitivity analyses.\n\nAfter carefully analyzing the reviewers' comments, it is clear that the paper has some strengths, such as its novel approach to domain adaptation and its potential to handle multiple domains simultaneously. However, it also has some significant weaknesses, such as the lack of experimental support, the failure to compare with state-of-the-art UDA methods, and the need for more ablation studies and hyperparameter sensitivity analyses.\n\nGiven the standards of a top-tier conference, it is expected that papers should have a strong theoretical foundation, be well-motivated, and provide convincing experimental results. While the paper has some promising ideas, it falls short in terms of experimental support and comparison with state-of-the-art methods.\n\nTherefore, based on the reviewers' comments and the standards of a top-tier conference, I conclude that the paper requires significant revisions to address the weaknesses pointed out by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel out-of-distribution detection method, MALCOM, which utilizes the normalized compression distance to identify out-of-distribution samples without requiring any out-of-distribution samples for validation or retraining the model. The method is based on extracting informative sequential patterns from the feature maps of convolutional neural networks.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 points out that the comparison with prior methods is unfair and that the performance gain from the compared method might be due to incorrect implementation. They suggest that the authors should provide the original numbers from the prior paper and claim that they fail to replicate the results. Reviewer 1 also questions the effectiveness of the proposed method and suggests an ablation study to investigate the impact of weighted averaging vs. concatenation.\n\nReviewer 2 appreciates the effort by the authors but maintains their rating due to concerns about the restrictive choice of not retraining the network or tuning parameters with out-of-distribution validation data. They suggest that the authors should justify the necessity of these constraints and compare their method with works that are not so tightly constrained. Reviewer 2 also points out that the paper could benefit from extra proofreading and suggests citing related works that involve compressive principles in image classification tasks.\n\nReviewer 3 finds the motivation of the proposed approach clear and the method novel, but suggests that the experiments could be done in a more complex setting, such as safety-critical applications like autonomous driving. They are not convinced of the significance or sufficient efficiency of the approach and suggest providing theoretical guarantees showing the exhaustiveness of the proposed method.\n\nAfter carefully analyzing the reviews and the paper, it is clear that the paper has some strengths, such as its novelty and the thorough evaluation of the method. However, the reviewers have raised several concerns that need to be addressed, including the fairness of the comparison with prior methods, the justification of the restrictive constraints, and the need for more complex experiments.\n\nThe authors have not fully addressed the concerns raised by the reviewers, and the paper requires significant revisions to address these issues. Therefore, based on the standards of a top-tier conference, the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The research paper in question has been evaluated based on the provided title, abstract, reviewer comments, and author responses. Given the information, several critical issues have been identified that significantly impact the paper's suitability for publication in a top-tier conference.\n\n1. **Lack of Clarity and Poor Writing**: All three reviewers have highlighted the paper's poor writing quality, with numerous grammatical issues, incomprehensible passages, and an inability to clearly convey the research's intent or methodology. This lack of clarity makes it difficult for reviewers to understand the paper's contributions, methodology, or results, which is a fundamental requirement for academic publication.\n\n2. **Absence of References**: The paper fails to cite any previous research, which is a critical oversight. References are essential for situating the research within the broader context of existing knowledge, demonstrating an understanding of the field, and showing how the current work contributes to or challenges previous findings.\n\n3. **Methodological and Technical Issues**: Reviewers have noted issues with the presentation of mathematical symbols, algorithmic descriptions, and the overall structure of the paper, including excessive blank space and poorly designed algorithm boxes. These issues suggest a lack of attention to detail and a failure to adhere to standard academic and technical writing practices.\n\n4. **Insufficient Evidence and Lack of Scientific Rigor**: The reviewers have expressed concerns over the paper's ability to demonstrate robust performance improvements over existing methods like Alpha Zero, citing a need for more rigorous testing and validation. Furthermore, the paper's claims are described as unjustified and inaccurate, indicating a lack of scientific rigor.\n\n5. **Failure to Meet Academic Standards**: The consensus among the reviewers is that the paper does not represent a serious academic endeavor. It lacks the preliminary elements of scientific writing, such as clear hypotheses, well-defined methodologies, and appropriate references. This failure to meet basic academic standards is a significant barrier to publication.\n\nGiven these considerations, it is clear that the paper, in its current form, does not meet the standards expected of research published in a top-tier conference. The issues identified are not minor and would require significant revisions to address. However, without a clear understanding of the paper's intended contributions due to its poor quality, it is challenging to assess whether such revisions could elevate the work to the required standard.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" proposes a generative model called ROOTS for unsupervised object-wise 3D-scene decomposition and rendering. The model is based on the Generative Query Networks (GQN) framework but provides object-oriented representation decomposition. The paper presents qualitative results demonstrating the model's abilities for disentanglement, compositionality, and generalization.\n\nHowever, the reviewers have raised several concerns that need to be addressed. Reviewer 1 points out that the paper is poorly written, with many typos and grammatical errors. The reviewer also questions the claim that ROOTS is the first unsupervised model to identify objects in a 3D scene, citing existing work such as MONet and Iodine. Additionally, the reviewer requests clarification on several aspects of the model, including the notation, the scene volume map, and the use of a Gaussian distribution for the position.\n\nReviewer 2 also raises concerns about the paper's clarity and the lack of proper citations. The reviewer points out that the paper ignores existing work on 3D representation learning and neural rendering, and that the experiments are not thorough enough. The reviewer suggests that the paper would benefit from additional comparisons to other methods, such as CGQN, and more detailed analysis of the results.\n\nReviewer 3 raises similar concerns, pointing out that the paper lacks proper citations and comparisons to existing work. The reviewer also questions the novelty of the proposed method, citing existing work on 3D representation learning and object-oriented scene understanding.\n\nReviewer 4 raises concerns about the paper's complexity and the lack of clear explanations. The reviewer points out that the method is difficult to follow, and that the notation and terminology are not consistent. The reviewer also questions the experimental results, pointing out that the comparisons to GQN are not thorough enough and that the results are not significant.\n\nIn the post-rebuttal update, the reviewers continue to raise concerns about the paper's clarity, experimental results, and comparisons to existing work. The authors have addressed some of the concerns, but the reviewers still feel that the paper is not ready for publication.\n\nBased on the reviews, it is clear that the paper has several major issues that need to be addressed. The paper's poor writing, lack of clarity, and insufficient comparisons to existing work are significant concerns. Additionally, the experimental results are not thorough enough, and the reviewers have raised questions about the novelty and significance of the proposed method.\n\nTherefore, I conclude that the paper is not ready for publication in its current form. The authors need to address the concerns raised by the reviewers, including improving the writing and clarity, adding more comparisons to existing work, and providing more thorough experimental results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Score and Lyrics-Free Singing Voice Generation\" proposes a novel approach to generating singing voices without pre-assigned scores and lyrics. The authors explore three different schemes: free singer, accompanied singer, and solo singer, and develop a pipeline to tackle these new tasks. The paper has been reviewed by five reviewers, who have provided detailed comments and suggestions.\n\nThe reviewers have raised several concerns and criticisms, including:\n\n1. **Lack of motivation**: Reviewer 2 and Reviewer 5 question the motivation behind the proposed task and the choice of schemes. They suggest that the authors should provide more context and justification for the problem they are trying to solve.\n2. **Weak evaluation**: Reviewer 1, Reviewer 2, and Reviewer 3 criticize the evaluation methodology, suggesting that the metrics used (vocalness and average pitch) are not sufficient to assess the quality of the generated singing voices. They also point out the lack of baseline comparisons and the need for more rigorous evaluation.\n3. **Methodology**: Reviewer 4 and Reviewer 5 question the choice of neural network architecture and the lack of ablation studies to support the design decisions. They also suggest that the authors should provide more details on the training data and the source separation process.\n4. **Literature review**: Reviewer 3 and Reviewer 4 suggest that the literature review is incomplete, and the authors should provide more context and references to related work.\n5. **Clarity and writing**: Reviewer 2, Reviewer 4, and Reviewer 5 point out several issues with the writing, including unclear sentences, typos, and lack of detail in certain sections.\n\nDespite these criticisms, the reviewers acknowledge the novelty and potential of the proposed approach. Reviewer 2 and Reviewer 5 suggest that the paper could be improved with additional comparisons to baseline methods and more rigorous evaluation. Reviewer 4 suggests that the paper could be rejected due to the lack of justification and clarity, but acknowledges the potential of the proposed approach.\n\nAfter carefully considering the reviews and the paper, I conclude that the paper requires significant revisions to address the concerns and criticisms raised by the reviewers. While the paper proposes a novel approach to singing voice generation, the evaluation methodology, literature review, and writing need to be improved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" proposes a novel approach to reduce the training time of large-scale convolutional neural networks (CNNs) by utilizing a multi-level optimization strategy that employs multiple precisions, including low-precision fixed-point representations. The training strategy, named MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run-time the transition between different precisions.\n\nThe reviewers have raised several concerns and criticisms about the paper. Reviewer 1 finds the paper interesting but feels that the reasoning behind the algorithm is not well-presented and that the motivation behind the switching mechanism is unclear. Reviewer 2 points out that the proposed approach does not match the description in the paper and that the setting of quantized precision levels is confusing. Reviewer 3 recommends a weak reject, stating that the core idea of the manuscript, i.e., the dynamic switching between precision levels, is not shown to be a necessary condition for good classification results.\n\nAfter carefully analyzing the paper and the reviewer comments, several major issues are evident:\n\n1. **Lack of clarity and motivation**: The paper lacks a clear and detailed presentation of the reasoning behind the algorithm, and the motivation behind the switching mechanism is not well-explained. This makes it difficult for readers to understand the underlying principles of MuPPET.\n2. **Confusing presentation**: The presentation of the precision switching policy is confusing, and the notations are unclear. This makes it challenging to reproduce the results and understand the methodology.\n3. **Insufficient evaluation**: The paper does not provide a thorough evaluation of the proposed approach, and the results are not superior enough compared to state-of-the-art methods. The training and validation curves are not shown, which makes it difficult to analyze the training process.\n4. **Methodological issues**: The paper raises several methodological concerns, such as the choice of quantized precision levels, the determination of scaling factors, and the difference between n and WL.\n\nDespite the novelty of the proposed approach, the paper requires significant revisions to address the concerns and criticisms raised by the reviewers. The authors need to provide a clear and detailed presentation of the algorithm, clarify the motivation behind the switching mechanism, and provide a thorough evaluation of the proposed approach.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an application of machine learning methods, specifically Recurrent Neural Networks (RNNs), to predict DNA folding patterns from epigenetic marks. The authors compare the performance of a bidirectional Long Short-Term Memory (LSTM) model with other classical machine learning approaches, including linear models with regularization and Gradient Boosting. The results show that the LSTM model outperforms the other methods, and the authors attribute this to the model's ability to capture the sequential dependencies in the DNA data.\n\nHowever, upon careful analysis of the paper and the reviewer comments, several concerns and limitations are raised. Reviewer 1 points out that the improvement in performance may be due to the proposed weighted mean squared error (wMSE) metric rather than the LSTM model itself. They also question the generalizability of the approach to other Hi-C datasets and epigenetic features. Reviewer 2 raises several minor issues with the paper, including the lack of clarity in the equation, the need for formalization of certain sentences, and the absence of a discussion on why regression is better or worse than a neural network in this application setting.\n\nReviewer 3 provides more significant criticisms, including the lack of methodological novelty, unclear description of the data and methods, and the need for additional baseline models and evaluation metrics. They also point out that the paper is similar to previously published works, which are not cited. The reviewer questions the choice of a bidirectional LSTM and its motivation, as well as the use of the weighted MSE as the evaluation metric.\n\nConsidering these concerns and limitations, the paper has several major issues that need to be addressed. The lack of methodological novelty, unclear description of the data and methods, and the need for additional baseline models and evaluation metrics are significant concerns. The paper's similarity to previously published works, which are not cited, is also a major issue. While the paper presents an interesting application of machine learning to DNA folding pattern prediction, the concerns and limitations raised by the reviewers outweigh the paper's contributions.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I conclude that the paper requires significant revisions to address the concerns and limitations raised. The authors need to provide more clarity on the data and methods, add more baseline models and evaluation metrics, and cite previously published works. The authors also need to address the concerns regarding the choice of the bidirectional LSTM and the use of the weighted MSE as the evaluation metric.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" proposes a novel approach for segmenting trajectories into sub-skills using weak supervision. The authors claim that their method can learn reusable skills from a collection of human demonstrations without requiring explicit segmentation or ordering of primitives.\n\nHowever, after carefully analyzing the paper, reviewer comments, and author responses, I have identified several concerns that raise doubts about the paper's validity and significance.\n\nFirstly, Reviewer 1 points out that the paper's framing and motivation do not correspond to the results and experiments reported. The paper is framed as a way to learn reusable sub-skills that can help generalize to new situations in control, but the method presented only provides a per-time-step labeling of each observation with the associated most likely sub-skill. This limitation is significant, as it does not provide any indication that the data could be useful for learning reusable skills for downstream tasks later.\n\nSecondly, the experimental results are limited in scope and do not provide enough evidence to support the claims made in the paper. Reviewer 1 suggests that the paper should include more experiments, such as training a behavior cloning (BC) agent on the observation-action pairs conditioned on the sub-skill, to demonstrate the usefulness of the learned skills. Reviewer 2 also points out that the results are underwhelming, with validation accuracy ranging from 35-60%, and that the paper lacks a comparison to a fully-supervised oracle method.\n\nThirdly, the paper lacks clarity and transparency in its experimental setup and results. Reviewer 1 points out that the details regarding the task setup, number of demonstrations needed per task/skill, architectural choices, and hyper-parameters are lacking, making it hard to follow and understand the experiments. Reviewer 3 also suggests that the paper should provide more analysis regarding the segmented trajectories, such as confusion matrices or precision and recall metrics, to assess the quality of the learned skills.\n\nLastly, the paper's novelty and contributions are questioned by Reviewer 2, who points out that the log-sum-exp pooling approach has been used successfully in other domains and that the results are not impressive enough to warrant publication in a top-tier conference.\n\nIn conclusion, while the paper proposes an interesting approach to weakly-supervised trajectory segmentation, the limitations and concerns raised by the reviewers significantly outweigh the paper's contributions. The paper lacks clarity, transparency, and convincing evidence to support its claims, and the experimental results are limited in scope and significance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" presents a novel approach for efficiently training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) on large amounts of high-dimensional data. The authors propose three key ideas: minimizing an upper bound to the GMM log likelihood, introducing a new regularizer to prevent convergence to pathological local minima, and enforcing constraints inherent to GMM training when using SGD. They also propose an SGD-compatible simplification to the full GMM model based on local principal directions.\n\nThe reviewers have raised several concerns and criticisms about the paper. Reviewer 1 feels that the work is largely incremental and that the authors lack understanding of the history of online EM. They also criticize the max-component approximation, the representation of multinoulli in terms of its natural parameters, and the smoothing approach. Reviewer 1 is concerned about the experimental evaluation, particularly the choice of datasets and the lack of comparison to other methods.\n\nReviewer 2 shares similar concerns, questioning the novelty and significance of the proposed method. They point out the lack of guarantees for convergence and the unclear motivation for the maximization in eq. (2). Reviewer 2 also criticizes the requirement for the covariance matrix, the expression for , and the local principal directions covariance structure.\n\nReviewer 3 finds the paper to be clear and well-written but raises concerns about the limited contribution of the max-component trick and the mysterious motivation for smoothing on the 2D grid. They also question the practical advantages of the method compared to standard EM and the lack of comparison to other baselines.\n\nDespite these criticisms, the paper has some strengths. The authors have proposed a novel approach for training GMMs using SGD, which could be useful for large-scale datasets. The experimental results show promising performance on standard image datasets. The paper is also well-organized and easy to follow.\n\nHowever, after careful consideration of the reviewers' comments and the paper's content, I believe that the paper has several major weaknesses that cannot be overlooked. The lack of novelty and significance, the unclear motivation for some of the proposed methods, and the limited experimental evaluation are significant concerns. Additionally, the paper does not provide sufficient comparison to other baselines, and the practical advantages of the method are not clearly demonstrated.\n\nTherefore, based on the provided information and the standards of a top-tier conference, I conclude that the paper requires significant revisions to address the concerns raised by the reviewers. The authors need to provide more convincing evidence of the novelty and significance of their approach, clarify the motivation for their methods, and strengthen the experimental evaluation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the strengths and weaknesses of the paper based on the provided information, including the abstract, reviewer comments, and author responses (although author responses are not provided in this case).\n\n1. **Paper Contribution and Originality**: The paper proposes two new regularizers for training neural networks to achieve tighter bounds in certified robustness against adversarial attacks. This contribution is novel and addresses a significant problem in the field of adversarial robustness.\n\n2. **Technical Soundness**: Reviewers have raised several concerns regarding the technical soundness of the paper, including issues with the analysis in Section 4, the comparison between the optimal convex relaxation and the proposed method, and the lack of clarity in explaining why the proposed regularizer is effective. These concerns suggest that the paper may have technical flaws or, at the very least, lacks clear explanations of its methodology and contributions.\n\n3. **Experimental Evaluation**: The paper presents experimental results showing that the proposed regularizations lead to tighter certification bounds than non-regularized baselines. However, reviewers have pointed out the need for more comprehensive experiments, including the use of additional datasets beyond MNIST, which is considered simplistic for serious research. The comparison with other state-of-the-art methods, such as those by Gowal et al. and Wong et al., is also deemed insufficient or unclear.\n\n4. **Presentation and Clarity**: Reviewers have noted presentation issues, such as typos, formatting problems, and a lack of clarity in certain sections, particularly in the analysis and explanation of the proposed regularizers. Clear and concise writing is crucial for a paper to be accepted at a top-tier conference.\n\n5. **Impact and Relevance**: The problem addressed by the paper is interesting and relevant to the field of neural network robustness. The proposed solution has the potential to contribute meaningfully to the area, as indicated by the experimental results. However, the paper's impact is somewhat diminished by its technical and presentation issues.\n\nConsidering these points, the decision to accept or reject the paper hinges on the severity of the technical concerns, the clarity and completeness of the experimental evaluation, and the overall presentation quality. While the paper addresses an important problem and shows promising results, the technical issues and lack of clarity raised by the reviewers are significant.\n\nGiven the information provided and the fact that author responses are not included to address the reviewers' concerns, the paper in its current form does not meet the high standards expected of a top-tier conference due to its technical flaws, presentation issues, and insufficient experimental comparison.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel connection between the expressivity of Deep Neural Networks (DNNs) and Sharkovsky's Theorem from dynamical systems. The authors leverage this connection to characterize the depth-width trade-offs of ReLU networks for representing functions with periodic points, providing a lower bound on the depth necessary to represent such functions.\n\nThe reviewer comments are overwhelmingly positive, with Reviewer 1 and Reviewer 2 expressing appreciation for the authors' thorough response and the inclusion of a synthetic dataset. Reviewer 2 strongly suggests acceptance, praising the paper's motivating examples, clean exposition of Sharkovsky's theorem, and correct proofs. Reviewer 3 finds the paper to be serious and well-written, appreciating the application of Sharkovsky's results, but suggests restructuring the paper to prioritize the main contribution to machine learning.\n\nThe reviewers' comments and questions primarily focus on minor aspects, such as the potential impact of introducing a bias term, providing guiding intuition for the sharpness of the lower bound, and restructuring the paper for better clarity. These suggestions are aimed at improving the paper's presentation and accessibility, rather than addressing major flaws or concerns.\n\nThe paper's strengths include:\n\n1. **Novel connection**: The authors establish a new connection between DNNs and Sharkovsky's Theorem, providing a fresh perspective on the depth-width trade-offs of ReLU networks.\n2. **Theoretical contributions**: The paper presents a lower bound on the depth necessary to represent periodic functions, which is a significant theoretical contribution to the understanding of DNNs.\n3. **Clear exposition**: The reviewers praise the paper's clear and well-written exposition, making the complex theoretical concepts accessible to readers.\n\nThe paper's weaknesses include:\n\n1. **Structural suggestions**: Reviewer 3 suggests reorganizing the paper to prioritize the main contribution to machine learning, which could improve the paper's clarity and impact.\n2. **Limited practical applicability**: Reviewer 3 notes that the period-dependent depth lower bound may not be directly useful in practice, as it can be challenging to assess or bound the period for a given classification task.\n3. **Minor definitions**: Reviewer 3 suggests providing intuition for some definitions, such as Def. 3, to improve the paper's readability.\n\nConsidering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper makes a significant theoretical contribution to the understanding of DNNs and their expressivity. While there are some minor suggestions for improvement, the paper's overall quality, novelty, and impact warrant publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" proposes a novel approach to defend against adversarial attacks on deep neural networks. The authors introduce a framework called $\\text{AE-GAN}_\\text{+sr}$, which utilizes an autoencoder and a GAN to purify input images by searching for the closest natural reconstruction in the latent coding space.\n\nThe reviewer comments provide valuable insights into the strengths and weaknesses of the paper. Reviewer 1 highlights the paper's contribution to reducing the computational cost of Defense-GAN, a similar method, and notes that the proposed method outperforms Defense-GAN in certain scenarios. Reviewer 2, however, finds the contribution to be too incremental and suggests that the paper should include comparisons with state-of-the-art adversarial training defenses. Reviewer 3 raises concerns about the method's reliance on the autoencoder's ability to detect adversarial samples and the potential vulnerability to white-box attacks. Reviewer 4 recommends rejecting the paper due to poor writing clarity and insufficient experimental evaluations.\n\nAfter carefully analyzing the reviewer comments and the paper itself, several concerns and weaknesses are evident:\n\n1. **Incremental contribution**: Reviewer 2's comment about the contribution being too incremental is a significant concern. The paper's main contribution is reducing the computational cost of Defense-GAN, but it is unclear whether this is a substantial enough improvement to warrant publication in a top-tier conference.\n2. **Lack of comparison with state-of-the-art methods**: The paper does not include comparisons with state-of-the-art adversarial training defenses, such as PGD Adversarial Training and TRADES. This omission makes it difficult to evaluate the proposed method's effectiveness and significance.\n3. **Vulnerability to white-box attacks**: Reviewer 3's concern about the method's reliance on the autoencoder's ability to detect adversarial samples and the potential vulnerability to white-box attacks is a valid one. The paper does not provide sufficient evidence to alleviate these concerns.\n4. **Poor writing clarity**: Reviewer 4's comment about the poor writing clarity is a significant issue. The paper's writing is indeed hard to follow, and the lack of clarity makes it challenging to understand the method's implementation and training details.\n5. **Insufficient experimental evaluations**: The paper's experimental evaluations are limited to MNIST and Fashion-MNIST datasets, which are relatively simple. The lack of evaluation on more complex datasets, such as CIFAR-10, makes it difficult to assess the method's effectiveness in real-world scenarios.\n\nConsidering these concerns and weaknesses, it is clear that the paper requires significant improvements before it can be considered for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a modular framework for coordinating manipulation skills in a multi-agent setting. The framework first trains sub-skills of each end-effector with skill behavior diversification and then learns to coordinate end-effectors using diverse behaviors of the skills. The paper demonstrates the effectiveness of the proposed framework in solving challenging collaborative control tasks.\n\nThe reviewer comments provide valuable insights into the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they raise several concerns, including inconsistent notations, the treatment of skills as primitive actions, high variance in performance, and the lack of discussion on alternative multi-agent methods.\n\nReviewer 2 questions the novelty of the paper, asking whether temporal abstraction under the multi-agent setting is different from the single-agent setting. They also ask why the DIAYN method, proposed under the single-agent setting, can be directly used in the multi-agent setting. Additionally, they raise concerns about the learning of q(z|s) to approximate p(z|s) and the design choice of executing a skill for a fixed number of steps.\n\nReviewer 3 provides a weak acceptance, praising the approach as simple and scalable but criticizing the reliance on pre-specified subtasks. They suggest that using the DIAYN objective on its own or with a larger latent skill vector might have made the approach more widely applicable.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has several strengths, including:\n\n1. The paper addresses an interesting and important problem in the field of multi-agent reinforcement learning.\n2. The proposed framework is well-motivated and demonstrates promising results in solving challenging collaborative control tasks.\n3. The paper provides a clear and well-structured presentation of the framework and the empirical results.\n\nHowever, the paper also has several weaknesses, including:\n\n1. The notations and diagrams could be improved for better clarity and understanding.\n2. The treatment of skills as primitive actions and the high variance in performance are concerns that need to be addressed.\n3. The paper could benefit from a more in-depth discussion on alternative multi-agent methods and the novelty of the proposed approach.\n4. The reliance on pre-specified subtasks limits the wide applicability of the approach.\n\nTo address these weaknesses, I suggest that the authors:\n\n1. Revise the notations and diagrams to improve clarity and understanding.\n2. Provide additional experiments to address the concerns about the treatment of skills as primitive actions and the high variance in performance.\n3. Include a more in-depth discussion on alternative multi-agent methods and the novelty of the proposed approach.\n4. Consider using the DIAYN objective on its own or with a larger latent skill vector to make the approach more widely applicable.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field of multi-agent reinforcement learning. With revisions to address the weaknesses, I think the paper can be improved to meet the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper based on the standards of such conferences, considering the novelty, technical soundness, experimental results, and overall impact of the work.\n\n1. **Novelty and Contribution**: The paper proposes a formulation of Mahalanobis metric learning as an optimization problem and presents a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time. This contribution is significant as it addresses an important problem in machine learning with a novel approach based on geometric approximation algorithms. However, Reviewer 3 questions the novelty, suggesting that transforming the problem into a linear programming formulation might not be a complicated step. The authors need to clarify and emphasize the novelty and significance of their approach compared to existing methods.\n\n2. **Technical Soundness**: The technical approach seems sound, leveraging tools from the theory of linear programming in low dimensions to achieve the FPTAS. However, several reviewers have raised specific technical questions and concerns, such as the explanation for Lemma 2.1, the definition of \"accuracy\" in Theorem 1.1, and the computational hardness of the problem. These concerns need to be addressed to ensure the technical soundness of the paper.\n\n3. **Experimental Results**: The paper presents experimental results showing favorable performance compared to other methods like ITML and LMNN, especially in the presence of adversarial noise. However, Reviewer 3 notes that the experiments do not clearly show an improvement in accuracy over existing methods, which diminishes the perceived impact of the proposed approach. The authors should provide more compelling experimental evidence or clarify the advantages of their method beyond accuracy, such as computational efficiency or robustness to noise.\n\n4. **Clarity and Presentation**: Several reviewers have pointed out issues with the clarity and completeness of the paper, including the lack of a conclusion, unclear definitions (e.g., \"combinatorial dimension\"), and missing comparisons with recent relevant work. The authors need to improve the presentation, address all raised questions, and ensure that the paper is self-contained and easy to follow.\n\n5. **Comparison with State-of-the-Art**: The reviewers have suggested that the paper lacks a clear discussion of the state-of-the-art in Mahalanobis metric learning and how the proposed method compares to or advances beyond existing approaches. A thorough review of related work and a clear positioning of the proposed method within the current landscape of metric learning algorithms are necessary.\n\nConsidering these points, while the paper presents an interesting approach to Mahalanobis metric learning with a novel application of geometric approximation algorithms, it falls short in several areas critical for publication in a top-tier conference. The concerns regarding novelty, technical soundness, experimental impact, and presentation clarity need to be thoroughly addressed.\n\nGiven the current state of the paper and the feedback provided, the most appropriate decision would be to reject the paper. However, this decision is based on the potential for significant improvement rather than a dismissal of the paper's core ideas. With careful revisions addressing the raised concerns, the paper could potentially meet the high standards required for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a modular framework for learning to coordinate manipulation skills in a multi-agent setting. The framework first trains sub-skills of each end-effector with skill behavior diversification and then learns to coordinate end-effectors using diverse behaviors of the skills. The paper demonstrates the effectiveness of the proposed framework in several challenging collaborative control tasks.\n\nThe reviewer comments provide valuable insights into the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they raise several concerns, including inconsistent notations, the treatment of skills as primitive actions rather than temporally extended behavior, high variance in performance, and the lack of in-depth discussion of alternative multi-agent methods.\n\nReviewer 2 questions the novelty of the paper, asking whether temporal abstraction under the multi-agent setting is different from the single-agent setting. They also ask why the DIAYN method, proposed under the single-agent setting, can be directly used in the multi-agent setting. Additionally, they raise several concrete questions, including the learning of q(z|s) to approximate p(z|s) and the design choice of executing a skill for a fixed number of steps.\n\nReviewer 3 settles on a weak acceptance, praising the approach as simple and scalable but criticizing the reliance on specifying subtasks in advance. They suggest that using the DIAYN objective on its own or a larger latent skill vector might have made the approach more widely applicable.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has several strengths, including:\n\n1. The paper addresses an interesting and important problem in the field of multi-agent reinforcement learning.\n2. The proposed framework is well-motivated and simple, making it scalable to complex tasks.\n3. The empirical results demonstrate the effectiveness of the framework in several challenging tasks.\n\nHowever, the paper also has several weaknesses, including:\n\n1. The notations are inconsistent and confusing at times, which can make it difficult to understand the paper.\n2. The treatment of skills as primitive actions rather than temporally extended behavior is concerning.\n3. The high variance in performance is a significant issue that needs to be addressed.\n4. The lack of in-depth discussion of alternative multi-agent methods is a limitation of the paper.\n5. The reliance on specifying subtasks in advance limits the wide applicability of the approach.\n\nTo address these weaknesses, I suggest that the authors:\n\n1. Clarify the notations and provide more details on the diagram to help readers understand the framework.\n2. Provide more discussion on the treatment of skills as primitive actions versus temporally extended behavior.\n3. Run more seeds to improve the variance in performance and provide more consistent results.\n4. Provide a more in-depth discussion of alternative multi-agent methods and compare the proposed framework to these methods.\n5. Consider using the DIAYN objective on its own or a larger latent skill vector to make the approach more widely applicable.\n\nOverall, I believe that the paper has the potential to be a strong contribution to the field of multi-agent reinforcement learning. However, the authors need to address the weaknesses and limitations of the paper to make it more robust and widely applicable.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" proposes a novel method for multitask learning, which dynamically adjusts task weights during model training. The method, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), utilizes task-specific updates of the model's parameters between training epochs to estimate the positive or negative influence of auxiliary tasks on a target task.\n\nThe paper is well-written and easy to follow, with a clear summary of related work and an intuitive methodology. However, the reviewers have raised several concerns that need to be addressed.\n\nFirstly, the reviewers have pointed out the lack of theoretical justification or formal analysis of the proposed methodology. Reviewer 1 and Reviewer 2 have emphasized the need for a more rigorous theoretical foundation to support the claims made in the paper. This is a significant concern, as a top-tier conference expects papers to have a strong theoretical basis.\n\nSecondly, the experimental results have been questioned by the reviewers. Reviewer 1 and Reviewer 3 have noted that the results are not convincing enough to justify the proposed method, with the improvements over baselines being minor and sometimes statistically insignificant. Reviewer 2 has also pointed out that the results on MNIST are within a standard deviation of the baselines, which suggests that the differences may not be significant.\n\nThirdly, the reviewers have raised concerns about the novelty of the proposed method. Reviewer 2 has noted that dynamic task weighting is not a new concept and that the proposed method bears similarities to existing methods, such as meta-learning of task weights.\n\nLastly, the reviewers have suggested several improvements to the paper, including providing more thorough discussions on the algorithm and its properties, performing more experiments and ablations, and presenting statistical significance scores for all results.\n\nIn light of these concerns and suggestions, it is clear that the paper requires significant revisions to address the theoretical, empirical, and novelty concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes a generalized probability kernel (GPK) on discrete distributions with finite support, which generalizes existing discrepancy statistics such as maximum mean discrepancy (MMD) and kernel Stein discrepancy (KSD). The authors also propose a power-MMD, a natural extension of MMD in the framework of GPK, and illustrate its usage for the task of two-sample test.\n\nHowever, after carefully analyzing the paper and the reviewer comments, I have identified several major concerns that need to be addressed:\n\n1. **Lack of clarity and rigor**: The paper is plagued by numerous typos, grammatical errors, and imprecisions, which make it difficult to follow. The notation is also inconsistent, and many definitions are not properly introduced or referenced.\n2. **Incomplete and unclear theoretical contributions**: The paper's theoretical contributions are not clearly explained, and many proofs are missing or incomplete. The authors' claims about the properties of the proposed kernels are not adequately supported, and the connection to existing literature is not well-established.\n3. **Lack of experimental evaluation**: The paper does not provide any experimental evaluation of the proposed estimators, which is a critical component of any machine learning paper. The authors' claims about the effectiveness of their approach are not supported by empirical evidence.\n4. **Unclear novelty and significance**: The paper's novelty and significance are not clearly established. The authors' claims about the generality and usefulness of their approach are not adequately supported, and the connection to existing literature is not well-established.\n5. **Major technical issues**: The paper has several major technical issues, including the unclear definition of the kernel, the lack of clarity on the relationship between the kernel and the distribution, and the incomplete and unclear proof of the theorem.\n\nGiven these concerns, I believe that the paper requires significant revisions to address the major issues identified by the reviewers. The authors need to provide a clear and rigorous presentation of their theoretical contributions, conduct a thorough experimental evaluation of their approach, and establish the novelty and significance of their work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"New Bounds For Distributed Mean Estimation and Variance Reduction\" presents a novel approach to distributed mean estimation, a fundamental problem in distributed machine learning. The authors propose a lattice-based quantization method that allows for efficient communication and estimation of the mean, with guarantees dependent on the input variance rather than the input norm. This is a significant improvement over previous work, which often relies on assumptions about the input norm.\n\nThe paper is well-written, and the authors provide a clear motivation for their work, as well as a thorough overview of prior research in the area. The technical contributions are sound, and the authors provide matching upper and lower bounds for the problems considered. The experimental evaluation demonstrates the practical applicability of the approach.\n\nThe reviewer comments are largely positive, with some suggestions for improvement. Reviewer 1 suggests adding more discussion and literature review, as well as comparing the proposed approach to a straightforward alternative. Reviewer 2 requests a more direct description of the computationally simple algorithm via reduction to . Reviewer 3 points out some minor errors and suggests improving the presentation of the experimental results. Reviewer 4 raises some questions about the definition of the packing radius and suggests providing more detailed constants in the analysis.\n\nAfter carefully considering the paper and the reviewer comments, I believe that the paper makes a significant contribution to the field of distributed machine learning. The authors' approach is novel, well-motivated, and thoroughly evaluated. While there are some minor issues and suggestions for improvement, these do not detract from the overall quality of the paper.\n\nThe paper's strengths include:\n\n* A clear and well-motivated problem statement\n* A novel and sound technical approach\n* Thorough analysis and evaluation of the proposed method\n* Good experimental results demonstrating the practical applicability of the approach\n\nThe paper's weaknesses include:\n\n* Some minor errors and suggestions for improvement in the presentation\n* A lack of detailed constants in the analysis\n* Some questions about the definition of the packing radius\n\nHowever, these weaknesses are minor and do not outweigh the paper's strengths. Overall, I believe that the paper is a significant contribution to the field and should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to training deep neural networks using a block minifloat representation, which allows for efficient training with low precision weights, activations, and gradients. The paper is well-written, and the idea is clearly presented. The experiments are sound, and the hardware evaluation gives impressive results.\n\nThe reviewers have raised several questions and concerns, which the authors have addressed in their responses. Reviewer 1 asks for more information on the Kulisch accumulator and its advantages, which the authors have provided. Reviewer 1 also questions the hardware overhead for denorm support, which the authors have clarified. Reviewer 2 raises concerns about the use of different formats for forward and backward passes, which the authors have addressed by explaining that the hardware area results take into account the need to support multiple formats.\n\nReviewer 3 suggests that the paper would benefit from a more detailed explanation of the minifloat distribution alignment and the calculation of the exponent bias. Reviewer 3 also asks for more information on the derivation of Equation 6 and its relation to the hardware synthesis results.\n\nDespite these concerns, the reviewers agree that the paper makes a significant contribution to the field of deep learning and numerical representations. The proposed block minifloat representation is a general and flexible framework that can be used to design new numerical representations for deep learning. The paper provides an exhaustive exploration of the design space and discovers new low-precision representations that offer high accuracy and computational density.\n\nThe reviewers also appreciate the hardware evaluation, which provides a rigorous assessment of the proposed representation. The results show that the block minifloat representation can achieve significant reductions in area and power consumption while maintaining comparable accuracy to standard floating-point representations.\n\nIn conclusion, the paper addresses the concerns raised by the reviewers, and the authors have provided sufficient explanations and justifications for their approach. The paper makes a significant contribution to the field, and the results are impressive.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Coherent Exploration For Continuous Control\" presents a novel exploration framework for deep reinforcement learning (RL) algorithms on continuous control tasks. The method, called Deep Coherent Exploration, generalizes step-based and trajectory-based exploration by modeling the last layer parameters of the policy network as latent variables and using a recursive inference step within the policy update.\n\nThe paper is well-written, and the authors provide a clear introduction to the relevant concepts and situate their work in the literature. The empirical results for on-policy methods (A2C and PPO) are strong and demonstrate the value of this approach. The ablation experiments are also insightful, and the detailed appendix provides confidence that readers can reproduce the method.\n\nHowever, there are some concerns raised by the reviewers. Reviewer 1 notes that the story for off-policy methods (SAC) seems almost unrelated, with a more restrictive generative model, a heuristic optimization method, and weaker experimental results. Reviewer 2 questions the significance of the proposed approach in practice, given that it does not obviously improve the performance of SAC, which is a state-of-the-art method for continuous control tasks. Reviewer 3 suggests that Section 3.2 could be moved to the appendix and that the font used in the figures is very small. Reviewer 4 notes that the idea of the paper directly follows van Hoof et al. (2017) and is not much different, and that the proposed method is not effective for SAC and even degrades the performance of the HalfCheetah task.\n\nDespite these concerns, the paper has several strengths. The proposed method is mathematically solid, and the authors provide detailed mathematical derivations to support their approach. The method significantly outperforms the baselines for on-policy methods, and the authors provide ablative studies to investigate the hyper-parameter values and components of the proposed method.\n\nTo address the concerns, the authors could consider reframing the paper as an on-policy method and explicitly addressing the off-policy case as a limitation, as suggested by Reviewer 1. They could also provide additional experiments to demonstrate the effectiveness of the proposed method on more complicated domains and pure exploration or sparse rewarded tasks, as suggested by Reviewer 4.\n\nOverall, the paper presents a valuable contribution to the field of reinforcement learning, and the strengths of the paper outweigh the weaknesses. The proposed method is novel, mathematically solid, and demonstrates strong empirical results for on-policy methods.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Structure and randomness in planning and reinforcement learning\" presents a novel method called Shoot Tree Search (STS), which modifies the expansion phase of Monte Carlo Tree Search (MCTS) by expanding multiple nodes at each simulation. The authors claim that STS can control the trade-off between depth and breadth of the search more explicitly and achieve higher scores in challenging domains.\n\nThe reviewers have raised several concerns and questions about the paper. Reviewer 1 questions the technical novelty of STS, as multi-step expansion has been used in existing works. They also suggest that the relative benefit of STS over MCTS may come from the bias of the value function approximator and request more thorough ablation experiments. Reviewer 2 asks for an intuitive explanation of why STS is better than naive MCTS and points out that the empirical details in the appendix look weird. Reviewer 3 likes the simplicity of the idea but is concerned about the comparison with MCTS, suggesting that STS may not outperform vanilla MCTS with a large number of simulations. Reviewer 4 thinks that the method is not very novel, as MCTS combined with rollouts was already used in AlphaGo, and questions the benefits of STS. Reviewer 5 raises several points of confusion, including the experiment setup and the discussion of past work, but likes the thorough empirical evaluation and the simple idea.\n\nAfter carefully analyzing the reviews, I believe that the paper has some strengths, such as the simplicity of the idea and the thorough empirical evaluation. However, the reviewers have raised several concerns and questions that need to be addressed. The technical novelty of STS is questionable, and the comparison with MCTS is not thorough. The experiment setup and the discussion of past work need to be clarified.\n\nTo address these concerns, the authors should provide more thorough ablation experiments to demonstrate the benefits of STS, clarify the experiment setup, and discuss the past work more thoroughly. They should also provide an intuitive explanation of why STS is better than naive MCTS and address the questions raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method, PERIL, which combines meta-reinforcement learning and imitation learning to efficiently learn new tasks. The approach is based on PEARL, a context-based meta-RL method, and incorporates demonstration data to improve adaptation to new tasks.\n\nThe paper has several strengths, including:\n\n1. **Addressing an important problem**: The paper tackles the challenge of quickly learning new tasks, which is a crucial aspect of real-world RL applications.\n2. **Encouraging experimental results**: The experiments demonstrate that PERIL outperforms baselines and can generalize to a wide distribution of 2D tasks.\n3. **Novel combination of techniques**: The paper combines meta-RL and imitation learning, which is a promising direction for improving sample efficiency in RL.\n\nHowever, the paper also has several weaknesses, including:\n\n1. **Lack of clarity**: The notation, terminology, and problem statement are often confusing, making it challenging to evaluate the approach.\n2. **Strong assumptions**: The paper assumes that an expert distribution over trajectories conditioned on the learned latent z is available, which may be a restrictive assumption.\n3. **Insufficient related work**: The paper lacks appropriate citations in several key places, including the introduction and related work sections.\n4. **Experimental limitations**: The experiments are limited to 2D tasks and do not require sophisticated exploration, which may not substantiate the claims of PERIL's ability to explore beyond demonstrations.\n5. **Lack of ablations**: The paper does not provide ablations to demonstrate the impact of each new design element, making it unclear which components are responsible for the observed performance.\n\nConsidering these strengths and weaknesses, I believe that the paper requires significant revisions to address the clarity, related work, and experimental limitations before it can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Uncertainty for deep image classifiers on out of distribution data\" proposes a post-hoc calibration method to improve the uncertainty estimates of deep image classifiers on out-of-distribution (OOD) data. The method uses outlier exposure to properly calibrate the model probabilities and has been evaluated on benchmark datasets such as CIFAR-10 and ImageNet.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 points out that the approach assumes knowledge of the possible novel distributions, which is not a realistic assumption in practice. They also mention that the approach is simplistic and that recent work in the area has not been cited. Reviewer 2 finds the method to be simple and effective but suggests that more experiments are needed to demonstrate its applicability to more realistic OOD cases. Reviewer 3 raises concerns about the clarity of the presentation, the definition of terms, and the lack of evaluation on different types of corruptions. Reviewer 4 is positive about the additional experiments provided but suggests that more empirical and theoretical analysis is needed to understand why the methodology works well for different types of corruptions.\n\nDespite these concerns, the reviewers agree that the paper addresses an important problem and that the proposed method is novel and effective. The authors have also provided additional experiments and clarifications in response to the reviewers' comments, which has improved the paper.\n\nHowever, after careful consideration of the reviewers' comments and the authors' responses, I believe that the paper still has some significant weaknesses that need to be addressed. The assumption of knowledge of the possible novel distributions is a major concern, and the lack of evaluation on different types of corruptions and completely OOD data is a limitation. Additionally, the presentation and notation could be improved for better clarity.\n\nGiven the current state of the paper, I think that it is not ready for publication in a top-tier conference. While the paper has some promising results and ideas, it needs further refinement and evaluation to demonstrate its effectiveness and applicability to a wide range of scenarios.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) for multitask learning, which adjusts task weights dynamically during model training. The method is intuitive and well-motivated, and the paper is well-written and easy to follow. However, the reviewers have raised several concerns that need to be addressed.\n\nOne of the major concerns is the lack of theoretical justification or formal analysis of the proposed methodology. Reviewer 1 and Reviewer 2 both mention that the paper would benefit from a more rigorous theoretical foundation, which would help to understand why the proposed method works and how it relates to existing methods. Reviewer 2 also notes that the method seems ad hoc and lacks intuition, which makes it difficult to understand why certain design choices were made.\n\nAnother concern is the experimental evaluation. While the paper presents results on several tasks, the reviewers argue that the baselines are limited and the results are not convincing enough to justify the proposed method. Reviewer 1 notes that the proposed method does not consistently outperform the baselines, and Reviewer 2 mentions that the results are within a standard deviation of the baselines. Reviewer 3 also notes that the improvements are minor and sometimes lower, and most of the results fall within the statistically insignificant range.\n\nAdditionally, the reviewers have raised several questions and concerns about the experimental setup and the presentation of the results. For example, Reviewer 2 asks about the balancing of tasks in the standard multitask baseline, and Reviewer 3 asks about the statistical significance of the results. Reviewer 2 also suggests that the paper would benefit from more ablations and discussions on the learned importance of auxiliary tasks.\n\nOverall, while the paper proposes a novel and interesting method, the reviewers' concerns and questions need to be addressed before the paper can be considered for publication. The lack of theoretical justification, the limited experimental evaluation, and the need for more ablations and discussions are major concerns that need to be addressed.\n\nTo improve the paper, the authors should consider providing a more rigorous theoretical foundation for the proposed method, including a formal analysis of its properties and a discussion of its relationship to existing methods. The authors should also expand the experimental evaluation to include more baselines and tasks, and provide a more detailed discussion of the results, including statistical significance and ablations. Additionally, the authors should address the reviewers' questions and concerns, such as the balancing of tasks in the standard multitask baseline and the presentation of the results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper $\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning proposes a novel method for multitask learning, which adjusts task weights dynamically during model training. The method is based on task-specific updates of the model's parameters between training epochs. The paper is well-written, and the authors provide a clear summary of related work. However, the reviewers have raised several concerns that need to be addressed.\n\nOne of the major concerns is the lack of theoretical justification for the proposed methodology. Reviewer 1 and Reviewer 2 both point out that the method is intuitive but lacks a formal analysis. This is a significant concern, as a top-tier conference expects papers to have a strong theoretical foundation. The authors should provide a more detailed analysis of the proposed method, including its convergence properties and any guarantees on its performance.\n\nAnother concern is the experimental evaluation. While the authors provide results on several tasks, including computer vision and natural language understanding, the results are not convincing. Reviewer 1 and Reviewer 3 point out that the proposed method does not consistently outperform the baselines, and the improvements are minor. Reviewer 2 also notes that the results on MNIST are within a standard deviation of the baselines, which suggests that the differences may not be statistically significant.\n\nAdditionally, Reviewer 2 and Reviewer 3 raise concerns about the novelty of the proposed method. Reviewer 2 notes that dynamic task weighting is not a new idea and that the proposed method bears some resemblance to meta-learning of task weights. Reviewer 3 suggests that the proposed algorithm is not superior to previous approaches and that the improvements are minor.\n\nThe authors should also address the concerns raised by Reviewer 3 regarding the experimental setup. Reviewer 3 notes that the choice of the experimental setup is convincing, especially for the vision domain, but suggests that the authors should perform more experiments and ablations to demonstrate the effectiveness of the proposed method.\n\nIn terms of specific suggestions, Reviewer 2 and Reviewer 3 provide several ideas for improving the paper. These include providing more theoretical justification, performing more experiments and ablations, and presenting statistical significance scores for all results.\n\nOverall, while the paper is well-written and proposes an interesting idea, it has several significant flaws that need to be addressed. The lack of theoretical justification, the unconvincing experimental evaluation, and the concerns about novelty and experimental setup all suggest that the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Complex Query Answering with Neural Link Predictors\" proposes a novel framework for answering complex queries on incomplete Knowledge Graphs (KGs) using neural link predictors. The framework, called Continuous Query Decomposition (CQD), translates each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. The authors analyze two solutions to the optimization problem, including gradient-based and combinatorial search, and demonstrate that their approach produces more accurate results than state-of-the-art methods without the need for training on a large and diverse set of complex queries.\n\nThe reviewer comments are overall positive, with Reviewer 2, Reviewer 3, Reviewer 5, and Reviewer 4 providing detailed and constructive feedback. Reviewer 1's comment is brief but indicates that their doubts were cleared. The reviewers appreciate the paper's elegance, efficiency, and effectiveness in solving the complex query answering problem. They also acknowledge the paper's contributions, including the ability to explain the outcome of the model in terms of intermediate solutions identified for each complex query atom.\n\nHowever, the reviewers also raise several concerns and suggestions for improvement. Reviewer 2 suggests that the paper could benefit from a more qualitative discussion of the importance of EPFO queries and the 8 complex query types. Reviewer 3 questions the difference between the proposed method and previous works, such as GQE and Q2B, and suggests that the authors should provide more insights into the neural link predictors and the optimization process. Reviewer 4 suggests that the paper could benefit from an ablation study to demonstrate the sensitivity of the approach to different relational learning methods and that the authors should provide more information about the timing results. Reviewer 5 suggests that the authors should justify their choice of embedding size and provide more clarification on the presentation.\n\nThe authors' responses to the reviewer comments are not provided, so it is unclear how they address the concerns and suggestions raised by the reviewers. However, based on the reviewer comments, it appears that the paper has the potential to make a significant contribution to the field of complex query answering on incomplete KGs.\n\nThe paper's strengths include its ability to answer complex queries without requiring training on a large and diverse set of complex queries, its efficiency and effectiveness, and its ability to provide explanations for the outcome of the model. The paper's weaknesses include the need for more qualitative discussion of the importance of EPFO queries and the 8 complex query types, the lack of clarity on the difference between the proposed method and previous works, and the need for more information about the timing results.\n\nOverall, based on the reviewer comments, it appears that the paper has the potential to make a significant contribution to the field of complex query answering on incomplete KGs. The paper's strengths outweigh its weaknesses, and with some revisions to address the concerns and suggestions raised by the reviewers, the paper could be even stronger.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents an interesting study on reducing the number of supervised synaptic updates required to produce a model of the primate ventral stream. The authors propose three complementary strategies to achieve this goal: reducing the number of supervised updates, improving the random distribution of synaptic connectivity, and training only a subset of model synapses.\n\nThe reviewers have raised several concerns and questions about the paper, which can be summarized into a few main points:\n\n1. **Interpretation of BrainScore**: Reviewers 1 and 2 question the interpretation of the BrainScore metric, which is used to evaluate the similarity between the model and the brain. They argue that achieving a high BrainScore does not necessarily mean that the model is biologically meaningful or similar to the brain.\n2. **Biological plausibility**: Reviewers 3 and 5 question the biological plausibility of the model and the assumptions made by the authors. They argue that the model is not a realistic representation of the brain and that the assumptions made about the brain's development and learning mechanisms are not supported by evidence.\n3. **Methodological concerns**: Reviewers 2 and 4 raise methodological concerns, such as the lack of a clear theoretical foundation, the arbitrary nature of the choices made, and the need for more rigorous testing and evaluation of the results.\n4. **Novelty and significance**: Reviewers 5 and 4 question the novelty and significance of the proposed methods, particularly the weight initialization technique and the Critical Training approach.\n\nDespite these concerns, the paper has several strengths, including:\n\n1. **Interesting problem**: The paper addresses an important problem that has not been given much attention previously, namely reducing the number of supervised synaptic updates required to produce a model of the primate ventral stream.\n2. **Thorough experiments**: The authors conduct a series of thorough experiments to evaluate the effectiveness of their proposed methods.\n3. **Well-written paper**: The paper is well-written and clearly describes the context, methods, and results.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper has some interesting ideas and contributions, but it also has several methodological concerns and limitations that need to be addressed.\n\nThe authors need to provide more rigorous testing and evaluation of their results, clarify the interpretation of the BrainScore metric, and provide more evidence to support their assumptions about the brain's development and learning mechanisms. Additionally, the authors need to address the concerns about the biological plausibility of the model and the novelty and significance of the proposed methods.\n\nGiven the current state of the paper, I would recommend rejection, but with the possibility of resubmission after the authors have addressed the concerns and limitations raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical and practical aspects of meta-learning by applying recent theoretical advances to improve the generalization capacity of popular meta-learning algorithms. The authors propose integrating theoretical assumptions into these algorithms as regularization terms and demonstrate their effectiveness on few-shot classification benchmarks.\n\nSeveral strengths of the paper are noted by the reviewers, including its well-organized and clearly written presentation, interesting motivation, and promising experimental results. The paper attempts to connect theoretical insights with practical improvements in meta-learning, which is a valuable contribution to the field.\n\nHowever, several concerns and weaknesses are raised by the reviewers that impact the paper's novelty, technical soundness, and overall contribution. A primary concern is the novelty of the proposed regularization methods. Reviewers 1 and 4 question whether the regularization techniques, such as penalizing the condition number of the weight matrix and using the Frobenius norm regularization, are truly new or if they resemble existing methods like spectral normalization and weight decay. This raises doubts about the originality of the contribution.\n\nAnother significant issue is the experimental evaluation. While the results show improvement over vanilla meta-learning methods, some reviewers (e.g., Reviewer 1 and Reviewer 3) suggest that these improvements might not be significantly beyond the standard deviation or might be achievable with simpler tricks, as shown in other works. The lack of comprehensive comparisons with recent methods and the absence of baseline values in some tables are also highlighted as weaknesses.\n\nAdditionally, there are technical concerns regarding the consistency of the theoretical setting with the few-shot learning setting, the clarity on task distribution assumptions, and the difference in the meta-learning loss function used in the paper compared to popular objectives like MAML. The calculation of subgradients of the singular values is noted as complicated and not fully detailed, which could affect the reproducibility and practicality of the proposed method.\n\nConsidering the points raised by the reviewers, while the paper has an interesting motivation and attempts to bridge an important gap between theory and practice in meta-learning, the concerns regarding novelty, technical soundness, and the experimental evaluation outweigh its strengths. The lack of clear novelty in the proposed regularization methods, combined with the technical and experimental concerns, suggests that the paper may not meet the standards expected for publication at a top-tier conference at its current stage.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes a novel algorithm for reducing the number of neurons in deep ReLU networks based on the theory of implicit and explicit regularization. The algorithm is claimed to significantly reduce the number of neurons without sacrificing much accuracy. However, after carefully analyzing the paper, the reviewer comments, and the author responses, several major concerns and weaknesses have been identified.\n\nFirstly, the paper lacks clarity and readability. Many reviewers have pointed out that the paper is hard to follow, with important concepts and definitions missing or poorly explained. The use of footnotes to explain crucial ideas and the lack of natural flow in the presentation make it difficult for readers to understand the methodology and contributions of the paper.\n\nSecondly, the experimental evaluation is insufficient. The paper only presents results on simple toy datasets and MNIST, which is not convincing enough to demonstrate the effectiveness of the proposed algorithm. The lack of comparison with state-of-the-art pruning techniques and the absence of experiments on more complex datasets, such as CIFAR, raise concerns about the robustness and generalizability of the method.\n\nThirdly, the paper relies heavily on an unpublished work, which makes it impossible to verify the correctness and validity of the proposed technique. The lack of derivation and explanation of the equations used in the algorithm, particularly in the third step of the pruning technique, raises concerns about the soundness of the methodology.\n\nFourthly, the presentation of the paper needs significant improvement. The architecture and methodology are not clearly explained, and the notation is often confusing. The paper contains numerous typos and grammatical errors, which further detract from its overall quality.\n\nLastly, the novelty and originality of the proposed algorithm are questionable. Several reviewers have pointed out that the ideas presented in the paper are not particularly new, and the technique bears similarities to existing methods, such as the highway network.\n\nIn light of these concerns and weaknesses, it is clear that the paper is not ready for publication in its current form. The authors need to address the major issues raised by the reviewers, including improving the clarity and readability of the paper, providing more convincing experimental results, and verifying the correctness and validity of the proposed technique.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Disentangling Representations of Text by Masking Transformers\" proposes a novel approach to learn disentangled representations from pre-trained language models, such as BERT, by applying binary masks to transformer weights or hidden units. The method aims to identify subnetworks that encode distinct, complementary aspects of the representation, which can be beneficial for various downstream tasks.\n\nThe paper's strengths include:\n\n1. **Novel approach**: The proposed method is innovative and has the potential to contribute to the field of representation learning and disentanglement.\n2. **Well-written paper**: The paper is well-organized, and the authors provide a clear explanation of their approach, making it easy to follow.\n3. **Convincing experimental results**: The authors demonstrate the effectiveness of their method on several tasks, including disentangling syntax and semantics, and sentiment from genre in movie reviews.\n\nHowever, the paper also has some weaknesses and concerns raised by the reviewers:\n\n1. **Limited experimental setup**: The authors only consider two genres (Drama and Horror) in their experiments, which may not be representative of the entire range of genres.\n2. **Lack of comparison to state-of-the-art methods**: The authors claim that their approach performs as well as or better than previous methods, but the comparison is not thorough, and the results are not always convincing.\n3. **Unclear generality**: The reviewers question the generality of the proposed method, as it is not clear whether it can be applied to other domains, tasks, or models.\n4. **Need for more quantitative measures**: The authors rely heavily on visualizations and qualitative analysis, which may not be sufficient to demonstrate the effectiveness of their method.\n5. **Minor typos and formatting issues**: There are some minor typos and formatting issues throughout the paper, which do not affect the overall quality but should be addressed.\n\nTo address these concerns, the authors could:\n\n1. **Conduct more extensive experiments**: Include more genres, tasks, and models to demonstrate the generality and effectiveness of their approach.\n2. **Provide more quantitative measures**: Use quantitative metrics to evaluate the performance of their method and compare it to state-of-the-art approaches.\n3. **Clarify the generality**: Discuss the potential applications and limitations of their method, and provide more insights into the learned masks and their properties.\n4. **Address minor issues**: Correct the minor typos and formatting issues to improve the overall quality of the paper.\n\nConsidering the strengths and weaknesses of the paper, I believe that it has the potential to contribute to the field of representation learning and disentanglement. However, the authors need to address the concerns raised by the reviewers and provide more extensive experiments, quantitative measures, and clarifications on the generality of their approach.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Cortico-cerebellar networks as decoupled neural interfaces\" presents a novel hypothesis that the cerebellum acts as a decoupling machine for efficient credit assignment in the brain, drawing parallels with decoupled neural interfaces (DNIs) in deep learning. The authors propose a cortico-cerebellar-DNI (CC-DNI) model, which they test on various sensorimotor and cognitive tasks, demonstrating its potential in facilitating learning and improving performance.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewer 1 praises the intriguing hypothesis and the potential benefits of the paper's publication but suggests improvements in clarity, particularly in distinguishing between feedforward and recurrent settings, and in providing more details on the model's mechanisms, such as the use of bootstrapping. Reviewer 2 is less enthusiastic, arguing that the paper does not offer new insights into either the cerebellum or DNIs, as it primarily juxtaposes existing knowledge without adding significant novelty. Reviewer 3 appreciates the novel idea linking DNIs with the cerebellum but criticizes the lack of comparison with experimental findings from cerebellum research and the need for more concrete, testable predictions.\n\nAnalyzing the comments, several key points emerge:\n1. **Novelty and Insight**: The paper's novelty is debated. While it offers a fresh perspective on the cerebellum's role, some reviewers question whether it truly advances our understanding of the cerebellum or DNIs.\n2. **Clarity and Detail**: There's a consensus on the need for improved clarity, especially in technical aspects and in distinguishing between different settings (feedforward vs. recurrent).\n3. **Comparison with Experimental Findings**: A significant criticism is the lack of direct comparison with experimental data from cerebellum studies, which weakens the paper's scientific impact.\n4. **Testable Predictions**: The absence of clear, testable predictions that can be verified through experiments is seen as a major shortfall.\n\nConsidering these points, the paper has potential due to its novel hypothesis and the clarity of its presentation. However, it falls short in providing substantial new insights, lacking a rigorous comparison with experimental cerebellum research, and not offering concrete predictions that can be tested. These are critical aspects for a paper aiming for publication in a top-tier conference, where novelty, scientific rigor, and potential impact are paramount.\n\nGiven the current state of the paper, it seems that while it presents an interesting idea, it requires significant revisions to address the reviewers' concerns, particularly in terms of grounding the hypothesis in experimental evidence and providing testable predictions. Without these revisions, the paper's contribution to the field, as presented, may not meet the standards expected for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Automatic Music Production Using Generative Adversarial Networks\" proposes a novel framework for automatic music accompaniment using Mel-spectrograms and CycleGAN. The authors claim that their approach can generate original arrangements for bass and voice lines, and they evaluate their model on two downstream tasks: generating drum accompaniment for a given bass line and arranging an acapella song into a full song.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 points out that the title is misleading, as the paper focuses on automatic accompaniment rather than music production. They also criticize the lack of motivation for the demand for automatic accompaniment technology from artists and producers. Additionally, Reviewer 1 notes that the authors claim to be the first to treat music audio as images, but this approach is already standard in many music information retrieval (MIR) tasks.\n\nReviewer 2 suggests that the authors should have used variants of CycleGAN and other unpaired image-to-image translation models for comparison. They also point out that the experimental settings could have more variety, such as generating a bass line given the drums. Reviewer 2 criticizes the evaluation and discussion, suggesting that the authors should have included more details on inter-annotator agreement and the effect of source separation in the generated audio.\n\nReviewer 3 finds the paper interesting and well-written but suggests that the experiments could be improved by quantitatively evaluating the impact of using automatically separated signals produced by DEMUCS on the FMA dataset. They also point out that many technical details are omitted, making it impossible to reproduce the results.\n\nAfter carefully considering the reviewers' comments, I conclude that the paper has several major flaws that need to be addressed before it can be considered for publication. The title is misleading, and the authors' claims about being the first to treat music audio as images are incorrect. The experimental settings are limited, and the evaluation and discussion lack depth. Additionally, many technical details are omitted, making it impossible to reproduce the results.\n\nThe authors need to revise the paper to address these concerns, including updating the title, providing more motivation for the demand for automatic accompaniment technology, and including more variety in the experimental settings. They should also provide more details on the evaluation and discussion, including inter-annotator agreement and the effect of source separation in the generated audio. Finally, they should provide more technical details, such as the specific method for recovering the waveform from the generated Mel spectrograms.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Training independent subnetworks for robust prediction\" proposes a novel approach to improve model robustness and uncertainty estimation by training multiple independent subnetworks within a single neural network using a multi-input multi-output (MIMO) configuration. The authors demonstrate that this approach can achieve state-of-the-art results on various benchmarks, including CIFAR10, CIFAR100, and ImageNet, with a negligible increase in parameters and compute cost.\n\nThe strengths of the paper include:\n\n1. **Novel approach**: The paper proposes a new and interesting approach to improve model robustness and uncertainty estimation.\n2. **Thorough experimental study**: The authors conduct a thorough experimental study, including a detailed analysis of the subnetworks' behavior and performance on various benchmarks.\n3. **State-of-the-art results**: The paper reports state-of-the-art results on various benchmarks, demonstrating the effectiveness of the proposed approach.\n\nHowever, the paper also has some weaknesses and concerns:\n\n1. **Lack of theoretical analysis**: The paper lacks a detailed theoretical analysis of the proposed approach, which makes it difficult to understand why the subnetworks behave independently.\n2. **Limited evaluation**: The paper only evaluates the proposed approach on a limited set of benchmarks and datasets, which may not be representative of all possible scenarios.\n3. **Comparison to other methods**: The paper does not provide a thorough comparison to other ensemble methods, which makes it difficult to assess the significance of the proposed approach.\n4. **Concerns about parameter increase**: Reviewer 1 points out that the paper claims that the benefits of using multiple predictions can be achieved \"for free,\" but the proposed model still increases the number of parameters, albeit by a small amount (1%).\n5. **Lack of citations**: Reviewer 4 notes that the paper lacks citations to related work, which declines the quality of the paper.\n\nAfter carefully considering the strengths and weaknesses of the paper, I believe that the paper has the potential to make a significant contribution to the field, but it requires some revisions to address the concerns and weaknesses mentioned above.\n\nThe authors should provide a more detailed theoretical analysis of the proposed approach, including an explanation of why the subnetworks behave independently. They should also conduct a more thorough evaluation of the proposed approach on a wider range of benchmarks and datasets. Additionally, the authors should provide a more thorough comparison to other ensemble methods and address the concerns about the parameter increase.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents a novel approach to integrating linguistic and visual information for image segmentation tasks based on referring expressions. The authors argue that using language to modulate both top-down and bottom-up visual processing pathways can lead to significant improvements over traditional methods that only consider top-down attention. The paper is well-written, and the task of segmenting objects based on referring expressions is important and well-chosen.\n\nHowever, upon careful analysis of the reviewer comments and the authors' responses, several concerns arise that impact the paper's potential for acceptance at a top-tier conference. \n\n1. **Novelty and Contribution**: Reviewer 1 suggests that the novelty and contribution of the paper are \"rather thin\" because many ways of modeling language are not explored. This concern is echoed by Reviewer 4, who questions the originality of the model and its conceptual contribution beyond numerical task improvements. The authors' response does not fully address these concerns, as they do not provide a clear motivation for their model based on cognitive science or a priori expectations for the task.\n\n2. **Empirical Results and Significance**: Reviewer 2 and Reviewer 4 express concerns about the empirical results. Despite the authors' claims of achieving state-of-the-art or competitive results, the performance on certain metrics and datasets is not consistently superior to existing works. Moreover, Reviewer 4 points out the lack of statistical significance tests, which makes it difficult to assess the reliability of the reported improvements. The variability in results across different evaluation sets and the absence of a thorough error analysis to understand the model's success also raise doubts.\n\n3. **Model Analysis and Interpretability**: Several reviewers (Reviewer 1, Reviewer 2, and Reviewer 4) request more insight into how the linguistic representations interact with the visual processing pathway. They suggest that the paper lacks a satisfactory analysis of what exactly the linguistic representations contribute to the model's performance, especially in terms of modulating low-level visual features. This lack of transparency and interpretability makes it challenging to understand the model's strengths and limitations.\n\n4. **Response to Reviewer Comments**: The authors' responses to some of the reviewer comments, particularly those related to the novelty of the work, the motivation behind the model design, and the empirical results, do not fully address the concerns raised. For instance, the reference to Mei et al. (2018) as a baseline, which is not found in the paper, raises questions about the completeness of the related work and the novelty of the proposed approach.\n\nConsidering these points, while the paper addresses an important problem and presents some promising results, the concerns regarding its novelty, the significance and reliability of its empirical contributions, and the lack of deep analysis into the model's workings are substantial. These issues undermine the paper's potential for making a significant and lasting contribution to the field as required by a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Ecological Inference\" presents an interesting approach to the ecological inference problem by utilizing deep learning techniques. The authors propose an efficient approximation to the loss function, enabling the construction of ecological versions of linear models, deep neural networks, and Bayesian neural networks. The paper evaluates the proposed models using real-world data from the Maryland 2018 midterm elections and demonstrates promising results.\n\nHowever, upon careful analysis of the reviewer comments, several concerns arise that impact the paper's overall quality and readiness for publication. \n\n1. **Clarity and Organization**: Multiple reviewers (Reviewer 1, Reviewer 2, and Reviewer 4) mention that the paper is not easy to follow, with suggestions for improving the structure, particularly in introducing key concepts like Poisson binomial/multinomial losses earlier and clarifying the input data. Reviewer 2 and Reviewer 4 also point out issues with the readability of specific sections, such as the experiment section, and the lack of clarity in explaining the evaluation tasks and baselines.\n\n2. **Experimental Analysis and Baselines**: Reviewer 2 criticizes the paper for incomplete and unclear experiments, lacking a comparison to a standard MLM as a baseline, which is crucial for understanding the improvements brought by the proposed deep MLM. The reviewer also questions the absence of detailed analysis for certain claims made in the paper, suggesting that training a regular MLM for comparison would be beneficial.\n\n3. **Reproducibility**: Reviewer 2 highlights the omission of key experimental details necessary for reproducibility, such as how train/test splits are created, the specifics of neural network architectures (e.g., number of hidden units, nonlinearities used), and the handling of binomial loss for third-party candidates.\n\n4. **Typos and Minor Errors**: All reviewers mention the presence of typos and minor writing issues throughout the paper, which, while not critical, detract from the overall professionalism and polish of the submission.\n\n5. **Originality and Related Work**: Reviewer 4 expresses concern that the proposed approach is not new and that related work is not adequately cited, suggesting that the paper does not significantly push the boundary of existing technology. The reviewer recommends discussing the relationship with other problems in the machine learning community, such as learning with label proportions, distribution regression, and collective graphical models.\n\n6. **Technical Contributions**: Reviewer 4 questions the novelty of the technical contributions, suggesting that the paper may not be ready for publication due to a lack of new insights and inadequate discussion of related work.\n\nConsidering these points, while the paper addresses an important problem and presents an interesting application of deep learning to ecological inference, the concerns regarding clarity, experimental analysis, reproducibility, and originality are significant. These issues undermine the paper's readiness for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"GG-GAN: A Geometric Graph Generative Adversarial Network\" proposes a novel approach to graph generation using a geometric perspective and a Wasserstein GAN architecture. The paper is well-written, and the authors provide a clear and concise explanation of their methodology.\n\nThe reviewers have raised several concerns and questions about the paper, which can be summarized into the following categories:\n\n1. **Experimental evaluation**: Reviewers 1, 4, and 5 have expressed concerns about the limited size of the datasets used in the experiments, the lack of comparison with other state-of-the-art methods, and the need for more convincing results.\n2. **Theoretical analysis**: Reviewers 2, 4, and 5 have questioned the novelty and significance of the theoretical contributions, including the proofs and theorems presented in the paper.\n3. **Methodology**: Reviewers 2 and 5 have raised concerns about the design of the GG-GAN architecture, including the use of a similarity function, the introduction of the $\\phi$ parameter, and the avoidance of collisions.\n4. **Literature review**: Reviewers 4 and 5 have pointed out the lack of a comprehensive literature review, particularly in the context of graph generation.\n\nAfter carefully analyzing the paper and the reviewer comments, I have identified several strengths and weaknesses:\n\nStrengths:\n\n* The paper proposes a novel approach to graph generation using a geometric perspective and a Wasserstein GAN architecture.\n* The authors provide a clear and concise explanation of their methodology.\n* The paper includes a thorough analysis of the theoretical contributions, including proofs and theorems.\n\nWeaknesses:\n\n* The experimental evaluation is limited, and the results are not convincing.\n* The theoretical analysis, while thorough, may not be significant or novel.\n* The methodology has several design choices that are not well-justified, including the use of a similarity function and the introduction of the $\\phi$ parameter.\n* The literature review is lacking, particularly in the context of graph generation.\n\nBased on these strengths and weaknesses, I believe that the paper requires significant revisions to address the concerns raised by the reviewers. The authors need to provide more convincing experimental results, clarify the significance and novelty of their theoretical contributions, and provide a more comprehensive literature review.\n\nFurthermore, the authors need to address the specific concerns raised by the reviewers, including:\n\n* Providing more detailed information about the datasets used in the experiments, including the number of non-isomorphic classes.\n* Comparing their method with other state-of-the-art methods, including NetGAN and TagGen.\n* Clarifying the design choices made in the GG-GAN architecture, including the use of a similarity function and the introduction of the $\\phi$ parameter.\n* Providing more insight into the avoidance of collisions and the learning of the $\\phi$ parameter.\n\nIn conclusion, while the paper has some strengths, the weaknesses and concerns raised by the reviewers outweigh them. Therefore, I recommend rejecting the paper in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Global Node Attentions via Adaptive Spectral Filters\" proposes a novel graph neural network (GNN) model that incorporates a global self-attention mechanism defined using learnable spectral filters. This allows the model to attend to any nodes in the graph, regardless of distance, which is particularly useful for disassortative graphs where local homophily is weak.\n\nThe reviewers have raised several concerns and criticisms about the paper. Reviewer 1 questions the novelty of the approach, arguing that ChevNet can already incorporate the entire graph by setting a large K, and that the failure of ChevNet is due to over-overparameterization and overfitting rather than the limitation of local attention. Reviewer 1 also criticizes the complexity of the proposed model, arguing that it is not linear in O(V) and that the non-local attention makes it unable to leverage efficient matrix eigen decomposition.\n\nReviewer 2 likes the paper but lacks the expertise to evaluate the experimental results and theoretical contributions. Reviewer 3 finds the paper to be well-motivated and intuitive, with thorough empirical evaluation, but criticizes the lack of scalability and the limited number of disassortative graphs used in the experiments. Reviewer 4 criticizes the limited novelty of the approach, arguing that using attention to capture features from far-away nodes is not new, and that the proposed model may suffer from overfitting.\n\nDespite these criticisms, the paper has several strengths. The proposed model is well-motivated and intuitive, and the empirical evaluation is thorough and demonstrates the effectiveness of the model on both assortative and disassortative graphs. The paper also addresses an important problem in the field of graph neural networks, which is the limitation of local homophily.\n\nHowever, the concerns about complexity and scalability are significant. The proposed model requires computing the entire  matrix, which has a complexity of more than O(|V|^2), and the non-local attention makes it unable to leverage efficient matrix eigen decomposition. This limits the scalability of the model to large graphs.\n\nAfter careful consideration of the strengths and weaknesses of the paper, I conclude that the concerns about complexity and scalability, combined with the limited novelty of the approach, outweigh the strengths of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" proposes a novel approach to learning layout representations for graphic design intelligence using a Transformer-based model. The authors contribute a large-scale dataset of over 1 million slides and demonstrate the effectiveness of their model on several downstream tasks, including element role labeling, image captioning, layout auto-completion, and layout retrieval.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 points out the lack of details on the dataset construction and statistics, and suggests that the evaluation tasks seem to be sub-tasks of pre-training rather than separate tasks. They also recommend including more baseline models, such as Neural Design Network and LayoutGAN, and conducting more systematic experiments with both quantitative and qualitative results.\n\nReviewer 2 criticizes the paper for lacking originality, as the task of learning layout embeddings is not new. They also point out that the paper does not provide sufficient details on the Transformer model and attention scheme, and that the evaluation is weak. They suggest comparing the proposed model to other state-of-the-art models, such as Graph Neural Networks, and conducting more thorough experiments to demonstrate the merits and advantages of the proposed approach.\n\nReviewer 3 believes that the evaluation of the method is lacking and could be improved significantly. They suggest proposing higher complexity tasks where the decision tree baseline would fail, and comparing the proposed model to other lower complexity methods that take 2D layout into consideration. They also recommend evaluating the model in the context of existing work to understand its importance and potential applications.\n\nDespite these concerns, the reviewers agree that the paper proposes an interesting and exciting direction for research. The contribution of a large-scale dataset and the demonstration of the effectiveness of the proposed model on several downstream tasks are significant strengths of the paper.\n\nHowever, after careful consideration of the reviewers' comments and the paper's strengths and weaknesses, I believe that the paper requires significant revisions to address the concerns raised by the reviewers. The lack of details on the dataset construction and statistics, the weakness of the evaluation, and the lack of comparison to other state-of-the-art models are significant limitations of the paper.\n\nTherefore, I recommend rejecting the paper in its current form, but encouraging the authors to revise and resubmit it after addressing the concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes a novel algorithm for reducing the number of neurons in deep ReLU networks based on the theory of implicit and explicit regularization. The algorithm aims to prune neurons without significantly affecting the network's performance. However, after carefully analyzing the paper, the reviewer comments, and the author responses, several major concerns and weaknesses have been identified that impact the paper's suitability for publication in a top-tier conference.\n\n1. **Lack of Clarity and Presentation Issues**: Multiple reviewers have pointed out that the paper is not well-written, lacks clarity, and is hard to follow. This includes issues with notation, missing definitions, and the use of footnotes to explain important concepts. Such presentation issues hinder the understanding and verification of the proposed method.\n\n2. **Insufficient Experimental Evaluation**: The experimental section has been criticized for being too limited, focusing only on simple toy problems and the MNIST dataset, which is considered too basic to adequately demonstrate the effectiveness of the pruning technique. Moreover, there is a lack of comparison with state-of-the-art pruning methods, which makes it difficult to assess the novelty and performance of the proposed algorithm.\n\n3. **Reference to Unpublished Work**: The paper heavily relies on an \"unpublished work\" by the authors, which cannot be reviewed or verified. This raises significant concerns about the validity and reproducibility of the results, as crucial details and derivations are not provided or are referenced to inaccessible sources.\n\n4. **Lack of Novelty and Motivation**: Some reviewers question the novelty of the proposed technique, suggesting that the ideas presented are not particularly new, especially given the extensive literature on network pruning. The motivation behind the technique and its positioning with respect to the state of the art are also found to be lacking.\n\n5. **Technical Concerns and Typos**: Several technical issues and typos have been identified, including unclear equations, undefined terms, and inconsistencies in the presentation. These issues further undermine the paper's credibility and readability.\n\nConsidering these points, it is clear that the paper, in its current form, does not meet the standards expected for publication in a top-tier conference. The issues with clarity, experimental evaluation, novelty, and technical soundness are too significant to overlook. Therefore, the most appropriate decision would be to reject the paper, encouraging the authors to address the raised concerns, improve the paper's quality, and resubmit it for future consideration.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\" proposes a novel exploration strategy for continuous reinforcement learning, leveraging the principle of optimism in the face of uncertainty. The reviewers have provided detailed comments, highlighting both the strengths and weaknesses of the paper.\n\nThe main strengths of the paper include:\n\n1. **Novelty**: The paper introduces a new exploration principle, Max-Q Entropy Search (MQES), which combines the advantages of information-theoretic principles and distributional reinforcement learning.\n2. **Theoretical foundations**: The authors provide a theoretical framework for MQES, including a derivation of the exploration policy and a discussion of the role of epistemic and aleatoric uncertainty.\n3. **Empirical evaluation**: The paper includes an empirical evaluation of MQES on Mujoco environments, demonstrating its effectiveness in comparison to state-of-the-art algorithms.\n\nHowever, the reviewers have also raised several concerns and criticisms, including:\n\n1. **Lack of clarity**: Multiple reviewers have noted that the paper is difficult to follow, with unclear notation, confusing equations, and vague language.\n2. **Insufficient ablation studies**: The reviewers have suggested that the authors should provide more ablation studies to demonstrate the effectiveness of the proposed method and its components.\n3. **Limited baseline comparison**: The paper only compares MQES to a limited set of baselines, and the reviewers have suggested that additional comparisons to other exploration methods would be beneficial.\n4. **Unclear hyperparameter tuning**: The reviewers have noted that the paper does not provide sufficient information about the hyperparameter tuning process, which could impact the reproducibility of the results.\n5. **Conceptual concerns**: Some reviewers have raised concerns about the underlying concept of MQES, including its reliance on the Q-function and its potential limitations in sparse-reward settings.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper requires significant revisions to address the concerns and criticisms raised. While the paper presents a novel and interesting idea, the lack of clarity, insufficient ablation studies, and limited baseline comparison are major issues that need to be addressed.\n\nFurthermore, the paper's writing quality and notation need to be improved to make it more accessible to the reader. The authors should also provide more details about the hyperparameter tuning process and consider additional comparisons to other exploration methods.\n\nIn light of these concerns, I believe that the paper is not ready for publication in its current form. However, with significant revisions to address the reviewers' comments and concerns, the paper has the potential to make a valuable contribution to the field of reinforcement learning.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"On the Implicit Biases of Architecture & Gradient Descent\" presents an interesting and timely investigation into the roles of neural network architecture and gradient descent in generalization. The authors provide a theoretical analysis of the implicit bias of architecture using a Bayesian approach and propose a novel estimator for the average classification error of a Gaussian process. They also explore the implicit bias of gradient descent and its relationship to margin.\n\nThe paper has several strengths, including its well-written and clear presentation, interesting theoretical results, and insightful discussion of margin. The experiments comparing different widths of MLP and the infinite width limit are also noteworthy. However, the paper has some weaknesses, such as the limited applicability of the analytical bounds to only MLPs of different depths, the use of a relatively new optimization algorithm (Nero) without justification or comparison to other methods, and the lack of empirical validation for more complex architectures like CNNs or ResNets.\n\nThe reviewer comments highlight several concerns, including the incremental nature of the contributions, the lack of novelty and significance in the empirical results, and the need for more comprehensive experiments to support the claims. Some reviewers also question the significance of the bound and the analysis of the implicit bias of gradient descent, suggesting that it may not be as novel or relevant as claimed.\n\nAfter carefully considering the paper and the reviewer comments, I conclude that while the paper has some interesting ideas and contributions, it falls short of the standards expected for a top-tier conference. The limitations in the applicability of the analytical bounds, the lack of empirical validation for more complex architectures, and the concerns about the novelty and significance of the contributions are significant enough to warrant rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Boosting Approach to Reinforcement Learning\" presents an interesting and novel approach to solving reinforcement learning (RL) problems with sample complexity independent of the number of states. The authors propose a way to combine weak learners effectively to generate a near-optimal policy, leveraging boosting ideas from supervised learning. The technical results needed to achieve the sample complexity guarantee may be of independent interest.\n\nHowever, the paper suffers from significant clarity and readability issues. Many reviewers have pointed out that the paper is extremely dense and hard to read, with notation overload, typos, and undefined notation. The lack of motivation and justification for definitions and algorithmic/analytical design choices makes it difficult for readers to understand the approach and results. The paper would benefit from more supporting text explaining the reasoning behind the definitions and algorithms.\n\nAdditionally, some reviewers have raised concerns about the novelty and significance of the contributions. While the idea of applying boosting to RL is interesting, some reviewers have noted that the techniques used are similar to those in prior work (e.g., Hazan and Singh, 2021). The paper could be strengthened by more discussion of the differences between the proposed approach and existing work.\n\nThe empirical novelty and significance of the paper are also limited, as there are no numerical experiments or comparisons with existing algorithms. The paper would benefit from the inclusion of some initial experiments to validate the effectiveness of the proposed algorithms.\n\nDespite these limitations, some reviewers have noted that the core idea of the paper is valuable and could be of interest to both the RL and optimization communities. With significant revisions to address the clarity and readability issues, as well as more discussion of the novelty and significance of the contributions, the paper could be strengthened.\n\nBased on the reviews, I believe that the paper requires major revisions to address the clarity, readability, and novelty concerns. While the idea is interesting, the current presentation and lack of empirical evaluation make it difficult to recommend for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" presents an interesting topic and a clear hypothesis about the greedy nature of learning in multi-modal deep neural networks. The authors propose a metric, conditional utilization rate, and its proxy, conditional learning speed, to measure the imbalance in modality utilization during training. They also introduce a training algorithm, balanced multi-modal learning, to address this issue.\n\nThe paper has several strengths, including:\n\n1. Interesting research topic: The paper focuses on a relevant problem in multi-modal learning, which is the imbalance in modality utilization during training.\n2. Clear hypothesis: The authors clearly state their hypothesis about the greedy nature of learning in multi-modal deep neural networks.\n3. Easy to follow: The paper is well-organized, and the authors provide a clear explanation of their proposed method and experiments.\n\nHowever, the paper also has several weaknesses, including:\n\n1. Lack of theoretical support: Several reviewers pointed out that the paper lacks theoretical support for the proposed method and the greedy learner hypothesis.\n2. Insufficient experimental validation: Some reviewers noted that the experimental results are not sufficient to validate the main claims, and the comparisons with other methods are limited.\n3. Unclear explanations: Some reviewers found the explanations of the re-balancing step and the derivation of uni-modal predictions from multi-modal DNNs to be unclear.\n4. Limited datasets: The paper only uses visual modalities, and some reviewers suggested that the authors should include more natural modalities, such as audio and visual.\n\nAfter carefully analyzing the reviewer comments and the paper, I conclude that the paper has some merits, but it also has significant weaknesses that need to be addressed. The lack of theoretical support, insufficient experimental validation, and unclear explanations are major concerns that need to be addressed.\n\nWhile some reviewers recommended a weak accept or suggested that the paper has some potential, the majority of the reviewers recommended a reject or pointed out significant weaknesses. Therefore, based on the provided information, I conclude that the paper should be rejected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents a novel approach to 3D snapshot microscopy using Fourier convolutional neural networks. The authors propose a neural network architecture that applies a series of convolution kernels on the Fourier domain to focus on global information of the captured images. The paper is well-written, and the results are promising, demonstrating the effectiveness of the proposed method over existing non-Fourier neural networks in the context of computational imaging.\n\nThe reviewers have provided detailed comments, highlighting both the strengths and weaknesses of the paper. The main strengths of the paper include:\n\n1. **Novel application of Fourier convolutional neural networks**: The authors propose a new approach to 3D snapshot microscopy using Fourier convolutional neural networks, which has shown promising results in simulated experiments.\n2. **Well-written paper**: The paper is well-organized, and the authors have provided a clear description of the proposed method and its evaluation.\n3. **Thorough evaluation**: The authors have conducted an extensive synthetic evaluation to validate the effectiveness of the proposed method over existing non-Fourier neural networks.\n\nHowever, the reviewers have also raised several concerns and weaknesses, including:\n\n1. **Lack of technical novelty**: Some reviewers have pointed out that the main technical ideas in this paper have already been explored in previous works, such as Rippel et al.\n2. **Limited evaluation**: The paper only evaluates the proposed method on simulated data, and the reviewers have raised concerns about the lack of real-world experiments.\n3. **Insufficient comparison with existing methods**: Some reviewers have pointed out that the comparison with existing methods, such as UNet, is not sufficient, and the authors should provide more detailed comparisons to demonstrate the effectiveness of the proposed method.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has some merits, but it also has some significant weaknesses. The lack of technical novelty and the limited evaluation are significant concerns that need to be addressed. However, the paper is well-written, and the authors have provided a clear description of the proposed method and its evaluation.\n\nTo address the concerns raised by the reviewers, I would suggest that the authors provide more detailed comparisons with existing methods, including UNet, and conduct real-world experiments to validate the effectiveness of the proposed method. Additionally, the authors should provide more context and discussion on the relation of this work to previous works, such as Rippel et al.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper proposes a novel method, Differentiable Diffusion Sampler Search (DDSS), to optimize fast samplers for pre-trained diffusion models. The method uses a non-Markovian hierarchical generative model, Generalized Gaussian Diffusion Models (GGDM), and optimizes the sampler parameters by minimizing a perceptual loss. The paper demonstrates strong results on unconditional image generation across various datasets, outperforming existing methods such as DDPM and DDIM.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers 1 and 2 raise concerns about the theoretical justification of the proposed method, the choice of the perceptual loss, and the lack of comparison to other methods. However, they also acknowledge the empirical improvements and the significance of the problem tackled by the paper. Reviewer 3 finds the paper interesting and the results compelling, but raises questions about the practicality of the method and the dependence on the dataset. Reviewer 4 is concerned about the theoretical guarantees of the method and the potential overfitting to the evaluation metrics. Reviewer 5 appreciates the simplicity and empirical performance of the method but criticizes the clarity of the paper and the lack of experiments on larger datasets.\n\nAfter carefully considering the reviewer comments, I believe that the paper has made significant contributions to the field of diffusion models and has demonstrated strong empirical results. While the theoretical justification of the method is not fully established, the paper has shown that the proposed approach can lead to improved sample quality and faster sampling times.\n\nThe authors have addressed some of the concerns raised by the reviewers in their rebuttal, providing additional explanations and justifications for their method. However, some concerns, such as the lack of experiments on larger datasets and the potential overfitting to the evaluation metrics, remain.\n\nDespite these concerns, I believe that the paper has the potential to make a significant impact on the field and that the contributions outweigh the weaknesses. Therefore, I recommend accepting the paper for publication, with the suggestion that the authors address the remaining concerns and clarify the theoretical justification of their method in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" presents an interesting analysis of the behavior of Stochastic Gradient Descent with momentum (SGDm) under non-iid sampling conditions, specifically when the data distribution exhibits covariate shift. The authors theoretically and empirically investigate the phenomenon of resonance-driven divergence in SGDm, which occurs when the oscillation frequency of the data covariate shift matches the natural frequency of the SGDm updates.\n\nThe paper's strengths include:\n\n1. **Identification of an important problem**: The authors highlight a critical issue in the convergence of SGDm under non-iid sampling conditions, which is relevant to various machine learning applications, such as continual learning and reinforcement learning.\n2. **Theoretical analysis**: The paper provides a thorough theoretical analysis of the resonance phenomenon in SGDm, using techniques from numerical analysis and ordinary differential equations.\n3. **Empirical validation**: The authors conduct extensive empirical experiments to validate their theoretical findings, demonstrating the resonance phenomenon in various settings, including linear and nonlinear models.\n\nHowever, the paper also has some weaknesses and limitations:\n\n1. **Technical novelty**: Some reviewers question the technical novelty of the paper, arguing that the mathematical techniques used are not particularly new or innovative.\n2. **Limited scope**: The theoretical analysis is limited to a specific setting (linear regression with periodic covariate shift), and the authors acknowledge that the results may not generalize to more complex scenarios.\n3. **Lack of practical examples**: Some reviewers suggest that the paper could benefit from more practical examples or empirical datasets that demonstrate the relevance of the resonance phenomenon in real-world applications.\n\nAfter carefully considering the reviewer comments and the paper's content, I conclude that the paper's strengths outweigh its weaknesses. The identification of the resonance phenomenon and its thorough analysis, both theoretically and empirically, contribute significantly to the understanding of SGDm's behavior under non-iid sampling conditions. While the technical novelty and limited scope of the paper are valid concerns, they do not outweigh the paper's overall contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a novel analysis of the optimization landscape of two-layer ReLU neural networks with weight decay. The authors provide a convex optimization program that can find all globally optimal solutions of the non-convex neural network training problem. The paper also establishes connections between Clarke stationary points and global minima of subsampled convex problems, provides a polynomial-time algorithm for testing optimality, and proves that the non-convex loss landscape has no spurious local minima for a large enough number of neurons.\n\nThe reviewer comments are overall positive, with Reviewer 2 and Reviewer 4 strongly recommending acceptance. Reviewer 1 and Reviewer 3 provide more nuanced feedback, highlighting some minor issues with the paper's claims and suggesting that the results could be extended to multiple layers or other activation functions.\n\nThe strengths of the paper include:\n\n1. **Novel contributions**: The paper provides a new and interesting perspective on the optimization landscape of two-layer ReLU neural networks, advancing the state-of-the-art in this area.\n2. **Technical correctness**: The majority of reviewers agree that the paper's claims and statements are well-supported and correct, with Reviewer 4 giving a correctness score of 4.\n3. **Significance**: The paper's contributions are significant, and the results have the potential to inspire further developments in the field.\n\nThe weaknesses of the paper include:\n\n1. **Limited scope**: The paper only considers two-layer ReLU neural networks with weight decay, which may limit the applicability of the results.\n2. **Minor issues**: Reviewer 1 and Reviewer 3 point out some minor issues with the paper's claims and suggest that the results could be improved with additional analysis or experimentation.\n3. **Lack of experimental validation**: Reviewer 3 suggests that the paper could be strengthened with experimental validations across different tasks.\n\nAfter carefully considering the reviewer comments and the paper's strengths and weaknesses, I conclude that the paper should be accepted for publication. The novel contributions, technical correctness, and significance of the results outweigh the minor issues and limitations of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the reviewer comments, the paper's technical novelty and significance, empirical novelty and significance, and its overall contribution to the field.\n\n1. **Technical Novelty and Significance**: The majority of the reviewers agree that while the platform itself is well-engineered and useful, the technical novelty of the paper is somewhat limited. Reviewers 1, 2, 3, and 4 mention that the methodological contribution is not significant enough for a top-tier conference, with Reviewer 2 stating it's more like an assembly of software design docs and a manual for an open-source platform. Reviewer 4 notes that the system gives the feeling of \"yet another Bayesian Optimization package\" with only minor contributions, particularly the Believer-Penalizer strategy.\n\n2. **Empirical Novelty and Significance**: The empirical results presented in the paper have been questioned by several reviewers. Reviewer 4 points out that the claims of outperforming other methods are overstated, and the results show that AutoOED is not always the best and sometimes underperforms. The comparison to other packages and the real-world benchmark have also been criticized for being potentially \"cooked up\" and lacking a thorough comparison.\n\n3. **Correctness and Clarity**: Most reviewers find the paper well-written and clear, but there are minor issues with some claims not being well-supported. Reviewer 5 suggests some minor corrections and improvements for clarity and consistency.\n\n4. **Overall Contribution**: Despite the limitations in technical and empirical novelty, the paper presents a useful platform for multi-objective optimization problems, which is a significant and pervasive issue in Optimal Experimental Design. Reviewer 5 appreciates the motivation of the problem and the clarity of the paper, suggesting that the open-sourcing of such a framework could complement current state-of-the-art packages.\n\nConsidering these points, the decision to accept or reject the paper hinges on the balance between its usefulness and contribution to the field versus the expectations of novelty and significance for a top-tier conference. The reviewers' comments suggest that while AutoOED is a valuable tool, its methodological and empirical contributions may not meet the high standards of novelty and significance expected for publication at such a conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"A Frequency Perspective of Adversarial Robustness\" presents a frequency-based understanding of adversarial examples in deep neural networks, challenging the common misconception that adversarial examples are high-frequency noise. The authors provide theoretical and empirical findings to support their claims, including experiments on CIFAR-10 and ImageNet-derived datasets.\n\nThe reviewer comments are mixed, with some reviewers praising the paper's insights and experimental design, while others raise concerns about the novelty and significance of the contributions. Reviewer 1 suggests that the authors improve their work by taking into account universal adversarial perturbations (UAPs), which could make their work more unique and interesting. Reviewer 2 questions the authors' claims, stating that the conclusions under Fourier transforms and DCT should be approximately the same, and that the authors' reasons for differentiating their work from prior studies are not strong enough.\n\nReviewer 3 provides a detailed analysis of the paper's strengths and weaknesses, stating that the conclusion that adversarial examples are dataset-dependent is too general and not sufficiently validated. Reviewer 3 also raises concerns about the paper's technical novelty and significance, stating that the contributions are significant but not entirely new. Reviewer 4 recommends acceptance, praising the paper's interesting experimental setups and findings, but suggests that the presentation of figures and minor grammatical errors should be addressed.\n\nReviewer 5 decides to reject the paper, stating that the content is confusing and that the authors' contributions are not well-supported. Reviewer 6 recommends acceptance, praising the paper's valuable insights and extensive experimental design, but raises concerns about the limitations of the study, including the sole examination of ResNet18 and VGG16 architectures.\n\nAfter carefully analyzing the reviewer comments, I conclude that the paper has both strengths and weaknesses. The paper provides interesting insights into the frequency properties of adversarial examples and presents extensive experimental results. However, the novelty and significance of the contributions are questioned by some reviewers, and the paper's limitations, such as the sole examination of certain architectures, are not fully addressed.\n\nDespite these concerns, I believe that the paper's strengths outweigh its weaknesses. The paper provides a unique perspective on adversarial robustness, and the experimental results are well-designed and extensive. The authors' responses to the reviewer comments address some of the concerns, and the paper's contributions, although not entirely new, are significant and somewhat novel.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"The weighted mean trick  optimization strategies for robustness\" presents an interesting idea of using weighted mean to optimize higher-order moments of the loss distribution, which can lead to improved robustness against outliers. However, after carefully analyzing the reviewer comments, I have identified several concerns that need to be addressed.\n\nFirstly, Reviewer 1 and Reviewer 4 point out that the empirical improvement may be due to tuning, and the choice of $\\lambda_i$ is not theoretically justified. This raises concerns about the validity of the results. Reviewer 2 and Reviewer 3 also question the correctness of the proposed algorithm, as it only uses the first term of the gradient descent update, which may not be minimizing the desired objective.\n\nAdditionally, Reviewer 4 and Reviewer 5 express concerns about the theoretical contribution of the paper, stating that the results are limited and not particularly novel. Reviewer 5 also points out that the paper drops the dependence on $\\theta$ everywhere, which makes the theoretical results straightforward but not particularly useful.\n\nReviewer 6 raises several technical questions about the paper, including the assumption of the existence of the first moment of $\\ell$, the convexity of the objective, and the concentration behavior of the samples from $W \\ell$ around their expectation.\n\nOn the other hand, Reviewer 7 finds the paper interesting and thinks that the observation could have consequences in several robust learning algorithms. However, they also mention that the theorems are relatively straightforward, and more consequences of this observation in the context of other algorithms would be desirable.\n\nConsidering all the comments, I believe that the paper has some interesting ideas, but the concerns raised by the reviewers need to be addressed. The paper lacks a clear theoretical justification for the choice of $\\lambda_i$, and the correctness of the proposed algorithm is questionable. Additionally, the theoretical contribution of the paper is limited, and more work is needed to demonstrate the significance and novelty of the results.\n\nTherefore, I think that the paper needs significant revisions to address the concerns raised by the reviewers. While the idea is interesting, the current version of the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"Target Propagation via Regularized Inversion\" should be accepted or rejected for publication at a top-tier conference, we must carefully consider the reviewer comments, the paper's technical novelty and significance, empirical novelty and significance, correctness, and clarity.\n\n1. **Technical Novelty and Significance**: The reviewers have mixed opinions on the technical novelty and significance of the paper. Reviewer 1 and Reviewer 3 consider the contributions to be only marginally significant or novel, while Reviewer 2 sees the algorithm as novel but notes that the numerical results are somewhat confusing. Reviewer 4 finds the manuscript to propose an interesting and potentially useful algorithm but believes it needs more thorough explanation of its theoretical properties. The consensus leans towards the contributions being somewhat new but not groundbreaking.\n\n2. **Empirical Novelty and Significance**: The empirical results presented in the paper are a crucial aspect of its evaluation. However, the reviewers express concerns regarding the experimental demonstrations. Reviewer 3 notes that the baseline and proposed models seem quite weak, with low accuracy on image datasets, which makes it unclear how the model could generalize to realistic datasets. Reviewer 4 emphasizes the need for more detailed and extensive experimentation to explain why Target Propagation (TP) outperforms Gradient Descent (GD) in certain cases and to evaluate the robustness of these improvements. The empirical novelty and significance are considered marginal by most reviewers.\n\n3. **Correctness**: The reviewers generally find the paper's claims to be mostly correct, with some minor issues or statements that are not well-supported. Reviewer 1 notes that some claims have minor issues, and Reviewer 3 states that a few statements require small changes to be made correct. Reviewer 4 also mentions that the paper's claims have minor issues but are largely correct. The correctness of the paper is not a major concern but could be improved with revisions.\n\n4. **Clarity**: The clarity of the paper is praised by Reviewer 2, who finds it clear and structured. However, specific comments, such as a typo mentioned by Reviewer 2, indicate that while the paper is generally well-written, there is room for improvement in terms of precision and detail, particularly in explaining complex concepts and theoretical analyses.\n\n5. **Overall Assessment**: Considering the comments from all reviewers, the paper presents an interesting approach to target propagation via regularized inversion. However, it faces challenges in terms of demonstrating significant technical and empirical novelty, providing convincing experimental results, and fully exploring the theoretical underpinnings of the proposed algorithm. The reviewers suggest several areas for improvement, including more thorough theoretical analysis, additional experiments to robustly demonstrate the advantages of the proposed method, and clearer explanations of how the method differs from and improves upon existing techniques like backpropagation and Gauss-Newton optimization.\n\nGiven these considerations, while the paper shows promise and contributes to the ongoing discussion on alternative optimization methods for neural networks, its current form does not fully meet the standards expected for publication at a top-tier conference due to the concerns raised about its novelty, empirical evidence, and theoretical foundations.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical analysis of the Feedback Alignment (FA) algorithm, an alternative to backpropagation for training neural networks. The authors provide convergence guarantees with rates for deep linear networks for both continuous and discrete dynamics and study incremental learning phenomena for shallow linear networks. They also identify an implicit anti-regularization phenomenon for certain initializations and propose initialization schemes that provide a form of implicit regularization.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers 1 and 2 express concerns about the assumptions made in the paper, such as the requirement for diagonalizable matrices and the focus on linear networks, which may not be representative of more complex, real-world scenarios. Reviewer 1 questions the significance of the results, given the simplifications made, and whether they can be generalized to more complex networks. Reviewer 2 notes that the paper does not explore the characteristic of feedback alignment that the forward weights get aligned with the random backward weights and that the results are primarily focused on linear networks, leaving a gap between the theory and empirical successes on networks with non-linear activation functions.\n\nOn the other hand, Reviewer 3 finds the paper to be a nice theoretical development, extending the theory of linear neural networks to the case of FA, and recommends acceptance. Reviewer 4 praises the clear writing and detailed analysis but expresses concerns about the significance of the results due to the restrictive assumption of spectral initialization and the lack of empirical evidence for non-spectral initialization.\n\nConsidering the comments, the paper's strengths include its clear writing, detailed analysis, and interesting observations about implicit regularization and anti-regularization phenomena. However, the weaknesses, such as the restrictive assumptions (e.g., diagonalizable matrices, spectral initialization), the focus on linear networks, and the limited empirical evidence, particularly for non-spectral initialization, are significant.\n\nGiven the mixed reviews, with some reviewers finding the paper's contributions significant and others questioning their novelty and applicability, the decision hinges on whether the paper's theoretical contributions and insights into FA are substantial enough to warrant publication in a top-tier conference, despite its limitations.\n\nThe paper's technical correctness is not in question, as all reviewers agree that the claims and statements are well-supported and correct. The debate centers around the technical novelty, significance, and empirical novelty of the contributions. While some reviewers see the paper as making significant and new contributions, others view the results as marginally significant or novel due to the restrictive assumptions and the focus on a specific, simplified scenario.\n\nUltimately, the decision to accept or reject the paper must consider the standards of a top-tier conference, which typically look for work that is not only technically sound but also significantly advances the field, offers substantial novelty, and demonstrates a clear impact or potential for impact on the broader research community.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" proposes a novel architecture for synthetically generating time-series data using Variational Auto-Encoders (VAEs). The authors claim that their approach offers interpretability, the ability to encode domain knowledge, and reduced training times. The paper is well-written, and the idea of using VAEs for time-series generation is interesting. However, after carefully analyzing the reviewer comments, several concerns and limitations have been raised that impact the paper's overall quality and contribution to the field.\n\nOne of the primary concerns is the lack of substantial originality in the proposed method. Reviewer 2 notes that the architecture seems to be a standard VAE applied to time-series data, and it is unclear how this approach better captures the temporal aspects of the data. Reviewer 3 also questions the technical contribution, stating that the paper lacks general context of the literature in several areas, including state-space models and deep generative models for sequential prediction tasks.\n\nAnother significant issue is the limited empirical evaluation. Reviewer 1 points out that the hyper-parameters of the core building blocks are not evaluated properly, and there are no ablation studies to support the claims made about the model's interpretability. Reviewer 3 also criticizes the experimental methodology, suggesting that the experiments should include probabilistic autoregressive models and be conducted on well-known datasets to allow for comparison with other state-of-the-art methods.\n\nFurthermore, the paper's claims about interpretability and the ability to encode domain knowledge are not adequately supported. Reviewer 1 asks for evidence on how interpretable the newly introduced building blocks are, and Reviewer 4 notes that there is no discussion about interpretability in the results section. This omission is significant, given that interpretability is claimed as one of the key contributions of the method.\n\nThe performance of the proposed model, TimeVAE, is also questioned by several reviewers. Reviewer 1 finds the results in Table 1 counter-intuitive, and Reviewer 4 states that there is no obvious better performance for the proposed model besides the dataset \"sine.\" This raises concerns about the model's effectiveness and its ability to generalize across different datasets.\n\nConsidering these concerns and limitations, it is clear that the paper requires significant revisions to address the issues raised by the reviewers. The lack of originality, limited empirical evaluation, and unsupported claims about interpretability and performance are major drawbacks that prevent the paper from being accepted in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning Sample Reweighting for Adversarial Robustness\" proposes a novel approach to improve the robustness of neural network classifiers against adversarial attacks. The authors introduce a bi-level optimization framework, BiLAW, which learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin. The approach is inspired by MAML-based methods and formulates weighted adversarial training as a bilevel optimization problem.\n\nThe paper has received a mix of positive and negative reviews from the reviewers. Some reviewers (e.g., Reviewer 4, Reviewer 7, Reviewer 10) appreciate the paper's novelty, sound methodology, and comprehensive experiments, which demonstrate improved clean and robust accuracy compared to related techniques and state-of-the-art baselines. They suggest that the paper's contributions, although not highly novel, are significant and somewhat new, and that the paper is well-written and easy to follow.\n\nOn the other hand, several reviewers (e.g., Reviewer 1, Reviewer 3, Reviewer 6) raise concerns about the paper's technical novelty, empirical novelty, and significance. They argue that the proposed approach is a direct adaptation of the classical MAML algorithm to adversarial training, which is of limited technical novelty. They also question the paper's evaluation, suggesting that the authors should have included adaptive attacks in the evaluation stage and provided more ablation studies to demonstrate the effectiveness of the proposed approach.\n\nAdditionally, some reviewers (e.g., Reviewer 2, Reviewer 5, Reviewer 8) point out minor issues with the paper, such as inconsistencies in notation, missing punctuations, and typos. However, these issues do not seem to affect the overall quality of the paper.\n\nAfter carefully analyzing the reviews, I believe that the paper's strengths outweigh its weaknesses. The paper's methodology is sound, and the experiments are comprehensive and well-designed. Although the technical novelty of the paper may be limited, the empirical results demonstrate significant improvements over state-of-the-art baselines. The paper is also well-written and easy to follow, making it accessible to a broad audience.\n\nHowever, I also agree with the reviewers that the paper could benefit from additional ablation studies and evaluations, particularly with respect to adaptive attacks. The authors should consider addressing these concerns in a revised version of the paper.\n\nIn conclusion, based on the reviews and the paper's overall quality, I believe that the paper should be accepted for publication, provided that the authors address the minor issues and concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Natural Language Descriptions of Deep Visual Features\" presents a novel approach to interpreting deep learning models for computer vision by generating natural language descriptions of neurons. The method, called MILAN, uses mutual information to guide the linguistic annotation of neurons, allowing for open-ended and compositional descriptions.\n\nThe reviewers' comments are overall positive, highlighting the paper's strengths in terms of technical soundness, novelty, and significance. Reviewer 2 notes that the paper is well-written and easy to follow, with sufficient technical details for reproducibility. Reviewer 3 appreciates the amount of effort put into testing the system and finds the results promising. Reviewer 4, a domain expert in NLP, advocates for acceptance, highlighting the paper's novelty in interpreting model behavior and its potential for downstream applications.\n\nHowever, the reviewers also raise several concerns and suggestions for improvement. Reviewer 2 questions the inter-annotator agreement among human annotations and suggests scaling up the method using existing large multimodal datasets. Reviewer 3 requests a better characterization of the limits of the approach and more experimentation with the technique. Reviewer 4 raises concerns about the scoping of the work, suggesting that the title and description may be too general, and requests more experiments to demonstrate the method's portability to other tasks and datasets.\n\nThe authors' responses to these concerns are not provided, but based on the reviewers' comments, it appears that the paper has the potential to make a significant contribution to the field of computer vision and interpretability. The method presented is novel, and the results are promising, with potential applications in understanding and controlling learned models.\n\nAfter careful analysis, I conclude that the paper should be accepted for publication, pending revisions to address the reviewers' concerns. The authors should provide more details on the inter-annotator agreement, scale up the method using existing large multimodal datasets, and demonstrate the method's portability to other tasks and datasets. Additionally, the authors should consider revising the title and description to better reflect the scope of the work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper proposes a novel method, Differentiable Diffusion Sampler Search (DDSS), to optimize fast samplers for pre-trained diffusion models by differentiating through sample quality scores. The method introduces a family of non-Markovian samplers, Generalized Gaussian Diffusion Models (GGDM), which can be optimized using a perceptual loss function. The paper presents impressive results on unconditional image generation across various datasets, achieving strong FID scores with a reduced number of inference steps.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers 1 and 2 appreciate the paper's contributions to the field of diffusion-based generative models, but raise concerns about the lack of theoretical guarantees, the optimality of the proposed method, and the potential for overfitting to evaluation metrics. Reviewer 3 finds the paper interesting and the results compelling, but notes that the proposed method's benefits are mainly due to the optimization procedure, and the GGDP sampler may not stand on its own as a better sampler. Reviewer 4 questions the theoretical soundness of the proposed approach, the generality of the results, and the novelty of the method. Reviewer 5 praises the paper's simplicity and empirical performance but criticizes the clarity and suggests improvements.\n\nAfter carefully considering the reviewer comments, it is clear that the paper has both merits and demerits. The proposed method achieves impressive results, and the optimization procedure is a significant contribution to the field. However, the lack of theoretical guarantees, the potential for overfitting, and the limited evaluation on larger datasets are concerns that need to be addressed.\n\nThe authors have provided a rebuttal that addresses some of the concerns raised by the reviewers. They have clarified the motivation behind the proposed method, provided additional results, and acknowledged the limitations of their approach.\n\nIn conclusion, while the paper has its weaknesses, the strengths of the paper, including its novelty, empirical performance, and potential impact on the field, outweigh its limitations. The authors have demonstrated a good understanding of the concerns raised by the reviewers and have provided a satisfactory rebuttal.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Meta-Referential Games to Learn Compositional Learning Behaviours\" proposes a novel benchmark for investigating state-of-the-art artificial agents' abilities to exhibit compositional learning behaviors. The authors introduce a Symbolic Continuous Stimulus (SCS) representation and cast the problem of learning compositional behaviors as a meta-reinforcement learning problem. However, after carefully analyzing the paper and the reviewer comments, several concerns and issues arise that impact the paper's overall quality and contribution to the field.\n\nFirstly, the paper is difficult to follow, with reviewers commenting on the lack of clarity in the introduction, the complexity of the sentences, and the absence of concrete examples to illustrate key concepts. This lack of clarity hinders the understanding of the proposed method and its significance, making it challenging for readers to grasp the main contributions of the paper.\n\nSecondly, the proposed SCS representation, while interesting, lacks a clear justification for its use over other representations, such as one-hot encodings (OHE) or continuous representations. Reviewers have pointed out that the arguments against OHE are not convincing, and the benefits of SCS are not clearly demonstrated. Furthermore, the limitations of SCS, such as its inability to adapt to more latent dimensions or extrapolate to dimensions with larger sizes, are not adequately addressed.\n\nThirdly, the experimental evaluation of the proposed method is limited, with reviewers criticizing the lack of ablation studies, the poor tuning of hyperparameters, and the failure to demonstrate the value of SCS on existing compositional generalization challenges. The results presented are barely above chance performance, which does not provide a compelling endorsement of the SCS representation.\n\nFourthly, the paper lacks engagement with the broader literature on compositional generalization, with reviewers pointing out the overlap with existing research and the need to situate the work within this literature. The authors' failure to address these concerns and provide a clear contribution to the field raises questions about the paper's technical novelty and significance.\n\nLastly, the reviewers have identified several minor issues with the paper's claims, requiring small changes to be made correct. However, the main concerns lie in the paper's overall clarity, the justification of the proposed method, and the limited experimental evaluation.\n\nConsidering these concerns and issues, it is clear that the paper requires significant revisions to address the reviewers' comments and improve its overall quality. While the idea of compositional learning through meta-referential games is intriguing, the paper's current state does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents a theoretical analysis of the convergence properties of three classical value estimation algorithms (TD, FVI, and RM) in the overparameterized linear case. The authors provide a unified interpretation of these algorithms as minimizing the Euclidean norm of the weights subject to alternative constraints. They also propose two regularizers to improve the stability of TD and FVI and demonstrate their effectiveness in experiments.\n\nThe reviewers' comments and scores indicate that the paper has both strengths and weaknesses. The strengths include:\n\n1. Novel perspective on the convergence properties of TD, FVI, and RM in the overparameterized linear case.\n2. Unified interpretation of these algorithms as minimizing the Euclidean norm of the weights subject to alternative constraints.\n3. Proposal of two regularizers to improve the stability of TD and FVI.\n\nThe weaknesses include:\n\n1. Limited impact of the results due to the restriction to linear representations.\n2. Lack of clear comparison between overparameterized and underparameterized cases.\n3. Some reviewers questioned the correctness and novelty of the technical contributions.\n4. Experimental results are not very persuasive, and the domains used are relatively simple.\n\nAfter carefully analyzing the reviewers' comments and scores, I conclude that the paper has some merits, but the weaknesses and limitations outweigh the strengths. The paper's contribution is not solid enough, and some results might be incorrect or not well-supported. The technical novelty and significance are only marginally significant or novel.\n\nHowever, I also consider the fact that some reviewers found the paper exciting and believed that the authors would address their concerns. Additionally, the paper's topic is relevant and important, and the authors' efforts to provide a unified interpretation of TD, FVI, and RM are commendable.\n\nGiven the mixed reviews and the fact that the paper's strengths are not sufficient to outweigh its weaknesses, I would recommend that the paper be revised and resubmitted to address the reviewers' concerns. However, based on the current version of the paper, I must make a decision.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper under review presents a theoretical analysis of epoch-wise double descent in a linear teacher-student setting, providing insights into the dynamics of feature learning and generalization error in neural networks. The reviewers' comments offer a range of perspectives on the paper's strengths, weaknesses, and potential contributions to the field.\n\nOne of the primary strengths of the paper is its ability to provide a simple, analytically tractable model that exhibits epoch-wise double descent, a phenomenon previously observed in deep neural networks. Reviewer 1 praises the paper's clarity, motivation, and presentation, highlighting its value in demonstrating that another exotic behavior of deep neural networks is already present in simple, linear models. Reviewer 2 notes that the work is solid, with theoretical analysis and empirical evaluation, although suggesting connections to other pioneering works in the field could enhance its significance.\n\nHowever, several reviewers raise concerns about the paper's novelty and significance. Reviewer 3 questions whether the intuition involved in the paper (introducing multiple scales into the problem) has appeared before, suggesting a need for more discussion on how this work relates to existing literature. Reviewer 2 points out the implicit regularization effects of SGD/GD on least square linear regression and suggests discussing connections to other works on Ridge-style implicit regularization. These comments indicate that while the paper makes a contribution, its novelty and impact might be somewhat diminished by existing research in the area.\n\nAdditionally, Reviewer 3 identifies numerous typos and minor errors in the manuscript, which, although not major, divert the flow of the calculations and make it challenging to assess the correctness of the final results. This highlights the need for careful proofreading and attention to detail to ensure the manuscript's quality.\n\nThe paper's technical novelty and significance are rated differently by the reviewers. Reviewer 1 sees the work as valuable with significant contributions, albeit with some aspects existing in prior work. Reviewer 2 and Reviewer 3 are more conservative in their assessments, with Reviewer 3 stating that the contributions are only marginally significant or novel.\n\nConsidering the comments and ratings provided by the reviewers, the paper demonstrates a good understanding of the topic and provides a clear, well-motivated analysis of epoch-wise double descent in a linear setting. However, the concerns about novelty, the need for clearer connections to existing literature, and the presence of minor errors suggest that the paper could benefit from revisions to strengthen its contribution and clarity.\n\nGiven the overall positive assessment of the paper's quality, its potential to contribute to the understanding of neural network dynamics, and considering the suggestions for improvement, the decision leans towards accepting the paper, provided that the authors address the raised concerns and improve the manuscript accordingly.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review presents a novel approach to improving the accuracy and robustness of Recurrent Neural Networks (RNNs) in hierarchical control of continually learned autonomous motor motifs. The authors draw inspiration from neuroscience, specifically the thalamocortical circuit, to develop a method that constrains RNNs to function similarly during motif transitions, thereby enhancing their performance in zero-shot transfer to new motif chains.\n\nThe reviewer comments provide a comprehensive overview of the paper's strengths and weaknesses. Reviewer 1 remains unconvinced despite the authors' efforts to address their concerns. Reviewer 2 questions the necessity of the proposed resetting mechanism, suggesting alternative approaches that could achieve similar results. Reviewer 3 expresses low enthusiasm for the work, citing the small scale of the models and data, as well as the lack of evaluation of alternative approaches. Reviewer 4 finds the modification to the control network simple and effective but has low confidence in the novelty and significance of the analytical work. Reviewer 5 acknowledges the relevance of the topic but finds the presentation confusing and difficult to understand, raising concerns about the clarity of the task description and the existence of a trivial solution.\n\nAfter carefully analyzing the reviewer comments, several key points emerge:\n\n1. **Technical Novelty and Significance**: The majority of reviewers (Reviewers 2, 3, and 5) rate the technical novelty and significance of the contributions as marginal or not well-supported. This suggests that the paper may not introduce substantially new or impactful ideas to the field.\n\n2. **Empirical Novelty and Significance**: Similarly, the empirical contributions are viewed as marginal by most reviewers, indicating that the experiments, while potentially interesting, may not significantly advance the state of the art in a way that is broadly applicable or impactful.\n\n3. **Correctness and Clarity**: There are concerns about the correctness of some claims and the clarity of the presentation. Reviewer 5, in particular, highlights issues with the writing style and the organization of the paper, which hampers the communication of the scientific ideas.\n\n4. **Alternative Approaches**: Several reviewers suggest that the authors have not adequately considered or tested alternative approaches that could achieve similar or better results without the need for the proposed thalamocortical-inspired mechanism. This raises questions about the necessity and uniqueness of the proposed solution.\n\n5. **Scale and Relevance**: The small scale of the models and the synthetic nature of the task, as pointed out by Reviewer 3, diminish the potential contribution of this work to the broader field of machine learning and its applicability to more complex, real-world problems.\n\nConsidering these points, while the paper explores an interesting intersection of neuroscience and machine learning, the concerns raised by the reviewers regarding novelty, significance, clarity, and the adequacy of the approach outweigh the potential benefits of the work as presented. Therefore, based on the information provided and the standards expected of a top-tier conference, the paper does not meet the criteria for acceptance in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes\" presents a novel approach to neural network compression using tropical geometry. The authors provide a theoretical framework for understanding the approximation error between two neural networks with ReLU activations and one hidden layer, and propose a geometrical method for compressing fully connected parts of deep neural networks.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Mathematical solidity**: Reviewer 1 praises the paper's mathematical analysis, stating that it is \"mathematically solid\" and that the presented algorithm seems to outperform a recent tropical baseline.\n2. **Novelty**: Reviewers 1, 2, 3, and 4 agree that the paper presents a novel approach to neural network compression, with Reviewer 4 stating that it is a \"very interesting and new idea\".\n3. **Theoretical contribution**: Reviewers 1 and 3 appreciate the paper's theoretical contribution, with Reviewer 1 stating that the tropical geometry section is \"intriguing\" and may be of interest to theorists.\n\nHowever, the reviewers also point out several weaknesses:\n\n1. **Limited empirical evaluation**: Reviewers 1, 2, and 3 criticize the paper's empirical evaluation, stating that it is limited to small-scale datasets and architectures, and that the results are not convincing.\n2. **Lack of comparison to state-of-the-art methods**: Reviewers 1 and 3 point out that the paper does not compare its results to state-of-the-art pruning methods, making it difficult to assess its performance.\n3. **Unclear practical applicability**: Reviewer 1 questions the practical applicability of the proposed method, stating that it is unclear whether it can be used to compress larger networks.\n4. **Minor issues with claims and supporting evidence**: All reviewers agree that the paper's claims have minor issues, with some statements not being well-supported or requiring small changes to be made correct.\n\nAfter carefully considering the reviewers' comments, I conclude that the paper has some significant strengths, including its mathematical solidity, novelty, and theoretical contribution. However, the limited empirical evaluation, lack of comparison to state-of-the-art methods, and unclear practical applicability are significant weaknesses that need to be addressed.\n\nGiven the current state of the paper, I believe that it requires significant revisions to address the reviewers' concerns before it can be considered for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "To determine whether the paper \"Sample and Computation Redistribution for Efficient Face Detection\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper based on its technical novelty and significance, empirical novelty and significance, correctness, and the overall quality of the research.\n\n1. **Technical Novelty and Significance**: The paper proposes two methods, Computation Redistribution (CR) and Sample Redistribution (SR), aimed at improving the efficiency and accuracy of face detection, particularly for small faces in low-resolution images. While the methods are straightforward, as noted by several reviewers, they demonstrate significant improvements over existing state-of-the-art methods. However, the novelty of the approach is somewhat debated among the reviewers, with some (Reviewers 4 and 7) suggesting that the contributions are only marginally significant or novel, and others (Reviewers 5 and 6) finding the contributions significant and somewhat new.\n\n2. **Empirical Novelty and Significance**: The empirical results presented in the paper are strong, showing that the proposed SCRFD method outperforms other state-of-the-art face detection algorithms, such as TinaFace, on the WIDER FACE dataset. This improvement is not only in terms of accuracy but also in computational efficiency, which is a significant contribution given the constraints of many real-world applications. Reviewers generally agree that the empirical contributions are significant, although some suggest that comparisons with other network search methods are needed for a more comprehensive evaluation.\n\n3. **Correctness**: Most reviewers found the paper's claims to be well-supported, although a few minor issues were noted. Reviewer 4 mentioned that some statements require small changes to be correct, and Reviewer 7 pointed out the lack of detail in certain aspects, such as the sample distribution and the need for an algorithm to describe the computation redistribution method. However, these issues do not seem to undermine the overall validity of the research.\n\n4. **Quality of Research and Writing**: The paper is well-written, and the experiments are extensively conducted on relevant datasets. The authors have also made their code available, which is a positive aspect for reproducibility. However, some reviewers suggested improvements, such as adding more figures to illustrate the framework, providing more details on certain methods, and conducting more comparisons with state-of-the-art network search methods.\n\nConsidering these aspects, the paper demonstrates significant empirical contributions, particularly in terms of improving the accuracy and efficiency of face detection. While the technical novelty might be debated, the practical impact and the thoroughness of the experimental evaluation are notable strengths. The minor issues regarding correctness and the need for additional comparisons or details do not outweigh the paper's strengths, especially given the authors' responsiveness to reviewer comments and the availability of their code for further research and verification.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the reviewer comments, the paper's contributions, its technical and empirical novelty, and its overall significance.\n\n1. **Technical Novelty and Significance**: Reviewers 1, 2, 3, and 4 express concerns that the paper's technical novelty is limited. They mention that the platform, while useful and well-engineered, implements existing techniques with only minor contributions, such as the Believer-Penalizer (BP) strategy. Reviewer 5 is more positive, noting the significance of the problem studied and the convenience of the framework but also acknowledges that aspects of the contributions exist in prior work.\n\n2. **Empirical Novelty and Significance**: The empirical results presented in the paper are a point of contention. Reviewers 1 and 4 note that the performance of AutoOED and its BP strategy is not consistently better than other methods across all benchmarks. Reviewer 4 specifically mentions that the claims of outperforming other methods are overstated based on the results shown. Reviewer 5 finds the claims well-supported by the multi-objective tasks on public datasets but does not delve into the specifics of the performance comparisons.\n\n3. **Correctness and Clarity**: Most reviewers agree that the paper is well-written and clear, with minor issues regarding claims, supporting evidence, and some inconsistencies or lack of clarity in certain explanations (e.g., the combination of KB and LP in the BP strategy). Reviewer 4 also points out the need for better motivation of the threshold used in the BP strategy and questions the comparison with other packages.\n\n4. **Engineering Contribution**: The paper is praised for its engineering contribution, particularly in making multi-objective Bayesian optimization accessible through a user-friendly interface. Reviewers acknowledge the value of AutoOED as a tool for both experts and non-experts, although some question its novelty and significance in the context of existing tools and methodologies.\n\n5. **Comparison with Existing Work**: Several reviewers note that the paper could benefit from a more comprehensive comparison with existing Bayesian optimization platforms and a clearer discussion of how AutoOED advances the state-of-the-art, especially considering its limitations (e.g., scalability to more than 3 objectives).\n\nGiven these considerations, the decision to accept or reject the paper hinges on the conference's priorities regarding novelty, significance, and the balance between technical contributions and engineering efforts. Top-tier conferences typically prioritize papers with significant technical novelty, strong empirical evidence, and clear advancements over the state-of-the-art.\n\nWhile AutoOED represents a valuable engineering effort and contributes to making multi-objective optimization more accessible, the concerns raised by the reviewers regarding its technical novelty, the consistency of its performance advantages, and the clarity of its contributions over existing work are significant.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Variational Neural Cellular Automata\" proposes a novel generative model that combines the concepts of neural cellular automata and variational autoencoders. The model is designed to learn a probabilistic generative process of data, and the authors demonstrate its effectiveness on several datasets, including MNIST, Noto Emoji, and CelebA.\n\nThe reviewers' comments and scores are mixed, with some reviewers praising the paper's novelty, clarity, and empirical results, while others express concerns about the model's performance, robustness, and comparison to existing methods.\n\nReviewer 1 and Reviewer 4 are leaning towards rejection, citing underwhelming experimental results and a lack of significant improvement over existing methods. Reviewer 2 and Reviewer 6 question the novelty of the approach, suggesting that the model can be viewed as a deep convolutional network with repeating blocks, which reduces its novelty.\n\nOn the other hand, Reviewer 3, Reviewer 5, and Reviewer 7 are more positive, highlighting the paper's clarity, empirical results, and potential contributions to the field. Reviewer 5, in particular, appreciates the paper's well-written presentation and the authors' efforts to address the reviewers' comments.\n\nAfter carefully analyzing the reviewers' comments and the paper's content, I believe that the paper has some merits, but also some limitations. The authors have made a genuine effort to propose a novel model and demonstrate its effectiveness on several datasets. However, the model's performance is not consistently better than existing methods, and some reviewers have raised concerns about its robustness and comparison to other approaches.\n\nThe paper's strengths include its clarity, empirical results, and potential contributions to the field of neural cellular automata. The authors have also made a good effort to address the reviewers' comments and improve the paper.\n\nThe paper's weaknesses include its limited performance on some datasets, the lack of significant improvement over existing methods, and some concerns about the model's robustness and comparison to other approaches.\n\nConsidering the mixed reviews and the paper's strengths and weaknesses, I believe that the paper is a borderline case. However, after careful consideration, I think that the paper's merits outweigh its limitations, and it has the potential to make a contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Behavior Proximal Policy Optimization\" presents a novel approach to offline reinforcement learning (RL) by leveraging the inherent conservatism of online on-policy algorithms. The authors propose an algorithm called Behavior Proximal Policy Optimization (BPPO), which solves offline RL without introducing extra constraints or regularization. The paper provides theoretical analysis for monotonic policy improvement and demonstrates the effectiveness of BPPO through extensive experiments on the D4RL benchmark.\n\nThe reviewers have raised several concerns and questions about the paper, including:\n\n1. **Novelty and significance**: Reviewers 2 and 5 have pointed out that the theoretical results are not entirely new and have similarities with prior works, such as GePPO. Reviewer 4 has also questioned the significance of the results, citing minor improvements over existing baselines.\n2. **Assumptions and approximations**: Reviewers 1, 4, and 5 have raised concerns about the assumptions made in the paper, such as Assumption 1, and the approximations used in the algorithm, including the replacement of $\\rho_\\pi$ with state data distribution.\n3. **Experimental evaluation**: Reviewers 1, 2, and 4 have requested additional experiments to demonstrate the effectiveness of BPPO in different settings, such as sparse reward environments, and to compare it with other state-of-the-art algorithms.\n4. **Soundness and convergence**: Reviewers 1 and 5 have questioned the soundness of the algorithm and its convergence to the optimal policy.\n\nDespite these concerns, the paper has several strengths, including:\n\n1. **Theoretical analysis**: The paper provides a thorough theoretical analysis of monotonic policy improvement, which is a significant contribution to the field.\n2. **Empirical results**: The experiments on the D4RL benchmark demonstrate the effectiveness of BPPO, and the results are comparable to or better than those of state-of-the-art algorithms.\n3. **Simplicity and ease of implementation**: The algorithm is simple to implement, and the authors have provided a clear and well-written presentation of the method.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper has made a significant contribution to the field of offline RL, despite some minor issues and concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel framework for online knowledge distillation using a specialist ensemble, which aims to improve the diversity of teachers and enhance student performance. The authors introduce a label prior shift to induce diversity among teachers and propose a new aggregation strategy using post-compensation in specialist outputs and conventional model averaging.\n\nThe reviewers have provided mixed feedback on the paper. Reviewer 1 finds the paper mostly clear but raises several concerns, including the lack of explanation for the choice of baselines, the need for more ablations, and the potential for applying the technique to non-online distillation. Reviewer 2 questions the reproducibility of the results and finds the novelty of the paper not sufficient. Reviewer 3 praises the paper's writing and empirical analysis but notes that the paper has some typos and that the technical novelty is only marginal. Reviewer 4 suggests that the paper lacks comparison with more current works and that the performance improvement is not significant. Reviewer 5 raises concerns about the experimental results, the novelty of the paper, and the correctness of the main claims.\n\nAfter carefully analyzing the reviews, several concerns arise:\n\n1. **Lack of novelty**: Multiple reviewers (1, 2, 3, 4, and 5) have questioned the novelty of the paper, suggesting that the contributions are only marginally significant or that the paper combines existing methods without clear explanations.\n2. **Experimental results**: Reviewers 2, 4, and 5 have raised concerns about the experimental results, including the lack of comparison with more current works, the reproducibility of the results, and the significance of the performance improvement.\n3. **Technical correctness**: Reviewer 5 has questioned the correctness of the main claims, and Reviewers 1 and 4 have noted that some statements are not well-supported or require minor changes to be correct.\n4. **Comparison with state-of-the-art**: The paper lacks comparison with more recent and relevant works, which makes it difficult to assess the significance of the contributions.\n\nConsidering these concerns, it is clear that the paper has some strengths, such as its clear writing and thorough empirical analysis. However, the lack of novelty, concerns about experimental results, and technical correctness issues outweigh the strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" proposes a novel framework for synthesizing data using deep generative models in a differentially private manner. The reviewers have provided detailed comments on the paper, highlighting both its strengths and weaknesses.\n\nThe main strengths of the paper, as pointed out by the reviewers, include:\n\n1. **Novel approach**: The paper proposes a new and well-motivated approach to privacy-preserving generative modeling, which is a significant contribution to the field.\n2. **Theoretical guarantees**: The paper provides theoretical guarantees of performance, which is a strong aspect of the work.\n3. **Empirical evaluations**: The paper presents empirical evaluations on multiple datasets, which demonstrate the effectiveness of the proposed approach.\n\nHowever, the reviewers have also identified several weaknesses and areas for improvement:\n\n1. **Lack of thorough benchmarking**: The paper lacks thorough benchmarking, particularly for tabular data and image data. The reviewers suggest that the authors should compare their approach with other methods, such as PATE-GAN and CTGAN, on more datasets.\n2. **Unclear privacy-preserving optimization method**: The reviewers are unclear about how the authors implement privacy-preserving optimization in the paper, particularly with regards to one-shot sampling.\n3. **Need for more related work**: The reviewers suggest that the authors should include more related work, particularly papers that are highly relevant to the topic.\n4. **Lack of complexity analysis**: The paper lacks a complexity analysis of the proposed algorithm, which is an important aspect of evaluating the practicality of the approach.\n5. **Editorial quality**: The paper's editorial quality is not always satisfactory, with inconsistent and non-precise descriptions.\n\nAfter carefully considering the reviewers' comments, I believe that the paper has the potential to be a significant contribution to the field of privacy-preserving generative modeling. However, the authors need to address the weaknesses and areas for improvement identified by the reviewers.\n\nTo improve the paper, the authors should:\n\n1. **Conduct more thorough benchmarking**: The authors should compare their approach with other methods on more datasets, particularly for tabular data and image data.\n2. **Clarify the privacy-preserving optimization method**: The authors should provide more details on how they implement privacy-preserving optimization, particularly with regards to one-shot sampling.\n3. **Include more related work**: The authors should include more related work, particularly papers that are highly relevant to the topic.\n4. **Provide a complexity analysis**: The authors should provide a complexity analysis of the proposed algorithm to evaluate its practicality.\n5. **Improve the editorial quality**: The authors should improve the editorial quality of the paper, ensuring that the descriptions are consistent and precise.\n\nGiven the current state of the paper, I believe that it is not ready for acceptance. However, with revisions to address the weaknesses and areas for improvement, the paper has the potential to be a significant contribution to the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration\" proposes a new contrastive token learning objective to address the issue of text degeneration in autoregressive language models. The paper is well-written, easy to follow, and presents a clear motivation for the proposed approach. However, after carefully analyzing the reviewer comments, several concerns arise that impact the paper's overall quality, novelty, and significance.\n\nFirstly, the novelty of the paper is questioned by multiple reviewers. Reviewer 1 states that the core idea of penalizing previously generated tokens has been proposed before, and the authors' claim that unlikelihood training only considers the probabilities of negative tokens is disputed. Reviewer 3 also mentions that the idea is very similar to \"unlikelihood training.\" This lack of novelty is a significant concern, as top-tier conferences typically expect innovative and groundbreaking research.\n\nSecondly, the experimental setup and results are criticized by several reviewers. Reviewer 1 suggests that the experiments are insufficient and that the metrics used (e.g., perplexity) are not reliable for evaluating text generation tasks. Reviewer 2 points out that the experimental setup is weak, using only GPT2-small, and that the metrics reported (e.g., dist-1) are not convincing. Reviewer 4 also questions the importance of the repetition issue for better or larger models and suggests that scaling experiments could help resolve this question.\n\nThirdly, the human evaluation results are not statistically significant, and some claims made by the authors are not supported by the human evaluation. Reviewer 4 mentions that the human evaluation does not show a preference for the proposed approach over one of the baselines (UL-TS).\n\nLastly, while the paper is well-written and easy to follow, the reproducibility is good, and the code is provided, the technical and empirical novelty and significance are only marginally significant or novel, according to multiple reviewers.\n\nConsidering these concerns, it is clear that the paper has some merits, such as a well-motivated approach and interesting ideas. However, the lack of novelty, insufficient experimental setup, and non-statistically significant human evaluation results are significant drawbacks.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" proposes a new adversarial training algorithm that applies more regularization to data vulnerable to adversarial attacks. The authors claim that their algorithm is theoretically well-motivated and empirically superior to other existing algorithms.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. **Theoretical contribution**: Many reviewers have questioned the novelty and significance of the theoretical contribution, particularly Theorem 1. Some reviewers have pointed out that the theorem is not new and is simply a rewriting of an existing equation. Others have argued that the theorem is meaningless and does not provide any new insights.\n2. **Looseness of the bound**: Several reviewers have noted that the bound in Theorem 1 is loose, particularly in the multi-classification case. This has led some reviewers to question the usefulness of the theorem and the method.\n3. **Experimental results**: Some reviewers have noted that the experimental results are not strong enough to support the claims made in the paper. The improvements over existing methods are marginal, and some reviewers have questioned the optimality of the baselines.\n4. **Clarity and rigor**: A few reviewers have pointed out that the paper lacks clarity and rigor, particularly in the presentation of the theoretical results.\n\nDespite these concerns, some reviewers have noted that the paper is well-written, and the authors have made an effort to address the concerns raised by the reviewers.\n\nAfter carefully considering the reviews and the authors' responses, I believe that the paper has some merits, but the concerns raised by the reviewers are significant enough to warrant rejection.\n\nThe lack of novelty and significance in the theoretical contribution, the looseness of the bound, and the marginal experimental results are major concerns that cannot be ignored. While the authors have made an effort to address some of the concerns, the paper still lacks the clarity and rigor expected of a top-tier conference.\n\nTherefore, my decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting observation about the interaction between batch normalization and bounded activation functions. The authors claim that swapping the order of batch normalization and activation functions can improve performance when using bounded activation functions. They also provide some analysis on the saturation and sparsity of the activation values.\n\nHowever, after carefully reviewing the paper and the reviewer comments, I have some concerns that need to be addressed.\n\nFirstly, the paper's claims are not well-supported by rigorous theoretical analysis. The authors rely heavily on empirical experiments, which, although interesting, do not provide a clear understanding of why the swapped order improves performance. Reviewer 1 and Reviewer 5 point out that the paper lacks clear explanations and rigorous proofs to support its claims.\n\nSecondly, the experimental setup and analysis have some limitations. Reviewer 2 and Reviewer 5 note that the paper only considers a limited set of architectures and activation functions, and the results may not generalize to other cases. Additionally, Reviewer 5 points out that the paper's comparison to ReLU-based networks is not fair, as the swapped order with bounded activation functions still performs worse than the conventional order with ReLU.\n\nThirdly, the paper's conclusions are not consistently supported by the experimental results. Reviewer 3 and Reviewer 5 note that the saturation metric seems incomplete, and the observations on saturation and sparsity do not hold uniformly across all layers. Reviewer 5 also points out that the paper's statement on the relationship between saturation and sparsity is contradictory.\n\nLastly, the paper's practical contributions are limited. Reviewer 2 and Reviewer 5 note that the swapped order with bounded activation functions still performs worse than the conventional order with ReLU, which is widely used in practice.\n\nConsidering these concerns, I believe that the paper requires significant revisions to address the limitations and improve the clarity and rigor of the analysis.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" proposes a novel framework that integrates Monte-Carlo Tree Search (MCTS) with the Transformer architecture for offline reinforcement learning. The idea of combining these two approaches is intriguing, and the paper demonstrates promising results on the SameGame domain.\n\nHowever, upon closer examination, several concerns arise that impact the paper's overall quality, novelty, and significance. \n\n1. **Lack of Clarity and Presentation**: Reviewer 1 highlights significant issues with the paper's clarity and presentation. The exposition of the method is unclear, and the writing requires multiple passes to understand. This lack of clarity hinders the reader's ability to fully grasp the contributions and methodology of the paper.\n\n2. **Limited Empirical Evaluation**: The empirical evaluation of the MCTransformer is limited to the SameGame domain, which is considered a relatively simple or \"toy-ish\" domain by Reviewer 1. The lack of comparison with other well-established methods, such as Decision Transformers or Online DTs, and the absence of experiments on more complex or benchmarked domains (like MiniGo or Atari), raises questions about the generalizability and robustness of the MCTransformer.\n\n3. **Novelty and Originality**: Reviewers 2 and 3 express concerns about the novelty of the paper, suggesting that the combination of MCTS with Transformers, although interesting, does not significantly depart from existing methods like AlphaGo/AlphaZero. The use of a Transformer instead of a ResNet in an MCTS setting is seen as a straightforward substitution rather than a groundbreaking innovation.\n\n4. **Fairness of Comparisons**: Reviewer 2 questions the fairness of the comparisons made in the paper, particularly the MCTS baseline, which does not utilize a learned prior policy or value function. This omission makes the comparison to MCTransformer somewhat unfair, as MCTransformer benefits from the integration of a Transformer.\n\n5. **Reproducibility and Code Availability**: While Reviewer 3 mentions that the paper provides enough information for reproducibility, Reviewer 2 notes the absence of source code, which is a significant concern for ensuring the reproducibility of the results.\n\nConsidering these points, the paper's limitations in terms of clarity, empirical evaluation, novelty, and fairness of comparisons outweigh its potential contributions. While the idea of combining MCTS with Transformers is promising, the execution and presentation of this idea in the paper fall short of the standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Regression with Label Differential Privacy\" presents a novel approach to training regression models with label differential privacy (DP) guarantees. The authors propose a label DP randomization mechanism that is optimal under a given regression loss function and demonstrate its efficacy through thorough experimental evaluations on several datasets.\n\nThe reviewer comments provide a comprehensive assessment of the paper's strengths and weaknesses. Reviewers praise the paper's clarity, novelty, and significance, as well as the empirical evaluations. However, some reviewers raise concerns about the comparison to other label-DP methods, the influence of the proposed mechanism on the Bayes optimal error, and the potential for a better label-DP mechanism.\n\nThe main strengths of the paper are:\n\n1. **Novelty and significance**: The paper proposes a new approach to regression with label DP, which is a significant contribution to the field.\n2. **Clarity and writing**: The paper is well-written, and the authors provide clear explanations of their methodology and results.\n3. **Empirical evaluations**: The authors conduct thorough experiments on several datasets, demonstrating the efficacy of their proposed mechanism.\n\nThe main weaknesses of the paper are:\n\n1. **Comparison to other methods**: Some reviewers suggest that the paper could benefit from a more comprehensive comparison to other label-DP methods, such as Ghazi et al. (2021).\n2. **Influence on Bayes optimal error**: Reviewers raise concerns about the potential influence of the proposed mechanism on the Bayes optimal error, which could be an important consideration in practice.\n3. **Technical novelty**: Some reviewers question the technical novelty of the paper, suggesting that the contributions are only marginally significant or novel.\n\nAfter carefully considering the reviewer comments and the paper's strengths and weaknesses, I conclude that the paper makes a significant contribution to the field of label differential privacy and regression. While there are some areas for improvement, the paper's novelty, clarity, and empirical evaluations outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel scheme, dubbed Proximal Validation Protocol (PVP), which aims to resolve the problem of validation set construction by assembling a proximal set as a substitution for the traditional validation set. The construction of the proximal validation set is established with dense data augmentation followed by a novel distributional-consistent sampling algorithm. The authors claim that PVP works better than existing validation protocols on three data modalities, demonstrating its feasibility towards ML production.\n\nHowever, upon analyzing the reviewer comments, several concerns and weaknesses are raised. Reviewer 1 points out that the paper lacks clarity, particularly in the experiments section, and that the empirical evaluation is not thorough. They also question the novelty of the proposed scheme and suggest that the authors should demonstrate the effectiveness of PVP in model selection and hyperparameter tuning. Reviewer 2 expresses concerns about the empirical evaluation, stating that the datasets used are too small and that the authors should provide more theoretical guarantees. Reviewer 3 agrees that the problem addressed is significant but points out that the experiments are limited and that more ablation studies are required to support the hypothesis. Reviewer 4 recommends rejecting the paper, stating that there is no reason to believe that the proximal set constructed is representative of the data generating distribution.\n\nConsidering the reviewer comments, the paper's weaknesses and limitations are significant. The lack of clarity, limited empirical evaluation, and lack of theoretical guarantees are major concerns. While the proposed scheme may be novel, the paper's contributions are not sufficiently supported by theory or empirical results.\n\nFurthermore, the paper's evaluation metrics and experimental design have been questioned by the reviewers. The use of small datasets, limited ablation studies, and lack of statistical significance results are notable weaknesses. The paper's claims about the effectiveness of PVP in improving model performance and reducing the test-validation gap are not convincingly supported.\n\nIn conclusion, based on the analysis of the reviewer comments and the paper's weaknesses, I believe that the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting observation about the interaction between batch normalization and bounded activation functions in neural networks. The authors claim that placing batch normalization after bounded activation functions (Swap model) achieves better performance than the conventional order (Convention model) on various benchmarks and architectures.\n\nThe reviewers have raised several concerns and questions about the paper, which can be summarized into the following categories:\n\n1. **Lack of clarity and rigor**: Several reviewers have pointed out that the paper's claims and arguments are not clearly explained or supported by evidence. For example, Reviewer 1 asks for more quantitative information to support the argument that asymmetric saturation is helpful for generalization. Reviewer 5 points out that some claims are not clearly clarified or rigorous, such as the statement about the center of the function at the origin.\n2. **Limited analysis and experiments**: Reviewers have noted that the analysis and experiments are limited to specific architectures and activation functions. For example, Reviewer 2 points out that the analysis is claimed to be valid only for bounded nonlinearity and without residual connection, excluding many widely used architectures. Reviewer 4 asks whether the swap is only useful for image classification and convolutional neural networks.\n3. **Contradictions and inconsistencies**: Reviewers have pointed out some contradictions and inconsistencies in the paper. For example, Reviewer 5 notes that the statement \"In short, higher saturation decreases sparsity\" in Section 5.2 seems to be contradictory to the observation that the Swap model has high saturation but higher sparsity.\n4. **Practical significance**: Reviewers have questioned the practical significance of the paper's findings. For example, Reviewer 3 points out that the performance of the Swap model with bounded activation functions cannot outperform the Convention model with ReLU activation functions.\n\nDespite these concerns, the paper has some strengths, such as:\n\n1. **Interesting observation**: The paper presents an interesting observation about the interaction between batch normalization and bounded activation functions.\n2. **New perspective**: The paper provides a new perspective on the correlation among saturation, sparsity, and performance.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper requires significant revisions to address the concerns and questions raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" proposes a novel method for predicting protein-protein interactions (PPIs) by leveraging pretrained structure embeddings and graph neural networks (GNNs). The authors claim that their approach leads to significant improvements in PPI prediction compared to sequence and network-based methods.\n\nHowever, after carefully analyzing the paper and the reviewer comments, several concerns and limitations have been identified. The main issues can be summarized as follows:\n\n1. **Limited novelty and contribution**: Reviewers 1, 2, and 3 all mention that the paper's technical contributions and novelty are limited. The authors adopt a pretrained structure prediction model (OmegaFold) and use its embeddings as input to a GNN, which is a straightforward and not particularly innovative approach.\n2. **Poor baseline choices**: Reviewer 4 points out that the baseline models used for comparison are weak and not representative of the state of the art in PPI prediction. This makes it difficult to evaluate the true performance of the proposed method.\n3. **Unclear model design and advantages**: Reviewers 1 and 4 question the design of the model and the advantages of using both structural embeddings and GNN aggregation. The results do not clearly support the benefits of this approach.\n4. **Lack of comparison to state-of-the-art methods**: Reviewer 4 notes that the paper fails to compare the proposed method to more recent and powerful approaches, such as AlphaFold-Multimer.\n5. **Methodological concerns and inconsistencies**: Reviewers 1 and 4 raise concerns about the negative sampling procedure, the use of mean pooling to obtain fixed-length protein representations, and inconsistencies in the results (e.g., Table 2 and 3).\n\nGiven these concerns and limitations, it is clear that the paper requires significant revisions and improvements to address the reviewers' comments and concerns.\n\nThe final decision is based on the overall evaluation of the paper's technical quality, novelty, and significance, as well as its potential to contribute to the field. While the paper's topic is important and well-motivated, the current version of the paper does not meet the standards of a top-tier conference due to its limited novelty, poor baseline choices, and methodological concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Functional Risk Minimization\" presents a novel framework for supervised learning problems, challenging the classic assumption of data coming from a single function followed by some noise in the output space. Instead, it models each data point as coming from its own function, providing an avenue for more realistic noise processes. The paper derives Functional Risk Minimization (FRM), a general framework for scalable training objectives, and shows its potential in small experiments in regression and reinforcement learning.\n\nThe reviewers have identified several strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novelty and potential impact**: The paper presents a new and interesting take on representing noise in a learning framework, which could be highly impactful in dealing with variation in real datasets.\n2. **Empirical results**: The paper demonstrates strong empirical results on linear regression with non-uniform noise, showing that FRM can outperform Empirical Risk Minimization (ERM) as the noise distribution changes.\n3. **Well-written article**: The paper is well-written, and the authors provide clear examples to motivate and explain the concepts.\n\nHowever, the reviewers have also identified several weaknesses:\n\n1. **Lack of clarity**: The paper's presentation is confusing, with non-standard use of terms like \"ERM\" and \"FRM\". The authors fail to clearly and explicitly lay out the FRM learning objective.\n2. **Limited scalability**: The proposed algorithm is not scalable, requiring significant approximations and Hessian information that could be difficult to obtain for large models.\n3. **Lack of theoretical arguments**: The paper lacks formal arguments for why FRM will outperform ERM, and there are no guarantees that the FRM minimizer will achieve low loss.\n4. **Confusion with Bayesian learning**: The reviewers are confused about the relation between FRM and Bayesian learning, and the authors' claims about using a single parameter at test-time are unclear.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper has significant potential and novelty, but it requires major revisions to address the weaknesses identified by the reviewers. The lack of clarity, limited scalability, and lack of theoretical arguments are significant concerns that need to be addressed.\n\nHowever, given the high technical novelty and significance of the paper, I believe that it is worth revising and resubmitting. The authors should carefully address the reviewers' comments, provide clear and explicit explanations of the FRM learning objective, and provide more theoretical arguments and guarantees for the performance of FRM.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach to protein sequence and structure co-design, leveraging a trigonometry-aware encoder and a roto-translation equivariant decoder to iteratively translate both protein sequence and structure into the desired state. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art baselines by a large margin, with significant improvements in terms of PPL, RMSD, and AAR.\n\nThe reviewer comments highlight several strengths of the paper, including:\n\n1. Good introduction and well-presented related work\n2. Strong empirical results, with state-of-the-art performance on multiple tasks\n3. Novel approach to protein sequence and structure co-design\n\nHowever, the reviewers also raise several concerns and weaknesses, including:\n\n1. Lack of specific hyperparameter and training details\n2. No mention of code and trained model release\n3. Discrepancies with prior work, particularly with regards to the Diffusion model\n4. Limited evaluation metrics, with a focus on structural metrics rather than functional metrics\n5. Concerns about the novelty and significance of the contributions, with some reviewers arguing that the approach is not significantly new or novel\n\nDespite these concerns, the majority of the reviewers recommend acceptance, citing the strong empirical results and the potential impact of the work on the field of protein engineering. Reviewer 7 notes that comparing to more recent models and on larger datasets would make the paper stronger, but still recommends acceptance. Reviewer 8 argues that the paper is a good contribution to the field, with useful modifications to previous co-design methods, and votes for acceptance.\n\nAfter carefully considering the reviewer comments and the paper's strengths and weaknesses, I believe that the paper should be accepted for publication. While there are some concerns about the novelty and significance of the contributions, the strong empirical results and the potential impact of the work on the field of protein engineering outweigh these concerns. Additionally, the authors have addressed some of the reviewer comments and concerns in their responses, and have provided additional details and clarifications to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes several heuristics to improve the performance of online label shift adaptation methods in the presence of conditional shift. The problem addressed is well-motivated and relevant to real-world applications, where distribution shifts are common. However, the reviewers have raised several concerns regarding the paper's clarity, quality, novelty, and reproducibility.\n\nOne of the primary concerns is the lack of a systematic approach to addressing general distribution shifts. The paper considers a few specific types of shifts, but the results are not generalizable, and the experiments are limited in scope. The reviewers have suggested that a more comprehensive empirical study is necessary to ascertain the usefulness of the proposed methods.\n\nAnother concern is the lack of clear definitions and explanations of key concepts, such as \"conditional shift\" and the notation used in the paper. The reviewers have also pointed out that some sections, particularly the algorithm design part, are not well-written and are difficult to follow.\n\nThe proposed heuristics, while novel, lack theoretical justification and intuitive support. The reviewers have suggested that the use of an identity matrix instead of the confusion matrix may sacrifice the unbiasedness of the gradient estimator, and the adjustment of the estimation of the confusion matrix may not be sufficient to handle the label shift condition.\n\nThe empirical results presented in the paper are not conclusive, and the proposed methods do not show significant improvements over the baseline methods. The reviewers have suggested that a stronger baseline, such as estimating the label distribution using a labeled validation set, should be used for comparison.\n\nThe paper also lacks a clear discussion of the difference between the proposed method and the previous work of Wu et al. (2021). The reviewers have suggested that the paper should provide a more detailed comparison with the baseline methods and a clearer explanation of the problem setup and method design.\n\nGiven these concerns, the majority of the reviewers have expressed a tendency to reject the paper. While the problem addressed is interesting and relevant, the paper's limitations in terms of clarity, quality, novelty, and reproducibility are significant.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CAT: Collaborative Adversarial Training\" proposes a novel approach to improve the robustness of neural networks by collaboratively training two robust models using different adversarial training methods. The authors demonstrate the effectiveness of their approach through extensive experiments on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art robustness without using any additional data.\n\nThe reviewers have provided detailed comments on the paper, highlighting both strengths and weaknesses. The main strengths of the paper include:\n\n1. **Novel approach**: The paper proposes a new approach to collaborative adversarial training, which is easy to understand and implement.\n2. **Improved robustness**: The experiments demonstrate improved robustness compared to baseline models trained non-collaboratively.\n3. **Well-written paper**: The paper is clear and well-written, making it easy to follow and understand.\n\nHowever, the reviewers have also raised several concerns and weaknesses, including:\n\n1. **Lack of novelty**: Some reviewers have pointed out that similar ideas of collaboratively fusing knowledge have been proposed and studied in prior work.\n2. **Increased computational cost**: The proposed method involves higher complexity, with 2x computational cost and 2x memory overhead, which may be a concern for large-scale datasets.\n3. **Limited experimental analysis**: The reviewers have suggested that the experimental analysis is limited, with only small-scale datasets considered, and a lack of comparison with more recent and stronger baselines.\n4. **Lack of theoretical analysis**: The paper does not provide a theoretical analysis of the proposed method, which may make it difficult to understand why the approach works.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I believe that the paper has some merits, but also requires significant revisions to address the concerns raised by the reviewers.\n\nThe authors need to:\n\n1. **Clarify the novelty of the approach**: The authors should provide a more detailed discussion of prior work and clarify how their approach differs from existing methods.\n2. **Provide a theoretical analysis**: The authors should provide a theoretical analysis of the proposed method to understand why it works and how it improves robustness.\n3. **Extend the experimental analysis**: The authors should consider larger datasets, such as ImageNet, and compare their approach with more recent and stronger baselines.\n4. **Address the computational cost**: The authors should discuss ways to reduce the computational cost of the proposed method, making it more feasible for large-scale datasets.\n\nGiven the current state of the paper, I believe that it requires significant revisions to address the concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" proposes a novel approach to improve the generalization of policies in complex games by maintaining a population of agents with diverse playing styles and high skill levels. The authors introduce a bi-objective optimization model and an evolutionary algorithm to achieve this goal.\n\nThe reviewers' comments provide valuable insights into the paper's strengths and weaknesses. Reviewer 1 praises the paper's clarity and the interesting topic but criticizes the lack of discussion on related work, particularly the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al. (2017). Reviewer 1 also questions the experimental design, suggesting that it may give an unfair advantage to the proposed method. Reviewer 2 finds the paper well-written and easy to read but requires more clarification on certain aspects, such as the reduction of \"playing style\" to a single scalar and the description of the $var_{\\pi_k}(s_t, s_{t+1})$ term. Reviewer 3 appreciates the algorithm's simplicity and the promising results but notes that the paper lacks comparison to quality-diversity algorithms and has limited evaluation domains.\n\nAfter carefully analyzing the reviewers' comments, I identify several key issues that need to be addressed:\n\n1. **Lack of discussion on related work**: The paper should provide a more comprehensive review of related work, including the paper by Lanctot et al. (2017) and other relevant studies on multi-agent reinforcement learning and quality-diversity algorithms.\n2. **Experimental design**: The authors should reconsider their experimental design to ensure that it is fair and unbiased. This may involve using a more balanced pool of opponents or providing a more detailed analysis of the results.\n3. **Clarity and reproducibility**: The authors should provide more clarification on certain aspects of the paper, such as the reduction of \"playing style\" to a single scalar and the description of the $var_{\\pi_k}(s_t, s_{t+1})$ term. Additionally, they should provide more details on the experimental setup and the code used to implement the algorithm to ensure reproducibility.\n4. **Comparison to quality-diversity algorithms**: The paper should include a comparison to quality-diversity algorithms, such as MAP-Elites, to demonstrate the effectiveness of the proposed approach.\n5. **Limited evaluation domains**: The authors should consider evaluating their approach on more domains to demonstrate its robustness and generalizability.\n\nGiven these concerns, I believe that the paper requires significant revisions to address the reviewers' comments and improve its overall quality.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" presents an interesting idea of leveraging graph structures and topological properties to detect out-of-distribution (OOD) data. The authors propose a mechanism for deriving low-dimensional representations suitable for effective use of established non-parametric and parametric OOD data detection methods. However, after carefully analyzing the paper and the reviewer comments, several concerns and weaknesses have been identified that impact the paper's overall quality and readiness for publication in a top-tier conference.\n\n1. **Lack of Clarity and Detail**: Multiple reviewers (Reviewer 1, Reviewer 2, Reviewer 3, and Reviewer 4) have pointed out that the paper lacks clarity and detail in its presentation. This includes poorly defined terms, insufficient explanation of the feature extraction procedure, and inadequate description of the experimental setup. For instance, Reviewer 1 mentions that the paper uses weakly defined terms like \"Common sense\" and that the relationship between the summary in the introduction and the actual implementation is not straightforward. Reviewer 4 notes that the experimental section is far from enough to validate the effectiveness of the introduced method and that the paper is poorly written with lots of glitches and missing details.\n\n2. **Weakness in Experimental Validation**: The reviewers have also criticized the experimental validation of the proposed method. Reviewer 2 states that the empirical results are not convincing and that the proposed method is not compared to existing state-of-the-art methods for OOD detection. Reviewer 4 suggests that the comparison of other feature-based OOD detection methods and the use of other commonly used in-distribution data like CIFAR-10/100 and ImageNet are missing. Furthermore, Reviewer 4 points out the need for an ablation study of the hyperparameters and the interpretation of the OOD detection by semantic graph.\n\n3. **Novelty and Significance**: While the idea presented in the paper is considered interesting by some reviewers (Reviewer 3 and Reviewer 4), the overall novelty and significance of the contributions are questioned. Reviewer 1 suggests that the paper's claims are plausible but require more rigorous demonstration. Reviewer 3 believes that the method lacks novelty as it combines several well-defined methods. Reviewer 4 notes that the assumption of having an oracle object detector for all in-distribution data is unrealistic.\n\n4. **Comparison to State-of-the-Art (SoTA) Techniques**: The paper claims to achieve performance comparable to SoTA techniques but does not provide a direct comparison. Reviewer 1 mentions that typically, such claims are validated by reproducing the SoTA techniques for controlled comparisons. The lack of a thorough comparison with existing methods undermines the paper's claim of being competitive with the state-of-the-art.\n\n5. **Grammatical Mistakes and Poor Writing**: Several reviewers have mentioned that the paper contains grammatical mistakes and is poorly written, which hinders understanding and clarity.\n\nConsidering these points, while the paper presents an interesting approach to detecting OOD data, its current form is not suitable for publication in a top-tier conference due to significant issues with clarity, experimental validation, novelty, and overall presentation quality.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi Task Learning of Different Class Label Representations for Stronger Models\" proposes a novel approach to improve image classification performance by using binary labels as an auxiliary task in a multi-task learning setting. The authors demonstrate that this approach can lead to higher accuracy across various datasets and architectures, especially when training data is limited.\n\nHowever, upon careful analysis of the reviewer comments, several concerns and weaknesses are raised that impact the paper's overall quality and significance. \n\n1. **Lack of Clarity and Reproducibility**: Multiple reviewers (Reviewer 1 and Reviewer 4) point out issues with clarity, including the explanation of key concepts such as \"dense labels\" and the procedure for generating binary labels. Reviewer 1 also mentions that the paper lacks sufficient detail for reproducibility, such as the implementation of \"Metabalance\" for multi-task learning and the specific hyperparameters used.\n\n2. **Weak Experimental Evaluation**: Reviewers (Reviewer 1 and Reviewer 2) criticize the experimental section for being weak, suggesting that more analyses, comparisons with additional baselines, and exploration of different datasets and scenarios are needed. Reviewer 2 also notes that the baseline algorithms used for comparison have too low performance, making it hard to assess the true effectiveness of the proposed method.\n\n3. **Limited Technical Novelty and Significance**: Reviewers (Reviewer 3 and Reviewer 4) question the technical novelty and significance of the contributions, suggesting that while the idea is simple and effective, it may not be significant enough for a top-tier conference due to the small empirical improvements and the lack of deeper analysis into the problem of label encoding.\n\n4. **Related Work and Comparison**: Reviewer 1 and Reviewer 3 mention that the discussion of related work is not thorough, particularly in relation to label smoothing, unsupervised meta-learning, and other forms of label encoding. A more comprehensive comparison with existing methods and a clearer explanation of how the proposed approach differs from or improves upon prior work are needed.\n\n5. **Minor Issues and Typos**: Several minor issues, including typos and calculation errors, are pointed out by the reviewers, which, while not critical, detract from the overall quality of the submission.\n\nConsidering these points, while the paper proposes an interesting idea and shows some promising results, the concerns regarding clarity, reproducibility, experimental evaluation, technical novelty, and related work discussion are significant enough to impact the paper's suitability for publication at a top-tier conference in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" presents a convergence analysis of gradient descent for deep linear neural networks with various initialization schemes and general strongly convex and smooth objectives. The reviewers have provided detailed comments on the paper, highlighting both its strengths and weaknesses.\n\nThe main strengths of the paper include its well-written and clear presentation, rigorous analysis, and significant contributions to the understanding of gradient descent behavior in deep linear neural networks. The paper provides a sharp convergence rate analysis, showing that overparameterized deep linear networks can converge as fast as gradient descent on the equivalent linear regressor. The results have fundamental importance and are somewhat surprising, as they suggest that deeper nets can converge as fast as a convex linear model.\n\nHowever, the reviewers have also raised several concerns and criticisms. Some of the main weaknesses of the paper include:\n\n1. **Lack of novelty**: Several reviewers have questioned the novelty of the paper, arguing that the results are not significantly new and build upon existing work. Reviewer 1, Reviewer 3, Reviewer 5, and Reviewer 6 have all raised concerns about the paper's novelty, with Reviewer 5 stating that the main innovation is an extension to general strongly convex and smooth objectives, which is well-known in convex settings.\n2. **Overclaiming**: Reviewer 6 has criticized the paper for overclaiming, arguing that the title and abstract should be more specific and avoid suggesting that the results apply to both overparameterized and underparameterized models.\n3. **Lack of insights and discussion**: Reviewer 6 has also criticized the paper for lacking insights and discussion, particularly with regards to the importance of overparameterization and the distinction between overparameterized and underparameterized models.\n4. **Technical issues**: Reviewer 3 has raised several technical issues, including minor errors in the paper and a lack of clarity in some of the definitions and notation.\n\nDespite these concerns, the majority of the reviewers have recommended acceptance or acceptance with revisions. Reviewer 2, Reviewer 4, and Reviewer 5 have all recommended acceptance, with Reviewer 4 stating that the paper is a solid theoretical work on the convergence of gradient descent of deep linear neural networks.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I believe that the paper should be accepted with revisions. The paper provides significant contributions to the understanding of gradient descent behavior in deep linear neural networks, and the results have fundamental importance. While the paper may not be entirely novel, it builds upon existing work and provides a rigorous and well-written analysis.\n\nTo address the concerns raised by the reviewers, the authors should:\n\n1. **Revise the title and abstract**: The authors should revise the title and abstract to be more specific and avoid overclaiming.\n2. **Provide more insights and discussion**: The authors should provide more insights and discussion, particularly with regards to the importance of overparameterization and the distinction between overparameterized and underparameterized models.\n3. **Address technical issues**: The authors should address the technical issues raised by Reviewer 3, including minor errors and a lack of clarity in some of the definitions and notation.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" presents a novel approach to neural-symbolic AI, introducing DeepSeaProbLog, a language that supports discrete and continuous random variables. The paper's main contributions include the introduction of DeepSeaProbLog, its semantics, an implementation for inference and learning, and an experimental evaluation.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewers 1, 3, and 4 appreciate the paper's novelty, technical soundness, and the potential of DeepSeaProbLog to tackle real-world problems. However, they also raise several concerns, including the need for more extensive empirical evaluations, comparisons with existing methods, and discussions on the choice of weighted model integration solvers. Reviewers 2 and 5 suggest that the paper would benefit from additional applications and experiments to demonstrate the scalability and effectiveness of DeepSeaProbLog. Reviewer 6 questions the novelty of the approach, arguing that the techniques used are well-known and that the paper does not provide sufficient details on handling discrete random variables. Reviewer 7 finds the paper to be incremental and suggests that it would benefit from a more condensed background section and more details on the semantics of DeepSeaProbLog.\n\nDespite these criticisms, the majority of the reviewers acknowledge the paper's technical soundness and novelty. The authors' responses to the reviewers' comments address some of the concerns, providing additional context and clarifications.\n\nConsidering the reviewers' comments and the authors' responses, the paper's strengths outweigh its weaknesses. The introduction of DeepSeaProbLog and its potential to handle discrete and continuous random variables is a significant contribution to the field of neural-symbolic AI. While the empirical evaluation could be more extensive, the experiments provided demonstrate the effectiveness of DeepSeaProbLog in certain tasks.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "To determine whether the paper \"Data Leakage in Tabular Federated Learning\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper based on its technical novelty, significance, empirical contributions, clarity, and overall quality, considering the comments and scores provided by the reviewers.\n\n1. **Technical Novelty and Significance**: Reviewers 1 and 2 express concerns that the technical contributions of the paper are not novel or significant, citing that the methods used (such as softmax relaxations and ensemble techniques) are well-known and not innovative in the context of this work. However, Reviewer 3 acknowledges the novelty of applying these methods to tabular data and the significance of the empirical contributions, suggesting that while the individual components may not be new, their application and combination in this specific context could be considered novel and significant.\n\n2. **Empirical Novelty and Significance**: The empirical evaluation of the paper demonstrates a clear improvement over baseline methods, especially in the context of tabular data, which is a significant contribution. Reviewer 3 highlights the thoroughness of the experiments and their convincing nature, indicating that the empirical contributions are both novel and significant.\n\n3. **Clarity and Quality**: Reviewer 1 criticizes the paper for being poorly written, with obscure sentences and grammatical errors, which hampers understanding. In contrast, Reviewers 2 and 3 find the paper well-written and easy to follow, although Reviewer 3 points out minor typos and suggests improvements for clarity and consistency in notation.\n\n4. **Correctness**: All reviewers agree that the paper's claims are mostly correct, with minor issues that could be addressed. Reviewer 3 provides detailed feedback on how certain claims could be better supported or clarified.\n\n5. **Reproducibility**: The authors are commended for providing experimental details, but Reviewer 3 notes a few aspects that could be improved for full reproducibility, such as clarifying how the accuracy over batches is computed and whether there are repetitions over different network initializations.\n\nConsidering these points, the paper's strengths lie in its empirical contributions, the novelty of applying known methods to a new domain (tabular data), and the significance of demonstrating data leakage attacks in federated learning for this type of data. However, the paper's weaknesses include concerns over the novelty of the technical contributions, some minor issues with clarity and correctness, and areas for improvement in reproducibility.\n\nGiven the detailed and mostly positive review by Reviewer 3, and considering the significance of the empirical contributions and the application of the methods to a new and important domain, the paper's strengths outweigh its weaknesses. The minor issues pointed out by the reviewers can be addressed through revisions, which would further strengthen the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review presents a study on the phase transition thresholds of permuted linear regression via message passing. The analysis is conducted for both the oracle case, where the signal of interest is known, and the non-oracle case, where the signal is not given. The authors claim to provide a precise identification of the phase transition thresholds and propose an estimator for partial permutation recovery.\n\nHowever, upon careful examination of the reviewer comments, several concerns arise that impact the paper's suitability for publication in its current form. \n\n1. **Clarity and Quality**: Multiple reviewers (Reviewer 1, Reviewer 2, Reviewer 4, and Reviewer 5) have expressed difficulty in following the paper due to its poor writing, numerous typos, and lack of clarity in presenting mathematical concepts and notation. Reviewer 5 explicitly states that the mathematical writing is \"terrible\" and prevents understanding of the paper. This issue is critical because it hinders the ability of readers to comprehend and verify the research.\n\n2. **Technical Novelty and Significance**: While the paper's contributions are considered somewhat new and significant by some reviewers (Reviewer 3 and Reviewer 4), others (Reviewer 2 and Reviewer 5) question the novelty and practical significance of the results, especially given the reliance on existing techniques and the lack of clear practical motivation for some aspects of the work.\n\n3. **Correctness and Reproducibility**: Reviewer 2 and Reviewer 5 express concerns about the correctness of some claims and the lack of rigorous support for certain statements. Additionally, the reproducibility of the results is questioned due to the absence of publicly available code, as noted by Reviewer 4.\n\n4. **Empirical Novelty and Significance**: The empirical contributions are seen as somewhat significant by some, but the presentation and interpretation of empirical results are criticized for lacking clarity and direct relevance to the theoretical predictions, as highlighted by Reviewer 4 and Reviewer 5.\n\nGiven these concerns, it is evident that the paper requires significant revisions to address issues of clarity, technical novelty, correctness, and reproducibility before it can be considered for publication in a top-tier conference. The lack of clarity and the poor presentation of mathematical concepts are particularly problematic, as they prevent a thorough evaluation of the paper's technical contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning\" proposes a novel aggregation strategy, named FedPA, that uses a Parameterized Aggregator to learn to aggregate model parameters of clients in federated learning. The paper identifies 'period drift' as a key issue in federated learning, which is caused by the heterogeneity of data distributions among different communication rounds.\n\nThe reviewers have provided detailed comments on the paper, highlighting both strengths and weaknesses. The main strengths of the paper include:\n\n1. Identification of 'period drift' as a key issue in federated learning, which is a novel contribution.\n2. Proposal of a novel aggregation strategy, FedPA, which uses a Parameterized Aggregator to learn to aggregate model parameters of clients.\n3. Experimental results showing that FedPA can achieve competitive performances compared with conventional baselines.\n\nHowever, the reviewers have also highlighted several weaknesses, including:\n\n1. Lack of theoretical or experimental justification to highlight the effectiveness of the algorithm.\n2. Limited novelty, as the concept of 'period drift' is not well-defined, and the proposed method is similar to other knowledge distillation-based techniques.\n3. Lack of ablation studies to show the effect of the data distribution in the proxy data on model performance.\n4. Experimental settings are not strong, with simple datasets and models used.\n5. Code is not provided, which limits reproducibility.\n6. Writing quality needs to be improved, with grammatical mistakes and unclear explanations.\n\nAfter carefully analyzing the comments from the reviewers, I conclude that the paper has some interesting ideas, but it requires significant improvements to be considered for publication in a top-tier conference. The paper lacks theoretical or experimental justification to support its claims, and the proposed method is not significantly novel. Additionally, the experimental settings are not strong, and the writing quality needs to be improved.\n\nTherefore, based on the current state of the paper, I would recommend rejecting it for publication. However, I would encourage the authors to address the weaknesses highlighted by the reviewers and resubmit the paper for consideration.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" proposes a deep learning system to create anonymized representations for data, aiming to maintain accuracy for Machine Learning as a Service (MLaaS) tasks while preserving privacy. The system splits a neural network into two parts: an encoder sent to the client and a classifier on the server, using a center loss function to minimize information leakage.\n\nHowever, upon careful analysis of the reviewer comments, several significant concerns arise that impact the paper's suitability for publication at a top-tier conference:\n\n1. **Lack of Novelty**: Reviewers 1 and 2 mention that the idea of splitting a neural network into two parts for privacy preservation is not new, particularly in the context of privacy-preserving learning. The novelty of the approach is questioned, with Reviewer 2 noting similarities to \"split learning\" without proper citation or differentiation.\n\n2. **Technical Flaws**: Reviewer 2 identifies severe technical flaws in the proposed approach, including the potential for the server to recreate a \"surrogate\" encoder, thereby compromising client data privacy. The reviewer also criticizes the threat model, suggesting it is unrealistic to assume the server would honestly implement the center loss without attempting to exploit the system for privacy invasion.\n\n3. **Limited Experimental Evaluation**: Reviewers 1 and 2 criticize the experimental section for being too simplistic, with evaluations on only two datasets (MNIST and CelebA) and limited model architectures (LeNet-5 and VGG-16). The lack of comparison with more robust baselines, such as fully homomorphic encryption-based approaches, and the absence of experiments on more challenging settings (e.g., identity inference vs. gender recognition) are highlighted as significant shortcomings.\n\n4. **Security Model and Privacy Guarantees**: Reviewers 2 and 3 express concerns about the paper's failure to define a clear security model and provide robust privacy guarantees. The use of middle features of a neural network is deemed risky for information leakage, especially in scenarios involving sensitive data like physiological time-series data from smartwatches.\n\n5. **Clarity and Reproducibility**: While the paper is generally considered clear and easy to follow, there are noted ambiguities and grammatical errors. The reproducibility of the work is questioned due to missing details such as optimizers used and the number of training epochs.\n\nGiven these concerns, the paper falls short of the standards expected for publication at a top-tier conference. The technical flaws, lack of novelty, limited experimental evaluation, and insufficient attention to security models and privacy guarantees are significant enough to outweigh any potential strengths of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the comments and scores provided by the reviewers, considering factors such as the paper's novelty, significance, correctness, and overall contribution to the field.\n\n1. **Novelty and Significance**: Reviewers 1, 2, and 3 express concerns that the paper's contributions are not significantly novel or that the results are incremental, especially considering prior works like PP16. Reviewer 4, however, believes the contributions are significant, particularly highlighting the importance of the problem addressed and the results for the special case of 2-by-2 games. The discrepancy in opinions suggests that while the paper tackles an important problem, its novelty and the significance of its contributions might be debated.\n\n2. **Correctness**: All reviewers agree that the claims and statements in the paper are well-supported and correct, indicating a high level of technical quality.\n\n3. **Comparison with Prior Works**: Several reviewers mention that the paper does not adequately compare its contributions with prior works, especially PP16. This lack of comparison makes it difficult to fully assess the paper's novelty and significance.\n\n4. **Scope and Generalizability**: The paper focuses on a specific class of games (2x2 symmetric coordination games), which some reviewers see as a limitation. The generalizability of the approach to more complex games or broader scenarios is not fully explored, which could limit the paper's impact.\n\n5. **Clarity and Quality**: The paper is generally well-written and easy to follow, which is a positive aspect. However, there are suggestions for improvement in terms of notation, citation formatting, and the clarity of certain explanations.\n\n6. **Relevance and Interest**: Reviewer 4 notes that while the problem is important, the number of people interested in this specific result might be smaller compared to other conferences, suggesting that the paper's relevance and appeal to the broader audience of the conference could be a consideration.\n\nConsidering these points, the decision to accept or reject the paper hinges on the trade-offs between its technical correctness, the perceived novelty and significance of its contributions, and its potential impact and relevance to the conference audience.\n\nWhile the paper is technically sound and addresses an important problem, the concerns about its novelty, the limited scope of its results, and the comparison with prior works are significant. However, the fact that all reviewers acknowledge the paper's technical quality and the importance of the problem it tackles suggests that it has value.\n\nGiven the mixed opinions on novelty and significance, and considering the suggestions for improvement and the need for clearer comparison with prior works, the decision could lean towards rejection based on the standards of a top-tier conference, which typically look for groundbreaking, broadly impactful research. However, the paper's strengths, especially its technical correctness and the importance of the problem it addresses, are notable.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" presents a new dataset for precipitation nowcasting, which is a crucial task in weather forecasting. The dataset, named RuDar, includes 3D radar echo observations from 30 weather radars in Russia, covering multiple climate zones, with a high spatial and temporal resolution. The authors evaluate several baseline models on this dataset and demonstrate the potential of RuDar for precipitation nowcasting.\n\nThe reviewers' comments provide valuable insights into the paper's strengths and weaknesses. Reviewer 1 praises the dataset's attractiveness and the promising results but notes that the methods used are relatively low-cost and wonders how more advanced models would perform. Reviewer 2 appreciates the dataset's comprehensiveness and the careful description of the acquisition process but criticizes the baseline methods as too naive and lacking ablation studies. Reviewer 3 finds the dataset rich and the comparison with other datasets useful but notes that the dataset's coarseness might be limiting and that there are no methodological novelties.\n\nThe main strengths of the paper are:\n\n1. The introduction of a new, comprehensive dataset for precipitation nowcasting, which can be useful for the research community.\n2. The careful evaluation of several baseline models on the proposed dataset.\n3. The potential of RuDar for other machine learning tasks, such as data shift studying, anomaly detection, or uncertainty estimation.\n\nHowever, the paper also has some weaknesses:\n\n1. The lack of methodological novelties, as noted by Reviewer 3.\n2. The simplicity of the baseline methods, as criticized by Reviewer 2.\n3. The limited domain relativity and novelty, as mentioned by Reviewer 1.\n4. The coarseness of the dataset, which might be limiting for precipitation nowcasting, as noted by Reviewer 3.\n\nConsidering the reviewers' comments and the paper's strengths and weaknesses, I believe that the paper makes a significant contribution to the field of precipitation nowcasting by introducing a new, comprehensive dataset. However, the lack of methodological novelties and the simplicity of the baseline methods are notable weaknesses.\n\nThe paper's correctness, technical novelty, and empirical novelty are well-supported, but the significance of the contributions is somewhat limited by the lack of methodological novelties. The paper is well-written, clear, and easy to understand, which is a significant strength.\n\nGiven the above analysis, I conclude that the paper is a valuable contribution to the research community, but it may not meet the high standards of a top-tier conference like ICLR, which typically expects more significant methodological novelties and contributions.\n\nFinal Decision: Reject"
    }
  ]
}