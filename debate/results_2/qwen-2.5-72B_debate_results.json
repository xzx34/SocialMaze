{
  "summary": {
    "model": "qwen-2.5-72B",
    "total_papers": 100,
    "correct_predictions": 82,
    "accuracy": 82.0,
    "confusion_matrix": [
      [
        66,
        10
      ],
      [
        8,
        16
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 86.8421052631579,
        "precision": 89.1891891891892
      },
      "Accept": {
        "total": 24,
        "recall": 66.66666666666666,
        "precision": 61.53846153846154
      }
    }
  },
  "results": [
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Writing Quality**\n- **Reviewer 1** and **Reviewer 2** both highlight significant issues with the clarity and writing quality of the paper. The abstract and the entire paper are described as poorly written, with grammatical issues and unclear sentences. This is a critical issue for a top-tier conference, where clarity and precision in communication are paramount.\n- **Reviewer 3** also emphasizes the poor writing quality, noting that the paper is full of incomprehensible passages and lacks coherence. The lack of clarity makes it difficult for reviewers and readers to understand the contributions and methodology of the paper.\n\n#### 2. **Scientific Rigor and Methodology**\n- **Reviewer 1** points out that the paper does not cite any previous research, which is a significant oversight. In a top-tier conference, it is essential to situate the work within the existing literature and to build upon previous research.\n- **Reviewer 2** and **Reviewer 3** both mention that the paper lacks scientific rigor. The methodology is not clearly explained, and there are many unjustified statements. The lack of references and the absence of a clear research framework suggest that the paper does not meet the standards of a serious academic endeavor.\n\n#### 3. **Technical Content and Contributions**\n- **Reviewer 1** and **Reviewer 3** both note that the paper's technical content is unclear. The proposed method for improving Monte Carlo Tree Search (MCTS) is not well-defined, and the experiments are not well-explained. The lack of clear technical content and experimental results makes it difficult to assess the paper's contributions.\n- **Reviewer 2** mentions that the paper is full of inaccurate and unjustified statements, which further undermines its credibility.\n\n#### 4. **Experimental Validation**\n- **Reviewer 1** and **Reviewer 3** both mention that the experiments are not well-conducted or well-reported. The experiments are limited to a small problem (tic-tac-toe), and the results are not robustly compared to existing methods like Alpha Zero. In a top-tier conference, robust experimental validation is crucial to demonstrate the effectiveness and generalizability of the proposed method.\n\n#### 5. **Overall Impression**\n- All three reviewers agree that the paper is poorly written, lacks scientific rigor, and does not provide a clear contribution. The lack of references, the poor writing quality, and the unclear methodology are significant issues that need to be addressed before the paper can be considered for publication in a top-tier conference.\n\n### Final Decision\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Contribution and Novelty\nThe paper introduces NORML, a novel meta-learning framework that uses an LSTM-based meta-learner to perform neuron-wise optimization on a learner network. The key claim is that NORML can scale to large networks and perform better than existing methods like MAML and the Meta-Learner LSTM (Ravi & Larochelle).\n\nHowever, the novelty of the contribution is questionable:\n- **Comparison to Ravi & Larochelle**: The paper claims that the Meta-Learner LSTM requires an enormous number of parameters, which is not accurate. Ravi & Larochelle use parameter sharing to keep the number of meta-learner parameters constant relative to the learner network size. This misunderstanding undermines the paper's claim of scalability.\n- **Incremental Improvement**: The proposed method is a variation of existing LSTM-based meta-learners. The specific neuron-wise optimization is an interesting idea, but it is not a significant leap from previous work.\n\n#### Experimental Evaluation\nThe experimental results are not convincing:\n- **Mini-ImageNet**: The paper builds on the method of Sun et al. but does not show the results from Sun et al. for comparison. This makes it difficult to evaluate the improvement.\n- **Omniglot**: The experiment is performed on a fully connected network, which is not typical for few-shot learning tasks. A comparison with a convolutional network would be more meaningful.\n- **MAML Comparison**: The paper claims that NORML can handle a large number of inner-loop updates better than MAML, but this is not supported by the experiments, which use very few inner-loop steps.\n\n#### Writing and Presentation\nThe writing and presentation of the paper are subpar:\n- **Grammar and Style**: There are numerous grammatical errors and stylistic issues that need to be addressed. The paper is not at the level expected for a top-tier conference.\n- **Clarity and Flow**: The flow of ideas is not clear, and the paper lacks coherence in several sections. This makes it difficult for readers to follow the arguments and understand the contributions.\n\n#### Technical Soundness\n- **Inaccurate Descriptions**: The paper contains several inaccurate descriptions of prior work, which can mislead readers and reviewers.\n- **Missing Baselines**: The paper lacks important baselines, such as a version of MAML with per-hidden-unit and per-inner-loop-step learning rates, and MAML++. These comparisons are necessary to validate the claims of the paper.\n\n### Final Decision\nGiven the limited novelty, unconvincing experimental results, poor writing, and technical inaccuracies, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel approach to learning hierarchical and disentangled representations using a progressive learning strategy. The idea of progressively growing the capacity of a VAE to learn hierarchical representations is innovative and addresses a significant challenge in the field of generative models. The authors provide a clear and well-motivated explanation of their method, which is a significant contribution to the literature.\n\n#### Technical Soundness\nThe technical approach is well-founded and builds upon existing methods like VAE and VLAE. The authors introduce a new metric, MIG-sup, which is a valuable addition to the existing disentanglement metrics. However, there are some concerns regarding the clarity and robustness of the experiments and the implementation details.\n\n#### Experiments and Results\nThe experiments are thorough and cover multiple benchmark datasets. The authors provide both qualitative and quantitative results, which are generally convincing. However, there are a few issues that need to be addressed:\n- **Reviewer 1** pointed out that the first layer is not as well trained as VLAE, and suggested additional experiments to compare mutual information of each layer with VLAE. This is a valid concern and should be addressed.\n- **Reviewer 2** raised several points about the clarity of the purpose, the relationship between hierarchical and disentangled representations, and the robustness of the proposed metric. The reviewer also suggested ablation studies to verify the individual effects of the progressive learning method and implementation strategies. These are important points that need to be addressed to strengthen the paper.\n- **Reviewer 3** provided positive feedback but suggested additional quantitative experiments on information flow and better formalization of certain concepts.\n- **Reviewer 4** suggested ablation studies and a more thorough comparison with VLAE, which would make the paper stronger.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to follow. The authors provide a clear explanation of their method and the motivation behind it. However, there are some areas where the clarity can be improved, as noted by Reviewer 2 and Reviewer 3.\n\n#### Rebuttal and Additional Experiments\nThe authors' rebuttal and additional experiments address most of the concerns raised by the reviewers. The new experiments show that pro-VLAE performs better than VLAE, and the information flow experiments on MNIST demonstrate that the first layer learns the intended representations properly. These improvements significantly strengthen the paper.\n\n### Final Decision\nGiven the novel contribution, the thorough experiments, and the improvements made in the rebuttal, the paper should be accepted for publication. However, the authors should address the remaining minor issues and suggestions from the reviewers to further enhance the quality of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Writing Quality**\n- **Reviewer Comments**: All three reviewers have highlighted significant issues with the clarity and writing quality of the paper. Reviewer 2 and Reviewer 3 specifically mention that the paper is \"badly written\" with \"grammatical issues almost in all the sentences.\" Reviewer 1 also notes that the paper is \"poorly written.\"\n- **Impact**: Poor writing and clarity can severely hinder the understanding and evaluation of the research. If the reviewers, who are experts in the field, cannot understand the paper, it is unlikely that the broader academic community will be able to engage with it effectively.\n\n#### 2. **Content and Substance**\n- **Reviewer Comments**: Reviewer 1 states that the paper is \"less than half-baked\" and lacks references. Reviewer 3 mentions that the paper is \"full of inaccurate and unjustified statements\" and \"does not represent even the preliminary elements of scientific writing.\" Reviewer 2 also notes that the paper is \"full of inaccurate and unjustified statements.\"\n- **Impact**: The lack of references and the presence of inaccurate statements suggest that the paper has not been rigorously developed or reviewed. This is a critical issue for a top-tier conference, where the quality and robustness of the research are paramount.\n\n#### 3. **Methodology and Experiments**\n- **Reviewer Comments**: Reviewer 1 mentions that the paper \"tries to improve exploration performed by alphazero in a game of tic-tac-toe using MCTS,\" but the method is not clearly explained. Reviewer 3 states that the paper \"does not appear to be a serious academic endeavor, merely a set of notes\" and that the algorithm boxes are \"very difficult to follow.\"\n- **Impact**: The methodology and experiments are the core of the research. If these are not clearly described or are difficult to follow, it undermines the credibility and reproducibility of the work.\n\n#### 4. **Relevance and Contribution**\n- **Reviewer Comments**: Reviewer 1 and Reviewer 3 both suggest that the paper lacks a clear contribution and does not cite previous research. Reviewer 2 mentions that the paper does not provide a clear understanding of its research focus.\n- **Impact**: A top-tier conference expects papers to make a significant and clear contribution to the field. The lack of a clear contribution and the absence of references to previous work suggest that the paper does not meet this standard.\n\n### Final Decision\nGiven the significant issues with clarity, writing quality, methodology, and lack of references, the paper does not meet the standards required for publication in a top-tier conference. The reviewers' comments consistently highlight these problems, and the paper would require substantial revisions to address them.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper proposes a method, AE-GAN+sr, which combines an auto-encoder (AE) and a generative adversarial network (GAN) to defend against adversarial attacks. The key innovation is the use of the encoder to provide a good starting point for the search in the latent space, which reduces computational cost compared to previous methods like Defense-GAN. However, the novelty is somewhat incremental, as it builds heavily on existing work.\n\n#### Technical Soundness\nThe method is technically sound and well-explained. The use of an adversarial loss in the AE to enhance the generative ability of the decoder and the preservation of the abstraction ability of the encoder are reasonable approaches. The search process in the latent space is also well-motivated. However, the paper lacks a thorough comparison with state-of-the-art methods, particularly those that are not based on GANs, such as PGD Adversarial Training and TRADES. This omission weakens the paper's contribution.\n\n#### Experimental Evaluation\nThe experiments are conducted on MNIST and Fashion-MNIST, which are relatively simple datasets. While the results show that AE-GAN+sr outperforms Defense-GAN in terms of computational cost and performance, the lack of evaluation on more complex datasets like CIFAR-10 is a significant drawback. The performance on more challenging datasets would provide a stronger case for the method's effectiveness. Additionally, the paper does not provide a clear explanation of why the method performs better on stronger white-box attacks compared to weaker black-box attacks, which is a point of concern.\n\n#### Clarity and Presentation\nThe writing is somewhat unclear and contains several typos and grammatical errors. The paper is also one page longer than the allowed length, which is a violation of the conference guidelines. These issues make the paper harder to follow and reduce its overall quality. The reviewers have pointed out specific sections that need improvement, such as the clarity of the implementation details and the explanation of the search process.\n\n#### Addressing Reviewer Concerns\nThe authors have not adequately addressed the concerns raised by the reviewers. For instance, the lack of comparison with state-of-the-art adversarial training methods and the need for evaluation on more complex datasets are significant gaps that need to be addressed. The paper also lacks a clear explanation of the trade-offs between computational cost and performance, which is crucial for understanding the practical utility of the method.\n\n### Final Decision\nFinal Decision: Reject\n\nThe paper presents a method that is technically sound and has some novel aspects, but the incremental nature of the contribution, the lack of thorough experimental evaluation, and the issues with clarity and presentation make it unsuitable for a top-tier conference. Addressing the reviewer concerns and conducting more comprehensive experiments would significantly improve the paper's chances of acceptance."
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novelty and Innovation**:\n   - The paper introduces a novel approach to unsupervised domain adaptation by allowing different domains to undergo different sequences of operations. This is a significant contribution as it addresses the issue of information asymmetry between domains.\n   - The multi-flow network architecture is innovative and provides a flexible framework for domain adaptation.\n\n2. **Technical Soundness**:\n   - The method is technically sound and well-motivated. The use of adaptive computation graphs and learnable weights for domain-specific processing is a sensible approach.\n   - The paper is well-written and easy to follow, making the ideas and methods clear to the reader.\n\n3. **Potential for Extension**:\n   - The proposed framework can be extended to multi-source and multi-target domain adaptation, which is a valuable direction for future research.\n\n#### Weaknesses:\n1. **Insufficient Experiments**:\n   - The experimental section is a significant weakness. The paper only compares the proposed method with RPT and DANN, which are not state-of-the-art (SOTA) methods. This makes it difficult to assess the true performance and effectiveness of the proposed method.\n   - The lack of comparison with more recent and advanced methods like CDAN is a major oversight. This comparison is crucial to demonstrate the superiority or at least the competitiveness of the proposed method.\n\n2. **Ablation Studies and Hyperparameter Analysis**:\n   - The paper lacks detailed ablation studies and hyperparameter sensitivity analyses. For instance, the number of parallel flows (K) and the configuration of computational units are not thoroughly explored.\n   - The paper does not address whether the gates in the final layer can be deactivated, which could lead to suboptimal performance.\n\n3. **Multi-Domain Adaptation**:\n   - The potential of the multi-flow network in multi-source or multi-target domain adaptation is mentioned but not investigated. This is a missed opportunity to demonstrate the full potential of the proposed method.\n\n4. **Baseline Results**:\n   - The baseline results in Table 1 are not comparable to those reported in other UDA papers, which raises concerns about the robustness and effectiveness of the proposed method.\n\n5. **Minor Issues**:\n   - The paper could benefit from linking to more transfer learning techniques outside the deep learning domain.\n   - The use of the term \"multi-flow network\" could be clarified to avoid confusion with established concepts in algorithmics.\n   - The references should be cleaned up, and arXiv references should be replaced with reviewed material where possible.\n\n### Final Decision\nGiven the significant weaknesses in the experimental section and the lack of comparison with SOTA methods, the paper does not meet the standards of a top-tier conference. While the idea is novel and the method is technically sound, the experimental validation is insufficient to support the claims made.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\n- **Novelty:** The paper proposes a novel approach to multi-task learning by modeling multi-level task dependencies using an attention mechanism. The idea of making the amount of transfer between tasks dependent on the particular sample at hand is a novel contribution.\n- **Contribution:** The paper claims to address the limitation of existing multi-task learning frameworks that assume all tasks are highly correlated. By modeling both general task dependency and data-dependent task dependency, the authors aim to improve the performance of multi-task learning in real-world scenarios.\n\n#### **2. Technical Soundness:**\n- **Methodology:** The proposed method uses an attention mechanism to model task dependencies. The authors introduce a \"Transfer Block\" and a \"Point-wise Mutual Attention Mechanism\" to capture these dependencies. However, the technical details are not clearly presented, and there are several notational and mathematical confusions.\n- **Clarity:** The paper suffers from significant issues in clarity and presentation. There are numerous grammatical errors and unclear explanations, making it difficult to follow the finer points of the arguments.\n- **Comparison to Prior Work:** The paper fails to adequately compare its method with relevant prior work, particularly \"Taskonomy: Disentangling Task Transfer Learning.\" This omission is critical, as the concepts and formulations in the proposed method have strong similarities to those in Taskonomy.\n\n#### **3. Experimental Evaluation:**\n- **Datasets:** The experiments are conducted on several public datasets, but the datasets are primarily for multi-label learning. The authors need to clarify whether the same input X can appear in both the training and test datasets, which could affect the evaluation's thoroughness.\n- **Baselines:** The experimental section lacks a comprehensive comparison with relevant baselines. The authors should include a method equivalent to their proposed one but without the attention mechanism to demonstrate the importance of the attention component.\n- **Results:** The improvements over current methods are relatively slight, which raises questions about the significance of the proposed approach.\n\n#### **4. Rebuttal and Author Responses:**\n- **Rebuttal:** The authors have addressed some of the issues, such as improving the presentation and clarifying the differences with existing methods. However, the major issues, particularly the lack of a clear comparison with Taskonomy and the need for additional baselines, remain unaddressed.\n- **Author Responses:** The authors' responses to the reviewers' comments are helpful but do not fully resolve the critical issues identified by the reviewers.\n\n### Final Decision\n\nGiven the significant issues in clarity, technical soundness, and experimental evaluation, and the lack of a clear comparison with relevant prior work, the paper does not meet the standards of a top-tier conference. While the idea of modeling multi-level task dependencies is novel, the current presentation and evaluation are insufficient to support the claims made in the paper.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\nThe paper proposes a deep neural approach to model continuous time-evolving graphs using a temporal point process (TPP) framework. The key contributions include:\n1. **Temporal Point Process with LSTM**: The paper uses an LSTM to model the intensity function of the TPP, which is a common approach in the literature (e.g., Dai, 2017; Mei & Eisner, 2017; Trivedi, 2019).\n2. **Stacked LSTM with Fusion**: The paper stacks multiple LSTM layers to capture multiple time resolutions and fuses them using attention mechanisms.\n3. **Temporal Attention Mechanism**: The paper introduces a selection mechanism to give important nodes large influence in the interaction.\n\nWhile the use of LSTM with time gates and the stacked LSTM approach are incremental, the temporal attention mechanism is novel. However, the paper lacks a detailed justification and analysis of these contributions, which is a significant weakness.\n\n#### Empirical Evaluation\nThe paper demonstrates improved empirical performance on multiple datasets and tasks, including interaction prediction and classification. However, there are several concerns:\n1. **Ablation Studies**: The ablation studies are not detailed enough. The paper should provide more granular analysis, such as the impact of different K values, the effect of removing time gates, and the performance with different subgraph depths.\n2. **Comparison with Baselines**: The paper misses a comparison with a recently proposed and relevant baseline, JODIE (Kumar et al., 2019). This is a significant oversight, as JODIE also models interaction processes in a novel way.\n3. **Parameter Comparison**: The proposed method has a higher number of parameters compared to shallow models, which could explain some of the performance improvements. The paper should provide results for models with a comparable number of parameters.\n\n#### Clarity and Presentation\nThe paper is not easy to understand in detail:\n1. **Explanation of TPP and Graph Dynamics**: The paper lacks a clear explanation of how the TPP framework handles growing/shrinking graph nodes and changing interactions.\n2. **Overview Diagram**: An overview diagram or a toy example would greatly improve the readability of the paper.\n3. **Notation and Consistency**: The paper uses inconsistent notation and lacks equation numbers, making it difficult to reference specific parts of the model.\n\n#### Computational Cost\nThe paper does not provide information on the computational cost of the proposed network. It is important to know how large a graph can be and how fast its changes can be captured.\n\n### Final Decision\nGiven the incremental nature of some contributions, the lack of detailed justification and analysis, the missing comparison with a relevant baseline, and the issues with clarity and presentation, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper aims to provide a theoretical analysis of the convergence of stochastic gradient descent-ascent (SGD) in the context of Wasserstein GANs (WGANs) with a one-layer generator and simplified discriminators. The authors claim to show that SGD can converge to a global solution in polynomial time and sample complexity under these conditions. This is a significant theoretical contribution, as understanding the convergence properties of GANs, especially WGANs, is a challenging and important problem in the field of machine learning.\n\n#### Strengths\n1. **Theoretical Analysis**: The paper provides a rigorous theoretical analysis of the convergence of SGD in a simplified setting. This is a valuable contribution, as it helps to build a foundational understanding of the behavior of GANs.\n2. **Clarity and Completeness**: The paper is well-written, and the theoretical analysis appears to be valid and complete. The authors have provided clear explanations and derivations of their results.\n3. **Relevance**: The problem of understanding the convergence of GANs is highly relevant to the field of machine learning, and any progress in this area is valuable.\n\n#### Weaknesses\n1. **Simplified Discriminators**: The primary criticism from the reviewers is that the discriminators used in the analysis (linear and quadratic) are too simple and do not capture the complexity of real-world WGANs. This is a significant limitation, as the results may not generalize to more complex discriminators used in practice.\n2. **Limited Scope**: The analysis is limited to a one-layer generator and specific types of discriminators. While this is a starting point, it does not address the broader and more practical scenarios where GANs are used with multi-layer networks.\n3. **Misleading Claims**: The title and claims of the paper might be misleading. The term \"one-layer network\" can be confusing, and the focus on simplified discriminators might not align with the expectations set by the title and abstract.\n4. **Lack of Practical Impact**: The results, while theoretically sound, may not have a significant practical impact due to the simplified nature of the discriminators. The analysis does not provide insights into why WGANs are successful in practice with more complex discriminators.\n\n#### Author Responses\nThe authors have attempted to address the reviewers' concerns, but their responses have not fully resolved the issues. They have acknowledged the limitations of using simplified discriminators but have not provided a compelling argument for why these results are still valuable or how they can be extended to more complex settings.\n\n### Final Decision\nGiven the significant limitations in the scope and practical relevance of the paper, and the concerns raised by the reviewers, the paper should be rejected for publication at a top-tier conference. While the theoretical analysis is sound and the problem is important, the simplified setting and limited scope do not justify acceptance at a highly competitive venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Novelty and Relevance**:\n   - The paper introduces a novel connection between the expressivity of ReLU networks and Sharkovsky's Theorem from dynamical systems. This is a significant and innovative approach that has not been explored before in the context of deep learning.\n   - The work addresses an open question posed by Telgarsky, which is a well-known and important problem in the field of deep learning and approximation theory.\n\n2. **Theoretical Contributions**:\n   - The paper provides a clear and rigorous mathematical framework for understanding the depth-width trade-offs in ReLU networks.\n   - The lower bounds on the width needed to represent periodic functions as a function of depth are well-derived and theoretically sound.\n   - The proofs are clean and well-written, as noted by Reviewer 2.\n\n3. **Clarity and Exposition**:\n   - The paper is well-written and the exposition is clear. The motivating examples and the clean exposition of Sharkovsky's Theorem make the paper accessible to a broad audience.\n   - The synthetic dataset and additional experiments (as mentioned by Reviewer 1) add practical value to the theoretical results.\n\n4. **Impact**:\n   - The results have the potential to deepen our understanding of the representational power of deep neural networks and could inspire further research in this area.\n   - The paper could serve as a foundational reference for future work on depth-width trade-offs in neural networks.\n\n#### Areas for Improvement:\n1. **Structure and Presentation**:\n   - Reviewer 3 suggests that the paper could benefit from a more structured presentation. The main results (e.g., Theorem 4.1) should be stated earlier, and the technical details can follow. This would make the paper more accessible and engaging for readers.\n   - Providing more intuition for some of the definitions (e.g., Definition 3) would enhance the clarity and understanding of the paper.\n\n2. **Practical Relevance**:\n   - While the theoretical results are strong, the practical implications of the lower bounds on depth and width are not fully explored. Reviewer 3 raises a valid point about the difficulty of applying these results to real-world classification tasks.\n   - Including a synthetic dataset and a plot showing classification error versus depth (as suggested by Reviewer 2) would help contextualize the results and make them more tangible.\n\n3. **Bias Term Analysis**:\n   - Reviewer 2's suggestion to speculate on the impact of introducing a bias term is a valuable addition that could provide deeper insights into the robustness of the results.\n\n### Final Decision\nDespite the areas for improvement, the paper makes a significant and novel contribution to the field of deep learning and approximation theory. The theoretical results are strong, and the paper is well-written and well-motivated. The suggestions from the reviewers can be addressed in the final version to further enhance the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Title and Abstract**\nThe title \"Improved Training of Certifiably Robust Models\" and the abstract clearly outline the problem and the proposed solution. The paper aims to improve the training of neural networks to achieve tighter certification bounds using convex relaxations. This is a relevant and important topic in the field of adversarial robustness.\n\n#### **Reviewer Comments**\n\n**Reviewer 1:**\n- **Summary and Comments:**\n  - The reviewer points out several technical inaccuracies and confusions in the paper, particularly regarding the optimal convex relaxation and the conditions under which the relaxation is tight.\n  - The reviewer suggests that the proposed regularizers might not be effective as they aim to minimize the margin, which is counterproductive.\n  - The reviewer also notes that the experimental results lack certain baselines, specifically the results from Gowal et al. (2019).\n\n**Reviewer 2:**\n- **Summary and Comments:**\n  - The reviewer highlights the paper's interesting perspective on convex relaxation and the proposed regularizers.\n  - However, the reviewer points out several unclear points, such as the comparison between the unrelaxed and relaxed problems, and the effectiveness of the proposed relaxation.\n  - The reviewer also questions the practicality of the proposed method and suggests additional experiments to clarify the results.\n\n**Reviewer 3:**\n- **Summary and Comments:**\n  - The reviewer finds the problem interesting and the proposed regularizers promising.\n  - However, the reviewer points out presentation issues and the use of the MNIST dataset, which is not considered a strong benchmark for serious research.\n  - The reviewer suggests adding more datasets to the experiments.\n\n#### **Strengths**\n- **Problem Relevance:** The problem of improving certified robustness in neural networks is highly relevant and important in the field of machine learning.\n- **Proposed Solution:** The paper proposes two regularizers that aim to tighten the certification bounds, which is a novel and potentially impactful contribution.\n- **Experimental Results:** The experiments show that the proposed regularizations result in tighter certification bounds compared to non-regularized baselines.\n\n#### **Weaknesses**\n- **Technical Issues:** The paper contains several technical inaccuracies and confusions, particularly regarding the optimal convex relaxation and the conditions under which the relaxation is tight.\n- **Experimental Design:** The experimental results lack certain important baselines, such as the results from Gowal et al. (2019), and the use of the MNIST dataset is not sufficient to demonstrate the effectiveness of the proposed method.\n- **Clarity and Presentation:** The paper has some presentation issues and is not entirely clear in its theoretical justification and experimental design.\n\n#### **Final Decision**\nGiven the technical issues, lack of important baselines, and the use of a less challenging dataset, the paper does not meet the standards of a top-tier conference. While the problem and proposed solution are interesting, the paper needs significant revisions to address the technical and experimental issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel training strategy, MuPPET, which aims to reduce training time by dynamically switching between multiple precision levels, including low-precision fixed-point representations. The core idea is to leverage a precision-switching mechanism based on gradient diversity to optimize the training process. While the concept is innovative and addresses a significant challenge in deep learning, the novelty is somewhat limited by the reliance on existing concepts like gradient diversity.\n\n#### Technical Soundness\nThe technical approach is well-motivated, but the paper lacks clarity in several critical areas:\n1. **Precision Switching Mechanism**: The motivation behind the switching mechanism is not well-explained. The heuristic used to determine when to switch precision levels is borrowed from another work, but the paper does not provide a clear rationale for why this heuristic is appropriate in the context of MuPPET.\n2. **Algorithm Details**: The algorithm description is vague, particularly regarding the threshold parameters and the ratio \"p\". The lack of clarity in these details makes it difficult to understand and reproduce the results.\n3. **Experimental Setup**: The choice of quantized precision levels (8-, 12-, 14-, and 16-bit) is not well-justified. The paper does not explain how these specific levels were chosen or why they are optimal.\n4. **Results**: The results are mixed. While MuPPET achieves an average training-time speedup of 1.28×, the Top-1 validation accuracy on ImageNet for AlexNet and ResNet is lower than the FP32 baseline. This suggests that the precision-switching mechanism may not always be effective.\n\n#### Presentation and Clarity\nThe paper is generally well-written, but it suffers from several issues:\n1. **Typos and Grammar**: There are minor typos and grammatical errors that need to be corrected.\n2. **Figures and Tables**: The text in Figures 1 and 2 is too small and barely readable, which hinders the reader's ability to understand the results.\n3. **Algorithm Description**: The algorithm in Section 3.3 is not clearly explained, particularly the meaning of \"p\" and \"y\" in the precision-switching policy.\n4. **Experimental Details**: The paper lacks important details such as the use of data augmentation and the motivation behind the different precision choices for baseline and MuPPET experiments.\n\n#### Impact and Relevance\nThe topic of reducing training time and improving energy efficiency in deep learning is highly relevant and of significant interest to the research community. However, the paper's impact is limited by the lack of clear and convincing evidence that the proposed method outperforms existing state-of-the-art approaches. The results are not consistently superior, and the paper does not provide a thorough analysis of the factors that influence the performance of MuPPET.\n\n### Final Decision\nGiven the mixed results, lack of clarity in the algorithm description, and the need for more detailed experimental analysis, the paper does not meet the standards of a top-tier conference. While the idea is novel and the topic is relevant, the paper requires substantial revisions to address the technical and presentation issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Relevance and Contribution**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning.\n   - The use of Recurrent Neural Networks (RNNs), specifically bidirectional LSTM, is a novel approach in this context, and the results show that it outperforms other models.\n\n2. **Methodology**:\n   - The authors compare their model with several classical machine learning methods, providing a comprehensive evaluation.\n   - The paper introduces a weighted mean squared error (wMSE) loss function, which is a thoughtful approach to handle the specific characteristics of the data.\n\n3. **Results**:\n   - The bidirectional LSTM model outperformed other models, demonstrating the importance of memory in sequential DNA states for chromatin folding.\n   - The paper identifies informative epigenetic features, contributing to the biological understanding of DNA folding.\n\n#### Weaknesses and Concerns\n1. **Lack of Generalizability**:\n   - The paper lacks an illustration of the model's generalizability to other datasets or epigenetic features beyond ChIP-seq.\n   - The specific choice of the loss function (wMSE with a factor of 11) is not well-justified and may not be generalizable.\n\n2. **Methodological Clarity**:\n   - The description of the data and methods is not clear enough. For example, the representation of input features and the treatment of genomic segments of different chromosomes are not well-explained.\n   - The motivation for using a bidirectional LSTM is not clearly articulated, and the paper does not compare it to non-recurrent models like fully connected or convolutional networks.\n\n3. **Evaluation**:\n   - The evaluation needs to be strengthened by including additional baseline models and evaluation metrics.\n   - The authors should use the same loss function for training all models to understand if the performance differences are due to the loss function or model architecture.\n   - The hyper-parameters of the biLSTM and baseline methods are not clearly described.\n\n4. **Feature Importance**:\n   - The paper uses linear regression weights to quantify feature importance, but it is unclear if the biLSTM assigned the same importance to features. The authors should analyze the activations of LSTM units to better understand feature importance.\n\n5. **Comparative Analysis**:\n   - The paper is similar to previously published works, and these works are not cited, which raises concerns about the novelty of the approach.\n   - The authors should compare their weighted MSE to other loss functions, such as mean squared logarithmic error, to ensure that the performance improvement is not a by-product of the loss function.\n\n### Final Decision\nGiven the significant contributions of the paper in applying RNNs to predict DNA folding patterns and the identification of informative epigenetic features, the paper has the potential to be valuable. However, the concerns regarding generalizability, methodological clarity, and evaluation need to be addressed to meet the standards of a top-tier conference.\n\nFinal Decision: **Reject**\n\nThe authors should address the major concerns and resubmit the paper for further consideration."
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\nThe paper introduces a novel problem in the field of singing voice generation: generating singing voices without pre-assigned scores and lyrics. This is a significant contribution as it opens up new possibilities in music generation and could potentially lead to more creative and flexible applications. The authors explore three different schemes: free singer, accompanied singer, and solo singer, each with its own unique challenges and potential applications.\n\n#### Methodology\nThe authors have developed a pipeline that includes data preparation, adversarial networks for audio generation, and customized metrics for evaluation. They have also provided code, which is a positive aspect for reproducibility. However, the methodology has several issues that need to be addressed:\n\n1. **Data Preparation**: The training data is described as in-house and covers diverse musical genres, but there is a lack of clarity about the specific genres, the nature of the accompaniments, and the process of data collection. This ambiguity makes it difficult to assess the quality and relevance of the training data.\n\n2. **Model Architecture**: The choice of the BEGAN architecture for sequence generation is not well-justified. The authors mention pilot experiments but do not provide concrete evidence or comparisons with other architectures. The conjecture about information loss due to compressing the output of the generator is not supported by existing literature or experimental results.\n\n3. **Evaluation Metrics**: The evaluation metrics (vocalness, average pitch, MOS) are not well-suited for assessing the quality of singing voice generation. Vocalness and average pitch do not capture the nuances of singing, such as melody, rhythm, and expressiveness. The MOS study lacks a baseline for comparison, making it difficult to judge the performance of the proposed models.\n\n#### Experimental Results\nThe provided audio samples are a valuable addition, but they are not sufficient to evaluate the effectiveness of the models without a clear understanding of the training data and a proper baseline for comparison. The audio samples are described as randomly sampled, which is good, but the lack of context and baseline makes it challenging to assess their quality.\n\n#### Motivation and Relevance\nThe motivation for the three different settings (free singer, accompanied singer, solo singer) is not well-articulated. It is not clear why these settings are chosen and what they aim to achieve. A more detailed discussion on the potential applications and the expected performance differences among the settings would strengthen the paper.\n\n#### Literature Review\nThe literature review is generally well-done but could be improved. The paper does not cite relevant works such as MelNet, which is a recent and relevant method for unconditional audio generation. Comparing the proposed method with existing approaches, even if they are not directly comparable, would provide a better context for the evaluation.\n\n### Final Decision\nGiven the significant novelty and the potential impact of the proposed problem, the paper has a strong foundation. However, the lack of a clear and well-justified methodology, the inadequacy of the evaluation metrics, and the absence of a proper baseline for comparison are major concerns. These issues need to be addressed to ensure the robustness and validity of the results.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Relevance and Novelty**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning.\n   - The use of bidirectional LSTM (biLSTM) to capture the sequential nature of DNA is a novel approach in this context.\n\n2. **Methodology**:\n   - The authors compare their biLSTM model with several other machine learning models, including linear models with regularization, Gradient Boosting, and Recurrent Neural Networks.\n   - The biLSTM model outperforms other models, which is a strong result.\n\n3. **Data and Results**:\n   - The paper uses Hi-C data and ChIP-Seq data from Drosophila melanogaster, which are relevant and well-established datasets in the field.\n   - The results are presented with clear metrics, and the authors identify informative epigenetic features.\n\n#### Weaknesses and Concerns\n1. **Lack of Generalizability**:\n   - The paper does not demonstrate the generalizability of the model to other datasets or organisms. This is a significant concern, as the model's performance on a single dataset may not be indicative of its broader utility.\n   - The specific choice of the weighted mean squared error (wMSE) with a fixed parameter (11) is not well-justified and may not be generalizable to other datasets.\n\n2. **Methodological Clarity**:\n   - The description of the data and methods is not clear enough. For example, the representation of input features and the treatment of genomic segments are not well-explained.\n   - The motivation for using a biLSTM is not well-articulated. The authors should provide a clearer explanation of why a biLSTM is suitable for this task, especially in comparison to other models.\n\n3. **Evaluation Metrics**:\n   - The evaluation metrics used (wMSE) are not consistent across all models. The authors should use the same loss function for training all models to ensure a fair comparison.\n   - Additional evaluation metrics such as MSE and R^2 score should be reported to provide a more comprehensive assessment of model performance.\n\n4. **Feature Importance**:\n   - The method for quantifying feature importance using linear regression weights is not appropriate for a biLSTM model. The authors should use techniques like activation analysis or feature attribution methods to understand the importance of features in the biLSTM model.\n\n5. **Hyperparameter Tuning**:\n   - The hyperparameters of the biLSTM and baseline methods are not clearly described. The authors should provide details on the hyperparameter tuning process to ensure reproducibility.\n\n6. **Comparative Analysis**:\n   - The paper lacks a comparison with other relevant works in the field, which are not cited. This diminishes the novelty of the approach.\n   - The authors should compare their model with other recurrent models and non-recurrent models like fully connected or convolutional networks.\n\n### Final Decision\nGiven the significant concerns regarding the lack of generalizability, methodological clarity, and the need for a more comprehensive evaluation, the paper does not meet the standards of a top-tier conference. While the results are promising, the paper requires substantial revisions to address these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in machine learning, specifically the challenge of training classifiers with a large number of classes (extreme classification). This is a relevant and important topic with practical applications in various domains.\n\n2. **Methodological Contribution:**\n   - The proposed method of adversarial negative sampling is innovative and addresses a key issue in existing methods: the poor signal-to-noise ratio in uniform negative sampling. The authors provide a clear and detailed explanation of their approach, including the mathematical proof that the adversarial sampling minimizes gradient variance.\n\n3. **Experimental Results:**\n   - The experimental results on large-scale datasets (Wikipedia-500K and Amazon-670K) demonstrate a significant reduction in training time compared to several competitive baselines. This is a strong empirical validation of the proposed method's effectiveness.\n\n4. **Clarity and Presentation:**\n   - The paper is well-written and accessible, even to non-experts. The authors provide a clear introduction to the problem and a detailed explanation of their method, making the paper educational and useful for a broader audience.\n\n#### **Weaknesses of the Paper:**\n1. **Lack of Comprehensive Literature Review:**\n   - The paper does not adequately discuss relevant prior work, particularly methods that also aim to sample negatives in a non-uniform manner. For example, the works by Reddi et al., Grave et al., Blanc and Rendle, and Rawat et al. should be discussed to put the proposed method in the right context.\n\n2. **Limited Baseline Comparisons:**\n   - The experimental evaluation is incomplete. The paper does not compare the proposed method with the full softmax loss (eq. 1) on smaller datasets, which would provide a more comprehensive understanding of its performance. Additionally, it does not include recent state-of-the-art methods like Slice and DiSMEC, which are important for a thorough comparison.\n\n3. **Evaluation Metrics and Datasets:**\n   - The paper does not evaluate the performance on tail-labels using appropriate metrics such as MacroF1. This is crucial because the fat-tailed distribution of instances among labels is a common property of extreme classification datasets. The proposed method should be tested on more datasets, including smaller ones like EURLex and larger ones like Amazon3M, to provide a more robust evaluation.\n\n4. **Multi-Label Setting:**\n   - The paper does not investigate whether the proposed method can be extended to the multi-label setting. This is an important consideration because many real-world applications of extreme classification involve multi-label problems.\n\n5. **Minor Issues:**\n   - There are a few minor issues with the writing, such as grammatical errors and typos in the tables, which should be fixed.\n\n### Final Decision\n\nGiven the significant contributions of the paper in addressing the problem of extreme classification and the strong empirical results, it has the potential to be a valuable addition to the literature. However, the lack of comprehensive literature review, limited baseline comparisons, and the need for more extensive evaluation on different datasets and metrics are substantial weaknesses that need to be addressed.\n\n**Final Decision: [Reject]**\n\nThe paper should be rejected in its current form but with the strong recommendation that the authors address the identified issues and resubmit a revised version. This will ensure that the paper meets the high standards of a top-tier conference."
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Theoretical Contributions**:\n   - The paper provides a comprehensive theoretical analysis of the impact of activation functions on the training of overparametrized neural networks. This is a significant contribution to the field of deep learning theory.\n   - The authors distinguish between non-smooth and smooth activation functions, providing clear conditions under which the minimum eigenvalue of the Gram matrix is large or small. This distinction is crucial for understanding the convergence properties of neural networks.\n   - The paper builds on and extends existing theoretical results, such as those from Du et al. (2019a) and Allen-Zhu et al. (2019), by providing more detailed and quantitative bounds.\n\n2. **Empirical Validation**:\n   - The authors include empirical experiments on both synthetic data and the CIFAR10 dataset. These experiments support the theoretical findings, particularly for non-smooth activation functions like ReLU.\n   - The inclusion of experiments that highlight the gap between theory and practice (e.g., on CIFAR10) is a strength, as it provides a realistic perspective on the practical implications of the theoretical results.\n\n3. **Clarity and Organization**:\n   - The paper is well-written and well-referenced, making it accessible to researchers in the field.\n   - The authors provide a clear problem setup and a detailed explanation of the G-matrix and its role in the convergence rate of training error.\n\n#### Weaknesses:\n1. **Appendix Length**:\n   - The appendix is quite long, which can make the paper difficult to navigate for a general audience. While the main results are well-explained in the main text, a more focused and organized appendix would enhance the readability of the paper.\n   - The reviewer suggests a list of extensions to clarify what is included in the appendix, which would be beneficial.\n\n2. **Clarity of Extensions**:\n   - Some of the extensions and results in the appendix are not clearly referenced in the main text. This can make it challenging for readers to understand the full scope of the paper's contributions.\n   - The reviewer suggests avoiding references to theorems in the appendix that do not have informal statements in the main paper.\n\n3. **Real-World Data Dimensionality**:\n   - The paper discusses the behavior of the minimum eigenvalue in the context of data dimensionality. While the theoretical results are strong, it would be useful to provide a more detailed discussion of how these results apply to real-world data, where the dimensionality is often much larger than log n.\n\n#### Additional Comments:\n1. **Explanation of Terms**:\n   - The reviewer suggests providing an explanation for the term \"DZXP setting\" and correcting some minor notational issues (e.g., \"grad_W F\" instead of \"L\" on page 4).\n\n2. **Further Analysis**:\n   - The reviewer suggests discussing the gap between the lower bound from Theorem J.4.2 and the upper bound from Theorem F.9 in the context of data dimensionality greater than log n.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper provides significant theoretical insights into the impact of activation functions on the training of overparametrized neural networks. The empirical validation supports the theoretical findings, and the paper is well-written and well-referenced. While there are some issues with the length and organization of the appendix, these can be addressed in the final version of the paper. The contributions are strong enough to warrant acceptance at a top-tier conference."
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Motivation**:\n   - The paper introduces a novel approach to continual learning by integrating Hebbian learning principles into a differentiable framework. This is a refreshing and innovative approach that addresses the problem of catastrophic forgetting in a unique way.\n   - The method is well-motivated by cognitive and neurobiological principles, which adds depth to the theoretical foundation of the work.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and clearly structured. The authors provide a thorough background and motivation for their approach, making it accessible to readers who may not be familiar with Hebbian learning.\n   - The inclusion of a PyTorch pseudocode and the promise of releasing the code are significant strengths, as they facilitate reproducibility and further research.\n\n3. **Empirical Evaluation**:\n   - The authors evaluate their method on several benchmarks, including Permuted MNIST, Split MNIST, and Vision Datasets Mixture. They also introduce an imbalanced variant of Permuted MNIST, which is a valuable contribution to the field.\n   - The results show that the proposed method can reduce forgetting and outperform some baselines, particularly when combined with other task-specific synaptic consolidation methods.\n\n#### Weaknesses of the Paper\n1. **Empirical Evaluation**:\n   - While the paper provides results on several benchmarks, the performance of the proposed method is not consistently strong. On some datasets, such as Split CIFAR-10/100, the method performs poorly compared to other methods like Synaptic Intelligence (SI).\n   - The authors are encouraged to conduct more extensive experiments, especially on more challenging datasets like CIFAR-100 and ILSVRC, to demonstrate the scalability and robustness of their method.\n\n2. **Comparisons to Other Methods**:\n   - The paper lacks a comprehensive comparison to other CLS-based approaches. The authors should provide a more detailed comparison to ensure that their method is evaluated against the state-of-the-art.\n   - The baseline methods used in the experiments, such as the finetuned MLP, may be too weak, which could undermine the significance of the results.\n\n3. **Clarity and Explanation**:\n   - Some aspects of the method, particularly the DHS Softmax layer, are not clearly explained. The reviewers have pointed out several areas where the paper could benefit from additional figures and equations to clarify the concepts.\n   - The motivation for applying the Hebbian strategy only to the final softmax layer is not well-explained, which could be a point of confusion for readers.\n\n#### Final Considerations\n- **Impact and Contribution**:\n  - Despite the weaknesses, the paper makes a valuable contribution to the field of continual learning by introducing a novel approach that integrates Hebbian learning principles. The method has the potential to inspire further research and development in this area.\n- **Addressing Reviewer Concerns**:\n  - The authors should address the concerns raised by the reviewers, particularly by providing more detailed explanations, additional experiments, and a more comprehensive comparison to other methods.\n- **Reproducibility and Accessibility**:\n  - The inclusion of PyTorch pseudocode and the promise of releasing the code are significant strengths that enhance the paper's value.\n\n### Final Decision\nFinal Decision: [Accept]\n\nThe paper introduces a novel and well-motivated approach to continual learning, and while it has some weaknesses in its empirical evaluation and clarity, the overall contribution is significant. The authors should address the reviewer concerns in the final version to strengthen the paper."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\n- **Incremental Nature:** The paper introduces a method for training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. While the idea of using SGD for GMMs is not entirely new, the paper proposes several novel techniques to make this approach feasible and stable.\n- **Key Contributions:**\n  - **Max-Component Approximation:** The authors propose minimizing an upper bound to the GMM log likelihood to improve numerical stability. However, this is a standard technique (log-sum-exp trick) and the authors do not adequately justify why their approach is superior.\n  - **Regularization:** The paper introduces a new regularizer to prevent SGD from converging to pathological local minima. This is a significant contribution, but the motivation and effectiveness of the regularizer are not well-explained.\n  - **Constraint Enforcement:** The authors present a method for enforcing constraints inherent to GMM training when using SGD. This is a practical contribution, but the computational efficiency of the method (e.g., QR decomposition) is not thoroughly evaluated.\n  - **Local Principal Directions:** The paper proposes a simplification to the full GMM model based on local principal directions to reduce memory usage. This is a useful contribution, but the implementation details are unclear.\n\n#### **2. Technical Soundness:**\n- **Numerical Stability:** The max-component approximation is claimed to improve numerical stability, but the authors do not compare it to the log-sum-exp trick, which is a standard method for handling numerical issues in log-likelihood computations.\n- **Regularization:** The regularizer is a novel contribution, but its effectiveness is not rigorously justified. The gridding approach is difficult to understand and lacks a clear theoretical foundation.\n- **Constraint Enforcement:** The method for enforcing constraints is practical but computationally expensive, and the impact on efficiency is not evaluated.\n- **Covariance Matrices:** The paper focuses on diagonal covariance matrices, which are easier to handle. The low-rank covariance matrix approach is mentioned but not thoroughly evaluated.\n\n#### **3. Experimental Evaluation:**\n- **Datasets:** The experiments are conducted on standard image datasets (MNIST, SVHN), but the dimensionality (28x28) is not very high. This limits the applicability of the results to truly high-dimensional data.\n- **Comparisons:** The paper compares the proposed method to EM with K-means initialization but lacks a comprehensive comparison in terms of convergence, running time, and hyperparameter tuning. The results are not convincing enough to demonstrate a clear advantage over existing methods.\n- **Hyperparameters:** The selection of hyperparameters is described as a manual process, which is a limitation. More automated methods for hyperparameter tuning would strengthen the paper.\n\n#### **4. Clarity and Presentation:**\n- **Writing:** The paper is generally well-written and clear, but there are several issues that need to be addressed:\n  - **Repetition:** The experimental details are repeated in the text.\n  - **Confusion:** Some sections (e.g., Section 3.3) are confusing and lack clear explanations.\n  - **Details:** The paper lacks important details, such as the specific log-likelihood used in the experiments and the fraction of data used for evaluation.\n\n### Final Decision\nGiven the incremental nature of the contributions, the lack of rigorous justification for the proposed techniques, and the limited experimental evaluation, the paper does not meet the standards of a top-tier conference. While the paper presents some interesting ideas, they are not sufficiently developed or evaluated to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\nThe paper introduces ROOTS, a generative model for unsupervised 3D scene decomposition and rendering. The key contributions are:\n- **Object-oriented representation**: ROOTS provides a hierarchical representation that is both 3D and 2D, which is a significant step forward from the GQN framework.\n- **Viewpoint invariance**: The model ensures that the object representations are viewpoint invariant, which is a crucial property for robust 3D scene understanding.\n- **Disentanglement and compositionality**: The model demonstrates strong abilities in disentangling and composing 3D scenes, which are essential for generalization and transfer learning.\n\nHowever, the novelty claims need to be refined. The paper states that it is the \"first unsupervised model that can identify objects in a 3D scene,\" which is not accurate given the existence of models like MONet and Iodine. The authors should acknowledge these related works and clearly differentiate their contributions.\n\n#### **2. Methodology:**\nThe methodology is complex and innovative, but the presentation is problematic:\n- **Clarity and notation**: The paper uses non-standard notation and is inconsistent with the GQN framework, making it difficult for readers to follow. The reviewers have pointed out several issues with the notation and the clarity of the method section.\n- **Ablation studies**: The lack of ablation studies is a significant weakness. It is crucial to understand the impact of each component of the model, especially given the complexity of the architecture. The authors should provide ablation studies to demonstrate the importance of the 3D grid, hierarchical representation, and unsupervised object representation.\n\n#### **3. Experiments:**\nThe experimental results are mixed:\n- **Qualitative results**: The qualitative results are impressive, showing the model's ability to decompose scenes, swap objects, and render from different viewpoints.\n- **Quantitative results**: The quantitative results are weak and lack depth. The comparisons with GQN are not robust, and the NLL results are not explained well. The object detection quality experiment is incomplete and lacks a clear baseline.\n- **Generalization**: The paper claims strong generalization capabilities but does not provide sufficient evidence. The authors should test the model on more complex datasets and compare it with other state-of-the-art methods.\n\n#### **4. Writing and Presentation:**\nThe writing and presentation are poor:\n- **Typos and grammatical errors**: The paper is riddled with typos and grammatical errors, which detract from the overall quality.\n- **Clarity and organization**: The paper is poorly organized, and the notation is inconsistent. The figures and tables are not well-explained, and the text often refers to concepts that are not introduced.\n\n#### **5. Related Work:**\nThe related work section is inadequate:\n- **Citations**: The paper lacks proper citations to relevant works in 3D representation learning and neural rendering. The authors should cite and compare their work with methods like 3D-R2N2, DeepVoxels, and other relevant works.\n- **Positioning**: The paper should better position itself with respect to the existing literature, clearly stating the differences and contributions.\n\n### Final Decision\nGiven the significant issues with clarity, methodology, and experimental robustness, the paper needs substantial revisions before it can be considered for publication in a top-tier conference. The authors should address the following:\n- **Refine novelty claims and related work**.\n- **Improve clarity and notation**.\n- **Conduct ablation studies**.\n- **Strengthen experimental results and comparisons**.\n- **Revise the writing and presentation**.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\n1. **Novelty**:\n   - The paper introduces a novel approach to Mahalanobis metric learning by formulating it as an optimization problem and providing a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time.\n   - The use of tools from the theory of linear programming in low dimensions to achieve this result is a significant contribution, as it leverages existing theoretical results in a new context.\n\n2. **Contribution**:\n   - The paper provides a theoretical foundation for the proposed method, including the development of an FPTAS.\n   - The experimental results on synthetic and real-world data sets demonstrate the practical effectiveness of the algorithm, particularly in the presence of adversarial noise.\n   - The algorithm is fully parallelizable, which is a valuable property for large-scale applications.\n\n#### Technical Soundness\n1. **Theoretical Soundness**:\n   - The paper presents a clear and rigorous mathematical formulation of the problem and the proposed solution.\n   - The use of linear programming in low dimensions to achieve the FPTAS is well-justified and supported by existing theoretical results.\n   - The proofs provided in the paper are detailed and appear to be correct, although some minor clarifications are needed (e.g., the definition of combinatorial dimension in Lemma 2.1).\n\n2. **Experimental Validation**:\n   - The experimental results are comprehensive and include comparisons with classical methods like ITML and LMNN.\n   - The results show that the proposed method performs favorably, particularly in the presence of adversarial noise.\n   - The experiments are conducted on both synthetic and real-world data sets, which adds to the robustness of the evaluation.\n\n#### Clarity and Presentation\n1. **Clarity**:\n   - The paper is generally well-written and easy to follow.\n   - The problem formulation and the proposed solution are clearly explained.\n   - However, some sections could benefit from additional clarity and detail (e.g., the definition of \"accuracy\" in Theorem 1.1 and the explanation of the computational hardness of the problem).\n\n2. **Presentation**:\n   - The paper is well-structured, with a clear introduction, problem formulation, solution, and experimental results.\n   - The figures and tables are well-presented and provide useful insights into the performance of the algorithm.\n   - However, the paper lacks a conclusion, which is a significant oversight.\n\n#### Addressing Reviewer Comments\n1. **Reviewer 1**:\n   - The paper should discuss recent state-of-the-art methods and compare with them.\n   - The proof for Lemma 2.1 should be clarified.\n   - The running time curve in Figure 4 should be explained.\n   - A conclusion should be added.\n\n2. **Reviewer 2**:\n   - The computational hardness of the problem should be discussed.\n   - The definition of \"accuracy\" should be clarified.\n   - The combinatorial dimension should be defined before use.\n   - The function call in Algorithm 1 should be corrected.\n   - The assumption that the dimension \\( d \\) is a constant should be made clear.\n   - The known results for minimizing the number of constraints should be stated.\n\n3. **Reviewer 3**:\n   - The novelty of the approach should be better motivated.\n   - The theoretical guarantees and key differences with other methods should be clearly stated.\n   - The need for the approximation should be explained.\n\n### Final Decision\nDespite the minor issues and the need for some clarifications, the paper presents a significant and novel contribution to the field of Mahalanobis metric learning. The theoretical foundation is strong, and the experimental results are compelling. The paper addresses an important problem and provides a practical and efficient solution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Problem Relevance and Novelty**:\n   - The paper addresses a significant problem in multi-agent reinforcement learning (MARL) by proposing a modular framework for skill coordination. The problem of coordinating multiple agents to perform complex tasks is both challenging and relevant, especially in robotics and autonomous systems.\n   - The use of skill behavior diversification to learn sub-skills and then coordinate them is a novel approach that adds to the existing literature on hierarchical reinforcement learning (HRL) and MARL.\n\n2. **Empirical Evaluation**:\n   - The authors provide empirical results on a variety of challenging tasks, including bimanual manipulation and multi-agent locomotion. The tasks are well-chosen and demonstrate the effectiveness of the proposed method.\n   - The method outperforms the baselines, which is a strong indicator of its practical utility.\n\n3. **Clarity and Presentation**:\n   - The paper is generally well-written and well-structured. The problem is clearly defined, and the proposed method is explained in a logical manner.\n   - The inclusion of videos and code is a significant plus, as it allows for reproducibility and further exploration by the research community.\n\n#### Weaknesses and Concerns\n\n1. **Notational Consistency and Clarity**:\n   - Reviewer 1 points out that the notations are inconsistent and confusing, which can make the paper difficult to follow. This is a valid concern and should be addressed to improve the clarity of the paper.\n   - The diagram (Figure 2) could benefit from more detailed annotations to help readers understand the overall framework.\n\n2. **Temporal Abstraction and Meta-Policy**:\n   - The choice of \\( T_{\\text{low}} = 1 \\) in the Jaco environments is concerning, as it suggests that the meta-policy is treating skills as primitive actions rather than temporally extended behaviors. This should be clarified and potentially re-evaluated.\n   - The high variance in performance, as noted in Figure 4, is another concern. Running more seeds and providing a more detailed analysis of the variance would strengthen the empirical results.\n\n3. **Hyper-Parameter Sensitivity**:\n   - The sensitivity of the hyper-parameters balancing diversity and external reward (Equation 4) is discussed, but the results are reported using \"Episode reward\" rather than success rate. Consistency in reporting metrics would improve the clarity of the results.\n\n4. **Multi-Agent Baselines**:\n   - Reviewer 1 suggests that the paper could benefit from a more in-depth discussion of alternative multi-agent methods. While the provided baseline is fair, recent advancements in MARL should be considered and discussed.\n\n5. **Temporal Abstraction in Multi-Agent Settings**:\n   - Reviewer 2 raises important questions about the differences between temporal abstraction (TA) in single-agent and multi-agent settings. The paper should address these questions to provide a clearer understanding of the unique challenges and benefits of TA in multi-agent systems.\n\n6. **Pre-Defined Subtasks**:\n   - Reviewer 3 points out that the method relies on pre-defined subtasks, which limits its wide applicability. The authors should consider using the DIAYN objective alone or with a larger latent skill vector to evaluate the importance of pre-defined subtasks.\n\n#### Final Decision\n\nDespite the weaknesses and concerns, the paper presents a novel and well-motivated approach to multi-agent skill coordination. The empirical results are strong, and the problem addressed is highly relevant. Addressing the notational inconsistencies, clarifying the choice of \\( T_{\\text{low}} \\), and providing a more detailed analysis of the variance and hyper-parameter sensitivity would significantly improve the paper.\n\nGiven the potential impact and the quality of the empirical evaluation, I believe the paper should be accepted with the condition that the authors address the reviewers' concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Problem and Motivation**\nThe paper addresses the problem of learning reusable sub-task primitives from human demonstrations using a weakly-supervised approach. This is a significant and relevant problem in the field of robot learning and control. The motivation is clear: direct manual supervision is tedious and prone to bias, and the ability to extract reusable skills from demonstrations can greatly enhance the adaptability of robotic systems.\n\n#### 2. **Methodology**\nThe proposed method is inspired by multiple instance learning (MIL) and involves training a classifier to label each observation per time-step with the probability of skills corresponding to that observation. These predictions are then accumulated throughout the trajectory to compute the probability of the skill in that trajectory. The method is end-to-end trainable and works directly from high-dimensional input (e.g., images) with only trajectory-level supervision.\n\n#### 3. **Experiments and Results**\nThe paper evaluates the method across four environments, ranging from simulation to real-world robots, and from procedurally generated to human-collected demonstrations. The results show that the model can achieve decent skill-level classification scores. However, there are several issues with the experiments and results:\n\n- **Scope and Limitations**: The experiments are limited in scope, with at most 4-6 primitive skills in each environment. The reviewer comments suggest that the method may overfit to the provided trajectories, and there is a lack of evidence that the learned segmentations are useful for training policies or solving downstream tasks.\n- **Evaluation Metrics**: The classification results are reported using accuracy, which is not sufficient to assess the quality of the segmentations. Confusion matrices, precision, recall, and other metrics are needed to provide a more comprehensive evaluation.\n- **Baseline Comparisons**: The paper lacks comparisons with relevant baselines, particularly unsupervised clustering methods. This makes it difficult to assess the novelty and effectiveness of the proposed approach.\n- **Behavioral Cloning**: The behavioral cloning results are underwhelming, with a success rate of only 50% on the task where the correct sequence of skills is known. There is no comparison to a fully-supervised oracle baseline, which is crucial for understanding the gap between fully- and weakly-supervised methods.\n\n#### 4. **Clarity and Presentation**\nThe paper is generally well-written, but there are several areas where the clarity and presentation can be improved:\n\n- **Experimental Setup**: The details of the task setup, the number of demonstrations needed per task/skill, architectural choices, and hyper-parameters are lacking. This makes it difficult to reproduce the experiments and understand the results.\n- **Qualitative Results**: The qualitative results, such as the logits of predictions per class and trajectory classification, are not provided. This is particularly important for understanding the confidence of the model and the consistency of the segmentations over time.\n- **Theoretical Analysis**: The paper does not provide a theoretical analysis of the minimum dataset requirements for learning all skills. This is a critical aspect that needs to be addressed to understand the feasibility of the approach.\n\n#### 5. **Reviewer Comments**\nThe reviewer comments highlight several significant issues with the paper:\n\n- **Reviewer 1**: Points out that the framing and motivation of the paper do not correspond to the results and experiments reported. The paper is limited in scope and lacks further experiments and comparisons to relevant baselines.\n- **Reviewer 2**: Notes that the novelty of the approach is low, and the results are underwhelming. The reviewer suggests including oracle fully-supervised results and more thorough baselines.\n- **Reviewer 3**: Emphasizes the need for a theoretical analysis of the dataset requirements and stronger evidence of the method's effectiveness. The reviewer also suggests including unsupervised clustering methods as baselines.\n\n### Final Decision\nGiven the significant issues with the experiments, results, and clarity of the paper, and the lack of evidence that the proposed approach is effective and novel, I recommend that the paper be rejected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Problem Relevance and Novelty**:\n   - The paper addresses a significant problem in multi-agent reinforcement learning (MARL) by focusing on the coordination of multiple end-effectors or agents. This is a crucial area of research, especially for complex manipulation tasks.\n   - The proposed modular framework is novel in its approach to learning diverse sub-skills and then coordinating them. The use of skill behavior diversification is a creative and effective method to enhance the adaptability of the learned skills.\n\n2. **Empirical Evaluation**:\n   - The paper provides a thorough empirical evaluation on a variety of challenging tasks, including manipulation and locomotion. The results demonstrate that the proposed method outperforms the baselines, which is a strong indicator of its effectiveness.\n   - The inclusion of videos and code is a significant plus, as it allows for transparency and reproducibility.\n\n3. **Clarity and Presentation**:\n   - The paper is generally well-written and well-structured. The problem is clearly defined, and the proposed method is explained in a logical and understandable manner.\n\n#### Weaknesses and Concerns\n\n1. **Notational Inconsistencies**:\n   - Reviewer 1 points out that the notations are sometimes inconsistent and confusing. This can make it difficult for readers to follow the technical details. The authors should address this by providing more detailed explanations and consistent notation.\n\n2. **Temporal Extended Behavior**:\n   - Reviewer 1 raises a concern about the meta-policy treating skills as primitive actions rather than temporally extended behaviors. This is a valid concern, and the authors should provide a more detailed explanation of why this choice was made and its implications.\n\n3. **High Variance in Performance**:\n   - The training curves show high variance in performance, which is concerning. The authors should run more seeds to improve the robustness of their results and provide a more detailed analysis of the variance.\n\n4. **Alternative Multi-Agent Methods**:\n   - Reviewer 1 suggests that the paper should provide a more in-depth discussion of alternative multi-agent methods. While the provided baseline is fair, the paper could benefit from a broader comparison to recent work in this area.\n\n5. **Temporal Abstraction in Multi-Agent Settings**:\n   - Reviewer 2 raises important questions about the differences between temporal abstraction in single-agent and multi-agent settings. The authors should address these questions to provide a clearer understanding of the unique challenges and benefits of their approach in the multi-agent context.\n\n6. **Pre-Defined Subtasks**:\n   - Reviewer 3 points out that the method relies on pre-defined subtasks, which limits its wide applicability. The authors should consider whether the DIAYN objective alone could be used to encourage sufficiently diverse behaviors without pre-defined subtasks. This would make the method more general and applicable to a wider range of tasks.\n\n7. **Latent Skill Embedding Size**:\n   - Reviewer 3 also notes that the size of the latent skill embedding is not reported. This information is crucial for understanding the method and should be included in the paper.\n\n### Final Decision\n\nDespite the noted weaknesses and concerns, the paper presents a novel and effective approach to multi-agent coordination in reinforcement learning. The empirical results are strong, and the method has the potential to be impactful in the field. However, the authors need to address the concerns raised by the reviewers to improve the clarity, robustness, and generalizability of their work.\n\nFinal Decision: Accept\n\nThe authors should carefully address the following points in their revision:\n- Clarify and standardize the notation.\n- Provide a more detailed explanation of the meta-policy's treatment of skills as temporally extended behaviors.\n- Run more seeds to reduce the variance in performance and provide a detailed analysis.\n- Include a more in-depth discussion of alternative multi-agent methods.\n- Address the questions about temporal abstraction in multi-agent settings.\n- Consider the use of the DIAYN objective alone and report the results.\n- Include the size of the latent skill embedding."
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **1. Problem Setting and Novelty**\nThe paper addresses the problem of out-of-distribution (OOD) detection in a constrained setting where no OOD samples are available for validation and the model parameters should remain unchanged. This is a relevant and challenging problem in real-world applications, especially in safety-critical domains like autonomous driving. The proposed method, MALCOM, is novel in its use of the Normalized Compression Distance (NCD) to extract informative sequential patterns from feature maps. This approach is innovative and has the potential to contribute to the field of OOD detection.\n\n#### **2. Methodology and Technical Soundness**\nThe methodology is well-explained, and the use of global average pooling and spatial patterns in feature maps is a reasonable approach to identify OOD samples. However, there are several concerns regarding the technical soundness and fairness of the comparisons:\n\n- **Comparison with Baselines**: The paper's comparison with the Mahalanobis detector is not entirely fair. The Mahalanobis method can be validated using adversarial samples, which should be considered in the comparison. The authors' implementation of the Mahalanobis method seems to underperform compared to the original paper, which raises questions about the validity of the comparison. The authors should provide both the original performance numbers and their replication results to ensure transparency.\n\n- **Ablation Study**: The ablation study is limited. The authors should conduct a more thorough ablation study to understand the contribution of each component of their method. This would help in validating the effectiveness of the proposed approach.\n\n#### **3. Experimental Evaluation**\nThe experimental evaluation is comprehensive, covering several datasets and different neural network architectures. However, the experiments could be more robust:\n\n- **Complex Settings**: The paper could benefit from experiments in more complex and realistic settings, such as those found in autonomous driving. The authors should demonstrate the performance of their method in scenarios where inputs are similar but have subtle differences, which are often the most challenging for OOD detection.\n\n- **Theoretical Guarantees**: The paper lacks theoretical guarantees showing the exhaustiveness of the proposed method in detecting all possible OOD examples. This is a significant gap, as theoretical guarantees are crucial for building trust in safety-critical applications.\n\n#### **4. Clarity and Presentation**\nThe paper is generally well-written, but there are some issues with clarity and presentation:\n\n- **Abstract**: The first sentence of the abstract is grammatically incorrect and should be revised for clarity.\n- **Proofreading**: The paper could benefit from additional proofreading to correct minor errors and improve readability.\n\n#### **5. Relevance and Impact**\nThe problem addressed by the paper is relevant and important, especially in safety-critical applications. The proposed method has the potential to make a significant impact if the technical issues are addressed. However, the current state of the paper, with its limited comparison and lack of theoretical guarantees, does not fully support the claims made.\n\n### Final Decision\nFinal Decision: **Reject**\n\n### Rationale for Rejection\nWhile the problem setting is relevant and the proposed method is novel, the paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference:\n\n1. **Unfair Comparison with Baselines**: The comparison with the Mahalanobis method is not fair, and the authors' implementation underperforms compared to the original paper. This raises concerns about the validity of the results.\n2. **Limited Ablation Study**: The ablation study is insufficient to fully understand the contribution of each component of the proposed method.\n3. **Lack of Complex Experiments**: The experiments are not conducted in complex and realistic settings, which are crucial for evaluating the effectiveness of OOD detection methods.\n4. **Lack of Theoretical Guarantees**: The paper lacks theoretical guarantees showing the exhaustiveness of the proposed method, which is essential for building trust in safety-critical applications.\n5. **Clarity and Presentation**: The paper has some issues with clarity and presentation, which need to be addressed to improve readability.\n\nAddressing these issues would significantly strengthen the paper and make it more suitable for publication in a top-tier conference."
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Contribution and Novelty**\nThe paper introduces a novel framework, Continuous Query Decomposition (CQD), for answering complex queries on incomplete Knowledge Graphs (KGs). The key contributions are:\n- **Elegant and Efficient Solution**: The method translates complex queries into an end-to-end differentiable objective, leveraging pre-trained neural link predictors.\n- **State-of-the-Art Results**: The proposed approach outperforms existing methods, achieving significant improvements in Hits@3 with much less training data.\n- **Explainability**: The model provides insights into the intermediate solutions, enhancing interpretability.\n\n#### **2. Technical Soundness**\n- **Methodology**: The paper is technically sound, with a clear and logical flow. The use of t-norms and t-conorms for logical operations is well-explained.\n- **Optimization Techniques**: The paper explores both gradient-based and combinatorial optimization methods, providing a comprehensive evaluation.\n- **Mathematical Formalism**: While some reviewers found the mathematical formalism excessive, it is necessary to clearly define the problem and solution.\n\n#### **3. Experimental Evaluation**\n- **Datasets and Benchmarks**: The experiments are conducted on standard datasets, and the results are compared against state-of-the-art methods.\n- **Performance**: The proposed method achieves state-of-the-art results, demonstrating significant improvements over existing methods.\n- **Ablation Studies**: The paper could benefit from additional ablation studies, particularly with different t-norms and t-conorms, and different neural link predictors (e.g., TransE).\n\n#### **4. Clarity and Presentation**\n- **Writing Quality**: The paper is generally well-written and easy to follow. However, some minor issues in notation and clarity need to be addressed.\n- **Figures and Tables**: The figures and tables are well-presented, but some clarifications are needed, especially regarding the interpretation of the results in Table 3.\n\n#### **5. Addressing Reviewer Concerns**\n- **Reviewer 1**: The author's response addressed the reviewer's concerns.\n- **Reviewer 2**: The reviewer provided constructive feedback, which the authors should address in the final version, particularly regarding the importance of EPFO queries and the choice of complex query types.\n- **Reviewer 3**: The reviewer raised valid points about the similarity to previous work and the computational complexity of the combinatorial method. The authors should provide more detailed comparisons and timing results.\n- **Reviewer 4**: The reviewer suggested additional experiments with different neural link predictors and timing results, which would strengthen the paper.\n- **Reviewer 5**: The reviewer provided a thorough review, highlighting the strengths and suggesting minor improvements in presentation and clarity.\n\n### Final Decision\nThe paper presents a novel and effective method for answering complex queries on incomplete KGs, achieving state-of-the-art results with significant improvements over existing methods. The technical soundness and experimental evaluation are strong, and the contributions are significant. While there are some minor issues and suggestions for improvement, these can be addressed in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Novel Hypothesis:**\n   - The paper proposes a novel and intriguing hypothesis that the cerebellum acts as a decoupled neural interface (DNI) to solve the locking problem in the brain. This hypothesis bridges the gap between systems neuroscience and deep learning, offering a fresh perspective on cerebellar function.\n2. **Clear and Well-Structured:**\n   - The paper is well-written and structured, making it accessible to readers from both neuroscience and deep learning backgrounds. The authors provide a clear explanation of the DNI framework and how it relates to the cerebellum.\n3. **Broad Applicability:**\n   - The model is tested on a range of tasks, from simple motor tasks to more complex cognitive tasks, demonstrating its versatility and potential impact across different domains.\n4. **Testable Predictions:**\n   - The authors make several testable predictions, which, if validated, could provide significant insights into cerebellar function and its role in learning and credit assignment.\n\n#### **Weaknesses of the Paper:**\n1. **Clarity and Consistency:**\n   - The paper could benefit from improved clarity, particularly in distinguishing between feedforward and recurrent settings. The reviewer's comment about the confusion between the two settings is valid and should be addressed.\n2. **Comparative Analysis:**\n   - The paper lacks a thorough comparison with the original DNI architecture proposed by Jaderberg et al. (2017). This comparison is crucial to demonstrate the added value of the cerebellar-DNI (CC-DNI) model.\n3. **Experimental Validation:**\n   - While the model is theoretically sound, the paper could benefit from more direct comparisons with experimental data from cerebellar studies. The reviewer's point about the weak link between the model and biological cerebellum is well-taken. The authors should provide more concrete testable predictions and validate them against empirical data.\n4. **Feasibility of the Mechanism:**\n   - The paper does not adequately address the critical issue of how real gradients are computed and transmitted to the inferior olive. This is a significant gap in the model's feasibility, and the authors should provide a more detailed explanation or propose a plausible mechanism.\n5. **References:**\n   - The paper contains several inaccuracies in the references, which need to be corrected. These inaccuracies can undermine the credibility of the paper and should be addressed in the final version.\n\n### Final Decision\n\nDespite the paper's novel hypothesis and well-structured presentation, the identified weaknesses, particularly the lack of direct experimental validation and the need for a more thorough comparison with the original DNI architecture, are significant. These issues need to be addressed to strengthen the paper's scientific impact and credibility.\n\nFinal Decision: **Reject**\n\nHowever, the paper has the potential to be a valuable contribution to the field with appropriate revisions. The authors should consider addressing the reviewer comments and resubmitting the paper for further consideration."
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Summary of the Paper\nThe paper \"Deep Coherent Exploration For Continuous Control\" introduces a method to improve exploration in deep reinforcement learning (RL) algorithms for continuous control tasks. The method, Deep Coherent Exploration, generalizes step-based and trajectory-based exploration by modeling the last layer parameters of the policy network as latent variables. The authors provide a recursive, analytic expression for marginalizing over these latent variables, which is useful for obtaining low-variance gradients with on-policy methods. The method is evaluated on A2C, PPO, and SAC, showing improvements in learning speed and stability for on-policy methods, but mixed results for SAC.\n\n#### Strengths\n1. **Theoretical Contribution**: The paper builds on prior work (van Hoof et al., 2017) and extends it to deep RL methods, providing a scalable exploration framework.\n2. **Empirical Results**: The method shows significant improvements in learning speed and stability for on-policy methods (A2C and PPO) on several continuous control tasks.\n3. **Clarity and Reproducibility**: The paper is well-written, and the detailed appendix provides clear instructions for reproducing the method.\n4. **Ablation Studies**: The authors include insightful ablation experiments, which help in understanding the contributions of different components of the method.\n\n#### Weaknesses\n1. **Off-Policy Methods**: The performance of the method on off-policy methods (SAC) is mixed, with some tasks showing no improvement or even degradation. This suggests that the method may not be as effective for off-policy settings.\n2. **Limited Scope of Experiments**: The experiments are primarily conducted on Mujoco environments, which are relatively simple. Additional experiments on more complex domains or tasks with sparse rewards would strengthen the paper.\n3. **Hyperparameter Sensitivity**: The paper does not provide a thorough analysis of the sensitivity to hyperparameters, which is crucial for practical applications.\n4. **Comparison with Baselines**: The baseline methods (NoisyNet-A2C/PPO and PSNE-A2C/PPO) underperform the vanilla A2C/PPO, which raises questions about the tuning of these baselines.\n\n#### Reviewer Comments\n- **Reviewer 1**: Positive about the paper's contributions and empirical results, but suggests reframing the off-policy results as a limitation.\n- **Reviewer 2**: Highlights the novelty and mathematical soundness but expresses concerns about the empirical performance on SAC and the limited scope of experiments.\n- **Reviewer 3**: Praises the clarity and theoretical contributions but suggests moving some technical details to the appendix and conducting additional experiments.\n- **Reviewer 4**: Acknowledges the performance gains for on-policy methods but criticizes the lack of novelty and the mixed results for SAC.\n\n### Final Decision\nThe paper presents a valuable contribution to the field of reinforcement learning by extending a known exploration method to deep RL and demonstrating significant improvements for on-policy methods. However, the mixed results for off-policy methods and the limited scope of experiments are notable weaknesses. Given the strengths in theoretical contributions, empirical results for on-policy methods, and the clarity of the paper, I believe the paper should be accepted with the recommendation to address the limitations in future work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This approach is intuitive and addresses a significant challenge in multitask learning: estimating the influence of auxiliary tasks on the target task.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and motivate the proposed method effectively. The methodology is described in a structured manner, and the experimental setup is well-documented.\n\n3. **Empirical Evaluation**:\n   - The authors conduct experiments on both computer vision (Multi-MNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method. The use of multiple domains adds credibility to the empirical results.\n\n#### Weaknesses of the Paper\n1. **Lack of Theoretical Justification**:\n   - The paper lacks a theoretical analysis or formal justification for the proposed method. This is a significant concern, as it leaves the reader questioning the robustness and generalizability of $\\alpha$VIL. A theoretical framework would help in understanding the conditions under which the method is expected to perform well.\n\n2. **Limited Baselines and Mixed Results**:\n   - The choice of baselines is somewhat limited. The paper compares $\\alpha$VIL to standard multitask learning and a single task-oriented approach, but it would be beneficial to include more sophisticated baselines, such as other dynamic task weighting methods. The experimental results are mixed, with $\\alpha$VIL not consistently outperforming the baselines. This raises questions about the effectiveness of the proposed method.\n\n3. **Ad Hoc Nature of the Algorithm**:\n   - The algorithm appears somewhat ad hoc, and the specific choices made (e.g., subtracting 1 from all weights) are not well-justified. This lack of clarity and justification makes it difficult to understand the rationale behind the method.\n\n4. **Statistical Significance**:\n   - The paper does not provide statistical significance scores for the results, which is crucial for interpreting the performance differences between $\\alpha$VIL and the baselines. This omission weakens the empirical claims.\n\n5. **Experimental Setup**:\n   - The choice of the Multi-MNIST dataset with only one auxiliary task is limiting. It would be more convincing to evaluate the method on datasets with multiple auxiliary tasks to better demonstrate its effectiveness in a true multitask learning setup.\n\n6. **Ablation Studies**:\n   - The paper lacks detailed ablation studies, which are essential for understanding the contributions of different components of the proposed method. Additional ablations would help in validating the design choices and providing insights into the method's behavior.\n\n### Final Decision\nGiven the mixed results, lack of theoretical justification, and limited experimental setup, the paper does not meet the high standards expected for a top-tier conference. While the proposed method is novel and intuitively appealing, the current presentation and empirical evaluation are not sufficiently strong to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Technical Novelty and Contribution**\n- **Novelty**: The paper introduces Shoot Tree Search (STS), which extends Monte Carlo Tree Search (MCTS) by allowing multi-step expansion. While the concept of multi-step expansion is not entirely new (as noted by Reviewer 1 and Reviewer 5), the specific implementation and the context in which it is applied (using deep neural networks for value function approximation) are novel. The paper's contribution lies in the detailed exploration and empirical validation of this idea in challenging domains.\n- **Contribution**: The paper provides a clear and detailed algorithmic description of STS, along with a thorough empirical evaluation. The authors demonstrate that STS can achieve better performance than standard MCTS and other baseline methods in Sokoban and Google Research Football. The ablation studies and hyperparameter tuning provide valuable insights into the effectiveness of the method.\n\n#### **Experimental Evaluation**\n- **Strengths**: The experiments are well-designed and cover a range of challenging environments. The results show consistent improvements of STS over MCTS and other baselines. The ablation studies help to understand the impact of different components of the algorithm.\n- **Weaknesses**: Reviewer 1 and Reviewer 5 raise valid concerns about the experimental setup and the need for more thorough ablation studies. Specifically, the impact of the value function approximator and the choice of hyperparameters need to be more clearly addressed. The comparison with MCTS using a larger number of simulations (as suggested by Reviewer 3) would strengthen the empirical evaluation.\n\n#### **Presentation and Clarity**\n- **Strengths**: The paper is generally well-written and easy to follow. The algorithms are clearly described, and the results are presented in a structured manner.\n- **Weaknesses**: Reviewer 1 and Reviewer 5 point out several issues with the presentation, including unclear or missing details in the algorithms and experimental setup. The paper could benefit from more detailed discussions and clarifications, especially regarding the assumptions and the reasons for the observed performance improvements.\n\n#### **Addressing Reviewer Concerns**\n- **Technical Novelty**: The authors should address the concerns raised by Reviewer 1 and Reviewer 5 about the novelty of multi-step expansion. They should provide a more comprehensive discussion of related work and explain how their approach differs from previous methods.\n- **Experimental Setup**: The authors should conduct additional ablation studies to isolate the effects of the value function approximator and the multi-step expansion. They should also compare STS with MCTS using a larger number of simulations to provide a more robust evaluation.\n- **Presentation**: The authors should revise the paper to address the specific issues raised by the reviewers, such as clarifying the algorithms, providing more details about the experimental setup, and improving the overall clarity of the presentation.\n\n### Final Decision\nDespite the valid concerns raised by the reviewers, the paper presents a novel and interesting approach to planning in large state spaces. The empirical evaluation is thorough, and the results are promising. With some revisions to address the technical and presentation issues, the paper has the potential to make a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Addressing an Important Problem**: The paper tackles a significant issue in computational neuroscience and machine learning: the biological plausibility of deep neural networks (DNNs) in modeling the primate ventral stream. The authors aim to reduce the number of supervised synaptic updates required to achieve a brain-like representation, which is a critical step towards more realistic models of brain development.\n2. **State-of-the-Art Model**: The work builds on CORnet-S, which is currently the leading model in the Brain-Score benchmark. This provides a strong foundation for the research.\n3. **Complementary Strategies**: The paper introduces three complementary strategies (reduced training, weight initialization, and selective parameter updates) that, when combined, significantly reduce the number of supervised updates while maintaining a high Brain-Score. These strategies are innovative and thought-provoking.\n4. **Thorough Experiments and Analysis**: The experiments are well-designed and the results are thoroughly analyzed. The paper includes a detailed exploration of the methods and their effectiveness.\n5. **Well-Written and Clear**: The paper is well-written and the context is clearly described, making it accessible to a broad audience.\n\n#### Weaknesses of the Paper\n1. **Interpretation of BrainScore**: The main concern raised by Reviewer 1 and Reviewer 2 is the interpretation of BrainScore. The authors need to be more explicit about the limitations of BrainScore as a metric and clarify that achieving 80% of the BrainScore does not necessarily mean 80% similarity to the brain. This is a critical point that could mislead readers.\n2. **Biological Plausibility**: Reviewer 3 and Reviewer 5 raise concerns about the biological plausibility of the methods. The initialization protocol and the assumption that the infant visual system can be modeled by compressing weights from a trained network are not well-justified. The authors need to provide a stronger theoretical foundation for these assumptions.\n3. **Methodological Details**: Some methodological details are missing from the main text, which makes it difficult to fully understand and reproduce the experiments. For example, the weight initialization method and the rationale for choosing specific layers for critical training are not clearly explained.\n4. **Significance Testing**: The paper lacks significance testing, which is crucial for validating the results. The improvements in BrainScore could be due to random chance, and the authors need to demonstrate that the results are statistically significant.\n5. **Relevance of Supervised Learning**: The assumption that the visual system develops primarily through supervised learning is questionable. The authors should address the role of unsupervised and reinforcement learning mechanisms in brain development.\n6. **Numerical Imprecisions**: There are some numerical discrepancies in the figures and text, which need to be corrected to ensure the accuracy of the results.\n\n### Final Decision\nDespite the significant contributions and innovative strategies, the paper has several critical weaknesses that need to be addressed. The interpretation of BrainScore, the biological plausibility of the methods, and the lack of significance testing are major concerns that could undermine the validity of the results. The authors need to provide a more rigorous theoretical foundation and address these issues to strengthen the paper.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Problem Relevance and Importance**:\n   - The paper addresses the ecological inference problem, which is highly relevant in political science and has practical applications in areas such as gerrymandering and voting behavior analysis.\n   - The use of deep learning techniques to tackle this problem is innovative and could have significant real-world impact.\n\n2. **Methodological Contributions**:\n   - The paper introduces an efficient approximation to the loss function for ecological inference, enabling the use of linear models, deep neural networks, and Bayesian neural networks.\n   - The proposed models are applied to real-world data from the Maryland 2018 midterm elections, demonstrating practical utility.\n\n3. **Data and Experiments**:\n   - The paper uses a large and relevant dataset, which is valuable for the ML community.\n   - The experiments show that the proposed models outperform baselines in several tasks, such as recovering demographic splits and vote covariance structures.\n\n#### Weaknesses of the Paper\n1. **Clarity and Structure**:\n   - The paper is not easy to follow, with issues in structure and clarity. Key concepts and methods are not introduced early enough, making it difficult for readers to understand the contributions.\n   - There are numerous typos and minor writing issues that need to be addressed.\n\n2. **Experimental Details and Baselines**:\n   - The paper lacks important experimental details, such as the specifics of train/test splits, hyperparameters, and the exact configurations of the models.\n   - The paper does not compare the proposed deep models to standard baselines, such as a standard multi-level model (MLM), which is crucial for validating the improvements.\n   - The results are not thoroughly analyzed, and some claims are not well-supported with quantitative evidence.\n\n3. **Related Work**:\n   - The paper does not adequately cite related work, particularly in the areas of learning with label proportions (LLP), distribution regression, multiple-instance learning, and collective graphical models.\n   - This lack of context makes it difficult to assess the novelty and significance of the proposed method.\n\n4. **Reproducibility**:\n   - The lack of detailed experimental settings and hyperparameters hampers the reproducibility of the results.\n\n#### Final Evaluation\nThe paper addresses an important and relevant problem in ecological inference and introduces a novel approach using deep learning. However, the current version of the paper has significant issues in clarity, experimental details, and related work. These issues need to be addressed to ensure the paper meets the standards of a top-tier conference.\n\n### Final Decision: Reject\n\nThe paper has the potential to make a valuable contribution, but it requires substantial revisions to improve clarity, provide more detailed experimental settings, and adequately cite related work. Therefore, it is recommended for rejection with the possibility of resubmission after addressing the identified issues."
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Problem Relevance and Novelty**\nThe paper addresses the problem of distributed mean estimation (DME) and variance reduction, which are fundamental in distributed machine learning. The novelty lies in the approach to handle cases where the input vectors are concentrated around the mean but have large norms. This is a significant improvement over previous methods that rely on the norm of the input vectors, which can be problematic in practical scenarios. The use of lattice theory to achieve this is innovative and provides a new perspective on the problem.\n\n#### **2. Technical Soundness and Contributions**\n- **Main Contribution**: The paper introduces a lattice-based quantization method that allows for efficient communication while maintaining accuracy. The method is theoretically sound and provides guarantees that are dependent on the distance between inputs rather than their norms.\n- **Theoretical Results**: The paper provides both upper and lower bounds for the communication to error trade-off, showing that the proposed algorithms are asymptotically optimal. This is a strong theoretical contribution.\n- **Practical Algorithm**: The paper also presents a practical algorithm using cubic lattices, which is computationally feasible and only loose up to a logarithmic factor in \\(d\\). This makes the method applicable in real-world scenarios.\n\n#### **3. Experimental Evaluation**\n- **Experiments**: The experimental results show practical improvements over previous methods. However, the experiments are limited to \\(n=2\\), which is a significant limitation. More extensive experiments with larger \\(n\\) would strengthen the paper.\n- **Visuals**: The fonts in the plots are too small, which affects the readability of the results. Consistency in the x-axis scales across different plots would also improve the clarity of the experiments.\n\n#### **4. Writing and Presentation**\n- **Clarity**: The paper is generally well-written and provides a clear description of the problem and the proposed solution. The main ideas are explained clearly, and the motivation is well-articulated.\n- **Minor Issues**: There are several minor typos and grammatical errors that need to be corrected. These issues, while not major, can affect the overall readability of the paper.\n\n#### **5. Reviewer Comments and Author Responses**\n- **Reviewer 1**: Suggests comparing the proposed method with a straightforward iterative approach and referencing related work on mean estimation of random vectors. The authors should address these suggestions to strengthen the paper.\n- **Reviewer 2**: Points out that the techniques used are not entirely new but appreciates the clean end result. The suggestion to describe the computationally simple algorithm directly is valuable and should be considered.\n- **Reviewer 3**: Provides detailed comments on the clarity of the paper and the experimental setup. The authors should address the communication cost of the setup algorithms and the consistency in the experimental plots.\n- **Reviewer 4**: Highlights the natural and clean problem definition and the practical applicability of the proposed method. The concern about the constants in the bounds and the definition of the packing radius should be addressed.\n\n### Final Decision\nDespite the minor issues and the limited experimental setup, the paper makes a significant contribution to the field of distributed mean estimation and variance reduction. The theoretical results are strong, and the practical algorithm is promising. Addressing the reviewer comments, particularly those related to the experimental setup and the clarity of the paper, would further strengthen the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Novelty and Contribution**:\n   - The paper introduces a novel approach, GG-GAN, which combines geometric graph generation with a Wasserstein GAN. This is a significant contribution as it addresses key challenges in graph generation, such as modeling complex relations, isomorphic graphs, and fully exploiting the latent distribution.\n   - The method is permutation equivariant and scales to generate large graphs, which is a valuable property for practical applications.\n\n2. **Theoretical Support**:\n   - The paper provides theoretical support for the proposed method, including theorems and propositions that justify the design choices and properties of GG-GAN. This adds rigor to the paper and helps in understanding the underlying principles.\n\n3. **Clarity and Organization**:\n   - The paper is well-written and clearly organized. The authors effectively communicate the problem, the proposed solution, and the experimental results. The introduction and motivation are strong, and the method is well-explained.\n\n#### Weaknesses and Concerns\n\n1. **Experimental Evaluation**:\n   - **Dataset Size**: The datasets used in the experiments are relatively small, with graphs having only 9 or 20 vertices. This limits the generalizability of the results and the ability to demonstrate the scalability of GG-GAN. Larger and more diverse datasets would provide a more robust evaluation.\n   - **Baseline Comparison**: The comparison with baseline methods is not comprehensive. The paper should include more state-of-the-art methods, such as NetGAN, and provide a more detailed analysis of the performance differences. The current results show limited improvement over some baselines, which weakens the claim of superior performance.\n   - **Mode Collapse and Diversity**: The paper does not adequately address the issue of mode collapse and generation diversity, which are common problems in GAN-based models. More evidence and discussion on this aspect are needed to strengthen the claims.\n\n2. **Theoretical Analysis**:\n   - Some of the theorems and propositions are not well-connected to the practical aspects of GG-GAN. For example, the existence of distribution \\(\\mathcal{D}_x\\) and the sampling procedure need more detailed justification and discussion.\n   - The paper could benefit from a more thorough discussion of the theoretical implications and how they translate into practical improvements in graph generation.\n\n3. **Literature Review**:\n   - The literature review is somewhat limited. The paper should provide a more comprehensive comparison with existing work, especially in terms of isomorphism, scalability, and expressive power. This would help in highlighting the unique contributions of GG-GAN.\n\n4. **Implementation Details**:\n   - Some implementation details, such as the role of \\(\\phi\\) and the collision avoidance mechanism, are not well-explained. More clarity and theoretical justification are needed to understand these components fully.\n\n#### Final Considerations\n\nWhile the paper presents a novel and theoretically sound approach to graph generation, the experimental evaluation and theoretical analysis have significant gaps that need to be addressed. The lack of comprehensive baseline comparisons and the use of small datasets limit the impact and generalizability of the results. Additionally, the paper could benefit from a more thorough discussion of the practical implications and limitations of the proposed method.\n\n### Final Decision\nFinal Decision: Reject\n\nThe paper has strong theoretical foundations and a novel approach, but the experimental evaluation and theoretical analysis are not robust enough to justify acceptance at a top-tier conference. Addressing the identified weaknesses and providing a more comprehensive evaluation would significantly improve the paper's quality and impact."
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n\n1. **Novel Representation:**\n   - The paper introduces Block Minifloat (BM), a new representation for training DNNs with low precision. This is a significant contribution, especially given the growing interest in efficient hardware for deep learning.\n   - The idea of using a shared exponent bias to reduce the exponent field is innovative and has practical implications for hardware design.\n\n2. **Hardware Implementation:**\n   - The authors provide a detailed hardware evaluation, which is a strong point. They synthesize a 4x4 systolic array multiplier using BM units and report area and power savings compared to FP8 and FP32.\n   - The use of a Kulisch accumulator for minifloat dot products is a novel and practical approach, and the results are impressive.\n\n3. **Comprehensive Evaluation:**\n   - The paper evaluates BM on a variety of models and datasets, including ResNet-18 and VGG-16 on CIFAR and ImageNet. This comprehensive evaluation adds credibility to the claims.\n   - The authors provide detailed tables and figures to support their findings, which is essential for a top-tier conference.\n\n#### **Weaknesses of the Paper:**\n\n1. **Clarity and Explanation:**\n   - The paper could benefit from more detailed explanations of key concepts, such as the alignment of minifloat distribution and the calculation of $\\beta$.\n   - Equation 6 is introduced abruptly, and more intuition and derivation would improve the paper's clarity.\n   - The tables and figures need better labeling and explanation. For example, Table 2 has undefined columns and footnotes, which can be confusing.\n\n2. **Hardware Overhead:**\n   - The need to support multiple BM formats in hardware is a significant concern. The authors mention this but do not provide a detailed analysis of the overhead.\n   - The hardware evaluation should include more details on power modeling and the test vectors used.\n\n3. **Comparative Analysis:**\n   - The paper could benefit from a more detailed comparison with other low-precision representations, such as Flexpoint. This would provide a better context for the contributions.\n   - The inclusion of larger networks, such as ResNet50 on ImageNet, would strengthen the experimental results.\n\n4. **Subnormal Handling:**\n   - The handling of subnormal numbers and the hardware overhead for denorm support need more detailed explanation. The authors mention this but do not provide a clear hardware design.\n\n#### **Author Responses:**\n\n- The authors have addressed some of the major concerns, particularly regarding the hardware overhead for supporting multiple formats. This is a significant improvement.\n- However, the paper still lacks clarity in some areas, such as the detailed explanation of Equation 6 and the handling of subnormal numbers.\n\n### Final Decision\n\nDespite the strengths of the paper, the weaknesses in clarity and detailed explanation are significant. The paper would benefit from additional revisions to address these issues. However, given the novel contributions and the strong hardware evaluation, the paper has the potential to be a valuable addition to the conference.\n\nFinal Decision: [Accept]"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Conceptual Contribution**: The paper proposes a generalized probability kernel (GPK) that aims to unify and extend existing discrepancy measures such as MMD and KSD. This is an interesting and potentially valuable contribution to the field of kernel-based hypothesis testing and distribution property estimation.\n2. **Theoretical Framework**: The paper attempts to provide a theoretical framework for the proposed GPK, including definitions and properties of the kernel, and its application to two-sample tests.\n3. **Novelty**: The idea of extending MMD to incorporate probability distributions in a more general way is novel and could open up new avenues for research.\n\n#### Weaknesses:\n1. **Clarity and Presentation**:\n   - The paper suffers from numerous typos, grammatical errors, and unclear notation, making it difficult to follow. This is a significant issue for a top-tier conference where clarity and precision are paramount.\n   - Definitions and notations are inconsistent, leading to confusion. For example, the distinction between \\( K \\) and \\( k \\) is not clear, and the role of \\( \\phi \\) is not well-defined.\n2. **Theoretical Rigor**:\n   - The theoretical results are not well-developed. Key definitions and theorems lack proper proofs or are incomplete. For instance, the proof of Theorem 5 is missing, and the properties of the proposed kernels are not rigorously established.\n   - The connection between the proposed GPK and existing discrepancy measures is not clearly demonstrated, and the novelty of the proposed kernels is not well-justified.\n3. **Experimental Evaluation**:\n   - The paper lacks any experimental evaluation. A top-tier conference expects empirical validation of theoretical claims, and the absence of experiments significantly weakens the paper.\n   - The lack of simulation results and comparisons with existing methods makes it difficult to assess the practical utility of the proposed GPK.\n4. **Relevance and Motivation**:\n   - The motivation for the proposed GPK is not well-articulated. The paper does not provide a clear explanation of why the proposed kernels are useful or how they improve upon existing methods.\n   - The relevance of the proposed setting (discrete distributions with labels in \\(\\mathbb{R}^k\\)) is not well-justified, and the paper lacks practical examples or applications.\n\n#### Specific Issues:\n1. **Definition 2**:\n   - The definition of the GPK is unclear and lacks proper justification. It is not clear why it is called a kernel and not a distance.\n   - The notation \\( K \\) and \\( k \\) are mixed up, and the domain of \\( k \\) is not correctly specified.\n2. **Theorem 5**:\n   - The proof of Theorem 5 is missing, and the removal of the supremum over \\( f \\) is not explained.\n3. **Section 4.2 and 4.3.1**:\n   - The results in these sections are trivial and known, and should be skipped or referenced appropriately.\n4. **Definition 6**:\n   - The definition of the symmetric KDSD is incomplete, and the proof that it satisfies the properties of a probability kernel is missing.\n5. **Polynomial Probability Kernel**:\n   - The polynomial probability kernel requires \\( l = k \\) to satisfy the conditions of Definition 3, and this is not clearly stated.\n6. **Experimental Section**:\n   - The experimental section is completely missing, which is a major flaw for a top-tier conference.\n\n### Final Decision\nGiven the significant issues with clarity, theoretical rigor, and experimental evaluation, the paper is not ready for publication in a top-tier conference. The authors should address the major and minor points raised by the reviewers, provide a clear and consistent theoretical framework, and include empirical validation of their claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\n- **Novelty**: The paper proposes a novel approach to integrating language throughout the visual pathway, from low-level to high-level features, which is a significant departure from the common top-down attention mechanisms. This is a valuable contribution to the field, as it addresses the limitations of existing methods and opens up new avenues for research.\n- **Contribution**: The paper argues that using language to modulate both bottom-up and top-down visual processing can lead to better performance. The experimental results on standard datasets show improvements, and the ablation studies demonstrate the importance of both top-down and bottom-up modulation.\n\n#### Technical Soundness\n- **Model Description**: The model is well described, and the integration of language at different levels of visual processing is clearly explained. The use of U-Net as the backbone is a reasonable choice for the task.\n- **Experiments**: The experiments are conducted on several standard datasets, and the results are generally positive. However, there are some concerns:\n  - **Statistical Significance**: The lack of statistical significance tests (e.g., bootstrap tests) is a significant issue. Without these tests, it is difficult to determine whether the improvements are statistically significant.\n  - **Consistency**: The results on the validation set are promising, but the performance on the test set is inconsistent across different datasets. This suggests that the model might be over-tuned to the validation set.\n\n#### Clarity and Presentation\n- **Writing**: The paper is well written and easy to follow. The introduction and motivation are clear, and the figures and tables are generally well presented.\n- **Figures and Tables**: Some figures (e.g., Figure 1) could be improved for clarity. The reviewer's suggestions for adding arrows and more space between branches are valid.\n- **Ablation Studies**: The ablation studies are thorough and provide valuable insights into the importance of different components of the model.\n\n#### Impact and Relevance\n- **Impact**: The proposed approach has the potential to significantly impact the field of multimodal processing, particularly in tasks involving language and vision. The idea of integrating language at multiple levels of visual processing is novel and could inspire further research.\n- **Relevance**: The task of image segmentation from referring expressions is important and well chosen. The paper addresses a timely and relevant problem in computer vision and AI.\n\n#### Addressing Reviewer Concerns\n- **Reviewer 1**: The reviewer raises valid points about the novelty and the need for more exploration of different modeling techniques. The suggestions for additional citations and figure improvements should be addressed.\n- **Reviewer 2**: The reviewer's concerns about the empirical results and the consistency of the model's performance on different datasets are significant. The authors should provide statistical significance tests and a more detailed error analysis.\n- **Reviewer 3**: This reviewer is generally positive about the paper, praising its clarity, motivation, and experimental results. The qualitative examples provided are impressive.\n- **Reviewer 4**: The reviewer's concerns about the conceptual motivation and error analysis are valid. The authors should provide a more detailed analysis of the interaction between language and visual input, as suggested by the reviewer.\n\n### Final Decision\nDespite the concerns raised by the reviewers, the paper presents a novel and significant contribution to the field of multimodal processing. The integration of language at multiple levels of visual processing is a valuable idea that could inspire further research. However, the authors need to address the issues related to statistical significance and provide a more detailed error analysis.\n\nFinal Decision: Accept\n\nThe authors should address the following points in the final version:\n1. Conduct statistical significance tests (e.g., bootstrap tests) to validate the improvements.\n2. Provide a more detailed error analysis to understand the interaction between language and visual input.\n3. Address the reviewer's suggestions for improving the clarity of figures and adding more citations."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This is a unique approach that has not been explored before, making it a valuable contribution to the field of multitask learning.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and motivate the proposed methodology effectively. The experimental setup is described in detail, and the results are presented in a structured manner.\n\n3. **Empirical Evaluation**:\n   - The authors conduct experiments on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method. The use of multiple datasets and tasks is a strong point.\n\n#### Weaknesses of the Paper\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation. The reviewers have pointed out that the method is intuitive but lacks formal analysis or theoretical justification. This is a significant gap that needs to be addressed to strengthen the paper.\n\n2. **Empirical Results**:\n   - The experimental results are not consistently superior to the baselines. On the MultiMNIST dataset, the proposed method outperforms the baselines, but the results are within a standard deviation. On the NLU tasks, the results are mixed, and the proposed method does not consistently outperform the baselines. This raises questions about the effectiveness and robustness of the method.\n\n3. **Algorithmic Clarity**:\n   - The algorithm is described as ad hoc, and some aspects of it are not well-justified. For example, the arbitrary subtraction of 1 from all weights in line 11 of Algorithm 1 is not clearly explained. The reviewers have raised concerns about the clarity and justification of the algorithm.\n\n4. **Experimental Setup**:\n   - The choice of experimental setup, particularly the use of only one auxiliary task in the vision domain, is limited. The paper could benefit from more extensive experiments with multiple auxiliary tasks to better demonstrate the method's effectiveness in a more complex multitask learning setup.\n\n5. **Statistical Significance**:\n   - The paper does not provide statistical significance scores for the results, which is crucial for validating the performance improvements. The reviewers have suggested that the authors should report standard deviations and perform statistical tests to ensure the results are significant.\n\n6. **Ablation Studies**:\n   - The paper could benefit from more ablation studies to understand the impact of different components of the proposed method. For example, the authors could explore the learned importance of auxiliary tasks and discuss any intuitive reasons for the observed task weights.\n\n#### Additional Considerations\n- **Relevance to the Conference**:\n  - The paper addresses a relevant and important problem in multitask learning, which is a hot topic in machine learning. However, the lack of strong theoretical and empirical support may make it less competitive for a top-tier conference.\n\n- **Potential for Improvement**:\n  - The paper has the potential to be improved by addressing the reviewers' concerns. The authors could provide a more thorough theoretical analysis, conduct more extensive experiments, and include statistical significance tests and ablation studies.\n\n### Final Decision\nFinal Decision: Reject\n\nThe paper introduces a novel and intuitive method for multitask learning, but it falls short in providing a strong theoretical foundation and robust empirical validation. The experimental results are not consistently superior to the baselines, and the algorithm's clarity and justification are lacking. Addressing these issues would significantly strengthen the paper, but in its current form, it does not meet the standards of a top-tier conference."
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Problem Importance and Relevance\nThe paper addresses a critical issue in machine learning: the overconfidence of deep learning models on out-of-distribution (OOD) data. This is particularly relevant in real-world applications where data distributions can shift unexpectedly. The ability to provide well-calibrated uncertainty estimates is crucial for reliable decision-making in safety-critical systems.\n\n#### Methodology and Novelty\nThe proposed method introduces a post-hoc calibration technique that uses outlier exposure to calibrate model probabilities. This approach is novel in that it leverages a set of corrupted validation data to improve calibration on OOD data. The method is simple and can be applied to various models, which is a significant advantage.\n\n#### Experimental Evaluation\nThe authors evaluate their method on standard datasets such as CIFAR-10 and ImageNet, using a variety of corruption types. The results show consistent improvements in calibration metrics (ECE) compared to existing methods. The inclusion of calibration diagrams (Figure 2) provides visual evidence of the method's effectiveness.\n\n#### Strengths\n1. **Relevance and Importance**: The problem of OOD calibration is highly relevant and important.\n2. **Novelty**: The proposed method is novel and simple, making it accessible and applicable to a wide range of models.\n3. **Empirical Results**: The experiments are well-conducted and show significant improvements over existing methods.\n4. **Clarity of Presentation**: The paper is generally well-written, with clear illustrations and results.\n\n#### Weaknesses and Concerns\n1. **Assumption of Known Corruptions**: The method assumes that the possible novel distributions are known, which is a strong assumption and limits its applicability in real-world scenarios where the nature of distribution shifts is often unknown.\n2. **Limited Evaluation**: The evaluation is primarily on artificially corrupted datasets. More extensive evaluation on real-world OOD data (e.g., SVHN for CIFAR-10) would strengthen the paper.\n3. **Clarity and Notation**: The notation and terminology are sometimes unclear, which can make the paper difficult to follow. For example, the meaning of $p_{max}$ and the \"m\" in $P^{CAL, j}_m$ are not clearly defined.\n4. **Baseline Comparisons**: The baselines considered are incomplete. Comparing with more recent work in the area (e.g., Park et al., Wang et al.) would provide a more comprehensive evaluation.\n5. **Theoretical Analysis**: More theoretical analysis or intuition about why the method works well for different types of corruptions would be beneficial.\n\n#### Post-Rebuttal Considerations\nThe authors have addressed some of the concerns raised by the reviewers, particularly by providing additional experiments and clarifications. However, the fundamental issue of the assumption of known corruptions remains a significant limitation.\n\n### Final Decision\nDespite the novel and effective method, the assumption of known corruptions and the limited evaluation on real-world OOD data are significant drawbacks. The paper would benefit from addressing these issues to strengthen its contributions and applicability.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\n- **Novelty**: The paper proposes a GNN model with a global self-attention mechanism using learnable spectral filters. This is a novel approach that aims to address the limitations of existing GNNs, particularly in handling disassortative graphs. However, the novelty is somewhat diminished by the fact that similar ideas, such as non-local attention, have been explored in previous works (e.g., Non-Local Graph Neural Networks [1]).\n- **Contribution**: The paper's main contribution is the introduction of a global attention mechanism that can attend to any nodes, regardless of distance. This is a significant step towards making GNNs more flexible and adaptive to different types of graphs. The authors provide a solid theoretical motivation and a clear explanation of how their model works.\n\n#### Technical Soundness\n- **Theoretical Soundness**: The paper is well-motivated and the proposed method is theoretically sound. The use of graph wavelets and learnable spectral filters is a well-founded approach to achieving global attention.\n- **Complexity**: One of the main concerns raised by the reviewers is the computational complexity of the proposed model. The authors claim that the model can be made efficient, but the reviewers argue that the full matrix eigen decomposition required for the attention mechanism is computationally expensive (O(V^2) or higher). This is a significant issue, especially for large graphs, and the authors have not provided a clear solution to this problem.\n\n#### Empirical Evaluation\n- **Datasets**: The paper evaluates the proposed model on six benchmark datasets, including both assortative and disassortative graphs. The empirical results show that the model outperforms state-of-the-art baselines on disassortative graphs and performs comparably on assortative graphs.\n- **Scalability**: The datasets used for evaluation are relatively small, with only a few hundred nodes. This is a limitation, as the model's performance on larger graphs is not demonstrated. The reviewers have raised concerns about the scalability of the model, and the authors have not provided a convincing response to these concerns.\n\n#### Clarity and Presentation\n- **Clarity**: The paper is well-written and easy to understand. The authors provide a clear explanation of the proposed method and the experimental results.\n- **Presentation**: The paper is well-structured, and the figures and tables are clear and informative.\n\n### Final Decision\nGiven the concerns about computational complexity and scalability, and the limited novelty compared to existing works, the paper does not meet the high standards of a top-tier conference. While the proposed method is theoretically sound and shows promising results on small datasets, the lack of scalability and the computational issues are significant drawbacks.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Motivation and Novelty:**\n   - The paper addresses a significant gap in the literature by bridging the theory and practice of meta-learning. The motivation to use theoretical insights to improve practical performance is strong and relevant.\n   - The paper is well-organized and clearly written, making it accessible to a broad audience.\n\n2. **Theoretical Insights:**\n   - The authors review recent theoretical advances in meta-learning and propose regularization terms based on these insights. This is a valuable contribution as it helps in understanding the behavior of meta-learning algorithms and improving their generalization.\n\n3. **Experimental Results:**\n   - The experimental results show improvements over vanilla meta-learning methods on classic few-shot classification benchmarks. This provides empirical evidence supporting the effectiveness of the proposed regularization terms.\n\n#### **Weaknesses:**\n1. **Lack of Novelty in Regularization Terms:**\n   - Reviewer 1 points out that the proposed regularization terms are similar to existing techniques such as spectral normalization and weight decay. This raises concerns about the novelty of the proposed methods.\n   - The Frobenius norm regularization is a common technique used in various settings, not just meta-learning, which further diminishes the novelty.\n\n2. **Theoretical Consistency:**\n   - Reviewer 2 highlights that the theoretical setting in the paper is not fully consistent with the few-shot learning setting. For example, the lack of a validation set in the theoretical framework could affect the practical applicability of the results.\n   - The task distribution, a crucial aspect of meta-learning theory, is not clearly defined, and the authors should provide more empirical results on tasks with different distributions.\n\n3. **Experimental Rigor:**\n   - Reviewer 3 notes that the improvements due to the proposed regularization terms are sometimes at the same level as the standard deviation, casting doubt on their true efficacy.\n   - The loss function in Eq. (4) could benefit from the introduction of tunable hyperparameters for the regularization terms to better control their impact.\n   - The authors should conduct synthetic experiments to test the robustness of the proposed regularization terms under different conditions.\n\n4. **Technical Details:**\n   - Reviewer 4 raises concerns about the technical details, particularly the calculation of subgradients of the singular values. The lack of these details and the potential for auto-differentiation tools to handle them incorrectly is a significant issue.\n   - The paper could benefit from more comparisons with recent methods to strengthen the empirical results.\n\n#### **Final Considerations:**\n- **Impact and Relevance:**\n  - The paper addresses a relevant and important problem in the field of meta-learning. The motivation to use theoretical insights to improve practical performance is strong and could have a significant impact.\n- **Novelty and Contribution:**\n  - While the proposed regularization terms are not entirely novel, the integration of theoretical insights into practical algorithms is a valuable contribution. However, the lack of clear novelty and the technical concerns raised by the reviewers are significant.\n- **Experimental Rigor and Robustness:**\n  - The experimental results are promising but need to be more robust and thoroughly validated. The paper could benefit from additional experiments and a more detailed analysis of the proposed methods.\n\n### Final Decision\nFinal Decision: [Reject]\n\nThe paper has a strong motivation and addresses a relevant problem, but the lack of clear novelty in the proposed regularization terms, the technical concerns, and the need for more robust experimental validation are significant issues. Addressing these concerns would strengthen the paper and make it more suitable for a top-tier conference."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Clarity and Presentation**\n- **Clarity**: The paper is generally poorly written and hard to follow. Multiple reviewers (Reviewers 2, 3, 4, 5, and 6) have pointed out significant issues with the clarity and presentation of the paper. The use of footnotes to explain important concepts, missing definitions, and unclear explanations of the proposed method (e.g., the stack network and the P-FUNCTIONAL) make it difficult to understand the contributions.\n- **Presentation**: The paper needs considerable improvement in terms of organization and clarity. The introduction and motivation for the pruning steps are not well-explained, and the notation and terminology are inconsistent and often undefined.\n\n#### 2. **Technical Soundness**\n- **Correctness**: The paper heavily relies on an \"unpublished work\" by the authors, which makes it impossible to verify the correctness of the theoretical claims (Reviewers 3, 4, and 5). This is a significant issue because the validity of the proposed method cannot be confirmed without access to the referenced work.\n- **Methodology**: The proposed pruning technique consists of three steps, but the third step (using weighted k-means to combine neurons) is not well-motivated or derived (Reviewer 5). The equations and methods used in this step are unclear and may contain errors.\n\n#### 3. **Experimental Evaluation**\n- **Experiments**: The experiments are limited and not convincing. The paper only conducts experiments on a simple toy dataset and MNIST (Reviewers 2, 3, 4, 5, and 6). These datasets are not complex enough to showcase the effectiveness of the proposed method. Additionally, the paper does not compare the proposed method to state-of-the-art pruning techniques, which is crucial for demonstrating its novelty and performance (Reviewers 1, 3, 4, and 5).\n- **Results**: The experimental results are not statistically significant, and the paper does not provide measures such as standard error to support the claims (Reviewer 3). The plots and figures are also not well-explained and are not informative (Reviewer 4).\n\n#### 4. **Novelty and Contribution**\n- **Novelty**: The proposed method is not particularly novel. The ideas of removing dead ReLU neurons and combining neurons that always act as the identity are well-known in the literature (Reviewers 2, 5, and 6). The paper does not clearly differentiate its contributions from existing work.\n- **Contribution**: The paper claims to preserve the network output exactly, but the actual implementation involves approximation methods (Reviewer 4). This discrepancy between the claim and the implementation is a significant issue.\n\n### Final Decision\nGiven the significant issues with clarity, technical soundness, experimental evaluation, and novelty, the paper is not ready for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Novelty and Relevance:**\n   - The paper introduces a novel approach to disentangle representations in pre-trained transformers by learning binary masks over weights or hidden units. This is a significant contribution as it leverages pre-trained models without the need for retraining, which is computationally efficient.\n   - The problem of disentanglement in large language models is relevant and important, especially for tasks where specific attributes need to be isolated or controlled.\n\n2. **Methodological Soundness:**\n   - The method is well-defined and the use of triplet loss, attribute classification loss, and a loss to encourage different masks for different attributes is a reasonable approach.\n   - The paper provides a clear explanation of the method and the training process, making it easy to understand and potentially reproducible.\n\n3. **Experimental Results:**\n   - The experiments are well-designed and show that the proposed method can disentangle representations of syntax and semantics, and sentiment from genre in movie reviews.\n   - The results are compared to baselines, and the proposed method outperforms or matches the performance of previous methods, which is a strong point.\n\n4. **Clarity and Presentation:**\n   - The paper is well-written and the ideas are presented clearly. The figures and tables are informative and support the claims made in the text.\n\n#### **Weaknesses and Concerns:**\n1. **Generalizability:**\n   - Reviewer 1 and Reviewer 5 raise concerns about the generality of the proposed method. The experiments are limited to a specific domain (movie reviews) and a specific set of attributes (sentiment and genre). More diverse datasets and attributes should be considered to demonstrate the method's broader applicability.\n   - The paper should address why the specific genres (Drama and Horror) were chosen and whether similar results can be obtained with other genres.\n\n2. **Comparative Analysis:**\n   - Reviewer 2 points out that the paper does not clearly show how the proposed method outperforms previous approaches, particularly variational autoencoders and adversarial training. The abstract mentions this but the main text lacks detailed comparisons.\n   - The paper should provide more detailed benchmarks and quantitative measures to support the claims of improved performance.\n\n3. **Technical Details:**\n   - Reviewer 3 and Reviewer 4 raise questions about the specifics of the method, such as the number of parameters, the impact of masking different layers, and the stability of the method after fine-tuning.\n   - The paper should provide more details on the training process, the sparsity of the masks, and the computational efficiency of the method.\n\n4. **Downstream Task Performance:**\n   - Reviewer 5 suggests that the paper should explore the use of disentangled representations in downstream tasks such as text classification, natural language inference, and semantic similarity. This would provide a more comprehensive evaluation of the method's utility.\n\n5. **Related Work:**\n   - Reviewer 4 points out that the paper should cite and discuss relevant previous work, such as using dropout for confidence modeling and the HUBERT model for disentanglement of semantics and structure.\n\n### Final Decision\nGiven the novel and relevant contribution of the paper, the well-defined method, and the promising experimental results, the paper has strong potential for acceptance. However, the concerns about generality, comparative analysis, and downstream task performance need to be addressed to strengthen the paper.\n\n**Final Decision: Accept with Revisions**\n\nThe authors should:\n1. Conduct additional experiments on more diverse datasets and attributes to demonstrate the generality of the method.\n2. Provide detailed comparisons with previous approaches, including variational autoencoders and adversarial training.\n3. Include more technical details about the training process, the sparsity of the masks, and the computational efficiency.\n4. Explore the use of disentangled representations in downstream tasks and provide quantitative measures to support the claims.\n5. Cite and discuss relevant previous work to place the contribution in a broader context."
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Innovative Approach:**\n   - The paper introduces a novel method (MIMO) to train multiple subnetworks within a single model, which can be evaluated in a single forward pass. This is a significant contribution to the field of efficient ensembling.\n   - The method is simple and straightforward, making it accessible and easy to implement.\n\n2. **Empirical Validation:**\n   - The authors provide extensive experimental results on standard benchmarks (CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants).\n   - The results show significant improvements in negative log-likelihood, accuracy, and calibration error compared to previous methods.\n   - The paper includes detailed analysis and visualizations (e.g., loss plane, parameter/activation projections) to demonstrate the independence of subnetworks.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and easy to follow. The method is clearly explained, and the experimental setup is well-documented.\n   - The figures and tables are well-designed and provide valuable insights into the performance of the proposed method.\n\n#### **Weaknesses:**\n1. **Parameter Increase:**\n   - The authors claim that the benefits of using multiple predictions can be achieved \"for free,\" but there is a 1% increase in the number of parameters. This claim should be more nuanced to avoid misleading readers.\n   - A detailed explanation of where the 1% increase in parameters comes from would strengthen the paper.\n\n2. **Theoretical Understanding:**\n   - The paper lacks a theoretical explanation for why and how the subnetworks become independent. This is a significant gap in the understanding of the method.\n   - Some reviewers suggest that a more thorough theoretical analysis would make the paper stronger.\n\n3. **Dataset Diversity:**\n   - The experiments are conducted on standard benchmarks, but more diverse datasets (e.g., OpenImages) could provide a more comprehensive evaluation of the method's robustness and generalizability.\n\n4. **Computational Costs:**\n   - The paper focuses on reducing inference delay but does not provide a detailed analysis of the total computational costs (e.g., MACs or FLOPS). This is a critical aspect that should be addressed to fully understand the trade-offs of the proposed method.\n   - The method's practical potential is limited by the size of M, which must be small due to the limited capacity of a single network.\n\n5. **Related Work:**\n   - The paper could benefit from a more comprehensive discussion of related work, particularly multi-branch architectures used in other contexts (e.g., robustness, knowledge distillation).\n\n#### **Final Decision:**\nWhile the paper presents a novel and effective method for efficient ensembling, it has several areas that need improvement, particularly in theoretical understanding, computational costs, and dataset diversity. However, the contributions are significant, and the empirical results are strong.\n\n**Final Decision: Accept**\n\nThe paper should be accepted with the condition that the authors address the following points in the final version:\n1. Provide a more nuanced explanation of the 1% increase in parameters.\n2. Include a theoretical analysis or discussion of why and how the subnetworks become independent.\n3. Conduct experiments on more diverse datasets to evaluate the method's generalizability.\n4. Provide a detailed analysis of the total computational costs.\n5. Expand the discussion of related work to include multi-branch architectures used in other contexts."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Clarity and Presentation**\n- **Clarity**: The paper suffers from significant issues in clarity and presentation. Multiple reviewers (Reviewers 1, 2, 3, 4, 5, and 6) have pointed out that the paper is hard to read, lacks detailed explanations, and contains numerous typos and grammatical errors. The use of footnotes to explain important concepts is not ideal, and the main text should be more self-contained and clear.\n- **Presentation**: The structure of the paper is poorly organized, making it difficult for readers to follow the logic and contributions. Sections are not well-defined, and important details are often missing or buried in footnotes.\n\n#### 2. **Technical Soundness**\n- **Theoretical Foundation**: The paper heavily relies on an \"unpublished work\" by the authors, which is not available for review. This makes it impossible to verify the correctness of the theoretical claims. Reviewers 3, 4, and 5 have specifically highlighted this as a major issue.\n- **Algorithm Description**: The proposed algorithm is not well-explained. Reviewers 4 and 5 have pointed out that the method to combine neurons using weighted k-means is not motivated or derived, and the equations are unclear and potentially contain errors.\n- **Equations and Definitions**: Important equations and definitions are missing or poorly defined. For example, the P-functional is not defined, and the notation used in the equations is inconsistent.\n\n#### 3. **Experimental Evaluation**\n- **Dataset Complexity**: The experiments are conducted on simple toy datasets and MNIST, which are not sufficient to validate the effectiveness of the proposed method. Reviewers 3, 4, and 5 have suggested that the experiments should be extended to more complex datasets like CIFAR-10.\n- **Comparison with Baselines**: The paper lacks a comparison with state-of-the-art pruning techniques. Reviewers 2, 3, and 4 have noted that the method should be compared to standard pruning methods based on weight magnitude, gradient norm, and Hessian-based metrics.\n- **Statistical Significance**: The experimental results are not statistically significant. Reviewers 3 and 4 have suggested that measures like one standard error should be provided to show the statistical significance of the results.\n\n#### 4. **Novelty and Contribution**\n- **Novelty**: The proposed method is not significantly novel. Reviewers 1, 2, 5, and 6 have noted that the ideas of removing dead ReLU neurons and combining neurons that are always active are well-known in the literature.\n- **Contribution**: The paper claims to preserve the network output exactly, but the actual implementation involves approximation methods, which is not clearly explained. Reviewers 4 and 5 have pointed out that the method introduces hyperparameters that are not tested, and the actual performance is not well-justified.\n\n### Final Decision\nGiven the significant issues in clarity, technical soundness, experimental evaluation, and novelty, the paper is not ready for publication in a top-tier conference. The theoretical claims cannot be verified, the experimental results are not convincing, and the presentation needs substantial improvement.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Title and Abstract\n- **Title**: The title \"Automatic Music Production Using Generative Adversarial Networks\" is misleading. The paper focuses on automatic music accompaniment, which is a specific subset of music production. The title should be revised to accurately reflect the scope of the work.\n- **Abstract**: The abstract is generally clear but could benefit from rephrasing to better motivate the need for automatic accompaniment. The claim that treating music audio as images is novel is incorrect, as this approach is standard in many MIR tasks. The abstract should be revised to acknowledge this and provide a more accurate context.\n\n#### Introduction\n- **Motivation**: The introduction lacks a clear motivation for the need for automatic accompaniment. It should provide more context and justification for the demand from artists and producers.\n- **Shortcomings of Waveform/Frequency Domain**: The introduction mentions the advantages of using the Mel-frequency domain but fails to discuss the shortfalls of this approach, such as the lack of source-separated training data and the difficulty in source separation.\n- **Demucs Algorithm**: The introduction mentions the use of the Demucs algorithm for source separation but lacks details about its implementation and potential downsides. This should be addressed to provide a more comprehensive understanding.\n\n#### Related Works\n- **Symbolic Domain Literature**: The paper does not cite the extensive literature on music generation in the symbolic domain, which is highly relevant. This omission should be corrected to provide a more balanced review of the field.\n- **Justification for Raw Audio**: The claim that raw audio representation is necessary for appealing results in music production is not well-supported. The paper should discuss the advantages and disadvantages of both symbolic and raw audio representations.\n\n#### Method\n- **Demucs Algorithm**: The method section lacks details about the Demucs algorithm, including its size, training, and potential artifacts. This information is crucial for reproducibility and understanding the limitations of the approach.\n- **CycleGAN**: The paper does not provide sufficient details about the CycleGAN model, such as its architecture, training parameters, and hyperparameters. This makes it difficult to reproduce the results.\n- **Mel Scale**: A reference or citation for the Mel scale would be useful for readers unfamiliar with this concept.\n\n#### Experiments\n- **Dataset Selection**: The paper does not provide enough details about the selection of the 10,000 tracks used for training. Sharing the IDs of the songs would enhance reproducibility.\n- **Evaluation Metrics**: The evaluation metrics (STOI, FID) are presented without justification. The paper should explain why these metrics are appropriate for evaluating the generated audio.\n- **Human Evaluation**: The human evaluation process lacks depth, including inter-annotator agreement and the cultural background of the experts. This information is crucial for assessing the reliability of the evaluation.\n\n#### Minor Comments\n- **Technical Details**: The paper lacks details on the phase retrieval and Mel spectrogram inversion processes, which are critical for the perceptual quality of the generated audio.\n- **Time-Equivariance**: The claim that the Demucs model is time-equivariant is not justified, given its architecture.\n- **Quantization and Windowing**: The paper should clarify the quantization process and the windowing function used in the short-time Fourier transform.\n- **Regression Method**: The binarization of targets in the evaluation metric is not well-justified. An ordinary least squares or isotonic regression might be more appropriate.\n\n### Final Decision\nGiven the significant issues with the title, abstract, introduction, related works, method, and experiments, the paper requires substantial revisions before it can be considered for publication in a top-tier conference. The lack of detail and the misleading claims need to be addressed to ensure the paper meets the high standards of the conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Problem Relevance:**\n   - The paper addresses a significant challenge in reinforcement learning (RL) and imitation learning (IL): combining the strengths of meta-RL and IL to improve sample efficiency and adaptability to new tasks. This is a relevant and important problem in the field.\n\n2. **Methodological Innovation:**\n   - The proposed method, PERIL, integrates ideas from meta-RL (specifically PEARL) and IL, which is a novel and promising direction. The use of demonstrations to precondition exploration policies is a valuable contribution.\n\n3. **Empirical Results:**\n   - The empirical results show that PERIL outperforms baselines in several benchmark tasks, which is a strong indicator of the method's effectiveness.\n\n4. **Clarity of Motivation:**\n   - The motivation for combining meta-RL and IL is well-articulated, and the paper clearly explains the benefits of this approach, such as improved adaptation rates and robustness to task alterations.\n\n#### **Weaknesses:**\n1. **Complexity and Clarity:**\n   - The method is quite complex, with multiple loss terms and components that are not well-explained. The notation and mathematical formulation are inconsistent and confusing, making it difficult for readers to understand the method's components and how they fit together.\n\n2. **Lack of Ablation Studies:**\n   - The paper lacks comprehensive ablation studies to determine the necessity and impact of each component of PERIL. This makes it hard to understand which parts of the method are crucial for its performance.\n\n3. **Comparison to Related Work:**\n   - The paper does not adequately compare PERIL to other methods that combine meta-RL and IL, such as Mendonca et al. and Zhou et al. This is a significant gap in the experimental section.\n\n4. **Assumptions and Limitations:**\n   - The assumption that an expert distribution \\( p_{\\pi_E}(\\tau \\mid z) \\) is available is strong and may not be realistic in many practical scenarios. The paper should discuss this limitation more thoroughly.\n\n5. **Statistical Significance:**\n   - The experiments lack multiple seeds and statistical significance tests, which are essential for validating the results. The lack of confidence intervals in the plots further reduces the credibility of the empirical findings.\n\n6. **Zero-Shot Learning Claims:**\n   - The claim of zero-shot learning is not well-supported. The experiments suggest that PERIL requires some samples from the target environment, making it a few-shot method rather than a zero-shot method.\n\n7. **Notational and Typographical Issues:**\n   - The paper has several notational inconsistencies and typographical errors, which detract from its overall quality and readability.\n\n#### **Reviewer Comments:**\n- **Reviewer 1** and **Reviewer 2** both highlight the complexity and lack of clarity in the method's formulation. They suggest adding more baselines and ablation studies to better understand the contributions of each component.\n- **Reviewer 3** emphasizes the need for a better treatment of prior work and more detailed experiments to substantiate the claims. They also point out the lack of high-dimensional observation spaces in the experiments.\n- **Reviewer 4** raises concerns about the strong assumptions and the clarity of the method. They suggest that the paper should provide more details on the optimization of the loss terms and the definition of the posterior \\( p(z \\mid \\tau) \\).\n\n### Final Decision\nGiven the significant weaknesses in clarity, lack of comprehensive ablation studies, and the need for better comparison to related work, the paper is not yet ready for publication in a top-tier conference. While the problem addressed is important and the method shows promise, the current presentation and empirical validation are insufficient to meet the high standards of a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper introduces Max-Q Entropy Search (MQES), an information-theoretic exploration principle for continuous reinforcement learning (RL). The key idea is to maximize the information about the globally optimal distribution of the Q function, which is a novel and interesting approach. The method aims to balance exploration and exploitation by recognizing both epistemic and aleatoric uncertainties. This is a significant contribution to the field, as efficient exploration in continuous action spaces remains a challenging problem.\n\n#### Technical Soundness\nThe technical approach is theoretically grounded, leveraging concepts from information theory and distributional RL. However, several issues have been raised by the reviewers:\n1. **Clarity and Presentation**: The paper is criticized for being hard to follow, with unclear notation and terminology. For example, the mutual information maximization in Section 4.1 is not well-explained, and the notation in Equations 8 and 9 is confusing.\n2. **Algorithmic Details**: The parameterization of policies \\(\\pi_E\\) and \\(\\pi_T\\) is not clearly explained, and the practical implementation details are lacking. This makes it difficult to understand how the method is implemented and how it can be integrated with other off-policy algorithms.\n3. **Theoretical Justification**: Some theoretical steps are not clearly justified, and the derivation of key equations is not detailed enough.\n\n#### Empirical Evaluation\nThe empirical evaluation is conducted on Mujoco environments, and the results show that MQES outperforms state-of-the-art algorithms like SAC and DSAC. However, there are several concerns:\n1. **Baseline Comparisons**: The paper does not compare MQES with other relevant methods like OAC (Optimistic Actor-Critic) and VIME (Variational Information Maximizing Exploration). This limits the robustness of the empirical results.\n2. **Hyperparameter Sensitivity**: The impact of hyperparameters (\\(\\alpha\\), \\(\\beta\\), and \\(C\\)) on the performance of MQES is not discussed, and no ablation studies are provided.\n3. **Horizon Length**: The horizon is set to 100, which is shorter than the default 1000 used in many other studies. This choice is not justified, and the performance of MQES with a longer horizon is not evaluated.\n4. **Sparse Reward Environments**: The paper does not evaluate the exploratory behavior of MQES in sparse-reward environments, which are a central motivation for sophisticated exploration techniques.\n\n#### Writing Quality\nThe writing quality is generally poor, with several notational inconsistencies and vague statements. The paper could benefit from a thorough revision to improve clarity and readability.\n\n### Final Decision\nGiven the novel and interesting idea of MQES, the paper has the potential to make a significant contribution to the field of continuous RL. However, the current version of the paper has several major issues that need to be addressed:\n- **Clarity and Presentation**: The paper needs to be rewritten to improve clarity and provide detailed explanations of key concepts and equations.\n- **Algorithmic Details**: The parameterization of policies and practical implementation details need to be clearly explained.\n- **Empirical Evaluation**: The paper should include comparisons with other relevant methods, ablation studies, and evaluations in sparse-reward environments.\n- **Writing Quality**: The paper should be revised to address notational inconsistencies and improve overall readability.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This approach is intuitive and addresses a significant challenge in multitask learning: how to effectively balance the influence of auxiliary tasks on the target task.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and motivate the proposed method effectively. The methodology is described in a logical and understandable manner.\n\n3. **Empirical Evaluation**:\n   - The authors conduct experiments on both computer vision (Multi-MNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method. The use of multiple domains is a strong point, as it shows the method's potential applicability across different types of tasks.\n\n#### Weaknesses of the Paper\n1. **Lack of Theoretical Justification**:\n   - The paper lacks a formal theoretical analysis or justification for the proposed method. This is a significant gap, as it leaves the reader unsure about the underlying principles and why the method should work. A more rigorous theoretical foundation would strengthen the paper.\n\n2. **Limited Baselines and Experimental Results**:\n   - The baselines chosen for comparison are somewhat limited. While the paper compares against standard multitask learning and a single task-oriented approach, it does not include a comprehensive set of state-of-the-art methods. This makes it difficult to assess the true novelty and effectiveness of $\\alpha$VIL.\n   - The experimental results are not consistently superior to the baselines. On the Multi-MNIST dataset, the results are within a standard deviation of the baselines, and on the NLU tasks, the results are mixed. This lack of clear superiority raises questions about the method's practical value.\n\n3. **Algorithmic Concerns**:\n   - Reviewer 2 raises valid concerns about the ad hoc nature of the algorithm and the lack of clear intuition behind certain steps (e.g., arbitrarily subtracting 1 from all weights). These issues need to be addressed to ensure the method is well-justified and robust.\n\n4. **Statistical Significance**:\n   - The paper does not provide statistical significance scores for the results, which is crucial for evaluating the robustness of the findings. This omission makes it difficult to determine whether the observed improvements are statistically significant.\n\n5. **Experimental Setup**:\n   - The choice of experimental setup, particularly the use of only one auxiliary task in the vision domain, is limiting. More comprehensive experiments with multiple auxiliary tasks would provide a better understanding of the method's performance in more complex scenarios.\n\n#### Additional Suggestions\n1. **Theoretical Analysis**:\n   - The authors should provide a more detailed theoretical analysis of the proposed method, including formal proofs or derivations that explain why the method works and under what conditions it is expected to be effective.\n\n2. **Comprehensive Baselines**:\n   - The paper should include a broader range of baselines, including state-of-the-art methods for dynamic task weighting and meta-learning. This will provide a more comprehensive comparison and better demonstrate the method's strengths and weaknesses.\n\n3. **Statistical Significance**:\n   - The authors should report statistical significance scores for all results to ensure that the observed improvements are not due to random chance.\n\n4. **Additional Experiments**:\n   - The authors should conduct more experiments, particularly with multiple auxiliary tasks, to provide a more robust evaluation of the method's performance.\n\n5. **Ablation Studies**:\n   - Ablation studies should be included to analyze the impact of different components of the proposed method, such as the task-specific updates and the weight adjustment mechanism.\n\n### Final Decision\nFinal Decision: Reject\n\nThe paper presents an interesting and intuitive method for multitask learning, but it falls short in several critical areas. The lack of theoretical justification, limited experimental results, and ad hoc nature of the algorithm are significant weaknesses that need to be addressed. While the paper has potential, it requires substantial improvements to meet the standards of a top-tier conference."
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Dataset Contribution**:\n   - The paper constructs a large-scale dataset of over one million slides, which is a significant contribution to the field. This dataset can be a valuable resource for future research in layout representation and graphic design intelligence.\n\n2. **Innovative Approach**:\n   - The paper proposes a novel method, CanvasEmb, which leverages transformer-based models for layout representation. This is a fresh and interesting approach, inspired by the success of self-supervised pre-training in natural language processing.\n\n3. **State-of-the-Art Performance**:\n   - The model achieves state-of-the-art performance on the proposed tasks of element role labeling and image captioning, which are important for understanding and generating graphic designs.\n\n4. **Clarity and Writing**:\n   - The paper is well-written and easy to follow. The authors provide a clear explanation of their method and the tasks they evaluate.\n\n#### Weaknesses of the Paper\n\n1. **Lack of Dataset Details**:\n   - The paper does not provide sufficient details about the dataset, such as the distribution of elements per slide, the completeness of properties, and the method of parsing. These details are crucial for understanding the richness and quality of the dataset.\n\n2. **Weak Evaluation**:\n   - The evaluation is limited to a few tasks and lacks a comprehensive comparison with existing methods. The paper should include comparisons with other state-of-the-art models, such as those proposed in Cao et al. (SIGGRAPH 2019), Li et al. (ICLR 2019), and READ (CVPR 2020).\n\n3. **Missing Baselines**:\n   - The paper only compares its method to a gradient-boosted decision tree, which is a relatively simple baseline. It would be more convincing to compare against more complex models, such as Graph Neural Networks, which are known to capture rich structural properties of layouts.\n\n4. **Task Complexity**:\n   - The tasks proposed for evaluation (element role labeling and image captioning) are relatively simple. The paper should evaluate the model on more complex tasks, such as layout auto-completion and layout retrieval, to demonstrate its full potential.\n\n5. **Technical Details**:\n   - The paper lacks detailed technical explanations of the transformer model and the attention mechanism. A figure or mathematical formulation would help readers better understand the model.\n\n6. **Motivation for Transformer-Based Approach**:\n   - The paper does not provide a strong motivation for using a transformer-based model over other architectures, such as Graph Neural Networks, which are computationally less intensive and can capture rich structural properties of layouts.\n\n#### Additional Considerations\n\n1. **Reproducibility**:\n   - The paper should provide more training details, such as the number and type of GPUs used, training time, and specific hyperparameters. This information is crucial for reproducibility.\n\n2. **Future Work**:\n   - The authors should discuss potential future work, such as extending the model to other types of graphic designs and exploring more complex tasks.\n\n### Final Decision\n\nGiven the significant contributions of the paper, particularly the large-scale dataset and the innovative approach, it has the potential to be a valuable addition to the field. However, the weaknesses in evaluation and the lack of detailed technical explanations and comparisons with existing methods are significant concerns. The paper would benefit from addressing these issues to strengthen its impact and credibility.\n\nFinal Decision: [Reject]\n\nThe paper should be rejected in its current form but with the strong recommendation for the authors to address the identified weaknesses and resubmit for consideration."
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Research Topic and Hypothesis\nThe paper addresses a significant and relevant problem in multi-modal deep neural networks (DNNs): the greedy nature of learning, where models tend to rely heavily on one modality while under-utilizing others. This issue is important because it can lead to suboptimal generalization performance. The hypothesis is well-motivated and the problem is clearly defined.\n\n#### Methodology\nThe authors introduce the concept of conditional utilization rate and conditional learning speed as metrics to quantify the imbalance in modality utilization. They propose a training algorithm, balanced multi-modal learning, to address the greedy learning issue. The methodology is logical and the proposed metrics are novel and potentially useful.\n\n#### Experiments and Results\nThe paper presents experiments on three datasets: Colored MNIST, Princeton ModelNet40, and NVIDIA Dynamic Hand Gesture Dataset. The results show that the proposed method can improve generalization performance. However, there are several issues with the experimental setup and results:\n\n1. **Inconsistencies in Results**: Reviewer 2 points out inconsistencies in the results between the initial and revised versions of Table 1, particularly for ModelNet40 and NVGesture-scratch. This raises concerns about the reliability of the experimental results.\n2. **Lack of Statistical Significance**: Reviewer 4 notes that the differences between the vanilla and guided methods are not statistically significant for ModelNet40, NVGesture-scratch, and NVGesture-pretrained. This weakens the support for the main claims.\n3. **Limited Dataset Diversity**: Reviewer 3 and Reviewer 6 suggest that the experiments should include more natural modalities, such as audio-visual datasets, to better validate the claims. The current datasets are primarily visual, which limits the generalizability of the results.\n4. **Adequate Baselines and Comparisons**: Reviewer 6 and Reviewer 7 point out that the paper lacks comparisons with other methods that address the imbalance problem, such as RUBi and other recent works. This makes it difficult to assess the novelty and effectiveness of the proposed method.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to follow. The problem is clearly stated, and the methodology is well-explained. However, there are some minor issues with clarity, particularly in the re-balancing step and the derivation of uni-modal predictions (as noted by Reviewer 5).\n\n#### Theoretical Support\nThe paper lacks strong theoretical support for the greedy learner hypothesis and the proposed method. Reviewer 6 and Reviewer 7 both mention the need for more detailed analysis and theoretical justification.\n\n### Final Decision\nGiven the issues with experimental results, lack of statistical significance, limited dataset diversity, and inadequate comparisons with related work, the paper does not meet the high standards required for a top-tier conference. While the topic is interesting and the methodology is novel, the current state of the paper does not provide strong enough evidence to support its main claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Technical Novelty and Significance\n- **Novelty**: The paper introduces a new approach to target propagation (TP) using regularized inversion, which is a novel contribution. However, the novelty is somewhat limited by the fact that the proposed method still requires weight transport, which is a significant drawback compared to other TP methods that avoid this requirement. The method is also computationally more expensive than backpropagation (BP), which limits its practical appeal.\n- **Significance**: The significance of the contribution is moderate. While the method is novel and has some theoretical underpinnings, the practical benefits over BP are not convincingly demonstrated. The paper does not provide strong theoretical guarantees or extensive empirical evidence to show that the proposed TP method outperforms BP in a wide range of scenarios.\n\n#### Empirical Novelty and Significance\n- **Empirical Results**: The empirical results are mixed. The paper shows some improvements in training recurrent neural networks (RNNs) with long sequences, but the results are not consistently better than BP. The experiments on image datasets (e.g., CIFAR-10) show low accuracy, which raises questions about the generalizability and scalability of the method. The lack of comparison with other RNN optimization techniques (e.g., Hessian-Free Optimization) further weakens the empirical significance.\n- **Experimental Design**: The experimental design is somewhat limited. The paper focuses on a narrow set of tasks and does not provide a comprehensive evaluation across a wide range of datasets and architectures. This limits the generalizability of the findings.\n\n#### Clarity and Correctness\n- **Clarity**: The paper is generally well-written and clear. The authors provide a good explanation of the proposed method and its relationship to existing TP methods. However, there are some minor issues with clarity, such as the typo mentioned by Reviewer 2.\n- **Correctness**: The claims and statements are mostly correct, but there are some minor issues. For example, the relationship between TP and Gauss-Newton methods is not fully explored, and the claim that the proposed method avoids the vanishing/exploding gradient problem is not well-supported.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Points out significant limitations of the proposed method, including the need for weight transport and the lack of theoretical guarantees. The reviewer is not convinced by the empirical results.\n- **Reviewer 2**: Finds the method novel and computationally tractable but is concerned about the variability in training loss and accuracy. The reviewer suggests further theoretical analysis.\n- **Reviewer 3**: Criticizes the experimental results, particularly the low accuracy on image datasets and the lack of comparison with other RNN optimization techniques.\n- **Reviewer 4**: Suggests that the theoretical analysis is incomplete and that the empirical evidence is lacking. The reviewer recommends more extensive experimentation and comparison with other methods.\n\n### Final Decision\nGiven the mixed feedback from the reviewers and the limitations in both the theoretical and empirical aspects of the paper, the paper does not meet the high standards of a top-tier conference. While the proposed method is novel, the practical benefits over existing methods are not sufficiently demonstrated, and the experimental results are not robust enough to support the claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Novel Approach**: The paper introduces a novel approach to reinforcement learning (RL) by leveraging boosting techniques, which is an interesting and potentially impactful idea. The use of boosting to convert weak learners into strong policies is a significant contribution.\n2. **Theoretical Rigor**: The paper provides solid theoretical analysis, including sample complexity and running time bounds. The bounds are polynomial in the natural parameters of the problem and do not explicitly depend on the number of states, which is a notable achievement.\n3. **Techniques and Methods**: The paper uses a non-convex variant of the Frank-Wolfe method and recent advances in gradient boosting to overcome the non-convexity of the value function over policy space. This is a technically sophisticated approach.\n\n#### Weaknesses:\n1. **Clarity and Readability**: Multiple reviewers have pointed out significant issues with the clarity and readability of the paper. The notation is dense and often undefined, making it difficult to follow the technical details. The paper lacks motivation and intuition for many of its definitions and algorithms, which hinders understanding.\n2. **Empirical Evaluation**: The paper lacks empirical evaluation, which is a major weakness. While the theoretical results are strong, some initial experiments would provide valuable validation and practical insights into the effectiveness of the proposed algorithms.\n3. **Comparison to Prior Work**: The paper does not adequately discuss the differences between designing boosted contextual bandits and boosted RL, which is a relevant comparison given the similarity to prior work (Hazan and Singh 2021). This makes it difficult to assess the novelty of the contributions.\n4. **Assumptions and Practicality**: The assumptions made in the paper, such as the availability of an efficient exploration scheme and the existence of weak learners, are not well-justified or discussed in detail. This raises questions about the practicality of the proposed method.\n5. **Notational Inconsistencies**: There are several notational inconsistencies and undefined terms throughout the paper, which can lead to confusion and errors in understanding the results.\n\n#### Reviewer Feedback:\n- **Reviewer 1**: Improved the score from 3 to 5 after author responses but still has concerns about clarity and suggests a rewrite.\n- **Reviewer 2**: Recommends weak accept but points out limited novelty and the need for experiments.\n- **Reviewer 3**: Recommends weak accept, noting the interesting topic and rigorous analysis but suggests addressing the restrictive assumption of weak learners.\n- **Reviewer 4**: Strongly recommends rejection due to lack of clarity, undefined notations, and practical implementation issues.\n- **Reviewer 5**: Recommends rejection due to significant clarity and readability issues, lack of empirical evaluation, and notational inconsistencies.\n- **Reviewer 6**: Recommends weak accept, highlighting the novel approach and theoretical results but also pointing out the need for better motivation, clarity, and empirical validation.\n\n### Final Decision\nGiven the significant issues with clarity, readability, and the lack of empirical evaluation, despite the strong theoretical contributions, the paper is not ready for publication in its current form. The authors should address the clarity issues, provide more motivation and intuition, and include some initial experiments to validate the effectiveness of the proposed algorithms.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions**:\n   - The paper provides a rigorous theoretical analysis of the Feedback Alignment (FA) algorithm in the context of deep linear networks. It offers convergence guarantees for both continuous and discrete dynamics, which is a significant contribution to the understanding of FA.\n   - The authors introduce the concept of implicit anti-regularization, which is an interesting and novel observation. This phenomenon, where certain initializations can lead to slower convergence for larger eigenvalues, adds to the understanding of the dynamics of FA.\n\n2. **Clarity and Structure**:\n   - The paper is well-structured and clearly written. The authors are transparent about their assumptions and the steps taken in their analysis.\n   - The numerical experiments support the theoretical findings and provide a good illustration of the results.\n\n3. **Relevance**:\n   - The study of FA is timely and relevant, given the recent interest in alternative training methods to backpropagation. The paper contributes to the broader understanding of how and why FA works, which is valuable for the machine learning community.\n\n#### Weaknesses of the Paper\n\n1. **Assumptions and Generalizability**:\n   - The analysis is heavily dependent on the assumption that the data matrices, FA matrices, and initial weight matrices can be diagonalized simultaneously. This assumption simplifies the problem to one-dimensional dynamics, which may not be representative of real-world scenarios.\n   - The results are primarily derived under specific initializations (e.g., spectral initialization), which are not commonly used in practice. This limits the generalizability of the findings to more practical settings.\n   - The gap between the theoretical results and the empirical successes of FA on non-linear networks is significant. The paper does not bridge this gap, which is a major limitation.\n\n2. **Empirical Evidence**:\n   - The numerical experiments are limited and do not explore the behavior of FA under commonly used initialization schemes (e.g., random initialization). This makes it difficult to assess the practical relevance of the theoretical findings.\n   - The paper could benefit from additional empirical evidence to support the theoretical claims, especially in non-spectral settings.\n\n3. **Comparative Analysis**:\n   - The paper does not provide a comprehensive comparison with other optimization methods, such as gradient descent (GD). While it mentions that FA can converge faster than GD in some cases, a more detailed comparison would strengthen the paper.\n\n#### Reviewer Feedback\n\n- **Reviewer 1** and **Reviewer 4** both raise concerns about the oversimplification of the problem due to the one-dimensional dynamics and the specific initializations used. They suggest that the results may not generalize well to more practical settings.\n- **Reviewer 2** and **Reviewer 3** acknowledge the theoretical contributions and the clarity of the paper but also point out the limitations in terms of generalizability and the need for more empirical evidence.\n- **Reviewer 3** is the most positive, recommending acceptance based on the theoretical insights and the novelty of the implicit anti-regularization phenomenon.\n\n### Final Decision\n\nGiven the significant theoretical contributions and the clarity of the paper, the paper has merit. However, the limitations in terms of generalizability and the lack of empirical evidence under practical settings are substantial concerns. The paper would benefit from additional work to address these issues, but as it stands, the contributions are not strong enough to justify acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Frequency-Based Understanding**:\n   - The paper provides a frequency-based understanding of adversarial examples, which is a novel and interesting perspective. This approach challenges the common misconception that adversarial examples are high-frequency noise.\n   - The authors present both theoretical and empirical findings to support their claims, which adds credibility to their work.\n\n2. **Insightful Contributions**:\n   - The paper highlights the dataset dependency of adversarial examples, which is an important observation. This insight can guide future research and development of more effective adversarial attacks and defenses.\n   - The introduction of a new measure of noise gradient and the analysis of frequency properties in adversarial training are valuable contributions to the field.\n\n3. **Experimental Rigor**:\n   - The authors conduct extensive experiments on multiple datasets (CIFAR-10, TinyImageNet, ImageNet) and provide detailed results. This thoroughness in experimentation strengthens the validity of their findings.\n   - The experimental design in sections 6.1 and 6.2 is well-thought-out and provides new insights into the frequency properties of adversarial examples and the trade-off between accuracy and robustness.\n\n4. **Practical Implications**:\n   - The paper's findings have practical implications for adversarial training, suggesting new methods to improve the robustness of deep learning models. This is particularly relevant given the growing importance of adversarial robustness in real-world applications.\n\n#### Weaknesses of the Paper\n1. **Motivation and Novelty**:\n   - The motivation for the paper is somewhat weak, as the common misconception about high-frequency adversarial examples has already been questioned in the literature. The authors should better justify their contribution in this context.\n   - The novelty of the dataset dependency observation is limited, as it has been previously noted in the community. The paper should provide more unique insights to stand out.\n\n2. **Scope and Generalization**:\n   - The paper primarily focuses on natural image datasets and does not explore other types of data (e.g., radar signals, vibration signals). This limits the generalizability of the findings.\n   - The experiments are mainly conducted using ResNet18 and VGG16, and the impact of other network architectures and optimization properties is not considered. This is a significant limitation that should be addressed.\n\n3. **Clarity and Presentation**:\n   - The paper's structure and presentation could be improved. The related works section is extensive, and the unique contributions of the paper are not clearly highlighted. This makes it difficult for readers to grasp the main ideas.\n   - The figures are presented earlier than mentioned in the text, which can be inconvenient for readers. Minor grammatical errors also need to be corrected.\n\n4. **Comparative Analysis**:\n   - The paper should provide a more detailed comparison with related works, particularly with Bernhard et al. (2021). The similarities and differences should be clearly outlined to highlight the unique contributions of this work.\n\n### Final Decision\nDespite the weaknesses, the paper provides valuable insights and contributions to the field of adversarial robustness. The frequency-based understanding of adversarial examples and the dataset dependency observation are significant and novel. The extensive experiments and practical implications further strengthen the paper's value. However, the authors should address the limitations and improve the clarity and presentation of their work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Problem Relevance and Motivation**\n- **Relevance**: The problem of multi-objective optimization in experimental design is highly relevant and well-motivated. The authors correctly identify the need for a platform that can handle multiple conflicting objectives efficiently, which is a common challenge in many scientific and engineering domains.\n- **Motivation**: The motivation for developing AutoOED is clear and well-articulated. The platform aims to address the limitations of existing tools, which often require significant coding expertise and do not support multi-objective optimization effectively.\n\n#### 2. **Technical Contributions**\n- **Novelty**: The technical novelty of the paper is somewhat limited. The main novel contribution is the Believer-Penalizer (BP) strategy for batch experiments. However, this strategy is described in a relatively straightforward manner and lacks a rigorous theoretical foundation. The authors do not provide a clear explanation of how the combination of Kriging Believer (KB) and Local Penalization (LP) works in theory, which is a significant weakness.\n- **Significance**: The significance of the contributions is also somewhat marginal. While the BP strategy is a useful addition, it is not a major breakthrough in the field of multi-objective Bayesian optimization. The platform itself is well-designed and implemented, but it primarily integrates existing techniques rather than introducing fundamentally new methods.\n\n#### 3. **Empirical Evaluation**\n- **Benchmarks**: The empirical evaluation is a weak point of the paper. The authors claim that AutoOED outperforms other methods, but the results in Figures 4 and 5 show a mixed picture. AutoOED does not consistently outperform the baselines, and in some cases, it underperforms. The claims of superior performance are overstated and not well-supported by the empirical results.\n- **Real-World Applications**: The real-world benchmark is a positive aspect, but it is limited in scope. The comparison is only against random search, which is a weak baseline. More robust comparisons with other state-of-the-art methods would have strengthened the empirical evaluation.\n\n#### 4. **User-Friendly Design and Accessibility**\n- **GUI and Usability**: The graphical user interface (GUI) is a significant strength of the platform. It makes AutoOED accessible to users with little or no coding experience, which is a valuable feature. The modular design of the platform also allows researchers to quickly develop and evaluate their own MOBO algorithms.\n- **Open-Source**: The fact that AutoOED is open-source is a positive aspect, as it promotes transparency and reproducibility. The platform can serve as a valuable tool for the research community.\n\n#### 5. **Clarity and Presentation**\n- **Writing and Structure**: The paper is well-written and well-structured. The authors clearly explain the problem, the methodology, and the results. The figures and tables are well-presented and help to illustrate the key points.\n- **Minor Issues**: There are a few minor issues with the writing, such as redundant specifications of abbreviations and minor typos. These can be easily addressed in the final version.\n\n### Final Decision\nWhile the paper has several strengths, particularly in terms of the user-friendly design and accessibility of the platform, the technical novelty and empirical evaluation are not strong enough to justify acceptance at a top-tier conference. The claims of superior performance are not well-supported by the empirical results, and the Believer-Penalizer strategy lacks a rigorous theoretical foundation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Clarity and Readability**\nThe paper is consistently criticized for being difficult to follow and lacking clarity. Reviewers 1, 2, and 4 all mention that the paper is hard to understand, with long and repetitive sentences, and missing details. This is a significant issue for a top-tier conference, where clarity and readability are crucial for the paper's impact and reproducibility.\n\n#### **2. Technical Novelty and Significance**\nThe proposed Symbolic Continuous Stimulus (SCS) representation is a novel idea, but its utility and significance are questioned by multiple reviewers. Reviewer 3 and Reviewer 4 both express doubts about the applicability of SCS outside the limited experimental setting. Reviewer 4 also points out that the SCS representation has limitations, such as the inability to adapt to more latent dimensions and potential failure to extrapolate to larger dimensions. The paper does not provide a compelling argument for why SCS is superior to other representations, such as fully continuous representations, which are widely used in modern approaches.\n\n#### **3. Empirical Evaluation**\nThe empirical evaluation is another major concern. Reviewer 1 and Reviewer 4 both note that the experiments are insufficient to support the claimed contributions. The results are barely above chance performance, and there is a lack of ablation studies and hyperparameter tuning. Reviewer 4 specifically mentions that the DNC, which contains an LSTM, performs worse than an LSTM, suggesting poor training or hyperparameter tuning. Additionally, the paper does not compare SCS with other representations, such as one-hot encodings, in a comprehensive manner.\n\n#### **4. Benchmark and Baselines**\nThe proposed benchmark is an interesting idea, but it is not well-developed or clearly explained. Reviewer 2 and Reviewer 3 both suggest that the benchmark should be detailed more, with more results and ablations. The failure of state-of-the-art models on the benchmark is intriguing, but without additional details and examples, it is difficult to understand the implications of these results.\n\n#### **5. Engagement with Literature**\nThe paper does not adequately engage with the existing literature on compositional generalization. Reviewer 4 points out that the paper's points about discrete vs. continuous representations overlap with long-standing debates in cognitive science, and the paper should be situated within this broader literature. This lack of engagement weakens the paper's contributions and makes it less compelling.\n\n### Final Decision\nGiven the significant issues with clarity, technical novelty, empirical evaluation, and engagement with the literature, the paper does not meet the standards of a top-tier conference. The ideas are intriguing, but they need substantial development and refinement to be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Biological Inspiration:**\n   - The paper draws on insights from the motor thalamocortical circuit to design a novel RNN architecture. This interdisciplinary approach is valuable and can provide new perspectives on both neuroscience and machine learning.\n2. **Zero-Shot Learning:**\n   - The proposed architecture demonstrates the ability to perform zero-shot learning, which is a significant achievement in the context of continual learning and hierarchical motor control.\n3. **Technical Clarity:**\n   - Reviewer 4 notes that the methods are clearly described and the experiments are replicable, which is a positive aspect for scientific rigor.\n\n#### **Weaknesses:**\n1. **Lack of Robust Evaluation:**\n   - Reviewer 3 and Reviewer 5 both highlight the need for more comprehensive evaluation. The small scale of the experiments (300 units, 10 motifs) and the lack of testing on more complex tasks or datasets diminish the paper's impact.\n2. **Alternative Hypotheses:**\n   - Reviewer 3 criticizes the paper for not testing obvious alternative hypotheses, such as training the RNN to generalize from end states or training to return to a consistent point at the end of each motif. This lack of thoroughness in exploring alternatives weakens the paper's claims.\n3. **Trivial Solutions:**\n   - Reviewer 5 points out that the task described in the paper admits a trivial solution (hard resetting to an initialization state). This raises questions about the necessity and effectiveness of the proposed architecture.\n4. **Writing and Presentation:**\n   - Reviewer 5 and Reviewer 4 both note issues with the writing and presentation. The paper is described as confusing and difficult to understand, with scattered information and unclear descriptions of the problem and task.\n5. **Technical Issues:**\n   - Reviewer 5 identifies technical issues, such as the lack of a robust description of the continual learning task and the need to demonstrate the necessity of the proposed solution empirically.\n\n#### **Overall Assessment:**\n- **Relevance and Significance:**\n  - The topic of continual learning and hierarchical motor control is relevant and important. However, the paper's contributions are not sufficiently novel or significant to justify acceptance at a top-tier conference.\n- **Empirical and Technical Rigor:**\n  - The paper lacks robust empirical evaluation and fails to address important alternative hypotheses. The technical issues and writing problems further undermine the paper's credibility.\n- **Interdisciplinary Contribution:**\n  - While the biological inspiration is valuable, the paper does not fully capitalize on this by providing a thorough and convincing demonstration of its effectiveness.\n\n### Final Decision\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Well-Written and Clear**: The paper is well-written and clearly conveys its main points and contributions.\n2. **Interesting Approach**: The idea of using VAEs for time series generation, particularly with the inclusion of interpretable components, is novel and interesting.\n3. **Empirical Evaluation**: The authors provide a comprehensive empirical evaluation on four datasets, comparing their method to several state-of-the-art methods.\n4. **Potential for Interpretability**: The proposed architecture has the potential to incorporate domain-specific time patterns, which can be highly valuable in applications requiring transparency.\n\n#### Weaknesses of the Paper\n1. **Lack of Hyperparameter Tuning**: The paper does not provide details on how to select hyperparameters for the introduced decoder building blocks. This is a significant gap, as hyperparameter tuning is crucial for the performance of such models.\n2. **Limited Evaluation of Interpretability**: Despite claiming interpretability as a key contribution, the paper lacks experiments or ablation studies to demonstrate the interpretability of the proposed building blocks.\n3. **Comparison to Related Work**: The paper does not adequately compare its method to other relevant works, such as GP-VAE, and does not discuss the relationship between time series imputation and next-step prediction.\n4. **Technical Clarity**: The paper contains some imprecise or non-standard explanations, particularly in the background and methods sections. For example, the categorization of generative models and the explanation of VAEs are not clear.\n5. **Experimental Methodology**: The experiments are conducted on synthetic or novel datasets, making it difficult to compare the proposed method to other works in the literature. Additionally, the next-step prediction evaluation using post-hoc learning of an LSTM model is not well-justified.\n6. **Limited Contribution**: The overall contribution of the paper is limited, as the proposed method does not seem to introduce substantial originality beyond applying a standard VAE to time series data.\n\n#### Reviewer Comments Summary\n- **Reviewer 1**: Highlights the need for better hyperparameter tuning and evaluation of interpretability. Suggests the paper is not ready for publication at ICLR.\n- **Reviewer 2**: Questions the originality of the proposed method and suggests the need for a more detailed explanation of the VAE loss function and a broader set of benchmarks.\n- **Reviewer 3**: Points out the lack of general context in the literature review, imprecise claims, and issues in the experimental methodology. Suggests major revisions.\n- **Reviewer 4**: Criticizes the lack of discussion on interpretability and the limited performance improvement over state-of-the-art methods.\n\n### Final Decision\nGiven the significant weaknesses in the paper, particularly the lack of hyperparameter tuning, limited evaluation of interpretability, and issues in the experimental methodology, the paper is not ready for publication at a top-tier conference. While the idea is interesting and the paper is well-written, these gaps need to be addressed to strengthen the contributions and empirical evaluation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Effectiveness of Proposed Methods**:\n   - The paper introduces two methods, Computation Redistribution (CR) and Sample Redistribution (SR), which are shown to be effective in improving face detection accuracy, especially for small faces.\n   - The ablation studies and experimental results on the WIDER FACE dataset demonstrate significant improvements over the baseline and state-of-the-art methods.\n\n2. **State-of-the-Art Performance**:\n   - The proposed SCRFD-34GF outperforms the best competitor, TinaFace, by 4.78% in AP at the hard set while being more than 3 times faster on GPUs with VGA-resolution images.\n   - The method achieves state-of-the-art performance across a wide range of compute regimes.\n\n3. **Generalizability**:\n   - The computation redistribution method is general and can be applied to other applications beyond face detection.\n\n#### Weaknesses:\n1. **Lack of Novelty in Search Strategies**:\n   - The search strategies for computation redistribution are straightforward and not particularly innovative.\n   - The paper lacks a comparison with other well-known network search methods, which makes it difficult to determine if the improvements are due to the search strategy or the proposed methods.\n\n2. **Insufficient Details and Explanations**:\n   - The paper lacks a clear overview figure of the framework and a detailed algorithm for the computation redistribution method.\n   - The sample distribution method is not well-explained, and the binary probability model's advantages over other models are not clearly stated.\n   - The cross-dataset evaluation is insufficient, and the paper does not compare with the best-performing methods on datasets like AFW, Pascal, and FDDB.\n\n3. **Minor Issues in Claims**:\n   - Some claims and statements require minor adjustments to be fully correct, as noted by Reviewer 7.\n\n#### Reviewer Feedback:\n- **Positive Feedback**:\n  - Reviewers 1, 2, and 3 are satisfied with the author's responses to their questions.\n  - Reviewer 4 and 5 acknowledge the effectiveness of the proposed methods and the significant improvements in performance.\n  - Reviewer 6 finds the paper acceptable despite some questions and suggests that the paper has enough innovation and good results.\n\n- **Negative Feedback**:\n  - Reviewer 4 and 7 point out the lack of novelty in the search strategies and the need for more detailed explanations and comparisons.\n  - Reviewer 7 highlights the need for a more comprehensive cross-dataset evaluation.\n\n### Final Decision\nDespite the paper's significant contributions in improving face detection accuracy and efficiency, the lack of novelty in the search strategies and the need for more detailed explanations and comparisons are notable weaknesses. However, the effectiveness of the proposed methods and the state-of-the-art performance on the WIDER FACE dataset are strong points in favor of the paper.\n\nGiven the balance of strengths and weaknesses, and the positive feedback from several reviewers, the paper should be accepted with the condition that the authors address the minor issues and provide more detailed explanations and comparisons in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Theoretical Insights**:\n   - The paper provides a novel analysis of the fixed points of classical value estimation algorithms (TD, FVI, and RM) in the overparameterized linear setting. This is a significant contribution as it extends the understanding of these algorithms beyond the traditional underparameterized setting.\n   - The unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to different constraints is a valuable theoretical insight.\n\n2. **Practical Implications**:\n   - The paper introduces two regularizers that improve the stability and performance of TD and FVI in deep RL settings. This has practical implications for improving the robustness of RL algorithms.\n\n3. **Empirical Validation**:\n   - The empirical results, while limited to toy environments, demonstrate the effectiveness of the proposed regularizers. This provides some validation for the theoretical findings.\n\n#### Weaknesses:\n1. **Theoretical Rigor**:\n   - Several reviewers have pointed out issues with the theoretical results, including:\n     - Lack of proper credit to existing results (e.g., Theorem 2 and Theorem 4).\n     - Mistakes in the proofs (e.g., in the proof of Theorem 4).\n     - The assumption that the matrix \\( M^\\top D_k(M - \\gamma N) \\) is eigendecomposable needs justification.\n     - The generalization bound in Theorem 5 has an explosive upper bound, which needs further explanation.\n\n2. **Relevance to Deep RL**:\n   - The paper claims to have implications for deep RL, but several reviewers have noted that the overparameterized linear setting is not a direct analog of deep neural networks. The width of the last layer in deep RL is often not as large as the number of features in the overparameterized linear setting.\n   - The connection to deep RL is not well-motivated, and the paper could benefit from a more detailed discussion of how the theoretical results translate to practical deep RL settings.\n\n3. **Empirical Evaluation**:\n   - The empirical evaluation is limited to toy environments, which may not be representative of real-world RL problems. More complex environments would provide stronger evidence of the practical utility of the proposed regularizers.\n   - The figures and captions are small, making it difficult to interpret the results.\n\n4. **Clarity and Presentation**:\n   - The paper could benefit from clearer explanations and better writing. Several reviewers noted grammatical issues and imprecise language.\n   - The motivation for the study and the implications of the results are not always clear.\n\n### Final Decision\nGiven the significant theoretical insights and practical implications of the proposed regularizers, the paper has the potential to contribute valuable knowledge to the field of reinforcement learning. However, the issues with theoretical rigor, relevance to deep RL, and empirical evaluation are substantial and need to be addressed.\n\n**Final Decision: Reject**\n\nThe paper should be rejected for publication in its current form but with a strong recommendation for revision. The authors should:\n- Address the theoretical issues and provide proper credit to existing results.\n- Strengthen the connection to deep RL and provide more complex empirical evaluations.\n- Improve the clarity and presentation of the paper."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Technical Novelty**: The paper introduces the Believer-Penalizer (BP) strategy, which is a novel approach for batch experiments in multi-objective Bayesian optimization (MOBO). However, the novelty of this strategy is somewhat limited. Reviewers 1, 2, and 3 point out that the BP strategy is a straightforward combination of existing techniques (Kriging Believer and Local Penalization). Reviewer 4 also notes that the threshold for deciding between KB and LP is not well-motivated, and alternatives are not evaluated.\n- **Empirical Novelty**: The empirical results are mixed. Reviewer 4 points out that the claims of BP outperforming other methods are overstated, as BP is not consistently the best and sometimes performs worse. Reviewer 2 and 3 also note that the empirical results are not robust and lack clear evidence of the effectiveness of BP.\n- **Significance**: The platform itself is well-designed and implemented, and it addresses a practical need in the field of multi-objective optimization. The graphical user interface (GUI) and the ability to control real-world hardware experiments are significant contributions. However, the overall scientific contribution is limited, as the paper is more of an engineering effort than a methodological advancement.\n\n#### **2. Correctness and Clarity**\n- **Correctness**: The paper has some minor issues with claims and statements. Reviewer 2 points out that the paper is misleading about the number of objectives it can handle (2 or 3, not m >= 2). Reviewer 4 also notes that the claims about the performance of BP are overstated. The paper also lacks clear theoretical justification for the BP strategy.\n- **Clarity**: The paper is well-written and well-organized. Reviewer 5 praises the clarity and organization of the paper. However, there are some minor issues with redundant specifications and typos, as noted by Reviewer 5.\n\n#### **3. Impact and Relevance**\n- **Impact**: The platform has the potential to be useful for researchers and practitioners in the field of multi-objective optimization. The GUI and the ability to handle real-world experiments are significant features that can make the platform valuable.\n- **Relevance**: The problem of multi-objective optimization in experimental design is well-motivated and relevant. The paper addresses a practical need in the field, and the platform can be a valuable tool for researchers.\n\n#### **4. Reviewer Consensus**\n- **Reviewer 1**: Increased score to 5 but leans towards rejection due to limited novelty and mixed performance.\n- **Reviewer 2**: Strongly leans towards rejection due to limited methodological contribution and misleading claims.\n- **Reviewer 3**: Leans towards rejection due to lack of theoretical justification and minor novelty.\n- **Reviewer 4**: Leans towards rejection due to overstated claims and limited empirical significance.\n- **Reviewer 5**: Leans towards acceptance due to well-motivated problem and well-organized paper, but notes minor issues.\n\n### Final Decision\nGiven the mixed reviews and the limited scientific novelty, the paper is more of an engineering effort than a methodological advancement. While the platform itself is well-designed and has practical value, the scientific contributions are not strong enough for a top-tier conference. Therefore, the paper should be rejected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in the field of diffusion models: the high computational cost associated with generating high-quality samples. This is a critical issue, especially for large-scale applications and high-resolution datasets.\n\n2. **Novelty:**\n   - The introduction of Generalized Gaussian Diffusion Models (GGDM) and the Differentiable Diffusion Sampler Search (DDSS) method is a novel approach. The non-Markovian structure of GGDM and the optimization of perceptual losses via gradient descent are innovative contributions.\n\n3. **Empirical Results:**\n   - The paper demonstrates strong empirical results, particularly in reducing the number of inference steps required to generate high-quality samples. The FID scores on LSUN church 128x128 are particularly impressive, showing significant improvements over baselines.\n\n4. **Technical Soundness:**\n   - The method is technically sound, with clear explanations of the reparametrization trick and gradient rematerialization. The use of perceptual losses is well-justified, and the optimization procedure is well-detailed.\n\n#### **Weaknesses of the Paper:**\n1. **Theoretical Justification:**\n   - The paper lacks a strong theoretical foundation for the proposed method. The optimization of perceptual losses, while empirically effective, lacks theoretical guarantees. This is a significant concern, as it is unclear what distribution the samples are optimized to match.\n\n2. **Clarity and Exposition:**\n   - The paper could benefit from clearer explanations and better notation. Several reviewers noted that the double subscripts and the overall notation were confusing. A more detailed explanation of the GGDM and DDSS methods, possibly with visual aids, would improve the clarity.\n\n3. **Generalization to Larger Datasets:**\n   - The paper primarily focuses on small-scale datasets like CIFAR-10 and ImageNet 64x64. While the results are compelling, the method's effectiveness on larger, high-resolution datasets (e.g., LSUN, ImageNet, CelebA) is not thoroughly demonstrated. This is a significant limitation, as the computational benefits of the method are more relevant for larger models.\n\n4. **Comparison with Baselines:**\n   - The paper could provide a more comprehensive comparison with existing methods, particularly in terms of computational efficiency and memory usage. The use of gradient rematerialization is a practical solution, but the computational overhead should be discussed in more detail.\n\n5. **Statistical Properties:**\n   - The statistical properties of the GGDM and the impact of optimizing perceptual losses on the underlying distribution of the DDPM are not well-explored. This is a critical aspect that needs further investigation.\n\n### **Final Decision:**\n\nDespite the significant empirical results and the novel contributions, the paper has several weaknesses that need to be addressed. The lack of theoretical justification, clarity in exposition, and comprehensive evaluation on larger datasets are major concerns. However, the problem addressed is important, and the method shows strong potential.\n\nGiven the feedback from the reviewers and the post-rebuttal comments, the paper has made some improvements, but the core issues remain. Therefore, I recommend that the paper be **rejected** for publication at this top-tier conference. The authors should address the identified weaknesses and resubmit a more polished and theoretically grounded version of the paper.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Theoretical Insight:**\n   - The paper provides a novel theoretical insight into the relationship between weighted empirical risk minimization (ERM) and the optimization of higher-order moments of the loss distribution. This is a valuable contribution to the field of robust optimization.\n   - Theorems 1 and 3 clearly demonstrate how specific choices of weights can lead to the optimization of the mean, variance, skewness, and kurtosis of the loss distribution.\n\n2. **Practical Relevance:**\n   - The proposed weighted mean trick is simple and easy to integrate into existing works, which is a practical advantage.\n   - The empirical results show that the weighted mean trick performs similarly to other specialized robust loss functions, providing a strong theoretical foundation for its use.\n\n3. **Clarity and Presentation:**\n   - The paper is generally well-written and structured, making it accessible to a broad audience.\n   - The authors have addressed some of the reviewer comments, improving the clarity of the paper.\n\n#### **Weaknesses:**\n1. **Theoretical Limitations:**\n   - **Dependence on $\\theta$:** Reviewers 2, 3, and 5 raise a significant concern about the dependence of the weights $W$ on $\\theta$. The current formulation of the theorems and lemmas assumes that $W$ is independent of $\\theta$, which is not the case in practice. This makes the theoretical results less robust and potentially incorrect.\n   - **Negative $\\lambda$:** Reviewer 4 points out that the use of negative $\\lambda$ is problematic and lacks a clear motivation. The paper does not provide a strong justification for why negative $\\lambda$ should be used, and the effects of negative and positive $\\lambda$ are opposite, which can be confusing.\n\n2. **Empirical Limitations:**\n   - **Hyperparameter Tuning:** Reviewer 1 suggests that the empirical improvement might be due to hyperparameter tuning rather than the proposed method itself. This is a valid concern, as the paper does not provide a clear method for choosing the $\\lambda_i$ values.\n   - **Convergence Guarantees:** Reviewer 4 asks for convergence guarantees for Algorithm 1, which are not provided in the paper. This is a significant gap in the empirical evaluation.\n\n3. **Clarity and Notation:**\n   - **Notation Inconsistencies:** Reviewers 4 and 6 point out several notation inconsistencies and unclear statements, which can make the paper difficult to follow. For example, the notation for $W$ and the definition of $\\lambda_{min}$ are not clear.\n   - **Explanation of Results:** The paper could benefit from more detailed explanations of the theoretical results, especially the link between the weighted mean and the regularized loss (Equation 1).\n\n4. **Comparative Analysis:**\n   - **Related Work:** Reviewer 4 suggests comparing the proposed method with other robust optimization techniques, such as the trimmed mean approach and MoM-minimization. This would provide a more comprehensive evaluation of the method's effectiveness.\n\n### Final Decision\n\nGiven the significant theoretical and empirical concerns raised by the reviewers, particularly the issues with the dependence of weights on $\\theta$ and the lack of clear motivation for negative $\\lambda$, the paper does not meet the standards of a top-tier conference. While the theoretical insight is valuable, the current formulation and empirical evaluation are not robust enough to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions**:\n   - The paper provides a novel and comprehensive characterization of the global optima for two-layer ReLU neural networks with L2 regularization. This is a significant theoretical advance over previous work, particularly Pilanci and Ergen (2020), which only provided a single solution.\n   - The authors introduce the concept of \"nearly minimal\" neural networks and show that every Clarke stationary point must be a nearly minimal network. This provides a deeper understanding of the optimization landscape.\n   - The paper offers a polynomial-time algorithm to check if a neural network is a global minimum, which is a practical and important contribution.\n   - The authors prove that for a sufficiently large hidden layer (at most 2*(n+1)), there are no spurious valleys, ensuring that gradient descent will not get stuck in local minima.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and organized, making the technical content accessible and easy to follow.\n   - The analysis is clean and precise, with clear definitions and theorems.\n\n3. **Potential Impact**:\n   - The results provide a rich framework for studying the landscape of neural network training loss through convexity, which can inspire further research and practical applications.\n   - The connection between non-convex loss landscapes and convex optimization is a significant theoretical insight that can help explain the success of deep learning.\n\n#### Weaknesses of the Paper\n\n1. **Scope Limitation**:\n   - The analysis is limited to two-layer ReLU networks with L2 regularization. While this is a significant contribution, extending the results to multi-layer networks or other activation functions would greatly enhance the paper's impact.\n   - The lack of empirical validation (e.g., experiments on real datasets) is a minor weakness, as it would provide practical evidence of the theoretical findings.\n\n2. **Technical Issues**:\n   - There are a few minor technical issues and typos that need to be addressed, as noted by the reviewers. These include:\n     - Correcting the upper bound in the rightmost summation in equation (1).\n     - Clarifying the definition of \\(C_i\\) for \\(i \\in [p+1, 2p]\\).\n     - Fixing the mismatching brace and the use of \\(\\sigma\\) in Section 1.3.\n     - Addressing the issue with the containment under the summation in equation (5).\n\n3. **Comparative Analysis**:\n   - The paper could benefit from a more in-depth comparison with related work, particularly in terms of technical novelty and practical utility. Mentioning and discussing relevant references such as Wang et al. (2019) and Nguyen et al. (2021) would strengthen the paper.\n\n#### Reviewer Feedback\n\n- **Reviewer 1**: Highlights the novel extensions of previous work and the importance of the theoretical contributions. Points out minor technical issues and typos.\n- **Reviewer 2**: Strongly recommends acceptance, praising the paper's novel tools and insights. Suggests future work on extending the results to other activation functions and deeper networks.\n- **Reviewer 3**: Provides a balanced review, noting the paper's strengths and suggesting improvements in terms of comparison with related work and empirical validation.\n- **Reviewer 4**: Strongly recommends acceptance, emphasizing the paper's significant theoretical extensions and practical contributions. Suggests minor improvements and clarifications.\n\n### Final Decision\n\nGiven the significant theoretical contributions, the clarity and precision of the analysis, and the potential impact on the field, the paper should be accepted for publication. The minor technical issues and typos can be addressed in the final version, and the suggestions for future work and additional references can be incorporated to enhance the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in diffusion models: the high computational cost of generating high-quality samples. This is a critical issue in the field of generative models, especially for large-scale applications.\n\n2. **Novel Methodology:**\n   - The introduction of Generalized Gaussian Diffusion Models (GGDM) and Differentiable Diffusion Sampler Search (DDSS) is a novel approach. The method leverages pre-trained diffusion models and optimizes the sampling process by differentiating through sample quality scores, which is a unique and innovative idea.\n\n3. **Empirical Results:**\n   - The paper demonstrates strong empirical results on several datasets, particularly on CIFAR-10 and ImageNet 64x64. The FID scores are significantly improved with fewer inference steps, which is a compelling advantage over existing methods.\n\n4. **Technical Contributions:**\n   - The use of gradient rematerialization and the reparametrization trick to backpropagate through the sampling process is a technically sound approach. The method is well-motivated and the technical details are well-explained.\n\n#### **Weaknesses of the Paper:**\n\n1. **Theoretical Justification:**\n   - The paper lacks a strong theoretical foundation for the proposed method. The optimization of perceptual losses (KID) instead of maximizing ELBOs is a practical choice, but it lacks theoretical guarantees. The authors need to provide more rigorous theoretical analysis to justify why this approach works and what the implications are for the distribution of the generated samples.\n\n2. **Clarity and Exposition:**\n   - The paper could benefit from clearer explanations and better notation. Several reviewers noted that the double subscripts and the overall notation were confusing. A more intuitive explanation of the GGDM and DDSS methods, possibly with visual aids, would improve the clarity of the paper.\n\n3. **Generalization to Larger Datasets:**\n   - The paper primarily focuses on small-scale datasets like CIFAR-10 and ImageNet 64x64. While the results are impressive, the method's performance on larger, high-resolution datasets (e.g., LSUN, ImageNet 256x256) is not thoroughly evaluated. This is a significant gap, as the computational efficiency of the method is more crucial for larger models.\n\n4. **Robustness and Consistency:**\n   - The results on ImageNet 64x64 in the appendix are not as strong as those on CIFAR-10, which raises questions about the robustness and generalizability of the method. The authors should provide more comprehensive results and analysis to address these concerns.\n\n5. **Comparative Analysis:**\n   - The paper could benefit from a more detailed comparison with other state-of-the-art methods, particularly in terms of computational efficiency and sample quality. This would provide a clearer picture of the method's strengths and limitations.\n\n### **Final Decision:**\n\nDespite the significant empirical results and the novel methodology, the paper has several weaknesses that need to be addressed. The lack of theoretical justification, clarity in exposition, and comprehensive evaluation on larger datasets are major concerns. However, the problem addressed is important, and the method shows promising results.\n\nGiven the importance of the problem and the potential impact of the proposed method, I believe the paper should be accepted with the condition that the authors address the identified weaknesses in the final version. The authors should provide more rigorous theoretical analysis, improve the clarity of the paper, and conduct additional experiments on larger datasets.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Theoretical Contribution**:\n   - The paper provides a novel theoretical framework for neural network compression using tropical geometry. Theorem 2, which relates the approximation error to the Hausdorff distance of tropical zonotopes, is a significant contribution.\n   - The background on tropical geometry is well-explained, making the paper accessible to non-domain experts.\n\n2. **Algorithmic Novelty**:\n   - The proposed method using K-means clustering for compressing fully connected layers is novel and has theoretical guarantees.\n   - The algorithm outperforms a previous tropical baseline (Smyrnis & Maragos, 2020) in terms of test accuracy.\n\n3. **Empirical Evaluation**:\n   - The paper includes empirical evaluations on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets, demonstrating the practicality of the proposed method.\n   - The experiments show that the method can achieve competitive performance against non-tropical methods like ThiNet.\n\n#### Weaknesses\n1. **Experimental Limitations**:\n   - The experiments are primarily on small-scale datasets and older architectures (LeNet, VGG). Modern architectures like ResNets and Vision Transformers are not evaluated, which limits the practical relevance of the results.\n   - The paper does not report computational and storage complexity or runtimes, which are crucial for practical applications.\n\n2. **Comparative Analysis**:\n   - The paper does not compare the proposed method with state-of-the-art (SOTA) pruning techniques, which are more recent and advanced than ThiNet (2017).\n   - The comparison with non-tropical methods is limited, and the paper does not provide a clear justification for why the proposed method is competitive against these methods.\n\n3. **Clarity and Reproducibility**:\n   - The paper lacks pseudo-code or code segments for the zonotope generators, making it difficult to reproduce the experiments.\n   - The captions for tables and figures are not self-explanatory, and the paper does not clearly state the limitations of the proposed method.\n\n4. **Overclaiming**:\n   - The abstract and conclusions may overclaim the empirical results. The paper states that the method advances baseline pruning methods and has competitive performance against modern techniques, which is not well-supported by the provided evidence.\n\n### Final Decision\nGiven the significant theoretical contributions and the novel approach to neural network compression using tropical geometry, the paper has strong merit. However, the experimental limitations and lack of comparison with state-of-the-art methods, along with issues in clarity and reproducibility, are significant concerns. These issues need to be addressed to strengthen the paper's practical relevance and robustness.\n\n**Final Decision: Reject**\n\nThe paper should be rejected for publication at this top-tier conference but with the strong recommendation that the authors address the identified weaknesses and resubmit a revised version."
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Relevance and Importance**:\n   - The paper addresses a significant and relevant problem in the field of optimization, particularly in settings where data is not independently and identically distributed (non-iid). This is particularly important for applications like continual learning and reinforcement learning.\n   - The phenomenon of resonance in SGD with momentum (SGDm) under covariate shift is an interesting and novel observation that could have practical implications.\n\n2. **Theoretical Contributions**:\n   - The paper provides a theoretical framework to understand the behavior of SGDm under periodic covariate shift, showing that it can be modeled as a parametric oscillator.\n   - The use of Floquet theory to analyze the stability of the system is a well-established mathematical tool, and the paper leverages this to derive conditions for divergence.\n\n3. **Empirical Validation**:\n   - The empirical results are comprehensive and well-designed, starting from a simple linear setting and progressively moving to more complex scenarios, including non-linear models and different optimizers.\n   - The experiments provide strong evidence for the theoretical predictions and show that the resonance phenomenon persists even under more relaxed assumptions.\n\n#### Weaknesses of the Paper\n1. **Technical Novelty**:\n   - Reviewer 2 and Reviewer 3 raise concerns about the technical novelty of the paper. They argue that the mathematical techniques used are not sufficiently novel, as they rely heavily on existing results from numerical analysis and Floquet theory.\n   - The proof of Proposition 2 is similar to existing work, and the handling of time-varying ODEs is delegated to existing results, which may undermine the claimed technical novelty.\n\n2. **Presentation and Clarity**:\n   - Reviewer 3 and Reviewer 5 point out that the presentation of Theorem 1 and the discussion of resonance could be improved. The connection between the theoretical results and the resonance behavior is not clearly explained, making it difficult for readers to understand why certain frequencies lead to divergence.\n   - The \"Example\" section in the paper is criticized for not providing enough concrete results and derivations, which could have helped in understanding the resonance phenomenon better.\n\n3. **Practical Relevance**:\n   - Reviewer 6 raises concerns about the practical relevance of the oscillatory covariate shift. While the theoretical results are interesting, it is not clear how often such clear-cut oscillatory patterns occur in real-world datasets.\n   - The paper could benefit from including an empirical example that demonstrates the relevance of the resonance phenomenon in a practical setting.\n\n4. **Analysis of Discrete-Time SGDm**:\n   - Reviewer 3 and Reviewer 5 question the analysis of the \"right\" algorithm. The theoretical results are based on the expected gradients, which removes the stochasticity in the data samples. This raises doubts about the applicability of the results to the actual discrete-time SGDm.\n\n#### Final Decision\nDespite the concerns raised by the reviewers, the paper makes a significant contribution to the understanding of SGDm under non-iid sampling. The theoretical framework is sound, and the empirical results are comprehensive and well-designed. The phenomenon of resonance is an interesting and novel observation that could have practical implications in various machine learning applications.\n\nHowever, the paper could benefit from addressing the concerns about technical novelty, improving the presentation of the theoretical results, and providing more practical examples to demonstrate the relevance of the resonance phenomenon.\n\nGiven the importance of the problem, the quality of the empirical results, and the potential impact on the field, I recommend accepting the paper with the suggestion for the authors to address the raised concerns in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Technical Novelty and Significance\n- **Technical Novelty**: The paper introduces the use of Fourier convolutional networks for 3D snapshot microscopy, which is a novel application of an existing technique. However, the core idea of using Fourier-domain convolutions is not entirely new, as it has been explored in previous works such as Rippel et al. (2015). The paper does not provide a clear comparison or demonstration of the specific improvements over these previous works, which is a significant weakness.\n- **Significance**: The application of Fourier convolutional networks to 3D snapshot microscopy is significant, as it addresses a specific challenge in computational imaging. The paper demonstrates that these networks can handle the global context required for 3D snapshot microscopy more efficiently than traditional UNet architectures. However, the significance is somewhat limited by the lack of real-world experiments and the focus on simulated data.\n\n#### Empirical Novelty and Significance\n- **Empirical Novelty**: The paper presents promising results on simulated data and a lensless imaging dataset. The use of Fourier convolutional networks to handle global PSFs is a novel approach in the context of 3D snapshot microscopy. However, the empirical evaluation is limited to simulated data and a single type of object (larval zebrafish), which raises concerns about the generalizability of the results.\n- **Empirical Significance**: The results on the lensless imaging dataset are significant, as they demonstrate the effectiveness of the proposed architecture in a different application. However, the lensless imaging results are not directly related to 3D snapshot microscopy, which is the main focus of the paper. This disconnect between the title and the experimental results is a weakness.\n\n#### Strengths\n- **Well-Written and Clear**: The paper is well-written and easy to understand. The authors provide a thorough explanation of the Fourier convolutional network architecture and its application to 3D snapshot microscopy.\n- **Thorough Limitations Section**: The paper includes a detailed limitations section, which is a strength. The authors acknowledge the limitations of their approach, such as the reliance on simulated data and the potential issues with real-world applications.\n\n#### Weaknesses\n- **Limited Real-World Evaluation**: The paper is evaluated only on simulated data, which is a significant limitation. Real-world experiments are necessary to validate the effectiveness of the proposed method in practical scenarios.\n- **Comparison with Previous Work**: The paper does not provide a clear comparison with previous works, particularly Rippel et al. (2015). This lack of comparison makes it difficult to assess the novelty and significance of the proposed approach.\n- **UNet Results**: The results from UNet are unclear. The authors claim that UNet is computationally inefficient and unable to handle global context, but the results in Figure 2 show that UNet performs much worse than FourierNet. A more detailed explanation of why UNet fails in this context is needed.\n- **Speed and Resource Requirements**: The paper does not provide a clear comparison of the computational resources required for the proposed method and the state-of-the-art. The speed comparison in Table 3 is inconsistent with the claims in the paper, and the missing FourierUNet result in this experiment is a concern.\n\n#### Reviewer Feedback\n- **Reviewer 1 and 2**: Both reviewers improved their ratings after the author's response, indicating that the authors addressed some of their concerns. However, both reviewers still have reservations about the evaluation and the comparison with previous work.\n- **Reviewer 3**: This reviewer is marginally above the acceptance threshold but has concerns about the novelty and the venue. They suggest that the paper might be better suited for an application-specific conference or journal.\n- **Reviewer 4**: This reviewer has significant concerns about the comparison with Rippel et al. and the lack of implementation details. They suggest that the paper needs more details to demonstrate the specific improvements over previous work.\n- **Reviewer 5**: This reviewer has a weak reject, primarily due to the limited evaluation and the unclear results from UNet.\n- **Reviewer 6**: This reviewer has a weak accept, with some concerns about the claimed contributions and the lack of real-world experiments.\n- **Reviewer 7**: This reviewer has a weak accept, with some questions about the relation to previous work and the redundancy in the Fourier convolution approach.\n\n### Final Decision\nGiven the mixed feedback from the reviewers and the significant weaknesses in the paper, particularly the limited real-world evaluation and the lack of a clear comparison with previous work, I believe that the paper should be rejected for publication at a top-tier conference. The paper has promising ideas and a well-written presentation, but it needs substantial improvements to address the identified issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Theoretical Contributions:**\n   - The paper introduces a new PAC-Bayes bound and an analytical unbiased estimator for the NNGP’s generalization error. These contributions are theoretically sound and provide a novel approach to understanding the implicit bias of neural architectures.\n   - The bounds and estimators are non-vacuous, which is a significant achievement in the context of generalization bounds for neural networks.\n\n2. **Insightful Analysis:**\n   - The paper provides a nuanced analysis of the roles of architecture and gradient descent in generalization. The discussion on the implicit bias of gradient descent, particularly its ability to control margin, is insightful and adds to the existing literature.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and clearly structured. The authors provide intuitive explanations and visualizations that help in understanding the complex theoretical results.\n\n#### **Weaknesses of the Paper:**\n1. **Limited Experimental Validation:**\n   - The experimental setup is limited to a specific optimizer (Nero) and a specific dataset. This makes it difficult to generalize the findings to other optimizers and datasets. The use of Nero, which is a relatively new and specialized optimizer, raises questions about the applicability of the results to more common optimization methods like SGD or vanilla GD.\n   - The experiments do not include a comprehensive comparison with other architectures (e.g., CNNs, ResNets) or different datasets (e.g., CIFAR-10, ImageNet). This limits the practical significance of the results.\n\n2. **Loose Bounds:**\n   - The bounds provided in the paper are loose compared to the empirical performance of gradient descent-trained networks. This gap is not fully explained, and the authors attribute it to the implicit bias of gradient descent without providing strong theoretical support.\n\n3. **Confusing Claims:**\n   - The claim that gradient descent maximizes margin is not well-supported and is based on limited experimental evidence. The discussion in Section 5 is confusing and lacks rigor, making it difficult to understand the exact implications and how it relates to existing literature on the implicit bias of gradient descent.\n\n4. **Incremental Nature of Contributions:**\n   - The theoretical contributions, while novel, are incremental and build heavily on previous work (e.g., Valle-Pérez et al. (2019)). The paper does not provide a significant leap in understanding beyond what is already known in the field.\n\n#### **Reviewer Feedback:**\n- **Reviewer 1:** Points out that the margin results are simplistic and not specific to gradient descent. Suggests that the paper should focus more on the implicit bias of NNGP and provide more comprehensive experiments.\n- **Reviewer 2:** Agrees with the theoretical contributions but finds the discussion on gradient descent confusing and lacking in rigor. Suggests more comprehensive experiments and a clearer comparison with other optimizers.\n- **Reviewer 3:** Finds the paper premature and lacking in significant contributions. Points out that the bounds are not tight and the experimental setup is limited.\n- **Reviewer 4:** Criticizes the experimental setup and the lack of clarity in the margin claims. Suggests that the paper should be more focused and provide more rigorous evidence.\n- **Reviewer 5:** Raises concerns about the use of Nero and the limited experimental evidence. Suggests that the paper should provide more ablation studies and comparisons with other optimizers.\n- **Reviewer 6:** Finds the paper interesting but limited in its experimental validation. Suggests that the paper should provide more comprehensive experiments and a clearer comparison with existing work.\n- **Reviewer 7:** Raises concerns about the experimental setup and the lack of clarity in the margin claims. Suggests that the paper should provide more ablation studies and comparisons with other optimizers.\n- **Reviewer 8:** Finds the paper interesting but limited in its contributions. Suggests that the paper should provide more comprehensive experiments and a clearer comparison with existing work.\n- **Reviewer 9:** Finds the paper interesting but limited in its experimental validation. Suggests that the paper should provide more comprehensive experiments and a clearer comparison with existing work.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, and the feedback from the reviewers, the paper has several significant contributions but also several limitations. The theoretical contributions are novel but incremental, and the experimental validation is limited and not comprehensive enough to support the claims about the implicit bias of gradient descent. The paper would benefit from more rigorous experiments and a clearer comparison with existing work.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Well-Written and Reproducible**:\n   - The paper is well-written and easy to follow, with sufficient technical details provided for reproducibility. The authors have committed to open-sourcing data, code, and trained models, which is a significant plus.\n\n2. **Innovative Methodology**:\n   - The introduction of MILAN, a method for generating natural language descriptions of neurons, is a novel and well-motivated approach. It addresses a significant gap in the interpretability of deep learning models, particularly in computer vision.\n\n3. **Generalizability and Applications**:\n   - The method shows generalizability across different model architectures, datasets, and tasks. The authors demonstrate three compelling applications: analysis, auditing, and editing of models, which highlight the practical utility of MILAN.\n\n4. **Empirical Validation**:\n   - The paper includes thorough empirical validation, with results showing higher agreement with human annotations compared to baseline methods. The experiments are well-designed and cover a broad range of scenarios.\n\n#### Weaknesses and Concerns\n\n1. **Inter-Annotator Agreement**:\n   - The inter-annotator agreement in the MILANNOTATIONS dataset is relatively low. This raises concerns about the quality and consistency of the dataset. The authors should provide more details on how they handle this inconsistency and any additional quality control measures.\n\n2. **Scalability and Generalization**:\n   - Reviewer 4 raises valid concerns about the generalizability of the method to other tasks and datasets. The paper focuses on image models, and it is unclear how well the method would perform on other types of data (e.g., text, audio). The authors should address this limitation and consider experiments with other modalities.\n\n3. **Baseline Comparisons**:\n   - The paper could benefit from more comprehensive baseline comparisons, particularly with existing image captioning models. This would help to better understand the unique advantages of MILAN.\n\n4. **Complexity of Descriptions**:\n   - The complexity and diversity of the generated descriptions are not fully explored. More details on the length, vocabulary size, and diversity of the descriptions would provide a more complete picture of the method's capabilities.\n\n5. **Impactful Applications**:\n   - While the applications of MILAN are interesting, they are somewhat contrived. The authors should consider more impactful applications, such as removing racial or gender bias in models, to demonstrate the method's real-world utility.\n\n#### Addressing Reviewer Comments\n\n- **Reviewer 1**: No significant comments.\n- **Reviewer 2**: Raises valid points about inter-annotator agreement and the potential for leveraging existing large datasets. The authors should address these concerns in their response.\n- **Reviewer 3**: Appreciates the thorough testing and potential applications but suggests a more detailed discussion of the method's limits.\n- **Reviewer 4**: Provides a critical review, highlighting concerns about generalizability, baseline comparisons, and the complexity of descriptions. The authors should address these points to strengthen the paper.\n\n### Final Decision\n\nDespite the concerns raised, the paper presents a novel and well-motivated approach to interpreting deep learning models. The method shows strong empirical results and has significant practical applications. The authors have committed to open-sourcing their work, which is a valuable contribution to the research community.\n\nHowever, the authors should address the concerns about inter-annotator agreement, generalizability, and baseline comparisons in their response. These improvements would strengthen the paper and address the valid points raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novelty in Adversarial Training:**\n   - The paper introduces a novel approach to adversarial training by learning to reweight training samples using a bilevel optimization framework. This is a significant step forward from heuristic reweighting methods.\n   - The use of multi-class margins as input to the reweighting network is a novel and interesting idea.\n\n2. **Methodological Soundness:**\n   - The methodology is well-explained and grounded in existing meta-learning techniques, specifically MAML. The adaptation of MAML to adversarial training is methodologically sound.\n   - The paper is well-written and easy to follow, with clear explanations of the proposed approach and its components.\n\n3. **Empirical Evaluation:**\n   - The paper includes extensive experiments on benchmark datasets (MNIST, CIFAR-10, CIFAR-100) and demonstrates improvements in both clean and robust accuracy.\n   - The evaluation includes strong adversaries like AutoAttack, which is a robust and widely accepted benchmark for adversarial robustness.\n\n#### **Weaknesses:**\n1. **Limited Novelty:**\n   - Several reviewers point out that the proposed method is a direct adaptation of MAML to adversarial training, which may not be sufficiently novel for a top-tier conference. The novelty is primarily in the application rather than in the underlying technique.\n   - The paper lacks a deeper investigation into the reweighting mechanisms and the specific effects of the multi-class margin on robustness.\n\n2. **Evaluation Concerns:**\n   - **Adaptive Attacks:** Reviewer 1 and others raise concerns about the lack of evaluation against adaptive attacks, where the attacker has knowledge of the reweighting module. This is a significant gap in the evaluation, as it could reveal vulnerabilities in the proposed method.\n   - **Comparison to SOTA:** Reviewer 10 points out that the paper does not compare to more recent and stronger baselines, such as Adversarial Weight Perturbation (AWP). The claim of outperforming state-of-the-art baselines is not well-supported.\n   - **MNIST and FMNIST:** Reviewer 10 notes that the MNIST and FMNIST models were not evaluated using AutoAttack or Auto-PGD, which are standard benchmarks for robustness.\n\n3. **Theoretical Justification:**\n   - The paper lacks a thorough theoretical justification for why the multi-class margin is a better input for the reweighting network compared to other potential inputs. This is a significant gap in the paper's contributions.\n   - Reviewer 3 and others suggest that the paper should include ablation studies to demonstrate the superiority of the multi-class margin over other input encodings.\n\n4. **Performance and Robustness:**\n   - The performance improvements, especially under AutoAttack, are marginal. The method often relies on combinations with other techniques (e.g., TRADES) to achieve better results, which raises questions about the standalone effectiveness of BiLAW.\n   - The robustness of the model trained with BiLAW alone is not significantly better than existing baselines, which is a concern for a paper claiming to improve robustness.\n\n5. **Minor Issues:**\n   - The paper has some minor issues with notation, consistency, and clarity in certain sections. These are relatively minor but should be addressed to improve the overall quality of the paper.\n\n### Final Decision\n\nGiven the strengths and weaknesses of the paper, the decision is not straightforward. However, the following points are critical:\n\n- **Novelty and Significance:** While the paper introduces a novel application of MAML to adversarial training, the overall novelty is limited. The method is a direct adaptation of existing techniques, and the paper lacks a deeper investigation into the reweighting mechanisms.\n- **Evaluation:** The evaluation is thorough but has significant gaps, particularly in the lack of adaptive attacks and comparison to more recent baselines. These gaps are critical for a top-tier conference.\n- **Theoretical Justification:** The paper lacks a strong theoretical foundation for the proposed method, which is a significant weakness.\n\nGiven these considerations, the paper does not meet the high standards of a top-tier conference. The contributions are valuable but not sufficiently novel or robustly evaluated to warrant acceptance.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Novelty and Motivation:**\n   - The paper introduces a novel framework, PEARL, for synthesizing data with differential privacy guarantees. The approach is well-motivated and addresses a significant problem in the field of data synthesis.\n   - The use of privatized embeddings and adversarial re-weighting is a unique and innovative approach that differentiates it from existing methods.\n\n2. **Theoretical Foundations:**\n   - The paper provides a solid theoretical foundation for the proposed method, including rigorous privacy guarantees and performance analysis.\n   - The mathematical formulation is clear and easy to follow, which is a strong point for a technical paper.\n\n3. **Empirical Evaluation:**\n   - The empirical results show that PEARL outperforms other methods on the datasets and tasks evaluated. This is a positive indication of the method's effectiveness.\n\n#### **Weaknesses of the Paper:**\n1. **Lack of Thorough Benchmarking:**\n   - The paper lacks comprehensive benchmarking against a wider range of datasets and baselines. For tabular data, comparisons with PATE-GAN and CTGAN are missing. For image data, more complex datasets like CIFAR-10 should be included.\n   - The inclusion of more benchmark datasets and advanced baselines would strengthen the empirical evaluation and provide a more robust comparison.\n\n2. **Clarity and Explanation:**\n   - Some aspects of the method, such as the sensitivity calculation and the auxiliary information release, are not clearly explained. This can lead to confusion for readers and reviewers.\n   - The privacy-preserving optimization method and the one-shot sampling strategy need more detailed explanations to ensure clarity and correctness.\n\n3. **Related Work:**\n   - The paper is missing important references that are highly relevant to the topic. Including these references and discussing them would provide a more comprehensive literature review and context for the proposed method.\n\n4. **Complexity Analysis:**\n   - The paper lacks a detailed analysis of the computational complexity of the proposed algorithm. This is an important aspect that should be addressed to provide a complete understanding of the method's practicality.\n\n5. **Editorial Quality:**\n   - The paper contains inconsistent and non-precise descriptions, which can affect the overall readability and clarity. Addressing these issues would improve the quality of the paper.\n\n#### **Reviewer Comments:**\n- **Reviewer 1** and **Reviewer 3** both highlight the need for more thorough benchmarking and the inclusion of additional datasets and baselines.\n- **Reviewer 2** raises specific questions about the sensitivity calculation and the performance of PEARL at different privacy budgets.\n- **Reviewer 3** provides a detailed list of improvements, including the need for more related work, complexity analysis, and additional experiments.\n\n### Final Decision\nGiven the strengths of the paper, particularly its novel approach and theoretical foundations, it has the potential to make a significant contribution to the field. However, the weaknesses identified by the reviewers, especially the lack of thorough benchmarking and clarity in certain aspects of the method, need to be addressed to meet the standards of a top-tier conference.\n\n**Final Decision: Reject**\n\nThe paper should be rejected in its current form but with a strong recommendation for revision. The authors should address the following points in a revised version:\n1. Include more benchmark datasets and advanced baselines.\n2. Provide detailed explanations for the sensitivity calculation and privacy-preserving optimization.\n3. Add missing related work and discuss their relevance.\n4. Conduct a computational complexity analysis.\n5. Improve the editorial quality and clarity of the paper.\n\nBy addressing these issues, the paper could be resubmitted for consideration in a future conference."
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Clarity and Reproducibility:**\n   - The paper is well-written and clearly explains the concepts and experimental setup. This is a significant strength, as it ensures that the work is reproducible and accessible to the community.\n   - Reviewers generally agree that the paper is well-structured and easy to follow.\n\n2. **Innovative Concept:**\n   - The introduction of Variational Neural Cellular Automata (VNCA) is a novel and interesting approach. It bridges the gap between generative models and self-organizing systems, which is a valuable contribution to the field.\n   - The paper addresses a limitation of typical morphogenesis approaches with NCA, which is that they are usually trained to generate a single image/state. This work is a step towards learning NCA that generalize to different target states.\n\n3. **Robustness and Self-Organization:**\n   - The paper demonstrates the robustness of the VNCA to early-stage perturbations, which is a unique and valuable property. This robustness is not a common feature in many generative models and adds to the practical utility of the VNCA.\n\n#### **Weaknesses:**\n1. **Underwhelming Experimental Results:**\n   - The experimental results are underwhelming, particularly in terms of sample quality and likelihoods. The VNCA performs well on MNIST but poorly on more complex datasets like Noto Emoji and CelebA.\n   - The need to average samples to achieve reasonable results is a significant drawback, as it limits the practical applicability of the model.\n\n2. **Limited Novelty:**\n   - Several reviewers point out that the VNCA can be interpreted as a deep convolutional network with repeating blocks, which is not a novel concept. The NCA interpretation, while interesting, does not add significant novelty to the architecture itself.\n   - The paper does not fully address the question of what is gained by the NCA interpretation of the deep convolutional network.\n\n3. **Lack of Comparative Analysis:**\n   - The paper lacks a thorough comparison with existing generative models, particularly in terms of tasks like inpainting and super-resolution. This makes it difficult to assess the practical advantages of the VNCA over other models.\n   - The claims of \"super-resolution\" are not well-founded, especially given the artifacts in the reconstructions on MNIST.\n\n4. **Technical and Empirical Novelty:**\n   - While the paper introduces a new model, the technical and empirical contributions are only marginally significant. The model's performance is not competitive with state-of-the-art generative models, and the robustness to perturbations, while interesting, is not a practical advantage in many applications.\n\n#### **Reviewer Feedback:**\n- **Positive Feedback:**\n  - Reviewers 3 and 5 have raised their scores after the rebuttal, indicating that the authors have addressed their concerns and improved the paper.\n  - Reviewer 5, in particular, found the paper interesting and valuable for the ICLR community, despite some limitations.\n\n- **Negative Feedback:**\n  - Reviewers 1, 4, and 6 remain skeptical about the paper's contributions, particularly the underwhelming experimental results and the limited novelty of the NCA interpretation.\n  - Reviewer 6 points out that the NCA interpretation does not add significant novelty to the deep convolutional network architecture.\n\n### Final Decision\n\nGiven the mixed feedback from the reviewers, the strengths and weaknesses of the paper, and the standards of a top-tier conference, the paper has several significant limitations that outweigh its strengths. The underwhelming experimental results, limited novelty, and lack of a thorough comparative analysis make it difficult to justify acceptance at a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Theoretical Insights**:\n   - The paper provides a clear and insightful analysis of epoch-wise double descent using a linear teacher-student model. The use of replica theory to derive closed-form expressions for the generalization error is a significant contribution.\n   - The findings are well-supported by both theoretical derivations and empirical validation, showing a close match between the theoretical predictions and the behavior of a ResNet18 on CIFAR10.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and the theory is well-motivated. The results are neatly presented, making it easy for readers to follow the arguments and understand the implications.\n\n3. **Relevance to Deep Learning**:\n   - The paper addresses a key challenge in deep learning, namely the non-monotonic behavior of generalization error over training time. The insights gained from the linear model can potentially be extended to more complex models, providing a foundation for further research.\n\n#### Weaknesses of the Paper\n\n1. **Technical Novelty**:\n   - While the paper provides a valuable contribution, the technical novelty is somewhat limited. The use of linear models and replica theory to study double descent is not entirely new, and similar insights have been discussed in prior works (e.g., Heckel & Yilmiz 2020, Stephenson & Lee 2021).\n   - The paper could benefit from a more detailed discussion of how it builds on and extends these prior works, particularly in terms of the specific mechanisms behind epoch-wise double descent.\n\n2. **Empirical Validation**:\n   - The empirical validation is solid but limited. The experiments with ResNet18 on CIFAR10 are a good start, but more extensive experiments with different architectures and datasets would strengthen the paper's empirical claims.\n   - The paper could also explore the robustness of the findings to different types of noise and regularization techniques.\n\n3. **Typos and Minor Issues**:\n   - The paper contains several typos and minor issues that need to be addressed. These issues can make the calculations more ambiguous and harder to follow, potentially detracting from the overall quality of the paper.\n   - The color code in Figure 2 is incorrect and misleading, which needs to be fixed.\n\n4. **Connections to Related Work**:\n   - The paper should provide a more comprehensive discussion of its connections to related work, particularly the implicit regularization effects of SGD/GD on least squares linear regression (Ali and Ray Tibshirani's work at ICML 2020 and AISTATS 2019).\n   - The paper should also discuss how its findings relate to the double descent of Ridgeless regression in high-dimensional settings (d >> p).\n\n#### Reviewer Comments\n\n- **Reviewer 1**:\n  - Positive: The reviewer finds the paper insightful and well-written, with a clear match between theory and empirical results.\n  - Suggestions: The reviewer suggests exploring the behavior of the analytical theory with more than two scales, the difference between epoch-wise and model-size double descent, and the interpretation of the peak in the epoch-wise descent curve.\n\n- **Reviewer 2**:\n  - Positive: The reviewer acknowledges the solid theoretical analysis and empirical evaluation.\n  - Concerns: The reviewer suggests discussing connections to prior work on implicit regularization and high-dimensional linear regression.\n\n- **Reviewer 3**:\n  - Positive: The reviewer finds the analysis of a simple model that exhibits epoch-wise double descent illuminating and worthwhile.\n  - Concerns: The reviewer points out numerous typos and minor issues, questions the technical novelty, and suggests more detailed connections to related work.\n\n### Final Decision\n\nGiven the strengths of the paper, particularly its theoretical insights and clear presentation, and considering the constructive feedback from the reviewers, the paper has the potential to make a valuable contribution to the field. However, the paper also has some weaknesses that need to be addressed, particularly in terms of technical novelty, empirical validation, and connections to related work.\n\n**Final Decision: Accept**\n\nThe paper should be accepted with the condition that the authors address the following points in the final version:\n1. Fix all typos and minor issues.\n2. Provide a more detailed discussion of the connections to related work, particularly on implicit regularization and high-dimensional linear regression.\n3. Consider exploring the behavior of the analytical theory with more than two scales and the interpretation of the peak in the epoch-wise descent curve.\n4. Conduct additional empirical validation with different architectures and datasets to strengthen the empirical claims."
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Clarity and Quality\nThe paper is generally well-written and clear, with a well-structured introduction and a detailed explanation of the proposed method. The figures, particularly Figure 1, effectively summarize the key ideas. However, there are some typos and minor issues that need to be addressed, as noted by Reviewer 3.\n\n#### Novelty and Significance\nThe paper introduces a novel framework for online knowledge distillation (KD) by using a label prior shift to induce diversity among the teachers and a post-compensation mechanism to aggregate their outputs. While the individual components (label prior shift, post-compensation) have been explored in previous works, the integration of these ideas into a cohesive framework for online KD is a significant contribution. However, the novelty is somewhat limited, as acknowledged by several reviewers.\n\n#### Empirical Evaluation\nThe empirical evaluation is thorough, with extensive experiments on multiple datasets (CIFAR-10, CIFAR-100, and ImageNet). The paper demonstrates improvements over several baselines in terms of top-1 error rate, negative log-likelihood, and expected calibration error. However, the performance gains are not always substantial, and the paper lacks comparisons with more recent and competitive methods, as pointed out by Reviewer 4 and Reviewer 5. Specifically, the paper does not compare with methods like CGL, PCL, and L-MCL, which have shown better performance in recent studies.\n\n#### Technical Soundness\nThe technical claims and methods are generally sound, with clear explanations and theoretical justifications. However, some of the claims, particularly regarding the significance of the improvements, are not well-supported by the empirical results. For example, the improvement over PCL on ImageNet is only 0.05%, which is marginal and may not be statistically significant.\n\n#### Addressing Reviewer Comments\nThe authors have provided responses to the reviewer comments, but some of the concerns remain unaddressed. For instance, the lack of comparison with more recent and competitive methods is a significant gap that needs to be addressed. Additionally, the authors should provide more ablation studies to demonstrate the effectiveness of each component of their framework, as suggested by Reviewer 1.\n\n### Final Decision\nGiven the thorough empirical evaluation and the novel integration of existing ideas into a cohesive framework, the paper has some merit. However, the limited novelty and the lack of comparison with more recent and competitive methods are significant drawbacks. The paper also needs to address some of the technical and empirical issues raised by the reviewers.\n\nFinal Decision: Reject\n\nThe paper should be rejected for publication at this top-tier conference due to the limited novelty, the lack of comparison with recent competitive methods, and the need for additional ablation studies to strengthen the empirical results. However, the authors are encouraged to address these issues and resubmit the paper in a revised form."
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novel Idea**: The paper introduces a novel framework, MCTransformer, which combines Monte-Carlo Tree Search (MCTS) with Transformers. This combination is a promising direction in the field of reinforcement learning, particularly for offline settings.\n2. **Potential Impact**: The idea of using Transformers to enhance MCTS has the potential to lead to more efficient and effective exploration and evaluation in reinforcement learning tasks.\n3. **Initial Results**: The initial results on the SameGame domain show that MCTransformer outperforms both Transformer-only and MCTS-only solutions, which is a positive indication of the framework's effectiveness.\n\n#### Weaknesses:\n1. **Limited Novelty**: Reviewers 2 and 3 point out that the method is a straightforward combination of existing techniques (MCTS and Transformers), similar to the structure used in AlphaGo/AlphaZero. This limits the novelty of the work.\n2. **Limited Evaluation**: The evaluation is conducted on a single, non-standard domain (SameGame), which is relatively toy-like. The paper lacks comparisons to well-established benchmarks and other relevant methods, such as Decision Transformers, Online DTs, and other MCTS-based programs with neural networks.\n3. **Clarity and Presentation**: Reviewer 1 notes that the paper's writing and presentation are unclear, making it difficult to understand the method and its contributions. The figures and explanations need significant improvement.\n4. **Fairness of Experiments**: Reviewer 2 raises concerns about the fairness of the experiments, particularly the comparison with MCTS-based solutions without a learned prior policy or value function. This makes the comparisons less convincing.\n5. **Reproducibility**: The paper does not provide the source code, which is crucial for reproducibility. While the method is similar to AlphaZero, the lack of code makes it harder for others to reproduce the results.\n6. **Additional Experiments**: Reviewer 3 suggests several additional experiments to justify the use of the Transformer architecture in MCTS and offline reinforcement learning. These experiments are necessary to support the claims made in the paper.\n\n#### Summary:\nWhile the idea of combining MCTS with Transformers is promising and has potential, the current paper falls short in several critical areas:\n- **Novelty**: The method is a straightforward combination of existing techniques, which limits its novelty.\n- **Evaluation**: The evaluation is limited to a single, non-standard domain and lacks comparisons to relevant benchmarks and methods.\n- **Clarity and Presentation**: The paper's writing and presentation are unclear, making it difficult to understand the method and its contributions.\n- **Fairness of Experiments**: The experiments are not conducted in a fair manner, which undermines the validity of the results.\n- **Reproducibility**: The lack of source code and additional experiments necessary to support the claims further weakens the paper.\n\n### Final Decision\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Novel Idea**: The paper introduces a novel approach to OOD detection by leveraging graph structures and topological properties. This is a promising direction that could enhance the interpretability of OOD detection methods.\n2. **High Performance**: The paper reports high AUROC scores (97.95% for far-OOD and 98.79% for near-OOD) on the LSUN dataset, which are comparable to state-of-the-art techniques.\n3. **Human-in-the-Loop**: The paper emphasizes the importance of human-interpretable concepts, which is a valuable aspect for practical applications.\n\n#### Weaknesses\n1. **Clarity and Writing**: The paper suffers from poor writing and numerous grammatical errors, making it difficult to follow. This is a significant issue for a top-tier conference.\n2. **Lack of Rigor**: The paper uses vague terms like \"common sense\" and \"child cognition\" without rigorous definitions or references. This weakens the scientific foundation of the claims.\n3. **Limited Experiments**: The empirical evaluation is limited to the LSUN dataset and lacks comparisons with state-of-the-art OOD detection methods. This makes it difficult to validate the claims of high performance.\n4. **Assumptions and Robustness**: The method relies heavily on the accuracy of the pre-trained object detection model. The paper does not provide an analysis of how the object detector's accuracy impacts the OOD detection performance, which is a critical detail.\n5. **Ablation Studies**: The paper lacks ablation studies to understand the impact of different components of the proposed method, such as the choice of graph embedding algorithms and hyperparameters.\n6. **Related Work**: The paper does not adequately discuss the existing literature on OOD detection, which is essential for placing the work in the broader context of the field.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Suggests improvements in clarity, detailed descriptions, and more rigorous validation.\n- **Reviewer 2**: Strongly recommends rejection due to poor writing, lack of novelty, and insufficient empirical evaluation.\n- **Reviewer 3**: Points out the lack of novelty and missing details, but acknowledges the interesting idea.\n- **Reviewer 4**: Highlights the need for more comprehensive experiments and better writing, but sees potential in the idea.\n\n### Final Decision\nGiven the significant issues with clarity, writing, and empirical validation, the paper does not meet the standards of a top-tier conference. While the idea is novel and has potential, the current state of the paper is not sufficient for acceptance. The authors should address the reviewers' comments and resubmit a more polished and rigorously validated version.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Theoretical Contributions:**\n- **Theorem 1 and Its Validity:**\n  - **Reviewer 5, 9, 10, 11, 16** raise significant concerns about the theoretical soundness of Theorem 1. They argue that the theorem is a simple extension of existing results from TRADES and does not provide a meaningful new bound, especially for multi-class classification problems.\n  - **Reviewer 7** and **Reviewer 14** acknowledge the simplicity of the theorem but argue that it is still valuable because it leads to a practical and effective regularizer.\n  - **Conclusion:** The theoretical contribution is questionable. The theorem is a straightforward extension of existing work and does not provide a significant new insight. The loose bounds and the lack of a clear, rigorous definition of \\( z(x) \\) further weaken the theoretical foundation.\n\n#### **2. Empirical Contributions:**\n- **Experimental Setup and Results:**\n  - **Reviewer 2, 6, 13, 14, 15** generally agree that the empirical results are strong and show consistent improvements over existing methods.\n  - **Reviewer 8** and **Reviewer 15** point out that the improvements over baselines are marginal, especially when additional data is used. They also note discrepancies in the reported results for some baselines compared to the original sources.\n  - **Conclusion:** The empirical results are solid but not groundbreaking. The improvements are consistent but small, and the use of additional data does not significantly outperform existing methods. The discrepancies in baseline results need to be addressed.\n\n#### **3. Clarity and Reproducibility:**\n- **Reviewer 13, 14, 15** praise the clarity and reproducibility of the paper. The paper is well-written, and the experimental setup is well-documented.\n- **Reviewer 10, 11, 16** raise concerns about the clarity and rigor of the theoretical definitions, particularly the definition of \\( z(x) \\).\n- **Conclusion:** The paper is generally well-written and reproducible, but the theoretical definitions need to be clarified and made more rigorous.\n\n#### **4. Novelty and Significance:**\n- **Reviewer 7, 13, 14, 15** acknowledge the novelty of the method, but note that it is similar to existing methods like MART and TRADES.\n- **Reviewer 5, 9, 10, 11, 16** argue that the novelty is marginal and the theoretical contributions are not significant.\n- **Conclusion:** The novelty is somewhat limited. The method is a small but practical improvement over existing methods, but it does not introduce a fundamentally new approach.\n\n### Final Decision\nGiven the significant concerns about the theoretical soundness of the paper, the marginal improvements in empirical results, and the need for more rigorous definitions, the paper does not meet the high standards of a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Interesting Observation**: The paper presents a novel and interesting observation that swapping the order of batch normalization (BN) and bounded activation functions (like Tanh) can significantly improve performance. This is a valuable contribution to the understanding of neural network architecture design.\n2. **Comprehensive Experiments**: The authors conduct extensive experiments with various bounded activation functions (Tanh, LeCun Tanh, and Softsign) on different benchmarks and architectures, providing a robust empirical basis for their claims.\n3. **New Metrics**: The paper introduces metrics to measure saturation, asymmetric saturation, and sparsity, which are useful for analyzing the behavior of neural networks.\n\n#### Weaknesses of the Paper\n1. **Lack of Rigorous Justification**: Several claims in the paper are not well-supported or rigorously justified. For example, the claim that asymmetric saturation enhances generalization is not convincingly demonstrated. The reviewers have pointed out that the relationship between saturation, sparsity, and performance is not clearly established.\n2. **Limited Scope**: The analysis is limited to bounded activation functions and architectures without residual connections. This limits the generalizability of the findings to more widely used architectures like ResNets.\n3. **Conflicting Results**: The paper shows that the Swap model with bounded activation functions does not outperform the Convention model with ReLU. This raises questions about the practical significance of the findings.\n4. **Clarity and Precision**: The paper contains several vague and imprecise statements. For example, the definition of \"the center of the function\" is not clearly explained, and the relationship between saturation and sparsity is not well-articulated.\n5. **Experimental Setup**: The experimental setup and analysis have some issues. For instance, the reviewers have pointed out that the observations are not uniformly consistent across all layers, and the impact of different hyperparameters (like weight decay) on the results is not thoroughly explored.\n\n#### Reviewer Feedback\n1. **Reviewer 1**: Suggests adding more figures to support the argument about the skewness of activation values and questions the exclusion of sigmoid from the analysis.\n2. **Reviewer 2**: Points out that the arguments connecting saturation and sparsity to generalization performance are weak and suggests that the analysis is limited in scope.\n3. **Reviewer 3**: Agrees that the topic is interesting but finds the analysis less comprehensive and questions the practical significance of the findings.\n4. **Reviewer 4**: Expresses concerns about the lack of rigorous justification for the claims and suggests further investigations to validate the findings.\n5. **Reviewer 5**: Raises several specific concerns about the clarity, correctness, and experimental setup of the paper.\n\n### Final Decision\nWhile the paper presents an interesting and novel observation, the lack of rigorous justification, limited scope, and issues with clarity and precision in the analysis are significant concerns. The findings, though intriguing, do not provide a strong enough basis for a top-tier conference publication. The paper would benefit from further refinement and additional experiments to address the reviewers' concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution**\n- **Novelty**: The paper proposes a contrastive token learning (CT) objective to address text degeneration in language models. While the idea is intuitive and well-motivated, the novelty is somewhat limited. The core concept of penalizing negative tokens has been explored in unlikelihood training (UL), and the paper's claim that UL only considers the probabilities of negative tokens is misleading. UL actually considers both positive and negative tokens, as pointed out by Reviewer 1. The CT objective is an incremental improvement over UL, but the incremental nature of the contribution is not sufficiently significant to justify a top-tier conference acceptance.\n- **Contribution**: The paper's main contribution is the introduction of a new training objective that aims to improve text generation quality by reducing repetition. However, the experimental setup and the metrics used are not robust enough to convincingly demonstrate the effectiveness of the proposed method.\n\n#### **2. Experimental Setup and Results**\n- **Experimental Setup**: The experiments are conducted on a fine-tuned GPT2-small model and a dialogue generation system. While the results show improvements in some metrics, the experimental setup is limited. Reviewer 2 points out that the primary result is on GPT2-small fine-tuned on wikitext, which is not a convincing setup by today's standards. Larger models and more diverse datasets are needed to validate the effectiveness of the proposed method.\n- **Metrics**: The paper uses perplexity, dist-1, and uniq-1 as metrics. However, perplexity is not a reliable metric for evaluating text generation tasks, and the interpretation of dist-1 is unclear. Reviewer 2 suggests that the paper should report dist-2,3, and 4, and use more recent neural metrics for a more comprehensive evaluation. The human evaluation is also not statistically significant, which further weakens the claims of the paper.\n\n#### **3. Clarity and Reproducibility**\n- **Clarity**: The paper is well-written and easy to follow. The idea is clearly explained, and the experimental setup is well-documented. The authors have also provided code, which is a positive aspect.\n- **Reproducibility**: The paper is reproducible, and the authors have provided code and a Google Colab notebook. However, the lack of robust experimental setup and metrics limits the reproducibility of the results.\n\n#### **4. Reviewer Feedback**\n- **Reviewer 1**: Points out the lack of novelty and the misleading claim about UL. Suggests additional experiments on other tasks to strengthen the paper.\n- **Reviewer 2**: Criticizes the experimental setup and the metrics used. Suggests using larger models and more recent metrics.\n- **Reviewer 3**: Agrees with the intuitive idea but finds the novelty lacking.\n- **Reviewer 4**: Suggests additional experiments to compare CT with UL using all previous tokens and to evaluate the method on larger models.\n\n### Final Decision\nGiven the limited novelty, the insufficient experimental setup, and the lack of robust metrics, the paper does not meet the standards of a top-tier conference. The incremental nature of the contribution and the need for more comprehensive experiments and evaluations are significant concerns.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Interesting Observation**: The paper presents a novel and interesting observation that swapping the order of batch normalization (BN) and bounded activation functions (like Tanh) can lead to better performance. This is a significant finding that challenges the conventional wisdom and could have practical implications for neural network design.\n2. **Comprehensive Analysis**: The authors provide a detailed analysis of the phenomenon, examining various metrics such as saturation, skewness, and sparsity. They also provide experimental results on multiple benchmarks and architectures, which adds to the robustness of their findings.\n3. **Clarity and Structure**: The paper is well-structured and clearly written, making it easy to follow the logic and reasoning behind the experiments and conclusions.\n\n#### Weaknesses of the Paper\n1. **Lack of Rigorous Support**: Several of the paper's claims are not well-supported or are based on limited evidence. For example, the claim that asymmetric saturation enhances generalization is not convincingly demonstrated. The reviewers have pointed out that the relationship between saturation and generalization is not clearly established.\n2. **Limited Scope**: The analysis is limited to specific types of bounded activation functions and architectures without residual connections. This limits the generalizability of the findings and their practical applicability. The paper does not explore how the findings might extend to more widely used architectures like ResNets.\n3. **Comparison with ReLU**: The paper does not show that the Swap model with bounded activation functions outperforms the Convention model with ReLU, which is a widely used activation function. This weakens the practical significance of the findings.\n4. **Technical Issues**: There are several technical issues and unclear points in the paper, such as the definition of \"the center of the function\" and the calculation of metrics like saturation and skewness. These issues need to be addressed to strengthen the paper's claims.\n5. **Reproducibility**: While the paper provides some details for reproducibility, the reviewers have noted that the code and specific calculations for metrics are not clearly documented, which could hinder reproducibility.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Suggests additional figures and a more detailed analysis of the skewness in different layers to support the claims.\n- **Reviewer 2**: Points out several issues with the arguments connecting saturation and sparsity to generalization performance. Also, questions the limited scope of the analysis and the lack of comparison with ReLU.\n- **Reviewer 3**: Agrees that the topic is interesting but finds the analysis less comprehensive and the conclusions less practical.\n- **Reviewer 4**: Expresses concerns about the lack of rigorous support for the claims and the need for further justifications and investigations.\n- **Reviewer 5**: Raises several specific concerns about the clarity and rigor of the claims, the experimental setup, and the practical significance of the findings.\n\n### Final Decision\nWhile the paper presents an interesting and novel observation, the weaknesses in the analysis, the limited scope, and the lack of rigorous support for the claims significantly undermine its overall quality and significance. The paper needs substantial revisions to address the reviewers' concerns and provide a more robust and comprehensive analysis.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **1. Technical Novelty and Significance**\n- **Strengths:**\n  - The paper introduces a novel mechanism for label differential privacy (DP) in regression tasks, which is a significant contribution to the field.\n  - The proposed \"randomized response on bins\" mechanism is theoretically proven to be optimal under a given regression loss function.\n  - The paper extends the concept of label DP from classification to regression, which is a valuable generalization.\n- **Weaknesses:**\n  - Some reviewers (e.g., Reviewer 3 and Reviewer 5) note that the technical novelty is somewhat limited, as aspects of the contributions exist in prior work, particularly in Ghazi et al. (2021a).\n  - The paper could benefit from a more detailed comparison with other label-DP methods, especially those that consider the second term in the loss function (as mentioned by Reviewer 5).\n\n#### **2. Empirical Novelty and Significance**\n- **Strengths:**\n  - The empirical evaluation is thorough and conducted on real-world datasets, demonstrating the efficacy of the proposed mechanism.\n  - The results show that the proposed method outperforms other common DP methods such as Laplace, Exponential, and staircase.\n- **Weaknesses:**\n  - The comparison with other label-DP methods is not as comprehensive as it could be, which might limit the impact of the empirical results.\n  - Some reviewers (e.g., Reviewer 5) suggest that the overall generalization error is more concerning in practical settings, and the paper could benefit from addressing this aspect.\n\n#### **3. Clarity and Quality**\n- **Strengths:**\n  - The paper is well-written and easy to follow, as noted by multiple reviewers.\n  - The derivations and proofs are clear, and the experimental setup is well-explained.\n- **Weaknesses:**\n  - There are some minor issues with the claims and statements, as noted by Reviewer 1 and Reviewer 6. These issues are generally minor and can be addressed with small changes.\n  - The paper could benefit from a more detailed discussion on why certain comparisons are not fair, as suggested by Reviewer 2.\n\n#### **4. Correctness**\n- **Strengths:**\n  - The claims and statements are generally well-supported and correct, as noted by Reviewer 4 and Reviewer 5.\n- **Weaknesses:**\n  - Some reviewers (e.g., Reviewer 3 and Reviewer 6) note that the correctness of some claims requires minor adjustments or additional explanations.\n\n### Final Decision\nGiven the significant technical contributions, the thorough empirical evaluation, and the clarity of the paper, the paper should be accepted for publication. However, the authors should address the minor issues and suggestions raised by the reviewers to strengthen the paper further.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Novel Framework**: The paper introduces a novel framework, Functional Risk Minimization (FRM), which models each data point as coming from its own function. This is a significant departure from the standard Empirical Risk Minimization (ERM) framework, which assumes a single function with additive noise.\n2. **Potential Impact**: The proposed framework has the potential to handle more realistic noise processes and could be highly impactful in various machine learning applications, particularly in scenarios where data variability is high.\n3. **Empirical Results**: The paper provides empirical results on small-scale experiments in regression and reinforcement learning, demonstrating that FRM can outperform ERM in certain settings.\n\n#### Weaknesses\n1. **Clarity and Presentation**: The paper suffers from significant clarity issues. The use of terms like \"ERM\" and \"FRM\" is non-standard and confusing. The relationship between the theoretical framework and the implementable algorithm is not clearly explained.\n2. **Theoretical Foundations**: The paper lacks formal arguments and theoretical guarantees. There is no clear explanation of why or when FRM will outperform ERM. The conditions under which the proposed algorithm approximates the population objective are not well-established.\n3. **Scalability**: The proposed algorithm is not scalable. It requires significant approximations and Hessian information, which can be computationally expensive for large models. The experiments are limited to small-scale settings, which raises questions about the practical applicability of the method.\n4. **Relation to Bayesian Models**: The relationship between FRM and hierarchical Bayesian models is not well-explained. The claim that FRM uses a single parameter at test time, despite each data point having its own function, is confusing and requires further clarification.\n5. **Experimental Evaluation**: The paper lacks a comprehensive experimental evaluation, particularly for the functional generative model. The experiments are limited and do not fully demonstrate the potential of the proposed framework.\n\n#### Reviewer Comments\n- **Reviewer 1**: Points out significant issues with clarity, lack of theoretical support, and scalability. The reviewer is concerned about the connection between the theoretical framework and the implementable algorithm.\n- **Reviewer 2**: Highlights the interesting idea but raises concerns about the relationship with hierarchical Bayesian models and the consistency between training and test objectives. The reviewer also notes the lack of experimental evaluation for the functional generative model.\n- **Reviewer 3**: Finds the paper marginally above the acceptance line, acknowledging the novel framework and potential impact but noting the high computational cost and the need for further discussion on the necessity of FRM over modified ERM.\n\n### Final Decision\nGiven the significant issues with clarity, lack of theoretical support, and scalability, the paper does not meet the standards of a top-tier conference. While the idea is novel and has potential, the current presentation and lack of robust empirical and theoretical validation make it difficult to support acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Novel Concept**: The idea of collaborative adversarial training (CAT) is innovative and addresses a gap in the literature by leveraging the complementary robustness of different adversarial training methods.\n2. **Empirical Results**: The paper demonstrates state-of-the-art robustness on CIFAR-10 and CIFAR-100 under the Auto-Attack benchmark, which is a strong empirical validation of the method.\n3. **Clarity and Readability**: The paper is well-written and easy to follow, making it accessible to a broad audience.\n4. **Relevance**: The problem of improving the robustness of neural networks is highly relevant and important in the field of machine learning.\n\n#### Weaknesses\n1. **Computational Cost**: The proposed method involves training two models simultaneously, which doubles the computational and memory requirements. This is a significant drawback, especially given the already high computational cost of adversarial training.\n2. **Lack of Theoretical Analysis**: The paper does not provide a theoretical analysis of why the proposed method works. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms and potential limitations.\n3. **Limited Baselines**: The empirical comparison does not include more recent and stronger baselines, such as ensemble adversarial training methods and other state-of-the-art techniques. This limits the robustness of the empirical claims.\n4. **Missing Citations**: The paper does not cite relevant prior work, such as ensemble adversarial training methods, which are crucial for placing the proposed method in the broader context of the field.\n5. **Ablation Studies**: The paper lacks detailed ablation studies, which are necessary to understand the contributions of different components of the proposed method.\n6. **Scalability**: The experiments are limited to small-scale datasets (CIFAR-10 and CIFAR-100). Larger datasets, such as ImageNet, are required to fully validate the effectiveness of the method.\n7. **Hyperparameters and Implementation Details**: The paper does not provide sufficient details about hyperparameters and implementation, which is crucial for reproducibility.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Highlights the increased computational cost and lack of theoretical analysis. Suggests comparing with ensemble methods and more recent baselines.\n- **Reviewer 2**: Criticizes the lack of novelty and the need for more detailed analysis and comparisons.\n- **Reviewer 3**: Points out the need for more extensive comparisons, including data augmentation methods and larger datasets.\n- **Reviewer 4**: Suggests additional ablation studies and comparisons with other techniques like Adversarial Weight Perturbation (AWP).\n\n### Final Decision\nWhile the proposed method of Collaborative Adversarial Training (CAT) is innovative and shows promising empirical results, the paper has several significant weaknesses that need to be addressed. The increased computational cost, lack of theoretical analysis, limited baselines, and missing citations are major concerns. Additionally, the paper would benefit from more detailed ablation studies and experiments on larger datasets.\n\nGiven these issues, the paper is not yet ready for publication in a top-tier conference. However, the authors are encouraged to address the reviewer comments and resubmit a revised version.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Convergence Analysis**: The paper provides a detailed and rigorous analysis of the convergence rates of gradient descent (GD) for deep linear neural networks. It shows that the convergence rate is depth-independent, provided the width of each hidden layer is sufficiently large. This is a significant theoretical contribution, especially in the context of overparameterized models.\n2. **Generalization of Loss Functions**: The paper extends the analysis to general strongly convex and smooth loss functions, which is a step beyond the commonly studied squared loss. This generalization is valuable and provides a broader understanding of the behavior of GD in different settings.\n3. **Initialization Schemes**: The paper considers multiple initialization schemes, including Gaussian, orthogonal, and balanced initializations. This comprehensive treatment of different initialization methods adds to the robustness of the results.\n4. **Trajectory Analysis**: Theorem 3.3, which shows that the trajectory of the product of weight matrices in the deep linear network closely follows the trajectory of the corresponding convex problem, is a novel and insightful result. This observation provides valuable intuition about the optimization dynamics of deep linear networks.\n\n#### Weaknesses and Concerns\n1. **Novelty and Technical Contributions**:\n   - **Reviewer 1** and **Reviewer 3** raise concerns about the novelty of the paper. They suggest that the extension to general loss functions and the handling of low-rank data might not be as challenging as claimed. They also point out that the techniques used are similar to those in previous works, such as Du and Hu (2019) and Hu et al. (2020).\n   - **Reviewer 5** and **Reviewer 6** express similar concerns about the novelty, particularly regarding the extension to general loss functions and the convergence region analysis.\n2. **Clarity and Insight**:\n   - **Reviewer 6** points out several issues with the clarity and depth of the paper. The reviewer suggests that the paper overclaims its results and lacks detailed discussions on the importance of overparameterization, the impact of regularization, and the behavior in a minibatch setting.\n   - The reviewer also notes that the minimum width requirement in Theorems 3.1 and 3.2 should be discussed in more detail, particularly how the constants depend on the problem's hyperparameters.\n3. **Empirical and Practical Relevance**:\n   - While the theoretical results are strong, the paper lacks empirical validation and practical insights. The reviewer suggests that the paper should include more intuitive explanations and discussions to make the results more accessible and relevant to a broader audience.\n\n#### Addressing the Concerns\n1. **Novelty and Technical Contributions**:\n   - The authors should provide a detailed comparison with existing works, highlighting the specific technical challenges and innovations in their analysis. This could include a discussion of the novel techniques used to handle general loss functions and the implications of these techniques.\n2. **Clarity and Insight**:\n   - The authors should revise the title and abstract to avoid overclaiming and to clearly specify the conditions under which the results hold (e.g., overparameterization).\n   - The paper should include more detailed discussions on the importance of overparameterization, the impact of regularization, and the behavior in a minibatch setting. This will help readers understand the practical implications of the theoretical results.\n3. **Empirical and Practical Relevance**:\n   - The authors should consider adding empirical results or simulations to validate their theoretical findings. This will make the paper more compelling and relevant to a broader audience.\n\n### Final Decision\nGiven the significant theoretical contributions and the novel insights provided by the paper, I believe it has the potential to be a valuable addition to the literature. However, the concerns raised by the reviewers about novelty, clarity, and practical relevance need to be addressed to ensure the paper meets the high standards of a top-tier conference.\n\nFinal Decision: [Accept with Revisions]"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Problem and Motivation**\nThe paper addresses a significant and practical problem in machine learning: the trade-off between using more data for training and having a reliable validation set for model evaluation. This is a well-recognized issue, especially in real-world applications where data is often limited. The motivation is clear and relevant to the ML community.\n\n#### **2. Novelty and Contribution**\nThe proposed Proximal Validation Protocol (PVP) is a novel approach that uses data augmentation and a distribution-preserving sampling method to create a validation set without splitting the training data. This idea is innovative and has the potential to improve the efficiency and reliability of model evaluation. The method is straightforward and easy to understand, which adds to its practical appeal.\n\n#### **3. Empirical Evaluation**\nThe empirical evaluation is a mixed bag. The paper demonstrates the effectiveness of PVP on three data modalities (images, text, and tabular data), which is a strength. However, there are several issues with the experiments:\n\n- **Dataset Choice**: The datasets used are relatively small, particularly in the image and text domains. The use of CIFAR10-LT and CIFAR100-LT is understandable, but the lack of evaluation on larger, more standard datasets like CIFAR-10, CIFAR-100, or ImageNet is a significant weakness. Similarly, the use of Distill-BERT instead of BERT for text classification is not well-justified.\n- **Evaluation Metrics**: The metrics used (F1 score, T-V gap, variance) are not clearly defined, and the interpretation of the results is ambiguous. For example, the \"deterioration\" metric in Tables 3 and 4 is not well-explained, leading to confusion.\n- **Ablation Studies**: The paper lacks thorough ablation studies to understand the impact of different choices, such as the type of data augmentation, the sampling method, and the size of the proximal validation set. This makes it difficult to assess the robustness of the method.\n- **Statistical Significance**: The paper does not provide statistical significance tests for the results, which is crucial for validating the claims.\n\n#### **4. Theoretical Justification**\nThe paper lacks a strong theoretical foundation. The authors do not provide a rigorous justification for why the proximal validation set is representative of the data distribution. This is a significant weakness, as the validity of the validation set is crucial for reliable model evaluation. The lack of theoretical guarantees makes it difficult to trust the empirical results.\n\n#### **5. Clarity and Presentation**\nThe paper is generally well-written and easy to follow, but there are several issues with clarity and presentation:\n\n- **Language and Terminology**: The use of informal language (e.g., \"plague,\" \"entertained,\" \"frustratingly\") is unprofessional and should be avoided in a scientific paper.\n- **Definitions and Metrics**: Key metrics and concepts are not clearly defined, leading to confusion.\n- **Tables and Figures**: The tables (especially Tables 3 and 4) are not well-presented and lack clear explanations.\n\n### Final Decision\nGiven the significant issues with the empirical evaluation, the lack of theoretical justification, and the clarity of presentation, the paper does not meet the standards of a top-tier conference. While the problem and the proposed solution are novel and important, the current version of the paper is not sufficiently robust or well-supported to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Technical Contributions and Novelty\n1. **Main Contributions:**\n   - The paper aims to identify the precise phase transition thresholds for permuted linear regression in both the oracle and non-oracle settings.\n   - It uses a message passing (MP) algorithm and the cavity method to derive these thresholds.\n   - The authors claim to provide a more precise identification of phase transition thresholds compared to previous works, which only provided convergence rates without specifying constants.\n\n2. **Novelty:**\n   - The precise identification of phase transition thresholds is a significant contribution.\n   - The use of the cavity method and MP algorithm to derive these thresholds is novel, though heavily inspired by prior work (e.g., Semerjian et al. 2020).\n\n3. **Incremental Nature:**\n   - Reviewers 1 and 2 point out that the results are incremental, particularly in the oracle case, which is a corollary of existing work.\n   - The non-oracle case, while more novel, relies on approximations and is not fully rigorous.\n\n#### Clarity and Quality of Writing\n1. **Clarity:**\n   - The paper is poorly written and difficult to follow, as noted by multiple reviewers.\n   - Notations are heavy and not well-explained, making it challenging for readers unfamiliar with the background material.\n   - The paper lacks clarity in explaining key concepts and transitions between sections.\n\n2. **Quality:**\n   - The paper contains numerous typos and grammatical errors, which detract from its overall quality.\n   - The organization is poor, with sections that are redundant or poorly structured.\n\n#### Correctness and Empirical Results\n1. **Correctness:**\n   - Reviewer 5 points out several inaccuracies and inconsistencies in the mathematical writing.\n   - Some claims, such as the precise identification of phase transition thresholds, are not well-supported by the provided theory or empirical results.\n   - The empirical results are not fully aligned with the theoretical predictions, as noted by Reviewer 5.\n\n2. **Empirical Results:**\n   - The numerical experiments are presented, but the results are not clearly explained or well-aligned with the theoretical predictions.\n   - The code for the numerical results is not provided, which affects reproducibility.\n\n#### Significance and Impact\n1. **Significance:**\n   - The problem of permuted linear regression is important, and understanding phase transitions is valuable.\n   - However, the incremental nature of the results and the lack of clarity in the presentation limit the paper's impact.\n\n2. **Impact:**\n   - The paper could be improved with better writing and a more rigorous analysis.\n   - The results, while interesting, are not groundbreaking enough to justify acceptance in a top-tier conference.\n\n### Final Decision\nGiven the significant issues with clarity, quality, and the incremental nature of the results, the paper should be rejected for publication in its current state. The authors are encouraged to revise the paper, addressing the reviewers' comments, and resubmit to a more appropriate venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Motivation and Relevance:**\n   - The paper addresses a critical problem in bioinformatics: predicting protein-protein interactions (PPIs). The motivation is well-founded, as understanding PPIs is crucial for unraveling cellular behavior and functionality.\n   - The use of structural embeddings to capture the physical binding of proteins is a novel and important direction, as it complements sequence and network information.\n\n2. **Methodology:**\n   - The authors propose a method that leverages pretrained structural embeddings from OmegaFold and integrates them into a graph neural network (GNN) for PPI prediction.\n   - The transferability of PPI prediction models across different species is an interesting and valuable aspect of the work.\n\n#### **Weaknesses:**\n1. **Limited Novelty:**\n   - The main contribution of the paper is the integration of pretrained structural embeddings into a GNN for PPI prediction. However, the novelty is limited because the embeddings are directly adopted from OmegaFold without significant modifications.\n   - The GNN architecture itself is not novel and is a standard approach in the field.\n\n2. **Empirical Evaluation:**\n   - The empirical results show only marginal improvements over baseline methods (GraphSAGE and GAT). This raises questions about the significance of the proposed method.\n   - The baselines chosen for comparison are not the state-of-the-art (SOTA) methods in PPI prediction. For example, AlphaFold-Multimer and dMaSIF are not included in the comparison, which weakens the validity of the results.\n   - The paper lacks confidence intervals and detailed statistical analysis, which are crucial for robust empirical evaluation.\n\n3. **Clarity and Presentation:**\n   - The paper has several clarity issues. For example, the AUC metric and the binary classification problem setup are not clearly explained.\n   - The tables and figures could be better formatted and more easily digestible. Confidence intervals should be included to provide a more comprehensive understanding of the results.\n   - The paper could benefit from additional proofreading to correct typos and grammatical mistakes.\n\n4. **Technical Soundness:**\n   - The negative sampling method used in the GNN is not clearly described, which affects the reproducibility of the results.\n   - The paper does not provide a clear rationale for using a GNN on a network of proteins instead of a pairwise interaction model. This choice should be justified and compared against a pairwise model.\n\n#### **Reviewer Comments:**\n- **Reviewer 1:** Points out the limited novelty and contribution, the potential issues with mean pooling, and the inconsistency in the results. Suggests minor improvements in clarity and reproducibility.\n- **Reviewer 2:** Emphasizes the limited novelty and the lack of significant modifications to existing methods.\n- **Reviewer 3:** Highlights the lack of clarity, the marginal improvements, and the weak baseline choices. Suggests comparing with SOTA methods and providing confidence intervals.\n- **Reviewer 4:** Criticizes the poor baseline choices, the unclear model design, and the lack of comparison with AlphaFold-Multimer. Questions the necessity of the GNN layer and the negative sampling method.\n\n### Final Decision\nGiven the limited novelty, the weak empirical evaluation, and the clarity issues, the paper does not meet the standards of a top-tier conference. The contributions are not significant enough to justify acceptance, and the empirical results do not convincingly demonstrate the effectiveness of the proposed method.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Novel Approach**: The paper introduces a novel method for protein sequence and structure co-design using a trigonometry-aware encoder and a roto-translation equivariant decoder. This approach is innovative and addresses the limitations of existing autoregressive and diffusion models.\n2. **Strong Empirical Results**: The method outperforms previous state-of-the-art baselines on multiple tasks, including antibody CDR design and protein design benchmarks. The results are significant and demonstrate the effectiveness of the proposed approach.\n3. **Efficiency**: The model significantly reduces inference time by updating all amino acids in one shot during each translation step, which is a substantial improvement over sampling-based methods.\n4. **Clarity and Presentation**: The paper is well-written, with a clear introduction and related work section. The method is reasonably well-explained, and the results are presented in a structured manner.\n\n#### Weaknesses:\n1. **Hyperparameter and Training Details**: The paper lacks specific hyperparameter and training details, which are crucial for reproducibility. This is a significant concern, as it makes it difficult for other researchers to replicate the results.\n2. **Discrepancies with Prior Work**: There are discrepancies in the reported results compared to previous work, particularly with the Diffusion model. The authors need to address these discrepancies to ensure the validity of their claims.\n3. **Functional Evaluation**: The paper relies heavily on metrics like PPL, RMSD, and AAR, which may not fully capture the functional performance of the designed proteins. Additional functional-oriented evaluations, such as protein-protein interaction patterns and charge distributions, would strengthen the paper.\n4. **Generalizability**: The method requires context features (e.g., contact maps and secondary structures), which may limit its applicability in scenarios where such information is not available. The authors should provide more discussion on the practicality of these requirements.\n5. **Novelty Claims**: Some reviewers have raised concerns about the novelty claims, particularly regarding the similarity to Alphafold2 and other methods. The authors need to clarify the unique contributions of their work and avoid overclaiming.\n6. **Ablation Studies**: The paper lacks ablation studies, which are essential to understand the contribution of each component of the model. This makes it difficult to assess the significance of the proposed modifications.\n\n#### Reviewer Feedback:\n- **Positive Feedback**: Reviewers 1, 3, 5, 6, and 8 generally support the paper, highlighting its strengths and the significance of the results. They suggest minor improvements and clarifications.\n- **Negative Feedback**: Reviewers 2, 4, and 7 raise important concerns about the paper's claims, reproducibility, and novelty. These concerns need to be addressed to strengthen the paper.\n\n### Final Decision\nGiven the strong empirical results and the innovative approach, the paper has significant potential to contribute to the field of protein design. However, the lack of hyperparameter and training details, discrepancies with prior work, and the need for additional functional evaluations are major concerns that need to be addressed. The authors should also clarify their novelty claims and provide ablation studies to demonstrate the significance of their contributions.\n\nFinal Decision: **Accept** with the condition that the authors address the following points:\n1. Provide detailed hyperparameter and training details.\n2. Resolve discrepancies with prior work.\n3. Include additional functional evaluations.\n4. Clarify novelty claims and provide ablation studies."
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Problem Relevance and Motivation\nThe paper addresses a highly relevant problem in machine learning: adapting models to distribution shifts, particularly in the context of online learning. The focus on label shift in the presence of conditional shift is well-motivated, as it reflects real-world scenarios where both types of shifts can occur simultaneously. This is a significant and practical problem that many machine learning practitioners face.\n\n#### Technical Contributions\n1. **Heuristics for Label Shift Adaptation**:\n   - **Heuristic 1 (OOD Validation Set)**: The idea of using an out-of-distribution (OOD) validation set to improve the estimation of the confusion matrix is a practical and intuitive approach. However, the empirical results show mixed performance, with improvements in some cases but not in others. The lack of a systematic exploration of different types of distribution shifts and the limited scope of experiments weaken the impact of this heuristic.\n   - **Heuristic 2 (Hyper-parameters for Conditional Shift)**: The introduction of hyper-parameters to account for conditional shift is a reasonable approach, but the paper does not provide a clear theoretical foundation or a comprehensive empirical evaluation to support its effectiveness.\n   - **Heuristic 3 (Non-invertible Confusion Matrix)**: The proposed methods to handle non-invertible confusion matrices are heuristic and lack a strong theoretical justification. The paper does not compare these methods to existing techniques like pseudo-inverses or soft probability matrices.\n\n2. **Regression Label Shift**:\n   - The extension of label shift adaptation to regression problems is a novel contribution. However, the paper does not provide a thorough exploration of this extension, and the empirical results are limited.\n\n#### Empirical Evaluation\n- **Scope and Depth**: The empirical evaluation is modest in terms of the types and strengths of distribution shifts, the types of data, and the alternative baselines/methods. The paper does not provide a comprehensive comparison with other domain adaptation or domain generalization methods, which limits the generalizability of the results.\n- **Clarity and Reproducibility**: The experiments are described clearly, and the results are reproducible. However, some details are missing, such as the specifics of the grid search for hyper-parameters and the exact protocols for dataset splits.\n\n#### Clarity and Presentation\n- **Clarity**: The paper is generally well-organized and easy to follow. However, some sections are unclear or lack necessary definitions and explanations. For example, the definition of \"conditional shift\" is not provided, and the notation in some equations is confusing.\n- **Quality**: The quality of the paper is affected by the lack of a systematic approach to the types of distribution shifts considered and the limited empirical scope. The paper is honest about the inconclusive nature of some results, which is a positive aspect.\n\n#### Novelty and Significance\n- **Technical Novelty**: The heuristics proposed are novel but not groundbreaking. The paper builds on existing work and introduces practical extensions, but the lack of theoretical support and comprehensive empirical validation limits the significance of these contributions.\n- **Empirical Novelty**: The empirical results are interesting but limited in scope. The paper does not provide a broad and systematic evaluation of the proposed methods across a wide range of distribution shifts and datasets.\n\n### Final Decision\nGiven the mixed empirical results, the limited scope of the experiments, and the lack of a strong theoretical foundation, the paper does not meet the high standards of a top-tier conference. While the problem is well-motivated and the heuristics are practical, the contributions are not sufficiently significant or novel to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Clarity and Quality of Writing**\n- **Reviewer 1** and **Reviewer 3** both comment on the clarity and quality of writing. Reviewer 1 finds the paper poorly written, with obscure sentences and grammatical issues, while Reviewer 3 finds it well-written and easy to follow. Reviewer 2 does not provide a specific comment on clarity but notes that the background section is too long.\n- **Conclusion**: The paper has mixed feedback on clarity. While Reviewer 3 finds it well-written, the issues raised by Reviewer 1 are significant and need to be addressed. The authors should revise the paper to improve clarity and correct grammatical issues.\n\n#### **2. Technical Novelty and Significance**\n- **Reviewer 1** and **Reviewer 2** both rate the technical novelty and significance low, with scores of 2 and 1, respectively. They argue that the methods are standard and not innovative, with softmax relaxations and ensemble techniques being well-known.\n- **Reviewer 3** provides a more balanced view, rating the technical novelty and significance as 3. They acknowledge the novelty of applying gradient attacks to tabular data and the use of pooling and softmax, but also raise concerns about the specificity of the challenges and the fairness of comparing with NLP methods.\n- **Conclusion**: The technical novelty is a significant point of contention. While the application to tabular data is novel, the underlying techniques are not highly innovative. The authors should address the concerns raised by Reviewer 3 regarding the specificity of the challenges and the fairness of comparisons.\n\n#### **3. Empirical Novelty and Significance**\n- **Reviewer 1** and **Reviewer 2** both rate the empirical novelty and significance low, with scores of 2 and 1, respectively. They argue that the benchmark models are weak and the improvements are marginal.\n- **Reviewer 3** rates the empirical novelty and significance highly, with a score of 4. They find the experimental demonstration compelling and provide detailed feedback on how to improve the experiments.\n- **Conclusion**: The empirical results are strong, but the benchmark models and the robustness of the methods need to be improved. The authors should address the suggestions from Reviewer 3 to strengthen the empirical evaluation.\n\n#### **4. Correctness**\n- **Reviewer 1** and **Reviewer 3** both rate the correctness as 3, indicating that some claims have minor issues and require small changes to be correct.\n- **Reviewer 2** rates the correctness as 4, indicating that all claims and statements are well-supported and correct.\n- **Conclusion**: The paper is generally correct, but some claims need to be refined. The authors should address the minor issues raised by Reviewer 1 and Reviewer 3.\n\n#### **5. Overall Contribution**\n- **Strengths**:\n  - Novel application of gradient attacks to tabular data.\n  - Thorough experimental evaluation.\n  - Clear and compelling experimental results.\n- **Weaknesses**:\n  - Standard techniques with limited novelty.\n  - Issues with clarity and writing.\n  - Some claims and comparisons need to be refined.\n\n### Final Decision\nGiven the mixed feedback, the paper has significant strengths in its empirical evaluation and the novel application to tabular data. However, the technical novelty is a concern, and the paper needs to be revised to address the issues raised by the reviewers, particularly regarding clarity, the specificity of the challenges, and the robustness of the methods.\n\nFinal Decision: **Reject**\n\nThe paper should be rejected for publication in its current form but with a strong recommendation for revision. The authors should address the issues raised by the reviewers and resubmit for consideration."
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Theoretical Contributions and Soundness**\n- **Strengths:**\n  - The paper provides a novel theoretical analysis of how on-policy algorithms can be adapted to the offline RL setting. The derivation of the policy improvement theorem in the offline context is a significant contribution.\n  - The authors extend the policy improvement theorem to the offline setting and propose a method (BPPO) that leverages the inherent conservatism of on-policy algorithms to address the challenges of offline RL.\n\n- **Weaknesses:**\n  - **Assumption 1:** The assumption that the learned behavior policy \\(\\hat{\\pi}_\\beta\\) is equal to the true behavior policy \\(\\pi_\\beta\\) is strong and may not hold in practice. This assumption is critical for the theoretical results, and the paper does not provide empirical evidence to support its feasibility.\n  - **Soundness of Approximations:** The approximations made in the algorithm, particularly the replacement of the importance sampling ratio and the use of the behavior policy's advantage estimates, are not rigorously justified. The reviewers raise concerns about the looseness of these approximations and their impact on the policy improvement guarantees.\n  - **Convergence to Optimal Policy:** The paper does not provide a theoretical guarantee that BPPO will converge to the optimal policy, which is a significant limitation.\n\n#### **2. Empirical Evaluation**\n- **Strengths:**\n  - The empirical results on the D4RL benchmark show that BPPO outperforms or is competitive with state-of-the-art methods. This is a strong point in favor of the paper.\n  - The authors provide ablation studies on hyperparameters, which adds to the robustness of the empirical evaluation.\n\n- **Weaknesses:**\n  - **Hyperparameter Analysis:** The paper lacks a thorough analysis of the impact of key hyperparameters, such as the clip coefficient decay and the asymmetric coefficient for the advantage. This is a significant gap in the empirical evaluation.\n  - **Comparison with Baselines:** The comparison with baselines is not comprehensive. The paper should include more recent and relevant baselines like EDAC, LAPO, and RORL to strengthen the empirical claims.\n  - **Sparse Reward Settings:** The paper does not evaluate BPPO on sparse reward settings, which are common in offline RL. This is a critical omission, as many recent works focus on such settings.\n\n#### **3. Clarity and Presentation**\n- **Strengths:**\n  - The paper is generally well-written and the presentation of the proposed algorithm is clear.\n  - The theoretical derivations are well-explained, and the experimental setup is well-documented.\n\n- **Weaknesses:**\n  - **Clarity Issues:** Some sections of the paper are unclear and need further polishing. For example, the first 5 lines after Eq. 11 are confusing, and the paper contains some colloquial language that could be improved.\n  - **Reproducibility:** The source code is not provided, which makes it difficult for other researchers to reproduce the results. The authors should make their implementation available to enhance reproducibility.\n\n#### **4. Novelty and Significance**\n- **Strengths:**\n  - The paper introduces a novel perspective on the use of on-policy algorithms in offline RL, which is a significant contribution to the field.\n  - The proposed method (BPPO) is a simple and effective adaptation of PPO to the offline setting.\n\n- **Weaknesses:**\n  - **Overlap with Prior Work:** The theoretical results and algorithm design have significant overlap with prior works like GePPO and other off-policy TRPO/PPO advances. The paper should clearly differentiate its contributions from these works.\n  - **Practical Impact:** The empirical results show only minor improvements over existing baselines, and the practical significance of these improvements is not clear. The paper should provide more compelling evidence of the practical utility of BPPO.\n\n### Final Decision\nGiven the strengths and weaknesses outlined above, the paper has several significant contributions but also notable limitations. The theoretical analysis is novel and the empirical results are promising, but the soundness of the approximations and the lack of comprehensive empirical evaluation are major concerns. The paper would benefit from addressing these issues in a revised version.\n\nFinal Decision: **Reject**\n\nHowever, the authors are encouraged to address the following points in a revised submission:\n1. **Strengthen Theoretical Soundness:** Provide a more rigorous justification for the approximations and address the concerns about Assumption 1.\n2. **Comprehensive Empirical Evaluation:** Include a thorough analysis of key hyperparameters and evaluate BPPO on a broader range of tasks, including sparse reward settings.\n3. **Comparison with Recent Baselines:** Include more recent and relevant baselines in the empirical comparison.\n4. **Improve Clarity and Reproducibility:** Polish the writing and provide the source code to enhance reproducibility."
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Novel Idea:**\n   - The paper introduces a novel concept of using Binary Labels as an auxiliary task in multi-task learning. This idea is interesting and has the potential to improve the performance of classification models.\n2. **Empirical Results:**\n   - The authors show that their approach can improve accuracy on several datasets, including Cifar-100, Caltech101, Sun397, FGVC-Aircraft, and Stanford Cars. They also demonstrate improvements in data efficiency, which is a valuable contribution.\n3. **Simplicity:**\n   - The proposed method is relatively simple to implement and can be easily integrated into existing neural network architectures.\n\n#### **Weaknesses of the Paper:**\n1. **Clarity and Explanation:**\n   - The paper has several clarity issues. For example, the concept of \"dense labels\" is mentioned without explanation, and the use of Metabalance for multi-task learning is not clearly described. The exact procedure for generating Binary Labels and the multi-task learning setup is not well-explained, making it difficult to reproduce the results.\n2. **Experimental Section:**\n   - The experimental section is weak. The authors should compare their method against more baselines, including other label representation techniques and multi-task learning variants. Additional experiments, such as few-shot learning tasks and robustness to distribution shifts, would strengthen the paper.\n3. **Related Work:**\n   - The paper lacks a thorough discussion of related work. The authors should compare their method to other label representation techniques, such as label smoothing, and discuss the differences. They should also discuss unsupervised meta-learning and other related approaches.\n4. **Technical Depth:**\n   - The technical contribution of the paper is limited. The multi-task learning architecture is basic, and there is no in-depth analysis of why Binary Labels improve performance. The paper lacks a detailed explanation of the properties that Binary Labels model and how they differ from one-hot labels.\n5. **Generalization:**\n   - The proposed approach is not evaluated on large benchmarks like ImageNet, which limits the generalizability of the results. The authors should also explore the applicability of their method to multilabel image classification.\n6. **Hyperparameter Tuning:**\n   - The paper does not provide a detailed analysis of the hyperparameters, such as the weighting between the two tasks. This is important for understanding the robustness of the model.\n\n#### **Summary of Reviewer Comments:**\n- **Reviewer 1:** Highlights the novelty of the idea but points out the need for more thorough experiments, better clarity, and a more comprehensive discussion of related work.\n- **Reviewer 2:** Criticizes the paper for poor notation, calculation errors, and insufficient empirical results. The reviewer also points out that the claim about the lack of research on labels is not well-supported.\n- **Reviewer 3:** Finds the method simple and effective but notes that the contribution is not significant enough due to small empirical improvements and the lack of further analysis.\n- **Reviewer 4:** Points out the limited technical contribution and the need for more detailed explanations and experiments. The reviewer also suggests evaluating the method on larger benchmarks and analyzing the weighting between tasks.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, and the feedback from the reviewers, the paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference. The experimental section is weak, the technical contribution is limited, and the paper lacks a thorough discussion of related work. While the idea of using Binary Labels as an auxiliary task is novel and interesting, the current state of the paper does not meet the high standards of a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Novelty**: The paper introduces a framework to study the average-case performance of q-replicator dynamics (QRD) in games, which is a novel and important direction. However, the novelty is somewhat diminished by the overlap with prior work, particularly PP16. The paper generalizes some results from PP16 to the QRD class, but the extent of this generalization and its significance are not fully clear.\n- **Significance**: The problem of understanding the quality of equilibria reached by learning dynamics is significant and relevant to the community. However, the results are currently limited to 2x2 symmetric coordination games, which is a very specific and restricted setting. The paper does not provide a clear path for extending these results to more general games, which limits its broader impact.\n\n#### **2. Technical Contributions**\n- **Formal Results**: The formal results, while solid, are partial and limited. Theorem 4.4, which compares the average performance of q=0 and q=1, is only valid for a specific range of parameters (α ≥ 0.5). The paper lacks a comprehensive treatment of the general case, and the empirical evidence for the monotonicity of regions of attraction is not formally proven.\n- **Empirical Results**: The empirical results are promising but preliminary. The paper provides some visual and experimental evidence, but the lack of a more systematic and comprehensive empirical analysis weakens the overall contribution.\n\n#### **3. Clarity and Presentation**\n- **Clarity**: The paper is generally well-written and easy to follow. The motivation and problem statement are clear, and the proof ideas are well-explained.\n- **Presentation**: There are some issues with citation formatting and notation overloading, which can affect readability. The abstract is somewhat misleading, as it suggests a more comprehensive treatment of the problem than what is actually presented in the paper.\n\n#### **4. Comparison with Prior Work**\n- **PP16**: The paper does not give enough credit to PP16, which has already explored similar concepts in a more restricted setting. The lack of a detailed comparison with PP16 is a significant weakness. The paper should provide a more thorough discussion of how its results extend or differ from those in PP16.\n\n#### **5. Potential Impact**\n- **Relevance**: The problem of average-case performance in learning dynamics is relevant and important. However, the current results are limited to a specific class of games, which may not have a broad impact on the field.\n- **Future Work**: The paper sets the stage for future research by introducing a new framework, but the current results are not sufficient to justify acceptance at a top-tier conference. More comprehensive results and a clearer path for generalization are needed.\n\n### Final Decision\nGiven the partial nature of the results, the limited scope of the analysis, and the lack of a thorough comparison with prior work, the paper does not meet the standards of a top-tier conference. While the problem is important and the initial results are promising, the paper needs significant improvements to be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Novelty**: The paper introduces a method called \"Scrunch\" for privacy-preserving representation learning in the MLaaS setting. The core idea of splitting the neural network and using an additional loss function (Center Loss) to control information leakage is interesting. However, the novelty is somewhat limited as the idea of splitting networks for privacy has been explored before (e.g., split learning). The addition of Center Loss to control mutual information flow is a novel aspect, but it is not a groundbreaking innovation.\n- **Significance**: The significance of the work is also limited. The paper does not address a new problem but rather proposes a new method to solve an existing problem. The method's effectiveness is not convincingly demonstrated, and the threat model is not well-defined, which undermines the significance of the contributions.\n\n#### **2. Technical Soundness and Correctness**\n- **Technical Soundness**: Reviewer 2 points out several critical flaws in the proposed approach:\n  - The method requires fine-tuning the encoder during the training phase, which contradicts the claim of privacy-preserving inference.\n  - The server can easily recreate the client's encoder, making the privacy guarantees questionable.\n  - The threat model is poorly defined, and the server can easily turn off the Center Loss, rendering the method ineffective.\n- **Correctness**: The paper makes several claims that are not well-supported or are incorrect. For example, the assumption that the server will honestly implement the Center Loss is not realistic, and the paper does not provide a convincing argument for why this assumption holds.\n\n#### **3. Empirical Evaluation**\n- **Datasets and Baselines**: The experimental evaluation is limited to two datasets (MNIST and CelebA) and two models (LeNet-5 and VGG-16). The choice of datasets and models is not representative of real-world scenarios. The paper does not compare the proposed method with a wide range of baselines, including fully homomorphic encryption-based approaches, which are relevant for privacy-preserving ML.\n- **Metrics and Results**: The evaluation metrics used are appropriate, but the results are not convincing. The paper does not provide a comprehensive comparison with other state-of-the-art methods, and the results on the MNIST dataset are not well-explained.\n\n#### **4. Clarity and Reproducibility**\n- **Clarity**: The paper is generally clear and easy to follow, but it contains several grammatical errors and ambiguous statements. The clarity of the paper could be improved with better proofreading and more precise language.\n- **Reproducibility**: The paper provides some details on the hyperparameter settings, but it lacks important information such as the optimizers used and the number of training epochs. This makes the reproducibility of the results less straightforward.\n\n#### **5. Overall Impact**\n- **Impact**: The paper does not make a significant impact on the field of privacy-preserving ML. The method is not robust against realistic threat models, and the empirical evaluation is not comprehensive enough to support the claims made in the paper.\n\n### Final Decision\nGiven the limited novelty, technical flaws, and insufficient empirical evaluation, the paper does not meet the standards of a top-tier conference. Therefore, I recommend rejecting the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Problem Relevance and Importance:**\n   - The paper addresses a significant challenge in reinforcement learning (RL) and game AI: maintaining a diverse set of high-skill agents through self-play. This is a relevant and important problem, especially in complex games where a single policy may not generalize well to diverse opponents.\n\n2. **Methodological Contribution:**\n   - The proposed bi-objective optimization model is a novel approach to maintaining diversity and skill level simultaneously. The use of an evolutionary algorithm to optimize this bi-objective function is a reasonable and well-motivated method.\n\n3. **Empirical Evaluation:**\n   - The paper provides empirical results on two different games: Pong and Justice Online. The results show that the proposed method can learn a well-generalized policy and maintain a diverse set of high-level agents. This is a strong empirical validation of the method's effectiveness.\n\n4. **Clarity and Readability:**\n   - The paper is generally well-written and easy to follow. The authors provide a clear explanation of the method and the experimental setup, making it accessible to a broad audience.\n\n#### **Weaknesses:**\n1. **Literature Review and Baselines:**\n   - **Missing Key Literature:** The paper does not discuss important related work, such as the \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al. (2017). This is a significant oversight, as it provides a general framework for multiagent RL that includes self-play algorithms like Iterated-Best Response, Fictitious Play, and Double Oracle. These algorithms should be considered as baselines in the experiments.\n   - **Quality-Diversity Algorithms:** The paper does not compare the proposed method to quality-diversity algorithms like MAP-Elites, which are widely used for maintaining diverse populations in RL. This is another important gap in the literature review and experimental design.\n\n2. **Experimental Design:**\n   - **Biased Opponent Sampling:** The experimental design in Table 3 is problematic because it gives an advantage to the proposed method (BiO) and EMOGI by over-representing them in the pool of opponents. This makes it difficult to draw fair conclusions about the relative performance of the methods.\n   - **Limited Domains:** The paper only evaluates the method on two games: Pong and Justice Online. While these are good choices, the results would be more robust if the method were tested on a broader range of benchmark domains to demonstrate its generalizability.\n\n3. **Technical Details and Reproducibility:**\n   - **Missing Details:** The paper lacks some important details, such as the number of games run to produce the win percentages in Tables 1 and 2, and the confidence intervals for these results. These details are crucial for assessing the statistical significance of the findings.\n   - **Code Availability:** The paper does not mention the availability of source code, which is important for reproducibility and further validation by the research community.\n\n4. **Conceptual Simplifications:**\n   - **Single Scalar for Playing Style:** Reducing playing style to a single scalar (0 for defensive, 1 for aggressive) is a significant simplification. While this may work well in the evaluated domains, it is a limitation that should be acknowledged and discussed in the paper. Future work could explore more complex representations of playing style.\n\n#### **Final Decision:**\nGiven the strengths and weaknesses of the paper, the following points are critical in making the final decision:\n\n- **Technical Novelty and Significance:** The paper introduces a novel method for maintaining diverse playing styles in self-play, which is a significant contribution. However, the lack of comparison to key related work and quality-diversity algorithms reduces the novelty and significance of the contribution.\n- **Empirical Novelty and Significance:** The empirical results are promising, but the experimental design has significant issues, particularly the biased opponent sampling. The results would be more robust if the method were tested on a broader range of domains and compared to a wider set of baselines.\n- **Clarity and Reproducibility:** The paper is well-written and provides enough details to reproduce the results, but some important details are missing, and the source code is not available.\n\nConsidering these factors, the paper has strong potential but requires substantial revisions to address the identified weaknesses. Therefore, the final decision is:\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Identification of Period Drift:** The paper introduces the concept of \"period drift,\" which is a novel and important consideration in federated learning. This concept addresses the heterogeneity in data distributions across different communication rounds, which is a significant and often overlooked issue.\n2. **Meta-Learning Framework:** The proposed method, FedPA, uses a meta-learning approach to learn an adaptive aggregation strategy. This is a creative and potentially effective way to handle the identified drifts.\n3. **Experimental Results:** The paper provides experimental results on two datasets (EMNIST and MovieLens) that demonstrate the effectiveness of FedPA compared to baseline methods.\n\n#### **Weaknesses:**\n1. **Lack of Theoretical Justification:** The paper does not provide a strong theoretical foundation for the concept of period drift or why the proposed meta-learning approach is particularly suited to address it. This makes it difficult to assess the robustness and generalizability of the method.\n2. **Proxy Dataset Assumption:** The use of a proxy dataset is a strong assumption that limits the practical applicability of the method. The paper does not explore alternative approaches or provide a thorough discussion of the implications of this assumption.\n3. **Ablation Studies:** The paper lacks comprehensive ablation studies to understand the impact of different components of the proposed method, such as the data distribution of the proxy dataset and the architecture of the neural networks used for aggregation.\n4. **Comparison with Related Work:** The paper does not compare FedPA with other relevant methods, such as FedET and DS-FL, which are also designed to handle non-iid data in federated learning. This makes it difficult to evaluate the relative performance and novelty of FedPA.\n5. **Experimental Settings:** The experimental settings are not strong enough. The datasets and models used are relatively simple, and the paper does not include results on more challenging datasets like CIFAR-100 or Stack Overflow.\n6. **Clarity and Quality:** The paper has several grammatical and formatting issues, which affect its readability and overall quality. The figures are also poorly formatted, making it difficult to interpret the results.\n7. **Reproducibility:** The paper does not provide code or detailed implementation details, which limits its reproducibility.\n\n#### **Summary of Reviewer Comments:**\n- **Reviewer 1:** Points out the lack of discrimination between client drift and period drift, the use of a proxy dataset, and the need for more comprehensive experiments and ablation studies.\n- **Reviewer 2:** Highlights the importance of the period drift concept but suggests more in-depth discussion on the advantages of the meta-learning approach and the need for additional baselines.\n- **Reviewer 3:** Expresses concerns about the strong assumption of a proxy dataset, the ad-hoc nature of the method, and the experimental performance. Also, suggests more ablation studies and hyperparameter tuning details.\n- **Reviewer 4:** Notes the importance of the drift issues but points out several writing and clarity issues, the need for more diverse datasets, and the lack of quantification of server overhead.\n\n### Final Decision\nGiven the significant weaknesses in theoretical justification, experimental settings, and reproducibility, as well as the need for more comprehensive comparisons and ablation studies, the paper does not meet the standards of a top-tier conference. While the concept of period drift and the meta-learning approach are interesting, the paper requires substantial revisions to address the identified issues.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Dataset Quality and Uniqueness**:\n   - The RuDar dataset is comprehensive, covering multiple climate zones and providing 3D radar echo observations, which is a significant improvement over existing datasets that often lack vertical dimension data.\n   - The dataset includes a wide range of measurements (precipitation intensity, reflectivity, radial velocity) at multiple altitude levels, which is crucial for accurate precipitation nowcasting.\n   - The dataset is large, with over 50,000 timestamps over a two-year period, and is publicly available, which is a valuable resource for the research community.\n\n2. **Evaluation and Baselines**:\n   - The authors evaluate several baselines, including optical flow and neural network models, which provides a solid foundation for future research.\n   - The inclusion of uncertainty quantification and ensemble scenario evaluation adds depth to the evaluation, showing the robustness of the dataset.\n\n3. **Clarity and Reproducibility**:\n   - The paper is well-written and easy to understand, with clear descriptions of the dataset and evaluation methods.\n   - Both the dataset and code for data processing and model preparation are publicly available, ensuring reproducibility.\n\n#### Weaknesses:\n1. **Methodological Novelty**:\n   - The paper lacks methodological novelties. The baselines used are relatively simple, and there is no exploration of more advanced models like transformers.\n   - The uncertainty estimation method is straightforward and could be enhanced with more sophisticated techniques.\n\n2. **Cross-Dataset Evaluation**:\n   - The paper does not evaluate the performance of models trained on the RuDar dataset on other datasets, which would provide a more comprehensive understanding of the dataset's utility.\n   - There is a lack of cross-evaluation with datasets from other regions, which could help in understanding the generalizability of the models.\n\n3. **Evaluation Metrics**:\n   - The evaluation metrics used (MSE) are limited and do not fully capture the accuracy of nowcasting. Additional metrics like MAE, RMSE, and others could provide a more comprehensive evaluation.\n\n4. **Additional Applications**:\n   - The authors claim that the dataset can be used for other machine learning tasks like data shift studying, anomaly detection, and uncertainty estimation, but they do not provide empirical evidence to support these claims.\n\n#### Reviewer Feedback:\n- **Reviewer 1** and **Reviewer 2** both highlight the attractiveness of the dataset but express concerns about the limited methodological novelty and the need for more advanced baseline models.\n- **Reviewer 3** emphasizes the lack of significant novelty in both the dataset and methods, suggesting that the paper might be more suitable for a journal rather than a top-tier conference like ICLR.\n\n### Final Decision\nWhile the RuDar dataset is a valuable contribution to the field of precipitation nowcasting, the paper lacks the methodological novelty and comprehensive evaluation that are typically expected in a top-tier conference like ICLR. The dataset is well-documented and publicly available, which is a significant strength, but the paper does not push the boundaries of existing research in terms of methods or evaluation.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n- **Novelty**: The paper introduces DeepSeaProbLog, an extension of DeepProbLog that supports both discrete and continuous random variables. This is a significant contribution because it addresses a major limitation of existing NPLP systems, which are restricted to discrete and finite probability distributions. The ability to handle continuous variables opens up new possibilities for modeling real-world problems.\n- **Significance**: The integration of continuous variables with neural networks and logic programming is a substantial step forward in the field of neural-symbolic AI. The use of weighted model integration (WMI) to handle mixed discrete-continuous domains is a novel and technically sound approach. The paper also provides a clear implementation and experimental evaluation, which demonstrates the practical utility of the proposed system.\n\n#### Empirical Evaluation\n- **Strengths**: The paper presents empirical evaluations on a neural-symbolic object detection task, a variational auto-encoder with a difference constraint in the latent space, and neural hybrid Bayesian networks. These experiments show that DeepSeaProbLog can handle complex tasks that involve both discrete and continuous variables.\n- **Weaknesses**: The empirical evaluation is somewhat limited. The paper focuses on toy applications, and there is a lack of comparison with other benchmarks and baselines, especially in the pure discrete setting. The MNIST subtraction task, while interesting, is a synthetic task and may not fully represent the challenges of real-world applications. The inclusion of more diverse and challenging benchmarks would strengthen the paper.\n\n#### Clarity and Quality\n- **Clarity**: The paper is generally well-written and provides a clear introduction to the problem and the proposed solution. However, some reviewers noted that the paper could benefit from more detailed explanations of the technical aspects, particularly the semantics of DeepSeaProbLog and the differentiation through WMI. The background section on logic programming is extensive, which may detract from the main contributions.\n- **Quality**: The technical content is sound, and the claims are well-supported. The paper provides a solid theoretical foundation and a clear implementation. However, the lack of detailed explanations in the main text and the relegation of some important details to the appendix are points of concern.\n\n#### Responses to Reviewer Comments\n- **Reviewer 2**: The suggestion to include more diverse benchmarks and applications is valid. The authors should consider using data from TensorFlow Probability or other relevant datasets to demonstrate the broader applicability of DeepSeaProbLog.\n- **Reviewer 3**: The reviewer's points about missing references to WMI solvers, the need for a comparison in the pure discrete setting, and the limitations of distributional assumptions are well-taken. The authors should address these points in the revised version.\n- **Reviewer 4**: The reviewer's comments on the performance and efficiency tradeoff, the potential for combining knowledge compilation with sampling, and the need for a more detailed comparison with DeepProbLog are valuable. The authors should provide more comprehensive empirical evaluations and discussions.\n- **Reviewer 5**: The reviewer's suggestion to add more applications and discuss the scalability of the implementation is important. The authors should consider including more real-world applications and providing scalability analysis.\n- **Reviewer 6**: The reviewer's concerns about the novelty and the handling of discrete random variables are valid. The authors should provide more details on how they handle discrete variables and clarify the key innovations in their approach.\n- **Reviewer 7**: The reviewer's comments on the incremental nature of the work, the need for more detailed explanations of the semantics, and the clarity of the paper are important. The authors should reorganize the paper to bring more of the novel contributions into the main text and provide more detailed explanations.\n\n### Final Decision\nGiven the significant technical contributions, the novel approach to handling mixed discrete-continuous domains, and the potential impact on the field of neural-symbolic AI, the paper should be accepted. However, the authors should address the reviewers' comments to improve the clarity, empirical evaluation, and overall quality of the paper.\n\nFinal Decision: Accept"
    }
  ]
}