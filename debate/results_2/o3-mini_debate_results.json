{
  "summary": {
    "model": "o3-mini",
    "total_papers": 100,
    "correct_predictions": 86,
    "accuracy": 86.0,
    "confusion_matrix": [
      [
        70,
        6
      ],
      [
        8,
        16
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 92.10526315789474,
        "precision": 89.74358974358975
      },
      "Accept": {
        "total": 24,
        "recall": 66.66666666666666,
        "precision": 72.72727272727273
      }
    }
  },
  "results": [
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Motivation: Although the idea of dynamically switching precision levels during training is interesting in principle—and pushing into fixed-point training is a promising direction—the reviewers highlighted that the core motivation behind the switching mechanism is unclear. The paper does not adequately explain why well-aligned gradients should trigger a switch in precision. This causes concerns about the fundamental justification of the approach.\n\n2. Algorithm Clarity and Reproducibility: Multiple reviewers noted that the presentation of the precision-switching policy is confusing. Key parts such as the precise role of the gradient diversity metric, the parameter “p” (and its related threshold “y”), and details regarding the quantisation scheme (e.g., scaling factors and difference between n and WL) are either under-specified or poorly explained. Without this clarity, independent reproduction or broader adoption of the method is compromised.\n\n3. Experimental Validation: Reviewers point out discrepancies in the experimental evaluation. On one hand, while the training time improvements are modest (1.28× speedup on average), there are cases (e.g., AlexNet and ResNet on ImageNet) where the classification accuracy is lower compared to full-precision FP32 baselines. Moreover, the experiments do not provide standard training or validation curves, and the benefit of dynamic precision switching over fixed switching points (or even over alternative mixed-precision schemes) appears marginal. \n\n4. Complexity versus Gain: Compared to previous work (e.g., Micikevicius et al. 2018), the proposed strategy introduces considerable complexity by involving multiple fixed-point precisions and a gradient diversity criterion, but the empirical gains are not clearly superior. In some cases, the additional complexity in the switching mechanism seems to conflict with the simple practical gains that are already possible with pre-defined switching schedules.\n\n5. Overall Presentation: There are several notation issues, typographical errors, and inconsistencies in the explanation, which detract from the overall quality of the presentation. For a top-tier conference, these aspects are crucial for a fair evaluation and clear understanding by the community.\n\nConsidering these points—especially the lack of clarity in algorithmic descriptions, the insufficient evidence that dynamic switching is critical, and the fact that sometimes the accuracy suffers—the paper does not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper is based on a concept—improving Monte Carlo tree search by coordinating episode generation—that has potential interest. However, the issues are profound. The reviewers point out that the manuscript is extremely difficult to read due to pervasive grammatical issues, undefined notation, and unclear exposition. The abstract itself is indecipherable, offering no clarity about the precise technical contributions or the improvements claimed relative to existing methods like Alpha Zero. Furthermore, the paper lacks references to prior work, which is essential for situating the contribution in the current literature. The overall impression is that the work is not yet mature enough to meet the standards of a top-tier conference. The paper appears more like a collection of notes rather than a rigorous academic treatment with properly defined methods, experiments, and theoretical results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper attacks an interesting and novel problem of singing voice generation without conditioning on scores or lyrics. While it introduces several setups (free, accompanied, and solo singer) and incorporates a variety of techniques (source separation, transcription models, GAN-based waveform synthesis), significant concerns persist.\n\nFirst, the evaluation framework is not sufficiently convincing. The proposed metrics (vocalness and average pitch) do not adequately capture the essential aspects of singing, and there is no strong justification that these metrics are reliable proxies for singing quality. Many reviewers noted that the metrics fail to assess key singing features (e.g., expressiveness, correct harmonic progression, phrasing). Moreover, in the absence of a comparison with a baseline (for instance, MIDI-based generation or extraction of scores followed by established singing synthesis methods), it is very difficult to position the results against related work. \n\nSecond, there is an overall lack of clarity regarding the training data. The paper gives insufficient details on the dataset, its genre distribution, and the separation between the training and validation/test splits. Questions remain, such as whether the data come from a homogeneous genre (like Jazz) or if the accompaniments are consistently representative. These ambiguities hinder a proper assessment of the model’s generalization capability.\n\nThird, while the authors introduce a novel task, the motivation behind the different pipeline settings is not fully elaborated upon, and little insight is provided regarding the relative merits of these settings. Critical architectural choices (e.g., adapting BEGAN techniques for sequence modeling, use of GRUs and dilated convolutions) are not thoroughly justified or ablated, leaving the reader uncertain about the superiority of the proposed approach.\n\nFinally, although the engineering effort (e.g., dataset preparation, model training) is commendable and code is open-sourced, the core contributions remain under-supported by rigorous experimental evidence and a careful evaluation. Multiple reviewers remarked that the paper “fails to justify the main idea and results” and “addresses too many problems at once” without adequate comparisons.\n\nGiven these issues—insufficient evaluation, lack of strong baseline comparisons, unclear data specifics, and unclear motivation for certain design choices—it is difficult to conclude that the work meets the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a weakly-supervised learning approach for trajectory segmentation in order to extract reusable sub-skills from human demonstrations. While the problem is interesting and relevant to the field of sensorimotor control and robotics, several critical issues remain unaddressed:\n\n1. Motivation vs. Results Mismatch: Although the paper is framed around learning reusable skills that can generalize to new tasks, the experiments only demonstrate per time-step labeling with weak trajectory-level supervision. There is insufficient evidence that the extracted segments can be effectively used to learn downstream policies or generalize to new task combinations, leading to a mismatch between the overarching motivation and the reported experiments.\n\n2. Limited Novelty: The approach relies heavily on existing ideas from multiple-instance learning (particularly the log-sum-exp pooling method), which has seen successful applications in other domains such as semantic segmentation. The incremental methodological contributions do not elevate the work to the level expected at a top-tier conference, especially without significant algorithmic innovations or thorough comparisons to state-of-the-art fully supervised baselines.\n\n3. Experimental Weaknesses: \n   - The reported classification accuracy (35-60%) appears underwhelming and raises concerns about the practical utility of the method. There is no clear comparison to fully-supervised oracle methods, which would help quantify the performance gap.\n   - The behavioral cloning and hierarchical control experiments yield modest success rates, but the experimental setup details are insufficient. The lack of clear comparisons to relevant baselines (e.g., Policy Sketches) further detracts from the impact of the experimental results.\n   - Important details such as dataset requirements, consistency of segmentation over time, the influence of Gaussian smoothing, and hyperparameter choices are not adequately described. This obscures the validity and reproducibility of the experiments.\n\n4. Evaluation Metrics and Clarity: The evaluation metrics used (accuracy alone) are insufficient to assess the quality of the segmentation, and additional metrics such as confusion matrices or precision/recall would be necessary given the possibility of trivial labeling strategies that might inflate accuracy. Reviewer concerns about noisy predictions and overfitting suggest that the current evaluation fails to provide a convincing demonstration of the method’s effectiveness.\n\nGiven these significant issues with the methodology, experimental validation, and overall presentation, the paper does not meet the rigorous standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\nStrengths:\n• The paper tackles an important challenge in extreme classification by proposing an adversarial approach for negative sampling that scales with log(C), which is potentially impactful given the typical computational bottlenecks in training with a huge number of classes.\n• The theoretical contribution – providing a mathematical analysis on reducing gradient variance and removing bias from non-uniform sampling – is interesting.\n• The paper is written in a mostly clear and didactic way that may be accessible even to the non-expert reader, as emphasized by Reviewer 1.\n• Experimental results on two large datasets demonstrate significant improvements in training time compared to several baselines.\n\nWeaknesses and Concerns:\n• There is a significant gap in situating the work among the rich literature on deliberately non-uniform or adaptive negative sampling. Both Reviewer 2 and Reviewer 3 point out that there are several recent works (e.g., the cited [1]–[4] and the Slice method, DiSMEC, and others) which propose methods that sample “hard negatives” or approximate the softmax distribution directly with logarithmic cost. The paper insufficiently discusses these works, and the proposed method is not compared against these stronger state-of-the-art baselines.\n• Reviewer 3 raises the important concern that the study does not include comparisons to standard softmax training (even on smaller datasets) or to other strong baselines (such as DiSMEC). Such a comparison is crucial to assess whether any accuracy loss is incurred by the approximation. It is concerning if a baseline method like DiSMEC provides a considerably better accuracy on Wiki500 (as noted) despite its computational cost.\n• The experimental evaluation is limited to only two datasets, missing an evaluation on additional extreme classification benchmarks (e.g., EURLex or Amazon3M) and without breakdown of performance (e.g., on tail labels) that could reveal the impact of the approximations on label imbalance.\n• In addition to the novelty in the method, some aspects (like the use of decision trees) might be seen as incremental if not compared rigorously with competitors.\n• There are also numerous minor issues (typos and figure/table inconsistencies) that detract from the overall polish, though these issues are secondary to the more fundamental concerns about baseline comparisons and experimental rigor.\n\nFinal Assessment:\nWhile the paper introduces an interesting idea, the shortcomings in experimental evaluation and relative comparisons to leading recent work prevent it from being competitive for a top-tier conference. Without more comprehensive comparisons to state-of-the-art baselines and richer experimental analysis (including multiple datasets and evaluations on performance on tail labels), it is difficult to assess the true benefits of the proposed method over existing approaches. A revision addressing these points and situating the work in its proper context is needed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis:\n\n1. Novelty and Prior Work Comparison:\n• The paper claims a novel scaling property of its LSTM meta-learner by having parameters increase linearly with the size of the learner’s parameters. However, reviewers note that prior works such as Ravi & Larochelle already share parameters across gradients to prevent such explosion, so the proposed method becomes merely an incremental variation. Reviewer 3 particularly points out that the proposed variant appears very similar to earlier Meta-Learner LSTM approaches, and the paper does not include a fair or direct comparison to these baselines.\n• Additionally, some statements about prior work (such as the LSTM requiring an enormous number of parameters) are inaccurate, undermining the authors’ motivation.\n\n2. Experimental Validation:\n• The experiments focus on few-shot learning benchmarks (Mini-ImageNet and Omniglot), but they lack key comparisons. Reviewer 1 criticizes the absence of results from important baselines like Sun et al., as well as insufficient experimental validation on setups that would best highlight the claimed advantages.\n• Reviewer 3 emphasizes that the comparisons for MAML are not entirely appropriate, and a more closely aligned version (for instance, MAML with per hidden-unit learning rates or even MAML++ benchmarks on convolutional setups) would have been necessary.\n• The method’s applicability is also limited by its requirement for fully connected networks. Most few-shot learning approaches are based on convolutional architectures, making the method less general.\n\n3. Technical and Presentation Issues:\n• Several reviewers point out that the method’s technical novelty is limited, the writing quality is poor (with grammatical and reference format issues), and the exposition could be improved considerably.\n• There are also confusions in formulation (e.g., the discussion surrounding Equation 1 and subsequent equations) that detract from clarity and correctness.\n\n4. Overall Importance for a Top-Tier Conference:\n• While the problem of scaling meta-learning optimizers is interesting and important, the proposed solution does not clearly outperform or offer a distinctly new approach compared to existing ones. At a top-tier conference, substantial theoretical or practical improvements along with rigorous, fair empirical evaluations are expected.\n• The reviewers' concerns – notably the limited novelty, the incomplete and somewhat misleading experimental study, and the significant writing and presentation issues – make it difficult to justify acceptance.\n\nGiven these major deficiencies in novelty, experimental rigor, and clarity, I conclude that the paper does not meet the standards expected for a top-tier conference publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper aims to improve multi-task learning by modeling task dependencies at multiple levels using attention mechanisms, but several serious issues remain that prevent it from meeting the standards of a top-tier conference. Here are the primary concerns:\n\n1. Clarity and Presentation:  \n • The manuscript suffers from numerous grammatical errors and confusing notation. Several components—such as the distinction between \"general task dependency\" and \"data dependency\"—are not clearly delineated. The use of terms like “multi-level” and “multi-view” seems inconsistent, leading to a muddled presentation.  \n • Critical components, such as the “Transfer Block” and the “general task dependency” matrix (akin to an affinity matrix), are described only superficially without sufficient detail, making the technical contribution hard to judge.\n\n2. Novelty and Relation to Prior Work:  \n • Several reviewers noted that the idea of modeling task dependencies—and specifically using attention mechanisms—is not entirely new, with works like Taskonomy addressing similar challenges. The paper does not provide a strong discussion of related work or demonstrate convincingly what sets it apart from these prior methods.  \n • The claimed assumption that most state-of-the-art multi-task learning methods assume tight correlations among tasks is debatable, and the paper does not convincingly justify its hierarchical dependency modelling from the standpoint of novelty.\n\n3. Experimental Evaluation:  \n • The empirical evaluation suffers from significant omissions. Reviewers pointed out the need to compare the proposed method with an equivalent version that does not use the attention mechanism (e.g., setting the task-dependency matrix D to all ones) to isolate the value of the attention component.\n • There is also insufficient information on dataset splits and potential data leakage issues, as well as an unclear explanation about whether the evaluation procedure might unintentionally lead to overlapping samples between training and test sets across different tasks.\n • The relative performance improvements appear slight compared to state-of-the-art, meaning that a more thorough experimental study is needed to substantiate the claims of significant performance gains.\n\n4. Technical Specificity:  \n • The paper is missing detailed descriptions of fundamental aspects of the proposed model (such as the mappings used in key equations, and the exact implementation of the “Point-wise Mutual Attention Mechanism”). Such details are critical for reproducibility and evaluating the technical correctness of the approach.\n • Several notational issues (like the reuse of index symbols and undefined notations) add to the confusion and hinder the reader’s ability to fully grasp the method.\n\nOverall, the paper currently does not provide a sufficiently clear and novel contribution in either its theoretical grounding or experimental evaluation. The lack of critical comparisons with closely related works, in addition to presentation issues and clarity concerns, outweigh the potential benefits of the proposed approach. These issues must be addressed significantly for the work to be accepted for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed assessment and reasoning:\n\n1. Contribution and Novelty:\n The paper addresses an important open problem in the theoretical understanding of overparametrized neural networks by analyzing how different activation functions impact the training dynamics via the spectrum (especially the minimum eigenvalue) of the associated Gram matrix. In particular, contrasting the behavior of non-smooth activations (such as ReLU, SELU, ELU) with smooth ones (like tanh, swish, polynomial) is both timely and insightful. The work builds on and extends the NTK/lazy training theory, adding significant depth by quantifying eigenvalue bounds where previous works provided non-quantitative guarantees.\n\n2. Technical Soundness:\n The approach is technically sound with comprehensive proofs (with details relegated to the appendix) and non-trivial extensions pertaining to deeper networks. The evidence from theory is also supported by synthetic experiments and partial real-data experiments (e.g., on CIFAR10), which further substantiates the theoretical claims. While some gaps exist between theory and practice in more complicated settings (as pointed out by Reviewer 1), the paper’s honest discussion of these limitations is a strength.\n\n3. Clarity and Presentation:\n Reviewer comments indicate that while the main text is clear, the extended appendix is somewhat bulky and could be better organized. Although the length of the appendix and the organization of extended material (and some ambiguous references) pose challenges for a general audience, these are presentation issues rather than fundamental technical flaws. Minor adjustments (e.g., clarifying the DZXP setting, cleaning up references to theorems in the appendix) would help increase clarity.\n\n4. Impact and Relevance for a Top-Tier Conference:\n Given the importance of understanding dynamics in deep learning theory, and the concrete contributions regarding activation functions—a topic of interest to both theoreticians and practitioners—the paper is highly relevant. The theoretical insights can potentially guide future research and practical design choices in neural network architectures.\n\n5. Comparison to Reviewers’ Comments:\n Reviewer 2 favors acceptance, noting that the detailed theoretical investigation and corresponding empirical validations make the paper a strong candidate. Reviewer 1, while noting organizational issues (especially regarding the lengthy appendix), still appreciates the contribution and the thoroughness of the proofs and experiments.\n\nConclusion:\n Despite minor issues in exposition and presentation, the paper’s theoretical contributions are solid, relevant, and timely. The identified weaknesses can be addressed with minor revisions regarding clarity and organization. Overall, the paper meets the high standards expected at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper aims to introduce a generalized probability kernel (GPK) on discrete distributions and apply it to two‐sample testing. While the underlying idea—to bridge aspects of MMD and kernel Stein discrepancies—has potential interest, the paper falls significantly short of the standards expected for a top-tier conference based on several critical aspects:\n\n1. Theoretical Incompleteness and Inaccuracies:\n   • The definitions and notations (especially in Definitions 1, 2, 5, and 6) are inconsistent and imprecise. There is substantial confusion regarding what exactly is being defined as a kernel versus a distance. The mixture of notations like “K” and “k” creates additional ambiguity.\n   • Key proofs, notably the proof regarding the removal of a supremum in Theorem 5, are missing. Essential details on the asymptotic behavior and theoretical guarantees are either trivialized or absent.\n   • Several mathematical constructs (e.g., the Stein operator in a discrete setting) and their connections to the proposed probability kernel are not rigorously justified. This is particularly concerning since the setting is critical yet lacks clear motivation or illustrative application examples.\n\n2. Experimental Deficiencies:\n   • The paper completely lacks any experimental evaluation. There are no simulation studies or empirical comparisons to validate the performance, variation, or practical feasibility of the proposed estimators.\n   • The proposed application to the two-sample testing framework is inadequately supported—crucial analysis regarding when and how to utilize these estimators in practice is missing.\n\n3. Presentation and Writing Quality:\n   • The paper suffers from numerous grammatical errors, typos, and unclear writing. This not only obscures the presentation of the novel contributions but also aggravates the issues related to the inconsistency in notation.\n   • Several parts of the paper (e.g., connections to Bernstein polynomials or detailed derivations) are referenced as if they will be included but are either trivial or completely skipped, leaving the reader with many unanswered questions.\n\n4. Reviewer Consensus:\n   • Multiple reviewers have raised these issues independently, highlighting the theoretical ambiguities, incomplete derivations, and the absence of experiments.\n   • The critiques uniformly point out that the work, while interesting in intent, requires a major overhaul in theory, experimentation, and overall presentation to meet the standards of rigorous academic discourse for a top-tier conference.\n\nConclusion:\nGiven the substantial and multiple deficiencies in both theoretical development and experimental validation, as well as the presentation issues noted by all reviewers, the paper currently does not merit acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\n\nThe paper tackles an important and practical problem in distributed machine learning—distributed mean estimation—with the novel twist of ensuring that the error bounds depend only on the input variance rather than the input norm. This is highly relevant in applications where the data vectors may be large in norm but have low dispersion. The main contributions involve developing a lattice‐based quantization method, establishing matching upper and lower bounds, and providing a practical adaptation using cubic lattices (with only a logarithmic loss). The work is theoretically solid, and the connection to lattice theory is both innovative and nontrivial, offering a new perspective on an established problem.\n\nAcross the reviews, the strengths are frequently highlighted:\n•\tThe problem formulation is natural and addresses a gap in previous work that assumed small-norm inputs.\n•\tThe theoretical contributions (upper/lower bounds) are significant and appear tight.\n•\tThe algorithmic ideas, especially the lattice-based approach and its practical variant using cubic lattices, are well motivated.\n•\tExperimental results, though somewhat limited in scope, corroborate the practical gains.\n\nThe reviewers offer some suggestions for improvement:\n•\tReviewer 1 and Reviewer 2 request additional discussion on alternative, simpler approaches (such as iterative centering or a random rotation approach) and a comparison with these methods. Such discussion would strengthen the paper by clarifying the advantages of the proposed method over seemingly straightforward alternatives.\n•\tReviewer 3 notes some minor technical issues and clarifications (e.g., details in Lemma 20, cost of leader election, and consistency in experimental plots) that can be addressed with further revision.\n•\tReviewer 4 raises a concern over constant factors in practice, but considers this a minor issue given the strong theoretical contributions.\n\nAlthough a few minor concerns (such as the need for additional comparisons, clearer exposition on the implementation details, and more comprehensive experiments) are raised, none of these comments are fundamental flaws. Instead, they indicate potential areas of improvement that can be addressed in a revision.\n\nOverall, the paper makes a solid contribution to the literature on distributed optimization with innovative techniques and strong theoretical guarantees. The relatively minor issues can be remedied, and they do not detract from the paper’s substantial merit.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper advances an interesting idea: using binary masks over pre‐trained transformer weights and activations to extract disentangled representations corresponding to different linguistic factors. This is an appealing alternative to training models from scratch for disentanglement, and the approach is simple in concept and has a potential computational benefit by not requiring full fine-tuning of the underlying model. Several reviewers acknowledge the novelty of extracting subnetworks responsible for specific attributes and the promise of improved general robustness under spurious correlations.\n\nHowever, multiple concerns were raised. A recurring theme is that the experiments are narrowly focused. For example, the evaluation on movie reviews (with only two genres) leaves the generality of the approach in doubt. While the paper claims performance that is as good as or better than prior methods (including those based on variational autoencoders and adversarial training), evidence for this is either not clearly presented or not thoroughly compared against deeper baselines. There are also questions about the computational benefits since the learned masks increase parameter counts and may require multiple forward passes, calling into question any speedup claims. Reviewers also pointed out the need for further clarity regarding what parts of the network are masked, how the method behaves if masks were trained on top of a fine-tuned model, and a more complete investigation of the learned mask characteristics.\n\nMoreover, several reviewers noted that important comparisons and error analyses are missing or under-discussed. For example, quantitative evaluations on standard benchmarks and further experiments on additional domains would help establish the impact of the method. There is also a question as to whether the proposed approach has clear advantages over finetuning, especially given that finetuned BERT-based classifiers often perform quite well. A deeper discussion of the tradeoffs (sparsity versus performance, masking weights versus activations) is needed.\n\nIn summary, while the conceptual novelty is appreciated, significant experimental and exposition shortcomings remain. The limited experimental scope, lack of comprehensive benchmark comparisons, and insufficient discussion of potential computational and methodological tradeoffs suggest that the work is not yet at the level required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n\nThe paper attempts an interesting idea—integrating linguistic cues throughout the entire visual pipeline rather than only at a high-level attention stage—and shows some quantitative improvements on several referring-expression image segmentation benchmarks. However, several critical issues prevent its acceptance at a top-tier conference:\n\n1. Conceptual Motivation and Analysis:\n • Both Reviewer 1 and Reviewer 4 highlight that the paper’s conceptual motivation is weak. Although the authors claim cognitive plausibility, the paper does not provide a sufficiently clear link between linguistic inputs at lower layers and expected improvements in low-level visual processing.\n • Reviewer 4 in particular notes a lack of satisfactory error analysis or interpretability regarding how language modulates low-level features. This theoretical insight is crucial for establishing a conceptual contribution beyond mere numerical improvements.\n\n2. Empirical Evaluation Concerns:\n • Reviewer 2 expresses serious concerns about empirical results on key metrics; in some cases, the full model does not match existing works on the test set, suggesting possible over-tuning of hyperparameters.\n • Inconsistencies in reported results (e.g., differences in IoU numbers between tables) and the absence of statistical significance tests undermine confidence in the generality of the improvements.\n • Reviewer 1’s ablation study clarity is also lacking, and questions about how different fusion strategies (and number of layers) influence the results remain largely open.\n\n3. Novelty and Clarity of Method:\n • Several reviewers (R1 and R4) pointed out that the innovation relative to prior work, such as Step-ConvRNN, appears incremental. The paper does not convincingly argue why and how integrating language into the bottom-up pathway yields substantial gains compared to established top-down approaches.\n • Important methodological choices (such as the splitting of language representation without clear attachment to particular feature layers, or the rationale for stopping at three layers) are insufficiently justified.\n\n4. Rebuttal and Post-Rebuttal Issues:\n • The post-rebuttal discussion, especially from Reviewer 2 and Reviewer 4, indicates that the authors’ responses did not resolve the core concerns regarding novelty, empirical consistency, and analysis of the language–vision interaction.\n • The absence of a concrete analysis to elucidate the model’s behavior means that the paper’s claims remain speculative rather than substantiated by rigorous experiments.\n\nWhile one reviewer (R3) does commend the paper for its efficiency and improvement on some benchmarks, in a top-tier conference setting, the combination of insufficient conceptual motivation, unresolved empirical inconsistencies, and lack of clear interpretability of the proposed method is too significant to be overlooked. The potential incremental nature of the contribution, without the necessary in-depth analysis and clarity, leads to the conclusion that the work does not yet meet the high standards required for top-tier acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and Motivation:\n • The paper tackles the challenging problem of coordinating multiple agents (or end-effectors) by first training individual diverse skills and then learning a meta-policy to coordinate them. This kind of hierarchical decomposition is well‐motivated by human strategies and is particularly relevant for intricate manipulation tasks.\n • The choice of using a diversity objective (in the flavor of DIAYN) to augment skill learning is an interesting angle to enable controllable variability in behaviors. Empirical results on challenging tasks (e.g., multi-arm manipulation and multi-agent locomotion such as two Ants pushing a block) underscore the practical value of the approach.\n\n2. Methodology:\n • The modular framework that first pre-trains diversified sub-skills and then coordinates them is conceptually clear. Both individual skill learning and meta-level coordination are standard ingredients, but their combination in multi-agent manipulation is a valuable contribution.\n • Concerns were raised by reviewers regarding critical design choices (for instance, the fixed duration T_low and its interpretation as temporally extended versus primitive actions). While these design choices could benefit from clearer justification or additional ablation studies, they do not invalidate the framework.\n\n3. Experimental Evaluation:\n • The experimental results are promising and show that the proposed method outperforms a decentralized baseline with a shared critic.\n • However, reviewers noted high variance in performance and raised questions about the consistency of the reported metrics. This indicates that while the empirical results are encouraging, more rigorous reporting and additional seeds/experiments would strengthen the paper.\n • Moreover, one reviewer suggested that comparisons to alternative methods (such as using DIAYN in a non-modular way) would further validate the benefit of the proposed decomposition.\n\n4. Clarity and Presentation:\n • There are concerns about the clarity of notation and insufficient details in diagrams, making some elements of the approach hard to follow.\n • In addition, several technical issues—like the choice of continuous latent conditioning and the implications of a fixed T_low—need more thorough discussion.\n • Nonetheless, the overall structure of the paper and its motivation remain accessible, and the availability of videos and code is a strong practical plus.\n\n5. Overall Assessment:\n • The work addresses an important problem in multi-agent learning with an approach that, while building on known ideas (e.g., DIAYN), applies them in a novel coordination setting.\n • The criticisms raised by the reviewers are largely about clarifying and justifying design decisions and improving empirical consistency rather than fundamental flaws in the research.\n • Given that the method shows promise for challenging tasks and the novelty is sufficient for a top-tier venue—provided that the authors adequately address the pointed issues—I lean toward acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning:\n\n1. Novelty and Technical Contribution:\n• The paper proposes STS as a variant of MCTS that expands multi-step trajectories instead of a single node per simulation. However, reviewers point out that similar multi-node expansions have been previously explored—for example, in works like Soemers et al. (2016) and even earlier in Coulom’s work—so the technical novelty is not clearly established. Without a strong argument on why these modifications are fundamentally different (or improved) compared to known approaches such as using mixed rollout/value estimates (e.g., as in AlphaGo), it is hard to justify a top-tier conference publication.\n\n2. Clarity and Presentation:\n• Multiple reviewers noted issues with the presentation. Several algorithms (e.g., Algorithm 3, 4, and 5) are unclear or contain missing definitions (like CALCULATE_TARGET), and there are questionable design choices (e.g., the tree policy that appears to be purely greedy rather than exploring/exploiting). These ambiguities undermine the clarity of the contribution.\n• In addition to presentation and writing issues, important details (e.g., hyperparameters for the comparison methods, details on value function pre-training, and the setup of experiments) are not sufficiently explained, making it challenging to reproduce or fully assess the method.\n\n3. Empirical Evaluation:\n• Although the experimental results show that STS can sometimes outperform a baseline MCTS implementation in certain domains, questions remain about the settings and baselines. Reviewers expressed concern that the benefits might simply be due to biases in the evaluated value function, and that proper ablations (e.g., disentangling the effects of multi-step expansion from the evaluation method) are missing.\n• Several reviewers raised the point that comparisons with stronger versions of MCTS (with tuned exploration parameters or many more simulations) were not provided, leaving doubts on whether STS really offers a significant advantage in realistic settings.\n\n4. Overall Impact:\n• While the idea is simple, the contribution is incremental and the paper suffers from a lack of clarity both in theoretical presentation and empirical details.\n• The reviewers’ concerns regarding the novelty, insufficient discussion of related prior work, and the insufficient experimental ablation studies make it difficult to conclude that the paper represents a substantial advance required for acceptance at a top-tier conference.\n\nBased on the above points and the critical feedback from multiple reviewers, I conclude that despite some interesting ideas and thorough experiments, the paper as submitted does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper addresses an interesting problem—improving certified robustness via training with novel regularizers aimed at tightening convex relaxations. However, several major concerns have emerged from the reviews. Both Reviewer 1 and Reviewer 2 point out significant issues in the clarity and consistency of the theoretical exposition. Specifically, the paper alternates confusingly between the exact nonconvex problem and its convex relaxation, raising doubts about key theoretical claims. There is also criticism that the analysis becomes convoluted and may only hold under trivial conditions (for example, in the absence of ambiguous ReLUs). In addition, some of the reported experimental gains appear to be due largely to the use of existing techniques such as CROWN-IBP rather than the novel contributions of the proposed regularizers.\n\nFurthermore, the experimental evaluation is also weak for a top-tier conference paper. Reviewer 3 notes that using only MNIST as the dataset does not adequately support the claims of improved robustness, and the lack of comparisons to additional strong baselines (e.g., reporting numbers that include previous work by Gowal et al.) further undermines the empirical contributions.\n\nWhile the idea behind integrating aspects of the convex relaxation process into training is promising, the current manuscript has major issues in theoretical clarity and experimental validation. As a result, the significant confusion over key definitions and the limited experimental scope signal that the paper is not yet ready for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces an “encoder-assisted search” mechanism aimed at improving Defense-GAN by reducing the computational cost during adversarial purification. While the idea of leveraging an autoencoder for better initialization is interesting, several major concerns have been raised by the reviewers.\n\nFirst, the contribution is perceived as incremental. The work essentially refines an existing method (Defense-GAN) by minimizing the computational expense via a single starting point for the latent space search. However, this modification comes at the cost of accuracy and is not compared against more robust and current adversarial defense methods such as PGD adversarial training or TRADES, which are standard baselines at the top-tier conference level.\n\nSecond, the experimental evaluation is limited to MNIST and Fashion-MNIST. These datasets are simplistic, and the method’s performance on more challenging datasets (e.g., CIFAR-10) is critical for demonstrating practical relevance and robustness. Moreover, aspects like the reliance on the autoencoder for detection and the unclear behavior of the latent space (e.g., ensuring the latent distribution matches the prior) are not sufficiently addressed.\n\nThird, several reviewers pointed out issues related to clarity and presentation. The paper has numerous language issues (typos, unclear phrasing) and even exceeds the page limit. Additionally, important details about the model training, inference process, and practical implementation (e.g., the number of gradient descent steps in the search process) are not clearly explained, making it hard to reproduce or understand the method fully.\n\nFinally, while the computational benefit is a noted strength, the trade-off in accuracy and lack of compelling evidence against stronger baselines detracts from the overall contribution.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Scope: The paper proposes a method (MALCOM) for out‐of‐distribution (OOD) detection that does not require retraining or using OOD samples for validation. The idea of leveraging normalized compression distance on the feature maps extracted from convolutional neural networks is novel and interesting. However, as noted by the reviewers, the paper restricts itself to a highly particular setting and it is not sufficiently justified that such stringent constraints (no retraining, no OOD validation) are of central importance in real-world applications. Reviewer 2, in particular, requested either a broader comparison or a stronger argument why one cannot use alternative means (e.g., using adversarial samples for validating parameters).\n\n2. Comparison Baselines and Experimental Validation: Reviewer 1 remains concerned that the experimental comparison is not set up fairly. In particular, the baseline method (a Mahalanobis detector) was not compared in its most effective incarnation – that is, if adversarial validation is used, a technique which the baseline paper already endorses. Moreover, there is a discrepancy in the reported performance figures between the original paper and the authors’ replication. The reviewers suggested that the authors provide both sets of numbers to clarify the gap rather than claim performance improvement only after combining several components. Also, some of the new experimental results were relegated to the appendix, which limits how convincing the evaluation is.\n\n3. Methodological Concerns: Although the method is clever, there is a sense (as expressed by Reviewer 3) that its effectiveness may to some extent depend on “minor tweaks” rather than on the core novel idea. This raises questions on the fundamental impact of the proposed distance metric compared to simpler or better‐established alternatives.\n\n4. Presentation and Writing: Several reviewers mentioned that the manuscript would benefit from additional proofreading and reorganisation. For example, Reviewer 2 noted that even the abstract’s first sentence suffers from some clarity issues. There is also a hint that the literature review could be enriched by citing other related earlier works that use compressive principles in image classification.\n\nIn summary, while the paper presents a novel idea and shows a promising direction for OOD detection under strict constraints, significant issues remain. The empirical comparisons are not fully convincing, the baseline setups seem unfair, and the overall justification of the restricted problem setting is not clearly articulated. Given these weaknesses, the paper does not yet meet the expected standards for a top‐tier conference and requires major revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Conceptual Contribution:\n • The paper presents an interesting idea linking cortico‐cerebellar interactions to decoupled neural interfaces, suggesting that the cerebellum implements predictive synthetic gradients analogous to those used in deep learning. This is indeed a creative attempt to bridge neuroscience and machine learning concepts.\n • However, while the conceptual link is intriguing, both Reviewer 2 and Reviewer 3 note that the work seems to be “juxtaposing” well‐known ideas rather than establishing new mechanistic insights into either the cerebellum or DNI. In other words, the paper does not convincingly provide new, testable insights or a novel computational benefit that goes beyond what is already known in the separate literatures.\n\n2. Empirical and Experimental Evidence:\n • The experimental results span a variety of tasks (from sensorimotor to caption generation) and demonstrate that adding a cerebellar-inspired component can improve learning and even mimic some deficits seen in cerebellar patients.\n • Reviewer 1 appreciates these aspects but suggests that clarifications are needed—especially with respect to the interplay between feedforward and recurrent setups as well as a baseline comparison to the original DNI model. In essence, the paper does not clearly isolate the contribution of its CC-DNI over existing synthetic gradient methods.\n • Reviewer 3 criticizes the lack of comparison with actual experimental findings from neuroscience. They suggest that the predictions made (for instance, the behavior of gradients in a well-learned network or lesion effects) are merely stated without further grounding in existing data. This theoretical gap limits the paper’s impact in a top-tier conference where clear testable predictions and rigorous empirical validations are expected.\n\n3. Clarity and Presentation:\n • There are several points of confusion noted by the reviewers: for example, the mixture of feedforward versus recurrent setups and mismatches in figure labels and references.\n • The paper would benefit from a clearer delineation of the precise architectural differences between the originally proposed DNI and the new CC-DNI models. This lack of clarity can mislead readers and weakens the argument that the cerebellar approach offers a substantive improvement.\n\n4. Biological Plausibility:\n • Reviewer 3 raises critical questions regarding the biological plausibility of the proposed mechanism (for instance, how synthetic gradients are computed and transmitted in the brain, particularly given the disparity in neuron numbers between the cerebellum and cortex). \n • The paper acknowledges such issues as “outside the scope,” yet addressing them, at least at a high level or via proposed experiments, would be critical for convincing neuroscientists of the viability of the hypothesis.\n\n5. Overall Assessment:\n • The central idea is creative and can potentially open new avenues of dialogue between deep learning and neuroscience. However, the execution falls short by not sufficiently distinguishing its contribution from prior work, by not making strong connections with experimental data, and by leaving important details unresolved.\n • In a top-tier conference, where the expectation is for a high-impact contribution supported by rigorous theoretical insight and empirical validation, these gaps are serious. The paper requires more work both in terms of clarifying technical details and in substantiating its claims with stronger empirical or neurobiological data.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Related Work:  \nReviewer 3 points out that the paper is similar to previous studies in predicting genomic contacts and that several related works are not properly cited. The lack of significant methodological novelty is a major concern for a top-tier conference. Additionally, the motivation for using a bidirectional LSTM – especially the connection drawn to the double-stranded nature of DNA – is not convincingly articulated, further weakening the contribution.\n\n2. Clarity and Methodological Details:  \nThere is ambiguity in several key aspects of the paper. For instance, the feature representation in section 3.1 is not clearly explained (binary vs. continuous values, treatment of different chromosome segments, and what “20kb” signifies in the context). This leaves a gap in understanding exactly what inputs the models are receiving, a critical point for computational reproducibility and interpretability.  \nMoreover, several details related to experimental design, such as which hyperparameters were tuned and why the center LSTM hidden state was used instead of a concatenation of terminal states, remain insufficiently justified.\n\n3. Experimental Evaluation and Fair Comparisons:  \nThe experimental setup raises concerns about the fairness of the performance comparisons. In particular, the weighted mean squared error (wMSE) loss design seems to have been exclusively employed with the LSTM, while it is not clear how or if it was adopted with the baseline models. This calls into question whether the performance gain is attributable to the architecture (LSTM) or simply to the novel loss metric. Reviewer 1 and Reviewer 3 both suggest that a fairer and more comprehensive evaluation should be performed by using the same loss function across models and by introducing additional evaluation metrics (e.g., standard MSE, R², or even mean squared logarithmic error).  \n\n4. Generalizability:  \nReviewer 1 raises an important point regarding the generalizability of both the loss function (e.g., the somewhat arbitrary choice of “11”) and of the model to different types of epigenetic datasets beyond ChIP-seq (such as ATAC-seq). For a paper aiming at a top-tier conference, evidence that the proposed approach works robustly across various datasets is essential.\n\n5. Presentation and Writing:  \nSeveral minor issues have been noted – from equation clarity and citation discrepancies to suboptimal figure labeling and sentence structure. While these issues might be fixable with a revision, they detract from the overall readability and can obscure important methodological points.\n\nOverall Assessment:  \nThe paper presents an application of recurrent neural networks (RNNs) to the prediction of DNA folding patterns in Drosophila and shows promising empirical results. However, due to the lack of clear methodological novelty, insufficient experimental rigor (particularly regarding loss function usage and fair baseline comparison), incomplete methodological descriptions, and concerns regarding the generalizability of the approach, the paper in its current state does not meet the high standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by the final decision.\n\n1. Experimental Evaluation and Datasets:\n• Several reviewers pointed out that the experimental evaluation employs very small graphs (with 9 or 20 nodes) and classical graph benchmarks. Since the goal is to propose a scalable and practical graph generator, the evaluation on such limited instances does not strongly support the claims of scalability and improved performance.\n• Reviewer 1 noted that because the datasets are intrinsically biased by repeated isomorphic graphs, the results may be misleading. The concern is that generating all small non-isomorphic graphs can be done easily by brute force. A convincing experiment should include medium-to-large graphs.\n• Reviewer 4 further criticized that the comparisons do not include key baselines (e.g., NetGAN, TagGen, or even simpler parametric methods) known in literature. The general lack of compelling comparisons diminishes the impact of the experimental results.\n\n2. Theoretical and Methodological Concerns:\n• Reviewer 2 and Reviewer 5 raised questions about the theoretical contributions: several of the theorems and propositions are either trivial in nature or not well connected to the proposed method. For example, Corollary 1’s existence statement seems uninformative, and Proposition 1 is said to require stronger assumptions.\n• There is also ambiguity in key components, such as the introduction and role of the parameter set φ, around collision avoidance, and modeling dependencies, particularly edge dependencies. Some design choices appear ad-hoc and are not sufficiently justified theoretically or empirically.\n• Reviewer 2 noted the absence of discussion about mode collapse and generation diversity, a critical issue for GAN models. Without clear discussion or evidence of avoiding such pitfalls, the claims of novelty and effectiveness remain questionable.\n\n3. Comparison with State-of-the-Art:\n• Several concerns revolve around whether GG-GAN truly improves upon existing methods. Reviewer 4 argued that many aspects (isomorphism handling, scalability, and expressive power) have been partly addressed by prior work, and the paper does not clearly delineate what is novel and why the proposed framework is preferable.\n• There are remarks in the reviews suggesting that the improvements (if any) are marginal compared to well-established baselines such as graphRNN and other GAN-based approaches.\n• The lack of important comparisons with recent methods such as NetGAN and transformer-based graph generators further weakens the paper’s claims.\n\n4. Overall Clarity and Contribution:\n• While some reviewers (e.g., Reviewer 3) appreciated the clarity of the writing and organization, substantial concerns remain regarding the clarity and rigor of some theoretical contributions and experimental validations.\n• In multiple reviews, it was mentioned that the contributions are overclaimed. The methodological innovations, although potentially interesting, are not sufficiently validated on challenging datasets nor compared extensively with potential state-of-the-art competitors.\n• The paper would require substantial revisions: a more comprehensive experimental evaluation with larger graphs and stronger baselines, clear justification of all theoretical contributions, and addressing ambiguities in the modeling choices.\n\nGiven these issues – especially the limitations in experimental setup, unclear theoretical novelty, and insufficient comparative analysis – the paper does not meet the standards expected for a top-tier conference.\n\nFinal Decision: Reject."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a dynamic task weighting approach, αVIL, that leverages task-specific parameter updates to estimate auxiliary task importance for multitask learning. While the idea is intuitively appealing and the paper is written in a clear and accessible manner, several significant concerns emerged during the review process:\n\n1. Theoretical Justification and Novelty:\n • All reviewers highlighted a lack of strong theoretical grounding or formal analysis. There is insufficient discussion on why the specific design choices (e.g., the separation of “alpha” from task-specific weights and operations such as subtracting one from all weights) are reasonable or necessary.\n • The proposed approach bears similarities to existing dynamic task weighting and meta-learning methods (e.g., approaches akin to MAML, DIW, or previously proposed adaptive weighting schemes), calling into question the true novelty of the approach.\n\n2. Experimental Evaluation:\n • The experiments on Multi-MNIST and NLU tasks show only marginal or statistically insignificant improvements compared to baselines. The performance gains are often within one standard deviation of the baselines, making the empirical benefits questionable.\n • There are concerns about the choice of experimental setups (e.g., using only one auxiliary task for the vision domain when the aim is to leverage multiple auxiliary tasks) and an insufficient range of baseline comparisons.\n • Reviewers asked for additional ablation studies, more detailed analysis of task weight evolution, and clearer statistical reporting, particularly for the NLU tasks. The current experimental evidence does not robustly justify the claimed benefits.\n\n3. Overall Impact and Presentation:\n • While the approach is presented clearly, the added value over existing methods is not convincingly demonstrated. The ad hoc elements of the algorithm, without clear theoretical or empirical advantages, suggest that the method requires further analysis, experiments, and discussion.\n • Moreover, several practical details (e.g., standard deviation reporting, discussion on why learned weights make sense) are missing or insufficiently addressed.\n\nGiven these major concerns—in particular the lack of strong theoretical justification, mixed empirical results with limited improvements, and some design choices that seem ad hoc—the paper does not meet the standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my analysis:\n\n1. Contribution and novelty:\n • The paper presents a variant of narrow floating-point representations that pushes the use of shared exponent biases across blocks (Block Minifloat or BM). While shared exponent ideas have been explored before (e.g., in block-floating point work and even in FP8 formats), the novelty here comes from pushing the exponent field down (e.g., 4-bit exponents) to enable a more efficient hardware implementation – in particular, a more compact Kulisch accumulator. This is an important contribution for systems where area and power are at a premium.\n • Reviewers acknowledge that many ideas (the shared exponent ideas) are not entirely new, but the interplay with hardware efficiency in BM (especially the use of reduced exponent bits to match the characteristics of a Kulisch accumulator) represents a timely and useful advance. Reviewer 2 noted that while the idea is a “fairly simple extension” of earlier work, the hardware implementation is where the paper shines.\n\n2. Evaluation of experimental results and methodology:\n • The paper provides both software-level experiments (training ResNet on ImageNet, other architectures on CIFAR, etc.) and hardware evaluations. The synthesized area and power numbers demonstrate substantial benefits compared to FP8. Even though one reviewer raised concerns regarding the use of different BM formats on the forward and backward passes (which might incur overhead), the authors clarified that the area results incorporate those design choices.\n • The experimental evaluation is extensive enough to cover different models and datasets. Although Reviewer 1 suggested that a larger network (e.g., ResNet50) might further strengthen the work, the current choice of benchmarks is acceptable for a top-tier conference submission.\n • Some reviewers raised issues with clarity regarding the details of the hardware (handling of denormals, details on the accumulator behavior, and explanation of Equation 6); however, these issues seem to be primarily about presentation and details rather than a fatal flaw in the technical contribution.\n\n3. Addressing reviewer concerns:\n • Reviewer 1’s concerns about explaining why “exponents up to 4 bits offer distinct advantages” for the Kulisch accumulator are valid, but these are questions that can be addressed in an improved exposition or a more detailed appendix. They do not undermine the overall contribution.\n • Reviewer 2, after discussion, raised his score and acknowledged that the hardware evaluation (including supporting multiple formats in the same circuit) is sound.\n • Reviewer 3 noted that more detailed explanations (especially of the derivation of key equations and communication of design choices) would improve the paper. These points seem addressable in a revision and do not lead to the paper being unpublishable.\n\n4. Overall decision:\n • All reviewers recognized the paper’s strengths in delivering a clear hardware implementation, extensive experiments, and a design that may offer significant improvements in efficiency and performance for deep neural network training.\n • The concerns raised are largely related to clarity and detail rather than a fundamental flaw in the idea or its evaluation.\n • Considering that the work is backed by both theoretical and practical hardware evaluations, and the potential impact of lower precision arithmetic with efficient hardware, the paper meets the standards for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces a practical method that leverages a multi-input multi-output (MIMO) network architecture to “free” ensemble benefits by having multiple subnetworks embedded within a single network. The core contribution is that using this method, one can achieve a robust ensemble performance (in terms of accuracy, negative log-likelihood, and calibration error) while incurring only a minimal (approximately 1%) increase in parameters and without a corresponding increase in inference cost. The empirical results on CIFAR10, CIFAR100, ImageNet, and several out-of-distribution benchmarks are compelling and indicate clear advantages over both single predictions and conventional ensembles.\n\nSeveral strengths have been noted by the reviewers:\n• The idea is clever and practically significant because it enables ensemble-like performance while reducing inference delay.\n• The experimental study is thorough, with investigations into the independence of subnetworks and a wide range of evaluation metrics.\n• The paper is well written and includes numerous experimental details and clear visualizations that enhance understanding.\n\nAt the same time, some concerns have been raised:\n• Although the increase in parameters is marginal, the claim that the benefits of multiple predictions come “for free” is slightly overstated.\n• More detailed analysis of where the parameter increase comes from and a broader evaluation on additional datasets (e.g., OpenImages) would strengthen the contribution.\n• There is some discussion about related work—specifically multi-branch or multi-head architectures—that could benefit from deeper comparison and discussion. Additional theoretical insights regarding the emergence of subnetwork independence would also further solidify the contribution.\n• Some reviewers pointed out potential issues in practical deployment, such as the effect of replicating the same test example and the necessary capacity trade-offs.\n\nOverall, the paper presents a novel and effective strategy with strong empirical support. While there are areas that could be improved (e.g., a more detailed evaluation of computational costs, better comparisons with related work, and hints at theoretical analysis), these are relatively minor in the context of a top-tier conference submission. They do not undermine the core contribution and significance of the work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe reviewers unanimously report substantial weaknesses in both the presentation and the validation of the proposed method. Several points stand out:\n\n1. Clarity and Rigor: Multiple reviewers note severe issues with the exposition. Critical sections refer to “unpublished work” without sufficient detail, making it impossible to verify the theoretical claims. Important derivations and definitions (e.g., the P-function, the precise meaning of reduction steps, and the exact formulation of the “stack network”) are unclear or missing. The presentation relies too much on footnotes and vague explanations rather than providing a coherent narrative.\n\n2. Novelty and Motivation: While the idea of pruning neurons based on activation properties is not entirely new, the theoretical underpinning appears to be inadequately motivated and not sufficiently distinguished from existing methods (e.g., highway networks, ReLU region studies, standard magnitude-based pruning). Several reviewers pointed out that the proposed architecture and techniques are either trivial or lack proper novelty relative to the existing literature.\n\n3. Experimental Validation: The experiments are limited to toy problems and MNIST, which are not challenging enough for a top-tier conference. There is no convincing experimental comparison with state‐of‐the‐art pruning methods, and statistical significance (error measures) is missing. The insufficient dataset scope and comparison limit the impact of the results, leaving the empirical support weak.\n\n4. Comparisons to Baselines: The manuscript does not compare the proposed method against several established pruning techniques (weight magnitude, gradient norm, Hessian-based methods, etc.). This makes it hard to assess the practical benefits of the approach and its potential adoption in a real-world setting.\n\n5. Overall Impact and Readiness: The quality of the writing, the presentation of results, and the reliance on unpublished, unverifiable work all undermine the overall contribution. The combination of theoretical inadequacies and an underwhelming empirical evaluation does not meet the standards expected at a top-tier conference.\n\nBased on these significant issues, the paper does not currently provide a solid and verifiable contribution to the field, and substantial revisions would be needed both in theory and experiments.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a gradient‐based SGD training approach for Gaussian Mixture Models in high-dimensional settings, and while the paper introduces some novel regularization and smoothing ideas, several critical issues emerge upon review. Reviewer 1 raises concerns that the proposed modifications (e.g., the max-component approximation and smoothing) are largely incremental and seem to repackage well-known numerical tricks (such as the log-sum-exp stabilization technique). Additionally, the reviewer questions the novelty in the context of a long history in online EM and points to potential inefficiencies (e.g., frequent recomputation of subspaces via QR decompositions). The experimental evaluation is seen as weak because the datasets used are rather low-dimensional when compared to the stated goal of handling high-dimensional data.\n\nReviewer 2 echoes these concerns by highlighting unclear motivations for several methodological choices. They question the handling of non-smooth max operations, the dubious interpretation of regularization (which appears mostly a heuristic for optimization, rather than a theoretically justified penalty), and ambiguities in the presentation of covariance matrix constraints and the low-rank approaches. Moreover, Reviewer 2 notes that there are missing details in the experimental comparisons (e.g., evaluation metrics, training time comparisons with EM, dataset splits), and the overall empirical results do not strongly favor the proposed method over existing baselines.\n\nReviewer 3 offers a somewhat more positive perspective regarding the clarity of presentation and the potential value as a “how-to” for training GMMs from random initialization. However, even this reviewer notes that the primary novelty arises from a particular regularizer and that the empirical advantages compared to standard EM (especially when using K-means initialization) are limited. The experiments, in their view, only modestly back up the claims, and several aspects (notably the benefits of the low-rank covariance parameterization) remain under-explored.\n\nIn summary, while the paper makes a reasonable case for the use of SGD in GMM training with some stabilizing heuristics, the contributions are incremental and the experimental evaluation falls short of the high standards expected at a top-tier conference. There are significant concerns about novelty, clarity in methodological motivations, and insufficient empirical evidence to show clear advantages over existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contribution: \n The paper introduces an approach that decomposes complex manipulation tasks into sub-skills and then learns to coordinate them using a meta-policy. The authors adapt DIAYN-inspired techniques to learn diverse individual skills that can then be coordinated in multi-agent settings. Although some aspects (e.g., DIAYN-style diversity bonuses) have been seen before, the way the paper leverages temporal abstraction for multi-agent coordination is interesting. The application domains—robotic arms and ant agents—demonstrate non-trivial task challenges. Overall, the work appears to be a meaningful extension to both skill discovery and multi-agent coordination.\n\n2. Technical Soundness and Empirical Evaluation:\n The reviewers agree that the empirical results are promising and that the method outperforms the selected baselines. Reviewer 1 noted that the environments and problem setting are important for testing cooperative multi-agent learning, and Reviewer 3 acknowledged that the approach is scalable. However, concerns include high variance in training performance, potential issues when the meta-controller essentially treats skills as primitives (T_low issues for the Jaco environment), and a dependency on pre-specified subtasks. While these issues warrant discussion and further experiments (e.g., additional seeds or an ablation study using DIAYN-alone), they do not fatally undermine the contributions.\n\n3. Experimental Clarity and Reproducibility:\n Some reviewers raised issues about inconsistent notations, missing details in figures (e.g., diagram of the meta-policy and low-level skills), and potential confusion regarding the setting (provided T_low details; latent vector sizes not reported). These criticisms are mainly presentation issues and can be addressed in revision. However, they are essential for clarity and reproducibility and should be improved moving forward.\n\n4. Relation to Prior Work:\n Reviewer 2’s questions about why one must discuss temporal abstraction in multi-agent settings, and why DIAYN is chosen over other skill discovery methods, highlight a need for a clearer positioning against both multi-agent and single-agent TA literature. More detailed discussion of alternative baselines in multi-agent coordination would strengthen the submission. Nonetheless, the authors’ approach is well-motivated if one considers the specialization required for cooperation among agents.\n\n5. Overall Impact:\n All reviewers agree that the paper addresses an interesting and challenging problem. While there are some gaps in the experimental analysis (e.g., need for further seed runs and more detailed hyper-parameter ablation) and presentation clarity (notations, diagram details), these issues are not fundamental theoretical or empirical flaws but rather improvements for the final version.\n\nConclusion:\nConsidering that the paper brings forward a novel modular approach for multi-agent manipulation tasks, that its experimental evaluation is promising (despite some variance issues), and that the concerns raised mostly pertain to presentation and additional analysis rather than the core contributions, I recommend that the paper should be accepted for publication. The potential impact and the innovative integration of diverse learned skills for multi-agent coordination are appropriate for a top-tier conference, provided that the authors address the reviewer concerns in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning:\n\n1. Novelty and Contributions:\n• Although the paper applies RNNs (specifically bidirectional LSTM) to the problem of predicting DNA folding patterns, similar applications using neural networks in computational biology have appeared in previous work. Reviewer 3, as well as Reviewer 1, note that the performance improvement might be more attributable to the modified loss function rather than any conceptual or methodological novelty in using LSTMs.\n• The contribution of a modified weighted MSE is questionable since it appears to be very narrowly defined (e.g., the “why 11?” concern) and effectively not compared on equal grounds with baseline methods.\n• Several reviews indicate that the paper does not adequately distinguish itself from previous studies and does not cite similar works. This is a major drawback given the high standard of novelty expected at a top-tier conference.\n\n2. Clarity in Methods and Data Description:\n• There are significant concerns regarding the clarity and detail of the description of the data and feature representation. Reviewer 3 questions how features are computed (binary or aggregated over regions) and how different chromosome segments are handled.\n• The choice and configuration of the biLSTM’s architecture (including hyperparameter tuning) is not sufficiently documented. For instance, Reviewer 3 asks why the center hidden state was used instead of a concatenation of forward and reverse states, suggesting a lack of detail that makes the results hard to reproduce or fully understand.\n• There is ambiguity around the definition and role of the proposed loss function. Reviewer 1 and Reviewer 2 note that a fair comparison with other methods is missing since similar loss function modifications were not applied to the linear models or gradient boosting methods.\n\n3. Evaluation and Experimental Rigor:\n• The experimental setup, especially concerning training and evaluation metrics, lacks clarity. Multiple reviewers (Reviewers 1 & 3) point out that additional evaluations – using metrics such as standard MSE, R² score, and comparisons using common alternative loss functions – would have provided a more comprehensive evaluation of the approach.\n• It is unclear whether the improvement of the biLSTM architecture is purely due to using a recurrent model or a combination with the modified loss function. Reviewer 1 explicitly calls for a comparison that isolates the contribution of the loss function design.\n• Several important details, including hyperparameter tuning of all models, choices in feature importance calculation, and the rationale for certain design decisions (such as the usage of a bidirectional LSTM in this setting), are inadequately explained.\n\n4. Overall Impact and Suitability:\n• While the application to biological data and prediction of chromatin folding is of significant interest, the experimental shortcomings, lack of methodological novelty, and clarity issues weigh strongly against the paper.\n• The paper does not clearly demonstrate that the biLSTM outperforms other architectures solely due to its sequential memory capabilities since comparable evaluations (e.g., with fully connected or convolutional models) are missing.\n• Given the concerns raised by multiple reviewers about methodological soundness, reproducibility, and clarity, the paper does not currently meet the rigorous standards expected of top-tier conference contributions.\n\nBased on these points:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a novel variant of the generative query network (GQN) framework, termed ROOTS, intended for unsupervised object-wise 3D scene decomposition and rendering. While the research idea is interesting and the qualitative results show promise (e.g., the object swapping and rendering examples), several significant issues make it unsuitable for acceptance in a top-tier conference in its current form.\n\n1. Clarity and Presentation: \nMultiple reviewers have pointed out that the writing and presentation are confusing and inconsistent. The notation is non-standard and poorly explained, with numerous typos and grammatical errors. Crucial equations are not clearly defined and are referenced only obliquely in the appendix. The method’s complexity is not well demystified by clear figures or systematic exposition. This lack of clarity makes the paper difficult to follow even for experts.\n\n2. Experimental Rigor:\nThe experiments are not sufficiently robust. There is a notable absence of thorough ablation studies that would allow us to isolate the contributions of various components in the system. While the paper makes strong claims about superior object-wise representation and generalization over GQN, the quantitative experiments (e.g., the marginal improvements in MSE and similar NLL scores) along with missing comparisons (e.g., to CGQN and other relevant baselines) do not convincingly support the claims. Additionally, some of the qualitative comparisons (e.g., in Figure 2) do not clearly differentiate ROOTS from the baseline.\n\n3. Comparison with Related Work:\nThe paper makes overly broad claims of novelty such as “first unsupervised model that can identify objects in a 3D scene,” which are not accurate when compared to existing work like MONet, Iodine, and various voxel-based approaches from the computer vision community. The authors also overlook many significant works on 3D reconstruction and neural rendering that should be acknowledged and compared against, thus weakening the paper’s position in the literature.\n\n4. Methodological and Notational Concerns:\nThe paper employs modifications (e.g., hierarchy in 3D and 2D representations, order-invariant object encoder) without providing enough detail on implementation or ablation studies demonstrating their specific utility. Several definitions (like that of f(3D→2D), STN-INV, and s^what) are either missing or inconsistently introduced, compromising reproducibility and clarity.\n\nConclusion:\nAlthough the paper shows promise and presents interesting qualitative demonstrations, the issues with clarity, incomplete experimental validation, insufficient comparisons, and methodological ambiguities are too significant for acceptance at a top-tier venue. The paper requires a major revision with significant improvements in writing, comprehensive ablation studies, stronger experimental evidence, and proper positioning with respect to related work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces a Differentiable Hebbian Consolidation framework for continual learning by integrating a fast-learning Hebbian Softmax component with slow weights to mitigate catastrophic forgetting. While the approach is motivated by cognitive theories such as complementary learning systems and offers a simple, differentially-programmable implementation, several concerns emerged from the reviews.\n\nKey issues include:\n1. Empirical Evaluation: Several reviewers note that while experiments on Permuted MNIST, Split MNIST, and a vision dataset mixture are provided, the evaluation is limited to rather simple benchmark tasks. There is a consistent call for stronger, broader empirical evidence, including comparisons on more challenging datasets (e.g., CIFAR-100 or CORe50) and scenarios such as class-incremental settings with many classes. In some cases (e.g., Split CIFAR-10/100 as noted by Reviewer 1), the proposed method does not clearly outperform existing approaches, and its benefits seem marginal or dependent on being combined with other consolidation techniques.\n\n2. Clarity and Presentation: Reviewer 2 raised technical concerns regarding the clarity of the method’s description—particularly the definition of indices, the equations, and the lack of a visual diagram to illustrate the fast versus slow weights dynamics. This lack of clarity could hinder accessibility for readers not already familiar with Hebbian learning nuances.\n\n3. Novelty and Motivation: While the idea of using a Hebbian approach alongside slow weights is appealing and biologically inspired, questions remain about why the strategy is applied solely at the final softmax layer and how it complements other state-of-the-art consolidation methods. Several reviewers express doubts that the method, in isolation, offers significant improvements over established baselines.\n\nGiven these concerns—especially the need for more comprehensive experiments on challenging continual learning benchmarks and some gaps in the presentation—the paper does not yet meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Clarity: While the authors introduce an approach to Mahalanobis metric learning based on tools from LP-type problems and geometric approximation, there is concern among the reviewers about the true novelty of the contribution. Converting the learning formulation into an LP framework appears incremental, and the paper does not sufficiently distinguish its approach from existing convex or approximation formulations. In addition, several key definitions (e.g., “combinatorial dimension”, “accuracy” in Theorem 1.1) and algorithmic details (such as the role of Exact‐LPTML and inputs) are not clearly defined.\n\n2. Theoretical Guarantees: Although the paper claims an FPTAS under the assumption of a fixed ambient dimension, multiple reviewers note an absence of in-depth discussions regarding the computational hardness of the underlying optimization problem. In addition, some steps in the theoretical proofs (e.g., the discussion in Lemma 2.1 regarding adding constraints) remain unconvincing or under-explained.\n\n3. Experimental Evaluation: The experimental results, as described, do not provide strong evidence of improvements over existing methods. Reviewer 3 especially points out that the empirical accuracy appears comparable to classical approaches such as ITML and LMNN, thus raising questions about the practical significance of the proposed method. There is also a noted inconsistency in reported running times (e.g., the non-monotonic trend in Figure 4) which detracts from the reproducibility and clarity of results.\n\n4. Literature Comparison: The reviewers mention that key recent related works (e.g., Verma and Branson, Ye et al.) were not adequately discussed. This omission makes it unclear where the proposed method advances the state-of-the-art compared to other recent efforts in the field.\n\n5. Overall Impact: Since the algorithm relies on the ambient dimension being fixed and the improvements appear more incremental than transformational—both theoretically and empirically—the paper does not clearly establish its impact beyond existing methods used for metric learning at a top-tier conference.\n\nTaking these points into consideration and given that the paper does not provide sufficient novelty, clarity, or empirical/theoretical advantages over prior work, my conclusion is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis and final decision.\n\nDetailed Reasoning:\nThe paper proposes to use CycleGAN for automatic music accompaniment generation by applying image-to-image translation techniques on Mel-spectrogram representations. Although the overall idea is interesting and potentially promising, several key issues prevent the work from reaching the standards of a top-tier conference.\n\n1. Novelty and Scope: \n • While applying CycleGANs on spectrograms is a creative approach, a number of similar works already exist in music information retrieval that treat spectrograms as images. In particular, the claim of being the first to use this paradigm is overstated. \n • The task is misrepresented in the title – “Automatic Music Production” suggests a broader scope than the actual focus on automatic accompaniment. The title and abstract need better alignment to the proposed contribution.\n\n2. Technical Details and Reproducibility:\n • The paper omits essential details regarding the CycleGAN architecture, hyperparameters, training settings, and optimization methods. This information is crucial for reproducibility and for assessing the technical soundness.\n • The source separation component (using Demucs) is not described adequately. Its limitations, artifacts, and potential impact on downstream tasks are not analyzed, which raises questions about the overall performance.\n • There is insufficient explanation about the inversion process from Mel-spectrograms back to waveform, including handling of phase information. The basic operations of quantization and the specific spectrogram settings also lack clarity.\n\n3. Experimental Evaluation:\n • The experiments, although conducted on recognized datasets, are limited in scope (only two tasks) and do not include sufficient ablation studies. For example, the impact of using automatically generated training data versus clean stems remains unexplored.\n • The evaluation methodology, including the subjective “human (and expert) judgment” metric, is not thoroughly justified. Many aspects, including the choice of quality attributes and inter-annotator agreement, are weakly motivated or insufficiently described.\n • Several reviewers note the need for additional experiments (e.g., reversing the accompaniment task) and comparisons with other unpaired image translation methods to set a proper baseline.\n\n4. Clarity and Exposition:\n • There are numerous presentation issues—from misleading use of terminology (e.g., “raw audio” when referring to Mel-spectrograms) to missing citations and details that would help understand trade-offs.\n • Many core decisions (e.g., why the chosen evaluation metrics are appropriate, the rationale behind the chosen music features) seem insufficiently motivated.\n\nIn summary, while the paper tackles an interesting and challenging problem with a promising idea, it is hindered by a lack of technical detail, incomplete experimental validation, and a presentation that leaves out crucial information for reproducibility and careful evaluation. These deficiencies are critical for a top-tier conference submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\nStrengths of the paper include addressing an important and practical problem—ecological inference—and attacking it from the perspective of deep and Bayesian neural networks. The application to a large-scale real-world dataset (Maryland 2018 midterm elections) is appealing and has the potential to offer new insights. The idea of leveraging deep architectures to recover latent representations and multi-task covariance structures is of interest and may spark further research in bridging political science and machine learning.\n\nHowever, multiple major concerns prevent the paper from meeting the standard of a top-tier conference:\n\n1. Clarity and Organization: Multiple reviewers highlighted that the paper is not written clearly. Key definitions (such as the Poisson binomial/multinomial losses, input data description, and evaluation tasks) are presented too late or too unclearly, impeding the reader’s ability to understand the contributions. This poor structure, along with numerous typos and formatting issues, detracts significantly from accessibility.\n\n2. Experimental Evaluation: A strong experimental section is critical for applied work. The experiments in this paper suffer from several issues:\n   - Incomplete baseline comparisons: For example, the deep multi-level model is not compared against a standard multi-level model, leaving uncertainty as to how much deep learning is truly adding.\n   - Lack of key details: The report omits important implementation specifics (e.g., data splitting, hyperparameters, exact definitions of latent representations used for few-shot prediction), which affects reproducibility.\n   - Inconsistent or unclear analysis: Some metrics (e.g., why a standard linear regression achieves the best test-set R² while deep models excel on other tasks) are not sufficiently analyzed or reconciled.\n   - Graphs and tables: Figures and tables contain missing information and do not always support the paper’s claims, further weakening the empirical validation.\n\n3. Novelty: While the ecological inference problem itself is important, there are concerns that the proposed approach mainly adapts conventional deep learning components to an established problem without introducing fundamentally new techniques. Some reviewers also noted that related work (e.g., related to label proportion methods, distribution regression, and collective graphical models) is not adequately discussed or compared to.\n\nGiven these issues—from clarity and presentation to incomplete experimental evaluation—the work, in its current state, does not provide the level of rigor and novelty typically expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nAcross the reviews, there is a clear consensus regarding several major shortcomings that prevent the paper from meeting the standards of a top-tier conference. First, the technical derivations and underlying theory are problematic. The paper heavily relies on unpublished work and does not sufficiently detail the derivations or provide verifiable proofs. This reliance makes it impossible for reviewers or readers to assess the technical correctness and significance of the proposed approach. In particular, key theoretical claims (e.g., the reduction in neurons based on implicit/explicit regularization) are either too vague or presented as informal observations that should be stated as precise theorems.\n\nSecond, the experimental evaluation is too limited. The experiments are conducted only on toy datasets and the MNIST dataset, which, in the context of modern deep learning research, are considered too simple and unrepresentative of realistic challenges. There is also a notable absence of comparisons with state-of-the-art pruning and compression methods. Without empirical evidence on more complex datasets (e.g., CIFAR or ImageNet) or rigorous statistical analysis, the proposed method’s practical benefit remains unsubstantiated.\n\nThird, the presentation and organization of the paper leave much to be desired. A significant number of organizational issues were reported by the reviewers, including unclear definitions, an overuse of footnotes, numerous typos, and poor structure that hampers the reader’s ability to verify the claims and follow the overall argument.\n\nFinally, the novelty of the approach is questionable. Several reviewers noted that many of the ideas (such as pruning based on activation patterns) are either well understood or trivial, and the method appears to closely resemble existing architectures like highway networks. The insufficient differentiation from established approaches further undermines its impact.\n\nIn summary, the paper has considerable gaps in theory presentation, experimental validation, and clarity of exposition, making it unsuitable for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning.\n\nThe paper provides an analysis of SGD-ascent descent for WGANs in the restricted setting of a one‐layer generator and simplified discriminator functions (rectified linear and quadratic forms). While the theoretical derivations appear technically sound and the authors establish polynomial-time convergence in this simplified setting, several significant concerns were raised by the reviewers.\n\nKey issues include:\n\n1. Over‐Simplification of the WGAN Framework:\n • All reviewers noted that the work replaces the usual discriminator architecture (which should approximate the Lipschitz set of functions) with extremely simple functions. This choice undermines the connection between the theoretical analysis and the practical success of WGANs. In essence, the paper’s analysis of linear and quadratic discriminators is very far from the complex neural networks used in practice.\n • Reviewer 2 and Reviewer 3 stressed that while it is an interesting exercise to study such simplified cases, the limitations mean the results do not fully explain the empirical behavior of WGAN training with complex discriminators. Reviewer 1 specifically questioned the justification for treating a quadratic discriminator in the WGAN framework.\n\n2. Misleading Claim and Insufficient Model Scope:\n • The title “SGD Learns One-Layer Networks in WGANs” and associated claims appear overstated. As Reviewer 2 points out, the phrase “one-layer network” is ambiguous and potentially misleading. Either it means a linear model with an elementwise nonlinearity, or it is confused with the more common “two-layer” or “single hidden layer” network.\n • Reviewer 1 and Reviewer 3 also mentioned that the absence of an analysis of more complex discriminators (such as a standard two-layer network with interactions across coordinates) leaves a gap. That omission weakens the paper’s contribution with respect to understanding the true dynamics of WGAN training.\n\n3. Lack of Error Analysis for Extending the Results:\n • Reviewers expressed concern over the inability to extend the analysis to more realistic settings. Of particular note is the worry regarding error propagation in the gradient ascent step when using a more complex discriminator. The current work does not offer bounds or discuss how the analysis could be adapted for networks closer to those used in practice.\n • Even the authors’ responses were not seen as sufficient to address these fundamental concerns.\n\nDespite the strength of the technical analysis on the simplified model system, the impact of the work is severely limited by the narrow scope and the resulting gap between theory and standard practice in WGANs. On the balance of these considerations—and given that the analysis does not directly illuminate the performance of real-world WGANs—the paper does not meet the standard expected at top-tier conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning.\n\nStrengths of the paper include its clear focus on an important deficiency of current deep network models of vision—namely, their need for orders of magnitude more supervised weight updates than seem biologically plausible. The authors propose complementary strategies (reduced training, weight compression for an “at-birth” initialization, and selectively training a small fraction of synapses) that together yield models achieving roughly 80% of the BrainScore of a fully trained model while reducing supervised updates by two orders of magnitude. The experimental work is detailed, includes transfer experiments (extending the method from CORnet‐S to models like ResNet and MobileNet), and the paper is overall well written and well positioned in the literature. Several reviewers (e.g., Reviewer 4 and Reviewer 5) find that these contributions are interesting and have potentially broad impact for both computational neuroscience and machine learning.\n\nThat said, there are non‐negligible concerns raised by other reviewers. A primary issue, raised by multiple reviewers (R1, R2, and R5), concerns the interpretation of the BrainScore metric. The paper tends to conflate “80% match” with an 80% biological correspondence, when in fact it is 80% of the numerical score—and the biological meaning of that score is itself quite uncertain. This point raises questions about whether the claims should be rephrased to avoid over-interpretation. Additionally, Reviewer 3 questions the theoretical grounding and biological plausibility of some of the choices (especially the weight compression and the selection of layers to update during “critical training”), indicating that many assumptions come without strong justification. There are also some outstanding questions on the variance of the measurements, significance testing, and what constitutes a “true zero” or baseline for these matching metrics.\n\nOn balance, however, while the interpretation and framing require some care (and perhaps additional experiments or clearer wording), the methods and experiments are interesting, and the overall message—that one can achieve a reasonably high BrainScore with dramatically fewer supervised updates—is novel and potentially impactful. In a top-tier conference the methodological novelty, the complementary strategies, and the thorough experimental assessment offer sufficient value. The concerns about interpretation can be addressed through clarifications and careful framing in the final manuscript.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a GNN model that employs global self-attention using learnable spectral filters to address the limitations of local aggregation under the homophily assumption. While the idea of incorporating non-local attention is intuitively interesting, several concerns undermine the contribution for a top-tier venue.\n\nMajor concerns raised by multiple reviewers include:\n1. Novelty – Multiple reviewers questioned whether the paper’s core idea of using global attention is substantially different from existing methods (e.g., ChebNet and Non-Local Graph Neural Networks). The arguments provided by the authors to distinguish their work from prior approaches are not sufficiently convincing, and some reviewers argue that similar effects can be achieved (perhaps with different tradeoffs regarding overfitting).\n2. Computational Complexity – There is significant concern regarding scalability. The global attention mechanism appears to involve computing pairwise interactions or even performing full eigen decompositions (as hinted by Eq. (3)), which would lead to prohibitive costs, especially on medium or large-scale graphs. Reviewers specifically noted that the claimed complexity benefits were not substantiated and demanded more detailed analysis and empirical runtime comparisons.\n3. Evaluation – The experimental evaluation, although thorough in some respects, is limited to relatively small disassortative graphs and a single task (node classification). This raises questions as to whether the method can generalize to larger networks or other tasks. One reviewer noted that without a more diverse set of experiments or synthetic studies that explore controlled parameters, it is hard to understand the advantages of the proposed approach.\n\nIn summary, while the paper presents an interesting mechanism, the concerns regarding novelty and especially the computational complexity and scalability are significant. The authors have not sufficiently addressed these issues, and the experiments do not convincingly demonstrate clear advantages over existing methods. Therefore, from the perspective of a top-tier conference, the paper falls short of the necessary standards.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is a detailed analysis of the paper along with the final decision.\n\n1. Novelty and Technical Contribution:\n• The paper proposes αVIL to update task weights dynamically by exploiting changes in model parameters from single-task updates. While this is an interesting idea, several reviewers noted that the method appears ad hoc with insufficient theoretical grounding.\n• The reviewers also highlighted that the proposed algorithm closely resembles existing ideas in meta-learning and dynamic task weighting (e.g., DIW, MAML variants), and the novelty is therefore somewhat limited.\n\n2. Theoretical Justification:\n• A major concern raised by multiple reviewers is the absence of a clear theoretical justification or formal analysis of why the proposed approach should work. Without proper analysis or insight into the algorithm’s behavior, it is difficult to appreciate its benefits over simpler or existing alternatives.\n\n3. Empirical Evaluation:\n• The experimental results on both vision (MultiMNIST) and NLU tasks are inconclusive. Improvements reported are minor and often within the range of statistically insignificant differences compared to baselines.\n• Questions regarding experimental design, such as the relatively trivial multitask setup (only one auxiliary task on MultiMNIST), have been raised. Moreover, important experiments, such as showing the evolution of learned weights or performing a sanity-check with noisy tasks, were omitted.\n• The baselines are somewhat limited, and the chosen evaluation does not convincingly demonstrate any clear margin of improvement. Even when improvements are seen, they are not consistent enough to satisfy the standards of a top-tier conference.\n\n4. Clarity and Presentation:\n• While the paper is generally easy to follow, there are several points that need clarification. For example, several steps in the algorithm (like the subtraction of 1 in line 11) appear arbitrary and lack sufficient intuitive explanation.\n• The inclusion of clearer ablation studies and additional discussion on the dynamics of the learned task importance would have strengthened the contribution substantially.\n\n5. Overall Evaluation:\n• The paper proposes an intuitively appealing idea for leveraging auxiliary tasks in multitask learning, but it falls short of providing a rigorous theoretical framework and conclusive empirical evidence.\n• The analysis provided in the paper is not sufficient to justify the claimed performance benefits over existing methods, and there is significant scope for improvement in both theoretical and experimental aspects.\n\nBased on the above considerations and the consistent concerns raised by the reviewers, the paper does not yet meet the standards expected for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning and final decision.\n\nOverall, the paper presents an interesting idea by adapting boosting techniques (and a non‐convex Frank–Wolfe variant) to reinforcement learning in order to “boost” weak RL learners and obtain a policy whose sample complexity is independent of the number of states. This is an appealing approach that could potentially bypass common structural assumptions found in large-scale RL problems. The theoretical results (sample complexity and running time guarantees) are nontrivial and may be of independent interest to both the RL and optimization communities. Two reviewers (Reviewers 3 and 6) in particular appreciated the theoretical contributions and the novelty brought by applying boosting ideas to RL.\n\nHowever, several significant weaknesses are repeatedly noted by the reviewers. Major concerns include:\n • Clarity and presentation: Multiple reviewers (1, 4, 5, and 6) remarked that the paper is extremely difficult to read. The heavy, over‐loaded notation and several instances of undefined or inconsistent symbols make it challenging for readers to parse the results and follow the underlying ideas.\n • Lack of concrete examples/experiments: Although the work is theoretical, reviewers (2, 5, and 6) noted that a simple numerical example or experimental evaluation would help demonstrate the practical relevance and build intuition about the method.\n • Incremental technical novelty: At least two reviewers (2 and 3) highlighted that many of the techniques are similar to previous work (e.g., Hazan and Singh 2021) and that the novelty is somewhat limited when compared to the state-of-the-art.\n • Implementation issues and missing discussions: Questions remain open on how certain algorithmic components (e.g., how to compute certain gradients, the existence of certain operators or functions claimed to exist, the assumption of an “efficient exploration scheme”) can be implemented in practice. More discussion comparing with state compression techniques and standard RL methods is needed.\n\nIn a top-tier conference, contributions are expected not only to be technically sound but also to be clear, well-motivated, and illustrate significant novelty. While the theoretical guarantees and the idea of state-space–independent error are promising, the current submission suffers from clarity issues, overly dense presentation, and an insufficient discussion regarding practical implementation and experimental validation. Given that several reviews call for major revisions (with some even stating that the paper seems unready for publication), the paper in its present form would not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper investigates an important question – disentangling the contributions of architecture versus gradient descent (GD) to generalization – and provides a novel analytical estimator and PAC‐Bayesian bounds for the generalization error of NNGP. While its theoretical contributions (e.g., the derivation of the bound and a discussion of the role of margin) are interesting, multiple reviewers express serious reservations. In particular, there is a consistent critique that the experimental evidence is limited and often appears to be based on highly specific settings. The experiments predominantly use a relatively new “Nero” optimizer rather than standard GD/SGD, and the comparison with the NNGP baseline is either too narrow or lacking in thorough ablation (e.g., comparisons across different architectures, optimizers, or datasets). Moreover, the section discussing gradient descent’s implicit bias (via margin maximization) is seen as imprecise, potentially misleading, and not sufficiently justified either theoretically or empirically. Several reviewers noted that much of the discussion appears incremental relative to prior works (e.g., Valle-Pérez et al. (2019)) and that some sections (notably Section 5) should be more focused or even removed. While there is some support (including a low-confidence weak accept) from some reviewers, the overall picture – with many pointing out that the results are premature and not broadly applicable – leans towards the contributions being insufficient for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning, followed by the final decision.\n\nReasoning:\nThe paper proposes a Fourier convolutional neural network architecture that explicitly tackles the challenge of global point-spread functions in 3D snapshot microscopy (and additionally in a lensless imaging setup) through an end‐to‐end optical encoder and deep learning decoder optimization. Overall, the paper is well written and clearly motivated by an important application in computational imaging. Multiple reviewers appreciated the clarity, thorough experiments (albeit mostly in simulation), and the novel adaptation of Fourier-domain operations to accelerate and improve reconstruction versus state‐of‐the‐art UNet based approaches.\n\nThere are some concerns to note. Several reviewers pointed out that many of the “machine learning ideas” (i.e., learning multiplicative weights in the Fourier domain) have close antecedents such as in the work by Rippel et al. (2015) and others. In this respect, while the global convolution idea is not entirely new, the authors demonstrate that it is essential for the highly non-local encoding required by 3D snapshot microscopy. The authors’ responses and additional comparisons have gone a long way toward clarifying differences and the impact of the proposed method.\n\nOther criticisms included that the evaluation is mainly on simulated data – with only additional experiments on lensless imaging – and that some implementation details (e.g., the handling of conjugate symmetry, memory/parameter comparisons) are missing. While these are valid concerns, the reviewers who addressed these issues have indicated that they could be addressed with further clarifications and revisions. Some reviewers, however, have upgraded their ratings after the author responses.\n\nBalancing all these points, most of the reviewers see this work as sufficiently mature and well-executed, with promising empirical results and potential for significant impact in computational microscopy and imaging. Although certain improvements (especially better contextualization with respect to prior Fourier-based methods and additional discussions on resource trade-offs) would strengthen the manuscript, these issues are not fundamental enough to block acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces a novel framework for characterizing the complete set of global optimal solutions for two-layer ReLU networks via a convex optimization formulation with cone constraints. Its contributions include an exact mapping between the solutions of the nonconvex training problem and those of a convex program, a characterization of (nearly) minimal neural networks, and several consequences such as a polynomial-time algorithm for checking global optimality and establishing the absence of spurious local minima under overparameterization. \n\nAcross the reviews, the strengths are consistently praised: the work presents elegant and clean constructions; the convex duality perspective is extended in a novel way that does not solely rely on duality-based approaches seen in previous literature (e.g., Pilanci and Ergen, 2020); and the results offer substantial theoretical insights with potential future extensions to deep networks.\n\nSome concerns raised include:\n•\tThe focus is limited to two-layer networks, and several reviewers ask about possible extensions to deeper architectures or different activation functions.\n•\tThere are a few minor typographical and clarity issues (e.g., mismatches in definitions, summation indices, and explanation of minimality). \n•\tThere were questions about the necessity of the minimality constraint in the statements, but these seem more technical refinements rather than fundamental flaws.\n\nGiven that the positive aspects significantly outweigh the minor criticisms and the reviewers largely support acceptance—with one reviewer rating \"clear accept\"—the paper presents a substantial theoretical contribution worthy of publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning for my decision:\n\n1. Novelty and Scientific Contribution: Multiple reviewers noted that the main novelty—the Believer-Penalizer (BP) strategy—is a rather straightforward or naive extension of existing techniques (e.g., Kriging Believer and Local Penalization). Reviewer 3 in particular stated that the theoretical justification is weak and “simply experiment results cannot be adopted as evidence,” and Reviewer 4 echoed that the threshold used is not well-motivated. Overall, while the system is well engineered, the methodological contributions are only marginally novel.\n\n2. Empirical Performance and Experimental Validation: The experimental results provide a mixed picture. Reviewer 1 pointed out that the performance in Figure 5 is not clearly superior, whereas Reviewer 4 mentioned that BP is not consistently the best and sometimes even the worst on certain problems. There is also concern about the limited comparison baselines and the potential for overstating claims. This inconsistency weakens the case for a top-tier conference publication.\n\n3. Engineering and System Contributions: Although the platform itself is well designed and can handle asynchronous, multi-objective optimization with a GUI and support for real-world hardware experiments, several reviewers commented that the paper primarily reads like a well-documented software package. Reviewer 2 even described it as an “assembly of software design docs” with limited methodology contribution. Many existing packages already address similar problems and the integration, while useful, does not fully meet the novelty threshold expected from a top-tier conference.\n\n4. Clarity and Reproducibility: The paper is generally well-written and structured (as acknowledged by Reviewer 4 and Reviewer 5), but there remain important issues such as unclear motivations for the BP method and inconsistency over its scope (e.g., limitations to 2 or 3 objectives). These issues could confuse readers and detract from the technical impact.\n\n5. Overall Assessment: Although the problem of multi-objective optimal experimental design is well-motivated and the platform has several useful features—including asynchronous optimization and a user-friendly GUI—the incremental improvement in methodology and mixed experimental results point to contributions that are not strong enough for acceptance at a top-tier conference.\n\nBased on these considerations, the paper, despite some positive aspects, does not meet the expected level of technical novelty, rigorous theoretical grounding, and empirical demonstration needed for acceptance at a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my reasoning:\n\n1. Overall, while the paper tackles an interesting phenomenon—that is, divergence “by resonance” in momentum SGD under covariate shift—the reviewers have raised significant concerns about both the technical novelty and the practical impact of the work. Multiple reviewers note that the theoretical derivations largely rely on established techniques (Floquet theory and operator splitting results) and that the presented proofs do not introduce substantial new mathematical tools. For example, Reviewer 2 and Reviewer 3 both expressed that the “technical novelty” is questionable and that many of the derivations are either elementary or direct applications of existing results.\n\n2. A recurring theme in the reviews is the presentation and interpretation of the results. While the paper shows that under a periodic covariate shift there may be divergence due to resonance, the discussion does not offer enough intuition about when and why this happens. Reviewer 2’s and Reviewer 3’s comments highlight that—even though the experiments align with the theory—the connection between the theoretical quantities (like the spectral radius of the monodromy matrix) and the algorithm’s actual behavior (e.g., the specific resonant frequencies) remains murky to a reader. This gap in the intuitive understanding reduces the appeal and potential impact of the results.\n\n3. Another major concern is the mismatch between the theoretical analysis and the practical “SGDm”. The analysis is based on expected gradients (a continuous-time approximation) which essentially strips away the intrinsic stochasticity of SGD. Several reviewers (e.g., Reviewer 3 and Reviewer 5) question whether this abstraction remains informative for practical algorithms. The resonance behavior, as presented in the theory, appears to emerge only under rather limited conditions (high momentum parameters and low noise). Reviewer 5 noted that resonance may only occur in contrived scenarios that are not reflective of real practice, thereby limiting the practical significance of the findings.\n\n4. While there is an empirical section that extends the qualitative observations to more general settings (including neural networks and optimizers like Adam), the negative reviewers are not fully convinced that the observed behavior in these settings can be directly attributed to the same “resonance” mechanism. In addition, Reviewer 6 requested a clearer empirical demonstration of where the practical relevance really emerges (e.g., on real datasets with realistic covariate shift profiles).\n\n5. In summary, though the premise is appealing and the phenomenon is potentially significant for understanding non-iid optimization, the paper suffers from several problematic aspects: low technical novelty, a disconnect between the abstraction used in the theory and the realities of SGD, insufficient intuitive explanations of the resonance mechanism, and reservations about the real-world impact of the results.\n\nGiven these reservations from multiple reviewers at a top-tier conference, I lean toward a decision of rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis:\n\n1. Theoretical Contributions and Clarity:\n • Multiple reviewers pointed out that the main theorems (e.g., Theorems 1 and 3, and Lemmas 2 and 4) are essentially straightforward algebraic derivations. Several of the technical contributions are seen as “basic computations” that, even though they formalize a connection between weighted ERM and moment penalization, add little new insight beyond past work.\n • There is considerable confusion regarding the interplay between the weight function and its dependence on the model parameters θ. Reviewers 2, 3, and 5 highlight that when the weight function depends on θ, the gradients in the proposed Algorithm 1 do not reflect a true gradient descent on the expected weighted loss. The omission of the additional term—arising from the derivative of the weight—calls into question the theoretical validity of the algorithm.\n • The presentation suffers from unclear motivation: negative versus positive λ values lead to opposite effects without a convincing explanation. Confusing remarks (such as the statement that “one can do one thing and its opposite”) leave the reader wondering how to choose the penalization parameter in practice. Reviewer 4 especially notes that the shift from standard variance penalization to using negative λ values undermines the coherence of the paper.\n\n2. Empirical Validation and Practical Considerations:\n • Although the experimental results indicate that the “weighted mean trick” can match specialized robust losses on noisy datasets, the reviewers argue that improvements might be a mere artifact of hyperparameter tuning. There is an absence of a clear connection between the algorithm’s iterative procedure and the minimization of a well-defined theoretical objective.\n • Several practical questions remain unanswered, including convergence guarantees for Algorithm 1 and how the approach compares with related methods such as trimmed means or median-of-means strategies. Reviewer 4, among others, suggests that these aspects are underdeveloped, thereby reducing the practical impact of the work.\n\n3. Overall Reception:\n • While a minority of reviewers (e.g., Reviewer 7) recognized that the observation could have consequences in later robust learning algorithm applications, the majority of reviewers express significant concerns over clarity, theoretical validation, and the novelty of contributions. Concerns about the proper treatment of the gradient update (and hence whether the algorithm minimizes the claimed objective) are particularly damaging.\n • Moreover, many issues with presentation, notation, and the choice/motivation of parameters remain unresolved in the current manuscript. These issues lead to the conclusion that the paper does not meet the standards expected for a top-tier venue.\n\nGiven these critical shortcomings in both theoretical and practical aspects—especially the lack of clarity regarding the algorithm’s correctness and the marginal novelty of the results—I recommend that the paper be rejected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is a summary of my detailed reasoning:\n\n1. The paper offers a theoretical study of the convergence behavior of feedback alignment (FA) for deep linear networks. The authors derive convergence results (both continuous and discrete time) as well as discuss “implicit regularization” (and its converse, “implicit anti-regularization”) phenomena under specialized conditions. While the mathematical derivations appear correct and the paper is clearly written, several key concerns remain.\n\n2. A recurring theme in the reviews is that the analysis is built on quite restrictive assumptions. In particular, diagonalizability of the involved matrices and the use of a “spectral initialization” allow the problem to be reduced to scalar (or one-dimensional) dynamics. Multiple reviewers raised concerns that the resulting insights may be too specialized:\n • Reviewer 1 worries that reducing the dynamics to one-dimensional problems oversimplifies the original matrix problem and questions if the convergence behavior is reflective of more common scenarios.\n • Reviewer 2 points out that the conditions imposed (such as the assumed dependence between weight matrices at initialization) are somewhat artificial, especially given that common practice in deep networks employs independent random initializations.\n • Reviewer 4 expresses skepticism about the significance of the results given that spectral initialization is not a typical method used in practice. The reviewer suggests that broader experiments and analyses are needed to convince readers that the key phenomena (such as backward learning or monotonicity) persist under realistic initialization schemes.\n\n3. While one reviewer (Reviewer 3) was enthusiastic about the results and described them as “interesting” and “novel”, the overall balance indicates that the technical contributions are incremental and the theoretical developments are confined to a special case. In top-tier conferences, contributions are expected not only to be mathematically rigorous but also to carry high significance and relevance to practical scenarios.\n\n4. In summary, despite the correctness of the proofs and some insightful observations, the paper’s reliance on strong (and somewhat artificial) assumptions as well as the limited empirical support substantially weaken the general impact of the results. The gap between the reduced (scalar) analysis and the actual high-dimensional dynamics encountered in practice prevents this work from reaching the broader significance expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n• The paper tackles a well-known limitation of diffusion models—that high-fidelity generation typically requires hundreds of iterations—and offers an alternative strategy that uses a differentiable sampler search to obtain high-quality samples using far fewer steps. This is an important technical problem with immediate practical impact.\n\n• The method introduces Generalized Gaussian Diffusion Models (GGDM) to allow non-Markovian sampling and then optimizes the sampler parameters via a differentiable perceptual loss (minimizing an objective such as KID rather than maximizing the ELBO) by leveraging the reparametrization trick and gradient rematerialization. Although this optimization target is arguably “engineered” toward improving the popular FID/KID metrics, several reviewers acknowledge that the improved perceptual quality in the low-step regime is compelling.\n\n• The reviewers note that while the idea is not entirely without precedent (considering the connections to DDPM/DDIM literature and even known numerical integration methods), the formulation and application to fast sampler search is novel and practically significant. The refined Theorem 1 in the revised paper adds clarity on the relationship between the new model and pre-trained DDPMs, reducing earlier concerns about changing statistical properties.\n\n• Some theoretical concerns remain (the precise statistical interpretation of the trained model is not fully settled, and the loss is rather hand-crafted), yet multiple reviewers agree that the empirical improvements—especially in regimes with very few steps—and the clear empirical comparisons across datasets make a strong case for significance.\n\n• There is some feedback about clarity and terminology and suggestions for additional experiments on high-resolution settings. While these issues should be addressed in revisions, they do not outweigh the overall novelty and empirical contribution.\n\n• Overall, the paper provides a novel, moderately well-justified contribution with promising results that advance the state of sampling from diffusion models. This would interest the ML community given the current prominence of diffusion models and the urgency for faster sampling methods.\n\nTaking all points into consideration, I conclude that the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n• Novelty and Contributions: Several reviewers noted that the method is essentially an adaptation of well‐known meta-learning and reweighting techniques (e.g. MAML and Meta-Weight-Net) to adversarial training. Although the paper introduces the idea of using multi-class margins as an encoding to a learned reweighting network, many reviewers considered that the technical novelty is only incremental and that a more in-depth study of “why” this embedding is effective (including ablations against alternative input encodings) remains to be conducted.\n\n• Evaluation and Experimental Results: While experiments on benchmarks such as MNIST, CIFAR-10, and possibly CIFAR-100 (though results on CIFAR-100 are missing) show some promise, several reviewers raised concerns. The performance gains, particularly when facing strong adversaries (e.g. AutoAttack), appear marginal or hinge on combining the method with another technique such as TRADES. Notably, there is a consensus that the current experimental section does not include a rigorous adaptive attack evaluation – an omission that is critical given the method introduces a hidden reweighting module potentially exploitable by an adaptive adversary. Furthermore, reviewers noted that additional comparisons with state-of-the-art baselines (such as AWP) are needed to justify the claims made in the abstract of outperforming “state-of-the-art baselines.”\n\n• Analysis of the Reweighting Mechanism: A few reviewers (e.g. Reviewer 3 and Reviewer 10) indicated that further studies (both theoretical and empirical) elaborating on the effectiveness of the chosen input encoding (the multi-class margin) for the meta network would have strengthened the work considerably. Several ablations as requested by the reviewers (including evaluating different input types, fixed versus learned reweighting, training time overhead, and additional visualizations) are necessary to decouple the effect of the learned reweighting from other confounding factors (such as increased parameter count or different optimization dynamics).\n\n• Summary of Review Signals: With reviewers like 1, 2, 3, and 6 voicing significant concerns regarding the evaluation, novelty, and empirical improvements, the overall picture – even though some reviewers (e.g. 4, 7, 9, and 10) see value – remains mixed and leans toward the need for further work before publication at a top-tier venue.\n\nGiven the incremental nature of the novelty, insufficient adaptive attack evaluations, marginal performance improvements in some settings, and the open questions regarding the design choices, I judge that the paper is not ready for publication without substantial additional experimentation and discussion.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contributions and Problem Relevance:  \nThe paper focuses on a challenging and practical problem: efficient face detection with emphasis on small faces in low-resolution images. It introduces two components (Computation Redistribution and Sample Redistribution) that address network architecture and training sample allocation, which are important factors in face detection. The reported improvements over previous methods (e.g., outperforming TinaFace on the hard set and achieving a favorable trade-off between accuracy and efficiency) make the paper relevant to the community.\n\n2. Experimental Results and Empirical Evidence:\nExtensive experiments on a challenging benchmark (WIDER FACE) and additional datasets provide strong empirical support. The ablation studies clearly demonstrate that both CR and SR contribute positively to the overall performance. The improvements, especially on small faces, establish the method’s effectiveness in a scenario where many real-world applications require detection at multiple scales.\n\n3. Novelty and Technical Merit:  \nWhile several reviewers noted that the search strategies (i.e., the random search in a crafted search space) are fairly straightforward, there is still significant empirical novelty. The approach to redistribute computation among the network’s components and to augment training samples effectively is not entirely present in prior work in this way. Although the technical novelty is somewhat incremental—relative to more sophisticated NAS methods—the strong empirical gain and the careful design of the search space provide sufficient technical contribution for a top-tier venue.\n\n4. Clarity and Quality of Presentation:\nSome reviewers suggested that additional details (e.g., an overview figure or an algorithm description) would improve clarity. Such issues, however, are not fundamental flaws and could be addressed in the camera-ready version. The core methodology remains sound and the experimental evidence is persuasive.\n\n5. Reviewer Consensus:\nThe majority of the reviews are favorable, with several reviewers explicitly stating that the paper is acceptable and even highlighting sufficient innovation with very strong performance results. While some comparisons to other NAS methods (like evolutionary approaches) would be beneficial, the overall merit of the proposed approach and its performance improvements justify acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Motivations: Multiple reviewers highlighted that while the paper’s experiments are interesting and provide additional observations, the novelty of the contributions is questionable. Several reviewers noted that the claim “adversarial examples are dataset dependent” is already known or at least hinted at in prior works. Moreover, the paper’s motivation—challenging the “high-frequency misconception”—has been partly addressed in existing papers. Hence, the incremental nature of the contribution reduces its value for a top-tier conference.\n\n2. Clarity and Presentation of Contributions: Reviewer 2 and Reviewer 5 in particular express concerns that the paper’s contributions (including the frequency-based adversarial training variation and the controlled trade-off mechanism) are not clearly articulated or convincingly demonstrated. The methods do not clearly demonstrate superior performance (e.g., better robust accuracy or improved accuracy–robustness trade-offs) compared to existing approaches such as TRADES. The paper’s overall exposition makes it difficult for the reader to separate prior work from the novel contributions of this paper.\n\n3. Experimental Validation and Scope: Although some experimental setups are appreciated (as noted by Reviewer 4 and Reviewer 6), the experiments themselves have limitations. For instance, the observations are validated only on a few datasets (primarily CIFAR-10 and ImageNet-derived datasets) and with PGD attacks only. This limited evaluation raises concerns about whether the findings generalize to other settings, such as different architectures or attack types. Additionally, questions about the imperceptibility of low-frequency attacks and the proper definition of the threat model remain unanswered.\n\n4. Comparison with Related Work: Several reviewers noted that there exists prior literature (including concurrent work) that has already discussed or provided evidence for similar conclusions. The marginal differences (if any) between this work and the prior art are insufficiently stressed or convincingly argued, which diminishes the impact of the contributions.\n\nTaking all the above aspects into account—especially the concerns about the clarity of the unique contributions, the incremental nature of the findings, and the limited experimental scope—I conclude that the weaknesses outweigh the strengths when judged by top-tier conference standards.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Clarity and Presentation: Multiple reviewers noted that the paper is hard to follow, with several areas lacking necessary clarity. Key definitions such as “positional disentanglement metric” and “systematic generalization” are either not well-defined or explained inadequately. The introduction and related work sections read repetitively, and important concepts are not sufficiently elaborated on (e.g., details about the hybrid symbolic-continuous stimulus (SCS) representation). Reviewers also asked for more concrete examples and detailed description of the experimental setup. This overall lack of clarity is a serious drawback for a top-tier conference paper.\n\n2. Experimental Evaluation: All reviewers agree that the empirical evaluation is weak. The experiments do not provide sufficient ablation studies or comprehensive comparisons (e.g., with one-hot encodings or even fully continuous representations). Some reviewer comments mention questionable results (for instance, performance around chance-level) and concerns regarding hyperparameter tuning and limited seeds. These factors make it hard to support the central claims regarding the usefulness and generality of SCS. Also, the benchmark and its evaluation are underdeveloped, and the paper leaves important questions (e.g., scalability and adaptability to tasks beyond referential games) unanswered.\n\n3. Technical Novelty and Significance: Although the paper’s problem—learning compositional behaviors via meta-referential games—is interesting, the contributions are seen as marginal by all reviewers. The SCS representation, while novel in presenting a continuous alternative to one-hot encoding, has limited applicability beyond the contained experimental setup. Many reviewers expressed skepticism about whether SCS offers clear benefits over existing continuous representations in truly challenging, real-world tasks.\n\n4. Overall Impact and Future Work: While there is potential in the idea of exploring meta-learning for compositional generalization, the paper in its current form lacks the necessary development both in theoretical justification and in substantial empirical evidence. The gaps in clarity and evaluation lead all reviewers to conclude that far more work is needed before it can be considered a strong contribution to the literature.\n\nGiven these significant weaknesses in clarity, experimental validation, and the marginal novelty of contributions, I conclude that the paper does not yet meet the bar required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis:\n\n1. Novelty and Technical Contribution:\n   • The paper introduces αVIL, a method that adjusts task weights dynamically by leveraging parameter updates. While this is an interesting idea, multiple reviewers noted that the approach is reminiscent of existing meta-learning methods (e.g., MAML or meta-weighting approaches) and dynamic task weighting is not particularly new.\n   • A critical concern is that the proposed mechanism (in particular, the use of “α” and the subtraction of 1 in Algorithm 1) is not clearly distinguished from standard task-specific weighting approaches. The intuition behind these design choices is unclear, and one reviewer pointed out that a simpler method might suffice.\n\n2. Theoretical Justification:\n   • A major criticism repeated by the reviewers is the lack of theoretical justification or formal analysis. Although the method is intuitive, a top-tier conference would expect a stronger theoretical grounding or at least some rigorous discussion of its properties, limitations, and relation to prior work.\n   \n3. Experimental Evaluation:\n   • The experimental evaluation is performed on both computer vision (MultiMNIST) and NLU tasks. However, the empirical results are mixed. In several cases, the improvements over baselines are marginal or within the standard deviation, which raises concerns about statistical significance.\n   • For the MNIST experiment, the justification of the method’s superiority is undermined by the fact that it sometimes sets one task’s weight to 1.0 and the other to 0.0 — a behavior that is not well motivated since it should provide little advantage over single-task training.\n   • The experimental design is also critiqued for not adequately reflecting a truly multitask learning scenario: for the vision task, there appears to be only one auxiliary task, while the method is intended for scenarios with multiple auxiliary tasks.\n   • Key ablation studies and sensitivity analyses (e.g., evolution of task weights over the training trajectory and experiments with a noisy or randomly labeled auxiliary task) are missing, which further weakens the empirical support for the proposed method.\n\n4. Clarity and Presentation:\n   • Although the paper is generally well written and easy to follow, there are several style and presentation issues (e.g., formatting of citations, inconsistent terminology) that could be improved.\n   • More detailed discussions of the algorithm’s nuances—especially the separation between α and the task-specific weights—would help to clarify the contributions.\n\nFinal Decision: Reject\n\nWhile the paper presents an interesting direction by leveraging auxiliary task updates for dynamic task weighting, the lack of theoretical justification, unclear algorithmic novelty relative to known meta-learning approaches, and mixed experimental results lead to the conclusion that the paper does not yet meet the standards expected for a top-tier conference."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper demonstrates several positive aspects. It presents a well-engineered, open‐source platform for multi‐objective optimal experimental design and provides an intuitive GUI, which many users would appreciate. The focus on an asynchronous, multi‐objective Bayesian optimization platform is a timely contribution and addresses a real need in experimental design tasks. Additionally, the provision of real-world demonstration and multiple benchmarks underscores the practical relevance of the work.\n\nHowever, across all reviews there is a common concern regarding the scientific and methodological novelty. While the system itself is well implemented, the additional contribution in the form of the Believer-Penalizer (BP) strategy is not sufficiently justified theoretically or experimentally. Reviewers noted that the idea behind BP is simplistic, lacking both rigorous theoretical support and clear performance benefits over established techniques. The performance results are inconsistent and, in several benchmarks, the proposed method underperforms compared to alternatives. Moreover, the work is seen primarily as an assembly of existing methods into a software package rather than as advancing state-of-the-art methodology. Other critical points include misleading claims concerning the generality of the platform (e.g., support for m>=2 versus practical limitations to 2 or 3 objectives) and insufficient discussion of alternative approaches.\n\nFor a top-tier conference, the expectation is a stronger, more novel technical contribution coupled with thorough experimental validation. In this case, while the platform is useful from an engineering standpoint, the methodological novelty and rigorous performance validation do not meet the highest standards expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\nThe paper addresses a significant problem – overconfident predictions on OOD data – and proposes a simple post-hoc calibration method that is applied to deep image classifiers using a calibration set generated with corrupted (contrast‐corrupted) images. While it is important to study uncertainty and calibration under distribution shift, several issues affect the contribution and clarity of this paper.\n\nOne major concern raised by multiple reviewers is that the proposed method explicitly assumes that the OOD samples come from one of a small set of pre‐specified “corruption” distributions. This assumption makes the problem considerably less general (and arguably simpler) than the typical real-world OOD scenario where shifts may be unanticipated. Reviewer 1 called this assumption “fundamentally flawed” and noted that if the shifts are known in advance then more natural data augmentation approaches could be taken. This results in a contribution that is less compelling for the broader community interested in unpredictable shifts.\n\nAnother significant drawback is the experimental evaluation. Although the paper reports improvements over prior work (e.g., over results in Ovadia et al. (2019)) on artificially corrupted data, the generalizability to realistic OOD examples is not convincingly demonstrated. Both Reviewer 2 and Reviewer 3 recommended additional experiments on truly novel corruptions (for example, translations, rotations, or using an entirely different dataset like SVHN) and further comparisons with alternative calibration baselines (such as applying the recalibration procedure to clean validation sets). This shortfall undermines the overall impact of the empirical validation.\n\nFurthermore, presentation issues raised by Reviewer 3 and Reviewer 4—inadequate definitions, unclear notation (e.g., what exactly is p_max, or details about the calibration sets) and limited discussion on why contrast is a good choice for the calibration distribution—lower the clarity and reproducibility of the work. Given a top-tier conference, both innovative ideas and a clear, comprehensive presentation are expected.\n\nFinally, while reviewers 2 and 4 saw potential in the simplicity of the method and its reported effectiveness with contrast corruption calibration, the majority of the concerns regarding baselines, generality and clarity have not been fully addressed. The extra positive experiments do not completely mitigate the fundamental issue that the method relies on additional knowledge about the type of shift, something that is rarely available in true OOD scenarios.\n\nIn summary, despite tackling an important topic, the paper’s assumptions, lack of comprehensive experimental validation, and unclear presentation lead me to conclude that it does not meet the standards of novelty and general applicability expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis and final decision.\n\nDetailed Reasoning:\nThe paper introduces a biologically inspired architectural modification to standard RNNs, aiming to improve their ability to chain together learned motor motifs without interference in a continual learning setting. Although the idea of using a preparatory module (inspired by thalamocortical interactions) to force network states toward the origin is interesting and supported in part by neuroscience intuition, the overall contribution suffers from several issues:\n\n1. Novelty and Significance:\n• While the architectural modification is simple and somewhat inspired by biological circuits, several reviewers noted that the proposed mechanism is only marginally novel. Reviewer 2 and Reviewer 3 both cite that the contributions seem incremental compared to existing work (e.g., Schuessler et al. 2020) and do not demonstrate a clear leap over alternative approaches.\n• The work is framed in the context of continual learning; however, the scale of the experiments (using only 300 units and 10 motifs) and the synthetic nature of the task leave doubts about the broader relevance and significance for real-world systems.\n\n2. Experimental Evaluation and Alternative Hypotheses:\n• Several reviewers (Reviewers 3 and 5) criticize the lack of investigation of more direct and learned solutions to the motif chaining problem. The paper does not provide a comprehensive comparison to baseline approaches such as training RNNs on held-out transitions or employing trivial resetting mechanisms.\n• When critics suggested that a hard-reset baseline or approaches incorporating slight modifications during training might solve the problem, the authors provided no convincing experimental evidence dismissing these alternatives. This limits confidence in the claim that the imposed inductive bias is essential.\n\n3. Clarity and Presentation:\n• Reviewer 5 in particular mentioned that the presentation is confusing, with scattered details regarding the task, network initialization, and continual learning setup. The main text suffers from a lack of a clear, self-contained formalism for the optimization and learning problem.\n• Several minor writing and presentation issues (inconsistent notation, typos, and formatting problems) further detract from the paper’s clarity.\n\n4. Biological Inspiration:\n• Although the paper aims to link to neuroscience findings (i.e., thalamocortical interactions), the motivation for biasing the network states and the correspondence to brain mechanisms is not entirely convincing. At times, as Reviewer 3 noted, the neuroscience link appears forced and not strongly substantiated by experimental data or comparisons to more plausible biological models.\n\nOverall, while the idea of imposing an inductive bias to improve RNN chaining capabilities is appealing and the work shows some promising results, the limited scale of the experiments, lack of thorough validation against alternative methods, and issues with clarity and presentation undermine the paper's overall contribution. The majority of reviewers express significant concerns about the technical and empirical novelty, as well as the task’s broader relevance. Given the high standards expected at a top-tier conference, these issues collectively lead me to recommend rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by the final decision.\n\nOverall, the paper proposes an interesting idea—leveraging demonstrations to precondition a meta‐RL algorithm (PEARL-inspired) in order to address sample inefficiency in sparse reward and adaptation settings—which is an important problem. However, despite the promising empirical improvements and the potential value of integrating meta-IL and meta-RL in one framework, several major concerns arise:\n\n1. Clarity and Notation:  \n • Multiple reviewers note that the paper suffers from inconsistent notation and poor explanations. Key equations and loss terms (e.g., the auxiliary loss terms, the mutual information loss, the behavior cloning loss, etc.) are not consistently introduced or explained.  \n • The mixing of variables (for demonstrations vs. rollouts) makes interpretation confusing.  \n • The problem statement mixes the assumptions of the environment (e.g., fixed latent task vs. POMDP state) with modeling choices, leading to ambiguities.\n\n2. Complexity and Ablation Studies:  \n • The method has many moving parts, including six components in the overall loss function. Several reviewers question whether all those terms are necessary, and the experimental section lacks ablations that isolate the contribution of each component.  \n • It remains unclear if a simpler approach—e.g., simply seeding a PEARL-style method with demonstration data—could achieve similar performance.\n\n3. Comparison to Prior Work and Baselines:  \n • Several reviewers mention that the relationship to prior work (especially PEARL and meta-IL methods) is insufficiently clarified.  \n • Crucial comparisons to other methods that combine imitation and reinforcement learning are missing, making it difficult to assess the true value of the proposed modifications.  \n • Suggestions for additional baselines, such as using demonstrations at test time in plain PEARL or preloading buffers with expert trajectories, are not addressed.\n\n4. Experimental Details and Evaluation:  \n • The experimental evaluation does not provide enough evidence (e.g., statistical significance, multiple seeds, error bars) to convincingly support the claims.  \n • Some experimental design choices (e.g., the peculiar use of expert demonstrations, the necessity of sparse reward signals even in the presence of demonstrations, and handling of off-distribution tasks) are not fully justified in the text.\n\n5. Theoretical Justification and Formulation:  \n • Several equations seem to be derived in analogy to prior work (e.g., PEARL, Yu et al.) and yet do not clearly distinguish how the proposed modifications yield the claimed benefits.\n • Important derivation steps (such as the role of the KL loss in the variational model and the exact definition of the latent context) are glossed over or obscured by non-standard terminology.\n\nIn summary, although the approach is promising and addresses an important direction, the paper in its current form suffers from significant issues in clarity, formulation, experimental validation, and justification of various components. In a top-tier conference the standards for clarity, solid experimental design, and clear attributions to prior work are high. The current submission does not yet reach that level.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Contributions and Novelty:\n • The paper studies the implicit bias in classical value estimation (TD, FVI, RM) when the function approximation is overparameterized. In doing so it provides a unified interpretation in terms of norm-minimization under different constraints. At first glance, this is interesting because in modern deep RL the overparameterized regime is the status quo.\n • However, several results (e.g., the identification of the least‑norm solution corresponding to TD/FVI) are either known from convex optimization literature or are direct analogues of classical results in the underdetermined setting. A few theorems effectively repackage known ideas with only superficial novelty. Many reviewers expressed concern that the technical contributions are marginal.\n \n2. Theoretical Analysis and Correctness:\n • There are nontrivial aspects—such as a generalization error for the FVI iterate—but several derivations have apparent errors (e.g. issues in indices in recurrences, misuse of matrix decomposition conditions, and possible misapplication of concentration inequalities). Reviewer 6 and Reviewer 9 pointed out specific mistakes or lack of justification.\n • The proof techniques are, in some instances, not novel (being simple adaptations of least-norm results from the literature) and, when presented as main theoretical contributions, appear to lack sufficient novelty for top‑tier venues.\n \n3. Relation to Deep RL and Empirical Contribution:\n • The authors claim that their analysis motivates new regularizers for deep RL but the connection is somewhat tenuous. In modern deep RL the entire network (not merely the last layer) is jointly adapted, and the overparameterization phenomena have been extensively explored by the NTK literature. Several reviewers have questioned why a linear overparameterized formulation is a good surrogate for deep models.\n • The experimental section is carried out on toy domains. While some regularization effects are observed, these experiments are not very well motivated and do not clearly demonstrate that the insights scale to practical deep RL environments.\n \n4. Writing and Attribution Issues:\n • Multiple reviewers noted that several derivations are written in ways that do not properly credit prior work or that contain mistakes (e.g., trivial statements being presented as new theorems, errors in equations, and unclear notation). Such presentation issues detract from the overall clarity and impact of the paper.\n \n5. Overall Impact:\n • Although the paper addresses an important topic by trying to understand the implicit bias of different value estimation methods in regimes that mirror deep learning practice, the contributions mostly revolve around the linear case and are accompanied by technical and presentational issues. The gap between the theoretical analysis in the linear setting and the intended application to modern deep RL is not convincingly bridged in either theory or convincing experiments.\n \nConsidering the limited technical novelty, errors in some derivations, issues with attribution, and a weak empirical demonstration relative to the claims, I conclude that the paper, in its current state, does not meet the rigorous standards expected at a top‑tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed analysis:\n\nStrengths:\n• The paper makes an interesting theoretical contribution by linking the approximation error of one-hidden-layer ReLU neural networks to the Hausdorff distance of their tropical zonotopes. The geometrical viewpoint appears novel and may stimulate further exploration in the intersection of tropical geometry and neural network analysis.\n• The theoretical results, particularly Theorem 2, are clearly stated and mathematically non-trivial. This innovative use of tropical geometry in a neural network compression setting is commendable.\n• Some empirical proof‐of-concept experiments indicate promise, and the comparisons to a tropical baseline (Smyrnis & Maragos) show improvements in test accuracy. The introductory exposition on tropical geometry also makes the work accessible to researchers unfamiliar with that framework.\n\nWeaknesses:\n• The experimental evaluation is quite limited. The empirical study is performed on small datasets (MNIST, Fashion-MNIST, CIFAR-10/100) and older architectures (e.g., LeNet and VGG). Since a central claim is that the proposed method is competitive and even competitive with modern pruning techniques, the absence of experiments on modern architectures (e.g., ResNets or Transformers) casts doubt on the practical relevance.\n• There is a lack of clarity on several implementation details. For instance, it is not clear whether the algorithm is applied solely to fully-connected layers in experiments involving CNNs, and the description of some experimental settings (e.g., what “deepNN” exactly refers to, or whether convolutional layers are also pruned) needs improvement.\n• Computational aspects such as memory requirements, runtime, and scalability are not discussed. A top-tier conference expects thorough analysis of the trade-offs involved, especially when proposing methods based on non-standard metrics like tropical geometry.\n• The paper also overclaims in its abstract by stating competitive performance against modern pruning methods without providing evidence on state-of-the-art benchmarks.\n• Several reviewers pointed out that some of the error bounds (e.g., in Propositions 4 and 5) appear counter-intuitive or not practically useful, which further reduces the confidence in the practical implications of the theoretical results.\n\nOverall Assessment:\nWhile the theoretical contributions are interesting and the novel perspective could spur new research avenues, the experimental evaluation does not convincingly demonstrate the practical utility or scalability of the method. For a top-tier conference the work should either provide more compelling evidence—including experiments on modern architectures with detailed runtime and complexity analyses—or clearly position itself as a primarily theoretical contribution with a limited proof-of-concept experiment, with a focused discussion on its limitations. The current version leaves several crucial points unanswered, and the overclaims in the abstract further weaken the paper's impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning.\n\nStrengths:\n• The paper studies an interesting variant of double descent—epoch‐wise double descent—by analyzing a simple teacher–student linear model. The insight that multiple feature scales could explain non-monotonic behavior in test error over training time is novel and adds to our understanding of generalization dynamics.\n• The theoretical analysis is carried out using closed‐form derivations via the replica method. This analytical tractability helps create intuition for the behavior seen in deep neural networks.\n• The paper demonstrates a qualitative match between its predictions and the behavior observed in a ResNet-18 on CIFAR10, suggesting that despite using a simplified model, the derived mechanisms could be more generally applicable.\n• Overall, all reviewers acknowledge that the work captures an interesting phenomenon with clear presentation of theory and numerical validation.\n\nWeaknesses and concerns:\n• Multiple reviewers raised the need for a deeper discussion of differences between epoch‐wise and model‐size double descent and how the phenomenon may extend when more than two scales are present (e.g., evidence or discussion on possible “triple descent”). Clarifying these aspects would strengthen the manuscript.\n• Several connections to prior literature are missing or underemphasized (for example, work on ridge-like implicit regularization and high-dimensional interpolation). Both theoretical analyses and empirical parallels to earlier works (e.g. by Tibshirani and coauthors, as well as connections to recently discovered ridgeless regression double descent) should be discussed.\n• Reviewer 3 noted numerous typos, label mismatches in figures, and some inconsistencies in notation (e.g., transitions in equations and unclear treatment of sources of randomness) that detract from the clarity and accessibility of the derivations. This raises concern about reproducibility and the ease with which readers can follow the derivations—even though the underlying approach appears sound.\n• While the paper’s qualitative matching with deep networks is promising, more in-depth discussion or evidence linking the simple model’s multi-scale learning dynamics (especially in the context of SGD noise, anisotropic data, and label noise) to practical deep network behavior would further bolster the contributions.\n \nOverall Assessment:\nThe paper provides a valuable theoretical perspective by showing how epoch‐wise double descent emerges from differences in feature learning speeds. The novel angle—analyzing this phenomenon in a simple yet analytically tractable setting—is of interest to the community. Though the contribution is partially incremental given the related literature in double descent and implicit regularization, the insights presented here can motivate further work. The issues regarding connections to previous work, notational clarity, and numerous typographical and labeling errors are important but appear addressable through revisions.\n\nGiven that the contributions are interesting and the weaknesses seem fixable (in a revised version addressing clarity, rigorous placement within the extant literature, and correcting typos), I lean toward acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents MILAN, a procedure for generating open-ended, natural language descriptions of individual neurons in deep vision models. The approach leverages mutual information between neuron activations and natural language descriptions to capture a spectrum of features—from low-level edges to high-level semantic descriptions. The experiments are thorough and span multiple model architectures, datasets, and even application scenarios (such as auditing for spurious correlations and neuron editing). This breadth of application strengthens the work’s empirical significance.\n\nReviewers generally found the work well-motivated and technically sound. Several points praised the clear experimental design, comprehensive testing, and the potential utility of the method in interpreting and controlling deep models. While technical novelty might be considered incremental given related prior work on neuron interpretability, the contribution is significant enough in terms of the practical applications and the insights provided into model behavior.\n\nSome concerns were raised regarding the quality control of the newly collected MILANNOTATIONS dataset (specifically regarding inter-annotator consistency) and questions about extending the approach beyond the current image domain. However, these issues appear to be addressable in revisions or future work rather than fundamentally undermining the paper's contributions. Minor comments on figure references and annotation details are not major roadblocks and can be clarified with some additional discussion.\n\nOverall, the paper’s strengths—in its clear exposition, significant empirical analysis, and useful applications for model interpretability—outweigh the noted limitations. The work meets the standards expected at a top-tier conference, and its open-source release of data, code, and models further supports its impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper tackles the important problem of generating multivariate time series using a VAE framework and proposes an architecture that claims to offer interpretability, domain knowledge integration, and faster training times. However, the reviewers collectively identify several critical shortcomings. First, although the paper is well written and the proposed idea is interesting, its novelty is marginal since many of the components resemble standard VAE design choices with only modest architectural modifications. Several reviewers question whether the method significantly departs from a standard VAE, and one specifically notes that incorporating convolutional layers without more specialized recurrent or autoregressive elements makes the contribution less distinctive.\n\nSecond, the empirical evaluation raises concerns. There are issues regarding the choice and explanation of hyper-parameters related to the new decoder building blocks. The paper lacks thorough ablation studies or guidance on hyper-parameter selection, which undermines confidence in the claims about the effectiveness of the interpretability features. In addition, while the authors conduct experiments on four datasets, some of these are synthetic or novel, making it difficult to benchmark against existing work. Several reviewers pointed out that the reported improvements, particularly in terms of the discriminator score and interpretability, are either ill-explained or not convincingly demonstrated.\n\nThird, the paper’s literature review and methodological exposition fall short. There is a lack of context regarding related methods, and many technical claims are not well-supported or sufficiently precise. Reviewers also suggest that the experimental design could be significantly improved by including more robust comparisons (for example, with autoregressive or state-space models) and by providing a clearer evaluation of the interpretability aspect, which is touted as a major contribution.\n\nFinally, while some reviewers acknowledged some strengths (such as clarity of writing and the general appeal of incorporating domain-specific insights), the consensus is that the contributions are only marginally significant without strong evidence to validate the claimed benefits.\n\nGiven all of these issues—the limited novelty, insufficient empirical validation (especially regarding interpretability), and lack of detailed methodological support—the paper, in its current form, is not ready for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed analysis:\n\nStrengths:\n• The paper is very well‐written, and some reviewers (e.g., Reviewer 3 and Reviewer 5) appreciated the clarity of the presentation and the detailed explanations of the model’s operation.\n• The idea of interpreting deep convolutional decoders as cellular automata is intriguing, and the work does provide some discussion on the self-organizing behavior and robustness properties that the NCA decoder may offer.\n• The experimental sections include several datasets, and the damage/perturbation experiments are interesting from a theoretical perspective.\n\nWeaknesses:\n• Multiple reviewers noted that the experimental results are underwhelming. In particular, the generative performance—both in terms of likelihood numbers and the quality of generated samples—is far behind state-of-the-art methods. Several reviewers questioned the practical significance of the robustness to early-stage perturbations.\n• There is a recurring concern regarding novelty. The core idea of using a repeated (weight‐tied) deep convolutional net as a VAE decoder is almost equivalent to what one would construct with standard upsampling networks, and the NCA interpretation seems to offer little concrete advantage. Reviewer 6, for example, argued that many of the findings are not surprising once one acknowledges that the decoder is essentially a deep convolutional network.\n• Several technical issues and architectural design choices remain underexplored. There are questions regarding the necessity of certain design elements (e.g., the upsampling step) and an overall lack of discussion on what is gained compared to traditional architectures.\n• The empirical contributions, though having some merit in terms of qualitative analysis (e.g., robustness and the “morphogenesis” process), do not convincingly support the claim of creating a fundamentally new or superior generative mechanism.\n\nOverall Assessment:\nWhile the paper offers an interesting perspective by bridging NCA ideas with variational autoencoders, its practical performance is noticeably below current state-of-the-art standards. Additionally, the novelty appears marginal—the core architecture largely corresponds to a deep convnet with repeated blocks, and many of its behaviors can be explained by known properties of such networks. Given that many reviewers flagged the experimental shortcomings and limited novel contributions, the paper does not meet the bar for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by my final decision.\n\nReasoning:\n• The paper tackles an interesting and relevant problem—the “greedy” use of only a single modality during multi-modal learning—and proposes metrics (conditional utilization rate and conditional learning speed) along with a corresponding training algorithm (balanced multi-modal learning). This line of inquiry is certainly of interest to the community.\n\n• However, multiple reviewers raised concerns about the strength and evidentiary support behind the major claims. In particular, several reviewers pointed out that the experiments do not consistently or convincingly validate that biased modality reliance hurts generalization, as results on realistic datasets (e.g., Princeton ModelNet40 and NVGesture) are either not statistically significant or lack detailed analysis. Similarly, the empirical improvements over simpler baselines are not demonstrated strongly enough.\n\n• There is also ambiguity regarding some key aspects of the methodology. Reviewer 5 and Reviewer 7 specifically questioned the derivation of uni-modal predictions from the multi-modal architecture and the clarity of the re-balancing procedure. Reviewer 2 noted inconsistencies in reported numbers, and Reviewer 3 urged for comparisons with SOTA methods on naturally multi-modal (e.g., audio–visual) data to justify the claims.\n\n• While one reviewer (Reviewer 7) gave a weak accept and Reviewer 5 was generally positive, the majority (and the concerns expressed by Reviewer 2, Reviewer 3, Reviewer 4, and Reviewer 6) indicate that the theoretical and experimental contributions, including concrete evidence supporting the central claims, are not sufficiently convincing for a top-tier venue.\n\n• Overall, although the problem is interesting and the paper shows some potential, the empirical support is mixed and the methodological presentation has several unresolved issues. The paper lacks the rigorous analyses and thorough experimental validations expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper addresses the important problem of bridging theory and practice in meta-learning by “enforcing” theoretical assumptions via regularization. While the paper is well-written and its motivation is interesting, several major concerns prevent it from reaching the standard of a top-tier conference.\n\nFirstly, a substantial part of the reviewers’ feedback pertains to novelty. Multiple reviewers pointed out that the proposed regularization methods are either very similar to known techniques (e.g., spectral normalization and standard weight decay) or simply restate previously known approaches under a new name. The paper’s claim of “putting theory to work” is undermined by the fact that the proposed regularizers do not significantly depart from established practices—even if they are theoretically motivated.\n\nSecondly, there are unresolved theoretical and experimental issues. Reviewer 2 noted that the theoretical setting discussed in the paper is not entirely consistent with the typical few-shot learning setup, raising questions about how the differences in assumptions (such as the absence of a validation set, the treatment of task distributions, and distinctions in the meta-learning objective) affect performance. Similarly, Reviewer 3 expressed concerns that the empirical improvements are marginal—often comparable to the reported standard deviation—and lacked clear evidence that the additional complexity is justified, especially when the assumptions might not hold in realistic settings. Reviewer 4 also raised a question about the practicality of the regularizers when applied to more complex models, further complicating the impact of the work.\n\nMoreover, the experimental evaluation, while promising, does not compare against a sufficiently rich set of recent methods, and the details of the proposed gradient computations (especially concerning singular values) are not fully provided. This leaves a gap in the method’s reproducibility for a broader audience and casts doubt on both the theoretical contribution and practical implementation of the work.\n\nIn summary, although integrating theoretical insights into meta-learning is a worthwhile direction, the lack of significant novelty in the proposed methods, combined with incomplete theoretical-experimental alignment and insufficient empirical validation, leads to the conclusion that this paper does not meet the threshold for acceptance in a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\nStrengths:\n• The paper addresses an important problem—synthesizing data using deep generative models under differential privacy constraints—and offers an interesting “one-shot” privatization that avoids the gradual privacy degradation seen in gradient sanitization techniques.\n• The theoretical formulation is clearly presented and novel elements (e.g., the use of characteristic functions and adversarial re-weighting) make the framework conceptually appealing.\n• In the preliminary empirical results the method shows promise compared to a few contemporary approaches like DP-MERF and DP-GAN.\n\nWeaknesses:\n• The experimental evaluation is limited. Multiple reviewers noted that the benchmarking is not thorough enough. For tabular data the method needs comparisons with additional baselines (e.g., PATE-GAN, CTGAN, PrivBayes, etc.) and more datasets, and for image data at least a somewhat more challenging dataset (such as CIFAR-10) should be included.\n• Several details critical to assess the integrity and practical performance are unclear. In particular, reviewers expressed confusion about how auxiliary information is handled and about the sensitivity calculation (the introduction of factor k from l2 norms requires more explanation).\n• There are concerns regarding the scalability and complexity analysis of the proposed algorithm. At a top-tier venue, a more in-depth analysis of the computational complexity and an empirical exploration over a wider variety of tasks would be necessary.\n• The presentation needs improvements (e.g., moving important algorithmic details into the main text rather than the appendix) and the paper should include more discussion of related work to adequately situate the contribution among recent approaches.\n\nOverall, while the idea is promising and the framework offers an interesting alternative to traditional iterative gradient sanitization methods, the current submission falls short in terms of empirical evaluation and clarity on some fundamental components. Given the strict standards of a top-tier conference, there are too many unresolved details and insufficient comparisons that prevent a full assessment of the method’s practical impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contributions:\n • Although the paper combines several ideas that have been explored in prior literature (e.g., label prior shift, importance sampling, post-compensation softmax), the proposed framework simply integrates these components for online knowledge distillation. Several reviewers remarked that the technical novelty is marginal, with contributions only incrementally significant over existing work.\n • The paper does bring forward the idea of enforcing teacher specialization to promote diversity, but the integration does not provide a strong enough theoretical or conceptual advance compared to the state of the art.\n\n2. Experimental Analysis:\n • While the empirical section is extensive and ablation studies are provided, many reviewers raised concerns that the experimental gains are minimal. In certain cases the proposed method achieved only small improvements (or even inferior performance) compared with baseline approaches such as PCL or other state-of-the-art methods.\n • Reviewer comments also noted inconsistencies in reproducing baseline results (e.g., MVCL), suggesting that the experimental evaluation did not fully validate the effectiveness of the proposed approach.\n • Reviewer suggestions to include comparisons on CIFAR with more recent related works (e.g., CGL, PCL, L-MCL, and further comparisons with approaches that specialize teachers) indicate that the evaluations fall short of top-tier standards in terms of comprehensiveness and strength.\n\n3. Clarity and Reproducibility:\n • Overall, the paper is mostly clear and sufficiently organized. There is enough detail in the exposition and experimental sections to enable reproducibility.\n • However, there are several typos and minor writing issues that distract from its overall quality, although these are not fatal to the contribution.\n\n4. Overall Assessment:\n • There is a mixed assessment: while the idea of enriching teacher diversity is promising, the contribution as presented relies on a combination of existing techniques with only marginal novelty.\n • Experimental results and comparisons are not thoroughly convincing, with some key baselines omitted or underperforming relative to state-of-the-art methods. The reviewers’ aggregated concerns regarding reproducibility of baseline results and insufficient novelty pull the overall assessment downward.\n • For a top-tier conference, both technical and empirical contributions are expected to be significantly stronger than what is presented.\n\nBased on these considerations, the paper does not meet the high standards required for acceptance in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces an innovative connection between the expressivity of ReLU networks and Sharkovsky’s theorem from dynamical systems, extending previous work by Telgarsky. Its theoretical contributions not only provide new lower bounds for the width needed to represent functions with periodic points as a function of depth but also give insights into why deeper networks may have a representational advantage compared to shallower ones. The use of dynamical systems concepts and eigenvalue analysis is both creative and well executed.\n\nReviewer 1 found the work exemplary and appreciated the thoroughness of the response and experimental inclusion (such as the synthetic dataset). Reviewer 2 strongly endorsed the paper, appreciating its clarity and intuitive presentation of a complex subject. Their questions regarding the extension to networks with bias terms and the interpretation of the sharpness of the lower bounds only serve to underscore the potential for further extensions, without detracting from the primary contribution. Reviewer 3 praised the technical depth and originality of the approach, although noting that the paper’s structure could be improved for better clarity and applicability. Overall, Reviewer 3’s minor concerns regarding presentation and practical applicability do not outweigh the strong theoretical contributions.\n\nGiven the robust positive feedback, the novelty of the theoretical insights, and the correctness and clarity of the proofs (despite some presentation improvements suggested), this paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper under review suffers from numerous critical issues that make it unsuitable for acceptance at a top-tier conference. Firstly, the writing quality is extremely poor, with pervasive grammatical and clarity deficiencies. Multiple reviewers noted that the paper is incomprehensible, making it nearly impossible to grasp the core contributions and the technical details of the proposed method. The abstract, intended to provide clarity on the work, is vague, with ambiguous statements such as “the coordination of episode generation was focused on,” which leave the reader uncertain about the significance and novelty of the approach.\n\nIn addition to the writing quality, the paper is lacking in academic rigor. It does not reference prior work or include related research, which is a basic requirement for contextualizing contributions within the existing literature. This omission not only diminishes the credibility of the study but also means that key comparisons or discussions with established methods like AlphaZero are missing.\n\nFurthermore, the method described appears to be evaluated on a very small problem domain without a detailed explanation of the algorithmic design or thorough experimental analysis. The lack of clear definitions for mathematical symbols and ambiguous algorithm descriptions only exacerbates the issue, suggesting that the work is more a set of undeveloped notes than a fully-formed research contribution.\n\nGiven that top-tier conferences maintain high standards in clarity, novelty, experimental validation, and proper academic referencing, this paper does not meet the necessary criteria. The reviewers unanimously identified significant issues related to both the technical substance and presentation of the work, indicating that extensive revision would be required for even a preliminary resubmission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Technical Contribution:\n• The paper proposes a multi-objective formulation for self‐play where both skill and playing style diversity are optimized. However, although the formulation is interesting, several reviewers have noted that the technical contribution is largely incremental. Reviewer 1 and Reviewer 3 both remark that the novelty is only marginal. The approach builds on NSGA-II-style ideas applied to a self-play setting, a concept already well explored in quality-diversity and related multi-agent reinforcement learning literature.\n• In addition, reducing playing style to a single scalar is seen by Reviewer 2 as a major simplification that should be more thoroughly discussed or justified. This design choice may limit the scope of the algorithm when applied to games with richer strategic dimensions.\n\n2. Related Work and Baselines:\n• Reviewer 1 raises a serious concern regarding the lack of comparison to established self-play baselines such as those described in Lanctot et al. (2017). Additionally, several baselines known for maintaining diverse policies (e.g., quality-diversity approaches like MAP-Elites and algorithms used in AlphaStar) are not adequately discussed or included.\n• Reviewer 3 explicitly points out that the experimental study omits comparisons to quality-diversity methods, which are directly relevant. This omission weakens the empirical validation since it remains unclear whether the proposed method adds significant value over existing approaches.\n\n3. Experimental Design and Empirical Evaluation:\n• While experiments in Pong and Justice Online show some promising results, several experimental design issues are noted. For instance, Reviewer 1 points out potential biases in the evaluation due to unbalanced sampling from the pool of opponents.\n• Reviewer 2 and Reviewer 3 also call for deeper experimental details such as the statistical significance of the results (confidence intervals, number of runs), possible issues with cherry-picking results, and a broader range of test domains. In particular, Reviewer 3 suggests that experiments in additional benchmark environments (or using simpler baseline augmentations like state randomization) would greatly solidify claims of generalization.\n• The limited experimental domains cast some doubt on the robustness and generality of the contributions.\n\n4. Overall Impression:\n• Although the idea of explicitly maintaining diversity in playing styles during self-play is interesting, the paper in its current form does not sufficiently establish a clear advantage over strong existing methods in both theory and experimental evidence.\n• The missing baselines and seemingly unbalanced evaluation methodology undermine the overall confidence in the claims. Combined with the largely incremental technical novelty, this makes it hard to justify acceptance for a top-tier conference.\n\nBased on these factors, my final decision is that the paper should be rejected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is the detailed reasoning based on the provided reviews and author responses:\n\n1. Novelty and Contribution:\n • The paper introduces a progressive learning strategy for hierarchical representation and disentanglement in VAEs that builds on a ladder architecture (VLAE) and employs techniques inspired by “starting small” and progressive GANs. \n • Compared to related works, the authors clearly articulate the differences, and—despite reviewer concerns regarding overlaps—the rebuttal and additional experiments help delineate its contributions.\n\n2. Experimental Validation:\n • The experiments on two benchmark datasets using multiple disentanglement metrics (including the newly proposed one) are thorough. \n • Reviewer 1’s concerns about the first layer's training and comparisons to VLAE are addressed by new experiments (e.g., Figure 10 and information flow experiments) that offer both qualitative and quantitative evidence.\n • Although reviewer 2 noted some issues regarding purpose clarity and comparisons, the subsequent author responses and additional experiments have improved the clarity and overall performance relative to the base model.\n • Suggestions for further ablations (e.g., on fade-out vs. full incremental learning) remain, but these do not detract from the convincing experimental demonstrations already provided.\n\n3. Theoretical Clarity and Writing:\n • The paper is generally well-explained and motivated although a few points (such as further formalization in Section 3.1) need refinement. However, these are not core issues that undermine the work’s contribution.\n • The presentation and justification of the new metric (MIG-sup) are mostly clear; minor notational issues (like the undefined v_k) need to be addressed, but they do not fundamentally impede understanding.\n\n4. Overall Assessment from Reviewers:\n • Reviewer 1, after revision and new experimental evidence, raises their score to 6 (Weak Accept).\n • Reviewer 3 and Reviewer 4 express overall positive views, emphasizing the novelty, experimental validation, and promising direction of the proposed method.\n • Although Reviewer 2 maintained concerns about clarity and relationships between hierarchical and disentangled representations, these issues have been addressed to a large extent in the rebuttal and additional experiments. The remaining comments are seen as improvements for future iterations rather than fundamental flaws.\n\nGiven the overall positive revision outcomes, satisfactory experimental validation, and a strong response to concerns raised in the reviews, the paper meets the standards expected at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Detailed Reasoning:\n\nThe paper introduces an interesting idea by proposing a “multibranch” network where different domains are processed through possibly different computational paths for unsupervised domain adaptation. The novelty lies in the concept of “adaptive computation” dedicated to each domain, which could in principle lead to improved performance when dealing with heterogeneous domains. Additionally, the approach can potentially be extended to multi-source or multi-task settings—a promising direction for future work.\n\nHowever, several significant concerns have been raised by the reviewers that weaken the overall contribution:\n\n1. Experimental Evaluation:\n • The experiments are limited to comparing the proposed method only against baselines such as DANN and RPT, which are not state-of-the-art in unsupervised domain adaptation. Reviewer 3 explicitly recommends comparing with newer and stronger methods (e.g., CDAN) to truly gauge the benefits of adaptive parameter sharing versus more conventional feature alignment techniques.\n • There is a lack of thorough ablation studies and hyperparameter sensitivity analyses. Questions about the number of parallel branches (i.e., the parameter K), the configuration of computational units, and the behavior of the gating mechanism remain unanswered.\n • The experimental setup does not fully reflect the motivation of the paper; for example, all domains undergo the same number of computations (layers), which contradicts the proposed advantage of adaptive computation paths. Furthermore, observations like the parameter-sharing behavior noted in Figure 4 are counter-intuitive and remain insufficiently explained.\n\n2. Technical and Presentation Issues:\n • Some naming conventions (e.g., using the term “multiflow” network) may be confusing or conflict with established terminology in other fields.\n • Reviewer 1 noted minor issues regarding references and the use of gray literature, which, although minor, detract from the overall professionalism expected at a top-tier conference.\n\nWhile the idea itself is intriguing and the paper is generally well written, the experimental support is incomplete and the analysis falls short of convincingly demonstrating that the claimed advantages translate into consistent improvements, especially when compared with current state-of-the-art approaches.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a deep neural approach for modeling continuous time-evolving graphs, leveraging a temporal point process framework with a stacked LSTM architecture enhanced by time gates and an attention-based selection mechanism. While the experimental results appear promising across several tasks and datasets—including a real-world financial application—the reviewers have raised several substantial concerns that undermine the overall impact and rigor of the work.\n\nFirst, regarding novelty, most of the individual components (using recurrent networks for intensity functions, stacked LSTM for multi-scale temporal modeling, and attention mechanisms) are already present in the literature. Although the specific combination and the introduction of a “time gate” and adaptive selection mechanism bring some new flavors to the approach, these contributions are incremental at best. Moreover, the novelty claim is weakened by the lack of detailed justification or analysis showing how these modules capture the data characteristics or improve interpretability in a non-trivial way.\n\nSecond, the empirical evaluation and ablation studies leave several questions unanswered. The design choices (e.g., the choice of varying K in the subgraph extraction and the roles of additional parameters such as scaling and alpha) are not sufficiently explored. The ablation studies, while indicating that each component improves performance, do not offer a granular analysis of how each design decision contributes to the final results. Additionally, the model comparison is incomplete—criticisms include the omission of an important baseline (JODIE) and the lack of comparisons with parameter-matched shallow models, which casts doubts on whether the performance improvements are primarily due to a deeper architecture with more parameters rather than the proposed innovations.\n\nThird, presentation and clarity issues further detract from the paper. Several methodological details are underspecified: it is unclear how the approach deals with graphs that change at uneven speeds, how the processing of dynamic node updates occurs in practice, and why certain algorithmic choices (e.g., the use of a coloring and hashing mechanism instead of a simpler subgraph collection method) were made. Reviewer 3’s comments indicate that an overview diagram or a toy example would significantly enhance readability, implying that the current exposition is too dense and not well motivated.\n\nFinally, there are issues with the formulation of key components such as the log-likelihood function for the temporal point process. Such formulation discrepancies leave doubts about the technical correctness and implementation details, which are central to the contribution of the paper.\n\nOverall, while the direction of combining TPPs with dynamic graph neural models is promising and the experimental results are encouraging, the paper suffers from insufficient novelty, lack of thorough methodological analysis, incomplete experimental comparisons, and clarity issues in presentation. These shortcomings suggest that the paper is not mature enough for acceptance at a top-tier conference in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n1. Novelty and Significance: All reviewers agree that the paper’s main idea—combining Monte-Carlo Tree Search with a Transformer—is interesting but offers limited novelty. Reviewers point out that this combination bears strong resemblance to established approaches like AlphaGo/AlphaZero, with the only difference being replacing a ResNet with a Transformer. This key weakness undermines the technical contribution and overall significance of the work.\n\n2. Empirical Evidence and Evaluation: The evaluation is conducted solely on SameGame, which is a rather toy-like environment. Reviewers note that the experimental setup is non-standard and that the domain choice limits the generalizability of the results. In addition, comparisons lack sufficient breadth: baselines such as Decision Transformers, other neural net–aided MCTS methods, and common offline RL benchmarks are missing. This omission makes it difficult to assess the empirical claims and the practical impact of the proposed method.\n\n3. Clarity and Presentation: Reviewer 1 and Reviewer 3 highlighted numerous clarity issues, including poor exposition, small figure fonts, lack of clear descriptions for key steps (e.g., rollout phase and details of MCTS), and missing details about the experimental setup. Such issues detract from the paper’s accessibility and reproducibility.\n\n4. Overall Impact: Although the idea of using Transformers to potentially enhance offline RL via MCTS integration is promising, the current work suffers from weak empirical evaluation, limited domain testing, and only incremental technical innovation. In its present form, the paper does not meet the standards expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning regarding the paper titled “Detecting Out-of-Distribution Data with Semi-supervised Graph ‘Feature’ Networks.”\n\nOverall, the key idea—leveraging semantic graphs generated from pre-trained object detectors for OOD detection—is interesting and potentially points toward an interpretable mechanism. However, several critical issues affect the suitability of this paper for a top-tier conference:\n\n1. Clarity and Writing:  \n   Multiple reviewers noted that the paper’s writing is poor with grammatical mistakes, unclear notations (e.g., inconsistent use of “Near OOD” and “Far OOD”), and missing details (e.g., a complete illustration of the inference pipeline). The lack of a clear, detailed description of the feature extraction procedure, including an example that takes the reader through the complete process, diminishes the paper’s clarity and overall quality.\n\n2. Methodological Description and Reproducibility:  \n   The paper relies heavily on the availability of a pre-trained and ideally capable object detection network. However, there is insufficient discussion on how the accuracy or limitations of the object detector might impact performance. Additionally, critical design choices (such as the graph construction and its ablation) are under-explained, with insufficient details provided to allow reproduction of the results.\n\n3. Technical Novelty and Comparison to State-of-the-Art:  \n   While the idea of using semantic graph structures is acknowledged as a promising direction, the paper appears to combine several existing techniques rather than introducing a fundamentally novel approach. Moreover, comparisons with true state-of-the-art OOD detection methods and baselines (which typically use embedding-based methods on a variety of datasets like CIFAR/ImageNet) are either missing or inadequate. Most experiments are limited to the LSUN dataset, and broader comparisons would be necessary for a top-tier conference submission.\n\n4. Experimental Evaluation:  \n   The experimental section appears weak relative to the claimed performance. Reviewers pointed out the absence of improvement studies (e.g., ablation studies on hyperparameters and analyses of the semantic graph’s role) and comparisons with successful OOD detection frameworks in current literature. This gap makes it hard to convince the reader that the framework is overall competitive or that its benefits are significant.\n\nIn summary, while the core idea to incorporate graph-based semantics for OOD detection may have potential, the current state of the paper does not adhere to the rigorous standards expected at a top-tier conference. The presentation, experimental comparisons, methodological clarity, and overall reproducibility need substantial improvement.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Detailed Reasoning:\nThe paper presents Deep Coherent Exploration, a method that effectively integrates step-based and trajectory-based exploration for continuous control in deep reinforcement learning. By modeling the last layer parameters as latent variables and using a recursive integration approach, the authors succeed in extending ideas from previous work (van Hoof et al., 2017) to deep neural networks. This extension enables more stable and faster learning for on-policy methods such as A2C and PPO, as confirmed by strong empirical results and thorough ablation experiments. The mathematical derivations and detailed appendix build confidence in the reproducibility of the method.\n\nSeveral reviewers pointed out that while the performance gains for on-policy learning are significant, the method's advantages for off-policy algorithms like SAC are less pronounced and sometimes even detrimental. Nonetheless, the overall contribution—scaling the framework of coherent exploration to deep RL—is of high interest to the community. The paper is well written overall, though some parts (e.g., Section 3.2) could be more accessible and the experimental evaluation might benefit from additional domains beyond Mujoco. However, these concerns do not overshadow the contribution, as the authors provide clear evidence of the efficacy of their approach in an important subset of continuous control tasks.\n\nGiven that the method offers a scalable and mathematically sound exploration framework with notable empirical benefits for on-policy methods, and considering that the shortcomings regarding off-policy performance are well acknowledged and can be framed as a limitation and inspiration for future work, the paper represents a valuable contribution for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper reports an interesting empirical phenomenon: that swapping the order of batch normalization and bounded activation functions (placing BN after the activation) yields better performance than the conventional ordering when using bounded nonlinearities (e.g., Tanh). The authors further attempt to explain this through analysis of saturation, asymmetric saturation, and induced sparsity. However, several concerns arise from the reviews. Many reviewers point out that the paper’s claims are not sufficiently substantiated—especially regarding whether asymmetric saturation and the resulting sparsity are truly causative factors in the generalization improvements. The definitions (e.g., “center of the function”) and certain derivations are vague or not rigorously supported. Additionally, although the swapped order with bounded activations outperforms the conventional order, none of the experiments demonstrate that such swap can make a bounded-activation network surpass standard networks using ReLU, which limits the practical impact of the contribution.\n\nOther issues include:\n• Incomplete experimental analysis – several reviewers request further clarification on layerwise skewness/saturation behavior, comparisons with additional architectures (e.g., residual networks or MLPs), and analyzing potential effects from training hyperparameters.\n• Limited generality – since the analysis is restricted to networks without residual connections and mainly on VGG-like models, it is difficult to generalize to commonly used architectures in practice.\n• Clarity and rigor – several reviewers noted that some arguments come off as contradictory or are not rigorously defined, and the proposed metrics (for saturation, skewness, sparsity) need clearer justification.\n\nWhile the empirical observations are intriguing, in a top-tier conference the paper is expected to provide more convincing evidence linking the phenomenon to generalization improvements and to broaden the analysis either theoretically or across more architectures. Given the issues raised (ambiguities in explanation, limited experimental coverage, and lack of demonstrated practical advantage over standard methods), the conclusion is that the paper does not meet the rigorous standards expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces an innovative framework—Continuous Query Decomposition (CQD)—for answering complex queries over incomplete knowledge graphs by leveraging pre-trained neural link predictors and formulating complex queries as differentiable objectives. The idea of decomposing complex queries into optimization problems, which can be solved either through continuous gradient‐based optimization or combinatorial search, is compelling. This approach avoids the need to train on a large set of complex queries, relying instead on training only on simple 1-hop queries and then generalizing to multi-hop queries. The paper provides extensive experimental evidence where CQD improves Hits@3 by 8% to 40% over state-of-the-art methods while using orders of magnitude less training data. Additionally, the method offers interpretability by showing intermediate reasoning steps.\n\nThe reviews are generally positive. Reviewer 2 and Reviewer 5 emphasize the practical significance and elegance of the approach, noting both its effectiveness and its novel use of fuzzy logic with entity embeddings. Reviewer 3 acknowledges that the paper is clearly written and scientifically sound, even while suggesting a few clarifications regarding differences from previous works and the time complexity trade-offs. Reviewer 4 also commends both the simplicity and the practical aspects of the solution, while raising concerns over excessive formalism and runtime details, which can be addressed in revisions.\n\nAlthough several reviewers raised minor concerns—such as the need for additional ablation studies comparing different neural link predictors (e.g., ComplEx versus TransE), more detailed discussion about time complexity (especially in the combinatorial search variant), and clarifications on certain notations and experimental details—these are not fundamental flaws. Rather, they reflect areas for further clarification and refinement in the final version of the manuscript.\n\nOverall, the paper makes a substantial contribution by addressing an important problem in complex query answering over knowledge graphs in a novel and efficient manner. The strengths of the paper (elegant formulation, state-of-the-art performance, improved interpretability, and efficiency with reduced training data) far outweigh the relatively minor concerns raised.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis and final recommendation.\n\n1. Contribution and Novelty:\n • The paper proposes a variation on adversarial training by weighting the regularization term to penalize “less robust” samples more. In effect, it modifies the well‑known TRADES loss. While this modified design may lead to some improvements, many reviewers noted that it is conceptually close to previous methods (e.g., MART) and that the novelty is incremental rather than groundbreaking.\n • The theoretical derivation—stated as Theorem 1—extends an equation from TRADES to multiclass classification. However, multiple reviewers expressed concern that the derivation is trivial in the binary case and, for multiclass problems, involves several loose relaxations. Disagreements over the precise interpretation of the mapping z(x) and whether the inequality (≤) is justified indicate that the “theoretical contribution” may be overclaimed and is far from providing a deep new insight.\n\n2. Theoretical Rigor:\n • Many reviewers (e.g., Reviewers 5, 10–12, 16) raised serious issues with the rigor and clarity of the theoretical statements. In particular, the correspondence between theory and practice is not tight: the derivation appears to “create a loose bound” that does not directly support the practical algorithm.\n • Although some appreciate the simplicity, several experts saw the result as merely a rewriting of an existing bound rather than a theoretically significant contribution, which is problematic given the standards of a top‑tier conference.\n\n3. Experimental Evaluation:\n • The experimental results show some improvement in both clean accuracy and adversarial robustness. Some reviewers noted that the improvements over TRADES or similar baselines are small and occasionally marginal. When extra data is employed, the performance gains are sometimes not convincingly superior to those of baselines like HAT.\n • Besides using relatively small datasets (CIFAR10, F‑MNIST, SVHN), the empirical evaluation does not address results on more challenging-scale benchmarks. This raises questions about the general applicability of the method.\n\n4. Overall Balance:\n • While the paper is clearly written and the empirical evaluation is thorough within its scope, the mixed views across reviewers indicate that the paper’s sole major technical contribution is a small modification of TRADES along with a loose theoretical explanation.\n • For a top‑tier conference, both the novelty/theoretical contribution and the empirical improvements are expected to be more substantial. Here, the theoretical part is both overclaimed and, under scrutiny, not as rigorous as one would hope, and the experimental gains, even if consistent, are marginal relative to the strong baselines in the literature.\n\nConclusion:\nGiven the criticisms regarding the theoretical contribution and concerns about the significance of the empirical improvements (especially in light of incremental novelty), the overall merit of the paper does not clearly reach the threshold expected by top‑tier venues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Clarity and Presentation:  \nMany reviewers noted that the paper is very hard to follow. Crucial parts of the algorithm—especially the parameterization of policies π_E and π_T, the exact formulation of the objective (e.g., in Equations 8 and 9), the meaning and treatment of uncertainty, and the derivations leading to the closed-form update—are not clearly presented. The notation and mathematical exposition (e.g., in Sections 4.1 and 4.3) are confused, and some key equations do not seem to be rigorously justified. This overall lack of clarity is a significant barrier for understanding and replicating the proposed approach.\n\n2. Conceptual and Theoretical Concerns:  \nThe core idea—leveraging both epistemic and aleatoric uncertainty through an information-theoretic formulation (Max-Q Entropy Search)—is interesting, but there is insufficient discussion about how the contributions generalize and relate to existing approaches (e.g., how this idea could plug into SAC, TD3, or why it would help in sparse reward settings). In particular, the reliance on the dependence on the optimal Q function (Q*) introduces conceptual concerns about whether the method truly encourages exploration independent of the reward signal. Several reviewers mentioned that integrating uncertainty information in such a way may end up coupling exploration too tightly with the external reward structure.\n\n3. Experimental Evaluation:  \nThe empirical results are, at best, preliminary. Multiple reviewers pointed out that the experimental comparisons use limited baselines (omitting important ones like OAC, VIME, etc.) and that the performance gains over DSAC and SAC are not consistently significant. Additionally, ablation studies are lacking especially for understanding key hyperparameters (e.g., the uncertainty ratio, α, β, and C) and the effects of using both types of uncertainty. Other experimental details (such as the use of a shortened horizon and the number of training seeds) leave concerns as to the statistical significance and robustness of the results.\n\n4. Overall Contribution:  \nWhile the idea of combining epistemic and aleatoric uncertainty in conventional continuous RL settings is promising, the paper does not yet provide a clear, convincing, nor sufficiently thorough presentation or evaluation of the method. The theoretical derivations are murky in parts and the experiments do not credibly demonstrate a robust, significant improvement over competitive baselines.\n\nGiven the significant issues in clarity, theoretical justification, and experimental validation identified by multiple reviewers, the paper is not ready for publication in its current form at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning:\n\n1. Contribution and Novelty:  \nThe paper identifies that for bounded activation functions (such as Tanh), a “swapped” order—placing Batch Normalization after the activation—can lead to noticeably different behavior (e.g., asymmetric saturation and increased sparsity) than the conventional BN-before-activation order. Although this observation is interesting and may shed light on the internal representations induced by normalization layers with bounded nonlinearities, many reviewers noted that the novelty is marginal. In particular, the work does not lead to a clear performance advantage when comparing architectures with bounded activations against the more standard ReLU-based networks, which still remain superior in practice.\n\n2. Clarity and Rigor:  \nSeveral points throughout the paper are not rigorously or clearly defined. Terms such as “the center of the function” and “asymmetric saturation” are imprecise, and key claims and justifications are unsupported by quantitative evidence in parts of the network (for instance, the behavior in deeper layers). Multiple reviewers have raised questions about potential confounding factors (e.g., training speed effects versus generalization, impact of weight decay, and the influence of different architectural choices) that are not sufficiently addressed in the current revision.\n\n3. Empirical Evidence:  \nWhile the experimental investigation is reasonably thorough in examining various metrics (saturation, skewness, and sparsity), there are concerns regarding the consistency of the observations. Some experiments show unexpected behavior in later layers or different trends that are not fully explained. Moreover, the empirical results do not convincingly show that the swapped architecture has a competitive edge over standard architectures—especially those using ReLU—which limits its practical appeal.\n\n4. Scope of Applicability:  \nThe approach is demonstrated mainly on VGG-like architectures without residual connections, raising questions about how generalizable the findings are to modern network designs (e.g., ResNets or MLPs). This further limits the significance of the proposed method in practice.\n\nOverall, while the paper highlights an interesting phenomenon regarding the interaction of Batch Normalization and bounded nonlinearities, the contributions are limited by unclear definitions, incomplete analysis of conflicting trends in the data, and a lack of practical impact when compared to widely used methods. As such, the paper does not meet the high standard expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contribution:  \nWhile the paper’s motivation of supplementing sequence and network information with pretrained structural embeddings is sound, the methodological innovation is limited. The paper simply utilizes OmegaFold’s embeddings (an existing method) and combines them with a standard graph neural network for prediction. Reviewers expressed that this approach is only marginally novel. There is also minimal innovation in the architecture, and several reviewers noted that the contributions do not spark new research directions.\n\n2. Baselines and Comparisons:  \nA major issue with the paper is the weak baseline choices. The experiments compare against only a limited number of methods (primarily GraphSAGE and GAT) without accounting for state-of-the-art approaches (e.g., dMaSIF, AlphaFold-Multimer, or even simpler pairwise models). Furthermore, Reviewer 4 stressed that key comparisons (such as using OmegaFold embeddings in a pairwise manner, comparing with OmegaPLM embeddings, or even using an AlphaFold-Multimer based approach) are missing. This weakens the empirical support for the claims made.\n\n3. Empirical Evidence and Clarity:  \nThere is concern regarding the clarity of the presentation and the experimental design. Several points raised include the explanation of how the AUC is defined in the context of PPI, how the binary classification is set up exactly, and the inconsistent results of different variants of the method across datasets. Moreover, the negative sampling technique—critical for training the GNN on only positive PPI networks—is not adequately explained. The empirical improvements, mostly marginal, are not reported with sufficient details (e.g., no confidence intervals) over sufficiently competitive baselines.\n\n4. Structural and Presentation Issues:  \nIn addition to the methodological limitations, the paper suffers from a lack of clarity in its exposition, as multiple reviewers remarked on the writing quality and presentation structure. Minor issues, such as table formatting and ambiguous explanations, further detract from the work’s impact.\n\n5. Overall Assessment:  \nGiven that top-tier conferences expect works with significant methodological novelty, strong empirical evidence with comprehensive benchmarking, and clear presentation, the current work falls short on multiple aspects. The transformative potential of using structural embeddings for PPI prediction is underexploited, and the paper leaves open several critical questions regarding model design, comparison with truly competitive baselines, and empirical reproducibility.\n\nBased on these observations, I conclude that the paper does not meet the threshold for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a Transformer-based framework named CANVASEMB for learning layout representations in graphic design and leverages an impressive large-scale slide dataset. While the idea of using a pre-training strategy inspired by BERT is sound and potentially impactful—especially due to the dataset’s size and the interesting angle of representing layouts—the major criticisms from the reviewers raise significant concerns.\n\nKey issues include:\n\n1. Dataset Details and Reproducibility: Multiple reviewers note the lack of sufficient details on how the large-scale dataset is constructed. Essential statistics such as the number of elements per slide, how the properties are parsed, and the extent of human intervention remain unclear. This omission hinders reproducibility and weakens the paper’s experimental foundation.\n\n2. Insufficient Evaluation and Baseline Comparisons: The evaluation is limited to relatively simple tasks (e.g., element role labeling and a binary classification “image captioning” task) with comparisons only against a gradient-boosted decision tree baseline. Reviewers explicitly recommended comparisons with other established methods (e.g., the SIGGRAPH 2019 work, Li et al. 2020b, READ on CVPR 2020) and even with Graph Neural Network approaches. The lack of rigorous experimental analysis and qualitative as well as quantitative comparisons undermines the paper’s claims of achieving state-of-the-art performance.\n\n3. Technical Description and Clarity: Although the overall writing is clear, reviewers point out that the description of the core transformer-based architecture is sparse. The paper would greatly benefit from detailed architectural schematics, complete explanation of the attention mechanism as applied here, and more in-depth training details (e.g., hyperparameters, setup, convergence details). These omissions further weaken confidence in the method’s reproducibility and clarity.\n\n4. Novelty and Motivation: While leveraging Transformers for layout embedding is an interesting idea and the dataset is a notable contribution, the reviewers express concerns regarding the degree of novelty over existing works. The paper does not sufficiently clarify what critical technical bottlenecks its approach overcomes compared to previous methods, nor does it convincingly justify the choice of a Transformer-based approach over potentially more efficient Graph Neural Networks.\n\nGiven these substantial concerns—particularly regarding evaluation rigor, lack of baseline comparisons, and insufficient dataset and technical details—the paper does not yet meet the standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my analysis of the paper:\n\n1. Contribution and Novelty:\n• The paper proposes a novel mechanism—a “randomized response on bins”—for label differential privacy in regression. Although it builds on ideas from label-DP for classification settings (as in Ghazi et al. 2021), extending these ideas to regression and obtaining an “optimal” mechanism under a given loss is novel and practically important.\n• Some reviewers (e.g., Reviewer 5) note that in terms of technical novelty the work is only a mild generalization of prior methods. However, other reviewers (e.g., Reviewer 4) emphasize the significance of a mechanism that is optimal with respect to expected loss, along with theoretical guarantees.\n\n2. Theoretical and Empirical Validation:\n• The paper derives an optimality result and provides proofs that, subject to a given loss, the mechanism achieves the best trade‐off for label privatization. Most reviewers are satisfied with the correctness, though a few point out that certain claims could use additional support or discussion.\n• The experimental evaluation is thorough and conducted on real datasets. It demonstrates clear improvements over alternative DP approaches (Laplace, Exponential, staircase) under the label privacy definition chosen.\n\n3. Clarity and Presentation:\n• Across the reviews the writing is praised for clarity and ease of follow. Minor typos and minor issues have been noted and are addressable.\n• Some reviewers also asked for clarifications or additional discussion regarding comparisons with other label-DP methods (e.g., Ghazi et al. 2021). The author responses and suggestions of additional comparisons (e.g., including some experiments or discussion in an appendix) point toward an openness to refine the presentation.\n\n4. Reviewer Concerns:\n• Some reviewers questioned whether different loss terms (like the Bayes-optimal prediction error from the features) should be integrated into the optimization. This is a subtle point that may merit future work but it does not diminish the theoretical contribution regarding the mechanism’s optimality for the privacy-protected label transformation.\n• There is also a mild concern about the assumptions (such as whether the label domain is finite or can be reasonably binned). The discussion and the mechanism itself seem well-suited for practical settings where labels are mapped to a finite set.\n\nOverall, the paper presents solid theoretical work with clear and comprehensive empirical validation. Although there are some minor technical and presentation issues, they do not swamp the overall quality or significance of the contribution. For a top-tier conference, the extension of label-DP from classification to regression with optimality guarantees is a strong positive, and the paper is well prepared for further improvements.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novel Idea with Promising Impact:  \nThe central idea of modeling each data point as arising from its own function and hence deriving a “Functional Risk Minimization” (FRM) framework is novel and potentially impactful. It challenges the standard assumptions in supervised learning, and if well-grounded, could offer new insights into over-parameterized models and generalization.\n\n2. Theoretical and Algorithmic Clarity Issues:  \nA primary concern is that the connection between the formal theory and the practical algorithm is not clearly established. Reviewer 1 points out that the derivation of the algorithm’s equations (notably Equations 7 and 8) is not formally provided, and key conditions under which they approximate the population objective are missing. This lack of rigorous derivation weakens the theoretical foundation expected at a top-tier conference.\n\n3. Scalability and Practicality:  \nThe proposed algorithm relies on approximations (including a Laplace approximation) and the use of Hessian information—and reviewer 1 explicitly mentions that this makes the algorithm less scalable. Additionally, the experiments are conducted on small-scale problems (e.g., linear regression with non-uniform noise) without evidence that the approach scales to more realistic or larger datasets. For a top-tier venue, ensuring scalability and empirical robustness is critical.\n\n4. Consistency Between Training and Testing:  \nReviewer 2 raises an important point regarding the consistency between training (where every data point is modeled with its own function) and testing (where a single parameter is used). This mismatch, alongside a potential reinterpretation as a hierarchical Bayesian model, introduces confusion regarding the assumptions underlying FRM compared to classical ERM. Such conceptual ambiguities need to be resolved to convince the community of the method’s superiority or even soundness.\n\n5. Lack of Theoretical Guarantees:  \nNone of the reviewers observe a clear theoretical guarantee accounting for when FRM can outperform standard ERM. There is no analysis showing that the finite-sample version of FRM approximates the population objective robustly or that the FRM minimizer achieves low loss. The absence of such supporting analysis is a serious drawback for acceptance at a top-tier conference.\n\n6. Experimental Evidence:  \nWhile the empirical results on small-scale problems are somewhat encouraging (with indications that FRM may outperform ERM under certain conditions), they are far from sufficient. Reviewer 3 calls the performance improvements “marginal” while reviewers 1 and 2 note missing experimental evaluations, especially for the generative model variant. For a top-tier conference, a more comprehensive empirical study is necessary.\n\nConclusion:  \nThe paper presents a conceptually interesting proposal; however, the current version falls short in several key areas: rigorous connection between theory and practice, clarity in the FRM vs. classical methods narrative, scalability considerations, and robust empirical validation. Given these shortcomings and the concerns raised by multiple reviewers, the paper, in its current form, does not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my reasoning:\n\n1. Contributions and technical correctness:\n • The paper provides a sharp convergence rate for gradient descent on deep linear networks with a variety of initialization schemes and a broader class of loss functions (namely, any strongly convex and smooth function defined over a subspace). The analysis is rigorous and “sharp” in that the O(κ log(1/ε)) rate is shown to match the convergence of the equivalent convex linear model.\n • Importantly, the result on the trajectories (Theorem 3.3) – showing that the trajectory of the deep network’s output remains close to that of the equivalent linear predictor – is not only technically interesting but can offer useful insight into why overparameterized nonlinear (or in this case, linear) nets can be optimized efficiently despite the non-convexity.\n\n2. Novelty and significance:\n • Several reviewers pointed out that for Gaussian and orthogonal initializations, analogous rates have appeared in earlier work. It is true that aspects of the proof build on the techniques developed in Du & Hu (2019) and others. However, extending the analysis to general strongly convex and smooth loss functions (beyond the squared loss) as well as carefully treating different initialization schemes requires nontrivial modifications. This extension is of interest because many practical problems use losses beyond the squared error.\n • Moreover, the paper provides an additional novel insight via trajectory comparison. Even though some parts of the techniques are similar to prior work, the observation regarding the close path to a convex minimization trajectory is insightful and could stimulate further work.\n\n3. Clarity and presentation:\n • The paper is well-written and clear. Even though several reviewers have suggested that some parts (e.g., the discussion regarding P(t)) be toned down or explained more intuitively, none of the criticisms question the technical correctness.\n • Minor presentation issues have been raised (notation, font sizes, etc.) but these are straightforward to address.\n\n4. Reviewer consensus:\n • Two reviewers (Reviewer 2 and Reviewer 4) explicitly recommended acceptance.\n • Reviewers 1, 3, and 5 had concerns mainly over the marginal novelty in parts of the technical presentation, but were generally positive about the correctness and significance of the results.\n • Reviewer 6, while recommending rejection in its current form, essentially provided a “revise if fixed” type of criticism. Their points (e.g., overclaims in the title/abstract, clearer discussions of “essential overparameterization,” etc.) appear to be primarily issues of emphasis and exposition rather than deep technical flaws.\n\n5. Final judgment:\n • Overall, the paper makes a solid technical contribution by bringing together sharp convergence guarantees across a broader class of problems (general losses and multiple initialization schemes) and by providing novel insights on the GD trajectory.\n • The concerns raised by some reviewers mostly point toward clarifying presentation and properly positioning claims relative to existing work. These can be addressed during the revision process.\n\nGiven the importance of the questions addressed and the overall positive assessments on correctness and potential impact, I lean toward acceptance for a top‐tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is a summary of my reasoning:\n\n1. Contributions and Empirical Performance: The paper proposes an iterative co-design methodology for protein sequence and structure that improves on state‐of‐the-art in terms of inference speed and design quality. Multiple reviewers (e.g., R1, R7, R8) note that the empirical performance is strong and that state-of-the-art results were demonstrated on several benchmarks from both antibody design and general protein design tasks.\n\n2. Clarity and Presentation: The overall presentation is clear, and the method is well described—albeit with some points needing minor clarifications (e.g., calculation of SeqIPA, specifics of some update equations, and more details on hyperparameters and training). Such comments are seen as fixable and can be addressed in the final version.\n\n3. Novelty and Technical Aspects: Several reviewers (e.g., R5, R6, R7) raised concerns that the technical novelty is marginal with many components resembling existing methods or being adapted from approaches like AlphaFold2. However, most found that the integration of these ideas toward faster inference and joint design is interesting and useful. While some reviewers question parts of the method’s presentation and require additional ablation or detailed explanation, these issues are not fatal but rather call for further clarification.\n\n4. Reproducibility and Experimental Details: Some reviewers have noted the lack of hyperparameter details, training specifics, and a code release commitment. Although these points need to be addressed, they are standard revisions that can be incorporated before the final camera-ready version, not necessarily undermining the significance of the contributions.\n\n5. Overall Recommendation: The majority of reviewers are positive with only some minor to moderate issues. While technical novelty has been questioned, the improvements in inference speed and the strong empirical results outweigh these reservations. The identified issues (lack of ablation study, clarity in method description, and training details) are amendable, making the paper suitable for a top-tier venue.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Motivation and Novelty: Although the paper introduces the notion of “period drift” as an extension of client drift in federated learning, the reviewers point out that the concept is not well defined or clearly separated from traditional non-iid issues. Several reviewers noted that the theoretical motivation for using a meta-learning based parameterized aggregator is not convincingly argued against simpler alternatives (such as using scalar weights or additional regularization). Consequently, the claimed novelty is marginal.\n\n2. Methodology: The idea of learning an aggregator via a meta-learning framework is interesting; however, many reviewers consistently mentioned that the proposed method is ad-hoc. There is insufficient justification for the choice of two separate neural networks for aggregating both the model parameters and their differences. The reliance on a proxy dataset to train the aggregator, while common in some knowledge distillation techniques, raises concerns about its applicability in truly decentralized FL settings -- and this limitation is not sufficiently discussed.\n\n3. Experimental Evaluation: The experimental settings seem weak overall. Multiple reviewers pointed out that:\n • The datasets and models used (e.g., FEMNIST and MovieLens) are too simple, with missing evaluation on standard benchmarks such as CIFAR-100 or Stack Overflow.\n • There is a lack of ablation studies (e.g., on the influence of the proxy dataset data distribution and the architecture choices).\n • Comparisons with closely related methods (e.g., FedET, DS-FL, and other regularization approaches) are missing. \n • Some experimental results are not competitive and, in some cases, even underperform compared to established baselines.\n\n4. Clarity and Reproducibility: While some reviewers liked the motivation, many noted that there are multiple issues with writing quality and clarity (syntax, small typos, and inconsistent citation styles) that hinder the paper’s readability. Also, there is no code provided, which makes reproducibility a concern in the context of a top-tier conference.\n\n5. Broader Consideration: There are also additional concerns regarding privacy and scalability. One reviewer pointed out that the use of proxy dataset and the access to individual model updates may run counter to federated learning’s privacy principles. Moreover, the increased complexity on the server side and the extra communication/computation overhead are not quantified, which could hamper real-world applicability.\n\nIn summary, while the problem the paper addresses is of interest, the overall contribution suffers from insufficient theoretical grounding, experimental validation, and clarity. The issues raised by multiple reviewers suggest that the paper’s contributions are marginal at best and do not meet the high standard for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n• The paper proposes DeepSeaProbLog as an extension to DeepProbLog in order to support continuous and mixed discrete‐continuous domains. This is an appealing goal because current neural probabilistic logic programming frameworks are limited to discrete (finite) domains.\n\n• In terms of technical contribution the paper clearly explains its reduction to weighted model integration and shows how one can obtain gradients by suitable relaxations. Several reviewers appreciated this combination of ideas and acknowledged that the language extension addresses an important limitation in neural symbolic AI. However, some reviewers raised concerns that much of the technical content relies on standard techniques—such as reparameterization, relaxations, and known WMI methods—and therefore the novelty (especially with respect to existing neural probabilistic programming frameworks) is marginal.\n\n• An important point across reviews is that the empirical evaluation consists of only two–or–three toy/contrived examples. Several reviewers wanted additional experiments on standard benchmarks, comparisons to related baselines (including testing the discrete reasoning capability in a pure discrete setup), and an investigation of performance/efficiency tradeoffs (e.g. comparing knowledge compilation with sampling). This lack of a thorough experimental study makes it hard to assess the method’s scalability and robustness.\n\n• There were also concerns about clarity and details in the semantics of the language and some concrete issues (e.g. handling of discrete random variables, the formulation of distributional facts, and the overall organization of the technical contributions which are sometimes relegated to the appendix).\n\n• In summary, while the idea of extending neural probabilistic logic programming to handle continuous domains is interesting and fills an important gap, most reviewers agree that the contribution is rather incremental over DeepProbLog. Combined with the limited empirical evaluation and some clarity issues, the paper does not yet meet the standard expected of a top-tier conference submission.\n\nTaking all these points together, especially the marginal novelty relative to existing work and the insufficient experimental validation, I lean toward a rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Technical Contributions:\n• The paper tackles gradient inversion attacks in the specific context of tabular federated learning. Although applying gradient inversion in this setting is interesting, the key techniques—namely the “softmax relaxation”, “pooled ensembling”, and an “entropy measure” for quality assessment—are all based on well‐known methods. Two reviewers commented that these technical ingredients are quite standard and known in the literature (with alternatives such as the Gumbel trick or direct mixed integer formulations being readily available).\n• Reviewer 2 explicitly noted that the underlying new ideas are not innovative and that the baselines are weak.\n• While Reviewer 3 appreciated the application and experimental demonstration, the overall contribution is considered only marginally novel compared to prior art.\n\n2. Clarity and Presentation:\n• Reviewer 1 found that the paper is not well written, with sentences that are obscure and grammar inconsistencies that hinder the reader’s understanding.\n• Although Reviewer 2 found it generally well written, such clarity issues raised by Reviewer 1 cannot be entirely dismissed, especially for a top-tier conference where clear exposition is paramount.\n\n3. Empirical Evaluation:\n• Experiments on several datasets (including the Adult dataset) show some improvement over baselines (e.g., a 10% improvement), and the methodology appears to work in practice.\n• However, there are concerns regarding the strength of the baselines and the demonstration of key claims—especially the behavior of mixed discrete-continuous optimizations—which remain not fully convincingly validated.\n• Some aspects, such as why pooling effectively reduces variance or how the entropy measure generalizes to non-Gaussian cases, need further clarification.\n\n4. Overall Assessment:\n• Although the paper addresses an important application field (tabular data in federated learning with high privacy risks) and employs a number of ideas that do lead to improved attacks, the techniques used are acknowledged as standard or incremental. \n• At a top-tier venue the expectation is both for significant novelty and a very high standard of writing and presentation. The present work struggles on both fronts: it is somewhat incremental in terms of novelty and suffers from clarity issues as noted by one reviewer.\n• Given these points, there is insufficient evidence that the paper represents a significant and novel contribution worthy of a top-tier conference.\n\nBased on the discussions above, my final decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contribution: The paper studies how no‐regret learning dynamics perform in an average-case sense rather than only guaranteeing convergence. It extends prior work (e.g., PP16) by considering a broader family of “q‐replicator” dynamics and by attempting to relate the regions of attraction (hence average performance) with the quality of the equilibria reached. However, several reviewers noted that many of the results are either partial—in the sense that they address only the 2×2 symmetric coordination game case—or incremental, as some of the technical ideas (and Theorem 3.2 in particular) closely resemble those in PP16. While the extension to “q‐replicator” dynamics is of interest, the contribution may be seen as marginally novel.\n\n2. Theoretical and Empirical Analysis: The paper provides both conceptual measures and experimental evidence about how the dynamics behave, including the investigation of average price of anarchy. Although the experiments bolster the merits of the approach, some reviewers were troubled by the fact that many of the rigorous proofs and formal treatments were limited to special cases (notably, the 2×2 games). There is also a desire for a more systematic treatment of the general case rather than a set of intriguing conjectures backed up by empirical plots. This leaves the perception that—apart from the 2-dimensional case—the formal results are incomplete.\n\n3. Clarity and Presentation: The paper is generally clearly written, and most reviewers appreciated its motivation and explanation of the challenges in equilibrium selection. Some issues remain with respect to citation formatting and notation consistency. Reviewer 1 and Reviewer 3 pointed out points of confusion concerning the use of “monotonicity” and overloaded notation. Such issues, while not fundamental, need to be refined.\n\n4. Relevance and Impact: Although Reviewer 4 correctly notes that the methods are limited to a narrow game class, the underlying problem—assessing the efficiency of learning dynamics beyond worst-case guarantees—is a challenging and important one. The novelty does not lie in completely new techniques but in the perspective of average-case analysis that has the potential to direct future work on more complex games. While this may appeal to a narrower community (say, at EC or similar venues), it remains of sufficient interest for a top-tier conference if presented with the proper context.\n\n5. Overall Assessment: The committee’s reviewers expressed mixed opinions. There is acknowledgment of the importance and correctness of the approach, but also concerns regarding technical novelty and progress beyond the state-of-the-art. The authors’ response (especially the added discussion regarding PP16) has alleviated some concerns—notably regarding proper attribution and clarity in the contribution. Given the soundness of the technical claims (as judged by several reviewers) and the promise of this direction, the paper—as a contribution—and with revisions addressing the forthcoming presentation issues, merits acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes to augment standard image classification training with an auxiliary task based on randomly generated “Binary Labels” – a strategy meant to encourage the network to learn useful groupings by considering random binary partitions of the classes. While the idea of modifying the label representation to improve model performance is novel and orthogonal to many of the standard avenues of improvement, the work exhibits several important shortcomings.\n\nMajor concerns include:\n\n1. Lack of clear motivation and analysis: Although the approach shows some improvements, the paper does not offer sufficient insight into why random binary label partitions can help beyond standard one-hot encoding. It is unclear in which settings or under what conditions these auxiliary tasks are most beneficial. A more detailed discussion analyzing when and why the method works (e.g., in fine-grained classification) would have strengthened the paper.\n\n2. Insufficient experimental validation: While several datasets were used, the empirical evidence is limited. The improvements over baseline methods are sometimes small and the experiments do not include comprehensive comparisons to state-of-the-art methods or more diverse baselines. Moreover, studies on hyperparameter sensitivity (e.g., the weighting of the auxiliary task), stability (effect of random seed in label generation), and a wider range of settings (e.g., robustness to distribution shifts, very large benchmarks such as ImageNet) are missing.\n\n3. Clarity and reproducibility issues: The paper leaves key methodological details ambiguous. For instance, the methods by which the Binary Labels are generated (whether generated only once or multiple times, and how the auxiliary head is set up) are not clearly explained. There is also insufficient detail regarding the multi-task learning strategy (e.g., the use of “Metabalance”) and missing implementation details that hinder reproducibility.\n\n4. Marginal technical novelty: While the idea of changing class label representations is interesting, the overall contribution is incremental. Several reviewers point out that similar ideas have been explored in the context of error-correcting output codes and unsupervised task generation. Thus, without deeper analysis or more significant experimental improvements, the contribution falls short of what is typically expected for a top-tier conference.\n\nWhile the paper has some positive aspects and highlights an intriguing new direction, the issues related to clarity, insufficient experimental analysis, and incomplete discussion of related work cause significant concern regarding both its reproducibility and overall impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes an approach that splits a neural network between the client and server for privacy-preserving representation learning in an MLaaS setting. While the use of an additional loss function (center loss) to control information leakage is interesting on paper, several key issues were raised across the reviews:\n\n1. Novelty and Significance:\n• The core idea of splitting a network has been explored in prior work such as split learning. The approach in this paper does not sufficiently distinguish itself from existing methods.\n• The contributions, both technical and empirical, are deemed only marginally novel and fail to provide a strong “aha” factor needed for a top-tier conference.\n\n2. Technical Flaws and Threat Model:\n• Reviewer 2 points out a fatal flaw in the privacy-preserving claim: The server has access to extensive information during training (labels, loss values, class centers) which suggests that model inversion attacks could easily reconstruct the client’s data.\n• Moreover, the threat model is ill-defined. If the server can dishonestly drop the additional loss during training, the assumed privacy benefits do not materialize—making the approach ineffective in practice.\n• Reviewer 3 also highlights that the choice of using intermediate features poses inherent risks of leakage, and the lack of a well-defined security model further undermines the approach.\n\n3. Experimental Evaluation:\n• The experiments are performed on simplistic datasets (MNIST and CelebA) with shallow neural networks (LeNet-5 and VGG-16). There is a desire for more extensive benchmarking on complex datasets and more modern architectures like ResNets or Transformers.\n• Comparisons are limited, with only one baseline in some cases missing more robust alternatives (e.g., cryptographic solutions or fully homomorphic encryption approaches).\n• Additional experiments and investigation into the hyperparameter sensitivity (such as the lambda parameter) would have strengthened the empirical claims.\n\n4. Clarity and Reproducibility:\n• Although the paper is generally clear, the presence of grammatical errors and ambiguous statements affects the overall quality.\n• Some crucial experimental details (e.g., configuration of optimizers, number of training epochs) are not provided, reducing reproducibility.\n\nConclusion:\nOverall, the paper suffers from insufficient novelty, serious issues with the threat model and technical design, and a limited experimental evaluation. These concerns indicate that it does not meet the standards for a top-tier conference, where both rigorous innovation and solid empirical evidence are expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Problem Significance and Motivation:\n • The paper addresses an important and practical problem arising from the conventional split of data into training and validation sets. Tackling the “split tradeoff” is indeed an interesting idea when considering constraints in real-world ML production. \n • However, using data augmentation and selection to create a “proximal” validation set raises theoretical questions on representativeness which remain unanswered.\n\n2. Technical and Empirical Contributions:\n • The proposed Proximal Validation Protocol (PVP) is novel in the sense that it attempts to use the full training data for training while generating a validation set via augmentation and distributional-consistent sampling.\n • However, reviewers note that the experimental evaluations focus on fixed model settings rather than the crucial application of validation sets for robust model/hyperparameter selection. Without experiments demonstrating that PVP leads to better model or hyperparameter selection, the central claim is weakened.\n • Moreover, many experiments are conducted on relatively small datasets and simplified settings (e.g., CIFAR10-LT instead of more challenging benchmarks). This limits the generality of conclusions for real-world industrial applications.\n • Several critical hyper-parameter choices (such as the size of the proximal validation set or the specific data augmentation methods) lack sufficient ablation studies to understand their influence.\n • Additionally, the paper displays clear gaps in theoretical support for why the constructed validation set should be statistically representative of the data-generating distribution. Without this, it is difficult to justify its proposed merits.\n\n3. Clarity and Presentation:\n • The writing of the paper is hampered by imprecise definitions and unclear experimental details (e.g., the usage of terms like “F1”, “deterioration”, and unclear metrics in Tables 3 and 4), which further obscures the contribution.\n • There is also the use of informal language that detracts from the professional tone expected at a top-tier conference.\n • Overall, the presentation makes it challenging to judge the strength of the empirical findings and the scope of the application.\n\n4. Overall Recommendation:\n • The potential significance of the topic is acknowledged by multiple reviewers. Nonetheless, the combination of insufficient empirical validation (particularly in hyperparameter/model selection settings), lack of robust theoretical justifications, and clarity issues create doubts about the overall impact and reliability of the contribution.\n • Given these concerns and the fact that a few reviewers (in particular, Reviewer 4 and parts of Reviewer 1 and 2) highlight that the claims are not well supported, the paper in its current form does not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper attacks an important problem for diffusion‐based generative models—namely, how to obtain high‐quality samples with very few inference steps—and does so by introducing a flexible family of non‐Markovian samplers as well as an optimization procedure (DDSS) that “differentiates through” sample quality. Several reviewers found that the empirical results (e.g. dramatic improvements in FID scores in the low‐step regime) are compelling and that the method is an interesting direction overall. In particular, three reviewers (Reviewers 1, 3, and 5) highlighted that, although the idea is conceptually simple, the approach is novel in terms of combining non–Markovian design with the ability to optimize directly a perceptual loss (which in practice correlates with FID/IS) via gradient rematerialization. Reviewer 2’s summary also supports that the overall evaluation procedure is reasonable, and the method shows promise for accelerating sampling from pre‐trained models.\n\nOn the other hand, one reviewer (Reviewer 4) expressed concerns regarding the theoretical justification and clarity. In particular, many reviewers noted that while the heuristic use of a perceptual loss leads to impressive FID numbers, it remains somewhat unclear what distribution the resulting samples follow – i.e. whether the procedure preserves the likelihood properties of the original DDPM. Other points related to clarity of the notation, the potential overfitting to evaluation metrics, and the limited set of high‐resolution datasets were raised. However, the authors’ revision (and the updated Theorem 1) helped clarify some of these issues by showing that under certain conditions the ELBO is preserved, thus justifying the approach of “plugging in” pre-trained models without fine-tuning.\n\nIn balance the majority of reviewers found the contributions significant enough to merit acceptance (with scores mostly hovering in the mid‐range or higher after rebuttal), even though some aspects (especially theoretical justification and clarity) could be improved further in the final version. Given that the improvements in sample quality with very few inference steps are both useful and an important direction for diffusion models—and that several concerns, while valid, do not overshadow the empirical and partly theoretical contributions—I believe that the paper’s positive impact outweighs its shortcomings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis:\n\n1. The core idea of the paper is to revisit target propagation by computing targets via regularized inversion. While the idea is interesting on its own—especially in view of providing an “alternative” to backpropagation and incorporating an analytical inversion—it is only an incremental step over prior work in TP (e.g., Lee et al. (2015) and the more recent developments). Several reviewers have noted that many insights are already known in the literature, and the contribution therefore matters only marginally from a novelty perspective.\n\n2. On the theoretical side, there are concerns that the paper does not sufficiently distinguish its theoretical contributions: while some lemmas (e.g., comparing the TP direction to the BP gradient) are provided, the derived bounds are loose in regimes of interest. Reviewer 4 in particular calls for a fuller discussion of these bounds as well as clearer exposition on the connection to Gauss-Newton methods. Moreover, Reviewer 1 asked for clarifications on several points about the derivation (e.g. the 0-th order comment and handling multiple minimizers) that remain inadequately addressed. Thus, the theoretical underpinnings are not solid enough for a top-tier venue.\n\n3. Experimentally, although the authors report some empirical results on recurrent neural networks with long sequences, the evaluation is limited. Multiple reviewers note that the experiments appear either insufficiently broad (e.g., only one or two tasks and not comparing to strong baselines such as Hessian-free optimization or improved RNN architectures) or not convincingly showing superiority over BP. In addition, the paper seems to struggle with critical practical issues such as weight transport requirements and potential instabilities (e.g., vanishing/exploding gradients via the Jacobian inverses), which further limits its applicability.\n\n4. Given the above, while the paper does have some strengths (e.g. implementation ease in differentiable programming systems, a natural interpretation via linear approximation of DTP), overall the incremental contributions and lack of convincing experimental and theoretical guarantees make it hard to recommend acceptance at a top-tier conference. Several reviewers have pointed out that the work is “only marginally significant or novel” in both aspects.\n\nConsidering all the points and the fact that the weaknesses (extra cost, reliance on weight transport, inadequate theoretical analysis, and insufficient experimental validation) raise major concerns for a top-tier publication, my recommendation is to reject the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper investigates online adaptation methods in a setting that “relaxes” the strict label shift assumption by allowing for concurrent conditional shifts. While the motivation for handling distribution shifts in real-world settings is valid, several critical concerns arise:\n\n1. Methodological and Theoretical Limitations:\n • The paper builds on Wu et al. (2021) but extends their work only marginally by introducing three heuristics. However, these heuristics lack rigorous theoretical justification. For instance, adding extra hyperparameters (and adjusting the confusion matrix by a pseudo-count or scalar on its diagonal) comes off as ad hoc without sufficient discussion of why these modifications should reliably work.\n • The presentation of concepts such as “conditional shift” and the exact formulation of the required re-weighting are not clearly defined, leaving the reader with ambiguities regarding the basic setup.\n • The Bayesian perspective introduced in Section 4 is not well-connected with the practical problems at hand, and the notation and derivations are confusing.\n\n2. Empirical Evaluation:\n • While the experimental section is generally clear, the empirical scope is limited. The datasets considered (three synthetic and two realistic) are not comprehensive enough to convincingly evaluate the impact of the heuristics over established methods.\n • Reviewer suggestions indicate that important baselines (e.g., using a labelled OOD validation set as a direct baseline, other domain adaptation methods) were not incorporated. Without such comparisons, it’s difficult to assess the advantage of the proposed approach.\n • The improvements shown in tables are either marginal or even counterproductive in some cases. Many reviewers note that the proposed improvements over Wu et al. (2021) are not significant.\n\n3. Clarity and Reproducibility:\n • Many parts of the paper, including the notation, experimental details (e.g., how the validation set is collected in an online environment), and methodological descriptions, are inadequately explained. This lack of clarity undermines the paper’s self-containedness and hurts reproducibility.\n • Several reviewers pointed out various writing and notation issues that add to the difficulty of following the contributions.\n\n4. Overall Contribution and Significance:\n • Given that the additional heuristics are only marginal extensions and appear to fix issues (such as non-invertible confusion matrices) through practical but standard modifications, the technical novelty and empirical significance are limited.\n • The paper does not provide sufficient evidence or insights that would make it a strong contribution warranting publication at a top-tier venue.\n\nTherefore, despite addressing an important and practically relevant problem, the unresolved issues in theoretical grounding, methodological clarity, limited experimental scope, and marginal empirical improvements lead to the conclusion that the paper, in its current form, does not meet the rigorous standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis:\n\n1. Technical Contributions and Novelty:\n • Although the paper tackles an interesting instance of permuted linear regression and aims at precisely locating phase transition thresholds via a message passing algorithm, several reviewers noted that the techniques are either incremental or heavily based on existing work (e.g. Semerjian et al. 2020 and Zhang et al. 2020). In the oracle case the results follow rather directly from known results, and the non‐oracle case is handled via an approximation. Reviewer 5 in particular calls into question some of the stated “precise” claims given that the non‐oracle threshold is only computed approximately. Overall, the technical novelty does not seem to rise to the level required for a top‐tier conference.\n\n2. Clarity and Presentation:\n • A common concern is that the paper suffers from severe issues in clarity, notation, and organization. Multiple reviewers point out that the exposition is confusing and overly reliant on notation and references to previous work – making it nearly inaccessible to readers not already deeply familiar with the literature. For instance, Section 3’s presentation of the message passing algorithm is heavily criticized, and many equations (and their motivations) are not explained in a self-contained way.\n • There are numerous typos and grammatical errors. This adds to the impression that the paper has not been polished for clarity and readability, which is a significant drawback for a top‐tier conference submission.\n\n3. Empirical Evidence and Practical Relevance:\n • Although there are some numerical experiments mentioned, the reviewers note a disconnect between the asymptotic theoretical predictions and the finite sample empirical results. In particular, claims that the “empirical phase transition points are well aligned with theoretical predictions” appear hard to support without a more comprehensive discussion or code/reproducibility details.\n • Additionally, the motivation for the partial permutation recovery, which is presented as a generalization over full recovery, is not supported by a convincing practical argument – its advantage is unclear in the discussion.\n\n4. Overall Recommendation:\n • While there are interesting aspects to the paper – especially the effort to provide “precise” characterization in an area known for only asymptotic results – the community’s standards for top‐tier venues mandate not only novelty but also a self-contained, clear, and rigorously motivated presentation. With serious issues in exposition, heavy reliance on prior work without sufficient new insight, and gaps in clarity regarding both assumptions and derivations, the paper is not ready for acceptance in its present form.\n • Several reviewers would consider resubmission to a journal or a specialized venue where additional space for exposition is available, but for a top‐tier conference the current state is below the required threshold.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Below is my detailed reasoning followed by the final decision.\n\n• The paper presents an offline reinforcement learning method called BPPO that is built around the interesting insight that the inherent conservatism of on‐policy methods (such as PPO) can be harnessed to address overestimation issues in offline RL. In other words, the paper seeks to explain why an online monotonic policy improvement theorem can also yield an effective offline algorithm when suitably modified.\n\n• On the positive side, the authors provide a theoretical analysis and extend policy improvement bounds to the offline regime. The derivations seem technically correct under stated assumptions and the overall idea—a very “minimal‐change” offline version of PPO—appeals in its simplicity and ease of implementation. Experiments on the standard D4RL benchmark show that BPPO can achieve performance competitive with (and sometimes slightly surpassing) several state‐of‐the‐art offline RL algorithms.\n\n• However, multiple reviewers identified several significant concerns. In particular, there are issues regarding the soundness and novelty of the theoretical contributions. Reviewers pointed out that the main theoretical result is closely related to prior work (e.g., GePPO and other off‐policy TRPO/PPO derivations), and that many of its “new” aspects amount to specific instantiations rather than fundamentally new ideas. In addition, the approximations introduced (for instance, in Eq. 13) come without clear guarantees – worries were raised that the bounds offered by the theory might be very loose in practice.\n\n• Experimentally, while BPPO is shown to be competitive on the D4RL benchmark, there are further concerns. Some reviewers noted that the empirical improvements over established methods (such as CQL, IQL, and even simple methods like TD3+BC) are minimal. Furthermore, important experiments such as ablations on the hyperparameters (e.g., clip coefficient decay, the number of iterative steps, the asymmetry in advantage estimation) are missing or insufficiently justified, which makes it hard to judge the robustness of the method.\n\n• Finally, a couple of reviewers highlighted clarity issues in the writing and some parts of the derivation, as well as questions regarding key assumptions (e.g., whether Assumption 1 is plausible in many practical settings). This diminishes the overall impact of the work because the theoretical claims become less convincing and the convenience of a “bare‐bones” modification might not outweigh the issues in how justification is provided.\n\nTaking these points together – with multiple reviewers expressing that the contributions are incremental, the theoretical ideas not clearly separated from prior work, and the empirical gains appearing minor with inadequate ablation studies – the overall assessment is that the paper does not meet the high standards of top‐tier conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n\nThe paper proposes a \"contrastive token learning\" (CT) objective aimed at mitigating text degeneration, especially repetition issues, by explicitly contrasting label tokens with selected negative candidates. Overall, the writing is clear, and the method is straightforward and well-motivated. However, several critical concerns arise when evaluating the paper against the standards expected at a top-tier conference.\n\n1. Novelty and Contribution:\n • Multiple reviewers point out that the method is marginally novel. The primary idea is an incremental modification to unlikelihood (UL) training.\n • Reviewer 1 and Reviewer 3 argue that the method largely borrows from UL with only minor adjustments. The individual contributions aside from UL are not sufficiently substantiated.\n • Reviewer 4 questions the particular design decisions (e.g., using only M previous tokens rather than all, as in UL) and calls for additional comparisons or ablations which are not provided.\n\n2. Experimental Evaluation:\n • The experiments are limited to relatively small setups (e.g., fine-tuned GPT2-small, wikitext-103, and a dialogue system). This does not reflect the performance on current large-scale language models or on a wider range of generation tasks.\n • Reviewer 1 and Reviewer 2 note that more convincing experiments (such as on tasks like summarization, machine translation, or image captioning using stronger evaluation metrics beyond perplexity) would be necessary to showcase the method's generality and effectiveness.\n • Reviewer 2 also raises concerns about the choice and interpretation of metrics, pointing out inconsistencies with established evaluation practices.\n • The human evaluation, as mentioned by Reviewer 4, does not yield statistically significant improvements over some baselines, further reducing the impact of the proposed method.\n\n3. Overall Significance:\n • While the idea of reframing cross-entropy through a contrastive lens is interesting, all reviewers agree that its impact is marginal and does not present a significant leap over existing approaches.\n • The lack of extensive experiments and the reliance on a narrow experimental context undermine the claim of state-of-the-art performance in reducing text degeneration.\n\nIn conclusion, despite the clarity of the writing and the ease of reproducing the experiments, the paper’s limited novelty, insufficient experimental rigor, and incremental contributions make it less suitable for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces the RuDar dataset, which provides a novel 3D radar echo view of precipitation measurements across multiple climatic zones over a two-year period. Its strengths lie in its detailed data acquisition (spanning multiple altitudinal layers, providing geographical and seasonal context, and including orography) and in its potential as a benchmark for precipitation nowcasting and related machine learning tasks. The dataset is rich and certainly of interest to the weather forecasting and ML communities, especially given its public availability and reproducibility.\n\nHowever, based on the reviews, several critical issues arise when evaluating submission for a top-tier conference:\n• Novelty: Although the dataset is attractive, most reviewers agree that the technical and methodological novelty is limited. The work largely consists of dataset creation and baseline evaluations using standard models (e.g., optical flow, ConvLSTM) without significant algorithmic or methodological innovations.\n• Experimental Rigor: The evaluation section relies on simple baseline methods and limited metrics (ex: only MSE reported in some evaluations). There is a lack of comprehensive experiments (e.g., ablation studies, evaluations on additional ML tasks like anomaly detection) and comparisons with heavier state-of-the-art models, which would be expected by top-tier venues.\n• Top-Tier Conference Requirements: For prestigious conferences, a stronger combination of technical novelty, state-of-the-art methodological contributions, and thorough experimental analysis is necessary. While the dataset is appealing, reviewers indicated that its contributions are more incremental and perhaps better suited for a specialized journal or workshop rather than a flagship conference track.\n\nTaking into account these points, while the dataset is valuable and could spur future research, the current submission lacks the breakthrough novelty and comprehensive experimentation required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning:\n\n1. Novelty and Motivation: \n • Although the authors observe that different adversarial training algorithms (e.g., AT and TRADES) make different mistakes and propose a collaborative mechanism, the idea seems to be a natural extension of ensemble/knowledge distillation methods that already exist in the literature. Two reviewers specifically noted that similar ideas have been proposed or that the contributions are only marginally significant.  \n • The paper does not include any substantial theoretical analysis to justify why combining the outputs (logits) from two differently trained models leads to better robustness. That raises concerns about how principled the approach is.\n\n2. Empirical Evaluation:\n • The experiments are conducted only on small-scale datasets (CIFAR-10 and CIFAR-100) and only a limited set of network architectures are evaluated. One reviewer noted the lack of experiments on larger datasets such as ImageNet, which is important for top-tier contributions.\n • There is a lack of comparison with more recent and stronger baselines in adversarial training as well as with relevant ensemble and multi-objective approaches.\n • Several important ablations are missing. For instance, comparisons such as training a single model with multiple attacks (or different hyperparameter instantiations) must be provided to assess the claimed benefits of collaboration.\n\n3. Efficiency Issues:\n • The approach requires training two networks from scratch, which doubles the computational cost and memory overhead. For adversarial training where efficiency is already challenging, this increased cost is a significant drawback and was not adequately justified.\n\n4. Clarity and Presentation:\n • The paper is generally well-written and easy to follow, with the reviewers noting its clarity as one of the few merits.\n • However, there are issues with typos and missing citations (notably on ensemble adversarial training) as well as unexplained abbreviations such as ALP. This detracts from the overall quality.\n\n5. Overall Impact:\n • Although preliminary experiments indicate some improvements in robustness, the contributions are incremental and lack theoretical depth. Top-tier conferences require either strong novelty, extensive empirical validation on large-scale benchmarks, or deep theoretical insight – the current paper falls short on these dimensions.\n\nBased on the above considerations, the weaknesses—the lack of novelty beyond prior art, insufficient empirical evaluation on challenging benchmarks, missing ablation studies, and the significantly increased computational cost—outweigh the reported improvements. Therefore, the paper does not meet the high standards required by a top-tier venue.\n\nFinal Decision: Reject"
    }
  ]
}