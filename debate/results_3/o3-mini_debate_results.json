{
  "summary": {
    "model": "o3-mini",
    "total_papers": 100,
    "correct_predictions": 79,
    "accuracy": 79.0,
    "confusion_matrix": [
      [
        55,
        21
      ],
      [
        0,
        24
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 72.36842105263158,
        "precision": 100.0
      },
      "Accept": {
        "total": 24,
        "recall": 100.0,
        "precision": 53.333333333333336
      }
    }
  },
  "results": [
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n1. The paper suffers from severe writing and language issues, making it extremely difficult to understand even the basic intent of the work. The abstract and many parts of the main text contain grammatical errors, undefined symbols, and ambiguous statements.\n2. The presentation is extremely poor, with many passages being incomprehensible. Critical parts of the paper, such as the description of the proposed \"master game tree\" that coordinates episode generation, are not clearly defined or justified.\n3. There are no references or discussion of related literature. The absence of citations suggests either a lack of awareness of the current state of research in MCTS-based reinforcement learning or a failure to place the work within the existing body of knowledge.\n4. The mathematical symbols and algorithmic boxes are inadequately defined, and the paper appears to be more of an incomplete set of notes than a finished research paper suitable for a top-tier venue.\n5. The author’s response does not provide a convincing plan for addressing the numerous issues raised by the reviewers. The response merely apologizes and states the intention to improve in future work, without demonstrating a clear plan for resolving the outlined deficiencies.\n\nBased on these points, the paper, in its current form, does not meet the standards expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThis paper attempts to address modifications to Monte Carlo Tree Search (MCTS) for reinforcement learning and uses the idea of a “meta-tree” to guide episode generation. However, several substantial issues are evident:\n\n1. Writing Quality and Clarity: The paper is riddled with grammatical errors, unclear phrasing, and incomprehensible passages. The abstract and body provide little insight into the specifics of the contribution, making it difficult to understand even basic statements such as “the coordination of episode generation was focused on.” Many symbols and algorithms are presented without proper definition.\n\n2. Lack of Related Work and Citations: The complete absence of references is a severe flaw. Without positioning the work within the relevant literature, it is impossible for readers to assess the novelty, significance, or relation to existing methods like AlphaZero or other studies in MCTS-based reinforcement learning.\n\n3. Incomplete and Underdeveloped Contribution: The paper appears more like a rough set of notes rather than a polished academic work. Important elements such as justification for methodological choices, detailed experimental results beyond a small-scale demonstration, and rigorous analysis are missing. Additionally, reviewer comments indicate that even the provided experimental results on \"a small problem\" lack sufficient explanation or rigor.\n\n4. Author Response: The responses by the author show a recognition of the shortcomings in both writing and methodology, and an expressed desire to improve in future iterations. However, at this stage, the paper does not meet the bar required for publication at a top-tier conference.\n\nIn summary, given the poor quality of writing, incomplete presentation of ideas, lack of citations and clear definitions, and the overall impression from both reviewers and the author’s own comments, the paper does not satisfy the standards for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper \"NORML: Nodal Optimization for Recurrent Meta-Learning\" attempts to introduce an LSTM-based meta-learner aimed at overcoming scalability issues in gradient-based meta-learning methods. While the topic itself (meta-learning and few-shot classification) is interesting and timely, there are several critical issues that prevent this paper from meeting the standards required for a top-tier conference.\n\nIn particular, the reviewers raised several points that reflect major concerns:\n1. Novelty and Contribution: The method appears to be an incremental variation on previous meta-learner LSTM approaches (e.g., Ravi & Larochelle). The paper argues that using a LSTM with a parameter count that scales linearly relative to the learner’s parameters is beneficial, yet similar approaches in prior literature use parameter-sharing to keep the meta-learner succinct. As a result, the comparative advantage of the additional parameters is not well motivated or substantiated.\n\n2. Lack of Fair and Comprehensive Experimental Comparisons: The empirical evaluation suffers from several shortcomings. The experiments do not include proper comparisons with the related methods such as the original Meta-Learner LSTM of Ravi & Larochelle, or updated versions like MAML++ (Antoniou et al.), which are essential to validate any claimed benefits. Furthermore, the claim that the proposed method can handle long inner-loop updates better than MAML is unverified due to the limited number of inner loop steps used in the experiments.\n\n3. Limited Applicability: The practical utility of NORML is severely limited because it is only demonstrated on fully connected networks. The prevailing trend in few-shot learning and meta-learning involves convolutional architectures, especially for vision tasks. This limits the broader applicability and impact of the approach.\n\n4. Presentation and Writing Issues: Multiple reviewers have noted that the paper has significant issues with clarity, grammatical mistakes, and improper citation practices. These problems hinder the readability and overall quality of the work, making it less suitable for a top-tier conference where clear and tight writing is essential.\n\nGiven these concerns – the incremental technical contribution, insufficient experimental validation against competitive baselines, limited scope of applicability, and poor presentation – the paper does not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning for the decision:\n\n1. Novelty & Technical Contributions\n • The paper attempts to extend GMM training to a streaming/online setting via SGD. It proposes using a simplified “max-component” trick for numerical stability, a form of annealing (labeled “regularization”) to prevent degenerate solutions, and an adaptation for covariance matrix estimation by employing local principal directions.\n • However, the “max-component” trick is a variant of well‐known numerical stabilization techniques (like the log-sum-exp trick), and its contribution seems incremental. Likewise, the annealing mechanism and the principal directions approach are somewhat heuristically motivated and not well-grounded compared to alternative approaches.\n • Overall, while the goal of SGD-based GMM training in an online setting is relevant, the technical ideas and their presentation appear to be incremental and at times ad hoc.\n\n2. Clarity and Presentation\n • Several reviewers pointed to unclear aspects including: the motivation behind some of the proposed modifications (e.g., the use of the max-component formulation, the precise meaning and impact of the gridding/annealing approach), and the specification of the covariance structure (notably the subspace method).\n • The exposition contains some degree of redundancy and confusing terminology (e.g., the term “regularization” is used when the mechanism is more like annealing).\n • Though the authors provided responses and clarifications, several of these points still remain insufficiently addressed for a top-tier publication.\n\n3. Experimental Evaluation\n • The experimental evaluation is considered insufficient by multiple reviewers. The experiments are performed on datasets that do not clearly exemplify “high-dimensional” problems (even if SVHN is argued to be 3000-dimensional, the standard image representation sizes remain relatively modest compared with other high-dimensional tasks).\n • There is a lack of comparisons with state-of-the-art baselines beyond EM (including variants which use effective initialization such as K-means) and missing details regarding metrics (e.g., a detailed comparison of running time and convergence guarantees).\n • Many of the quantitative and qualitative evaluation details (e.g., criteria for selecting images, proof of convergence properties) need better presentation, which weakens the evidence for a clear advantage of the proposed method.\n\n4. Impact and Relevance\n • The idea of extending GMM training to an online regime is attractive, and SGD is naturally appealing in streaming settings. However, for a top-tier venue the contributions need to be both technically more substantial and supported by strong empirical improvements or novel insights.\n • In the current form the paper’s improvements over classical EM (with effective initialization) or recently proposed methods (like moment matching approaches) are not convincing enough.\n\nGiven the cumulative concerns regarding the incremental nature of the technical contributions, the exposition and clarity of the proposed methods, and the limited and partially unconvincing set of experimental results, the paper does not meet the standards for a top-tier conference at this time.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper makes a significant theoretical contribution by rigorously analyzing how different activation functions affect the training dynamics of overparameterized neural networks. In particular, it clearly distinguishes between the behavior of non-smooth activations (e.g., ReLU, SELU, ELU) and smooth activations (e.g., tanh, swish, polynomial) with respect to the spectrum of a Gram matrix that is key to training speed. The derived bounds and the careful discussion on the dependency on the data’s geometry (or span) offer novel insights that enhance our theoretical understanding in the NTK/lazy training regime.\n\nThe reviewer comments are generally positive. Reviewer 1 appreciated the thorough proofs and the experimental validation (despite noticing a gap in the non-ideal CIFAR10 experiment), and Reviewer 2 was favorable, praising both the depth of theoretical analysis and the empirical illustrations—even though both reviewers noted that the presentation (especially the extensive appendix) could have been more concise and navigable. The authors have adequately addressed these concerns in their revision by organizing the appendix with a clear table of contents, streamlining the main paper results, and clarifying statements that were ambiguous.\n\nFurthermore, the authors’ responses to minor issues (including clarification of assumptions and typographical corrections) have been convincing. Overall, the paper not only provides rigorous theoretical results but also takes care to connect these to empirical observations, a balance that is key in top-tier conferences.\n\nGiven the solid theoretical contributions, the relevance of the results to the deep learning community, and the satisfactory responses to the concerns raised by reviewers, I believe the paper meets the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed assessment and final decision.\n\n1. Novelty and Problem Setting: \nThe paper addresses the important problem of detecting out‐of‐distribution (OOD) samples in deployed neural networks without the need for any OOD validation samples or retraining. This setting is meaningful from a practical point of view. The idea of applying a normalized compression distance to capture spatial patterns in the feature maps is interesting and somewhat novel. However, several reviewers remain unconvinced about whether the proposed tweak is fundamentally more effective than existing ideas, or if it simply represents a slight modification to standard techniques that are effective only by carefully combining components.\n\n2. Experimental Evaluation and Comparisons:\nThe reviewers have noted that while the experimental results in the revised version (with additional experiments in the appendices) are encouraging, the choice of baselines and comparisons is still somewhat problematic. In particular, comparison with Mahalanobis-based methods using adversarial-sample based validation was missing in the main paper initially, and even though the authors have added extra experiments (MALCOM++), the experimental setup is still debatable. There is a concern that the improvements might stem from differences in implementation or choice of hyperparameters rather than any fundamental superiority of using compression-based pooling. In addition, reviewer remarks suggest that standard baselines in less restricted settings should have been included to better justify the strict constraints imposed by the authors.\n\n3. Clarity and Significance:\nWhile the motivation is clearly stated in the revised manuscript and additional ablation studies were added, some questions remain about the overall significance. In particular, the work is constrained to a setting that few existing works have considered (i.e., no OOD validation and no retraining) but it is not entirely clear how commonly such a setting occurs in real-world applications. Furthermore, some of the theoretical and practical benefits are not fully elaborated upon; reviewers have asked for more detailed justification regarding the necessity of these constraints and whether they are inevitable in safety-critical systems such as autonomous driving.\n\n4. Author Response and Revisions:\nThe authors have provided a comprehensive response and added new experiments, ablation studies (including comparisons with a variant that uses adversarial samples for validation), and clarifications. While these revisions improve the paper, several reviewers have maintained a borderline or weak reject stance. In particular, concerns about fair and informative comparisons, reproducibility (with some replication issues of prior works), and the overall effectiveness of the proposed technical novelty persist.\n\n5. Final Assessment:\nAlthough the paper is clearly written and the topic is relevant and timely, the cumulative concerns regarding experimental setup, comparative effectiveness, and the significance of the contributions lead to the conclusion that the improvements made in the rebuttal do not fully settle the key reservations raised by the reviewers. Therefore, while the work shows promise, it does not meet the high standards needed for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper “Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem” provides an intriguing and rigorous theoretical contribution by linking the representational capacity of deep neural networks with a classical result in dynamical systems. By leveraging Sharkovsky's theorem, the authors characterize the depth-width trade-offs needed to represent periodic functions or functions having particular periodic points. This work addresses a prominent open question raised by Telgarsky regarding the limitations of shallow networks by establishing lower bounds and providing insight into why certain functions are difficult for shallow networks to approximate.\n\nReviewer feedback is largely positive. Reviewer 2 highlights the clarity of exposition and the significance of answering an important open question using insightful dynamical systems analysis, raising only minor points regarding empirical intuition and the role of bias terms. Reviewer 3 commends the clever use of Sharkovsky's theorem and suggests some improvements in structuring the paper to lead with the main contribution, a suggestion that the authors have already addressed in their revised version. Reviewer 1, while very positive, provided additional suggestions that were acknowledged and answered by the authors.\n\nThe author responses indicate that the questions regarding biases and empirical validation have been well-handled, with additional experimental results and clarifications provided in the updated manuscript. While some limitations remain regarding the direct practical utility (such as assessing the period in real tasks), this is recognized as an open and challenging problem within the theoretical framework and does not detract from the paper’s theoretical impact.\n\nGiven the novelty, rigor, and relevance of the theoretical results, as well as the authors’ willingness to improve clarity and presentation, the paper meets the standards expected from a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed analysis:\n\nStrengths:\n• The paper tackles an important and challenging problem—how to coordinate pre‐trained, diverse low‐level skills in a multi-agent setting. This is a gap in the literature and is clearly motivated.\n• The methodological contribution is a modular, hierarchical reinforcement learning framework that leverages DIAYN-style skill diversification. The idea of reusing pre-trained skills with expert-specified subtasks to enable the solution of complex collaborative tasks is compelling.\n• The experiments span varied environments, including manipulation tasks with dual Jaco arms and a locomotion task with two ant agents. The coordinated performance improvements over baselines (with both centralized and decentralized DIAYN baselines) illustrate the strength of the method.\n• The authors’ responses are thorough. They provide additional baselines, more experiments with varied T_low values, clarifications on notation, and discussions of design choices such as fixed low-level skill horizons.\n• The work is positioned well relative to multi-agent RL literature demonstrating that prior works on temporal abstraction in multi-agent setups do not handle the coordination issue as addressed here.\n\nWeaknesses:\n• While the method’s reliance on expert-specified subtasks limits its generality, reviewers acknowledged that for complex environments this additional structure appears necessary. The authors also note that unsupervised learning of such skills remains an open challenge.\n• Some concerns were raised regarding high variance and inconsistent reporting (e.g., reward vs. success rate) but the authors have addressed these by adding moving-average plots and supplementary success plots.\n• A question about whether the same ideas extend to learning without a priori subtasks is raised; however, the authors convincingly argue that DIAYN-only baselines fail in these complex tasks, highlighting the need for structured skill specification in such settings.\n\nOverall, the paper makes a significant contribution by presenting a practical method for coordinating separate skills in multi-agent settings and addressing the accompanying challenges. Given the experimental evidence, solid methodological foundations, and satisfactory responses to the reviewers’ concerns, the paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Relevance:  \nThe paper tackles the problem of predicting chromatin folding patterns (TAD-related characteristics) from ChIP-seq data by leveraging a bidirectional LSTM model. While recurrent neural networks have been widely applied, the paper’s contribution lies in combining a custom (modified) weighted loss with the use of sequential properties of epigenetic data for the prediction of 3D chromatin structure. Although Reviewer 3 noted that much of the applied technique (bidirectional LSTM) is standard, the application towards predicting Transitional Gamma from ChIP-seq data on Drosophila is not well‐covered in the literature. The authors have also expanded the literature review to distinguish their work from others. In an interdisciplinary application area—where biological insight is key—even an “incremental” methodological contribution could have significant impact.\n\n2. Clarity and Experimental Evaluation:  \nThe initial submission contained several points with unclear description of key design choices (for example the definition of the loss, feature construction, and hyper-parameter tuning) and baseline comparisons. The author responses have clarified these issues by expanding on the experimental details, data preprocessing, and providing additional evaluation metrics including MSE, MAE, and R² scores. Importantly, the authors have now ensured that all models are trained with the same loss function for a fair comparison, and new experiments were added to address negative error cases in training versus testing.\n\n3. Baseline Comparisons and Methodological Concerns:  \nWhile one reviewer suggested that additional comparisons (e.g., with vanilla LSTM, convolutional architectures, or even extended baseline models) would further strengthen the work, the authors have expanded the set of experiments and detailed their comparative analysis. Even though the bidirectional LSTM is not a novel architecture in itself, its use in this specific biological context and under a carefully weighted evaluation framework represents a meaningful contribution.\n\n4. Generalizability:  \nThere were concerns regarding how generalizable the approach is beyond the chosen dataset (Drosophila and Hi-C data at 20 Kb resolution) and additional epigenetic assays (e.g. ATAC-seq). The authors clarify that biological differences across organisms/cell types limit straightforward generalization of predictors of chromatin folding. This is a defensible point given the paper’s focus on a specific biological model and the intended contribution of understanding biologically meaningful relationships rather than a “one-size-fits-all” solution.\n\n5. Overall Assessment:  \nAfter revisions, the paper has addressed many of the early concerns regarding methodology clarity, evaluation strength, and experimental setups. Although methodological novelty is somewhat incremental, the work is sufficiently differentiated due to its novel application domain, careful experimental validation, and the biological insight it offers. This balance between established deep learning methods and an innovative application in computational biology makes the paper a strong candidate for a top-tier interdisciplinary conference.\n\nBased on all of the above, I conclude:\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed evaluation:\n\n1. The main merit of the paper is its novel theoretical contribution. In formalizing the intuition behind “hard” negative sampling and providing Theorem 2, the paper makes a meaningful advancement regarding variance reduction in negative sampling. This formalization, along with the derivations showing that “hard” negatives (drawn via an auxiliary adversarial model) lead to a higher signal-to-noise ratio, is both interesting and nontrivial. The clarity of the exposition—as noted by one reviewer—is also a plus.\n\n2. The experimental evaluation supports the theoretical claims. Although it is limited to two benchmark datasets (Wikipedia-500K and Amazon-670K), the results illustrate a significant reduction in training time relative to competitive baselines within the negative sampling framework. This is especially relevant given the focus on scalable extreme classification settings.\n\n3. There are legitimate concerns raised by reviewers regarding comparisons. Reviewer 2 and Reviewer 3 noted that the paper fails to situate its contributions fully in the context of other related works (e.g., Slice or DiSMEC) and to compare against full softmax (even on smaller datasets). However, the author response explains that the contributions are focused on a negative sampling paradigm and that the experimentally oriented comparisons outside this framework (such as those provided by DiSMEC) do not align directly with the paper’s purpose. The authors also indicate that further experiments (e.g., on EURLex) will be included in the final revision, and that the related work discussion will be expanded to incorporate important references.\n\n4. While the writing has minor grammatical issues and typesetting typos, the authors have acknowledged these and promised corrections in the final version. These issues, although they need fixing, are not central enough to merit rejection.\n\n5. The paper’s strength in theoretical insight and the clarity of the derivations (making the contribution accessible even to non-experts, as noted by Reviewer 1) outweigh the relatively narrow experimental comparisons. The focus on negative sampling (and its careful analysis) is well-motivated and provides novel insights that may spur further work, even if broader baseline comparisons are desirable. \n\nIn top-tier conferences, a paper that advances theoretical understanding and proposes a novel, scalable method (even if the experiments can be further extended) typically meets the standards for acceptance—provided the authors address the related work discussion and minor experimental suggestions in the final version. \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contribution: Although the paper combines several elements – a temporal point process formulation with a graph‐structured LSTM equipped with time gates, a stacked (deep) model architecture, and an attention-based selection mechanism – much of the contribution appears to be incremental. Reviewers have noted that while stacking and attention have been used before (even in the temporal domain with models such as JODIE and methods by Trivedi et al.), the new ingredients here (the DIP units and the particular selection mechanism) are not explained with enough theoretical or intuitive depth. Many of the modifications are presented as engineering improvements over prior work but without convincing justification that they capture properties that simpler models cannot.\n\n2. Analysis and Ablation: A major concern from the reviews is that the paper, while reporting improvements on interaction prediction and classification tasks, does not provide sufficient analysis of how and why the proposed modifications (e.g., the introduction of time gates, the k-hop subgraph extraction, scaling parameters, and the stacked fusion across time resolutions) drive the performance gains. The ablation studies are overall coarse (turning off entire modules) rather than showing a fine-grained sensitivity analysis (e.g., how varying k, depth L, or adaptive gate functions affect performance). This lack of detailed empirical insight limits our understanding of the contribution.\n\n3. Baseline Comparison and Experimental Setup: Initially, one key baseline (JODIE) was missing, and although the authors claim to have added it along with further experiments, there remains the issue that the proposed method appears to have many more parameters than the compared shallow models. It is unclear whether the improvement is simply due to deeper architecture and higher parameter count rather than due to the novel modeling of temporal and graph-structured interactions. Additionally, the experiments do not use the common benchmarks (like those used in DyRep or NeuralHawkes), which further makes it hard to gauge how the improvements stand among established dynamic graph methods.\n\n4. Clarity and Presentation: Multiple reviewers highlighted that several parts of the paper are hard to follow. The description of how the temporal dependency graph is built (using coloring/hashing) and how it integrates with the neural components is not sufficiently clear, despite added overviews and toy examples. Questions regarding computational cost, scalability, and how the model handles unevenly changing graphs have only limited explanation. Such presentation issues may hinder reproducibility and practical adoption.\n\nConclusion:\nAlthough the paper addresses an important problem – modeling time-evolving graphs with interaction processes – and the experiments show promising performance on a financial application and other tasks, the contributions seem incremental, the analysis is insufficient, and both the experimental comparisons and clarity of the presentation remain a concern for a top-tier conference. The innovations appear to be modest modifications that are not thoroughly motivated or analyzed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper poses an interesting problem—segmenting trajectories into reusable sub-skills with only weak supervision (i.e., knowing only which skills appear in a demonstration, with no ordering)—and shows application across several domains. However, the reviewers have raised several substantial issues:\n\n1. Scope and Motivation: There is a mismatch between the strong motivation (learning reusable skills for downstream tasks) and the actual experimental evidence, which mostly centers on per time-step segmentation accuracy. None of the experiments convincingly demonstrate the downstream utility of the learned skills, and a simple behavior cloning experiment to compose skills into a controller is missing or underwhelming.\n\n2. Novelty: The main algorithmic contribution is the use of log-sum-exp pooling derived from multiple-instance learning. While adapting known methods to weak supervision in this context is interesting, the novelty is limited as similar pooling techniques have been widely used in other domains. Reviewers noted that the incremental contribution does not clearly exceed existing methods.\n\n3. Experimental Evaluation: There are detailed concerns about underwhelming performance metrics (classification accuracy ranging only ~35–60%), sparse and unclear comparisons with relevant baselines (especially fully supervised oracle baselines), and insufficient analysis of the segmentation quality (e.g., lack of confusion matrices or temporal consistency evaluation). Moreover, certain experimental design details—such as hyperparameter tuning (like Gaussian smoothing), dataset requirements, and instance/socket selection—are not thoroughly reported or justified.\n\n4. Theoretical and Practical Limits: Reviewer 3 raised valid concerns regarding the inherent limitations of the weak supervision signal, particularly when certain skills always appear together. The paper does not sufficiently discuss the theoretical bounds or conditions under which the method is expected to succeed, making it unclear when the approach is applicable.\n\nWhile the authors provided clarifications in their response, these answers do not adequately address the need for stronger empirical evidence and more robust baseline comparisons. The overall contributions remain incremental, and the experimental validation does not clearly support the claims regarding the usefulness of the segmented skills in real downstream tasks.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper introduces a modular framework for multi-agent coordination by first pre-training diverse sub-skills with a DIAYN-style diversity objective and then learning a meta-policy to coordinate these skills. This setup builds on recent work in hierarchical reinforcement learning and unsupervised skill discovery, but it is tailored to address the coordination challenges specific to multi-agent (or multi-end-effector) systems.\n\nKey strengths include:\n• An interesting problem formulation: The work addresses the non-trivial problem of coordinating skills learned in isolation, which is an important and relevant challenge for complex, cooperative tasks in robotics and multi-agent systems.\n• Modular design and extensive experiments: The paper presents clear experiments on cooperative manipulation and locomotion tasks (e.g., using two Jaco arms and two Ant agents) and demonstrates that engagement of pre-trained skills coordinated by a meta-policy outperforms several baselines. The addition of centralized and decentralized SBD baselines clarifies how expert knowledge (predefined subtasks) is critical when DIAYN-only methods fail.\n• Thorough responses to concerns: The authors have addressed reviewer concerns about notational consistency, hyperparameter choices (such as the fixed T_low), and the need for comparisons with alternative methods. They provide additional experiments and analysis (e.g., moving-average curves, success rate plots, and ablations on T_low) to back their design choices.\n\nThe main critiques come from relying on predefined subtasks to segment the task, which may constrain the generality of the approach when compared to fully unsupervised skill discovery. However, the authors convincingly argue that for complex coordination tasks such domain expert guidance is beneficial, and the framework is designed to complement, not replace, fully unsupervised methods.\n\nOverall, the paper makes a solid contribution by combining ideas from DIAYN and hierarchical learning to effectively coordinate diverse skills across multiple agents. Although the novelty is incremental relative to some recent works, the application to complex multi-agent coordination along with detailed experimental validation and careful handling of reviewer feedback makes it suitable for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a novel method (αVIL) for dynamic task weighting in multitask learning using meta-optimization based on task-specific parameter updates. While the paper is well written, easy to follow, and introduces an interesting perspective on using model update differences to adjust weights, several critical issues adversely affect its contributions for a top-tier conference.\n\n1. Theoretical Justification: All reviewers noted that the paper lacks a deeper theoretical analysis or motivation. The intuition behind the adjustments (e.g., subtracting 1 from the alpha parameters) is not rigorously justified, which makes it difficult for readers to assess when and why the proposed method would be expected to outperform other approaches.\n\n2. Empirical Evaluation: Although experiments are conducted on computer vision (MultiMNIST) and several natural language understanding tasks, the results are mixed. Improvements over strong baselines (especially DIW) are marginal and often within the standard deviation range. This weak empirical impact, combined with the limited set of baselines, makes it tough to conclude that the method offers a substantial advancement over existing techniques.\n\n3. Novelty and Comparisons: Multiple reviewers pointed out the strong resemblance of αVIL to existing meta-learning and dynamic task weighting methods. The proposed method appears to be a small variation (e.g., accumulating task-specific weight adjustments) rather than a fundamentally new approach. Moreover, some design choices are ad hoc, and the distinction between the new alpha parameters versus direct task weight optimization is not sufficiently clarified.\n\n4. Ablation Studies and Analysis: Greater insight into how the task weights evolve over time, additional ablation studies, and further experiments with various auxiliary tasks (beyond MultiMNIST and the limited set in NLU) would have strengthened the contributions. The current experimental design and analysis leave open the possibility that the improvements could be due to a data-specific phenomenon rather than general effectiveness.\n\nGiven these concerns—primarily the lack of theoretical backing, only marginal empirical gains, and concerns regarding novelty—the paper does not yet meet the high standards expected for a top-tier conference publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning followed by the final decision.\n\n–––––––\nDetailed Reasoning:\n\nStrengths:\n1. Novel and interesting idea: The paper proposes a new method to extract disentangled representations by learning binary masks over transformer weights or hidden activations without fine-tuning the underlying BERT parameters. This is a clever twist on model pruning and sharpening interpretability and robustness.\n2. Robustness to spurious correlations: The experiments target disentangling factors like sentiment versus genre and syntax versus semantics. This focus on disentanglement and reducing reliance on spurious correlations is timely and addresses important robustness issues.\n3. Experimental analysis and clarity: The paper describes experiments on two distinct tasks/datasets (movie reviews for sentiment/genre and separate syntax/semantics tasks). The authors make an effort to compare their method with both adversarial and VAE-based baselines and provide quantitative as well as qualitative (e.g., Figures 3 and 4) insights.\n4. Thoughtful author responses: In their response, the authors appropriately clarified concerns such as the rationale for selecting specific genres, the scalability of the triplet-loss formulation, details on which layers are masked, and comparisons with relevant baselines (including additional VAE baselines). They also addressed potential implementation and interpretability issues such as the overlap of masked weights/activations.\n  \nWeaknesses:\n1. Limited experimental domains: Multiple reviewers raised concerns that experiments were limited to a movie review domain (using only Drama and Horror for spurious correlations) and a couple of disentanglement tasks. While the authors motivate the choice as a way to create pronounced spurious correlations, broader experiments (e.g., on additional domains such as books or music, or with more than two labels) would strengthen the claim of generality.\n2. Scalability of the method: The current formulation (e.g., using a triplet loss) is demonstrated only for two factors, and while the authors mention generalization to more factors is possible (albeit potentially heavy in terms of constructing triplets), further exploration would be beneficial.\n3. Increased effective parameter count: Although the masks are binary and relatively lightweight, the method increases the number of tunable parameters. This point, while mitigated by memory efficiency, is worthy of further analysis in future work.\n\nOverall Assessment:\nThe paper presents a solid and novel approach to disentangling representations in pretrained language models. The innovative use of masking strategies to \"select\" sub-networks that encode specific attributes is compelling and has potential impact, especially given the growing interest in model interpretability and robustness. While experiments are limited to selected domains and a limited number of factors, the authors provide clear empirical evidence that their method can outperform standard fine-tuning and some baseline disentanglement approaches. The paper is well written and the responses adequately address many reviewer concerns.\n\n–––––––\nFinal Decision: Accept"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by the final decision.\n\nDetailed Reasoning:\n1. Novelty & Contribution: The paper proposes an improvement over Defense-GAN by using an autoencoder to provide a better latent space initialization for a search-based purification process. Although the idea of reducing computational cost is attractive, it is largely an incremental modification on existing ideas (Defense-GAN, BiGAN) rather than a fundamentally new approach. Several reviewers pointed out the contribution as being too incremental.\n\n2. Technical Soundness & Clarity: The method is described in detail with an encoder-assisted search process. However, reviewers noted that the paper is hard to follow at times, with unclear descriptions about the training and inference phases. The presentation issues (including typos and some extra pages) further detract from the overall clarity. While the author responses have clarified many points (e.g., number of gradient steps, pseudo-code available in the appendix), the revised presentation still appears to struggle with clarity and completeness when judged by top-tier standards.\n\n3. Experimental Evaluation: The experiments are primarily carried out on MNIST and FashionMNIST; although the authors have supplied additional results on CelebA, key comparisons (e.g., with TRADES) are still missing. Moreover, the scalability of the method to more challenging datasets such as CIFAR-10 is deferred. The defense is evaluated against a range of attacks, including white-box and BPDA attacks. However, the reliance on obfuscated gradients (with the search process itself serving as an optimization stage) raises doubts about whether the improvements truly reflect robust defense or just gradient masking. Reviewers noted the absence of a clear discussion on false positives in detection as well.\n\n4. Comparison to Baselines: Although there is a comparison with PGD adversarial training and Defense-GAN, some baselines such as TRADES (an important state-of-the-art adversarial training method) are only mentioned without experimental results. This weaker baseline comparison limits confidence in the method’s overall benefits.\n\n5. Author Response: The authors have provided extensive responses and additional experimental comparisons that address some concerns. They demonstrated through extra experiments that the method can achieve high accuracy under certain conditions and clarified many implementation details. However, the core contribution still appears as an efficiency improvement rather than a robust, widely applicable new idea.\n\nFinal Decision:\nWhile the method offers a modest improvement in computational efficiency over Defense-GAN and the responses helped clarify some technical details, the overall contribution is too incremental, the experiments remain limited in scope, and the presentation issues persist. In light of these factors, I conclude that this paper does not yet meet the rigorous standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a Domain-Adaptive Multibranch Network designed for unsupervised domain adaptation via adaptive computation graphs. The core idea is both novel and practical—it builds on adaptive parameters sharing across different “flows” or branches that are selectively weighted based on the domain characteristics. Although the concept of adaptive computation itself is not entirely new, its application in the domain adaptation context and the focus on handling multiple domains in parallel represent solid contributions.\n\nReviewer 1 provided mostly minor suggestions regarding naming conventions, linking and reference style that are easily fixable. Reviewer 2 acknowledged the technical soundness and novelty of the approach, while emphasizing that the experimental validation is somewhat limited. In response, the authors have provided clarifications and additional experiments (e.g., using MCD in addition to DANN) that reinforce the technical soundness of the proposed method.\n\nReviewer 3 raised concerns regarding the lack of extensive ablation studies, comparisons with state-of-the-art methods, and sensitivity analyses. The authors’ responses indicate they are addressing these concerns by including additional experiments on adapting different computational capacities per branch, discussing the behavior of the gating mechanism in detail, and extending their experiments to multi-source domain adaptation. Moreover, the authors plan to update the literature review and references accordingly.\n\nOverall, although the paper initially had some gaps in experimental comparisons and ablation studies, the author responses show that these issues are being rectified or clarified with additional results. The novelty in adapting the computational paths per domain and the potential for extension to other methods make this a meaningful contribution to the field. Based on these factors, the paper meets the standards expected at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a framework for dynamically adjusting task weights in a multitask learning setting by directly leveraging parameter updates. While the writing is clear and the method is motivated through empirical results on computer vision and NLU tasks, several major concerns remain that impact the paper’s suitability for a top-tier conference.\n\nFirst, the method lacks sufficient theoretical grounding. All reviewers, and especially Reviewer 1, stressed that a formal justification or clearer analysis of why the proposed approach should work is missing. This is a serious shortcoming given the expectation for a level of rigor in top-tier venues.\n\nSecond, the empirical results, while promising in some settings (e.g., on MultiMNIST), are only marginally better than strong baselines like Discriminative Importance Weighting (DIW) and standard multitask training. Reviewer 2 and 3 highlight that many of the gains reported are within the standard deviation, suggesting that the improvements might not be statistically significant or robust across different settings. The experimental setup appears somewhat limited (for example, using MultiMNIST with only one auxiliary task), and additional ablation studies or experiments would be needed to convincingly establish the method’s benefits.\n\nThird, some details of the algorithm are unclear. Both Reviewer 2 and Reviewer 3 question certain design choices (such as the subtraction of 1 from the alpha-values) and whether those steps can be simplified without losing functionality. The lack of clarity in these key parts of the algorithm raises concerns about both the novelty and the robustness of the proposed approach.\n\nWhile the authors’ response clarifies some points and outlines plans for additional experiments and theoretical discussion in the camera-ready version, the current submission does not yet meet the expected standards for a top-tier conference. The incremental improvements over strong baselines, coupled with the lack of a robust theoretical framework, suggest that the contribution might not be substantial enough for acceptance in its current state.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is a summary of my detailed reasoning:\n\n1. Novelty and Concept:  \n• The paper tackles automatic music accompaniment generation by treating Mel‐spectrograms as images and applying CycleGAN for unpaired translation. This is an interesting idea and extends previous work on using visual representations for various MIR tasks.  \n• However, many aspects of “treating audio as images” have been used in other MIR contexts. The paper’s claim of being the “first” in this area is at best overstated, and while the authors have clarified their innovations, the contribution feels somewhat incremental compared to the state‐of‐the‐art.\n\n2. Technical and Methodological Details:  \n• Multiple reviewers noted that key methodological details were missing. These include specifics on the CycleGAN architecture (e.g., network depth, hyperparameters, training regimes), the pre-processing details (e.g., windowing schemes, quantization formulas for the spectrograms), and especially the details surrounding phase retrieval and waveform reconstruction.  \n• Although the authors have provided extensive responses outlining additions (with a new section, equations, and citations), it is not entirely clear that these additions fully address the reproducibility concerns – in particular, the lack of an ablation study (e.g., comparing training on MUSDB only versus inclusion of FMA data, or experiments that compare variants of CycleGAN) remains a point that lessens the overall impact.\n\n3. Evaluation Concerns:  \n• The evaluation involves both subjective human ratings and automatic metrics (modified STOI and FID). The subjective evaluation is inherently challenging and the authors’ approach of binarizing the targets and relying on expert consensus (with a small number of annotators) does not completely resolve concerns on evaluation rigor.  \n• The potential contamination caused by source separation inaccuracies (with Demucs) is acknowledged by the authors (and even manually inspected), but no solid quantitative ablation is presented to assess its effect on the final output quality. One reviewer’s suggestion to demonstrate performance when using only high-quality stems remains unaddressed by proper experiments.\n\n4. Scope and Clarity:  \n• Although the paper’s title and abstract have been defended by the authors as justified within the broader context of “music production”, the mismatch between the ambitious title and the relatively narrow focus on accompaniment generation is still a concern from a top-tier venue perspective.  \n• Many aspects of exposition (e.g., the explanation of the experimental settings, the rationale behind the four evaluation attributes, and the details of the Demucs implementation) needed improvements, and while the author response indicates many changes have been or will be made, the current form suggests the paper is not yet mature enough in terms of clarity and reproducibility.\n\n5. Overall Impact and Readiness:  \n• Although innovative in certain aspects, the paper suffers from several experimental and presentation shortcomings. Remedial experiments (such as comparisons with alternative architectures or ablation studies) would have strengthened the claim that the proposed approach is not only novel but also robust and generally applicable.  \n• While the authors’ responses are thorough in addressing many of the reviewers’ concerns, the remaining issues—especially those related to experimental rigor and reproducibility—make it difficult to justify acceptance at a top-tier conference without substantial additional work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is a detailed analysis of the paper:\n\n• Relevance and Novelty: The paper addresses an important and practical problem—ecological inference from aggregated data—which is highly relevant to political science and computational social science. The idea of combining deep neural networks with ecological inference, including Bayesian extensions and multi-task architectures, is interesting. However, the technical contributions seem incremental, and the proposed approximation to the loss function is closely related to existing methods (for example, techniques in collective graphical models, distribution regression, or learning with label proportions).\n\n• Clarity and Presentation: All reviewers agree that the paper suffers from clarity issues. The narrative is hard to follow, with several parts of the methodology defined in later sections rather than upfront. Key details such as formal definitions of inputs, the construction of the loss functions (Poisson binomial/multinomial), and the description of experimental tasks are presented in a confusing manner. Additionally, numerous typos, writing issues, and formatting errors distract from the presentation.\n\n• Experimental Evaluation and Reproducibility: The experimental section has significant shortcomings:\n  – Important experimental details are missing, such as how train/test splits are constructed, hyperparameter choices, and precise descriptions of baselines.\n  – The baseline comparisons are incomplete. For instance, standard models such as the regular multi-level models (MLM) are not adequately compared, making it hard to isolate how much of the performance gain is due to deep learning versus improvements from better implementations/configurations.\n  – Some results (e.g., a standard linear regression outperforming more complex models in test set R²) raise questions that are not convincingly answered in the manuscript.\n  – The experiments are not detailed enough to ensure reproducibility or to let the reader interpret the model’s behavior (e.g., the claimed improvements in capturing crossover voting behavior are only conjectured without quantitative support).\n\n• Related Work and Positioning: The paper misses important related work in the areas of learning with label proportions, distribution regression, and multiple-instance learning. As a result, it is not clear how the paper’s contribution differentiates itself from established methods within these fields.\n\nOverall, while the topic is important and the application interesting, the paper currently falls short of top-tier standards because:\n  – The technical contributions are not enough beyond existing approaches.\n  – The writing, clarity, and presentation need substantial improvement.\n  – The experiments are incomplete and lack reproducibility, with insufficient baseline comparisons and inadequate analysis to support the claimed benefits.\n\nGiven these factors, the paper does not meet the standards for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning followed by my final decision.\n\n1. The paper addresses an important issue in adversarial robustness—reducing the gap between the nonconvex objective used in robust training and its convex relaxation. Improving the tightness of certification bounds is highly relevant for the community.\n\n2. The reviewers raised substantial concerns regarding clarity of the analysis, exposition regarding the convex relaxation (e.g., with respect to Fast-Lin versus the “optimal” convex relaxation), and details about the experiments and baselines. Reviewer 2 in particular found the theoretical justification unclear, while Reviewer 3 noted some presentation issues and a desire for additional datasets.\n\n3. The author response is detailed and thorough. They clarify confusion regarding the theoretical presentation, address the notation issues (e.g., distinguishing between the nonconvex problem and its convex relaxation), and add an illustrative example in the appendix to better demonstrate when the regularizer leads to tight bounds. They also explain why their regularizer is not merely “killing” ambiguous ReLUs and how it differs from previous works (e.g., Xiao et al.).\n\n4. Importantly, additional experimental results (now including comparisons on CIFAR10 against strong baselines such as IBP and CROWN‐IBP) have been added. The results show state‐of‐the‐art certified accuracies under relevant perturbation budgets. This answers one of the reviewers’ concerns about the limited experimental evaluation.\n\n5. While some concerns were initially raised (e.g., regarding presentation and theoretical clarity), the authors have substantially revised the manuscript. The changes improve clarity without compromising the novelty of the approach. The enhanced experimental validation further strengthens the paper’s contribution.\n\nGiven that the revised manuscript addresses the reviewers’ concerns, improves clarity in the analysis, and provides compelling experimental evidence demonstrating its advantages over baselines, I consider that the paper now meets the standard required for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis followed by the final decision.\n\nReasoning:\n\n• Novelty and Contribution: The paper makes a promising contribution by extending Generative Query Networks with an object-oriented decomposition for 3D scene representation. The idea of learning factorized, hierarchical 3D object representations in an unsupervised manner is interesting and could potentially be impactful.\n\n• Methodology and Technical Presentation: Almost all reviewers noted that while the method is technically interesting, its presentation is significantly hindered by inconsistent notation, poor clarity (particularly for readers not already familiar with GQN), and an overall complex exposition. Although the authors’ revision appears to ameliorate some of these issues, there is still lingering concern regarding clarity and the accessibility of the detailed methodology (e.g., exposition in Section 2 and appendix details).\n\n• Experimental Evaluation: A substantial number of reviewers expressed concerns about the experiments. Key issues include the lack of ablation studies to isolate contributions, unclear or weak quantitative evidence (e.g., only marginal improvements in MSE/NLL without clear significance), and insufficient comparisons with strong baselines from both the GQN literature and broader 3D vision methods. Some reviewers also noted that qualitative results, while impressive, are not enough to fully validate the benefits of the proposed object-aware decomposition versus a scene-level representation.\n\n• Related Work and Positioning: Multiple reviewers pointed out that the work does not sufficiently situate itself among the vast literature on unsupervised 3D representation and segmentation—the discussion is missing key comparisons and appropriate citations, which is especially important for work claiming a “first” in unsupervised object identification in 3D.\n\n• Overall Judgment: Although the idea is promising and the updated version addresses several of the clarity and notation issues, the experimental evidence and detailed analysis remain insufficient for a top-tier conference publication. The paper would need stronger, more comprehensive experiments (including ablation studies and comparisons with more recent related approaches) and further improvements to readability before it could meet the standards required.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Detailed Reasoning:\nThe paper addresses an important biological problem—predicting features of the 3D chromatin structure (specifically TAD characteristics) from epigenetic data—and uses a bidirectional LSTM to leverage the sequential nature of DNA. While the use of a standard bidirectional LSTM is not in itself novel, the application to predict TAD characteristics from ChIP‐seq data (instead of using Hi‑C maps as input) is sufficiently differentiated from prior work. Several reviewer concerns were related to clarity in model comparisons, description of the data and feature representation, and the design of the custom loss function. The authors have responded in detail by expanding the description of their methods, clarifying experimental design (including ensuring that all baseline models are trained under the same loss function conditions), and providing a broader set of evaluation metrics. They have also extended their literature review to distinguish their work from similar studies, arguing that previous work either focused on different targets or appeared in less-central venues.\n\nAlthough there remains some concern about methodological novelty—as the paper uses a relatively standard bidirectional LSTM architecture—the new insights offered (including the effective use of a modified weighted MSE tailored to the distribution of the genomic target metric) along with the promising biological application, make this an interesting contribution. The improved clarity and additional baseline and metric comparisons strengthen the paper’s evaluation. Given that the paper provides experimental evidence that the sequential modeling aspect contributes to performance and that it opens new directions (including feature importance analysis in biological data), the work meets the standards of a top-tier conference, even if the methodological novelty is more incremental than revolutionary.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is our detailed reasoning:\n\n1. Novelty and Motivation: The paper proposes a new framework – Differentiable Hebbian Consolidation – that leverages the intuitive, neurobiologically inspired idea of splitting fast (plastic) and slow (stable) weights. The approach is motivated by complementary learning systems theory and aims at alleviating catastrophic forgetting. This idea, while related to Hebbian softmax approaches, is re-formulated specifically for continual learning. Overall, the approach is innovative and well-motivated.\n\n2. Clarity and Presentation: Although some reviewers initially had issues following aspects of the notation and details (e.g., clarifying what is meant by pre/post-synaptic connections or the indexing in various equations), the authors have provided substantial clarifications and even added a figure to help readers. These improvements bolster clarity and make the paper accessible—even for those not already expert in Hebbian learning. The overall presentation has improved in response to feedback.\n\n3. Experimental Evaluation: The experiments have been carried out on standard continual learning benchmarks such as Permuted MNIST, Split MNIST, and a 5-Vision Datasets Mixture. The paper also introduces a difficult imbalanced variant of Permuted MNIST. In addition, the authors have addressed reviewer concerns by providing new experiments on Split CIFAR-10/100 in the appendix. While there are still debates over whether the proposed method surpasses state-of-the-art synaptic consolidation methods on all metrics and if it works best only when coupled with those methods, the authors’ results show improvements over naive fine-tuning and forward transfer benefits. The evaluation, though not perfect, is competitive for a top-tier venue.\n\n4. Comparison to Related Work and Analysis: Reviewers noted mixed performance when DHP Softmax is used alone versus when it is combined with methods like SI or EWC. However, the authors provide clear explanations on why their method may be complementary to task-specific consolidation techniques. The paper distinguishes itself from previous rehearsal or replay techniques by focusing on task-incremental learning and by avoiding the need to store past examples. The discussion—and the additional experimental results—help position this work within the broader literature on fast weights and continual learning.\n\n5. Overall Assessment: While one reviewer voiced concerns about experimental strength and generality (especially on more challenging scenarios such as class-incremental settings with many classes), the other reviewers appreciated the novelty and the theoretical grounding. The author response has addressed the major concerns regarding clarity and additional experiments. Although a few details remain open for further exploration in future work (e.g., scaling to even larger numbers of classes), the current contribution has sufficient merit and experimental support to justify acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed evaluation and reasoning:\n\n1. Contributions and novelty:\n • The paper claims to be the first to prove global convergence of SGD for training a single‐layer (nonlinear) generator within a WGAN framework, via a carefully chosen “quadratic” discriminator. In a strictly theoretical sense, obtaining convergence proofs for GAN training is challenging and the results could be of interest.\n • However, a significant part of the contribution hinges on studying a generator/discriminator pair that deviates substantially from how WGANs are used in practice. By considering very simplified discriminators—a rectified linear one and a quadratic one—the work sidesteps many difficulties encountered in standard settings.\n\n2. Concerns raised by reviewers:\n • All reviewers stress that the choice of discriminator (especially the quadratic discriminator) is too simple. This raises doubts as to whether the analysis genuinely explains why WGANs can be trained successfully with SGD on practical architectures. \n • Reviewer 1 and Reviewer 3 point out the misalignment between the studied framework and the actual WGAN formulation (for instance, handling the Lipschitz condition for unbounded data). They note that the simplified discriminator does not capture the “Wasserstein” nature of the problem.\n • Reviewer 2, while acknowledging the technical difficulty of analyzing gradient descent-ascent, remains unconvinced that limiting consideration to a quadratic discriminator is a strong justification. Even if one views this choice as lending a parametric sample complexity rate, it is not clear that this result will generalize to settings of more interest.\n\n3. Author response:\n • The authors argue that their choice of a simple discriminator is a strength because it helps both the convergence and the sample complexity analysis. They also point out that using a more complicated discriminator would only worsen the sample complexity.\n • However, many questions remain. In particular, the primary shortcoming is that the simplified setup does not truly capture the dynamics of practical WGANs. The reviewer concerns about the treatment of the quadratic discriminator and the inability to address more complex, interaction-rich discriminators are only partially resolved.\n\n4. Overall evaluation:\n • While the technical analysis appears sound and the convergence results under the simplified setting are nontrivial, the overall impact of the paper is limited by its choice of a discriminator structure that is far removed from standard practice with WGANs.\n • The main contributions, although interesting from a purely theoretical standpoint, seem to provide limited insight into why WGANs are successfully trained in practice. In a top-tier conference, results are expected to either push theory with broader applicability or offer practical insights that help guide real-world deep learning practice. This paper’s focus on a restricted setting makes its relevance questionable.\n\nGiven these considerations—the limited modeling of realistic WGAN settings, the discrepancy between the theoretical model and practice, and the unresolved concerns raised by reviewers—I judge that the work does not meet the impact and generality requirements expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a multi-task learning framework that models multi-level task dependency through an attention mechanism and a decomposition approach to reduce parameter complexity. The intended contributions are to capture both the general task dependency and the data-specific (e.g., word/node level) dependency, particularly exploiting the hierarchical nature of text and graph data. The authors also emphasize that their approach reduces the quadratic growth of parameters present in previous methods.\n\nHowever, the reviewers raised several major concerns:\n\n1. Clarity and Presentation: Multiple reviewers noted that the manuscript is hard to follow and full of grammatical errors. The presentation suffers from vague definitions, notation inconsistencies (e.g., indices used in equations), and unclear motivation particularly in how the attention mechanism yields a sample-dependent task dependency. Despite the authors’ extensive response and promises to restructure and proofread, the clarity issues remain a major hurdle.\n\n2. Novelty and Relation to Prior Work: Several reviewers questioned the extent of the novelty of the approach. The similarities to prior works (e.g., Taskonomy and other multi-task attention approaches) were highlighted, and it was not clear enough why the proposed multi-level (both general and data-specific) dependency modeling is significantly better. The comparisons to the most related methods, including evaluation baselines like “end-to-end multi-task learning with attention,” were missing or not fully convincing. Although the authors clarified key differences in their response, the experimental comparison against these baselines remains inadequate.\n\n3. Experimental Evaluation: Reviewers observed that the evaluation section needs to be extended. Specific improvements suggested include:\n   - A stronger motivation and clearer setup for the sample-dependent attention mechanism.\n   - A detailed description of the datasets and experimental settings (e.g., ensuring the training–test split guarantees no data leakage).\n   - Inclusion of an ablation study comparing the proposed model with and without the attention-based task dependency (using just a general dependency matrix, for example).\nThe author response indicates that some of these evaluations have been added or will be added, but the current state of the paper still appears to have insufficient experimental depth as per reviewers’ expectations.\n\n4. Theoretical and Technical Details: The paper’s technical exposition (e.g., the mapping and decomposition method, the “position-wise mutual attention mechanism”) is not detailed enough, making it difficult to judge the correctness and the potential impact of the method. Reviewers expressed serious concerns regarding the mathematical formulation and clarity of implementation.\n\nOverall, while the idea of capturing multi-level task dependency, particularly for hierarchical data such as text and graphs, is interesting and potentially impactful, the submission in its current form suffers from substantial writing and presentation issues, insufficient clarity in technical details, and inadequate experimental comparisons to key baselines. As work intended for a top-tier conference, these issues are critical.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nAll reviewers consistently raised significant concerns regarding both the theoretical exposition and the experimental validation of the proposed method. The primary issues are as follows:\n\n1. Theoretical Foundations and Clarity:  \n • The paper builds its main motivation and algorithm on a theoretical foundation that largely refers to unpublished work. This reliance on inaccessible results makes the derivations and theoretical guarantees difficult or impossible for the reader to verify, undermining the paper’s scientific rigor.  \n • Several reviewers noted that important aspects of the theory (e.g., the derivation of the clustering schemes or the explanation of when neurons can be safely removed) are not clearly explained or rigorously stated. Key contributions, such as the precise formulation of Remark 4.1 or the equivalence of critical equations, are not presented in a verifiable manner.  \n • The notation is inconsistent and the overall presentation is confusing, with many definitions spread across footnotes instead of integrated into the main text.  \n\n2. Experimental Evaluation:  \n • The experimental evidence is limited to toy experiments and MNIST, a dataset that is too simple to convincingly demonstrate the advantages of a new pruning technique for deep neural networks.  \n • There is a lack of comparisons to existing state-of-the-art pruning methods. Without comparisons or statistical significance measures (e.g., standard error), it is difficult to assess whether the proposed reduction algorithm actually improves upon existing approaches.  \n • Some aspects of the “reduction” (such as the vector representations for clustering or the motivation behind the affine skip connections) remain poorly justified by experiments, thus raising questions about their practical impact.\n\n3. Overall Contribution and Readability:  \n • While the idea of using properties of ReLU networks for controlled neuron reduction is intriguing, the paper currently fails to communicate its contributions in a clear, accessible, and verifiable manner.  \n • The writing contains numerous typos and grammatical issues, and the presentation demands a major overhaul before it can be reliably understood or its claims validated.  \n • The authors’ response shows a willingness to improve the clarity and extend the experimental study, but these revisions seem critical and fundamental rather than minor fixes.\n\nIn light of the above issues—specifically the heavy reliance on unpublished theoretical material, inadequate experimental validation, and the overall lack of clarity in communicating the contributions—the paper does not yet meet the standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\nStrengths:\n• The paper tackles an important challenge in reinforcement learning—exploration in continuous control—and proposes a coherent framework that unifies step‐based and trajectory‐based exploration. This topic is of clear interest to the community.\n• The authors extend a prior idea (van Hoof et al., 2017) to deep neural network policies by focusing on injecting noise into the last layer and analytically marginalizing over these parameters. This modification makes the approach scalable for deep RL methods.\n• The derivation and accompanying theoretical treatment are rigorous and well documented. The detailed ablation studies, especially for on-policy methods like A2C and PPO, are persuasive.\n• The manuscript is generally well written (with only presentation issues that can be remediated) and the method appears to be reproducible given the details provided in both the main text and appendix.\n• The authors’ responses address several concerns from the reviewers—for instance, the integration with SAC has been revised, key hyperparameter choices have been clarified, and concrete plans for presentation improvements have been outlined.\n\nAreas for Improvement:\n• While the integration for on-policy methods (A2C, PPO) is promising, the off-policy (SAC) component was originally handled with a heuristic approach. Although the authors now propose a revised formulation that unifies the treatment, the empirical results on SAC are not yet fully mature in the paper. However, the authors indicate that updated experimental results will be provided shortly.\n• Some presentation issues (e.g., very dense sections and small figure fonts) have been noted. These issues seem addressable in the final version.\n• One reviewer noted that the exploration experiments are limited to Mujoco tasks. Although a broader range of experiments would strengthen the work, the authors justify their focus on undirected exploration methods for continuous control, which is acceptable given the scope of the paper.\n\nOverall Assessment:\nDespite the somewhat incremental nature of extending an existing idea, the paper’s contributions—chiefly, making generalized exploration practical for modern deep RL methods—are nontrivial and valuable. The methodological contributions, theoretical clarity, and strong empirical results (especially on the on-policy methods) make this paper a solid contribution. With the planned revisions and improved integration for SAC, the work meets the standards expected of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a new “Block Minifloat” (BM) representation with a novel twist: by exposing the exponent bias to enable training with fewer exponent bits, it pushes the design into a more hardware‐efficient regime (especially when combined with Kulisch accumulators). This work covers both the algorithmic challenges (dynamic range, error analysis, training convergence in low precision) and hardware implementation (RTL synthesis, area and power estimation) aspects. The experimental evaluation across different network architectures (including ResNet variants and VGG-16) and datasets demonstrates that BM representations (in particular BM6 and BM8 configurations) can almost match FP32/FP8 performance while achieving significant reductions in hardware area and energy.\n\nThe reviewers raised concerns regarding the detailed explanation of certain aspects such as the accumulator design, the formulation and interpretation of key equations (like the ones used to choose the block size), and the hardware support for denormals. In addition, there were questions about the overhead incurred when supporting multiple BM formats in the backward and forward passes. The author response provided detailed clarifications, additional mathematical exposition (with new equations added) and more detailed hardware evaluation breakdowns that address these issues. Although some points such as training on larger networks (e.g., ResNet50) were mentioned as being constrained by training times, the overall experimental focus on embedded networks is justifiable given the paper’s hardware-centric motivation.\n\nMoreover, the hardware evaluation was deemed rigorous compared to the scope of similar quantization and low-precision training works. Although the idea of shared exponent bias is not entirely new, the authors’ application of this concept to push the efficiency advantages of the Kulisch accumulator in the context of deep neural network training is a valuable contribution. The combination of clear hardware synthesis results, energy and area analysis, and detailed response to reviewer concerns strengthens the paper considerably.\n\nGiven the paper’s solid contributions both in novel representation and in hardware evaluation, and taking into account that the authors have addressed the key concerns raised by the reviewers, the paper meets the high standards expected of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper “MQES: Max‐Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning” proposes an exploration framework that incorporates an information‐theoretic principle combined with distributional Q-estimation to separately capture epistemic and aleatoric uncertainty. In principle, this is an interesting direction that might offer advantages over standard methods in difficult or sparse‐reward settings.\n\nHowever, multiple reviewers raised significant concerns regarding several aspects. First, the clarity of the presentation is a serious issue: many reviewers noted that the formulation in Section 4.1 (i.e., key equations, notation, and the definition of the mutual information objectives) is vague and difficult to follow. Although the authors claim to have rewritten parts of the theory in their revised version, many reviewers still warn that the theoretical derivations and explanations remain confusing and insufficiently rigorous.\n\nSecond, the experimental evaluation is deemed preliminary. Several review comments stressed that while improvements over DSAC (and sometimes SAC/OAC) are reported on Mujoco tasks—especially in sparse or difficult settings—the absolute gains are not consistently remarkable, and the evaluation (with only five seeds in some cases) lacks further statistical significance analysis (e.g., p-values) or comprehensive comparisons (e.g., with more established exploration strategies such as VIME and additional ablations). Although the authors have added some ablation studies and sensitivity analyses in the revised version, many reviewers still felt that the experimental details (including hyperparameter influences, settings, and the impact of using only one type of uncertainty) were not adequately discussed.\n\nFinally, the paper’s overall contribution, while novel in intent, is clouded by significant issues in both clarity and empirical validation. For a top-tier conference, both the theoretical formulation and experimental demonstration must be communicated with high clarity and rigor. The current submission—even after the author response—leaves open many questions regarding the precise behavior of the proposed method in different regimes, the way the exploration framework integrates with standard actor-critic methods, and the interpretation of the mutual information objectives.\n\nIn summary, while the idea is promising and potentially impactful, the clarity, rigor of the theory, and the empirical evaluation do not yet meet the standards expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a method for multitask learning that dynamically weighs auxiliary tasks using a meta-optimization approach. The proposed algorithm, αVIL, is presented in a clear and intuitive manner, and the empirical evaluation over computer vision and natural language tasks is a plus. However, there are several concerns that impact its suitability for a top-tier conference.\n\nFirst, there is a notable lack of theoretical justification or formal analysis of why the dynamic task weighting via α optimization should be expected to yield improved performance. While the use of model updates to adjust task weights is an interesting idea, the paper’s explanation remains mostly heuristic. Reviewer comments indicate that this point is central, especially given that similar concepts have been explored in meta-learning (e.g., MAML and related approaches).\n\nSecond, the experimental results are only marginally better than existing baselines, including an approach that is closely related (Discriminative Importance Weighting). The improvements, where present, are minor and in many cases fall within the range of statistical significance. This weak empirical backing makes it difficult to confidently claim that αVIL offers a substantial advantage over existing techniques.\n\nMoreover, several details in the algorithm (such as the subtraction of 1 in the α update step) were seen as ad hoc and insufficiently explained, leading reviewers to question both the novelty and the soundness of the method. Concerns were also raised over the experimental setup—particularly, the relatively simple multitask scenario in MultiMNIST and the lack of detailed ablation studies—limiting the evidence that the proposed weighting mechanism would generalize to more complex, realistic multitask settings.\n\nWhile the authors have provided thorough responses that clarify certain points and outline promising future work (such as additional experiments and theoretical analysis), the current submission does not sufficiently address these crucial concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed analysis:\n\n1. Contribution and Motivation:\n• The paper tackles a timely and important question in computational neuroscience – namely, can we build models that not only match adult brain responses (as measured by BrainScore) but also do so with substantially fewer supervised synaptic updates? The motivation to bring neural network developmental models closer to biological plausibility (e.g., fewer training updates) is innovative.\n• The authors propose three complementary strategies (reduced training epochs, a novel weight compression initialization, and selectively updating “critical” layers) that together lead to an 80% match (relative to full training) while using two orders of magnitude fewer supervised updates. This point is appealing both from a computational efficiency standpoint and for shedding light on potential biological mechanisms (or at least hypotheses).\n\n2. Methodology and Analysis:\n• The experimental design is thorough. The paper includes results from various ablation studies and comparisons (e.g., comparing weight compression against Kaiming Normal). The empirical results are supported by additional statistical tests (e.g., permutation tests for significance) that address some earlier concerns.\n• The paper’s transfer learning experiments (from CORnet-S to ResNet and MobileNet) strengthen the argument of generalization across architectures.\n• Some criticisms (raised by reviewers) centered on the interpretation of “80% match” (i.e., 80% of the BrainScore rather than absolute brain similarity) and the limitations of BrainScore as a proxy. The authors acknowledge these limitations and plan to improve the wording—this is acceptable given the current state of the field.\n\n3. Biological Plausibility and Clarity:\n• Several reviewers raised concerns regarding the mapping of these approaches to biological development. The authors are clear that these are not claims that the brain actually uses “weight compression” or undergoes supervised learning in the same way but are rather hypotheses to stimulate further research. They have also clarified that current models aren’t complete faithful brain models and that their work is a starting point.\n• The authors’ overall argument, especially in their revised response, provides a more cautious interpretation. They place their contributions in the context of a “proof-of-principle,” acknowledging that further work is needed. Although there is room for future improvement, the current interpretation is sufficiently responsible.\n\n4. Reviewer Opinions and Overall Impact:\n• The reviewers are split: R1, R2, and R3 have serious interpretative and methodological questions, while R4 and R5 are largely positive—emphasizing the novelty of reducing training updates and the broader implications for neural modeling. \n• In the author response the authors provide detailed clarifications and statistical evidence that address most of the concerns raised (for example, showing a significant improvement of 54% relative to 43% for weight compression).\n• Importantly, the paper opens up the discussion of linking the genome’s constraints (information bottleneck) with initializing network architectures, as well as the idea that some layers (e.g., down-sampling layers) might be “critical.” These insights are valuable for both the machine learning and computational neuroscience communities.\n\n5. Final Recommendation:\nWhile the paper is not without its limitations – particularly regarding the mapping of BrainScore to “real” brain similarity and the overall biological interpretation – the authors have provided a thoughtful discussion and clarified their intentions. With careful revision to clarify that the “80% match” refers to the score (and not absolute representational identity) and minor experimental details addressed, the work represents a promising direction.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Conceptual Contribution:\n• The paper proposes modulating both bottom‐up and top–down visual processing with language rather than the prevailing top–down attention only. Although this idea is interesting and inspired by some cognitive science insights, several reviewers (R1, R4) found that the conceptual motivation and analysis are not sufficiently developed. In particular, Reviewer 4 noted that there is little detailed discussion regarding why language should affect low-level visual representations and no in-depth error analysis to explain the mechanism behind the performance gains.\n• The authors’ contribution, as defended in the rebuttal, rests mainly on an ablation study that shows numerical improvements when jointly modulating both processing streams. However, it appears that similar ideas have been considered in related work, and the model innovation (e.g., splitting the language features) does not strongly differentiate itself from previous approaches as evidenced by the concerns regarding overlap with Step-ConvRNN.\n\n2. Empirical Evaluation and Statistical Rigor:\n• Although the reported results show some improvement on several metrics, Reviewer 2 pointed out that on some test sets the full model does not consistently match or surpass competing methods. The discrepancy between validation and test set performance raises questions about potential over-tuning.\n• The absence of statistical significance tests is a drawback. Both Reviewer 2 and Reviewer 4 emphasized that even moderate improvements need to be backed by rigorous evaluation—especially when improvements on some critical metrics remain ambiguous.\n• There are also issues with consistency of reported numbers (e.g., differences between tables) that have not been satisfactorily clarified in the rebuttal.\n\n3. Analysis and Interpretation:\n• The paper would benefit from a clearer conceptual explanation and an interpretable analysis of how the language input is affecting different layers of the visual processing hierarchy. Although some ideas were floated (such as clustering language filters or word-masking experiments), these analyses were not fully performed.\n• This lack of insight into the interaction between language and the visual features means the work, despite providing some empirical gains, does not offer the strong conceptual insight expected at a top-tier conference.\n\n4. Conclusion:\n• While Reviewer 3 was positive and found the qualitative results convincing, two of the reviewers (Reviewers 2 and 4) have substantial concerns about the conceptual motivation, consistency of empirical results, and absence of statistical rigor. The necessity of a strong contribution in broad conceptual terms, rather than solely numerical improvements, is not adequately met.\n• Given the mixed and in some cases critical reviews—and considering the overall need for more comprehensive analysis and stronger conceptual contributions—I lean toward rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "After carefully reviewing the paper, the following are the key issues:\n\n1. The paper’s presentation and clarity are problematic. Many key definitions and derivations are relegated to footnotes or merely referenced via an unpublished work without providing sufficient details. This makes it hard for the reader to verify the algorithm’s correctness or assess its impact. Several reviewers noted that the theory driving the approach, while potentially interesting, is not communicated in a way that stands on its own.\n\n2. The theoretical foundations of the work rest heavily on results from an unpublished paper and on “common” intuitions (e.g., about always active neurons, clustering via a custom vector representation) that are not clearly elaborated nor compared to existing literature. In several instances, claimed novel contributions are either trivial or not sufficiently differentiated from prior approaches (for instance, similarities to highway or ResNet architectures are mentioned).\n\n3. The experimental evaluation is very limited. The paper only contains experiments on toy datasets and MNIST, without comparison to state-of-the-art neuron pruning methods. The benchmarking is minimal and does not provide sufficient evidence to convince the reader that the proposed approach outperforms, or even stands on equal footing with, known techniques.\n\n4. Reviewer feedback uniformly points out that the algorithm would benefit from addressing issues such as better comparisons, a more detailed description of the theoretical derivations (e.g., precise statements rather than vague “remarks”), and extended experiments (including more challenging datasets like CIFAR). While the author response acknowledges many of these points and promises to improve in these areas, the current submission falls short.\n\nGiven these significant concerns – in clarity, theoretical underpinning, and experimental validation – the paper does not yet meet the standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Technical Contribution:\n• The paper proposes a variant of MCTS—termed Shoot Tree Search (STS)—that performs multi‐step expansion. This is motivated by controlling the bias–variance trade‐off in tree search, similar in spirit to n‐step TD methods. While multi‐step expansion ideas have appeared in earlier work, the authors clearly situate their contribution in the current “post–AlphaZero” context, where neural network estimates are used. Thus, although the core idea is simple, its integration with deep RL and the accompanying analysis adds a new perspective.\n• Several reviewers expressed concerns about novelty in comparison to prior works. However, the authors’ further discussion and additional experiments (e.g., contrasting different leaf evaluation strategies and comparing to AlphaGo-style rollout evaluation) helped clarify that there is a nontrivial algorithmic impact if one builds the tree in this new way (e.g., improved bias and smoothing effects during backpropagation). In this light, the method does provide an incremental but meaningful advance.\n  \n2. Empirical Evaluation:\n• The paper contains thorough empirical evaluation across challenging domains like Sokoban and Google Research Football. Results indicate that, particularly under modest computational budgets, STS outperforms standard MCTS and various shooting methods.\n• Despite some reviewers questioning whether the gains would persist with large numbers of simulations, the authors’ additional experiments indicate that while the advantage might diminish with very high simulation budgets, there remains a significant performance boost in many practical, constrained scenarios.\n• The authors also provided clear responses regarding experimental design (how hyperparameters are chosen, memory usage, and details on value network training), which addresses several concerns raised by the reviewers.\n\n3. Clarity and Presentation:\n• A number of reviewers pointed out issues with exposition (unclear definitions in algorithms, presentation of ablations, related work insufficiently discussed). The author response indicates that these issues will be improved in the revision, with additional clarifications (e.g., definitions in algorithms, explanation of trade-offs, and restructuring parts of the paper).\n• Even though there are presentation issues, they appear addressable without undermining the core technical and experimental contributions.\n\n4. Overall Impact and Relevance:\n• The work addresses a critical and practical problem in planning under limited computational budgets and shows how algorithmic modifications can lead to substantial benefits in performance.\n• The insights regarding the structure of the search tree, memory usage, and the bias-variance trade-off make this contribution of interest to the community.\n• While some reviewers remain cautious regarding the extent of the novelty, the combination of a simple idea with careful empirical evaluation and discussion in the context of deep RL planning is of sufficient interest for a top-tier conference.\n\nAfter weighing the concerns (mostly about clarity and relation to prior work) against the clear empirical improvements, insightful analysis, and the authors’ promising revisions and extra experiments, I conclude that the paper meets the standards for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contributions and Motivation:\n• The paper tackles an important issue – speeding up diffusion generative models while retaining high sample quality. The proposed approach (DDSS) optimizes a flexible non-Markovian sampler family (renamed GGDM) by differentiating through sample quality (now more clearly linked to Kernel Inception Distance). This tackles a practical bottleneck in diffusion models.\n• Although early concerns were raised regarding the motivation and theoretical grounding (e.g., why perceptual loss over ELBO and potential issues on model interpretability), the authors have clarified these points. They now clearly relate the loss to an unbiased MMD estimator (i.e. KID) and have improved the exposition regarding the reverse process and the ELBO. \n\n2. Technical Soundness and Novelty:\n• The method uses a novel combination of generalizing the forward process and optimizing sampler parameters using gradient rematerialization, leading to better sample quality with fewer steps. Although some aspects (e.g., the connection to multistep numerical integration) relate to prior work, the paper adds value by comparing with DDIM and providing empirical improvements.\n• On the theoretical side, what was previously provided as “proof sketches” have now been expanded and moved to the appendix, addressing earlier criticisms.\n    \n3. Empirical Evaluation:\n• The experimental results on CIFAR10 and ImageNet (with additional results promised on challenging high-resolution datasets for the camera-ready version) show clear improvements over baselines in terms of FID scores with far fewer sampling steps.\n• Reviewers generally found the empirical evidence compelling, even noting that in regimes with very limited steps the improvements are striking. While there were early concerns about possible metric overfitting, the limited degrees of freedom and additional ablations alleviate these worries.\n\n4. Reviewer Opinions and Author Response:\n• The majority of reviewers (3, 5, and even 1 and 4 after revisions) have raised their scores in the rebuttal phase, indicating that the revisions and additional clarifications have addressed their major concerns.\n• Although some reviewers still noted clarity issues and raised questions about scalability to higher resolutions, these concerns seem to be on the margin relative to the method’s strengths.\n• Overall, the revised paper now provides better motivation, clearer exposition, and additional theoretical details supporting the empirical findings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Significance:\n– The paper presents a new formulation for Mahalanobis metric learning as an optimization problem with an objective of directly minimizing the number of violated similarity/dissimilarity constraints. This is in contrast to nearly all prior methods that employ convex relaxations or error functions that penalize violations.\n– The key theoretical contribution is the design of an FPTAS (fully polynomial time approximation scheme) with nearly linear running time for fixed constant dimensions. This is established by applying LP-type problem techniques, which is a novel approach in the metric learning context.\n– Although reviewer 3 voiced concerns regarding the perceived simplicity of reducing the problem to a linear program, the authors have clarified that their contribution lies in furnishing the first provable guarantee on the number of violated constraints for arbitrary, adversarial inputs. This theoretical guarantee has not been provided by previous methods such as ITML or LMNN.\n\n2. Relation to State-of-the-Art and Comparisons:\n– The paper benchmarks its approach against classical methods (ITML and LMNN). While some reviewers requested more comparisons with other recent works, the authors provided clarifications by distinguishing their setting (adversarial inputs, guarantee on the number of violated constraints) from the statistical settings addressed in related literature.\n– The authors also made clear that their method is the first to have guarantees even under adversarial noise — a strong argument in favor of its novelty and practical robustness.\n\n3. Technical Soundness and Clarity:\n– Although there have been concerns induced by some confusing phrasing—especially regarding Lemma 2.1 and certain algorithmic details—the authors acknowledged these issues and indicated that they will be clarified in the final version. This suggests that the technical foundation is sound, pending revisions.\n– Reviewer 2’s technical observations (such as proper definitions of “accuracy”, “combinatorial dimension”, etc.) have also been addressed by the authors in the response.\n– There is also an appreciation that the method is primarily of theoretical interest in light of the NP-hardness suspect for higher dimensions and large running time even for small dimensions, yet the focus is to provide a rigorous theoretical guarantee, which is a valuable perspective.\n\n4. Experimental Evaluation:\n– The experiments demonstrate that while the algorithm might not always improve accuracy in every scenario, it clearly outperforms state-of-the-art methods in adversarial settings. This is an important insight that justifies the proposed approach.\n– Concerns such as the running time not being strictly monotonic as a function of the dimension were addressed by the authors with a plausible explanation (PCA effects on the combinatorial structure).\n\n5. Overall Impact:\n– The work bridges a gap between computational geometry (specifically LP-type approximations) and machine learning metric learning, which can encourage further research in this intersection.\n– Despite some minor issues with presentation that will be fixed in the revision, the theoretical contribution is significant enough for a top-tier conference.\n\nBased on these considerations, the strength of the theoretical contributions, the novel perspective on metric learning under adversarial conditions, and the authors’ commitment to rectifying the presentation issues, I conclude that the paper is a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a novel progressive learning strategy for variational autoencoder models aimed at progressively learning disentangled hierarchical representations. The overall idea, motivated by “starting small,” is to progressively grow the network’s capacity—first capturing abstract factors and then gradually adding finer details. This is achieved by introducing a ladder architecture with incremental “fade-in” of new latent variables and lateral connections. The authors further provide a new metric (MIG-sup) and support their claims with extensive quantitative and qualitative experiments on benchmark datasets (e.g., MNIST and 3DShapes) as well as ablation studies, including analyses on training stability and information flow.\n\nThroughout the review process, concerns were raised regarding:\n• The clarity between hierarchical versus disentangled representations and the connection between the two.\n• The need for additional comparisons with baseline models such as VLAE, especially concerning per-layer mutual information.\n• Some details in methodology (e.g., missing definitions and potential confusion regarding the metric) and a request for further ablation studies.\n\nIn response, the authors provided detailed clarifications, additional experiments (including quantitative comparisons of mutual information across layers, extended ablation studies on the “fade-in” and pre-KL penalty strategies, and more thorough comparisons with VLAE). These actions have addressed the initial concerns regarding evaluation metrics, stability, and clarity in motivation. The rebuttal evidence demonstrates that the proposed method not only improves training stability but also results in a better disentanglement behavior and hierarchical information organization compared to existing methods.\n\nAfter careful consideration of the reviewers’ initial reservations, subsequent rebuttal responses, and the additional experimental validations provided by the authors, it is clear that the contribution is novel, the results are promising, the methodology is well motivated, and the additional details have sufficiently addressed outstanding criticisms.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n• The paper tackles an important and timely problem in distributed mean estimation. It proposes a new quantization technique based on lattice theory that promises error bounds in terms of the input variance rather than the norm, addressing a known limitation of previous methods. This is a meaningful advancement for distributed variance reduction in machine learning.\n\n• The reviewers overall appreciated the approach. Reviewer 1 and Reviewer 4 highlighted the clarity in the problem definition and the natural extension of prior work. Reviewer 2 acknowledged that while many of the technical building blocks (lattice quantization, reduction to ℓ∞, etc.) are known, their combination here yields a clean end result with asymptotically tight upper and lower bounds.\n\n• Some criticisms were raised. For example, Reviewer 1 suggested that a comparison with a more straightforward iterative procedure be discussed, while Reviewer 2 requested a direct description of a simplified version without a full lattice treatment. Reviewer 3 raised some implementation details (e.g. practical issues with leader election, experimental settings limited to very small numbers of machines, and minor errors in definitions), though these were not viewed as fatal flaws. Reviewer 4 acknowledged the gap between the “theoretical” algorithms and their “practical” implementations but saw the trade-off as acceptable given the focus of the work.\n\n• The authors’ responses are comprehensive and appropriately address the raised concerns. They indicate plans to include additional discussion—such as clearer comparisons to iterative methods and a direct description of the ℓ∞ approach—and to fix minor issues in the text and proofs. They also clarify the reason for the differences between the theoretical and practical versions, noting that the optimal theoretical versions are mainly intended to demonstrate the asymptotic tightness of the bounds.\n\n• Despite some experiments being performed on small settings, the emphasis of the paper is on theoretical results, including matching lower and upper bounds. The practical experiments, while limited, support the improvements claimed. Future revisions promise to add further clarification and expanded empirical evaluations where possible.\n\nOverall, the contributions are substantive, the approach is novel in its combination of existing tools to address an important issue, and the authors have convincingly addressed reviewers’ concerns. In light of these points, I conclude that the paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Contribution:\n • The paper’s stated goal is to introduce a generalized probability kernel (GPK) for discrete distributions and use it to propose new statistics (power‐MMD) for two‐sample testing.\n • Several reviewers noted that many of the theoretical results are either elementary or follow directly from known results in the literature. The proposed generalization appears to amount to a re‐interpretation and minor manipulation of the maximum mean discrepancy (MMD) framework rather than a fundamentally new idea.\n\n2. Clarity and Rigor:\n • A repeated concern from all reviewers is that the definitions, notation, and proofs are confusing and incomplete. For example, Definition 2 and subsequent derivations mix up different objects (kernels, gram matrices, mapping functions) without a clear and unified presentation.\n • Key components of the theoretical development are missing or only stated with “trivial” claims. Several proofs (e.g., of Theorem 5) were explicitly noted as missing.\n • Numerous typos and grammatical issues derail the clarity of the exposition. Even with the author’s revision, many issues remain in the definitions and the overall formal presentation.\n\n3. Experimental Evaluation:\n • Not one reviewer could identify any experimental or simulation results that support the proposed estimator’s usefulness in the context of two‐sample testing.\n • In a top‐tier conference paper that proposes new estimators for hypothesis testing, empirical validation is essential. Missing experiments make it difficult to assess the practical impact or the actual improvements (if any) over existing methods.\n\n4. Relation with Prior Work:\n • The paper claims a unification of MMD and KSD via a common framework. However, the decision to remove the KSD part (due to a faulty proof) leaves only the polynomial GPK side, which appears to be a straightforward extension of MMD.\n • The authors’ motivation for the setting (discrete distributions with values in ℝ^d) is not sufficiently compelling. Although the author response mentions possible connections to NLP (e.g., word2vec similarities), this connection is not elaborated enough in the paper to justify the novelty or the intended application.\n\n5. Overall Evaluation:\n • The revised submission addresses some notation and presentation issues, but the fundamental concerns remain. The theoretical contributions seem incremental and the paper lacks the necessary experimental or simulation validation for two-sample testing.\n • Given the state of the paper—with incomplete proofs, unclear definitions, and no empirical results—it does not meet the standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Problem Formulation:  \nThe paper tackles a novel formulation of singing voice generation that does not rely on pre-assigned scores or lyrics. This “score and lyrics-free” paradigm, with its three different generation modes (unconditional/free singer, accompaniment-conditioned, and a “solo singer” that first generates an accompaniment), represents an interesting departure from most prior work that condition on strong symbolic inputs. This is a challenging problem with potential applications in creative composition and generative music, making the overall motivation and novelty interesting for a top-tier conference.\n\n2. Technical Approach:  \nThe authors propose a pipeline that includes source separation, transcription, and a GAN-based synthesis network (with modifications inspired by BEGAN). Although there were concerns about the justification for some architectural choices and losses, the authors have provided experimental comparisons (including ablation studies against GAN, LSGAN, and BEGAN objectives) and clarified their choices in the revision. Their adaptations to process time‐frequency representations and deal with variable length sequences demonstrate solid technical work.\n\n3. Evaluation:  \nInitially, many reviewers were concerned about the evaluation including the choice of objective metrics such as “vocalness”, “average pitch”, and “matchness”. The authors have responded by:\n • Explaining the rationale for these metrics and then further refining them (e.g., revising the vocalness measure to take into account both vocal activation and pitch range).  \n • Expanding evaluations to include two well-known singing synthesis baselines (Sinsy and Synthesizer V) with a dedicated MOS (Mean Opinion Score) study.  \n • Providing more comprehensive quantitative metrics (including comparing metrics computed on the training data) to give proper context.  \nThese additional experiments and analyses help ground the evaluation despite the inherent difficulty of assessing generated music/singing.\n\n4. Clarity and Data Issues:  \nSeveral reviewers noted that initial descriptions of the data and experimental setup were confusing. The authors have now provided clearer descriptions of the training, validation, and testing splits (including the Jazz domain details), and a table summarizing dataset usage. Even though some details (like track overlaps in training/validation splits) remain debatable, the clarifications are sufficient for the top‐tier conference standard.\n\n5. Overall Contribution and Revisions:  \nWhile the initial submission received mixed reviews (ranging from weak rejection to conditional acceptance), the revised version now directly addresses the main concerns. In particular, the inclusion of two baseline synthesis methods, revised objective metrics, expanded discussion of motivation, and clearer experimental details strengthen the paper’s contribution. Given the inherent difficulty of evaluating generative audio approaches, the expanded user studies and quantitative comparisons enhance the credibility of the results.\n\nIn summary, the paper makes a noteworthy contribution by tackling a challenging, underexplored task and providing both technical innovations and improved evaluations through new baselines and experiments. Although the evaluation of generative art remains subjective, the efforts to provide quantitative and qualitative comparisons make a strong argument for its publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Detailed Reasoning:\n\nThe paper proposes a dynamic, multi-precision training strategy (MuPPET) that progressively increases the numerical precision during CNN training based on a metric inspired by gradient diversity. The contribution lies in extending reduced-precision training to include fixed-point regimes and, importantly, in devising a run‐time switching policy that adapts to the network’s learning behavior. This is especially relevant given current hardware trends, with support for low-precision fixed-point arithmetic.\n\nSeveral strengths were noted in the reviewers’ comments. In particular, the strategy is orthogonal to existing methods and can be incorporated into systems that already use reduced-precision training. The idea of basing precision switching on gradient statistics across epochs is interesting, and the work clearly shows how a modest speedup (average 1.28× across several architectures) can be achieved without compromising final accuracy. The experimental study across multiple architectures and the demonstration that the approach generalizes across datasets are additional positive points.\n\nOn the other hand, critical issues were raised regarding clarity and justification. Key concerns related to the details of the switching mechanism – specifically its reliance on gradient diversity – and the explanation for the choice of various hyperparameters and precision levels. Some experimental results (e.g., on AlexNet) were seen as ad hoc, and the presentation of the algorithm (including notation in Section 3.3) was seen as unclear in spots. One reviewer also noted that the experimental gains, while existent, were somewhat modest relative to prior works.\n\nIn response, the authors have revised the manuscript extensively. They clarified:\n•\tThe motivation behind the switching mechanism, providing additional text and revised figures that pinpoint when the thresholds are exceeded.\n•\tDetails of the quantization strategy including newly added equations and explicit discussion on scaling factors and parameter choices.\n•\tExperimental details such as training and validation curves, now included in the Appendix, were added to better substantiate the claims.\n\nEven though the idea is somewhat incremental, the revisions have addressed much of the ambiguity in the description and experimental validation. The method provides a novel perspective on the use of fixed-point training across multiple precision levels, which is of interest in the context of reducing training time on modern hardware. \n\nGiven that the contributions have been clarified, experimental details improved, and the algorithm’s design justified both empirically and conceptually, the paper meets the standards for a top-tier conference. The novelty, even if modest, along with the thorough revisions addressing reviewer concerns, make the work worthy of publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning followed by my final decision.\n\nOverall, the paper tackles an important limitation in diffusion models—that is, the prohibitive number of inference steps needed before high‐quality samples can be generated. By proposing a generalized parametric family (GGDM) and a method (DDSS) that differentiates through sample quality metrics (now better grounded via a KID loss), the paper presents both technical and empirical contributions that advance the state‐of‐the-art in efficiently sampling from pretrained diffusion models. Several reviewers noted that the empirical improvements are significant, especially in the few-step regime, and that the idea of differentiating through perceptual loss is compelling. Although some concerns were raised regarding clarity, motivation, and the theoretical underpinnings of optimizing a perceptual objective rather than an ELBO, the authors have addressed many of these points in the revised version.\n\nIn particular, \n• The revised paper now explicitly shows the connection between the perceptual loss and KID, making it clear that the optimization is indeed matching the model’s distribution with the data distribution. This substantially improves the theoretical motivation for the method. \n• The authors provide additional empirical evidence as well as a more formal presentation of the reverse process and the ELBO. \n• Clarity has been improved across the description of GGDM and DDSS, which helps in understanding the technical details. \n• Multiple reviewers found the empirical gains (fewer steps leading to much lower FID scores on challenging datasets) persuasive—even if the contributions are somewhat incremental relative to prior work, these improvements are both significant and practically useful.\n\nWhile some reviewers still mentioned potential concerns (e.g., scalability to very high-resolution datasets, the novelty in light of prior work on similar non-Markovian treatments for fast sampling, and connections to multistep integration methods), these points do not overshadow the overall contribution. The method has low additional degrees of freedom (limiting overfitting risk) and adds valuable techniques for optimization under memory constraints.\n\nConsidering the paper’s solid empirical improvements, the clarified motivation and theoretical justifications after revisions, and the potential impact in improving inference speed of diffusion models—contributions that are important for the community—I lean toward acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and Relevance:  \nThe paper tackles calibration under distribution shift for deep image classifiers—a problem of substantial interest given that overconfident predictions on OOD data remain a major issue. The idea of using a surrogate “outlier exposure” (using contrast corruption) to build calibration sets and then calibrate the predictions at test time is both simple and surprising. Although the overall approach is conceptually straightforward, it addresses a real need for post-hoc calibration methods that work even when the test corruptions differ from those seen during training. This is especially relevant for safety-critical applications.\n\n2. Methodology and Experimental Validation:  \nThe method is presented in two variants (single-image and multiple-image methods). While some reviewers originally found the presentation unclear, the authors have improved the exposition (e.g., clarifying notation and providing additional details in the Appendix). New experiments have been added that not only evaluate the method on corrupted versions of CIFAR-10 and ImageNet but also extend the evaluation to entirely OOD datasets (e.g., SVHN in the context of CIFAR-10, MNIST models evaluated on Fashion-MNIST/Not-MNIST). These experiments strongly support the claim that the proposed calibration method can transfer across different types of corruption.\n\n3. Concerns and Author’s Response:  \nA central critique from one reviewer was that the method assumes that the OOD data belongs to one of a small set of possibilities (in effect, “known” shifts), making the problem potentially less interesting. However, the authors convincingly clarify that their setting is “calibration under distribution shift” (as distinct from unsupervised domain adaptation) and emphasize that the surrogate calibration set (built from a controlled corruption – contrast) is used exactly because it works effectively even when the actual test corruptions are different. They also discuss why including clean images in the calibration data is vital to remain well calibrated for in-distribution data. Other gaps in motivation (e.g. why use contrast) and baseline comparisons have been addressed by additional experiments and explanation.\n\n4. Clarity and Impact:  \nAlthough the method is simple, it is also fast and does not require extra training. The simplicity can be seen as a virtue when it achieves improvements in calibration across several benchmarks. The additional figures (e.g., Figures 3, 7, 8, and 11) and detailed discussion in the revised version have clarified the approach. The paper also situates itself relative to prior work by providing improved ECE and other metrics, making its potential practical impact clear.\n\n5. Final Assessment:  \nDespite some initial reservations regarding the underlying assumptions, the revised submission (with additional experimental validation, clarifications, and discussion in both the main text and Appendix) makes a compelling case that the method is novel, practical, and effective for calibration under distribution shift. The simplicity does not detract from its utility—in fact, its low computational cost and post-hoc nature are strengths.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis of the paper and final decision:\n\n1. Novelty and Motivation:\n• The paper addresses the well‐known limitation of “local homophily” assumed by standard GNNs and proposes a model that uses adaptive, learnable spectral filters with a global attention mechanism. This is motivated by the need to work well on heterophilic (disassortative) graphs.\n• However, several reviewers noted that using non-local attention (or global attention) is by now a familiar idea in graph learning (e.g. see prior work on non-local GNNs). Although the authors argue that their formulation “learns” spectral filters in an adaptive fashion (via multi-head attention) which is different from fixed or pre-defined wavelet filters, this contribution appears to be incremental relative to existing approaches.\n• Thus, the novelty is not viewed as a radical departure but rather an interesting incremental extension. Given the high bar for top-tier conferences, this incremental contribution is not entirely convincing.\n\n2. Complexity and Scalability:\n• One major concern raised by the reviewers was the computational cost of evaluating the global attention. The authors’ defense is that they use Chebyshev polynomial approximations – a well-known trick – to reduce the complexity to O(m|E|) rather than having to compute a full dense attention matrix.\n• While the authors’ response is reasonable and they showed runtime comparisons, scalability to very large graphs remains a concern. Most of the empirical evaluation is on relatively small datasets. For a top-tier conference, convincing demonstration on larger graphs or further empirical runtime comparisons (and perhaps synthetic controlled experiments) would strengthen the work.\n\n3. Experimental Evaluation:\n• The experiments include comparisons on both assortative and heterophilic graphs and show that the proposed method outperforms state-of-the-art baselines on disassortative networks while performing comparably on assortative ones.\n• However, additional evaluation tasks (beyond node classification) and tests on larger datasets would improve the paper. Several reviewers flagged the limited scope of the experiments.\n• The authors acknowledge this and promise further experiments on larger graphs (e.g. Actor, Squirrel) and synthetic data, but these are not yet available.\n\n4. Overall Assessment:\n• The paper is clearly written and the idea is presented with a decent theoretical grounding. Its adaptive spectral filtering mechanism is intuitively appealing.\n• Nonetheless, the method is not sufficiently novel in its overall spirit and does not fully address concerns regarding computation and scalability. Furthermore, with only node classification experiments on relatively small graphs, the empirical validation does not meet the bar expected from top-tier conference papers.\n• Two of the reviewers remain unconvinced on both novelty and the claimed computational advantages even after the rebuttal.\n\nGiven these points, while the paper has interesting elements and the authors have provided solid clarifications and runtime comparisons, the overall contribution appears incremental and the experimental support remains limited relative to top-tier standards.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces a novel benchmark based on meta-referential games aimed at exploring compositional learning behaviours, along with an SCS stimulus representation intended to overcome limitations of traditional one‐hot encoding. However, the reviewers consistently noted several major issues that substantially weaken the paper’s contributions. In particular, reviewers express concerns about clarity and presentation, including heavy writing that is hard to follow, missing detailed descriptions of the representation and experiment setup, and an insufficient discussion of why SCS is advantageous over existing continuous or one‐hot encoding representations. The experimental evaluation is regarded as inadequate by almost every reviewer. They specifically note that crucial ablations and comparisons (e.g., versus purely continuous representations and one-hot baselines) are missing and that the experimental performance is barely above chance. Additionally, although the ideas are interesting and the benchmark might have potential, the novelty and empirical impact are rated as only marginally significant. Reviewer feedback calls for a substantial revision regarding clarity, more extensive experiments, and stronger positioning within the literature.\n\nGiven these aggregated concerns – from technical novelty, insufficient empirical work, to clarity and discussion of limitations – the paper in its current form does not meet the standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper tackles a very important and timely question—disentangling the roles of architecture and optimization (via gradient descent) in neural network generalization. In doing so, the work derives new PAC‐Bayes bounds and develops an estimator for the NNGP classification error. The theoretical aspects are presented in a clear and careful way, and several reviewers appreciated the elegance of the technical contributions.\n\n2. However, a consistent criticism across reviewers is that the experimental evidence is narrowly focused. Many of the experiments consider only MLPs and a newly introduced optimizer (“Nero”), without a thorough ablation over the broader spectrum of optimization methods (such as vanilla GD, SGD, comparison with NTK behavior, or other settings). This limited scope makes it hard to separate the gains from the novel analysis from known results in the literature—for example, the standard narratives around large margin behavior in exponential loss settings.\n\n3. Moreover, there is considerable confusion over key terminology and claims. Multiple reviewers noted that the discussion on “implicit bias” of gradient descent is either misleading or under-supported. In particular, the claim that GD (or the specific optimizer used) inherently “targets” large margin solutions comes off as perplexing and seems to merely make margin an explicit hyperparameter rather than revealing a hidden property of optimization.\n\n4. Several reviewers also expressed concerns that the improvements exhibited by relying on margin control, while interesting, seem to be a rephrasing of standard regularization (or data scaling) effects. In addition, the theoretical bound, although non-vacuous, is not shown to be close enough to the actual observed performance to be practically meaningful. There is also indication that many of the bounds and derivations overlap with or extend prior work (e.g., Valle-Pérez et al.), which lessens the overall incremental novelty.\n\n5. Lastly, the experimental validation does not generalize beyond basic MLP settings. The abstraction to more realistic architectures (such as CNNs/ResNets), or even a more extensive ablation study of the effects of different optimization and regularization techniques, would be needed to convincingly support the central claims regarding the interplay between architecture bias and the optimization-induced bias.\n\nBased on these reasons, while the technical work has some merits and is articulated clearly, the combination of incremental contributions, insufficient empirical validation, and unresolved conceptual clarity regarding the role of GD ultimately leads to the conclusion that the paper is not yet ready for acceptance at a top‐tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper provides a theoretically elegant and technically rigorous treatment of the landscape of two-layer ReLU networks by linking the global optima of the nonconvex neural network training problem with those of a particular convex optimization formulation. In particular, it extends prior work by Pilanci and Ergen (2020) by (i) characterizing the entire set of globally optimal solutions via a convex program with cone constraints, (ii) introducing the notions of minimal/nearly minimal networks to structure these optima, (iii) establishing that certain Clarke stationary points from SGD correspond to global optima of subsampled versions of the convex problem, and (iv) providing an algorithm that runs in polynomial time to test whether a given solution is globally optimal.\n\nThe reviewers generally agree on the novelty and significance of these contributions. While there are some concerns about minor issues (typos, slight inconsistencies in definitions, and questions about extension to different activations or deeper networks), the reviewers consider these to be fixable either via improved exposition or relegated to future work. One reviewer gives a strong accept recommendation, and the overall consensus is that the paper makes an important theoretical contribution—even if it is restricted to the two-layer case—which is common for theoretical deep learning work at top-tier venues.\n\nThe rebuttal and author responses are clear and address the raised points, indicating plans to improve the exposition and include references.\n\nThus, considering the overall clarity, technical soundness, and theoretical significance, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning for this paper’s evaluation:\n\n1. The paper studies adversarial examples from a frequency perspective and challenges the common “high frequency” narrative by showing that the frequency characteristics of the adversarial perturbations are dataset‐dependent. It then uses these insights to analyze and even “control” the accuracy–robustness trade‐off when training robust models. This perspective is nonstandard and offers the community additional ways to think about adversarial robustness.\n\n2. Several reviewers found the experimental design interesting and appreciated that the authors conducted extensive experiments (including supplementary results on ImageNet, new architectures, and additional class‐wise and attack settings). The empirical evaluations in both PGD and Auto-Attack settings add weight to the claims.\n\n3. However, there are real concerns regarding novelty and clarity of contributions. Reviewers (e.g., Reviewer 2, Reviewer 3, and Reviewer 5) questioned how the proposed contributions stand apart from existing work. They remarked that similar observations have been made previously, that the “dataset dependent” nature of adversarial perturbations is already known, and that the “new methods” do not necessarily yield improved performance by standard metrics. Furthermore, clarity issues (with regards to what exactly is novel versus what is an extension of existing approaches like TRADES) were raised.\n\n4. The author response addresses many of these issues by clarifying that (a) their work focuses on the frequency properties of the adversarial noise itself (not the input images), (b) they show novel ways to control the accuracy–robustness trade‐off using frequency restrictions, and (c) they add experiments and discussion to differentiate from related work. In several instances, the rebuttal provides concrete details (for example, on the use of DCT instead of Fourier transforms, the adversarial training formulation in a restricted frequency subspace, and additional experiments on universal perturbations).\n\n5. While some reviewers remained skeptical about the “actual impact” and “uniqueness” of the contribution – believing that the insights are either incremental or already partially known – at least two reviewers (Reviewers 4 and 6) explicitly supported acceptance because the new perspective and the comprehensive experiments offer valuable insights for the community.\n\n6. Given the extensive experimental analysis and the authors’ efforts to clarify their methodological contributions (including a detailed explanation of the frequency-based adversarial training and its implications), the paper provides fresh empirical evidence and a nuanced understanding of the role that frequency plays in adversarial robustness. Although the work does not establish new state-of-the-art robustness benchmarks, many top-tier conferences have space for work that advances understanding through careful analysis and new perspectives.\n\nTaking into account the strengths in experimental design, the depth of analysis, and the fact that a significant number of reviewers lean toward acceptance (with the author’s responses addressing many concerns), my recommendation is to accept this paper. It may not be without its limitations, but the analysis is likely to stimulate further discussion and research in adversarial robustness from a frequency perspective.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my reasoning for the decision:\n\n1. Scope and Importance: The paper tackles a pertinent problem in modern learning systems—analyzing the behavior of SGD with momentum under non‐i.i.d. (covariate shift) conditions. In light of many applications in continual and reinforcement learning, where data correlation is common, understanding such resonance phenomena is of clear interest.\n\n2. Technical Analysis and Novelty: The work connects ideas from parametric resonance, Floquet theory, and classical oscillator analysis to study the divergence of momentum-based methods. While some reviewers are critical of the “technical novelty” because the proof techniques largely rely on existing tools and operator splitting papers, the contribution lies in the novel application and connection of these techniques to understand optimization under covariate shift. The idea that SGDm behaves as a parametric oscillator—exhibiting resonance driven by covariate shifts—is insightful and extends our understanding of momentum methods.\n\n3. Presentation and Clarity: There were criticisms regarding presentation – in particular, the intuitive explanation of the frequency interactions and the reliance on expected gradients rather than a complete stochastic analysis. The authors responded constructively by promising clearer intuitions (e.g. the “ball and bowl” explanation) and additional experiments showing that the resonance phenomenon persists even in the stochastic regime. Overall, while the presentation could be improved, the paper’s core ideas are communicated and justified.\n\n4. Empirical Contribution: The experimental section, though focused on synthetic settings, covers a breadth of settings including extensions to neural networks and alternative optimizers (e.g., Adam). Although real-world datasets were not included, the experiments convincingly delineate the resonance phenomenon. The discussion on how such oscillatory behavior might be manifest in practice (using examples such as audio or sensor data) further motivates the work and opens further avenues of research.\n\n5. Reviews Summary and Author Response: The reviews span a range of opinions—from a more critical stance on the extent of technical novelty (which was acknowledged and softened by the authors) to several positive assessments emphasizing the importance of the phenomenon and clarity of exposition. The author responses have addressed many of the concerns regarding both novelty and experimental validation, reinforcing that the key insights do extend beyond well-known theoretical results, even if some aspects (such as full discrete-time convergence analysis) remain as interesting directions for follow‐up work.\n\nIn light of the above, although there are some weaknesses in terms of presentation and the “fully stochastic” analysis, the paper makes a valuable and novel conceptual connection between momentum (as used in machine learning) and resonance phenomena from nonlinear dynamics. This insight is likely to spur further research and discussion in the community. Therefore, I judge that the paper is sufficiently novel, thorough, and impactful for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper proposes a novel method (MILAN) to generate natural language descriptions for neurons in deep vision models by maximizing the mutual information between candidate linguistic descriptions and the image regions in which the neurons are active. It introduces a new annotation dataset (MILANNOTATIONS) and provides extensive experiments across multiple architectures, datasets, and tasks. The work is well-motivated, technically sound, and its experimental sections demonstrate that the generated descriptions not only agree well with human annotations but also offer actionable insights into model behavior. The experiments are thorough (e.g., analyzing the distribution of descriptive attributes, auditing neuron properties for spurious correlations, and even editing the network to improve robustness) which further reinforces the paper's contributions.\n\nReviewers appreciate the clarity of presentation, extensive empirical evaluation, and the potential applications of MILAN. Although some reviewers raised points about annotation consistency, potential baselines (such as off-the-shelf captioning models), and the scalability/generalizability of the approach, the authors have addressed these issues with detailed responses and have planned further experiments or clarifications in the updated manuscript. The method makes incremental yet meaningful progress in the interpretability of deep models by providing natural language explanations for the activation of neurons—a perspective that is both novel and useful.\n\nGiven the overall technical merit, the promising empirical results, and the clear, focused revisions and clarifications provided in the author response, I believe this paper merits inclusion in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents a new framework (PEARL) for differentially private data synthesis that uses a one-shot computation of privatized embeddings via characteristic functions followed by adversarial reconstruction learning. Its primary contributions include a novel training pipeline that does not accumulate privacy costs over multiple iterations—a notable deviation from the common DP-SGD approach—as well as theoretical performance guarantees and an analysis of computational complexity.\n\nThe reviewers generally appreciated the paper’s motivation, clarity of writing, and theoretical underpinnings. They recognized that while aspects of the methodology exist in prior work (e.g., DP-MERF, DP-GAN), the one-shot privacy approach and the use of adversarial re-weighting offer a meaningful extension. Reviewers raised concerns regarding the need for more rigorous benchmarking: notably, inclusion of additional datasets and baselines such as PATE-GAN or CTGAN-style methods could improve the empirical validation. Comments were also made about clarity in the privacy-preserving optimization details, sensitivity calculation, and the auxiliary information release; however, the authors provided detailed responses and additional experimental results (with an extra dataset and another baseline) to address these points. \n\nWhile there are still some limitations regarding the diversity of benchmarks and comparisons, the paper’s revised version demonstrates careful consideration of these concerns. Its technical contributions, combined with theoretical and empirical support, make it a competitive submission for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper presents AutoOED, an automated platform for optimal experimental design that integrates multi-objective Bayesian optimization techniques, including a new asynchronous strategy (Believer-Penalizer, BP) and a user-friendly GUI. Although its engineering contribution is solid and its modular design addresses a relevant need (especially by lowering the barrier for non-coders), several reviewers expressed concerns about the overall novelty and scientific contribution. Most reviewers agree that the technical novelty is limited—with only minor, intuitive extensions (e.g., BP as a combination of existing asynchronous methods) and no rigorous theoretical grounding—and that the experimental results are mixed rather than convincingly superior. While Reviewer 5 and aspects of the author response stress strong usability aspects and the importance of asynchronous MOBO, the majority of comments point to the fact that AutoOED is mainly a well-engineered system with modest new algorithmic ideas. This type of software platform might be valued by some communities, but for a top-tier conference there is an expectation for substantial methodological innovation or deeper theoretical contributions. The updated features (like extending support for arbitrary numbers of objectives) help address some concerns, but do not fully overcome the perception that the paper is more of a system/software paper, serving as an introduction and manual to an open-source platform rather than offering a scientifically significant advance in MOBO methodology. \n\nOverall, while the platform is useful and user-friendly, the relatively minor methodological novelty and the mixed experimental evidence make it fall short of the standard expected for a top-tier conference paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper attacks a significant problem (offline RL) and proposes a simple method—BPPO—that uses an on‐policy algorithm (PPO) with a twist by replacing the usual on‐policy advantage with one estimated from the behavior policy. This “advantage replacement” idea and the theoretical claims about monotonic policy improvement are interesting, especially given the known challenges of offline RL (data shift and overestimation).\n\n2. The reviewers appreciated several aspects of the paper. In particular, reviewers noted:\n • The theoretical derivations (although they overlap with some earlier work) are non‐trivial and the authors clearly stated assumptions.\n • The proposed method is simple to implement compared to more complicated methods, and the empirical performance is highly competitive (even showing significant improvement on some challenging tasks like Antmaze, Adroit, and Kitchen).\n\n3. There were concerns raised:\n • The strength (or even feasibility) of Assumption 1 and the potential looseness in the policy improvement bound caused by replacing Aπk with Aπβ. Some reviewers questioned the soundness of this approximation.\n • The experiments (in earlier submission) appeared to be missing some ablations for hyperparameters and a direct demonstration that overestimation is mitigated.\n • Some reviewers felt that there is overlap with related work (e.g., GePPO and other off-policy TRPO/PPO advances).\n\n4. The authors’ rebuttal addressed these issues. They replaced the overly strong assumption with a distance based on the offline dataset; they supplemented ablations with additional experiments (including on sparse reward environments like Antmaze) and compared with extra baselines. They also provided further explanation and discussion regarding the theoretical trade-offs (advantage replacement) and clarified the implementation details. Although some theoretical loose‐ends remain, the modifications and clarifications have alleviated several concerns.\n\n5. For top-tier conferences the quality of theory, overall empirical performance and clarity of presentation are key. While some reviewers admitted that aspects of the work are incremental or similar to existing lines of work, the overall contribution (a simple, effective algorithm backed by theoretical reasoning and thorough experimental evaluation) and its strong performance on challenging benchmarks tip the balance in its favor.\n\n6. Considering all points, although the novelty is not revolutionary and there remain some approximations in theory, the paper provides an interesting perspective and a practically useful algorithm that advances the state-of-the-art in challenging offline RL environments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes an online knowledge distillation framework that diversifies teacher models through a label prior shift and uses an aggregation strategy involving PC‐Softmax to improve both accuracy and calibration. On the positive side, the paper is clearly written and organized; it includes detailed experimental studies and theoretical proofs that support the proposed framework. The additional experiments supplied in the response address several reviewer concerns, and the empirical results show that the method consistently outperforms baseline methods (such as PCL, MVCL, L‐MCL, and CGL) on several datasets, particularly improving calibration error and maintaining competitive error rates.\n\nHowever, several concerns persist. Most reviewers comment that the contributions, while well‐executed, are only marginally novel. There is a sense that the method essentially combines known techniques (importance sampling, label prior shift, and post-compensation through PC‐Softmax) in an online KD setting. Furthermore, questions remain regarding whether these ideas can be generalized beyond the specialized peer-based online distillation framework (an issue partially addressed by the authors but still leaving the impression of narrow applicability). Some reviewer criticisms about baseline selections and missing ablation studies also suggest that the method’s improvements over existing approaches, although present, are incremental rather than transformative.\n\nConsidering the standards of a top-tier conference—where significant technical novelty and broad impact are required—the incremental nature of the contributions, combined with the limitations in baseline comparisons and the rather specialized scope of the method, leads to the conclusion that the paper does not rise to the level expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes a VAE-based generative model “TimeVAE” for synthetic multivariate time series generation with an additional focus on interpretability by incorporating domain-specific decoder blocks (e.g., for level, trend, seasonality). While the idea of using VAEs instead of or in conjunction with GANs for time series data is interesting, several concerns emerge:\n\n1. Novelty and Technical Significance:\n• Multiple reviewers noted that the core architecture appears to be a standard VAE for time series data with additional convolutional layers and specialized decoder blocks. There is little evidence that these components provide substantial improvements or clear novel insights compared to existing methods.\n• The claim that the decoder components yield interpretability or allow the injection of domain knowledge is not properly substantiated with experiments or ablations that verify these benefits.\n• The paper does not clearly differentiate from existing models such as conditional VAEs, state-space models, or even recent work like GP-VAE for time series imputation, which further undermines the technical novelty.\n\n2. Empirical Evaluation:\n• The empirical validation on four datasets raises concerns. Several reviewers highlighted that the experiments do not include well-known benchmarks, and the evaluation metrics (e.g., discriminator scores, next-step prediction) show counter-intuitive results in some settings.\n• The absence of ablation studies or guidance on selecting the new hyperparameters compromises the robustness of the experimental findings.\n• There is also a lack of thorough comparison with additional strong baselines (including autoregressive and probabilistic models) which would be expected in a top-tier conference submission.\n\n3. Presentation and Clarity:\n• Although the paper is well written overall, several key methodological descriptions (e.g., the detailed derivation and motivation for using weighted VAE losses, specifics about hyperparameter tuning for the new decoder blocks) are insufficiently addressed.\n• Some aspects of the explanation (such as the discussion around interpretability and the simulation of trend/seasonality) are not adequately validated in the results section.\n\n4. Overall Contribution:\n• While the idea of leveraging domain knowledge to shape the decoder is promising, the paper in its current form does not provide convincing evidence that this approach produces significantly better results than standard methodologies.\n• The absence of an author response further limits any opportunity to address the raised concerns.\n\nBased on these limitations—particularly the lack of strong novelty, insufficient empirical support and ablation analysis, and deficient baseline comparisons—I conclude that the paper is not yet suitable for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty & Significance: Although some reviewers noted that many aspects of the VNCA can be seen as reinterpreting standard deep convolutional (residual) networks with weight–sharing, the paper’s main contribution is to demonstrate that a true generative model using iterative local rules (i.e. an NCA-inspired decoder) can be built and evaluated as a VAE. In particular, the added experiments on damage recovery and latent space analysis have strengthened the original contribution. While the innovation is incremental, it is of interest given the growing attention on self-organizing systems and cellular automata in machine learning.\n\n2. Experimental Results: Many reviewers had concerns about the quantitative performance (with a significant gap to the state-of-the-art) and sample quality, especially on challenging datasets like CelebA. However, the work is careful to evaluate the model under the proper ELBO framework and stress that the aim is not merely to beat SOTA in likelihoods but rather to showcase the self-organizing properties inherent to NCAs. The additional experiments introduced during rebuttal (e.g., damage recovery and latent interpolations) bolster the claim that the NCA formulation has interesting properties that traditional decoders do not.\n\n3. Clarity and Reproducibility: The paper is very well-written and the authors have provided extensive details (including architecture choices, hyperparameter selection, and training strategies). They have also committed to releasing code and including comparisons to relevant baselines, which will help future work in the community.\n\n4. Reviewer Opinions: Among the seven reviewers, while a few (R1, R4, and R6) remained critical (mainly on experimental quality and novelty), an equal number (R2, R3, R5, and R7) increased their scores after the rebuttal and acknowledged the interesting aspects of the work, as well as the valuable insights into the link between NCAs and (iterative) deep nets.\n\n5. Overall Recommendation: Although the generative performance (log-likelihoods) is not state-of-the-art, the paper’s strength lies in its principled treatment of NCA as a generative process with inherent self-organizing properties. In a top-tier conference that values novel perspectives and thorough empirical analysis—and given the substantial author response—the paper represents a solid contribution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is the detailed reasoning and the final decision.\n\nDetailed Reasoning:\n• Novel idea but limited impact on experiments: The paper presents a geometric perspective to the graph generation problem through a novel GAN architecture and supplies some theoretical insights. However, the empirical evidence is limited by using very small graphs (e.g., graphs with 9 or 20 vertices) and while the theoretical contributions are interesting, several of the proofs (such as those in Corollary 1 and Proposition 1) appear to be not fully rigorous or informative. The reviewers have noted that many approaches in the literature already tackle isomorphism and scalability issues by alternative means.\n\n• Limited experimental validation and baselines: Multiple reviewers remarked that the experiments are not convincing enough. For instance, the datasets are too small to be representative of real-world complex graphs, and the study did not compare against important baselines like NetGAN, TagGen, or even simpler random graph models (e.g., Watts–Strogatz or Barabási–Albert). Thus, the performance improvements claimed by the authors remain under-supported by comprehensive experiments.\n\n• Scalability and independence issues: There are concerns about the method’s ability to model complex edge dependencies given that the edge sampling is done independently, which questions the claims regarding modeling capacity. This also relates closely to the broader discussion about potential mode collapse and diversity issues in GAN-based graph generation approaches.\n\n• Theoretical and presentation issues: Some of the provided theoretical contributions and proofs seem either overclaimed or are lacking in rigor. The importance of certain components, like the role of the learned parameters φ, is not well clarified, which makes it harder for readers to fully understand the contribution. Similarly, some parts of the presentation (e.g., the clarity in Section 2.2.3 about avoiding collisions) have been flagged as obscure.\n\nWhile there are interesting contributions and a novel perspective in the method, the combination of limited experimental evidence, insufficient comparisons with key state-of-the-art approaches, and theoretical sections that do not fully justify the main claims results in a paper that does not yet meet the standards for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes an interesting idea of leveraging semantic graphs derived from pre-trained object detection outputs to perform out-of-distribution detection. While the method may offer an intuitive and interpretable approach, major issues remain with the work.\n\nSeveral reviewers pointed out significant weaknesses:\n1. The paper suffers from poor writing quality, unclear definitions (e.g., “common sense”, “child cognition”), and inconsistent notation; these factors make it difficult for the reader to follow the method.\n2. The methodology is not described in sufficient detail. Critical parts of the pipeline, such as the feature extraction procedure and the subsequent graph generation steps, are not adequately elaborated. This lack of clarity hinders reproducibility.\n3. The experimental evaluation is limited. The study focuses solely on the LSUN dataset while ignoring larger comparisons with state-of-the-art OOD detection techniques and other in-distribution datasets. Reviewer comments stressed the absence of proper baseline comparisons and ablation studies (e.g., sensitivity to the pre-trained object detector’s performance).\n4. Important assumptions, such as relying on an “oracle” object detector for semantic extraction, are not convincingly justified. This assumption may not hold in many practical settings, particularly in domains where pre-trained object detectors cannot capture all relevant semantics.\n5. Despite promising AUROC figures reported by the authors, the experiments lack a controlled and comprehensive analysis to support these claims. The evaluation is not extensive enough to confirm that the approach outperforms current best practices.\n\nWhile some reviewers recognized the novelty in constructing human-interpretable semantic graphs, the combined concerns regarding clarity, methodological rigor, and experimental comprehensiveness outweigh the contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThis paper introduces MCTransformer, a framework that integrates Monte-Carlo Tree Search (MCTS) with a Transformer in an offline reinforcement learning setting. While the idea of combining sequence modeling with MCTS for improved exploration is interesting, several critical issues undermine the work’s contribution at the level required by top-tier conferences.\n\nFirst, the novelty of the approach is limited. Multiple reviewers pointed out that the method is essentially a combination of existing approaches: Transformers for decision modeling and MCTS, in a framework conceptually comparable to methods like AlphaGo/AlphaZero. Simply substituting a ResNet with a Transformer, without additional significant technical innovation, does not amount to a substantial contribution.\n\nSecond, the experimental evaluation is severely lacking. The evaluation solely on SameGame—a relatively toy and non-standard domain—raises concerns regarding the generalizability and robustness of the proposed method. The absence of experiments on more widely accepted benchmarks (e.g., Atari, MiniGo) or comparisons with relevant baselines such as Decision Transformers or modern offline RL methods makes it hard to assess the true effectiveness of the approach. Moreover, there are important questions regarding fairness in comparison (simulation counts versus wall-clock time) that remain unanswered.\n\nThird, the clarity and presentation of the paper are problematic. Multiple reviewers noted that the exposition is unclear and that numerous details (e.g., aspects of the rollout policy, the role of various components in the framework) are left ambiguous. This lack of clarity hinders reproducibility and undermines the impact of the contribution.\n\nFinally, while the idea has potential, it is premature given the limited empirical evidence and the low degree of technical novelty. The reviewers unanimously convey significant concerns about both the methodology and the experimental setup.\n\nTaking these points into account, the weaknesses in novelty, evaluation, and exposition indicate that the paper is not ready for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning leading to my final decision.\n\n1. Contribution and Relevance: \n • The paper tackles one of the central theoretical problems in understanding why gradient descent methods work efficiently in training deep neural networks. Even if considered on the linear network side, it addresses both the convergence of gradient descent and a very interesting phenomenon whereby the trajectories of deep overparameterized networks can be closely tracked by an equivalent convex problem. This insight is novel and provides useful intuition for the nonlinear setting. \n\n2. Novelty and Technical Strength: \n • While several parts of the analysis build upon techniques and results from previous work (notably Du & Hu (2019) and Hu et al. (2020)), the paper makes important extensions. In particular, it extends the convergence analysis beyond the squared loss to a class of general strongly convex and smooth objectives, and handles more general architectures with arbitrary widths. \n • The key novel contributions are in establishing that the discrete dynamics can be approximated by a scaled version of the standard convex dynamics and in showing that the deep linear network trajectories avoid issues such as bad saddles; these are non-obvious technical challenges.\n • The paper further enriches the discussion with a new Section 4 that provides insights into why overparameterization is essential and how it leads to favorable optimization properties. This explanation, and the closed-loop trajectory result in Theorem 3.3, are important advances.\n\n3. Responses to Criticisms:\n • Some reviewers were concerned about novelty, with the view that much of the analysis is an extension of Du & Hu (2019). In their author response, the authors clearly delineated the technical barriers that arise when one moves from the very structured squared loss (and full-rank X) to more general objectives (where low-rank X must be handled via a seminorm). \n • Other concerns revolved around overclaims and presentation issues; the authors have addressed these by revising the title, abstract, and by adding remarks and a whole new section for clarification.\n • The technical contributions regarding the analysis of the eigenvalue bounds of the linear operator P(t) using concentration inequalities for the beta distribution and related techniques are nontrivial.\n • Although some of the main ideas follow previous approaches, the improved analysis and the new insights on GD trajectories and the avoidance of bad saddles do add to the literature.\n\n4. Overall Evaluation:\n • The reviewers were split but leaning towards acceptance. Reviewer 2 and Reviewer 4 clearly recommended acceptance, while Reviewer 3, despite some reservations on novelty, found the work enjoyable and important. Reviewer 6 had concerns mainly about discussion and insight — points that the revised version has addressed with additional explanations. \n • The paper meets the standards for a top-tier conference in terms of technical correctness, insight and potential impact on understanding optimization in deep networks.\n • The weaknesses are either addressed by the authors’ revisions (clarifications, additional discussion) or are minor relative to the significant contributions.\n\nBased on the above reasoning, I conclude that the paper should be accepted.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and Importance:  \nThe paper tackles an important and timely problem—understanding the implicit bias behind widely‐used value estimation algorithms (TD, FVI, RM) when using overparameterized linear models. Given the recent interest in overparameterization (and its extensions via NTK) in deep learning, studying the simplified linear case in RL is a natural and useful step toward understanding more complex deep RL settings. The paper provides an analysis showing that, even when approximation error vanishes, different update rules converge to distinct fixed points. This insight—as well as the unified view recasting these algorithms as solving a constrained norm minimization problem—is a conceptually interesting contribution.\n\n2. Theoretical and Empirical Analysis:  \nThe authors derive convergence properties and generalization bounds in the overparameterized regime and even propose practical regularizers that improve stability in experiments. Although some aspects (e.g., the least-norm solution characterization) are known in convex optimization, the contribution here is to show how these classical facts uniquely manifest in the RL context (particularly in differentiating the fixed points of TD versus RM/FVI). Moreover, the authors connect the results with the neural tangent kernel framework to suggest extensions beyond linear settings. On the other hand, several reviewers noted that the technical novelties in the linear setting are incremental and that the experimental validations are on relatively simple environments. Nonetheless, the theoretical insights and the connection made with modern overparameterization theory provide value.\n\n3. Technical Correctness and Clarity:  \nThere were concerns expressed by several reviewers on some technical details (e.g., matrix decomposition arguments, rewriting of certain equations, and proper credit for some known results). Importantly, however, the author responses provided clear, detailed clarifications and promised additional discussion and corrections in a revised version. Although some presentation issues and minor mistakes remain, they are fixable and do not undermine the main claims.\n\n4. Overall Reception and Balance of Opinions:  \nWhile a few reviewers (e.g., Reviewer 6, Reviewer 9) were more critical—raising points about proper recognition of known results and clarity in derivations—the majority of reviewers (including those who increased their scores after rebuttal) expressed positive assessments. The author responses largely address the concerns, and the work is viewed as a respectable contribution to the theory of temporal-difference learning, even if some improvements (especially on experiment scale and exposition) are warranted.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes Collaborative Adversarial Training (CAT), a framework that trains two robust models simultaneously by exchanging adversarial examples and peer logits during training. The idea is motivated by the observation that different adversarial training methods (e.g., AT and TRADES) have complementary strengths and make different mistakes. Experimental results on CIFAR-10 and CIFAR-100 indicate some improvement in robustness compared to individual strategies.\n\nHowever, several significant concerns have been raised by the reviewers:\n\n1. Novelty and Technical Contribution:\n • The approach is conceptually straightforward and resembles existing ensemble and collaborative methods in adversarial training. Several similar ideas have been explored in prior work (e.g., ensemble adversarial training), which were not adequately cited or compared.\n • There is a lack of theoretical or in-depth empirical analysis regarding why and how combining different adversarial training methods results in increased robustness. The mechanism behind the observed gains remains insufficiently explained.\n\n2. Experimental Evaluation:\n • The evaluation is limited to small datasets (CIFAR-10 and CIFAR-100) with limited network architectures, leaving questions about scalability to larger datasets like ImageNet.\n • Key baselines are missing. Comparisons with recent state-of-the-art methods and ablations (e.g., training one network with multiple attacks or a setup where both networks use the same adversarial training method) are needed to support the effectiveness of the proposed method.\n • The increased computational cost (roughly 2x) and memory overhead were noted, which further diminishes the appeal of the method in a setting where efficiency is crucial.\n\n3. Clarity and Presentation:\n • While the writing is generally clear, the paper contains several typos and lacks detailed descriptions of some aspects (e.g., certain abbreviations like “ALP”), which affect the overall quality.\n • The method’s motivation to overcome the shortcomings of standard adversarial training is not sufficiently substantiated by rigorous analysis or comparison against relevant baselines.\n\nGiven these significant concerns around novelty, incomplete experimental analysis, missed comparisons, and higher computational cost, the paper does not yet meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes maintaining a diverse population of agents by framing the problem as a bi-objective optimization that focuses on skill level and playing style. While the idea of enforcing diversity explicitly during self-play is interesting, the reviewers collectively have raised notable concerns.\n\nReviewer 1 highlights that the paper omits a discussion and experimental baseline from an important related work (Lanctot et al., 2017). In addition, the experimental design appears to favor the proposed method (BiO) since the sampling of opponents is skewed towards it relative to other methods. Furthermore, there is concern over whether the claimed playing style diversity is actually superior compared to what is already achieved in works such as AlphaStar. The reviewer also questions some general claims and the necessity of certain details, like the inclusion of PPO’s equation.\n\nReviewer 2 finds the work promising but identifies weaknesses in simplification (reducing playing style to a single scalar) and calls for further clarification on several technical details, such as the definition of state changes and the explanation of reward weighting. Moreover, the evaluation lacks information about statistical significance (e.g., confidence intervals) and could benefit from a larger number of experiments and transparency regarding cherry-picking of examples.\n\nReviewer 3 acknowledges that while the results are promising on two domains, the comparisons are rather narrow. Specifically, the absence of quality-diversity baselines (e.g., MAP-Elites) and additional evaluation domains raises questions about the generality and robustness of the claims. Reviewer 3 also raises concerns about the scaling properties of the approach when the dimensionality of play style increases, and the experimental setup (e.g., randomized sampling advantages and match-up imbalances) further weakens the empirical validation.\n\nOverall, while the technical idea is interesting, the concerns about missing baselines, experimental design biases, limited empirical settings, and marginal novelty relative to existing literature (especially in quality-diversity and multi-objective optimization for RL) suggest that the paper does not reach the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper proposes ARoW, a modification to adversarial training (specifically TRADES‐like losses) that weights the regularization term to penalize less robust samples more heavily. On the surface, the experimental results demonstrate small but consistent improvements in both standard and robust accuracy on several benchmarks. The authors also mention extra benefits (e.g., improved fairness and ease‐of‐implementation relative to methods such as HAT).\n\n2. However, several issues limit its overall impact for a top‐tier venue. Most reviewers agree that the theoretical contribution is not very strong. Many concerns have been raised regarding the formulation and rigor of Theorem 1. In a number of reviews the “novel” theoretical bound is characterized largely as a re‐expression or a loose relaxation of the prior bound used in TRADES. In the binary case it is equivalent and in the multi‐class case it relies on an ad-hoc relaxation. The lack of clarity on the definition and uniqueness properties of z(·) further raises doubts.\n\n3. Experimentally the improvements over baselines such as TRADES and HAT—especially when extra data is available—are minimal. Although the authors have added additional ablations and clarified some points during rebuttal, the numerical gains are marginal (e.g. improvements on robust accuracy on the order of 0.1–0.2 percentage points in some cases). Furthermore, the evaluations are limited to relatively small datasets (CIFAR-10 etc.), thus leaving open questions on scalability and impact on larger datasets.\n\n4. While some reviewers were willing to increase their scores during the rebuttal, several others remain concerned with the overclaim about the theoretical bound and its clarity. In a top‐tier conference the combination of novel theoretical insight and strong empirical performance is expected. Here the theoretical part appears to be mostly a rehashing of ideas from TRADES without sufficient novelty or mathematical rigor, and the empirical gains, although consistent, are not large enough to outweigh this weakness.\n\nIn summary, while the paper does show a method that is simple, has some empirical advantages, and offers potential practical benefits (such as simplicity and fairness improvements), the contributions in terms of novelty and theoretical rigor are too modest for a top‐tier venue. The marginal empirical improvements and insufficiently clear theoretical formulation lead me to conclude that the paper does not meet the high standard expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Contribution & Novelty:\n • The paper provides a rigorous analytic treatment of epoch‐wise double descent using a linear teacher–student framework. In doing so, it derives closed‐form expressions for test error dynamics and interprets the mechanism in terms of multi-scale learning (fast vs. slow features). This treatment builds upon and complements recent work (e.g., Heckel & Yilmaz and Stephenson & Lee) while also connecting with literature on implicit ridge regularization under SGD/GD.\n • Although aspects of the contributions have some overlap with prior work, the extension to transient (finite‐time) dynamics and construction of an alternative loss (with connections to ridge regularization) is nontrivial and provides a fresh perspective.\n\n2. Technical Soundness and Clarity:\n • The theoretical derivations are fundamentally sound. While some reviewers noted typos, clarity issues, and minor notational inconsistencies, the author’s response outlines extensive revisions aimed at remedying these issues.\n • The authors have strengthened their exposition (by adding detailed appendices and clarification of assumptions) and provided additional experiments (including NTK analysis and triple descent illustrations) that enhance the paper’s clarity and impact.\n\n3. Relevance and Connection to Practice:\n • By relating the linear model findings to realistic deep networks (as with the ResNet18 experiments on CIFAR-10) and invoking the NTK approximation, the work successfully builds a bridge between theory and practice. It provides intuitive grounding for an observed phenomenon in deep learning, making it interesting for both theorists and experimental researchers.\n\n4. Reviewer Feedback and Author Response:\n • All reviewers recognized the general merit and interesting aspects of the work. Although some concerns and suggestions were highlighted (e.g., clarifications regarding SGD noise, detailed connections to prior literature, and handling multiple scales), the authors have offered a comprehensive response.\n • The revised manuscript appears to have incorporated additional derivations, clarifications, and improvements to notation and presentation. This iterative process reassures that major concerns are being addressed.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Scope and Contribution: The paper introduces AutoOED, a software platform for multi‐objective optimal experimental design. It emphasizes ease-of-use (with a GUI) and a modular design that supports asynchronous MOBO (including a new Believer-Penalizer strategy). While these features make the tool attractive for certain application communities, many reviewers felt that the “novel” algorithmic contribution is either superficial or a natural extension of existing work rather than a breakthrough.\n\n2. Novelty & Technical Contributions: Several reviewers noted that most of the techniques implemented (e.g., extensions of Kriging Believer and Local Penalization for asynchronous optimization) are incremental, with little compelling theoretical support or deep methodological innovation. In particular, concerns were raised about the lack of clarity and theoretical justification behind BP, and that many of the design choices are either already seen or only a modest modification.\n\n3. Empirical Evaluation: Although the authors present extensive benchmarks and even comparisons against other established platforms, the experimental results seem mixed. Some figures show that BP is not always the best performer, and the benefits over existing approaches can be tenuous. The claim that BP is “state-of-the-art” is considered overstated by multiple reviewers, and the experiments sometimes appear to be cherry-picked.\n\n4. Engineering vs. Scientific Contribution: Even though the tool is well-engineered and useful from a practical standpoint – in particular for non-expert users – a majority of the reviewers concluded that the paper is more “systems/engineering‐oriented” with only marginal new technical insights. For a top-tier conference, a balance is expected that not only shows impressive engineering but also high scientific novelty. The paper seems to lean too far towards being a software systems paper, with its “novel” algorithmic idea being presented in only a preliminary or intuitive manner.\n\n5. Author Response: The authors’ responses clarify several concerns (e.g., support for many-objective problems, details about asynchronous evaluation, and clarifying the motivation behind BP) which are useful, but still do not offset the central critique that the core technical novelty and theoretical development are limited.\n\nIn conclusion, while AutoOED is undoubtedly a well-engineered platform with potential impact for applied users, the contributions from a research perspective—both in terms of theoretical innovation and robust empirical novelty—are too marginal for acceptance at a top-tier venue. The paper is better suited as an application/system demonstration paper rather than a full technical contribution that meets the rigorous standards of a top conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper presents a novel theoretical connection between neural network approximation and tropical geometry. Formulating error bounds in terms of the Hausdorff distance between tropical zonotopes is innovative and, as the authors stress, represents a first step toward a purely geometrical understanding of NN approximation.\n\n2. The theoretical contributions—including Theorem 2 (and its subsequent reformulation into a more general result for neural networks)—are solid and written in a rigorous manner. This establishes a new perspective that should be of interest to the community working on neural network theory and geometric analysis.\n\n3. On the application side, the paper proposes two compression methods based on K‐means applied either to zonotope generators or directly to neurons. While the experiments serve largely as a proof‐of-concept, they confirm that the theoretical insights hold in practice. All reviewers noted that the empirical evaluation is somewhat limited (e.g. using older architectures and small datasets) and that comparisons with “state‐of‐the‐art” methods are not entirely comprehensive. However, the authors have been clear about the scope (focusing on structured pruning in fully-connected layers) and provided adequate clarifications in their response.\n\n4. Several concerns raised by the reviewers (for example, clarifications regarding experimental settings, comparisons to unstructured methods, and complexity details) have been addressed by the authors. They have provided code as supplementary material, included a runtime analysis, and acknowledged both limitations and future extensions. This responsiveness improves the paper’s overall clarity and reproducibility.\n\n5. Although the empirical contributions are limited and the experiments remain proof‐of-concept level, the novelty and correctness of the theoretical work are significant. Given that many top-tier conferences (especially those with a focus on machine learning theory) value strong theoretical insights even if the empirical part is not yet fully exhaustive, the paper meets the standard for a top-tier venue.\n\nBased on all of the above, the paper’s strong theoretical contribution, novelty in methodology, and the added improvements in the updated version outweigh the concerns over limited experiments. Therefore, I lean toward accepting the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper “Sample and Computation Redistribution for Efficient Face Detection” presents two complementary methods (Computation Redistribution and Sample Redistribution) to improve face detection with low computational budget. The method focuses on optimizing the backbone, neck, and head computation distribution as well as designing a scale augmentation strategy tailored for small faces. Across a series of experiments on WIDER FACE and additional datasets (AFW, PASCAL, FDDB), the proposed SCRFD models achieve competitive state‐of‐the‐art results, and the reported performance improvements over methods such as TinaFace, BFBox, and HAMBox are impressive.\n\nThe reviewers acknowledge the significant empirical contributions and the practical importance of the approach. Although several reviewers observed that the search strategy is relatively straightforward (using random search in a simplified search space) and that comparisons could be more exhaustive regarding other network search methods (evolutionary approaches and one-shot NAS), the authors have addressed these concerns thoroughly in their response by including additional comparisons, algorithm details, and experimental results. In particular, the authors added comparisons against evolutionary search baselines and an alternative one-shot NAS approach, demonstrating that their method outperforms these alternatives under similar computational constraints.\n\nWhile the technical novelty might be assessed as only marginally novel by some reviewers due to the simplicity of the search strategy, the combination of effective computational redistribution, efficient sample augmentation, and the strong empirical results on challenging face detection tasks essentially justify its contribution. The paper is well motivated by the real-world constraint of detecting small faces in low-resolution images, and the improvements in accuracy–efficiency trade-offs make it highly relevant for academic and industrial applications.\n\nGiven the strong empirical evidence, the thorough author response addressing potential weaknesses, and the overall positive feedback across multiple reviewers, the paper meets the standard of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my reasoning.\n\nThe paper introduces a bilevel optimization framework for adversarial robustness where a meta‐learned reweighting network uses multi‐class margins to assign importance to individual samples. While several reviewers appreciated that the approach is well motivated, clearly written, and shows some empirical improvements (especially as an add-on to methods like TRADES), multiple concerns remain. In particular, a number of reviewers pointed out that the technical novelty is largely incremental: the work adapts ideas (MAML‐style meta-learning and margin based reweighting) that already exist in the literature with only modest improvements. There were repeated concerns about the evaluation. In particular, the treatment of adaptive attacks is not fully convincing, the gains under strong adversaries are marginal (especially when the network is combined with other methods such as TRADES), and the performance results seem to depend on certain design choices (e.g. the specific network architectures, small-scale datasets such as MNIST/FMNIST, omission of CIFAR-100 results in the main text, etc.). Furthermore, some of the ablation studies and discussions regarding the input encoding for the meta network (and its relation to increased capacity) were deemed insufficient to fully justify the proposed design.\n\nOverall, while half of the reviewers felt that the paper is interesting enough to warrant acceptance, the other half raised significant concerns about novelty, evaluation completeness (especially adaptive attacks), and clarity regarding what exactly drives the reported improvements. For a top‐tier conference where both novelty and rigorous, clear empirical validation are critical, these concerns lead me to conclude that the paper is not yet ready for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed evaluation:\n\n1. Novelty and Potential Impact:\n • The paper tackles a core assumption in supervised learning by modeling each data point as coming from its own function, thereby defining “functional risk minimization” (FRM). This is a bold take that challenges traditional ERM and could have significant repercussions, especially in the context of modern over‐parameterized neural networks.\n • Although similar ideas appear in parts of hierarchical Bayesian modeling, the authors clearly differentiate FRM as a fundamentally different approach—the work shows promise to impact multiple areas (supervised, unsupervised, reinforcement learning).\n\n2. Theoretical and Algorithmic Contributions:\n • The theoretical contributions include demonstrating that, for several common loss functions, FRM subsumes the standard ERM formulation. However, several reviewers noted that some key equations (notably eq. 7 and eq. 8 initially) are not fully or clearly derived.\n • The authors have now responded with clarifications, improved derivations, and extra details (e.g., explanation of the Laplace and Taylor approximations leading to the scalable eq. 9). While the derivations may still require further polishing for absolute clarity, they now more clearly connect the implementation with the theoretical framework.\n\n3. Scalability and Empirical Evidence:\n • Initial concerns focused on the scalability of the algorithm due to the use of Hessian approximations; the authors address these concerns in detail by describing efficient Hessian–vector computations and by adding experiments which show that FRM scales to CNNs.\n • New experiments on popular variants of MNIST using a VAE with FRM demonstrate competitiveness, particularly when structured data variation is present. This strengthens the claim that FRM may yield large gains in situations where the assumptions of structured noise hold.\n • In addition, experiments across supervised, unsupervised, and reinforcement learning strengthen the empirical contribution.\n\n4. Clarity and Presentation:\n • There were legitimate concerns about the clarity of the presentation and the proper distinction between modeling assumptions (functional generative models) and training objectives (FRM). The authors have acknowledged these issues and promised improvements in the final version.\n • Even though some aspects remain less polished, the work’s novelty and the new experimental validations help lift the overall quality above that of many incremental submissions.\n\n5. Overall Assessment:\n • Reviewer evaluations range from borderline (mostly regarding clarity and derivation details) to mildly positive. Reviewer 3, for example, was on the accept side citing novelty.\n • The author response has been comprehensive, addressing scalability, theoretical derivations, and experimental validations. While some gaps remain (mainly in exposition), the potential impact and the breadth of the experiments lend significant strength to the paper.\n\nGiven that top-tier conferences can afford to accept papers which open up new research directions even if some parts need further refinement (especially when the authors have clearly planned improvements), I conclude that the paper represents a strong and daring new idea with promising results.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n• The paper tackles a practically important problem – how to construct a reliable validation set without losing training data – and the idea of using augmented data to form a “proximal” validation set is certainly interesting. However, the reviewers independently point to significant issues.\n\n• Multiple reviewers note that the paper suffers from a lack of clarity. Critical details, especially in the experimental section, such as definitions of metrics (e.g., what “F1” means in context, what “deterioration” in the tables represents) and how improvements in model selection (e.g., hyperparameter tuning outcomes) are measured, remain ambiguous. This lack of clarity makes it difficult to fully assess the contribution.\n\n• There is a recurring concern about experimental validation. Reviewer 1 argues that experiments are performed on fixed models rather than addressing their impact on model selection. Reviewer 2 highlights that the datasets used are very small, raising questions about the generalizability of the claims to real-world or larger-scale problems. Reviewer 3 also notes that more experiments (especially in industrial or more challenging setups) are needed to fully justify the claims. Reviewer 4 finds that the evaluation metrics used might not be sufficient to claim that the constructed validation set faithfully represents the test distribution.\n\n• The theoretical foundations of the proposed method appear to be weak or missing entirely. Even the authors acknowledge limitations in theoretical analysis, but this is a critical shortcoming, particularly for a top-tier conference paper where both empirical and theoretical support are expected.\n\n• There are also concerns about the originality and significance of some contributions, as well as the dependencies on choices such as the type of data augmentation and weighting scheme, which are not adequately ablated or justified.\n\nConsidering these points – lack of clarity in both formulation and experimental details, insufficient empirical evaluation (both in scope and in relevance to production scenarios), and missing theoretical guarantees – I conclude that the paper does not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n\nThe paper presents an interesting empirical observation that swapping the order of batch normalization and bounded activation functions (from the conventional “conv/BN+act” to “conv/act+BN”) leads to a phenomenon described as “asymmetric saturation” on bounded activations such as Tanh, which in turn increases sparsity and appears to improve generalization, at least for certain architectures and datasets. This observation is nontrivial and offers a fresh perspective on how seemingly minor architectural rearrangements can influence network behavior.\n\nHowever, across the reviews several weaknesses are consistently highlighted:\n\n1. Clarity and Rigor of Claims:\n – There are multiple points where the precise definitions—such as the “center of the function” or the “asymmetric saturation”—remain vague. Several reviewers pointed out that the chain of reasoning from saturation through skewness to improved generalization is not rigorously substantiated.\n – The explanations provided regarding why swapping BN and activation leads to improved sparsity and why this effect differs across layers are not definitive. Reviewer concerns about whether accelerated training rather than the saturation-induced sparsity is behind the performance boost remain only partly answered.\n\n2. Empirical and Methodological Limitations:\n – The analysis is limited to architectures that do not make use of standard residual connections, leading to questions about the broader generality of the findings. Although the authors explain that the residual design with Tanh is problematic, some additional investigation to extend or generalize the observations would strengthen the work.\n – The improvements, even though interesting, are limited to bounded activation functions and do not demonstrate a clear advantage over ubiquitous ReLU-based models. As several reviewers noted, the Swap model’s performance still does not outperform standard configurations – which could limit its practical significance.\n – Certain metrics (e.g., saturation, skewness, sparsity) and their computation lack clarity, and while the code is provided, some reviewers could not easily locate or understand the details.\n\n3. Theoretical Novelty and Impact:\n – While the observation about asymmetric saturation is interesting, the underlying theoretical arguments remain partly hand-wavy. The analysis—largely empirical—fails to fully isolate or quantify the connection between saturation and generalization in a convincing way.\n – Several reviewers found the technical contribution incremental and somewhat overclaimed, with marginal novelty beyond prior discussion in literature.\n\nAuthor responses address many points raised by the reviewers. They provide additional details in the appendix and clarifications on technical issues (e.g., the computations in the provided code, explanations on residual designs, and additional experiments on modified network architectures). However, these responses do not fully remedy concerns regarding the clarity, rigor, and generalization of the arguments. The core phenomenon is interesting, but the contribution appears incremental with unresolved questions that are important for a top-tier conference paper.\n\nGiven the combination of intriguing observations but insufficient rigor and broad applicability, as well as the limited performance advantages versus conventional models, the overall impact does not yet meet the threshold of a top-tier conference publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\n\nThe paper addresses the important problem of adapting black‐box classifiers to changing distributions, specifically when both label shift and additional conditional shift occur. The problem is relevant for real-world applications where distributional changes are not strictly limited to standard assumptions. The paper also makes an interesting attempt to extend the framework to regression and to design heuristics for handling non‐invertible confusion matrices. However, after carefully considering all reviewer feedback and the author’s responses, several major concerns remain:\n\n1. Lack of Systematic/Theoretical Foundations:\n • The paper relies heavily on heuristic modifications of previous methods (e.g., from Wu et al. 2021) without offering a clear theoretical basis. The proposed modifications, such as the use of an additional scaling hyperparameter or the identity approximation for the confusion matrix, are acknowledged by the authors as ad-hoc.\n • The Bayesian interpretation is used to motivate some components but suffers from unclear notation and a lack of detailed connection to the central problem.\n\n2. Empirical Evaluation and Novelty:\n • The empirical results across synthetic and realistic datasets are not clearly compelling. Most improvements are marginal at best, and in several cases the proposed methods show little advantage (or even a slight deterioration) versus baselines.\n • The experimental study lacks comparison to important baselines – for example, methods that directly use an oracle or strong validation, as well as other domain adaptation or domain generalization approaches. The scope of experiments (e.g., only considering constant shifts rather than fully online, continuously drifting scenarios) further limits the generality of the insights.\n • The contribution in extending to regression and handling degenerate matrices appears marginal and not backed up by a sufficiently convincing empirical story.\n\n3. Clarity and Reproducibility:\n • The presentation suffers from unclear notation, incomplete definitions, and insufficient self-contained background. Several key concepts (like “conditional shift” and details of heuristics) are not rigorously defined.\n • Important methodological details (such as how the OOD validation set is used in practice) remain ambiguous.\n • The clarity issues and notation confusion hinder both understanding and reproducibility.\n\n4. Overall Novelty and Significance:\n • While the problem is significant, the methodological contributions are incremental. The heuristics presented, although potentially useful, do not provide a demonstrably superior alternative to existing methods.\n • The novelty is further diminished by the reliance on a methodology (“logit adjustment” via reweighting) that still implicitly assumes label shift, even under settings where this assumption is explicitly broken.\n • Several reviewers expressed that while the work is interesting, the contributions in their present form are not sufficient for a top-tier conference.\n\nThe author responses address some of these points by clarifying notations and experimental protocols, and by adding additional comparisons—but many concerns (lack of theoretical grounding, limited empirical gains, and ambiguous presentation) persist.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes ProtSeed, a method for protein sequence and structure co-design that uses a trigonometry-aware encoder and a roto-translation equivariant decoder to iteratively update protein representations. The approach is motivated by the need to overcome the inefficiencies of autoregressive and diffusion models by jointly refining sequence and structure. Most reviewers appreciated the strong empirical performance and the overall clarity of the method, with many noting that the iterative cross-conditioning of sequence and structure is an interesting and useful contribution.\n\nSeveral reviewers raised concerns about details that need clarification:\n• Some parts of the method (e.g., the SeqIPA calculation) lack sufficient exposition and require clearer derivations or algorithm boxes.\n• There are minor issues regarding hyperparameter settings, training cost details, and reproducibility which the authors have promised to address in the final version (with added ablation studies, appendix details, and code release).\n• A few reviewers questioned the technical novelty compared to existing methods like AlphaFold2 or previous co-design approaches, although most agreed that while some modules are inspired by earlier work, the overall framework and its joint design capability represent a valid contribution.\n• Minor discrepancies in evaluation metrics and comparisons were pointed out, but the authors responded with clarifications and additional evaluations.\n\nOverall, while some aspects (such as exposition details and complete ablation studies) need to be further refined, the reviewers generally view the contributions as significant and useful for the protein design community. Improvements have been proposed by the reviewers and the authors have provided satisfactory responses and plans for additional explanations and resources.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by my final decision.\n\nDetailed Reasoning:\n• Scope and Novelty: While the paper addresses an interesting problem—identifying the exact phase transitions in permuted linear regression using a message-passing framework—the reviewers agree that much of the methodology builds heavily on previous work (e.g., Semerjian et al. 2020, Zhang et al. 2020). Several reviewers pointed out that the key results either follow directly as a corollary or are obtained using approximations whose validity is not thoroughly justified. In short, the technical novelty is only marginal relative to the prior art.\n\n• Clarity and Presentation: The primary concern across all reviews is that the paper suffers from severe clarity, organization, and writing issues. Reviewers note that the paper is “notation heavy” and that key sections (especially on deriving the message-passing updates and the transition thresholds) are confusing and poorly motivated. There is little self-contained explanation, and many notations and derivations are either copied nearly verbatim from previous publications or introduced with insufficient context. The writing problems also include numerous typos and grammatical mistakes.\n\n• Rigor and Reproducibility: Several reviewers expressed concern that the technical claims are not fully supported or that some steps in the derivations lack sufficient rigor. Furthermore, while numerical experiments are cited, it is unclear how these results bridge the asymptotic theory and finite-sample practice, and the code for experiments is not released. These factors impact reproducibility and make it difficult for readers not already familiar with the referenced literature to follow the work.\n\n• Overall Assessment: Despite the potential interest of studying phase transitions in such a structured problem, the current paper does not meet the standards of a top-tier conference. The combination of modest technical novelty, heavy reliance on existing results, and significant clarity and presentation issues means the submission is not ready in its current form for a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper introduces FedPA, a meta-learning based aggregator aimed at addressing heterogeneity in federated learning by accounting for both client drift and the newly introduced notion of period drift. While the idea of using meta-learning for adjusting aggregation is interesting and potentially relevant, the reviewers consistently raised several significant concerns:\n\n1. Novelty and Theoretical Justification: \n • Although the idea of “learning to aggregate” is promising, the novelty is limited given that similar ideas exist in knowledge distillation and other FL aggregation techniques. \n • There is insufficient theoretical or experimental discrimination between client drift and period drift, which is central to the paper’s motivation. Reviewers agree that the notion of period drift is not well defined or connected rigorously to the methods proposed.\n\n2. Experimental Validation and Scalability:\n • The experimental settings are quite weak: the datasets (e.g., EMNIST and MovieLens) and models used are too simplistic compared to the state-of-the-art; suggestions to include CIFAR-100, Stack Overflow, or LEAF datasets were noted.\n • There is a lack of proper ablation studies (e.g., sensitivity to proxy data distribution, model architecture choices for the aggregator, effect of hyperparameters).\n • Some experimental results (e.g., on FEMNIST) show unexpectedly low performance compared to what is reported in the literature.\n • Scalability issues and the overhead on the server due to the parameterized aggregation method were not well addressed, which is critical for federated learning applications.\n\n3. Use of a Proxy Dataset and Privacy Concerns:\n • The reliance on a proxy dataset limits the applicability of the method in real-world federated learning scenarios, where obtaining and using such data might not always be feasible.\n • Concerns were also pointed out regarding the method’s consistency with FL privacy principles, especially when the server accesses detailed client updates.\n\n4. Writing and Clarity:\n • Multiple reviewers noted the presence of grammatical errors, poor formatting, and unclear explanations. Several sections and figures require significant improvement in clarity and presentation.\n\nGiven the combination of limited novelty, theoretical and experimental weaknesses, scalability concerns, and issues related to privacy and clarity, the paper, in its current form, falls short of the standards typically expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper proposes using an auxiliary multi-task learning strategy in which, in addition to the conventional one‐hot label classification, the network is also trained on randomly generated Binary Labels. The method is interesting because it suggests that an alternative label representation can induce the network to learn complementary features that enhance performance. However, after careful evaluation of the reviews and author response, several concerns stand out:\n\n1. Experimental Evidence and Analysis: \n • Multiple reviewers noted that while there are instances of performance gains, the improvements over the baseline (standard one-hot training) are minor and not consistently significant across datasets. \n • The experimental section suffers from a lack of thorough analysis. For example, it does not sufficiently address the circumstances (e.g., fine-grained datasets, limited data scenarios) where the auxiliary task helps the most.\n • Additional comparisons against stronger baselines (including the SOTA methods and parallel label-transformation approaches from label smoothing or error-correcting output codes) are missing.\n • Important hyperparameter studies, such as weighting between tasks and the sensitivity to the random generation of binary labels, are either inadequately explored or omitted.\n\n2. Clarity and Reproducibility:\n • The reviewers highlighted several issues with clarity (e.g., confusing notation, vague descriptions of how Binary Labels are generated and used). \n • Key details necessary for reproducibility (optimizer settings, learning rate, epoch counts, details of “Metabalance,” etc.) are lacking.\n • The paper sometimes discusses related concepts (like “dense labels”) without clear explanation, making it challenging for a reader to fully grasp the methodology.\n\n3. Technical Novelty and Significance:\n • While the idea is novel to some degree, the technical contribution is marginal. The core idea of using an auxiliary task based on random binary partitions is reminiscent of existing techniques in label coding and unsupervised meta-learning. \n • The reviewers noted that similar ideas have been considered in earlier literature on multi-class learning via alternative label representations.\n • Given the limited novelty and the relatively small empirical gains, the overall impact remains in question.\n\n4. Overall Impact:\n • Given the minor empirical improvements, the limited theoretical insight into why binary labels improve performance, and the missing experimental depth, the paper appears more like a preliminary exploration than a sufficiently mature contribution for a top-tier conference.\n • The clarity and reproducibility issues further reduce confidence in its readiness for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nAll four reviewers praise the clarity, ease of understanding, and sound motivation behind the paper’s proposed contrastive token (CT) learning objective. However, there is a common concern regarding the lack of novelty. The main contribution – a contrastive formulation that separates positive and negative tokens – is seen as only a slight modification of unlikelihood (UL) training. Several reviewers point out that the core technical ideas have substantial overlap with UL, and the claimed differences appear to be minor or even questionable upon closer inspection.\n\nAdditionally, there are concerns regarding the empirical evaluation. Most experiments are performed on relatively small models (e.g., GPT2-small) and a limited set of tasks (mostly language modeling and dialogue generation). Reviewer 2, in particular, highlights that using perplexity as a quality metric and not including stronger baselines or evaluations on larger models with more competitive metrics makes the experimental validation unconvincing. There is also an issue with some baseline comparisons and experimental details (e.g., decoding strategies, metric definitions, and scaling experiments) that could have provided more robust evidence.\n\nOverall, although the proposed method is simple and well-motivated and the paper is easy to follow, the contributions are seen as incremental with insufficient empirical evidence and a lack of strong novelty. The reviewers collectively indicate that the paper does not meet the bar for publication at a top-tier conference like ICLR given the limited scope of experiments and marginal novelty relative to existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and Novelty: The paper reports an empirical study on the effect of swapping the order of batch normalization and bounded activation functions (e.g., Tanh). The authors identify that when BN is placed after the activation, the network exhibits “asymmetric saturation” which, after normalization, leads to higher sparsity and, in some cases, improved generalization. Although the observation is interesting and the authors propose metrics to quantify saturation, skewness, and sparsity, the overall contribution appears incremental. Many reviewers noted that while the behavior is counterintuitive and worthy of study, the net improvement remains modest, especially compared to standard unbounded activations like ReLU.\n\n2. Experimental Evidence and Analysis: The paper contains a range of experiments – with VGG-like networks and even some comparisons using a ResNet variant – and provides ablation studies on the impact of weight decay and a detailed analysis of layerwise behavior. However, several aspects cause concern:\n • Some reviewers noted that the improvement with bounded activations (and the Swap order) fails to outperform conventional architectures with ReLU, which limits the practical applicability.\n • There are inconsistencies in the reported “saturation” and “skewness” metrics over different layers. Although the authors have attempted to address these issues (for example by using a more appropriate model variant for CIFAR), questions remain about how generally applicable these insights are.\n • The experimental setup is somewhat narrow—mostly using VGG-like models on image classification benchmarks. Even though the authors present a rebuttal that includes experiments on a ResNet variant, the bulk of the analysis is still tied to overparameterization on small datasets.\n\n3. Clarity and Theoretical Justification: The paper has been critiqued several times for vague descriptions and loose definitions (for instance, the “center of the function” and the precise meaning of “asymmetric saturation”). The authors have provided additional clarifications and some adjustments in the revision; however, the explanations remain at times more intuitive than rigorously established. This contributes to a general lack of conviction about whether the observed phenomena are a matter of accelerated convergence, better generalization, or artifacts of specific experimental settings.\n\n4. Reproducibility and Technical Rigor: On the positive side, the authors provide code and detailed information about how metrics are computed. Still, some reviewers noted that important details (such as the exact calculation lines for saturation and skewness) were not initially transparent. Although the authors clarified this in the rebuttal, it suggests that the presentation needed significant polishing.\n\n5. Overall Strengths vs. Weaknesses: The paper introduces an interesting viewpoint—studying BN’s position relative to bounded activation functions and linking it to sparsity via asymmetric saturation—but the contributions are still seen as only marginally novel and not yet sufficiently general or practically advantageous relative to existing methods. The main claims about improved generalization remain somewhat controversial and supported by evidence that is not fully conclusive.\n\nAfter weighing these points and considering the standards of a top-tier conference—which require both precise, rigorous analysis and a level of novelty/practical impact that significantly advances understanding or performance—I find that while the study is interesting, the contributions are too incremental and several claims remain insufficiently justified or lacking in generalizability.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper attempts to leverage pretrained protein structural embeddings from OmegaFold to enhance protein-protein interaction (PPI) prediction using graph neural networks (GNNs). While the motivation is pertinent—bringing structural information into PPI prediction—the technical and methodological contributions are marginal. In essence, the proposed method relies on existing embeddings and a standard GNN architecture without innovating on either front.\n\nSeveral key issues were raised by the reviewers:\n1. Limited Novelty: Multiple reviewers noted that the paper's approach is essentially a combination of known methods (pretrained structural embeddings directly from OmegaFold and established GNN models like GraphSAGE and GAT). This combination is too straightforward and does not introduce any significant modifications or new insights that could advance the state-of-the-art.\n2. Baseline and Experimental Concerns: The experiments, while showing some improvement (approximately 5%), lack comparisons to more competitive baselines, such as state-of-the-art methods like AlphaFold-Multimer or dMaSIF. This makes it difficult to assess the true impact of the improvements, and reviewers have questioned if the performance gains are significant given the hard nature of PPI prediction.\n3. Methodological Issues: The approach of using mean pooling over structural embeddings may oversimplify the representation, potentially discarding important local structural nuances that could play a key role in protein interactions. Additionally, the use of a GNN on an entire network, with unclear details on negative sampling and architectural decisions, raises questions about the necessity of this design when the objective is to predict pairwise interactions.\n4. Clarity and Reproducibility: While some aspects are reasonably documented (e.g., hyperparameter settings), the paper lacks clarity in descriptions of experimental setups, including performance metrics (such as AUC measurement) and the construction of the binary classification problem.\n\nTaking into account the aforementioned criticisms—especially the low technical novelty, suboptimal baseline comparisons, and the ambiguity in method presentation—the combined contributions of the paper do not meet the high standards expected of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper introduces a novel mechanism—RR-on-Bins—for performing regression while ensuring label differential privacy. The work extends ideas from label-private classification (e.g., Ghazi et al. 2021) to the regression setting, where the loss function intrinsically encodes a similarity notion between labels (e.g., the squared error between nearby integers can be smaller than between distant ones). The novelty stems from both the formulation of the problem and the derivation of an optimal mechanism via an LP formulation with a structured solution that permits an efficient dynamic programming approach. In addition, the authors extend their results to cover the case where the label space is continuous (through the use of bins), and provide both theoretical guarantees and extensive experimental evaluations.\n\nThe reviewer comments are largely positive. Reviewers 3, 4, and 6 appreciate the clarity of presentation, soundness of the theoretical analysis, and the practical relevance in settings where label privacy is important. While Reviewer 5 raised a concern that the novelty is only marginal relative to prior work, the authors emphasize that the technical contributions (in terms of the LP and the dynamic programming solution) distinguish their work from past approaches. Furthermore, the experimental comparison (even if not “fair” in a strict sense) supports the effectiveness of the proposed mechanism, and Reviewer 2 mentioned that including these additional experiments in an appendix could add value. Reviewer 1’s comment regarding the distinction between regression and classification is also addressed via the authors’ clarification in the response.\n\nSome weaknesses were noted regarding the fact that the overall error (inclusive of the effect on the Bayes optimal error for predicting from features) has not been fully addressed in the mechanism design. However, the authors clearly articulate that this is an interesting research direction and that their method focuses on the specific aspect of optimal label randomization for a given loss function. This focus is well-justified in the context of ensuring label privacy under regression losses.\n\nIn summary, the paper provides a significant and novel contribution to the differential privacy literature by addressing regression scenarios with label privacy guarantees. The theoretical results are well-supported by proofs (with details available in the appendix), and the experimental results strengthen the practical relevance of the work. While some points are open for further exploration, they do not detract from the contribution or its potential impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and Impact: The paper introduces RuDar, a weather radar dataset for precipitation nowcasting that adds a new dimension (3D observations covering reflectivity and radial velocity at multiple altitude levels) and includes orography. This is useful for the community and may lead to follow‐up works. However, the contribution is primarily in the dataset release and its benchmarking rather than proposing innovative methodology. Several reviewers noted that while the dataset is attractive and useful, its novelty is limited and the use cases (e.g., for nowcasting) have been studied before with other datasets.\n\n2. Technical and Empirical Novelty: The baselines used are standard deep learning models such as ConvLSTM and, in response to reviewer concern, an additional experiment with Earthformer (a transformer-based model) was added. However, the new evaluation was only partially demonstrated (only with intensity input) and many points raised by the reviewers—such as additional ablations, more comprehensive metrics, and experiments beyond nowcasting—were deferred for future work or the camera-ready version. Thus, the technical and empirical novelty remains on the margin.\n\n3. Clarity, Reproducibility, and Scope: The paper is clearly written with a thorough description of the dataset, experimental setup, and benchmarking. All codes and datasets are promised to be publicly available. However, many comments noted that although the dataset is a strong enabler for future research, the work is largely a dataset paper without enough new insights on methodology to justify acceptance at a top-tier venue like ICLR.\n\n4. Reviewer Opinions and Community Fit: Reviewer comments are mixed. While there is overall agreement on the dataset’s potential utility, multiple reviewers (e.g., Reviewer 2 and Reviewer 3) have concerns about the limited technical novelty as well as the narrow focus on nowcasting. One reviewer even suggests that the work might be more appropriate for a journal with a higher impact in its specific community. \n\n5. Overall Evaluation: For a top-tier conference, the expected contributions are not only about releasing a dataset but also about accompanying it with significant new methodological advancements or rigorous, extensive experimentation that firmly establishes new state-of-the-art performance or deep insights. The presented work, while of good quality and clear, mainly provides a useful dataset and baseline benchmarks without sufficient novelty or extensive experimental depth to fully meet top-tier conference standards.\n\nBased on the above, my final decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Significance:  \nSeveral reviewers rated the technical and empirical novelty as marginal (mostly scoring a 2 on both aspects). Although the idea of leveraging biological insights (specifically from the thalamocortical circuit) to design RNN modules for continual learning and smooth motif transitions is interesting, the main contributions appear only incrementally different from existing methods. The toy nature of the experimental setup (300-unit networks with only 10 motifs) along with the limited scale raises doubts about the broader impact and generalization of the approach.\n\n2. Comparison and Baselines:  \nA recurring concern from multiple reviewers is that the paper does not convincingly rule out simpler alternatives – such as hard resets or other learned initialization strategies – through controlled experiments. Even though the authors have provided additional evidence and clarifications (with new figures and further explanation), the central objection remains that it is unclear why the proposed biologically inspired preparatory module is necessary and superior. The experimental comparisons to alternatives and expanded baselines are still not fully convincing given the simplicity (and toy scale) of the task.\n\n3. Task Formulation and Experimental Scale:  \nReviewer 3 and Reviewer 5 pointed out that the task is formulated as a novel synthetic motor pattern generation problem. However, its utility for real-world or larger-scale machine learning problems is unclear. The use of a small network (300 units) and a limited number of motifs means that even if the proposed method works in this controlled setting, its relevance to more challenging and realistic scenarios (e.g., tasks with orders of magnitude more elements than 10 motifs) is uncertain.\n\n4. Clarity, Presentation, and Writing:  \nWhile there are positive comments on the details provided (as noted by Reviewer 4 regarding reproducibility), several reviewers (notably Reviewer 5) highlighted significant issues with clarity and organization. The manuscript suffers from non-intuitive presentation of the task and experimental details, with formal specifications scattered between the main text and appendices. Even with a revised draft and responses addressing some concerns, the writing and exposition remain less accessible and well‐structured than expected for a top‐tier conference paper.\n\n5. Overall Evaluation:  \nEven though the authors have attempted to address many of the reviewers' concerns (by adding clarifications, additional experiments, and improved explanations) the combined issues – marginal novelty/significance, limited empirical evidence on a toy setting, and deficiencies in the exposition – lead to the conclusion that the paper does not yet meet the high standards expected at a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning followed by my final decision.\n\nDetailed Reasoning:\n1. Contribution and Scope: The paper addresses the timely and important problem of data leakage attacks in federated learning specifically for tabular data—a setting that had not been explored as thoroughly as image or text domains. In this sense, the paper contributes by identifying the unique challenges arising from mixed discrete-continuous features and proposing a reconstruction attack (TabLeak) that incorporates a softmax relaxation, pooled ensembling, and an entropy-based heuristic.\n\n2. Technical Novelty: Although the problem formulation is important, two reviewers (Reviewer 1 and Reviewer 2) expressed significant concern that the core components (softmax relaxations, ensemble pooling) are standard techniques and not sufficiently innovative compared to related literature. While Reviewer 3 acknowledges some novelty in applying these ideas to the tabular setting, the overall level of technical innovation appears marginal for a top-tier conference where substantial novel insight or methodology is expected.\n\n3. Clarity and Presentation: Reviewer 1 raised issues regarding poor writing, unclear sentences, and inconsistent grammar. Reviewer 2 did not mention clarity issues while Reviewer 3 found the paper well written. The author’s extensive rebuttal has addressed many of the concerns raised (e.g., clarifying the technical claims and providing added experimental evidence). However, weaknesses in presentation and notational consistency still detract from the paper’s overall quality at the level expected in top conferences.\n\n4. Experimental Evaluation and Reproducibility: The work includes thorough experiments, ablation studies (with additional results added in appendices), and the authors explain the evaluation protocols clearly. The reproducibility is a strength noted by multiple reviewers. Yet, while the engineering and experimental parts are solid, for acceptance in a top-tier venue the work is expected to combine high-quality experiments with innovative conceptual advances. \n\n5. Overall Assessment: Although the paper studies an interesting and emerging problem space and offers useful insights regarding vulnerabilities in federated learning on tabular data, the proposed method is mostly an adaptation of known techniques rather than a significant breakthrough. Two out of three reviewer assessments focus on the limited novelty and marginal impact of the contributions, and the concerns regarding presentation clarity have not been entirely resolved. Given the standards of top-tier conferences—which demand both significant conceptual novelty and impeccable presentation—the current version falls short.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Detailed Reasoning:\nThe paper “Scrunch: Preventing sensitive property inference through privacy‐preserving representation learning” aims to provide a method to preserve privacy with an encoder–classifier split approach suitable for MLaaS scenarios. However, the reviews indicate significant issues that undermine both its novelty and its technical soundness.\n\n1. Novelty & Technical Contributions:\n• The central idea of splitting a network into a client-side encoder and server-side classifier is not new; it closely mirrors the well-known split learning and existing methods in privacy-preserving machine learning.\n• Although the paper introduces an additional loss component (center loss) to control information flow, reviewers agree that this addition is incremental and does not offer substantial innovative protection mechanisms.\n• Key reviewers noted that the proposed privacy-preserving guarantees are questionable because the server has access to critical information (loss values, class centers, complete model parameters) that enable potential reconstruction attacks and model inversion attacks.\n  \n2. Security Threat Model:\n• The paper fails to provide a clear and rigorous security model. As highlighted by reviewers, if the server is already “curious” and has access to the shared encoder, it is in an advantageous position to simulate the client-side operations, nullifying the intended privacy benefits.\n• The approach only superficially addresses inference privacy during the prediction phase, while neglecting vulnerabilities during training.\n  \n3. Experimental Evaluation:\n• The evaluation is simplistic and limited to only two datasets (MNIST and CelebA) using relatively shallow models (LeNet-5 and VGG-16). This does little to demonstrate robustness in real-world scenarios or against stronger baselines.\n• Multiple reviewers commented that additional comparisons (e.g., with fully homomorphic encryption based approaches or with more advanced deep architectures) and richer experiments are required to substantiate the contribution.\n  \n4. Clarity and Reproducibility:\n• While the overall structure is understandable, several ambiguous and erroneous statements hurt the paper’s clarity (e.g., mixed messaging regarding inter- vs. intra-class distances).\n• Important details such as optimizer settings, training epochs, and finer architectural specifics are missing, making reproducibility uncertain for a work that leans heavily on empirical validation.\n\nGiven these significant shortcomings—the marginal innovation, flawed threat model, limited experiments, and potential for privacy vulnerability—the paper does not meet the rigorous standards required for top-tier conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Summary of contributions and context:\n • The paper extends previous work—most notably PP16—by looking not only at convergence of no‐regret learning dynamics but also at their “average-case” performance in terms of equilibria quality. In particular, it studies a family of q‐replicator dynamics that includes classic replicator, gradient descent, and log-barrier dynamics as special cases.\n • The paper provides both theoretical convergence guarantees (point-wise convergence to Nash equilibria) and an analysis of the “average price of anarchy” in certain game classes. For 2×2 symmetric coordination games, the results yield constant performance guarantees (e.g., the 0‐replicator dynamics yielding better social welfare than the 1‐replicator dynamics under a certain condition).\n\n2. Strengths:\n • The work tackles the interesting issue that convergence alone does not guarantee good equilibria – an aspect sometimes overlooked in learning-in-games literature.\n • It proposes a unified framework both conceptually and experimentally to compare different no-regret learning dynamics.\n • The technical development is nontrivial, with use of tools from Lyapunov functions, invariant/unstable manifolds, and an interplay between dynamics and equilibrium selection. This breadth of techniques is a strong point.\n • The response to reviewers has improved the presentation (e.g., clarifying the open questions, cleaning up the formatting, and better situating the work with respect to PP16).\n\n3. Weaknesses and concerns:\n • The results, especially the average-case performance analysis, are established fully only for 2×2 symmetric games, which is a restricted setting. The extension (e.g., to general q) remains partially resolved or is provided only as numerical/empirical evidence.\n • Some of the contributions (e.g., point-wise convergence, ideas behind invariant functions) appear incremental relative to the PP16 paper. The authors now explain the differences, but the novelty remains modest in parts.\n • There were issues with notation clarity and formatting, though these have largely been addressed in the revised version.\n\n4. Overall evaluation relative to top-tier standards:\n • While there is a degree of incremental contribution over prior work, the paper opens an important line of research by shifting focus from convergence per se to average-case quality analysis.\n • The technical content is sound, and the experimental results support the presented theory in the 2×2 setting.\n • Given the intellectual challenge of analyzing the regions of attraction and the quality of equilibrium outcomes, the paper represents a step forward and is likely to stimulate further work in this direction.\n\nIn light of these considerations—and noting that reviewers have largely converged favorably once the issues were addressed—I recommend acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is a summary of my detailed reasoning:\n\n1. The paper addresses an important gap in neural-symbolic AI by extending deep probabilistic logic programming to handle continuous and mixed discrete–continuous domains. This is not merely a trivial extension of DeepProbLog but involves rethinking the semantics (linking to weighted model integration) and making WMI differentiable. The authors support their contributions with formal propositions (e.g., Proposition 4.1 and 4.2) and proofs that clarify how differentiable WMI is achieved—a point that is both theoretically interesting and practically relevant.\n\n2. The submitted work bridges the gap between exact symbolic inference for discrete variables (as in DeepProbLog) and sampling-based approximations for continuous variables while maintaining overall consistency. The paper’s treatment of neural distributional facts and probabilistic comparison formulas represents non-trivial innovation. In addition, the authors go beyond known techniques by showing that under “infinite coolness limit” one obtains unbiased derivative estimates. This theoretical contribution is novel and goes beyond the immediate context of the programming language.\n\n3. While there are concerns regarding the experimental evaluation—the applications are mostly toy scenarios and the scalability/efficiency trade-offs have not been fully explored—the authors’ experiments (including MNIST subtraction, a neural-symbolic VAE, and neural hybrid Bayesian networks) do illustrate the potential benefits in data efficiency and generalization. There is also clear acknowledgment (and discussion) of these limitations, indicating directions for future work.\n\n4. Several reviewers criticized the work for being somewhat incremental and relying on known components. However, the detailed author response clarifies that the contributions are not mere incremental modifications but represent a substantial extension and theoretical clarification of differentiable WMI. The detailed responses also alleviate many of the clarity and semantic precision issues raised in the reviews.\n\n5. In summary, the paper makes significant theoretical advances in the area of neural probabilistic logic programming and combines several challenging ideas into a coherent framework that extends the state-of-the-art. Although there are some shortcomings with respect to expansive empirical validation and efficiency analysis, these do not outweigh the importance of the theoretical contributions and the novel integration strategy.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my reasoning:\n\n1. The paper proposes a “weighted mean trick” that essentially reweights the empirical risk in order to penalize not just the mean loss but also higher-order moments (variance, skewness, and kurtosis). On the surface, the idea of linking weighted empirical risk minimization with such moment-based regularization is interesting. However, several concerns substantially weaken the contributions.\n\n2. Several reviewers pointed out that many of the technical results, like Theorem 1, Lemma 2, etc., follow from rather elementary algebraic manipulations and do not offer substantial novelty beyond previously known approaches (e.g., Duchi & Namkoong (2019) and Li et al. (2021)). In essence, the “trick” does not seem to provide deeper theoretical insights or significant algorithmic benefits over existing robust loss strategies.\n\n3. A major technical issue raised by multiple reviewers is regarding the proposed algorithm: when reweighting the loss the gradient descent update is shown to only use the term E[w∇ℓ] while neglecting the term that would naturally arise from the gradient of w(θ). This omission brings into question whether the algorithm is actually descending the correct objective. Several reviewers noted that the connection between the claimed objective (the expectation of wℓ) and its minimization via gradient descent was unclear or incorrectly implemented. The authors’ response acknowledged the concern but essentially deferred a more in‐depth justification.\n\n4. There is also concern on the clarity and internal coherence of the work. The use of both positive and negative regularization factors (λ) leading to seemingly contradictory effects, the switching discussion on variance penalization versus higher-order moment penalization, and confusion about notations (for example, the dependence on θ) make the presentation muddled. Even though the authors promised clarifications in the camera-ready version, the deficiencies in presentation and coherence are significant for a top-tier conference paper.\n\n5. Although one reviewer (Reviewer 7) was more positive, highlighting that the observation might lead to interesting applications, the majority of reviewers (and many detailed concerns across several reviews) criticized the theoretical contribution as marginal, the algorithm implementation as inadequately justified, and overall the work as not being sufficiently novel or robustly developed.\n\n6. Given that a top-tier conference demands both solid theoretical contributions and clear algorithmic advances with compelling experimental evidence, the paper—in its current form and based on the critiques provided—falls short on multiple fronts. The reviewers and the detailed responses indicate that the core issues (technical correctness of the algorithm, clarity in derivations, and novelty beyond basic manipulations) have not been satisfactorily resolved.\n\nTherefore, my final decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper proposes a new target propagation variant through regularized inversion for RNN training. While the method is relatively straightforward to implement in an autograd framework, its primary novelty is incremental compared to existing variants. Several reviewers noted that the algorithm loses many of the attractive features of classic target propagation (e.g., avoiding weight transport), which is one of the reasons why TP has been considered an appealing alternative to backpropagation in the literature.\n\n2. Reviewer 1 highlighted several conceptual weaknesses. In particular, the proposed technique requires weight transport, is computationally costlier than standard backpropagation (even if the cost gap diminishes with longer sequences), and the paper does not offer strong theoretical guarantees or descent properties that would justify its use over gradients. Several key questions (about multiple minimizers, the sensitivity to the regularization parameter, and differences with previous methods) were raised without satisfactory answers.\n\n3. Reviewer 2 and Reviewer 3 acknowledged that the formulation provides a natural interpretation and is implementable. However, the experiments were not entirely convincing. There are concerns about the variability of training loss/accuracy and results that are less competitive (e.g., less than 20% accuracy on CIFAR10) without clear evidence that the proposed method consistently outperforms backpropagation. In short, the empirical validation lacks breadth and depth.\n\n4. Reviewer 4 also pointed to weaknesses in the theoretical analysis. Although some bound on the difference between the directions computed by TP and BP is provided, the argument is weak under many conditions, and the relationship to Gauss-Newton methods is only superficially explored. Furthermore, the experimental comparisons are limited and do not convincingly justify the practical benefits of the method.\n\n5. In summary, while the paper examines an interesting alternative to backpropagation, the contribution is only marginally novel. The lack of strong theoretical guarantees along with limited and sometimes unclear empirical performance means the paper does not yet meet the standards expected for a top-tier conference.\n\nBased on the above, my final decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper proposes an elegant way to train independent subnetworks within a single network using a multi-input multi-output (MIMO) configuration. The experimental results on standard datasets (CIFAR-10, CIFAR-100, ImageNet, and their OOD variants) are strong and indicate improvements in accuracy, negative log-likelihood, and calibration error compared with conventional methods. The key benefit—that an ensemble of predictions may be obtained in a single forward pass with only a minimal (≈1%) increase in parameters—is clearly compelling, especially in scenarios where inference speed is critical.\n\nSeveral strengths were noted by the reviewers. The experimental analysis is thorough and demonstrates that the subnetworks indeed converge toward different local minima, offering functional diversity comparable to that of independently trained networks. In addition, the paper provides practical measurements (e.g., FLOPs, latency) that support the claim that the increased computational cost is negligible. The clarity and presentation of the method, with well-designed figures and detailed experimental results, also contributed to the positive evaluations.\n\nSome concerns included the need for more discussion on theoretical underpinnings regarding the emergence of independence, further comparisons with closely related multi-branch methods (such as BatchEnsemble), and additional experiments on a more diverse set of datasets. While these are valid and important aspects, the authors have responsive revisions. They clarified where the extra parameter count comes from, expanded on the relationship with BatchEnsemble, and provided more insights into the independence mechanism. Although more theoretical analysis could strengthen the paper further, the practical contribution and empirical evidence already constitute a solid advancement.\n\nIn overall evaluation, while there are some minor originality and analysis issues, the strengths in empirical performance and efficiency, combined with clear presentation and responsiveness to the reviewers’ concerns, make the paper a valuable contribution to the field. It meets the standards expected for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Overall, this paper makes a solid contribution by presenting a novel framework for answering complex, EPFO queries over incomplete knowledge graphs. The method is both elegant and practical, largely because it avoids the need to train on large numbers of complex queries and instead leverages a pre-trained neural link predictor to score atomic queries. The approach, using continuous relaxations through t-norms and t-conorms combined with either gradient-based or combinatorial optimization, not only outperforms the state-of-the-art in terms of ranking accuracy (e.g., Hits@3 improvements of up to 40%) but also offers desirable explainability by deconstructing the query into interpretable sub-steps.\n\nThe review comments from multiple reviewers were generally very positive, noting strengths in clarity, contribution, and performance. They did raise questions regarding clarity around certain aspects such as the motivation behind EPFO queries, specific examples, and timing complexity. However, these concerns were addressed satisfactorily in the paper and through the detailed author responses and additional experiments, including ablations with alternative neural link predictors.\n\nThe concerns about runtime, scalability for longer query chains, and the role of the underlying neural link predictor are acknowledged, but they are either mitigated by the experimental datasets (where queries are of modest depth) or by planned future extensions. Further, the paper’s results with lower embedding sizes also provide evidence of its robustness and data efficiency compared to baselines like GQE and Q2B.\n\nGiven the thorough experimental validation, theoretical clarity, interpretability, and significant performance improvements over prior methods, the paper meets the high standards expected at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Detailed Reasoning:\nThe paper proposes a novel framework that links deep learning decoupled neural interfaces (DNI) with the biological function of the cerebellum. It argues that the cerebellum may serve as a decoupling machine for efficient credit assignment by providing synthetic gradients that alleviate the locking problem (whether in feedforward or recurrent settings). The authors test the cortico-cerebellar-DNI (CC-DNI) and its sparse variant on a range of tasks, including sensorimotor tasks (e.g., target reaching, sequential MNIST) and a cognitive task (caption generation). They also show that models without the cerebellum-DNI component mimic deficits observed in cerebellar patients. In addition, the paper generates neuroscientifically testable predictions and demonstrates that imposing cerebellar-inspired sparsity can improve learning efficiency via decorrelation.\n\nThe reviewers provided mixed feedback:\n• Reviewer 1 was generally positive, praising the intriguing hypothesis and interdisciplinary importance, although suggesting improvements in clarity and explicit comparisons between DNI and CC-DNI.\n• Reviewer 2 acknowledged the clear presentation and thorough experiments but raised concerns about the novelty, arguing that the juxtaposition of two well-established domains did not by itself yield new insight.\n• Reviewer 3 appreciated the novelty of the link, while questioning whether the model’s predictions matched experimental data and whether the biological details (e.g., gradient transmission, correspondence with cortical/cerebellar cell counts) were sufficiently addressed.\n\nThe authors’ revision and response have substantially ameliorated these concerns. They introduced new figures (Figs. 5–7) and a new section elaborating on specific predictions that both strengthen the neuroscience connection and clarify the advantages of their cerebellum-inspired architecture over standard DNIs. Their discussion now explicitly outlines experimental predictions, comparisons to biological findings (including ablation studies and correlation analyses consistent with existing experimental literature), and clarifies the role of bootstrapping and connectivity differences—enhancing both the conceptual clarity and practical implications for neuroscience and deep learning communities.\n\nGiven the interdisciplinary novelty, the substantial revisions which address earlier criticisms, and the potential of this work to stimulate further research bridging biological and artificial learning systems, the paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. The paper tackles an interesting and important problem—the “greedy” behavior in multimodal deep networks where one modality dominates—and proposes a training algorithm (balanced multimodal learning) along with new metrics (conditional utilization rate and its surrogate, conditional learning speed). The intuition behind the problem is appealing and the empirical observations are consistently reported across several tasks.\n\n2. However, many concerns remain:\n • Several reviewers noted that the main claims are supported mostly by empirical observations on datasets that are either contrived (i.e., Colored-and-gray-MNIST) or yield changes in accuracy that are marginal (e.g., in ModelNet40 and NVGesture). The gain on generalization is not always statistically significant or fully compelling.\n • There are significant concerns about the clarity and rigor of the method’s definitions. For example, the decomposition of the contributions in multimodal branches and the definition of conditional learning speed are problematic; this raises doubts about whether the proposed surrogate really captures the intended phenomenon.\n • The theoretical foundation is limited. While empirical observations help, many reviewers asked for deeper theoretical explanation or formal analysis to support why the greedy learner hypothesis holds, and why the balanced training procedure should work. The authors’ response acknowledges that such a theory is forthcoming but doesn’t deliver it in the current paper.\n • Reproducibility issues and clarity concerns persist. Several reviewers asked for improved explanations (especially regarding the re-balancing step and extraction of uni-modal outputs), and while the authors have provided clarifications in the response and appendix, these sections remain insufficiently clear for a top-tier conference publication.\n • Many experiments are limited to visual modalities. It would strengthen the paper to include experiments on truly natural multimodal data (such as audiovisual tasks) to demonstrate generalizability beyond “multiple visual modalities.”\n • Finally, comparisons to other state-of-the-art methods for mitigating modality bias (such as Rubi and others from the related literature) remain inadequate. The revised experiments partially address this, but questions remain regarding consistency and whether the experimental settings are correctly reflected.\n\n3. Overall, while the paper addresses an important problem and presents a promising idea, the experimental evidence is mixed and the explanations of the method (theoretical, methodological, and algorithmic) are still not solid enough to overcome the concerns raised. Given the standards of a top-tier conference, these unresolved issues make it difficult to recommend acceptance at present.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed analysis and final decision regarding the paper “PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning”.\n\n1. Overall Assessment:\n• The idea of leveraging demonstrations within a meta-RL framework is promising, particularly given the challenges of sparse rewards. However, the paper builds on prior work (notably PEARL and meta-IL methods) and introduces several new loss components and design choices that—while potentially useful—are not clearly motivated or decomposed from the overall performance.\n• Across reviews, a recurrent concern is that the paper’s presentation is overly complex and lacks clarity. There are numerous notational inconsistencies and ambiguities about which loss components are used where, what role they play, and how they compare to simpler alternatives.\n\n2. Methodological Concerns:\n• Many reviewers note that the method combines several loss terms (up to six) and architectural choices without clear ablations or explanations to isolate which parts drive the performance improvements. It is unclear whether a simpler method could perform comparably.\n• The presentation of the variational objective (including the definitions of information loss, auxiliary loss, behavior cloning losses, etc.) is muddled and inconsistent. There are also questions on derivation of key equations (e.g., in Section 3.2, the unspecified form of the likelihood functions, the KL term missing, and numerous variables with overlapping names).\n• The paper seems to mix up the overall problem formulation with implementation design choices. For example, the latent variable “z” is used both as part of the underlying state/task description and as part of the representation learned by the model, which creates confusion.\n\n3. Empirical and Experimental Analysis:\n• Although initial empirical results are encouraging, reviewers agree that the experiments lack sufficient baselines. For instance, there is a clear need to compare against a version of PEARL augmented with demonstrations (including simple extensions like preloading demonstration data), and to report performance against other prior meta-IL/meta-RL methods.\n• There is also a need for statistical validation (multiple seeds, error bars/confidence intervals) and clear ablation studies that detail which components contribute to the overall performance.\n• The clarity in the experimental setup is lacking. Details regarding how test-time adaptation is performed (e.g., how many rollouts versus demonstration samples) remain ambiguous.\n• Some assumptions (e.g., the reliance on an expert distribution conditioned on the learned latent z) are rather strong and further diminish the practicality of the method for real-world settings.\n\n4. Clarity and Presentation:\n• Nearly all four reviewers complain that key concepts, mathematical formulations, and loss function details are opaque. The notation is inconsistent and does not follow standard conventions (e.g., reuse of “k” for different purposes, mixing up trajectories from policy rollouts with demonstration trajectories).\n• There is also criticism regarding missing or poorly explained motivation for several unconventional design decisions (such as auxiliary losses and mutual information terms) compared to established methods.\n• The paper’s introduction and problem formulation do not clearly differentiate between design decisions and inherent limitations of the problem space (such as the POMDP formulation versus the fixed latent tasks).\n\n5. Final Decision:\nBecause the paper suffers from major clarity issues, inconsistent notation, an overcomplex formulation with too many loss terms, and insufficient experimental ablations and baselines, it does not yet meet the standards of a top-tier conference. Although the idea is promising and relevant, the lack of clarity and rigorous experimental support in its current state precludes acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\n1. Novelty and Dataset Contribution:\n• The paper’s idea of pre-training a transformer-based layout representation model – CanvasEmb – using a large slide dataset is interesting and potentially impactful. The effort to compile a dataset with >1M slides is a positive contribution.\n• However, while the dataset contribution is promising, its specifics (e.g., statistics, parsing details) are insufficiently described, which raises questions about reproducibility and overall dataset quality.\n\n2. Methodology:\n• Using a transformer framework in the context of layout representation is a reasonable idea given recent successes in pre-training for NLP.\n• That said, the paper fails to provide detailed descriptions of the model architecture. Reviewers have noted the omission of clear figures, mathematical formulations, or sufficient discussion of the attention mechanisms. This lack of technical details weakens the paper for a top-tier venue.\n\n3. Evaluation:\n• The experimental evaluation is consistently criticized by multiple reviewers. The evaluation is limited to a couple of downstream tasks (element role labeling and a simplified “image captioning” task) which do not convincingly demonstrate the necessity or strength of the proposed method. \n• More comprehensive experiments and rigorous comparisons with relevant baselines such as existing transformer variants, tree-based generative models (e.g., Cao et al., Li et al., READ), and Graph Neural Networks on layout tasks are missing.\n• Several reviewers suggest that the use of a simple decision-tree baseline does not justify the heavy machinery of a transformer-based model and pre-training.\n• While the authors promise additional experiments and more systematic evaluation (including visualization and more detailed human evaluation) in future versions, the current submission does not meet the standard of a top-tier conference.\n\n4. Clarity and Related Work:\n• Overall clarity is reasonable; however, parts of the paper (such as the “Related Work” section) are poorly organized and do not clearly differentiate the proposed approach from previous works.\n• The comparisons with prior art (particularly with content-aware generative modeling from SIGGRAPH 2019 and other tree-based methods) are not sufficiently addressed.\n\n5. Conclusion:\nWhile the paper is moving in an interesting direction with a large dataset and the use of pre-training for layout representation, the current version suffers from significant gaps in technical detail, evaluation rigor, and comparative analysis with strong baselines. Given the standards of a top-tier conference, these issues are critical. The paper presents potential ideas but requires more substantial experimental validation, better clarity on dataset details, and deeper architectural and baseline comparisons.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. Contribution and novelty: Although several reviewers noted that the basic idea of using Fourier-domain operations (as in Rippel et al. 2015) is not entirely new in the ML community, the paper’s focus on applying and adapting these techniques to optimize highly non-local point spread functions (PSFs) in 3D snapshot microscopy is significant. The work extends the idea to an end-to-end simulation-based pipeline for programmable computational microscopy. In doing so the paper provides a tailored solution for an important application domain (e.g., high-speed volumetric imaging in neuroscience) that was previously out of reach using state-of-the-art UNets. In addition, the paper demonstrates applicability in a different scenario (lensless imaging reconstruction), which strengthens the overall contribution.\n\n2. Technical execution: The paper tackles two major challenges: (a) the efficient simulation of large-scale optics (with >10^6 phase-modulating pixels) using a distributed multi-GPU differentiable simulator, and (b) the design of a Fourier convolutional network that can overcome the locality bias of conventional CNNs/UNets. The experiments, though primarily synthetic for the 3D microscopy task, are thorough and include a real-data evaluation for lensless imaging. The experiments are well designed and the supplementary results (e.g., comparisons in Table 3 and detailed analyses) support the claims. Although there were requests for additional details (e.g., resource requirements, speed comparisons, and clarifications on the UNet weaknesses) these concerns were addressed in detail by the authors in their response.\n\n3. Reviewer opinions and author response: The majority of the reviewers have improved and shifted toward a view that the paper is acceptable—with several reviewers rating it marginally above the acceptance threshold. Even though one reviewer issued a “weak reject” primarily because of evaluation limitations (limited real data for 3D microscopy and unclear UNet comparisons), the authors’ responses provided convincing rationale (e.g., the unique challenges in real 3D snapshot microscopy and how simulation is the de facto testbed in this domain). Furthermore, the discussion on the relative parameter counts and computational considerations (including speed differences in Table 3) has been clarified. \n\n4. Venue fit: Despite concerns raised about its “application-focused” nature, the authors rightly point out that ICLR welcomes work that bridges machine learning with applications in neuroscience, computational biology, and vision. The demonstrated interplay of differentiable physics, deep simulation-based optimization, and an innovative network architecture makes the work interesting to a broad ML audience.\n\nConsidering all of these factors and the fact that the concerns raised have been adequately addressed by the authors, it is my conclusion that the paper represents a compelling contribution at the intersection of ML and computational imaging.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Below is my detailed reasoning:\n\n1. The paper introduces a full theoretical analysis of the Feedback Alignment (FA) algorithm in the setting of deep linear networks. It provides convergence guarantees (with rates) under both continuous-time and discrete-time dynamics and further studies subtle implicit (anti-)regularization phenomena related to initialization schemes. This contribution is valuable given that FA has been used widely despite a lack of rigorous convergence analysis.\n\n2. Several reviewers agree that the technical correctness is strong. The proofs are generally well supported, and the authors have provided detailed responses addressing the technicalities of the discrete update, the choice of initialization, and the differences versus standard GD. This gives confidence in the theoretical contributions.\n\n3. The main concerns among some reviewers center on the strong assumptions required—particularly the use of spectral (diagonalizable) assumptions and specific initialization schemes. One reviewer noted that while these assumptions may oversimplify the complexity of general deep learning problems, an analysis in the linear setting is a standard (and acceptable) simplification in theoretical work. The authors clarify these points in their revision and provide evidence (via additional experiments and discussion) that even when the spectral alignment assumption is relaxed, the phenomena (such as incremental learning) are still observed.\n\n4. There is a discussion in the reviews regarding the significance and novelty compared to existing work on GD implicit regularization. Although a couple of reviewers rate the technical and empirical novelty as marginal, at least one reviewer (Reviewer 3) praises the novel insights (particularly on implicit bias and anti-regularization) and the clarity of the presentation. In a theory paper, a “first rigorous analysis” of an algorithm that has practical relevance is itself a valuable contribution.\n\n5. The theoretical results, while limited to deep linear networks and requiring certain initialization conditions, are an important stepping stone. Linear networks have a rich history as a testbed for understanding more complex neural network dynamics, and many prestigious conferences accept such theoretical insights in their original form.\n\n6. The authors’ responses adequately address concerns regarding initialization and extension to non-spectral settings – providing additional empirical evidence and clarifications. Also, they point out that further work is planned to extend these ideas to non-linear networks.\n\nConsidering these points, the strengths in technical clarity, correctness, and the advance in understanding FA (an algorithm of growing interest) make a convincing case for acceptance. The limitations (e.g., focusing on linear networks and special initialization) are typical in foundational theoretical works and are clearly acknowledged by the authors. As such, the paper meets the standards of a top-tier conference both in novelty (by rigorously studying an understudied algorithm) and in technical contribution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my detailed reasoning:\n\nStrengths:\n• The paper introduces an interesting idea by extending boosting techniques from supervised and bandit settings to full reinforcement learning. In particular, the main theoretical contribution—a boosting algorithm whose sample complexity does not explicitly depend on the number of states—is intriguing.\n• The use of a non‐convex variant of the Frank–Wolfe method to overcome the nonconvexity of the value-function over policy space is a neat technical contribution compared to prior work (e.g., Hazan and Singh 2021).\n• Some reviewers (e.g., Reviewer 6) see value in the theoretical formulation and view the work as one of the first steps toward rigorous boosting methods in RL.\n\nWeaknesses:\n• There is a pervasive clarity and presentation issue. Multiple reviewers noted that the paper is extremely dense and notation-heavy. Many symbols are not adequately defined or motivated, making it difficult for readers to follow the technical narrative.\n• Several core definitions, algorithmic steps, and assumptions (e.g., regarding the weak learner, the extension operator, policy completeness, and key notations like the “shrub” and “policy tree”) are not well explained, leaving doubts about the practical relevance of the algorithm.\n• The novelty is felt by some reviewers to be incremental. In particular, much of the methodology shares similarities with prior work, and the paper does not make a strong case differentiating the contribution from earlier studies on boosted contextual bandits or boosting methods in RL.\n• The lack of empirical illustration (even on a simple problem such as a grid world) is also a notable omission for a conference of this level. At least some numerical experiment for intuition on how the boosting algorithm performs would have made the paper more convincing to a broader audience.\n• The author response indicates that much of the notation and explanation would be “clarified” in the final version. However, for publication at a top-tier conference, the clarity and presentation need to be solid in the version under review.\n\nOverall Assessment:\nWhile the theoretical idea and sample complexity results are interesting and could be of independent interest to the RL and optimization communities, the many clarity issues, heavy notation, and somewhat incremental novelty (relative to existing works like Hazan and Singh 2021) are significant hurdles. The paper would need substantial rewriting and revision—particularly in terms of explanatory material, a notation list, and even some motivating experiments—to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Below is my reasoning:\n\n1. Contribution and Novelty:\n • The paper’s core idea is to bridge recent meta‐learning theory with practice by introducing two “theory‐inspired” regularizers. Although the goal of verifying whether theoretical conditions can be “enforced” to improve few‐shot learning is interesting, many reviewers note that the individual regularization terms are similar to known methods (e.g. weight decay or spectral normalization). Even though the authors argue that enforcing both assumptions simultaneously is novel, the reviewers remain unconvinced that this joint formulation marks a significant conceptual or technical advance. This raises concerns about the overall novelty and conceptual contribution expected at a top-tier venue.\n\n2. Experimental Evaluation and Empirical Impact:\n • The experimental results show some improvement over baseline meta-learning methods when the proposed regularizations are added. However, the empirical gains are often around the level of the standard deviation and sometimes not clearly substantiated over simple implementation “tricks” or known regularizers.\n • Reviewers also commented that the evaluation lacks comparisons with a broader set of recent competitors and that some details (such as a careful synthetic experiment designed to “control” the assumed properties) are missing.\n • Although the authors have added extra experiments (including hyperparameter tuning) in the supplementary material and compared with additional baselines such as Meta-Curvature, the improvement is seen as marginal by some and does not dramatically alter the state-of-the-art performance.\n\n3. Theoretical-Practice Connection:\n • The paper’s stated goal is to investigate whether the assumptions behind existing meta-learning bounds are useful in practice. Several reviewers appreciate the attempt at this “theory-to-practice” bridge. \n • However, there remain concerns that the theoretical assumptions, defined on optimal predictors and task distributions, do not align precisely with the few-shot learning practice. Although the authors provide clarifying examples and additional discussion in their revision, the connection remains somewhat loose. \n • In a top-tier conference, one expects tighter integration of theory and practice with either significant experimental breakthroughs or novel theoretical insights driving them.\n\n4. Overall Evaluation:\n • While the paper is clearly written and the authors have provided extensive responses to the reviewers—including adding clarifications and extra experiments—the overall contribution is seen as incremental rather than groundbreaking. \n • Some reviewers still doubt that the proposed regularization is fundamentally different from techniques already known, and that its practical benefits are robust enough to warrant publication at a top-tier venue.\n • Given these issues, particularly the novelty concerns and the modest empirical gains, the reviewers’ overall sentiment tends toward rejection.\n\nFinal Decision: Reject"
    }
  ]
}