{
  "summary": {
    "model": "qwq",
    "total_papers": 100,
    "correct_predictions": 79,
    "accuracy": 79.0,
    "confusion_matrix": [
      [
        56,
        20
      ],
      [
        1,
        23
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 73.68421052631578,
        "precision": 98.24561403508771
      },
      "Accept": {
        "total": 24,
        "recall": 95.83333333333334,
        "precision": 53.48837209302325
      }
    }
  },
  "results": [
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about generating singing voice without scores or lyrics, which is a novel approach. The reviewers had mixed feedback. Let me go through each point.\n\nFirst, the main contributions seem to be introducing three new tasks (free, accompanied, solo singer) and developing a pipeline with GANs. The reviewers noted that the evaluation was weak initially but the authors added baselines like Sinsy and Synthesizer V, plus more metrics. That's a big improvement.\n\nReviewer 1 was concerned about metrics not assessing singing quality properly. The authors addressed this by refining vocalness and adding MOS studies. They also clarified their dataset and training details. The audio samples were made clearer, which helps.\n\nReviewer 2 pointed out lack of motivation and comparison with baselines. The authors added baselines and explained why their tasks are meaningful. They also clarified dataset composition and training splits. The mention of MelNet was addressed by citing it and discussing differences.\n\nReviewer 3 thought evaluation was weak but the authors added more metrics and baselines. They also improved literature review on neural singing synthesis. The technical issues like the 24 keys typo were fixed, and vocalness metric was improved using JDC.\n\nReviewer 4 was harsh, saying the paper tried too much and lacked justification. The authors added ablation studies on GAN losses, clarified architecture choices, and moved some details to appendices. They also addressed the \"inner idea\" vagueness in solo singer.\n\nReviewer 5 wanted MIDI baseline comparisons. The authors included Sinsy and Synthesizer V, which are waveform-based but the reviewer might still prefer MIDI. However, the authors argued their approach is different and suitable for different use cases.\n\nThe authors did a lot of revisions: added baselines, clarified dataset, fixed technical issues, improved metrics, and addressed architectural concerns. The main criticisms about evaluation and baselines were mitigated. The problem is novel and the approach is technically sound despite some limitations.\n\nHowever, some points remain. The evaluation still relies on subjective MOS and the new metrics might not fully capture singing quality. But top conferences accept pioneering work even if not perfect, especially with improvements. The authors showed progress in a new area, and the baselines now provide better context.\n\nFinal call: The paper addresses most reviewer concerns, introduces a new problem with plausible solutions, and the revisions strengthen it. Accepting it would encourage further work in this area.\n</think>\n\nThe paper proposes a novel approach to singing voice generation without relying on musical scores or lyrics, exploring three distinct scenarios: free, accompanied, and solo singer generation. While the initial submission faced significant criticism regarding evaluation, motivation, and technical rigor, the authors have made substantial revisions addressing many concerns. Here’s the breakdown:\n\n### Key Strengths:\n1. **Novel Problem Space**: The work introduces a new and challenging problem in singing voice generation, which is a clear contribution to the field. The three task settings (free, accompanied, solo) provide a structured framework for future research in this area.\n2. **Technical Effort**: The authors developed a comprehensive pipeline involving source separation, transcription, and adversarial networks. They also provided code and audio samples, enhancing reproducibility.\n3. **Revisions Addressing Criticisms**: \n   - **Baselines**: Added comparisons with Sinsy and Synthesizer V, providing context for their results.\n   - **Metrics**: Improved vocalness and pitch evaluation by incorporating JDC for more robust pitch detection and filtering non-vocal frames.\n   - **Dataset Clarity**: Clarified training/test splits, genre composition (Jazz-focused), and data preparation details.\n   - **Motivation**: Expanded the introduction to better justify the problem’s relevance and potential applications (e.g., improvisation, interactive systems).\n\n### Remaining Concerns:\n1. **Evaluation Limitations**: While the authors added baselines and refined metrics, the subjective MOS studies and objective metrics (e.g., vocalness, average pitch) still do not fully capture the complexity of singing quality (e.g., melody coherence, harmony). However, this is partially mitigated by the inclusion of baselines and the acknowledgment of subjectivity in the task.\n2. **Scope and Focus**: Reviewer 4 criticized the paper for addressing \"too many problems at once,\" but the revisions clarified the architecture choices (e.g., BEGAN adaptation) and provided ablation studies on GAN losses. The core contribution—the novel problem and initial solutions—remains coherent.\n3. **Domain Specificity**: The dataset is limited to Jazz with piano accompaniment, which restricts generalizability. However, this is a common trade-off in pioneering work, and the authors transparently addressed this in their response.\n\n### Conclusion:\nThe paper’s novel problem formulation and technical contributions outweigh its remaining limitations, especially after revisions. While the evaluation could be stronger, the work sets a baseline for future research in score/lyrics-free singing voice generation—a significant gap in the field. The authors have demonstrated sufficient rigor in addressing reviewer concerns, and the problem’s novelty justifies acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\". The abstract mentions they propose a generative model called ROOTS for unsupervised 3D scene decomposition. They base it on GQN but add object-oriented representations. The key claims are viewpoint invariance, hierarchical representation (3D global and 2D local), and improvements in disentanglement, compositionality, and generalization.\n\nLooking at the reviewer comments, there's a mix of positive and negative points. Reviewer 1 initially wanted to reject due to poor writing but after revisions, they're more satisfied, though still suggest further improvements. Reviewer 2 points out that some claims aren't supported by experiments (like Figure 2 not showing better generations), and the paper is unclear in places. Reviewer 3 criticizes lack of citations, comparisons, and ablation studies, noting that the contributions are minor over existing work. Reviewer 4 also mentions method complexity, unclear explanations, weak experiments, and insufficient related work.\n\nThe authors responded by addressing many of these points. They rewrote sections for clarity, fixed notation, added experiments on generalization and composition, and included more related work. They also clarified some technical aspects in their rebuttal.\n\nNow, considering the standards of a top-tier conference, the key factors are novelty, technical soundness, clarity, and experimental validation. \n\nFirst, the paper's main contribution is an object-oriented 3D scene representation model. The idea of extending GQN to include object-wise decomposition is novel, but Reviewer 3 and 4 point out that similar approaches exist (like MONet, Iodine) and that the 3D aspect might not be sufficiently novel. The authors addressed some of these by adding citations but might still be missing key references.\n\nTechnical soundness: The method seems complex, and the initial write-up had notation issues and unclear explanations. The authors revised the paper to improve clarity, which is good, but Reviewer 4 still found the method hard to follow even after revisions. The experiments were criticized for being weak—comparisons to GQN and other baselines were lacking, and there were no ablation studies. The authors added some experiments in the appendix, but Reviewer 4 noted that the results didn't clearly show superiority over GQN in some aspects, like generalization. The lack of ablation studies is a problem because it's hard to tell which components contribute to the results.\n\nClarity: The initial version had significant writing issues, but the authors claim to have rewritten much of the paper, fixed notation, and made explanations clearer. If the revisions addressed these issues sufficiently, the clarity might now be acceptable. However, Reviewer 4 still had concerns post-rebuttal about the complexity and clarity, suggesting that even after revisions, the paper might not be as clear as needed for a top conference.\n\nExperiments: The main experiments showed qualitative results and some quantitative metrics (MSE, NLL). However, the quantitative results didn't show a strong advantage over GQN. The added experiments in the appendix on generalization and composition are positive, but the reviewers wanted more rigorous comparisons, like precision-recall curves and ablations. The authors didn't address all these points fully, leaving some concerns about the empirical validation.\n\nRelated work: The authors added more references, but Reviewer 4 still felt the paper didn't adequately position itself against existing 3D reconstruction and object-centric works. This is a problem because top conferences require thorough related work sections to establish the contribution's novelty.\n\nConsidering all this, the paper has a novel idea but faces challenges in technical clarity, experimental rigor, and thorough related work. The revisions improved some aspects, but the remaining issues—like unclear method explanations, insufficient ablations, and weak experiments—might not meet the high bar of a top-tier conference. The authors' rebuttal addressed some points but didn't fully resolve the reviewers' concerns, especially regarding experiments and clarity.\n\nFinal Decision: Reject\n</think>\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. The main idea is using progressive learning to improve disentanglement in hierarchical representations with a VAE. The key contributions seem to be the progressive approach to building the model from high to low levels, and a new disentanglement metric called MIG-sup. They compare against existing methods on benchmarks and show quantitative and qualitative results.\n\nLooking at the reviewer comments. Reviewer 1 had some concerns about the first layer's training and suggested more experiments on mutual information. The authors addressed this by adding mutual information comparisons in the appendix, which seems to have satisfied the reviewer. They also did ablation studies on training strategies, which helped with stability. The reviewer even upgraded their score to a weak accept after the rebuttal.\n\nReviewer 2 was more critical. They questioned the paper's focus—whether it's on disentanglement or hierarchical representations—and pointed out that the experiments leaned more towards disentanglement. The authors clarified that the goal is disentanglement through progressive hierarchical learning, which might have addressed that. The reviewer also mentioned that the proposed metric MIG-sup was similar to existing work, and the metric requires ground truth factors. The authors responded by explaining differences with prior work and noting that most disentanglement metrics need ground truth. They also addressed the comparison with [3], showing their approach is different in how capacity is increased. The reviewer's main issues were about experiments not being convincing, but the rebuttal added more comparisons and ablation studies. However, the reviewer initially recommended rejection but after the rebuttal, did they change their mind? Wait, the after rebuttal section only shows Reviewer 1's change. Hmm, maybe Reviewer 2's final score isn't shown here? Wait, the user provided the after rebuttal comments only for Reviewer 1. The others might still have their original scores. Wait, the user's input includes \"After rebuttal\" section only for Reviewer 1. The other reviewers' final scores aren't provided. So I need to consider their initial comments and the author responses.\n\nReviewer 2's main points were about the paper's focus, the metric's limitations, and the comparison with [3]. The authors addressed these by clarifying the purpose, discussing the metric's relation to prior work, and explaining differences with [3]. They also added ablation studies and mutual information comparisons. The reviewer also wanted ablation studies on progressive learning vs. implementation strategies, which the authors partially addressed. The reconstruction error issue with beta was acknowledged but the authors argued their method still outperforms in most cases. The undefined v_k was fixed. However, the reviewer initially said \"not ready for publication,\" but after the rebuttal, maybe their stance softened? Not sure, but the user didn't provide updated scores for Reviewer 2.\n\nReviewer 3 was more positive. They found the idea fresh and experiments thorough, but wanted more quantitative info on information flow and better formalization. The authors added the mutual information experiments across layers and fixed the formalization. The reviewer's comments were constructive but overall positive, leading to a recommendation for acceptance.\n\nReviewer 4 thought the approach was well-motivated and results impressive but wanted more comparisons with VLAE and ablation on fade-out. The authors added more VLAE comparisons and ablation on strategies. The reviewer's main concern was addressed, so they likely support acceptance.\n\nNow, considering all this. The key issues were:\n\n1. Clarity of the paper's focus: The authors clarified that it's about disentanglement via progressive hierarchical learning, which seems resolved.\n2. Metric concerns: Addressed by explaining differences and acknowledging prior work.\n3. Comparison with [3]: The authors argued their method is different in capacity growth, which seems valid.\n4. Experimental robustness: Added ablation studies, mutual info comparisons, and fixed some issues.\n5. Qualitative results: The authors provided better comparisons with VLAE's figures, showing their method's improvements.\n\nThe main remaining concerns might be from Reviewer 2's initial rejection, but given the rebuttal addressed many points, maybe their score improved. The fact that Reviewer 1 upgraded to a weak accept, and others had positive feedback, suggests the paper has been strengthened. Top-tier conferences require strong novelty and solid experiments. The paper introduces a new approach combining progressive learning with VAEs for disentanglement, which is novel. The experiments, though initially lacking, were bolstered in the rebuttal. The metric's dependency on ground truth is standard in disentanglement work, so that's acceptable.\n\nPotential weaknesses: The MNIST results in Figure 6 were initially criticized as worse than VLAE, but the authors clarified the generation method difference and provided new figures. The information flow experiments show the method works as intended. The ablation studies on training strategies help.\n\nOverall, the paper addresses most reviewer concerns, shows promising results, and the contributions seem significant enough for a top conference. The rebuttal effectively mitigated several issues, so I think it should be accepted.\n</think>\n\nFinal Decision: Accept\n\nThe paper presents a novel approach to disentangled representation learning by integrating progressive learning with hierarchical VAE architectures. Key strengths include the innovative combination of progressive growth strategies with VAEs, the introduction of a new disentanglement metric (MIG-sup), and thorough experimental validation. While initial reviewer concerns highlighted issues such as unclear focus, metric limitations, and comparison gaps, the authors addressed these effectively in their rebuttal through additional experiments, ablation studies, and clarifications. Notably, they demonstrated improved disentanglement over baselines, clarified the relationship between hierarchical learning and disentanglement, and resolved ambiguities in their methodology. The revised experiments, including mutual information analyses and comparisons with VLAE, strengthen the empirical evidence. Despite minor lingering concerns (e.g., hyperparameter sensitivity), the contributions and improvements align with the standards of a top-tier conference. The rebuttal sufficiently mitigated critical weaknesses, leading to a consensus among reviewers toward acceptance. \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\". The keywords are Reinforcement Learning, MCTS, and AlphaZero. The abstract mentions that they're looking at the coordination of episode generation using MCTS to handle exploration vs exploitation, and they tested it on a small problem like tic-tac-toe, showing better robustness than AlphaZero. \n\nNow, looking at the reviewer comments. Reviewer 1 says it's \"less than half-baked\" because there are no references and it's poorly written. They suggest resubmitting when it's more complete. Reviewer 2 had a hard time understanding the paper due to grammar issues and couldn't even parse the abstract. They mention the paper has inaccurate statements and lacks basic scientific writing elements. Reviewer 3 also points out poor writing, undefined terms, and structural issues like blank spaces and hard-to-follow algorithms. They even say it's not a serious academic work, just notes. \n\nThe author's response is apologetic, mentions they removed references because they didn't know how to use .bib, and they found similar work in their country but not internationally. They want to improve writing and do more research.\n\nHmm, so the main issues here are the writing quality, lack of references, and clarity of the method. The reviewers are pretty harsh, especially on the writing. The paper is described as incomprehensible, with grammatical errors everywhere. Even the abstract is unclear. The authors admit to not having references because of technical issues, which is a big no-no in academic papers. Also, the method isn't clearly explained, and there's confusion about what exactly they did. The fact that the reviewers couldn't understand the core idea is a major red flag. \n\nTop-tier conferences have high standards. Even if the idea has potential, the presentation is so poor that it's hard to assess the actual contribution. The lack of references shows they didn't engage with existing literature, which is fundamental. The author's response doesn't address the core issues like clarifying the method or explaining how they handle prior work. They mention finding similar work but didn't cite it, which is a problem. \n\nThe experiments are on a small problem (tic-tac-toe), which might not be sufficient for a top conference unless the method is groundbreaking. But given the unclear writing, it's hard to tell. The reviewers' consensus is that the paper isn't ready and isn't taken seriously. \n\nSo, putting this all together, the paper doesn't meet the basic requirements for publication. The writing is a barrier to understanding, there's no literature review, and the presentation is sloppy. Even if the idea has merit, it's not presented well enough. The authors need to do significant revisions, get proper proofreading, add references, and clarify their methodology. Therefore, it should be rejected now but maybe resubmitted after major revisions.\n</think>\n\nThe paper under review faces significant and fundamental issues that preclude its acceptance at a top-tier conference. Here is the detailed reasoning:\n\n### Key Issues Identified:\n1. **Poor Writing Quality**:\n   - Multiple reviewers highlighted severe grammatical errors, unclear sentences, and poor organization. The abstract itself is incomprehensible, failing to convey the problem, method, or contributions.\n   - The paper’s prose is described as \"unprofessional,\" \"inaccurate,\" and \"full of unjustified statements,\" making it difficult for reviewers to discern the core ideas or evaluate the work’s validity.\n\n2. **Lack of References and Literature Review**:\n   - The paper contains **no references**, a critical omission in academic writing. This suggests the authors did not engage with prior work, including foundational MCTS and AlphaZero literature, or related methods like meta-tree approaches mentioned in the author response.\n   - The absence of references undermines the paper’s credibility and makes it impossible to assess the novelty or context of the proposed method.\n\n3. **Unclear Methodology**:\n   - Reviewers could not determine the specifics of the proposed method. Key terms like \"coordination of episode generation\" and \"master game tree\" are undefined, and the algorithm descriptions are described as \"difficult to follow.\"\n   - The paper’s structure is flawed, with \"blank space\" and poorly formatted content (e.g., algorithm boxes), further obscuring the method.\n\n4. **Insufficient Experimental Validation**:\n   - The experiments are limited to a \"small problem\" (e.g., tic-tac-toe), which is unlikely to demonstrate the method’s scalability or practical relevance for top-tier conferences. Without stronger empirical results or theoretical analysis, the claim of \"robust performance\" lacks说服力.\n\n5. **Author Response Limitations**:\n   - The authors acknowledge the paper’s flaws (e.g., lack of references due to technical inexperience) but do not address core issues like methodological clarity or engagement with prior work. The mention of similar work in their country but not internationally suggests potential duplication or lack of awareness of existing literature.\n\n### Standards for Top-Tier Conferences:\nTop-tier conferences require rigorous, well-written papers that clearly articulate novel contributions, situate work within existing literature, and provide compelling evidence of impact. This paper fails on all counts:\n- **Clarity**: The writing and structure are barriers to understanding.\n- **Novelty**: Without references or a literature review, the work’s originality cannot be assessed.\n- **Technical Soundness**: The method is inadequately described, and experiments are underwhelming.\n- **Professionalism**: The lack of references and poor presentation suggest the work is not yet ready for peer review.\n\n### Conclusion:\nWhile the paper’s abstract hints at a potentially interesting idea (improving MCTS-based RL via coordinated episode generation), the execution is fundamentally flawed. The paper lacks the clarity, rigor, and scholarly foundation required for acceptance at a top-tier conference. The reviewers’ consensus—that this is \"not a serious academic endeavor\" and requires \"significant work\"—is compelling. The authors must address these issues through substantial revisions, professional proofreading, and thorough engagement with prior work before resubmission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"Improved Training of Certifiably Robust Models,\" and the keywords are Convex Relaxation, Certified Robustness, Regularization. The abstract mentions that convex relaxations are effective but have a gap between certifiable and empirical robustness. The authors propose two regularizers to tighten these bounds, showing improvements over baselines in experiments.\n\nNow, looking at the reviewer comments. There are three reviewers here. Let me parse each one.\n\nReviewer 1 has several points. They point out some inaccuracies in the paper, like the claim that IBP can't handle L2 adversaries, which the reviewer says is incorrect. They also note that the convex relaxation by Ehlers isn't optimal unless ReLUs are relaxed independently. The reviewer is confused about some sections, like Section 4 where Eq O is called the non-convex problem, not the optimal relaxation. They also criticize the analysis in Section 4.1 and 4.2, saying the approach in 4.2 might be counterproductive. The experiments are lacking because they don't include Gowal et al.'s results, which are better. The reviewer mentions typos and formatting issues. Their opinion is that the analysis is confusing and needs rethinking, and the experiments aren't convincing enough without proper baselines.\n\nReviewer 2's summary says the paper proposes two regularizers inspired by convex relaxation literature. They note the strength is the perspective on convex relaxation. Weaknesses include confusion in Section 4 between the original problem and the relaxation, unclear why the regularizer works, and uncertainty about the δ₀* being within the norm ball. They also question if the regularizer can be applied with optimal relaxations. The experiments need clarification on which relaxation baselines used. The reviewer leans towards weak rejection due to unclear theoretical justification and presentation issues.\n\nReviewer 3 thinks the problem is interesting but points out presentation issues and that MNIST is not a strong dataset. They suggest adding more datasets. The experiments are promising but the dataset choice is weak. Their conclusion is a weak accept, but they want more datasets.\n\nThe author response addresses several points. They fixed typos, added CIFAR10 results, clarified that δ₀* is within the norm ball, explained their regularizers don't kill ambiguous ReLUs like Xiao et al., and added comparisons with Gowal et al. They also clarify that their method works with Fast-Lin but can be extended to other relaxations, though optimal ones are too slow. They provided an appendix example showing their regularizer's effectiveness even with ambiguous ReLUs.\n\nNow, considering all this. The initial reviews had concerns about the analysis being confusing, experiments lacking strong baselines, and dataset choice. The authors addressed some of these: added CIFAR10, included Gowal's results, clarified theoretical points, and fixed typos. The main issues seem to have been tackled. The experiments now include better baselines and more datasets. The theoretical confusion was addressed with appendices and clarifications. The key weakness from Reviewer 2 about the regularizer's justification and δ₀* was addressed by the authors, showing δ₀* is within the norm ball and explaining the regularizer's mechanism.\n\nHowever, Reviewer 2 still had concerns about whether the regularizer could be applied with optimal relaxations, but the authors explained that optimal relaxations are too slow for training, so their approach is practical. The main contribution is improving the tightness of existing relaxations (like Fast-Lin) through regularization, which the experiments now show achieves SOTA on MNIST and CIFAR10. The dataset issue (only MNIST before) was addressed by adding CIFAR10.\n\nThe revised paper now has stronger experiments and clearer explanations. The authors also clarified the relationship between their regularizers and existing work. The remaining concerns (like the optimal relaxation's applicability) are acknowledged but not critical since the method is practical and shows improvements over baselines.\n\nTop-tier conferences require strong contributions, clear presentation, and thorough experiments. The revised paper seems to meet these after addressing reviewer points. The experiments now include better baselines and more datasets, and the theoretical parts are clarified. The initial weaknesses in presentation and experiments have been mitigated. The authors' response shows they took feedback seriously and improved the paper significantly.\n\nTherefore, I think the paper should be accepted now, especially since the authors addressed major concerns and strengthened the experiments.\n</think>\n\nThe paper proposes two regularizers to improve the tightness of convex relaxation-based bounds for certifiably robust neural networks. The initial reviews highlighted several concerns, including theoretical confusion, insufficient baselines, and dataset limitations. However, the authors' revisions addressed many of these points effectively:\n\n1. **Theoretical Clarifications**: The authors clarified ambiguities in Sections 4 (e.g., distinguishing between the non-convex problem and its relaxations) and provided an illustrative example in the appendix to demonstrate when their regularizers work. They also addressed reviewer concerns about δ₀* lying within the norm ball and the relationship between their approach and existing methods like Xiao et al. (ICLR 2019).\n\n2. **Experimental Strengthening**: The revised experiments now include comparisons with stronger baselines (e.g., Gowal et al.'s IBP and CROWN-IBP) and results on CIFAR10, addressing the critique that MNIST alone was insufficient. The authors achieved state-of-the-art certified accuracies on both MNIST and CIFAR10, demonstrating practical improvements over non-regularized baselines.\n\n3. **Presentation and Typos**: The authors fixed formatting issues, clarified confusing sections, and added appendices to address technical subtleties (e.g., the role of ambiguous ReLUs and L2 IBP limitations).\n\n4. **Key Contributions**: The paper’s core idea—using regularizers to align the convex relaxation’s solution with the true network behavior—appears novel and impactful. The experiments validate this approach, showing tangible improvements in certified robustness.\n\n**Remaining Concerns**: \n- Reviewer 2’s point about the optimal convex relaxation (e.g., LP-All) remains unresolved in practice due to computational infeasibility, but the authors clarify that their method is compatible with such relaxations in theory and that their focus is on improving practical relaxations like Fast-Lin and CROWN-IBP.\n- The theoretical analysis still requires careful reading, but the revised explanations and examples reduce confusion.\n\n**Conclusion**: The revisions sufficiently address the reviewers' major critiques. The paper now presents a clear, theoretically grounded method with strong empirical validation on relevant datasets and baselines. Its contributions align with the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Out-of-Distribution Image Detection Using the Normalized Compression Distance.\" The abstract mentions they propose a method called MALCOM that doesn't require out-of-distribution samples for validation or retraining the model. They use global average pooling and spatial patterns from feature maps with a similarity metric based on normalized compression distance (NCD). The key constraints here are no OOD data for validation and no model retraining, which they argue makes their method practical for real-world deployment.\n\nLooking at the reviewer comments first. Reviewer 1 gave a weak reject. Their main points are about the comparison with existing methods being unfair because the Mahalanobis method (Lee et al. 2018) can use adversarial samples for validation without OOD data, which the authors didn't properly consider. The reviewer also notes that the paper's results in the main text are weak, with better results pushed to the appendix. They mention that the replication of prior methods had lower performance than original papers, which might be due to incorrect implementations. The author response addressed this by adding experiments with adversarial samples and a new variant (MALCOM++) but the reviewer still thinks the comparisons weren't fair initially.\n\nReviewer 2 is on the fence. They like the cleverness and novelty but question the necessity of the strict constraints (no OOD validation, no retraining). They want more justification for why these constraints are important and common in real scenarios. The author response tried to clarify that using OOD samples for validation assumes prior knowledge of the test distribution, which isn't practical. They also added experiments comparing against less constrained methods. However, the reviewer still feels the paper needs more work, especially in justifying the constraints and presenting broader comparisons.\n\nReviewer 3 found the method novel but questioned the experimental rigor and real-world significance. They wanted more complex scenarios, like safety-critical applications, and theoretical guarantees. The authors added some experiments and addressed some points, but the reviewer remains unconvinced about the method's efficiency and significance.\n\nNow, the author response addressed several points. They added more experiments in the appendix, including using adversarial samples for validation (MALCOM++), which improved results. They also fixed some typos and added figures. However, the main paper's tables still might not show the best results, relying on the appendix. The replication of prior methods had discrepancies, which the authors acknowledged but didn't fully resolve in the main text.\n\nKey issues to consider for a top-tier conference:\n\n1. **Methodological Soundness**: The core idea of using NCD on feature maps is novel. The authors address some reviewer concerns by adding experiments, but the initial comparisons were flawed. The replication of prior work's lower performance could be a red flag unless they clearly state their replication results vs. original papers, as Reviewer 1 suggested.\n\n2. **Experimental Rigor**: The main paper's results are weaker, with better results in the appendix. Top conferences require strong main results. The authors didn't fully address the fairness of comparisons in the main text, relying on appendices which might not be read carefully.\n\n3. **Necessity of Constraints**: Reviewer 2's point is critical. If the constraints (no OOD validation, no retraining) aren't common or important, the method's practicality is questionable. The authors argue these are practical, but need stronger justification. They added comparisons with less constrained methods, but the main paper might not highlight this enough.\n\n4. **Clarity and Writing**: Reviewer 2 noted grammatical issues and lack of some references. The authors fixed typos and added citations, which helps, but the abstract's first sentence still might be awkward.\n\n5. **Theoretical Backing**: Reviewer 3 wanted more theoretical guarantees. The authors didn't address this directly, focusing on experiments instead. For a top conference, some theoretical grounding could strengthen the paper.\n\n6. **Reproducibility**: The authors provided code, which is good, but mention it needs refactoring. That's a minor issue but shows some effort.\n\nConsidering all this, the paper has merit but several unresolved issues. The main concerns are the comparison fairness, the necessity of the constraints, and the relegation of key results to the appendix. While the authors made improvements in the rebuttal, the reviewers' points about the main paper's shortcomings remain. Top-tier conferences require high standards in methodology, experiments, and clarity. The paper's current state, with its reliance on appendix results and unresolved replication discrepancies, might not meet those standards yet. However, the problem is important, and the method is novel. But given the reviewers' consensus leaning towards rejection (especially Reviewer 1's weak reject and others' concerns), I think the decision should be to reject, but with an invitation to resubmit after addressing these points more thoroughly.\n</think>\n\nThe paper proposes MALCOM, an out-of-distribution (OOD) detection method that avoids using OOD samples for validation and retraining deployed models. While the approach is novel and addresses practical constraints, several critical issues undermine its readiness for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty**: The use of normalized compression distance (NCD) to capture spatial patterns in feature maps is innovative and offers a distinct angle for OOD detection.\n2. **Practical Constraints**: The focus on methods that do not require OOD validation or model retraining is a valuable direction, especially for real-world deployment.\n3. **Additional Experiments**: The authors addressed reviewer concerns by adding experiments (e.g., MALCOM++ with adversarial validation) and fixing errors, demonstrating improved performance in some cases.\n\n### Major Concerns:\n1. **Unfair Comparisons**: \n   - The initial comparisons with prior work (e.g., Mahalanobis) were flawed. Reviewer 1 noted that the Mahalanobis method can use adversarial samples for validation without OOD data, a scenario the authors initially ignored. While the rebuttal added experiments addressing this, the main paper still lacks fair comparisons, relegating key results to the appendix.\n   - Replications of prior methods yielded lower performance than original papers, raising concerns about implementation accuracy. The authors did not clearly present original vs. replicated results, as reviewers requested.\n\n2. **Justification of Constraints**:\n   - Reviewer 2 questioned the necessity of the strict constraints (no OOD validation, no retraining). The authors argued these constraints ensure practicality, but the paper does not sufficiently justify why scenarios where *both* constraints apply are common or critical. While additional experiments with relaxed constraints were added, the main narrative still emphasizes the restrictive setting without convincing evidence of its prevalence.\n\n3. **Experimental Rigor**:\n   - Core results in the main paper are weaker than those in the appendix (e.g., MALCOM++ outperforms baselines only with adversarial validation). Top-tier venues require strong main results to support claims.\n   - The method’s effectiveness hinges on combining multiple components (e.g., concatenation of feature layers), but ablation studies in the main text are insufficient. The rebuttal added some ablation results, but they remain in the appendix.\n\n4. **Clarity and Writing**:\n   - The abstract’s opening sentence is grammatically awkward, and the paper could benefit from further polishing. While typos were fixed, the prose remains uneven in places.\n\n5. **Theoretical Grounding**:\n   - Reviewer 3 noted a lack of theoretical guarantees or analysis explaining why NCD-based spatial patterns improve OOD detection. The authors focused on empirical results but did not address this gap.\n\n### Conclusion:\nWhile MALCOM introduces an interesting idea and addresses important practical constraints, the paper’s unresolved methodological flaws (e.g., unfair comparisons, reliance on appendix results), insufficient justification for its constraints, and lack of theoretical depth prevent it from meeting the high standards of a top-tier conference. The authors have made improvements in the rebuttal, but the core issues remain unaddressed in the main submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" with keywords GMM and SGD. The abstract mentions three main contributions: using an upper bound on the log-likelihood for numerical stability, a regularizer to prevent bad local minima, and a method to enforce constraints with SGD. They also simplify the GMM model using local principal directions to handle high-dimensional covariance matrices. Experiments are on image datasets like MNIST and SVHN, and they have a TensorFlow implementation.\n\nNow, looking at the reviewer comments. Reviewer 1 says the work is incremental and shows a lack of understanding of online EM. They point out that the max-component approximation is similar to the log-sum-exp trick but the authors didn't reference it. They also question the gridding approach and the experimental evaluation, noting that the datasets aren't very high-dimensional (like 28x28 images). \n\nReviewer 2 mentions the lack of convergence guarantees and compares to moment matching methods. They also point out several technical issues: the requirement for covariance entries being non-negative (should be positive definite), confusion about local minima vs. unbounded solutions, and unclear regularization. The experiments are not convincing enough, with figures selected manually and missing details in comparisons. \n\nReviewer 3 thinks the paper is clear but the contributions are incremental. The max-component idea is seen as a standard trick, and the main novelty is the regularizer. However, the experiments don't show clear advantages over EM with K-means initialization. They also note that the low-rank covariance approach isn't well demonstrated beyond diagonal matrices. \n\nThe authors responded by clarifying some points, like the max-component vs log-sum-exp, explaining the regularizer as annealing, and addressing some technical issues. They also mention the focus on online learning where K-means isn't feasible. \n\nHmm, so the key points against acceptance are: incremental contributions, lack of thorough experimental validation (especially on truly high-dimensional data), and some technical inaccuracies. The authors addressed some concerns but maybe not all. The reviewers point out that the datasets used (like MNIST's 784 dimensions, SVHN's 3072) are considered high-dimensional, but maybe the methods aren't tested on even higher dimensions. Also, the main contribution's novelty is questioned—using SGD for GMMs with some tricks, but existing methods like online EM might be better. The experiments don't clearly show superiority over EM in terms of both performance and efficiency. The regularizer is a nice idea but presented as heuristic without strong theoretical backing. \n\nOn the plus side, the paper is well-written, the approach is practical, and they provide code. The focus on online learning is a valid niche, especially if K-means initialization isn't possible in streaming scenarios. The authors addressed some reviewer points, but maybe not enough to overcome the concerns about novelty and evaluation. \n\nTop-tier conferences usually require significant contributions and strong experiments. Here, the method seems to offer a way to apply SGD to GMMs with some modifications, but the contributions are incremental. The experiments don't fully convince that this is better than existing methods, especially in high-dimensional settings. The technical issues raised (like the covariance handling, unclear regularization) need more rigorous treatment. \n\nFinal call: The paper has some merit but falls short in innovation and experimental rigor for a top conference. The reviewers' concerns about being incremental and insufficient evaluation are valid. So, I think it should be rejected unless major revisions address all these points, which the authors haven't fully done yet based on their responses.\n</think>\n\nThe paper proposes an approach to train Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces, addressing numerical stability, local minima, and memory constraints. While the authors present a clear motivation and provide a TensorFlow implementation, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Concerns:\n1. **Novelty and Incremental Nature**:\n   - Reviewers highlight that core ideas (e.g., the max-component approximation, log-sum-exp trick, and natural parameterization) are either standard or incremental. The proposed \"regularizer\" is reinterpreted as annealing, which further reduces its novelty. The focus on SGD for GMMs is not entirely novel, and the paper does not sufficiently distinguish itself from existing methods like online EM or moment-matching approaches.\n\n2. **Technical Gaps**:\n   - **Numerical Stability**: The authors claim their max-component approach avoids underflow/overflow but fail to convincingly differentiate it from established techniques like log-sum-exp. Reviewers question the necessity and justification of their method.\n   - **Covariance Handling**: The local principal directions approach is under-explained, and experiments only validate diagonal covariances, limiting its practical impact. The subspace method (Section 3.4) raises unresolved computational efficiency concerns.\n   - **Regularization and Annealing**: The \"regularizer\" is heuristic and lacks a clear theoretical foundation. The annealing schedule (e.g., \\(\\tau\\) in Section 3.3) is poorly justified, and the authors’ responses do not fully address these issues.\n\n3. **Experimental Weaknesses**:\n   - **Dataset Dimensionality**: While the authors argue that 28x28 (784D) or 32x32x3 (3072D) qualify as high-dimensional, these are modest compared to modern high-dimensional problems (e.g., 10k+ dimensions in vision or NLP). The experiments do not test scalability to truly large dimensions.\n   - **Baseline Comparisons**: The comparison to EM lacks rigor. For instance, EM with K-means initialization is not fairly evaluated, and runtime comparisons are missing. The authors’ focus on streaming/online settings is valid but under-supported empirically.\n   - **Ablation Studies and Reproducibility**: The experiments lack clear ablations (e.g., impact of regularization without K-means initialization) and manual selection of figures undermines credibility.\n\n4. **Clarity and Rigor**:\n   - Technical ambiguities persist (e.g., \\(\\Sigma_{i,j} \\geq 0\\) misstated as a requirement for covariance entries, unclear definitions like \\(\\mu^i\\), and incomplete explanations of the principal directions approach).\n   - Theoretical guarantees (e.g., convergence) are absent, and the authors’ response on convergence relies on generic gradient descent properties, which are insufficient for a top-tier venue.\n\n### Author Responses:\nThe authors addressed some points (e.g., clarifying the annealing interpretation, fixing typos), but critical issues remain unresolved. They did not convincingly demonstrate the superiority of their method over EM in high-dimensional settings or provide rigorous ablations. The focus on online learning is a valid niche, but the experiments do not yet validate this claim adequately.\n\n### Conclusion:\nWhile the paper is well-written and tackles an important problem, its contributions are incremental, technical gaps remain unresolved, and the experiments lack the rigor needed to justify publication at a top-tier conference. The authors have not sufficiently addressed reviewers’ concerns about novelty, experimental validity, and theoretical grounding.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about MuPPET, a method for quantized training of CNNs using multiple precisions and a switching mechanism. The main claims are improved training time and energy efficiency without sacrificing accuracy. The experiments are on AlexNet, ResNet18, and GoogLeNet on ImageNet with an NVIDIA Turing GPU. The speedup is 1.28x on average.\n\nLooking at the reviewers' comments first. Reviewer 1 found the paper interesting but criticized the heuristic nature of the switching mechanism. They didn't understand why gradient co-alignment would indicate the need to switch precisions. They also pointed out typos and that the justification for AlexNet's switching timing was ad hoc. \n\nReviewer 2 had more critical points. They said the approach doesn't match the description because it only decides when to switch precisions but not the actual levels, which are fixed. The choice of bit levels (8,12,14,16) was confusing. They also mentioned unclear notations (like 'p' and 'y'), discrepancies in Figure 2, and that the results aren't superior to state-of-the-art. The accuracy for some models was lower than FP32, and the authors didn't show training curves. \n\nReviewer 3 thought the core idea (dynamic switching) isn't shown as necessary for good results. They wanted clearer contributions, more on how parameters like 'p' affect performance, and reproducibility details. Minor issues like figure readability and equation explanations were also noted.\n\nThe author responses addressed some points. They clarified that the switching is about timing, not precision levels, and that the framework is generalizable. They added figures and explanations for equations, parameters, and addressed typos. However, some concerns remain, like the accuracy not being much better than SOTA and the lack of clear superiority.\n\nNow, considering top-tier standards. The paper's main contribution is a new strategy for precision switching during training. The experiments show a modest speedup (1.28x) but the accuracy is close to FP32. However, Reviewer 2 pointed out that in some cases, like AlexNet and ResNet, the Top-1 accuracy was much lower than FP32. The authors counter that the difference is less than 1%, but if that's true, maybe it's within acceptable variance. However, if the accuracy is indeed lower, that's a problem.\n\nThe method's novelty is questioned because the precision levels are fixed, and the switching is based on a heuristic. The reviewers wanted a clearer explanation of why the gradient diversity metric works and how the parameters are chosen. The authors added some explanations but maybe not enough. The notation issues and figure clarity were addressed in the response, which is good.\n\nThe main issues are: \n1. The method's core idea (dynamic switching) isn't shown to be necessary. The results aren't significantly better than existing methods, especially when considering accuracy trade-offs.\n2. The heuristic nature of the switching mechanism isn't well justified. The reviewers wanted a stronger theoretical or empirical basis for why gradient co-alignment indicates a switch.\n3. The experiments might not be strong enough. The speedup is modest, and the accuracy isn't clearly better. The lack of training curves makes it hard to assess the training dynamics.\n4. The paper's clarity and reproducibility have issues, like unclear parameters and quantization steps. Even with author responses, some confusion remains.\n\nTop-tier conferences require significant contributions, clear explanations, and strong empirical validation. While the paper introduces a new approach, the reviewers' points about the method's necessity, clarity, and results being not superior enough are valid. The authors addressed some concerns but not all. The modest speedup and potential accuracy issues (if real) might not meet the high bar. The heuristic aspects and lack of clear necessity for the dynamic switching could be deal-breakers.\n\nFinal Decision: Reject\n</think>\n\nThe paper proposes MuPPET, a precision-switching strategy for quantized training of CNNs, aiming to reduce training time while maintaining accuracy. While the idea of using multiple fixed-point precisions and a dynamic switching mechanism is novel, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Core Contribution and Necessity**: Reviewers questioned whether the dynamic switching mechanism is a *necessary* innovation. The results show only a modest speedup (1.28× average) and do not clearly outperform state-of-the-art methods. For instance, Reviewer 2 noted that accuracy for AlexNet and ResNet18 was \"much lower\" than FP32 baselines, though the authors argue this difference is within acceptable variance. The lack of training/validation curves (addressed post-review via appendices) further complicates assessing the method’s reliability.\n\n2. **Heuristic Justification**: The switching mechanism relies on gradient diversity, but its rationale is poorly explained. Reviewers found the heuristic for transitioning precisions (e.g., why co-aligned gradients signal reduced informativeness) insufficiently grounded in theory or empirical validation. While the authors added clarifications, the explanation remains heuristic and lacks a rigorous justification for why gradient co-alignment indicates the need for higher precision.\n\n3. **Methodological Clarity and Reproducibility**: Key details about quantization (e.g., scaling factors in Equations 4–5, the choice of precision levels like 12/14 bits) were unclear. The notation (e.g., \"p,\" \"y\") and algorithm steps were confusing, even after revisions. The discrepancy in Figure 2 (switching not triggered despite threshold violations) was only partially addressed, raising concerns about methodological rigor.\n\n4. **Empirical Weaknesses**: The experiments lack strong evidence of superiority over existing methods. While MuPPET matches FP32 accuracy in some cases, the speedup is marginal. Reviewer 2 highlighted that the method is \"more complex but not superior enough,\" and the authors did not convincingly refute this. The absence of ablation studies or hyperparameter sensitivity analyses further weakens the empirical foundation.\n\n5. **Presentation and Writing**: Despite revisions, minor issues like figure readability and ambiguous terminology persist. The introduction’s delayed focus on contributions and the ad hoc justification for AlexNet’s switching timing (later revised) suggest a lack of polish.\n\n**Conclusion**: While the topic is timely and the approach has potential, the paper falls short of top-tier standards due to insufficient justification for its core mechanism, unclear methodology, and underwhelming empirical results. The modest gains and unresolved ambiguities do not justify acceptance at this stage.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether the paper \"NORML: Nodal Optimization for Recurrent Meta-Learning\" should be accepted or rejected for a top-tier conference. I need to go through all the provided information carefully.\n\nStarting with the paper's abstract: It introduces NORML, a meta-learning framework using an LSTM-based meta-learner for neuron-wise optimization. The key points are that the meta-learner parameters scale linearly with the learner parameters, addressing scalability, and that experiments show effectiveness in few-shot tasks. The authors claim their approach improves over existing methods by making effective weight updates beyond just feature reuse.\n\nNow, looking at the reviewer comments. There are three reviewers, each with their own critiques. Let me break them down one by one.\n\nReviewer 1 points out several issues. First, the paper's contribution relative to Ravi & Larochelle's work isn't clear. The authors claim that Ravi's method requires an enormous number of parameters, but Reviewer 1 says that Ravi actually uses parameter sharing to keep the meta-learner parameters constant relative to the learner size. So the authors might have misrepresented prior work. This is a problem because it undermines the claimed novelty. \n\nNext, the experiments don't compare fairly with Ravi's method under the same settings. The paper uses a pre-trained convolutional network and only tunes the last layer, but Ravi's original setup might be different. Without a direct comparison, it's hard to see the benefit of NORML's approach. \n\nThen, the paper claims that NORML can handle more inner-loop steps better than MAML, but the experiments only use few steps. The reviewer wants evidence for that claim. Also, the results on Mini-ImageNet don't include Sun et al.'s results, which might have better performance. That's a big oversight because not showing competing methods weakens the results. The Omniglot experiment uses fully connected networks instead of convolutions, making it less comparable to standard benchmarks. \n\nReviewer 2's comments are harsher. They mention poor writing quality—grammatical errors, citation issues, and unclear flow. The paper is described as \"well beyond the level required by a scientific manuscript,\" which is a strong criticism. The technical novelty is minimal, and there are inaccuracies in describing prior work. For example, the authors claim Andrychowicz uses separate LSTMs per parameter, but Reviewer 1 and 3 say that's not true; instead, parameter sharing is used. This inaccuracy suggests a lack of thorough literature review.\n\nReviewer 3 also points out limited novelty, incremental changes over Ravi & Larochelle, and the lack of fair comparisons. They note that NORML is tested only on fully connected networks, limiting its applicability since most few-shot methods use convolutions. The method's description has inaccuracies regarding prior work's parameter usage. Additionally, there are technical issues like equations with typos and missing details on whether gradients are propagated into the base learner's gradients. \n\nThe author response isn't available, so we can't see any rebuttals to these points. \n\nPutting this together: The main issues are:\n\n1. **Novelty and Contribution**: The paper's claims about scaling and improvement over Ravi & Larochelle are questionable. Reviewer 1 and 3 argue that the prior work already addressed scaling through parameter sharing, so the contribution isn't as novel. The authors might have misrepresented existing methods, which is a red flag.\n\n2. **Experimental Weaknesses**: \n   - No direct comparison with Ravi's method under the same conditions.\n   - Missing baselines like MAML++ and Sun et al.'s results.\n   - Experiments don't test the claim about handling many inner-loop steps.\n   - Omniglot uses FC instead of convolutions, making results less impactful.\n\n3. **Writing Quality**: Reviewer 2's comments on grammar, citations, and flow are severe. A top-tier conference requires clear writing, and these issues could hinder readability and credibility.\n\n4. **Technical Accuracy**: Inaccurate descriptions of prior work (e.g., Andrychowicz's method) and possible errors in equations or method descriptions (like parameter indexing in LSTMs) suggest the paper needs more rigorous proofreading and validation.\n\n5. **Applicability**: Limited to FC networks, which is a drawback since most applications use convolutions. This reduces the method's practical value.\n\nConsidering all these points, the paper has significant flaws in both technical contribution and presentation. The lack of fair comparisons and accurate prior work analysis undermines its claims. The experiments are not robust enough to support the conclusions, and the writing issues are substantial. While the idea of using LSTMs for meta-learning isn't new, the paper doesn't sufficiently demonstrate a meaningful advancement over existing methods. \n\nTherefore, the decision should be to reject the paper unless major revisions address all these issues, which the reviewers have pointed out but the authors haven't had a chance to respond to. Since the author response isn't provided, and the reviewers' critiques are substantial, it's unlikely to meet the standards of a top-tier conference without significant improvements.\n</think>\n\nThe paper \"NORML: Nodal Optimization for Recurrent Meta-Learning\" faces significant challenges that undermine its potential for acceptance at a top-tier conference. Below is the detailed analysis:\n\n### Key Issues Identified:\n1. **Novelty and Contribution**:\n   - **Misrepresentation of Prior Work**: The authors claim that Ravi & Larochelle’s method requires \"enormous\" meta-learner parameters, but Reviewer 1 clarifies that Ravi et al. use parameter sharing to avoid this, making the scaling argument invalid. This mischaracterization weakens the paper’s claimed novelty.\n   - **Incremental Contribution**: Reviewer 3 notes that NORML is an incremental variation of existing LSTM-based meta-learners (e.g., Ravi & Larochelle) without a fair or direct comparison. The lack of experiments comparing NORML to Ravi’s method under identical conditions (e.g., pre-trained convolutional networks) makes it difficult to assess its superiority.\n\n2. **Experimental Weaknesses**:\n   - **Missing Baselines**: \n     - Sun et al.’s results are omitted despite being relevant to the proposed method’s setup, and their reported performance appears stronger. This omission raises concerns about the paper’s thoroughness.\n     - MAML++ and other advanced baselines (e.g., methods with per-parameter learning rates) are not included, limiting the validity of comparisons.\n   - **Unsubstantiated Claims**: The paper claims NORML handles \"large numbers of inner-loop steps\" better than MAML, but experiments only use few steps. No evidence supports this critical claim.\n   - **Limited Applicability**: NORML is tested only on fully connected networks, whereas most few-shot learning methods use convolutional architectures. The Omniglot experiment uses an atypical setup, reducing comparability to prior work.\n\n3. **Technical and Writing Flaws**:\n   - **Inaccurate Descriptions**: The authors incorrectly state that Andrychowicz et al. use separate LSTMs per parameter (they use parameter sharing). This error suggests a lack of thorough literature review.\n   - **Equation and Methodological Issues**: \n     - Equations (e.g., Equation 16’s typo) and unclear descriptions (e.g., indexing of LSTM parameters) introduce ambiguity.\n     - The paper does not clarify whether gradients are propagated into the base learner’s gradients (first-order vs. second-order optimization).\n   - **Writing Quality**: Reviewer 2 highlights severe grammatical errors, citation inconsistencies, and poor flow, which detract from clarity and professionalism.\n\n4. **Scalability and Claims**:\n   - While the authors argue NORML scales linearly with learner parameters, Reviewer 3 notes that the method uses layer-specific LSTMs (contrary to the claim of a \"single meta-learner\"), complicating scalability and contradicting the abstract’s assertion.\n\n### Conclusion:\nThe paper’s core contributions are undermined by flawed comparisons to prior work, insufficient experimental validation, and technical inaccuracies. The writing quality and presentation fall below the standards of a top-tier conference. While the idea of using LSTMs for meta-learning is interesting, the lack of rigorous validation, fair baselines, and clear novelty makes it difficult to justify acceptance. The reviewers’ critiques collectively indicate that the paper requires substantial revisions to address these issues, which are not reflected in the author response (which is unavailable).\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Extreme Classification via Adversarial Softmax Approximation.\" The abstract mentions they're addressing the problem of training classifiers with a huge number of classes, which is known as extreme classification. The main issue they're tackling is the computational cost of traditional softmax regression, which scales linearly with the number of classes (C). They propose using an adversarial model to sample negative examples more effectively, which reduces gradient variance and speeds up training. Their contributions are threefold: an adversarial sampling method with logarithmic cost, a proof about minimizing gradient variance, and experiments showing a 10x speedup over baselines.\n\nNow, looking at the reviewer comments. \n\nReviewer 1 is positive, saying the paper is well-explained and technically sound, even for a non-expert. They recommend acceptance. \n\nReviewer 2 has some concerns. They point out that the paper doesn't discuss related work enough, specifically mentioning four papers that also deal with non-uniform negative sampling. They also note that the experiments don't include comparisons with the full softmax loss on smaller datasets and point out some typos. \n\nReviewer 3 is more critical. They argue the experiments are incomplete, missing recent state-of-the-art methods like Slice and DiSMEC. They also mention that the paper doesn't evaluate performance on tail-labels or use metrics like MacroF1. Additionally, they suggest testing on more datasets, including smaller ones like EURLex and larger ones like Amazon3M. They also question the multi-label applicability of the method.\n\nThe author response addresses some of these points. They acknowledge the lack of related work and agree to add more references. They defend their focus on negative sampling, arguing their theoretical contributions (like Theorem 2) are novel. They also mention running additional experiments on EURLex but couldn't include them in the rebuttal. They clarify differences between their approach and the cited works, like the distinction between negative sampling and sampled softmax.\n\nSo, considering all this, what's the verdict?\n\nFirst, the paper's main contributions are the adversarial sampling method, the theoretical proof about gradient variance, and the empirical results showing faster training. The reviewers agree on the technical merit but have concerns about related work and experiments.\n\nReviewer 2's points about missing related work are valid. The authors admit they didn't cover all prior work and plan to add it. That's fixable in the final version. The experiments not including the full softmax on smaller datasets is a minor issue since they're addressing extreme classification where C is huge, but adding smaller datasets could help. The author response mentions they're doing that, so maybe that's okay.\n\nReviewer 3's criticisms are more significant. The lack of comparison with Slice and DiSMEC is a problem because those are state-of-the-art methods. If the proposed method isn't compared against the best existing approaches, it's hard to argue it's an improvement. The authors argue their focus is on negative sampling methods, but in a top conference, you need to compare against the strongest baselines, even if they're different approaches. Also, not evaluating on tail-labels or using MacroF1 is a gap, especially since extreme classification datasets often have skewed distributions. The paper's experiments are limited to two datasets, and adding more (like EURLex and Amazon3M) would strengthen it. The authors mention they're working on that, but since they couldn't include it yet, the current submission might be incomplete.\n\nThe theoretical contribution (Theorem 2) is a plus, but if the experiments are lacking in key areas, that's a problem. The authors' defense about focusing on negative sampling is valid, but top-tier conferences require thorough comparisons. The reviewer's point about DiSMEC's higher accuracy on Wiki500 is concerning—if their method is slower but more accurate, that's a trade-off that needs discussion.\n\nThe author response clarifies some points, like the difference between their adversarial model and the \"summary vector\" in other works. They also mention that their auxiliary model is static, which is an advantage. However, without experimental validation against the newer methods, it's hard to assess the actual impact.\n\nIn summary, the paper has strong theoretical contributions and a novel method, but the experiments are not comprehensive enough. The authors acknowledge some issues and plan to address them, but the current submission might not meet the high standards of a top conference without those additions. Since the rebuttal can't include new experiments, the paper might be borderline. However, given that the authors are willing to make revisions and the core idea is solid, maybe it's worth accepting with the expectation that they'll strengthen the experiments and related work in the final version. Alternatively, if the missing comparisons are critical and can't be fixed without major changes, rejection might be warranted.\n\nBut top conferences often require the experiments to be thorough in the submission. Since the reviewers pointed out that Slice and DiSMEC are missing, and those are key baselines, the current experiments are insufficient. The authors' argument about focusing on negative sampling baselines is valid, but in a top venue, you need to at least mention and discuss why those other methods aren't directly comparable, even if you can't run them. The fact that DiSMEC might be computationally expensive but still needs discussion is a point against the paper's completeness.\n\nThe theoretical contribution is a strong point, but without empirical validation against the best methods, it's a bit of a gap. The reviewers' comments highlight that the paper's experiments are not as strong as they could be. However, the authors are addressing some of these in their response, so maybe the paper can be accepted conditionally on those revisions. Since the conference decision is based on the current submission, though, the missing comparisons and limited datasets might lead to rejection.\n\nHmm. The final decision is tricky. The paper has merits but also significant experimental shortcomings. The authors are responsive and plan to improve, but the current version might not be up to top-tier standards. However, the theoretical part is novel and the method is simple yet effective. Maybe the reviewers' mixed feedback suggests it's borderline. Since two reviewers are positive (1 and 3's positives) and one has major concerns but the authors address some points, perhaps the consensus leans towards accept with revisions. But in top conferences, sometimes experiments are a deal-breaker. \n\nAlternatively, if the missing comparisons are critical and the experiments are not sufficient, rejection is better. Since the authors couldn't include the EURLex results in time, and the Slice/DiSMEC comparisons are missing, maybe the paper isn't ready yet. But the theoretical contribution is a plus. \n\nI think the final decision would be to accept, given the theoretical novelty and the authors' commitment to address the issues, but with the understanding that the revisions are necessary. However, some conferences might require those experiments to be present in the submission. Since the reviewers pointed out that the experiments are incomplete, maybe the safer choice is to reject, but the positive aspects are strong. \n\nWait, the author response says they are running experiments on EURLex and will include them in the final version. They also mention adding the related work. So if the paper is accepted, they can make those changes. Since the conference's decision is based on the current submission, but the authors can revise, maybe the reviewers' comments are constructive and the paper can be accepted with the expectation that the revisions are made. \n\nGiven that two reviewers (1 and part of 2) are positive, and the third has valid concerns but the authors are addressing them, I think the paper should be accepted. The core idea is solid, and the issues can be fixed in the final version. The theoretical contribution is a strong point, and the experiments, while not perfect, show promise. The missing baselines are a problem, but the authors are working on it. \n\nSo, Final Decision: Accept.\n</think>\n\nThe paper proposes an adversarial sampling method for extreme classification, addressing the computational challenges of traditional softmax by reducing gradient variance through non-uniform negative sampling. The theoretical contributions, including a proof of gradient variance minimization, are novel and well-motivated. The experiments demonstrate a significant speedup over baselines, which is compelling. However, the paper faces criticism for insufficient engagement with prior work (e.g., [1–4]) and incomplete experimental comparisons, particularly against state-of-the-art methods like Slice and DiSMEC. Reviewer 3 highlights critical gaps, such as missing tail-label evaluations and datasets, which weaken the empirical validation. The authors acknowledge these issues and commit to addressing them in the final version, including additional experiments and related work. While the current submission has shortcomings, the theoretical novelty and the authors' responsiveness suggest that revisions can sufficiently strengthen the paper. The core idea is impactful, and the proposed method’s simplicity and speed are valuable contributions to the field. \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Effect of Activation Functions on the Training of Overparametrized Neural Nets.\" The abstract mentions that they analyze how activation functions affect training, especially in overparametrized networks. They distinguish between smooth and non-smooth activations, showing that non-smooth ones (like ReLU) have better eigenvalue properties in the Gram matrix, leading to faster training. For smooth activations (tanh, swish), they find that the minimum eigenvalue can be small if data dimension is low, but this improves with higher dimensions or deeper networks. They also did experiments on synthetic and CIFAR10 data, with some gaps in the latter.\n\nLooking at the reviewer comments. Reviewer 1 is positive but points out weaknesses like the long appendix and some unclear extensions. They also mention minor typos and a request for more explanation on terms like DZXP. The author addressed these by revising the appendix structure, fixing typos, and clarifying assumptions. \n\nReviewer 2 is more enthusiastic, recommending acceptance. They praise the thorough theoretical analysis, especially on the eigenvalues related to NTK (Neural Tangent Kernel) and the comparison between smooth and non-smooth activations. They suggest some organizational improvements and ask about the gap between lower and upper bounds in certain cases. The authors responded by adding comparisons and reorganizing the appendix.\n\nThe author's response seems to have addressed most of the reviewers' concerns. They fixed the DZXP typo, clarified assumptions, restructured the appendix with a table of contents, and provided more details on the bounds. The main issues were about the paper's length and organization, which the authors worked on. \n\nNow, considering the standards of a top-tier conference. The paper's contributions are significant: it provides theoretical insights into activation functions' impact on training dynamics, which is a hot topic. The distinction between smooth and non-smooth activations is a solid contribution. The experiments, while having some gaps on CIFAR10, still support the theory. \n\nThe weaknesses mentioned are about the paper's length and some unclear parts in the appendix. However, the authors have made efforts to address these by reorganizing and adding explanations. Reviewer 2's positive take and the thoroughness of the theoretical work suggest that the paper meets the conference's quality. \n\nI think the paper's theoretical contributions are strong enough for acceptance. The authors have adequately addressed the reviewers' concerns, and the remaining issues (like length) are mitigated by their revisions. The empirical validation, even with some gaps, adds value. \n\nFinal Decision: Accept\n</think>\n\nThe paper presents a theoretically rigorous analysis of how activation functions influence the training dynamics of overparametrized neural networks, focusing on the eigenvalues of the Gram matrix (G-matrix) in the lazy training regime. The key contributions are:  \n1. **Theoretical Insights**: The work distinguishes between non-smooth (e.g., ReLU) and smooth (e.g., tanh) activations, showing that non-smooth activations guarantee large minimum eigenvalues of the G-matrix under minimal assumptions, while smooth activations can have small eigenvalues if data dimensions are low. This provides a foundational understanding of why non-smooth activations often perform well empirically.  \n2. **Extensions and Applications**: The analysis extends to deeper networks and discusses scenarios where smooth activations can overcome limitations with sufficient depth or high-dimensional data.  \n3. **Empirical Validation**: Experiments on synthetic data align with the theory, and while there are gaps with real data (e.g., CIFAR10), the authors acknowledge these and frame them as opportunities for future work.  \n\n**Strengths**:  \n- The theoretical framework is novel and addresses a critical gap in understanding activation function effects on convergence.  \n- The distinction between smooth and non-smooth activations is a significant contribution to the field, especially given the empirical prevalence of ReLU-like activations.  \n- The authors provide thorough proofs in the appendix and address reviewer concerns about organization and clarity.  \n\n**Weaknesses**:  \n- The paper’s length and complexity could hinder accessibility, but the revised version includes a table of contents and reorganized appendices to improve navigation.  \n- The CIFAR10 experiments show discrepancies between theory and practice, though this is partially attributed to the complexity of real-world data.  \n\n**Reviewer Feedback**:  \n- **Reviewer 1** acknowledges the paper’s strengths but critiques its length and organization. The authors addressed these by reorganizing the appendix and clarifying technical points.  \n- **Reviewer 2** strongly endorses the work, praising its thoroughness and theoretical depth, and the authors addressed their concerns about notation and bounds.  \n\n**Conclusion**:  \nThe paper’s theoretical contributions are substantial and align with the high standards of a top-tier conference. While the length and empirical gaps are valid concerns, the authors have adequately addressed structural issues, and the core insights are impactful. The work advances the theoretical understanding of neural network training and activation functions, making it a valuable addition to the literature.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem.\" The keywords include depth-width trade-offs, ReLU networks, chaos theory, and Sharkovsky's Theorem. The abstract mentions that they're connecting DNN expressivity with Sharkovsky's Theorem from dynamical systems, specifically looking at periodic points and their implications on depth and width requirements. They reference Telgarsky's earlier work on depth benefits and address an open question he posed about why certain functions are hard for shallow networks.\n\nLooking at the reviewer comments:\n\nReviewer 1 is very positive, even saying they can't raise their score higher because it's already maxed. They mention the synthetic dataset was added, which is good.\n\nReviewer 2 is also very positive, calling the paper a delight to read. They appreciate the theoretical contribution and the clean proofs. They asked two questions: one about bias terms affecting lower bounds and another about empirical intuition via a plot of classification error vs depth. The authors addressed both in their response, adding experiments and examples. The reviewer's main points are positive, so this is a strong endorsement.\n\nReviewer 3 is more critical on structure but still positive overall. They liked the application of Sharkovsky's theorem but thought the paper was too theory-heavy upfront. They suggested moving the main theorem earlier. The authors restructured the paper as suggested, which should help. Reviewer 3 also points out that the period-dependent lower bound might not be very useful in practice because assessing the period of a function is hard. The authors acknowledge this but argue that their work provides a natural property (periodic points) and references practical examples where such dynamics might be relevant (like physics problems). \n\nThe author response addressed all the reviewers' points. They restructured the paper to present contributions first, added examples for definitions, and included experiments as requested. They also discussed the brittleness of periodic points with bias terms and provided empirical results on classification error.\n\nNow, considering the standards of a top-tier conference. The paper seems to make a novel theoretical contribution by connecting DNN expressivity with Sharkovsky's theorem, which is a non-trivial link. The problem they're addressing (depth-width trade-offs) is fundamental in deep learning theory. The fact that they're building on Telgarsky's work and answering an open question is a big plus. The theoretical results are solid, as confirmed by Reviewer 2, and the proofs are correct. \n\nThe structure was an issue, but the authors fixed that by reorganizing the paper. The practical utility concern from Reviewer 3 is valid, but theoretical papers don't always need immediate practical applications. The authors provided context about how their work relates to natural functions (like those in physics) and added empirical experiments to give intuition, which helps bridge the theory-practice gap. \n\nThe experiments they added in response to Reviewer 2's question about classification error vs depth would help readers intuitively grasp the results. The bias term analysis also shows they considered potential extensions, even if it's not required for publication. \n\nNo major flaws are evident. The reviewers' criticisms were addressed, and the paper's contributions are significant. Since top conferences value both theoretical depth and clear exposition, the restructuring should meet those standards. The combination of a novel theorem application, addressing an open problem, and providing empirical validation (even synthetic) strengthens the case for acceptance.\n\nI don't see any red flags. The paper seems well-written after revisions, the contributions are clear, and the theoretical underpinnings are strong. The concerns raised were either addressed or are minor in the context of a top conference's expectations for theoretical work.\n</think>\n\nThe paper presents a novel and theoretically rigorous exploration of depth-width trade-offs in ReLU networks by leveraging Sharkovsky’s Theorem from dynamical systems. It addresses an open question posed by Telgarsky regarding the representational limitations of shallow networks for functions with specific periodic properties. The key contributions include:  \n1. **Theoretical Innovation**: A creative connection between neural network expressivity and Sharkovsky’s Theorem, which characterizes depth-width trade-offs based on periodic points. This advances the understanding of why certain functions (e.g., those with period-3 points) require deeper networks.  \n2. **Technical Soundness**: The proofs are deemed correct and well-structured by Reviewer 2, and the eigenvalue analysis of dynamical systems provides a solid foundation.  \n3. **Empirical Validation**: The authors addressed Reviewer 2’s request by adding experiments on synthetic datasets, demonstrating classification error trends with depth, which strengthens intuition and practical relevance.  \n4. **Clarity and Structure**: While Reviewer 3 initially criticized the structure, the authors reorganized the paper to prioritize contributions upfront, added examples for key definitions, and clarified intuition. This aligns with top-tier standards for readability and accessibility.  \n5. **Broader Impact**: The work provides a \"natural\" complexity measure (periodic points) for functions, moving beyond worst-case analyses. Though assessing periods for real-world tasks remains challenging, the authors highlight domains like physics where such dynamics are inherent, suggesting practical relevance.  \n\n**Addressed Concerns**:  \n- **Bias Terms**: The authors analyzed their impact on periodicity and provided concrete examples, showing theoretical robustness.  \n- **Usefulness**: While practical assessment of periods is difficult, the work offers a certificate-based framework akin to NP problems, emphasizing qualitative insights.  \n- **Empirical Intuition**: Added experiments and figures directly address Reviewer 2’s request, bridging theory and practice.  \n\n**Weaknesses**:  \n- The structural critique was valid but resolved through revisions.  \n- The practical utility of period-based bounds is acknowledged as an open challenge, but this is common in foundational theory.  \n\n**Conclusion**: The paper meets the high bar for a top-tier conference. It advances theoretical understanding of neural network expressivity, answers a significant open question, and is now well-structured and empirically grounded. The reviewers’ positive assessments, especially from Reviewer 2, and the thorough author responses confirm its merit.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about weakly-supervised trajectory segmentation for learning reusable skills. The goal is to extract sub-task primitives from demonstrations without needing detailed segmentation labels. The method uses multiple instance learning (MIL) with log-sum-exp pooling, and they test it on various environments. \n\nLooking at the reviewers' comments, there's a mix of concerns. Reviewer 1 says the framing and motivation don't align with the results. The paper claims to help with reusable skills, but the experiments don't show downstream tasks using those skills effectively. They mention that the method just labels each timestep without ensuring consistent segments, which is needed for training policies. Also, the experiments are limited in scope, with small numbers of skills and possible overfitting. The reviewer points out that the authors didn't compare with fully-supervised baselines, making it hard to gauge performance.\n\nReviewer 2 thinks the approach isn't novel enough since log-sum-exp pooling is used elsewhere. The results are underwhelming with low accuracy (35-60%) and the behavior cloning success rate is only 50%. They also note missing comparisons to important baselines like Policy Sketches and no oracle supervised results. The reviewer suggests the contributions are minor.\n\nReviewer 3 is concerned about the dataset requirements. They give an example where skills can't be learned if their co-occurrence is ambiguous. The classification accuracy is too low, and without confusion matrices or qualitative analysis, it's hard to trust the results. They also mention the dial experiment's smooth predictions might be due to post-processing, not the model's actual performance. Unsupervised clustering comparisons are missing.\n\nThe authors responded by addressing some points. They clarify that their supervision is weaker than Shiarlis et al. (no permutation info), which is a valid point. They mention Gaussian smoothing helps but didn't detail hyperparameters. They also say they'll add fully-supervised baselines and clustering comparisons. However, some issues remain, like the lack of downstream task experiments showing reusability of skills, which Reviewer 1 emphasized.\n\nThe key issues here are the lack of strong empirical validation for the main claims. The paper's main selling point is enabling reusable skills, but the experiments don't demonstrate that well. The accuracy numbers are low, and without seeing how the segmented skills actually help in new tasks, it's hard to see the practical value. The reviewers all point out missing baselines and insufficient analysis. The authors' responses address some points but don't fully resolve the concerns. For a top-tier conference, the work needs to show significant novelty and strong results. The method's novelty is questioned, and the results aren't compelling enough given the shortcomings. The experiments are limited in scope and don't fully validate the approach's effectiveness. \n\nConsidering all this, the paper probably doesn't meet the high bar for acceptance. The reviewers' concerns about novelty, experimental rigor, and lack of downstream validation are substantial. Even with the authors' responses, the paper's current state doesn't resolve these sufficiently. So, I think it should be rejected.\n</think>\n\nThe paper proposes a weakly-supervised trajectory segmentation method for learning reusable skills using multiple instance learning (MIL) with log-sum-exp pooling. While the problem of extracting reusable skills from demonstrations is important, the paper faces significant criticisms that undermine its suitability for a top-tier conference:\n\n### Key Concerns:\n1. **Misalignment Between Claims and Results**:\n   - The paper frames itself as enabling *reusable skills* for downstream tasks, but experiments lack evidence of this. Reviewer 1 notes that per-timestep labeling alone does not demonstrate reusability, and the authors only mention \"preliminary evidence\" of zero-shot transfer without rigorous evaluation. The absence of experiments like behavior cloning with segmented skills or meta-controller demonstrations weakens the claim of practical utility.\n\n2. **Novelty and Baseline Comparisons**:\n   - Reviewer 2 highlights that log-sum-exp pooling is not novel (used in prior work like semantic segmentation) and the method’s contributions are incremental. The lack of comparisons to fully-supervised baselines and critical baselines like Policy Sketches undermines the significance of the results. The authors’ response addresses some gaps (e.g., adding oracle baselines), but these are not yet present in the current submission.\n\n3. **Empirical Weaknesses**:\n   - **Low Accuracy**: Classification accuracies of 35–60% are insufficient for a top-tier venue, especially without analysis of confusion matrices or precision/recall. Reviewer 3 points out that these results might reflect random guessing in ambiguous cases (e.g., RoboSuite’s pick-and-place task).\n   - **Unconvincing Experiments**: The dial-Jaco video’s smooth predictions are attributed to Gaussian smoothing, raising questions about the model’s inherent performance. The absence of unsupervised clustering comparisons (Reviewer 3) and unclear architectural/hyperparameter details (Reviewer 1) further limit reproducibility and credibility.\n\n4. **Theoretical and Practical Limitations**:\n   - Reviewer 3’s dataset requirement example (e.g., skills 2 and 3 always co-occurring) demonstrates fundamental limitations in the approach’s applicability. The authors’ response sidesteps this by arguing that such ambiguity is inherent to the problem, but this does not address the lack of theoretical guarantees or practical guidance for dataset construction.\n   - The method’s reliance on post-processing (e.g., Gaussian smoothing, rejecting short segments) introduces ad-hoc fixes without systematic evaluation of their impact.\n\n5. **Downstream Task Evaluation**:\n   - The behavior cloning success rate of 50% is underwhelming, and the absence of success rates for baselines like CCNN (due to its collapse) calls into question the method’s robustness. The authors’ response does not provide a clear path to resolve this.\n\n### Author Responses:\nThe authors address some points (e.g., dataset requirements, Gaussian smoothing), but critical gaps remain. Their defense of weaker supervision (no permutation labels) is valid, but the lack of downstream task validation and rigorous comparisons negates the method’s practical impact. The proposed fixes (e.g., adding oracle baselines, clustering comparisons) are promising but not yet implemented in the current submission.\n\n### Conclusion:\nWhile the problem is valuable, the paper falls short in demonstrating the method’s novelty, robustness, and practical utility. The empirical results are underwhelming, and the lack of thorough comparisons and downstream validation undermines its contribution to the field. For a top-tier conference, the work requires stronger experimental rigor, clearer theoretical grounding, and more compelling evidence of real-world applicability.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about showing that SGD converges to a global solution in WGANs when the generator is a one-layer network. The reviewers have some concerns, so I need to weigh the pros and cons based on their feedback and the authors' responses.\n\nFirst, the main contribution the authors highlight is the first result on global convergence for non-linear generators using SGD-ascent in WGANs. That's a big deal because GANs are notoriously hard to analyze, especially with non-convex min-max problems. The reviewers acknowledge the technical merit but question the practical relevance because the discriminators used (linear and quadratic) are too simple compared to real WGAN setups which use neural networks.\n\nReviewer 1 points out that the quadratic discriminator might not satisfy the Lipschitz constraint properly unless data is bounded, which the authors say they'll address but haven't yet. Also, they want the authors to consider two-layer discriminators, which the authors argue would complicate things and increase sample complexity. The authors counter that their simple discriminators are sufficient for their generator's class and that using more complex ones isn't necessary here.\n\nReviewer 2 thinks the analysis is valid but the setup is too simplified. They mention that linear/quadratic discriminators don't capture the essence of WGANs, which rely on Lipschitz neural networks. The authors respond by saying their choice minimizes sample complexity and that their discriminator has enough power for their specific generator. They also reference prior work to back this up.\n\nReviewer 3 is concerned that the results are too weak (only joint distributions of two coordinates) and that the analysis can't extend to more complex discriminators. The authors clarify that their setup does recover the true distribution and that their focus is on the one-layer generator, which doesn't need a complex discriminator.\n\nThe authors' responses emphasize that their work is a first step, providing the first global convergence result for non-linear generators. They argue that their discriminator choices are intentional to simplify analysis without losing the ability to learn the target distribution. They also mention that future work could handle more complex discriminators but that's beyond the current scope.\n\nNow, considering top-tier conference standards, the paper needs to make a significant contribution. The first result on global convergence for non-linear GANs is a strong point. However, the limitations (simple discriminators, specific generator setup) are valid. But since the authors clearly state their assumptions and the theoretical contribution is novel, maybe it's still acceptable. The reviewers' main issues are about applicability and generality, but the paper's theoretical advance might outweigh that. The authors have addressed some points, like the Lipschitz constraint, and justified their choices with references. The title might be slightly overreaching, but the abstract clarifies it's for one-layer generators with specific discriminators. \n\nSo, balancing the contributions against the limitations, I think the paper should be accepted. It's a solid theoretical result that opens the door for future work, even if it's not immediately applicable to all WGAN cases. Top conferences do accept such foundational work even if they have simplifications.\n</think>\n\nThe paper presents a theoretical analysis of the convergence of stochastic gradient descent-ascent (SGD-Ascent) in Wasserstein GANs (WGANs) when the generator is a one-layer network. The authors claim this is the first result demonstrating global convergence to equilibrium points for non-linear generators in the GAN setting. While the technical contributions are novel and the analysis is rigorous, several critical concerns raised by reviewers must be carefully evaluated.\n\n### Key Strengths:\n1. **First-of-its-Kind Contribution**: The paper provides the first global convergence guarantee for SGD-Ascent in WGANs with non-linear generators, addressing a significant open problem in GAN theory. This is a major theoretical advance, particularly given the non-convex nature of the min-max problem.\n2. **Sample Complexity**: The analysis matches known parametric rates (e.g., \\(O(1/\\epsilon^2)\\)), demonstrating efficiency in learning the target distribution under their setup.\n3. **Clarity of Setup**: The authors explicitly define their generator and discriminator classes, ensuring their results are well-scoped and mathematically precise. Their focus on a simplified yet non-trivial generator (one-layer with non-linear activation) is justified as a foundational step.\n\n### Key Weaknesses:\n1. **Overly Simplified Discriminators**: \n   - The use of linear and quadratic discriminators deviates significantly from practical WGAN implementations, which rely on neural networks. Reviewers argue this undermines the relevance to real-world training dynamics.\n   - The authors counter that these discriminators are sufficient for their generator class and avoid excessive sample complexity. While valid, this limits the work’s applicability to more complex scenarios.\n2. **Limited Scope of Contributions**:\n   - The results apply only to a specific generator architecture (one-layer with a fixed activation function) and discriminators lacking interaction between coordinates (except in the quadratic case). The authors acknowledge this as a deliberate simplification but risk over-selling the contribution by titling it as a general result for \"one-layer networks in WGANs.\"\n   - The quadratic discriminator’s Lipschitz constraint justification (via bounded data) is incomplete, as reviewers note unbounded data is common in practice. The authors’ promise to address this in revisions was unfulfilled.\n3. **Terminology Ambiguity**: The term \"one-layer network\" is ambiguous (e.g., linear vs. single-hidden-layer). The authors clarify it aligns with prior work (e.g., Wu et al., 2019), but the title could mislead readers expecting broader applicability.\n\n### Author Responses:\nThe authors effectively address some concerns:\n- They clarify their contributions’ scope and emphasize the discriminators’ sufficiency for their generator class.\n- They justify the simplicity of discriminators as a deliberate trade-off to achieve global convergence guarantees and parametric sample complexity.\n- They reference prior work (Feizi et al., Bai et al.) to support their design choices, strengthening their argument that simpler discriminators are advantageous for their setup.\n\nHowever, unresolved issues remain:\n- The quadratic discriminator’s Lipschitz constraint explanation is still lacking.\n- The title and abstract could be misleading to practitioners expecting results for standard neural network discriminators.\n\n### Final Evaluation:\nWhile the paper’s limitations are substantial, its theoretical contribution is foundational and novel. The analysis of global convergence in a non-trivial GAN setting is a critical step forward, even if restricted to simplified components. Top-tier conferences often prioritize groundbreaking theoretical results over immediate practical applicability, especially when the work opens new avenues for future research (e.g., extending to more complex discriminators). The authors’ responses sufficiently clarify their contributions and limitations, mitigating concerns about overreach. The paper’s flaws are acknowledged but do not negate its value as a pioneering result.\n\n**Final Decision: Accept**"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification.\" The abstract mentions a modular framework for hierarchical reinforcement learning (HRL) where sub-skills are learned individually with diversity, then coordinated by a meta-policy. The tasks involve bimanual manipulation and multi-agent scenarios like two robots pushing a box. The authors claim their method outperforms baselines.\n\nLooking at the reviewer comments first. Reviewer 1 found the problem interesting and impactful but had several concerns: notation inconsistencies, T_low being 1 making skills seem like primitive actions, high variance in results, unclear success rates, and lack of discussion on recent multi-agent methods. They want clarifications but are open to increasing their score if addressed.\n\nReviewer 2 is confused about the novelty, especially regarding temporal abstraction (TA) in multi-agent vs single-agent settings. They question why DIAYN (a single-agent skill discovery method) is used here and whether the approach is just a straightforward extension. They also point out technical issues like the continuous latent variable learning and the fixed T_low design choice. Their main issue seems to be the lack of clear scientific insight and the method's reliance on predefined subtasks.\n\nReviewer 3 thinks the approach is good but weakly accepts it because it requires predefined subtasks. They suggest a baseline without subtasks (just DIAYN) to test the necessity of expert knowledge. They also note the lack of latent vector size info and the method's limitation in applicability due to needing pre-specified subtasks.\n\nThe author responses address these points. They added baselines (centralized/decentralized SBD) showing that without predefined subtasks, performance drops, implying expert knowledge is crucial. They clarified that their method is modular and can use any skill discovery method, not just DIAYN. They explained T_low as a design choice from prior HRL work, balancing simplicity and performance. They also addressed notation issues and provided more experimental details like the latent vector size (5) and success rates.\n\nNow, evaluating against top-tier standards. The problem is relevant and impactful for multi-agent and hierarchical RL, especially in complex manipulation tasks. The method introduces a modular framework that separates skill learning and coordination, which is a solid contribution. The experiments show better performance than baselines, and the added baselines (centralized SBD, etc.) strengthen their claims.\n\nHowever, the reviewers' concerns were valid. The high variance in initial results was addressed by adding more seeds and moving averages. The predefined subtasks are a limitation, but the authors argue that complex tasks require this prior knowledge, and their experiments support that. The T_low choice is a trade-off, and they provided analysis showing smaller T_low works better for their tasks. The use of DIAYN is justified as it's a standard method, and their approach is modular.\n\nThe main issues seem adequately addressed in the author responses. The paper's contributions are clear: a framework for coordinating pre-trained skills in multi-agent settings, which is novel and addresses a gap noted by reviewers (no prior work on coordinating pre-trained skills). The experiments cover both manipulation and locomotion tasks, showing versatility.\n\nThe weaknesses are the dependency on predefined subtasks and the fixed T_low. But these are acknowledged, and the authors provide reasoning. Since top-tier conferences value both innovation and thorough evaluation, the paper's contributions outweigh the limitations, especially with the added experiments and clarifications.\n\nFinal Decision: Accept\n</think>\n\nThe paper proposes a modular hierarchical reinforcement learning framework for coordinating skills in multi-agent systems, addressing the challenge of temporal and behavioral coordination between pre-trained sub-skills. The method combines skill behavior diversification (via DIAYN) with a meta-policy to adapt these skills for collaborative tasks. The experiments demonstrate success in complex scenarios like bimanual manipulation and multi-agent pushing, outperforming baselines. \n\n**Strengths:**\n1. **Novelty and Relevance:** The paper tackles a gap in multi-agent coordination by focusing on leveraging pre-trained skills, which is a critical problem in robotics and cooperative tasks. The modular approach allows for scalable skill composition, which is a meaningful contribution to hierarchical and multi-agent RL.\n2. **Empirical Validation:** The experiments cover diverse tasks (manipulation, locomotion) and show clear improvements over baselines. Added baselines (centralized/decentralized SBD) and analysis (e.g., T_low sensitivity) strengthen the results.\n3. **Technical Soundness:** The method builds on established techniques (DIAYN, HRL) but integrates them in a novel way for multi-agent coordination. The authors address reviewer concerns about predefined subtasks by demonstrating their necessity for complex tasks and clarify design choices like T_low.\n\n**Weaknesses:**\n1. **Dependency on Predefined Subtasks:** The method requires prior knowledge of subtasks, limiting its applicability to tasks where such decomposition is feasible. While the authors argue this is necessary for complex interactions, it is a valid limitation.\n2. **Fixed T_low:** The choice of a fixed horizon for low-level skills is a design trade-off. However, the authors provide justification and analysis, showing smaller T_low values work better for their tasks, which mitigates this concern.\n3. **Notation/Clarity Issues:** Initially unclear notation and figures were addressed in revisions, improving readability.\n\n**Reviewer Responses:** The authors effectively addressed key concerns: they added critical baselines, clarified technical points (e.g., latent variable learning, T_low), and provided additional experiments to reduce variance. These responses strengthen the paper’s rigor and clarity.\n\n**Conclusion:** The paper’s contributions—novel framework for coordinating pre-trained skills, empirical validation across tasks, and clear addressing of reviewer feedback—outweigh its limitations. It advances the state of the art in multi-agent hierarchical RL and meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification.\" The abstract mentions a modular framework for hierarchical reinforcement learning (HRL) where sub-skills are learned individually with diversity, then coordinated by a meta-policy. The tasks involve bimanual manipulation and multi-agent scenarios like two robots pushing a box. The authors claim their method outperforms baselines.\n\nLooking at the reviewer comments, there are three reviewers. Let's break down their points.\n\nReviewer 1 is generally positive but has several concerns. They mention notation inconsistencies, the choice of T_low=1 making skills seem like primitive actions, high variance in results, unclear success rates vs. episode rewards, and lack of discussion on recent multi-agent methods. The author response addressed these by adding more baselines, clarifying T_low's role, explaining the latent vector size, and adding success rate plots. They also ran more seeds to reduce variance. The reviewer seems satisfied if concerns are addressed, which they were.\n\nReviewer 2 is more critical. They question the novelty, especially regarding whether temporal abstraction (TA) in multi-agent is different from single-agent. They also ask why DIAYN was chosen over other methods and challenge the need for predefined subtasks. The authors clarified that their method is orthogonal to skill discovery methods and that DIAYN was chosen for its controllability. They also addressed the predefined subtasks by adding baselines showing that without them, performance drops. The reviewer's key questions were addressed, though I wonder if the authors sufficiently justified the necessity of predefined subtasks beyond just showing DIAYN alone fails.\n\nReviewer 3 gives a weak acceptance, noting the method is simple and scalable but limited by requiring predefined subtasks. They suggest testing DIAYN without subtasks, which the authors did, showing it doesn't work. They also asked about the latent vector size, which was added. The author response seems to have mitigated these concerns by adding experiments and clarifications.\n\nNow, considering the author responses: they addressed notation issues, clarified T_low's design choice with experiments, added baselines, and provided more details on hyperparameters. The main criticisms were about the predefined subtasks and the multi-agent context. The authors argue that prior work doesn't handle coordination of pre-trained skills, which is a valid point. The experiments show their method works better than alternatives, even if it requires some prior knowledge.\n\nTop-tier conferences usually require significant novelty, impact, and thorough evaluation. The paper's contribution is a modular framework for coordinating pre-trained skills in multi-agent settings, which seems novel. The experiments cover several tasks and show improvement over baselines. The predefined subtasks are a limitation, but the authors argue it's necessary for complex tasks and demonstrate that without them, performance is poor. The reviewer concerns were mostly addressed through revisions.\n\nPotential issues: The dependency on predefined subtasks might limit applicability, but the paper's focus is on coordination given those skills, which is still a valid problem. The high variance was addressed by more seeds and moving averages. The multi-agent discussion was expanded in the response.\n\nOverall, the paper seems to meet the standards. The authors have adequately addressed the reviewers' concerns, and the method shows promise in a challenging problem area. The limitations are acknowledged, and the experiments support the claims. Therefore, I think it should be accepted.\n</think>\n\nThe paper proposes a modular hierarchical reinforcement learning framework for coordinating skills in multi-agent or multi-end-effector systems. The core idea involves pre-training diverse sub-skills with DIAYN-style behavior diversification and then learning a meta-policy to coordinate these skills. The method is evaluated on challenging manipulation and locomotion tasks, demonstrating superior performance over baselines. \n\n**Strengths:**\n1. **Novelty and Problem Relevance:** The work addresses the coordination of pre-trained skills in multi-agent settings, a problem under-explored in prior work. The authors highlight that existing methods either lack temporal abstraction or struggle with complex cooperative tasks, positioning their approach as a meaningful contribution.\n2. **Empirical Validation:** The experiments cover diverse tasks (e.g., bimanual manipulation, Ant pushing), and the results show clear improvements over baselines. The addition of DIAYN-only baselines (as suggested by reviewers) strengthens the argument for the necessity of predefined subtasks in complex scenarios.\n3. **Clarity and Response:** The authors addressed reviewer concerns effectively, clarifying notation, providing additional experiments (e.g., varying T_low, success rates), and justifying design choices. They also expanded discussions on related work and method limitations.\n\n**Weaknesses:**\n1. **Dependency on Predefined Subtasks:** The method requires prior knowledge to define subtasks, limiting its applicability to tasks where such decomposition is feasible. While the authors argue this is necessary for complex tasks and demonstrate that unsupervised skill discovery (DIAYN alone) fails, this remains a notable limitation.\n2. **Fixed T_low:** The choice of T_low=1 (treating skills as primitive actions) was criticized, but the authors provided a rationale and experiments showing its effectiveness for behavioral coordination. While this choice is defensible, it reduces the method’s flexibility compared to variable-horizon approaches.\n\n**Reviewer Concerns Addressed:**\n- Notation inconsistencies and figure clarity were improved.\n- The high variance in results was mitigated with more seeds and moving averages.\n- The role of T_low and predefined subtasks was thoroughly explained, with experiments supporting their necessity.\n- Multi-agent baselines and related work were expanded to contextualize contributions.\n\n**Conclusion:** The paper advances the state of the art in coordinating pre-trained skills for multi-agent systems, addresses key criticisms effectively, and provides rigorous empirical validation. While limitations exist (e.g., predefined subtasks), the problem tackled is significant, and the solution is both novel and impactful for complex manipulation tasks. The improvements and clarifications in the author response sufficiently address reviewer concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space.\" The keywords are adversarial defense, auto-encoder, adversarial attack, GAN. The abstract mentions they propose AE-GAN+sr, which uses an auto-encoder with adversarial loss to form a latent space, then searches in that space during inference to find the closest natural image. They claim it's robust with lower computation than Defense-GAN.\n\nLooking at the reviewer comments:\n\nReviewer 1 says the method is an alternative to Defense-GAN, which is computationally expensive. The proposed method has lower accuracy but better performance with fewer iterations. They mention experiments on MNIST and F-MNIST, but the accuracy trade-off is noted.\n\nReviewer 2 points out the contribution is incremental. They want comparisons with state-of-the-art defenses like PGD adversarial training and TRADES. Also, mentions typos and exceeding page limits. The figure (Fig.4) is unclear regarding false alarms. The reviewer thinks the contribution isn't strong enough for a top conference.\n\nReviewer 3 is concerned about the autoencoder's detection mechanism being susceptible to white-box attacks and the model's performance on more complex datasets. They also question the latent distribution alignment between F(x) and p(z). The performance on stronger vs weaker attacks is confusing.\n\nReviewer 4 says the writing is hard to follow, experiments aren't convincing enough, and they want results on CIFAR-10. They also ask for clarification on the encoder-assisted search steps and how it compares to BiGAN.\n\nAuthor responses addressed some points. They added comparisons with PGD adversarial training, showing their method is better on some attacks but worse on BPDA. They included CelebA results but not CIFAR-10 yet. They tried to align latent distributions but couldn't. They clarified the search process steps (15 steps, pre-trained models) and detection mechanism effectiveness.\n\nNow, considering all this:\n\nStrengths:\n- The method reduces computational cost compared to Defense-GAN, especially with fewer iterations.\n- They show some improvement over Defense-GAN and PGD in certain attack scenarios (like FGSM, PGD on MNIST).\n- Added experiments on CelebA, addressing some reviewer concerns about dataset complexity.\n- Addressed typos and improved clarity in the response.\n\nWeaknesses:\n- The contribution is incremental over existing work like Defense-GAN and adversarial training methods. The core idea of using an autoencoder with GAN components and latent space search isn't entirely novel.\n- The comparison with state-of-the-art defenses like TRADES wasn't fully addressed (only PGD was added, not TRADES).\n- The paper initially had writing issues and exceeded page limits, though authors claim they fixed typos and met limits.\n- Performance on more complex datasets like CIFAR-10 isn't shown yet, which is a concern for scalability.\n- The detection mechanism's vulnerability to white-box attacks was addressed, but there's still a reliance on the autoencoder's robustness.\n- The latent distribution issue wasn't resolved, which could affect performance in some cases.\n- The BPDA attack shows their method isn't foolproof, and adversarial training (PGD) still outperforms in some scenarios.\n\nTop-tier conferences usually require significant contributions, clear writing, thorough experiments, and strong results. Here, the method is an improvement but not a major breakthrough. The experiments are on smaller datasets (MNIST, F-MNIST, CelebA) and lack results on more challenging datasets like CIFAR-10. The incremental nature and unresolved issues (like latent distribution alignment) might not meet the high bar. However, the authors did address some concerns in their response, but maybe not enough.\n\nThe reviewers' main points against acceptance were the incremental contribution, lack of SOTA comparisons, and insufficient experiments. The authors added some comparisons and CelebA results but not CIFAR-10. The writing issues were acknowledged but not fully resolved in the response. The core idea is a good step but not groundbreaking for a top conference.\n\nFinal Decision: Reject\n</think>\n\nThe paper proposes AE-GAN+sr, an adversarial defense method that combines auto-encoders and GANs to reduce computational costs compared to Defense-GAN. While the method shows improvements in efficiency and performance over Defense-GAN in certain scenarios, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Incremental Contribution**: The core idea builds on existing frameworks (Defense-GAN, adversarial training) without a transformative innovation. The reduction in computational cost is notable but insufficient for a top venue, especially given unresolved limitations (e.g., vulnerability to BPDA attacks).\n\n2. **Incomplete Comparisons**: While the authors added comparisons with PGD adversarial training, they did not address TRADES as requested. The lack of results on CIFAR-10 limits confidence in scalability and real-world applicability.\n\n3. **Technical Limitations**: \n   - The latent distribution alignment issue remains unresolved, potentially affecting robustness.\n   - The detection mechanism’s susceptibility to white-box attacks was partially addressed but not fully mitigated.\n   - Performance on stronger attacks (e.g., BPDA) lags behind adversarial training methods like PGD.\n\n4. **Experimental Rigor**: Experiments are limited to MNIST, F-MNIST, and CelebA. Without results on more complex datasets (e.g., CIFAR-10), the method’s broader utility is unproven.\n\n5. **Writing and Presentation**: Despite revisions, initial writing issues and page limit overruns suggest the paper requires substantial polishing, which may detract from clarity and impact.\n\nWhile the paper contributes a useful refinement to adversarial defense, its incremental nature, unresolved technical challenges, and insufficient experimental validation fall short of the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Differentiable Hebbian Consolidation for Continual Learning,\" and the abstract mentions addressing catastrophic forgetting using a method inspired by Hebbian learning and the complementary learning system (CLS). They propose a DHP Softmax layer that combines slow and fast weights. The experiments include standard benchmarks like Permuted MNIST, Split MNIST, and a new imbalanced variant. They also mention a 5-Vision dataset mixture. The authors claim their method outperforms baselines without extra hyperparameters.\n\nLooking at the reviewer comments:\n\nReviewer 1 points out that while the method does okay, it's outperformed by Synaptic Intelligence (SI) on Split CIFAR-10/100 except the final split. They want stronger empirical evaluation, maybe combining with other methods. The author response added some experiments on Split CIFAR but admits SI still does better alone. They argue their method works well with SI, but the reviewer is concerned about standalone performance.\n\nReviewer 2 found the idea novel but had trouble with the technical details. They asked for clarifications on equations and figures, which the authors addressed in their response. The motivation for applying Hebbian only to the softmax was explained, and hyperparameters were clarified. The method seems better when combined with others, but the reviewer wanted more intuitive explanations on how it complements EWC/SI.\n\nReviewer 3 was positive overall but wanted comparisons to other CLS-based methods and more datasets like CIFAR. The authors added experiments on Split CIFAR but didn't compare directly with other CLS approaches beyond mentioning Hebbian Softmax. They also addressed the notation issues and figure requests.\n\nReviewer 4 noted that while the method is well-motivated, the results on challenging datasets are weak. DHP alone lags behind SI, and combining it doesn't surpass SI much. They suggested testing on larger datasets like CIFAR-100 or CORe50. The authors added Split CIFAR results but acknowledge SI still performs better.\n\nThe author responses addressed some points: added experiments, clarified equations, and explained why certain baselines weren't compared. They emphasized that their method is an alternative direction in synaptic plasticity and showed improvements over finetuning, especially when combined with consolidation methods.\n\nNow, considering top-tier standards. The paper introduces a novel approach inspired by neurobiology, which is a plus. The experiments cover several benchmarks, including a new imbalanced one, which is good. However, the main issue is performance: on key benchmarks like Split CIFAR-10/100, SI outperforms DHP alone. The authors argue that combining with SI helps, but the reviewer points out that the combined approach doesn't surpass SI alone much. This suggests limited added value.\n\nThe method's simplicity and lack of hyperparameters are strengths. The theoretical grounding in CLS and Hebbian learning is solid. But the empirical results, while showing some improvement over finetuning, don't convincingly outperform state-of-the-art methods like SI on critical datasets. The authors added experiments but reviewers still feel the results are underwhelming on more challenging tasks.\n\nThe paper's contributions are incremental but novel in applying Hebbian principles to continual learning. The lack of strong performance on established benchmarks might be a barrier for a top conference. However, the method's unique angle and potential for future work could be a plus. The reviewers' concerns about empirical evaluation and comparisons to other CLS methods are valid. The authors addressed some but not all (like direct comparisons to other CLS approaches beyond Hebbian Softmax).\n\nFinal thoughts: The paper has merit in its novel approach and clear motivation. However, the empirical results, especially on more challenging datasets, don't demonstrate a significant advantage over existing methods. The combination with SI doesn't surpass SI alone, which weakens the case. For a top-tier conference, the results need to be stronger to justify acceptance. The authors did improve the experiments but the reviewers' points about underperformance remain. Therefore, it's a borderline case but leaning towards rejection unless the contributions are deemed sufficiently impactful despite the empirical shortcomings.\n</think>\n\nThe paper proposes a novel approach to continual learning inspired by Hebbian plasticity and the complementary learning system (CLS), introducing the Differentiable Hebbian Consolidation (DHC) framework. The method combines slow and fast weights in the softmax layer to mitigate catastrophic forgetting, demonstrating improvements over finetuning on several benchmarks. However, the empirical results reveal limitations: \n\n1. **Performance on Key Benchmarks**: While DHC outperforms finetuning, it underperforms against established methods like Synaptic Intelligence (SI) on critical datasets such as Split CIFAR-10/100. Even when combined with SI, the results do not significantly surpass SI alone, raising questions about the method's standalone utility and added value. \n\n2. **Comparisons to CLS-Based Methods**: The authors address reviewer concerns by adding experiments and clarifying technical details but do not provide direct comparisons to other CLS-inspired approaches (e.g., exact/generative replay methods), which often outperform non-replay methods. This gap weakens the paper's empirical validation.\n\n3. **Theoretical and Practical Contributions**: The work is theoretically grounded in neurobiological principles and offers a simple, hyperparameter-free implementation. The introduction of an imbalanced Permuted MNIST variant is a positive contribution. However, the method’s performance on more complex, real-world datasets (e.g., CORe50, ILSVRC) remains untested, limiting its demonstrated scalability.\n\n4. **Clarity and Reproducibility**: The authors addressed reviewer concerns about notation and figures, improving clarity. However, the lack of a direct comparison to CLS-based baselines and the modest performance gains on challenging benchmarks reduce confidence in the method’s broader impact.\n\n**Final Decision: Reject**  \nWhile the paper introduces an interesting neuro-inspired approach and shows promise in certain scenarios, its empirical results on established benchmarks are insufficient to justify acceptance at a top-tier conference. The method does not convincingly outperform state-of-the-art techniques like SI, and the lack of strong performance on more complex datasets undermines its practical relevance. Further experiments and comparisons are needed to establish its value in the continual learning landscape."
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning to Transfer via Modelling Multi-level Task Dependency,\" and the keywords are multi-task learning and attention mechanism. The abstract mentions that existing methods assume tasks are related, but real-world tasks might not be, so they propose a framework using attention to model dependencies at multiple levels. The experiments show improvements over current methods.\n\nNow, looking at the reviewer comments. Reviewer 1 points out grammar issues and that the distinction between general and data dependency isn't clear enough. They also note the improvements are slight compared to SOTA. Reviewer 2 wants better motivation, clarification on some equations, and additional experiments, like a baseline without attention. They mention the paper is hard to follow and has notation issues. Reviewer 3 says the claim that most methods assume tasks are correlated is incorrect, and the model has too many parameters. They also note lack of comparison to related attention-based works. Reviewer 4 is critical, pointing out that the Taskonomy paper already did similar things, missing details in the method, and confusing notation. They think the paper has major issues and needs significant revisions, especially regarding prior work.\n\nThe author responses address some points. They clarify that their work handles hierarchical data (text/graph) by adding data-specific dependencies, which Taskonomy didn't. They mention parameter reduction via decomposition and interaction between tasks. They also address some notation and experimental setup concerns, adding a baseline without attention and fixing some grammar issues.\n\nHmm, so the key issues here are:\n\n1. **Prior Work Overlap**: Reviewer 4 says Taskonomy already models task relationships, which the authors acknowledge but argue their approach handles hierarchical data and reduces parameters. The authors' response explains their differences, but I need to assess if that's sufficient. If Taskonomy is indeed similar, the novelty might be in extending it to text/graph with hierarchical dependencies. But does that make the contribution strong enough?\n\n2. **Clarity and Presentation**: Multiple reviewers mention the paper is hard to follow, with grammar issues and unclear explanations. The authors revised grammar and added details, but if the core ideas are still not clearly presented, that's a problem for a top conference.\n\n3. **Experimental Validation**: Reviewer 2 and 4 wanted comparisons with specific methods like End-to-End Multi-Task Learning with Attention. The authors added a baseline without attention and addressed some experimental setup concerns. However, if the experiments don't include key baselines or show significant improvements, that's an issue.\n\n4. **Technical Soundness**: Reviewer 4 points out notation confusion and missing details in the method. The authors clarified some parts but if the core method (like the attention mechanism and parameter decomposition) isn't clearly explained, it might still be a problem.\n\n5. **Contribution Significance**: The authors argue their method handles hierarchical data better and reduces parameters. If these are valid and novel contributions, that's a plus. But if Taskonomy already covers the task dependency part, then the extension to text/graph might be the main contribution. However, the reviewers are concerned about the significance, especially if the improvements are marginal.\n\nConsidering all this, the authors have addressed some points but major issues remain. The prior work issue with Taskonomy is critical. If their work is just an extension but not a substantial improvement, it might not meet the bar for a top conference. Also, the clarity and presentation issues, even after revisions, might still hinder understanding. The experiments added a baseline, but if the comparisons with Taskonomy and other attention-based methods are missing or not thorough, that's a problem. The parameter reduction and handling hierarchical data could be contributions, but the reviewers' concerns about novelty and clarity need to be resolved.\n\nThe final decision hinges on whether the authors sufficiently addressed the prior work concerns and clarified their contributions. Since Reviewer 4 still thinks the issues are major even after rebuttal, and the other reviewers also have valid points about experiments and clarity, I'm leaning towards rejection unless the contributions are clearly novel and the paper is significantly improved. Since the rebuttal didn't fully resolve the prior work overlap and clarity, especially regarding Taskonomy, I think the paper isn't ready for a top-tier conference yet.\n</think>\n\nThe paper proposes a multi-task learning framework that models both general and data-specific task dependencies using attention mechanisms, aiming to address limitations in existing methods that assume strong task correlations. While the authors highlight contributions such as handling hierarchical data (text/graph) and parameter reduction, several critical issues remain unresolved:\n\n1. **Prior Work Overlap**: Reviewer 4 emphasizes that the core idea of modeling task relationships (e.g., via a task dependency matrix) is already addressed in Taskonomy (2018). The authors argue their work extends this to hierarchical data and reduces complexity, but the rebuttal does not convincingly demonstrate that these extensions are novel or sufficiently impactful for a top-tier conference. The distinction from Taskonomy and other cited works (e.g., [2], [3]) remains unclear, especially regarding the claimed \"multi-level\" dependency.\n\n2. **Clarity and Presentation**: Multiple reviewers note significant readability issues, including grammatical errors, confusing notation (e.g., in Equations 2–4), and lack of methodological details (e.g., the \"Transfer Block\" and \"Position-wise Mutual Attention Mechanism\"). While the authors addressed some points, the rebuttal does not resolve ambiguities in the core method’s design or its mathematical formulation, which are critical for reproducibility and technical evaluation.\n\n3. **Experimental Rigor**: Key comparisons with relevant baselines (e.g., End-to-End Multi-Task Learning with Attention) are missing. The added baseline without attention is a step forward, but the experiments lack thorough validation against Taskonomy and other attention-based methods. The authors’ claim of improved performance under low-labeled data is insufficiently supported without explicit comparisons or ablation studies.\n\n4. **Technical Soundness**: The parameter reduction via decomposition (from O(T²) to O(T)) is a potential strength, but the rebuttal does not clarify how this interacts with the hierarchical data structure or whether it introduces new limitations (e.g., loss of task-specific nuances). The hierarchical justification (e.g., word vs. sentence dependencies) is weakly motivated and not empirically validated in the provided results.\n\n**Final Decision: Reject**  \nThe paper’s contributions are incremental relative to existing work (e.g., Taskonomy), and unresolved issues in clarity, experimental rigor, and novelty prevent it from meeting the standards of a top-tier conference. Significant revisions would be required to address prior work gaps, clarify technical details, and provide stronger empirical validation."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning DNA folding patterns with Recurrent Neural Networks.\" The keywords include machine learning, RNNs, 3D chromatin structure, TADs, and computational biology. The abstract mentions using ML methods like linear models, gradient boosting, and RNNs (specifically bidirectional LSTM) to predict chromatin folding patterns from epigenetic marks. They claim the LSTM outperforms others and that the memory aspect is key. They also mention identifying informative features, which they link to biological significance.\n\nNow, looking at the reviewer comments. There are three reviewers. Let's go through each one.\n\nReviewer 1 points out that the improvement might be due to the new loss function (weighted MSE) rather than the LSTM itself. They suggest a fair comparison would use the same loss for other models or test a vanilla LSTM. Also, they question the generalizability of the loss function (why 11?) and whether it works on other datasets or features like ATAC-seq.\n\nReviewer 2 has several minor issues: unclear equation (K?), wording on wMSE, citation error, figure labels, and lack of discussion comparing regression vs. NNs. They also note that Figure 6's training vs test errors need explanation. The major point here is the lack of clarity in some parts and the need for more discussion on model choices.\n\nReviewer 3 is more critical. They argue the paper lacks methodological novelty, unclear data/methods description, and weak evaluation. They mention similar prior work not cited, unclear features (binary vs. continuous?), why bidirectional LSTM was chosen (connection to DNA's double-strandedness?), using the same loss for all models, feature importance analysis for LSTM, hyperparameter tuning details, and why the center LSTM state was used instead of concatenating hidden states. They also suggest comparing with other architectures like CNNs.\n\nThe authors responded to these points. They clarified the wMSE (parameter K is number of input objects), renamed it to modified wMSE, fixed citations, rephrased sections, explained the bidirectional LSTM choice (DNA's direction independence), addressed the 11 in the loss (related to gamma transitional distribution), added more metrics, and mentioned that all models used the same loss. They also addressed some of the data representation issues, but some points like hyperparameters and feature importance analysis remain as future work.\n\nNow, considering the standards of a top-tier conference. Such venues usually require strong methodological contributions, clear and thorough evaluations, and a significant contribution beyond existing work. Let's assess each criterion.\n\nMethodological Novelty: The authors use bidirectional LSTM for predicting TADs from ChIP-seq data. Reviewer 3 points out that similar work exists but wasn't cited. The authors responded by adding citations but argue their focus is different (predicting TAD characteristics from ChIP-seq instead of reconstructing 3D structures). However, if prior work exists on predicting TADs using similar methods, that's a problem. The authors claim no other work does this, but if that's not true, it's an issue. The response mentions they didn't find full papers in main tracks, but workshop papers might still be relevant. The lack of proper literature review could weaken the novelty claim.\n\nClarity and Data Description: Reviewers 1 and 3 noted unclear feature representation (binary vs. continuous, 20kb meaning). The authors clarified that features are continuous (mean over 20kb bins), but the initial description was unclear. The bidirectional LSTM's rationale was improved but still hinges on DNA's directionality, which might not be directly tied to the model's bidirectional aspect (since the input is features, not raw DNA sequence). The explanation about using bidirectional to capture both sides of a central bin makes sense, but the connection to DNA's double-strandedness is a bit tenuous.\n\nEvaluation: Reviewer 1 and 3 pointed out that the loss function (wMSE) might be the reason for LSTM's success, not the model itself. The authors state all models used the same loss, which addresses that concern. They added more metrics (MSE, MAE, R²), which is good. However, they didn't compare with other architectures like CNNs, which was a point raised. The response mentions that other models (linear, GBR) were included, but CNNs weren't. Also, the hyperparameter tuning details are in the methods but perhaps not detailed enough. The discussion on why regression vs. NNs was expanded, which helps.\n\nGeneralizability: Reviewer 1 asked about other datasets and features. The authors responded that they focused on Drosophila and ChIP-seq because mechanisms might differ, but this limits the paper's broader impact. A top-tier conference might expect some exploration of generalizability, even if limited.\n\nTechnical Soundness: The model's use of bidirectional LSTM seems reasonable for sequential data. The loss function's parameter (11) was explained as related to the gamma transitional distribution, which is a valid approach. However, the choice of 11 isn't derived theoretically but empirically, which is okay but should be clearer.\n\nOriginality: If the problem (predicting TADs from ChIP-seq with RNNs) hasn't been done before, that's a plus. But if similar work exists, it's a minus. The authors addressed this by adding citations but their defense about workshop papers might not hold if those works are relevant.\n\nOverall, the paper has some merit in applying RNNs to this problem, but there are significant issues. The main concerns are:\n\n1. **Lack of Methodological Novelty**: If prior work exists (even in workshops) that does similar things, this weakens the contribution. The authors didn't adequately address this, just citing that those works weren't in main tracks. However, top conferences expect thorough literature reviews.\n\n2. **Evaluation Shortcomings**: Not comparing with CNNs or other sequential models, and the hyperparameter details are sparse. The loss function's impact wasn't fully isolated (though the authors say all models used it).\n\n3. **Clarity Issues**: The initial paper had unclear descriptions of features and model rationale, which were addressed in the response but might still be present in the revised version. The bidirectional LSTM's justification could be better.\n\n4. **Generalizability**: The authors limited their scope, which is okay, but a top conference might expect more exploration or at least a discussion of potential limitations.\n\n5. **Technical Soundness**: The loss function's parameter (11) is explained, but the choice isn't theoretically grounded. The use of center hidden state instead of concatenation was addressed but might still be a point of contention.\n\nConsidering all this, the paper has potential but falls short in several areas critical for a top-tier conference. The lack of proper literature review and comparison with relevant prior work, insufficient evaluation (e.g., no CNN baseline), and clarity issues in the original submission are significant drawbacks. While the authors addressed some points in their response, the remaining concerns (like novelty and thorough evaluation) might not be sufficient to meet the high standards of a top conference. Therefore, I lean towards rejection unless the revised version fully addresses all these points, which isn't clear from the provided responses.\n</think>\n\nThe paper proposes using bidirectional LSTM RNNs to predict chromatin folding patterns (TADs) in Drosophila melanogaster from epigenetic features (ChIP-Seq data). While the work addresses an important problem in computational biology and demonstrates improved performance over baseline models, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novel Application**: The focus on predicting TAD characteristics from ChIP-Seq data (rather than reconstructing 3D structures) is a niche area, and the authors claim no prior work exists in this exact formulation.\n2. **Technical Contribution**: The bidirectional LSTM architecture is justified as leveraging DNA's sequential and directional-agnostic properties, and the modified weighted MSE loss (wMSE) is tailored to the problem.\n3. **Evaluation**: The authors added more metrics (MSE, MAE, R²) and clarified that all models used the same loss, addressing some reviewer concerns.\n\n### Major Concerns:\n1. **Lack of Methodological Novelty**:\n   - Reviewer 3 and others highlighted that similar approaches exist in prior work (e.g., workshop papers), which the authors dismissed due to publication venue. However, top-tier conferences require thorough literature reviews, and omitting relevant prior work weakens the novelty claim.\n   - The bidirectional LSTM's rationale (DNA's double-strandedness) is tenuously connected to the model's design, as the input features are aggregated over 20kb bins, not raw DNA sequences.\n\n2. **Insufficient Evaluation**:\n   - No comparison with convolutional neural networks (CNNs) or other sequential models (e.g., transformers), which are standard baselines for spatial/sequential data.\n   - Hyperparameter tuning details remain sparse (e.g., how were LSTM units or sequence lengths selected?).\n   - The loss function's parameter (11) is empirically derived but not theoretically grounded, and its generalizability to other datasets or features (e.g., ATAC-Seq) is untested.\n\n3. **Clarity and Reproducibility**:\n   - Initial descriptions of input features (e.g., \"20kb\" resolution, binning process) were unclear, though clarified in the response. However, the revised version's clarity is not fully confirmed.\n   - The justification for using the center LSTM hidden state instead of concatenating forward/backward states remains underexplained.\n\n4. **Scope Limitations**:\n   - The authors restrict their analysis to Drosophila and ChIP-Seq data, arguing biological mechanisms differ across organisms/cell types. While valid, this limits the paper's broader impact and generalizability, which are expected at top conferences.\n\n### Conclusion:\nWhile the paper addresses an interesting problem and demonstrates technical competence, its lack of thorough literature review, insufficient methodological novelty, and incomplete evaluation (e.g., missing baselines, unclear hyperparameter details) prevent it from meeting the high standards of a top-tier conference. The authors' responses partially address these issues but do not resolve all concerns, particularly regarding prior work and model justification.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\". The keywords are Reinforcement Learning, MCTS, and AlphaZero. The abstract mentions that they're looking at the coordination of episode generation using MCTS to handle exploration-exploitation trade-offs, tested on a small problem (like Tic-Tac-Toe) showing better robustness than AlphaZero. \n\nNow, looking at the reviewer comments. All three reviewers are pretty harsh. Reviewer 1 says the paper is \"less than half-baked\", no references, poorly written. They suggest resubmitting when it's more complete, starting with related work and references. Reviewer 2 also points out bad writing, grammatical issues, and that they couldn't understand the abstract or the main points. They mention the paper has inaccurate statements and lacks scientific writing elements. Reviewer 3 says it's very poorly written with incomprehensible passages, no references, undefined math symbols, and structural issues like blank spaces and hard-to-follow algorithms. They think it's not a serious academic work but just notes, needing a lot of work even for any venue.\n\nThe author's response addresses some points. They apologize for the writing issues, mention they couldn't use references because of .bib file issues, and express a desire to improve. They also mention finding a similar idea in their country but not internationally, and they believe there's room for improvement over AlphaZero.\n\nHmm. The main issues here are the writing quality and lack of references. All three reviewers agree the paper is poorly written, with grammatical errors, unclear explanations, undefined terms, and structural problems. The absence of references is a big red flag because academic papers must situate their work within existing literature. The author admits to removing references due to technical issues with .bib, which is a problem they should have resolved before submission. \n\nThe content itself might have potential—improving MCTS in RL, especially for exploration-exploitation, is a valid area. The experiments on a small problem (Tic-Tac-Toe) showing robustness compared to AlphaZero could be interesting. But without clear writing and proper references, the ideas aren't communicated effectively. The reviewers couldn't even understand the method, which is critical for evaluation. \n\nThe author's response shows they recognize the writing issues and want to improve, but for a top-tier conference, the paper needs to be polished and rigorous now. The lack of references and poor structure might indicate that the work isn't yet mature enough. Also, the fact that the author found similar work after submission but didn't cite it suggests possible oversight in literature review, which is essential. \n\nTop conferences have high standards. Even if the idea is good, the execution here is lacking. The reviewers' consensus is that the paper isn't ready for publication. The author needs to do significant revisions, including proper literature review, fixing writing, and clarifying the method. Since the current version is so poorly presented, it's unlikely to meet the standards required for acceptance. \n\nSo, the decision should be to reject. The paper needs substantial work beyond minor revisions, and as it stands, it doesn't meet the quality expected for a top conference.\n</think>\n\nThe paper under review faces significant and fundamental issues that preclude its acceptance at a top-tier conference. Here is the detailed analysis:\n\n### Key Issues Identified:\n1. **Poor Writing Quality**:\n   - All reviewers unanimously criticized the paper's writing as \"badly and unprofessionally written,\" with grammatical errors, unclear sentences, and incomprehensible passages. Reviewer 2 explicitly stated they could not understand the abstract or core ideas, which is critical for a paper's evaluation.\n   - The method and contributions are obscured by unclear explanations, undefined mathematical symbols, and poorly formatted algorithm boxes. Reviewer 3 noted that even the paper's intent was difficult to decipher.\n\n2. **Lack of References and Related Work**:\n   - The paper contains **no references**, a severe flaw in academic writing. This omission suggests a lack of awareness of existing literature, including foundational work on MCTS, AlphaZero, and related methods. The author admitted to removing references due to technical issues with a `.bib` file, which is an inexcusable oversight for a submission to a top-tier venue.\n   - The author’s response mentions discovering similar work post-submission but failing to cite it, further highlighting a lack of thorough literature review.\n\n3. **Technical and Structural Flaws**:\n   - The paper’s structure is problematic, with \"blank space\" and poor formatting (e.g., algorithm boxes are hard to follow). Reviewer 3 described the paper as \"not a serious academic endeavor\" but \"a set of notes.\"\n   - The proposed method (\"master game tree\" or \"meta-tree\") is inadequately explained, leaving reviewers unable to assess its novelty or validity. The experiments are limited to a \"small problem\" (e.g., Tic-Tac-Toe), which may not demonstrate the method’s broader applicability or significance.\n\n4. **Lack of Rigor**:\n   - The paper lacks theoretical guarantees or empirical analysis to support its claims of \"robust performance.\" Reviewer 1 noted the absence of evidence for stability and final performance, which are critical for evaluating reinforcement learning methods.\n   - The author’s response does not address these technical concerns, focusing instead on writing improvements and future work.\n\n### Author Response Analysis:\n- The authors acknowledge the writing issues and express a desire to improve, but their response does not clarify the method, address the lack of references, or refute the reviewers’ technical criticisms. The mention of finding similar work post-submission raises concerns about prior art and the novelty of the contribution.\n- The technical debt (e.g., missing references due to `.bib` file issues) suggests the paper was submitted prematurely, without adequate preparation for a top-tier venue.\n\n### Conclusion:\nWhile the paper’s topic—improving MCTS-based reinforcement learning—is relevant, the current submission is fundamentally flawed in writing, structure, and academic rigor. The lack of references, unclear explanations, and poor presentation render it incomprehensible and unassessable. Even if the core idea has potential, it is not communicated effectively, and the paper does not meet the standards of a top-tier conference. The reviewers’ consensus is that the work requires substantial revision (beyond minor edits) to address these foundational issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"Domain Adaptive Multibranch Networks\" and it's about unsupervised domain adaptation. The main idea is that different domains might need different processing paths to achieve a common feature representation. They propose a multi-branch architecture where each domain can take a different path, allowing more flexibility compared to existing methods that use the same operations for all domains. The experiments show higher accuracy and the ability to handle multiple domains.\n\nNow, looking at the reviewer comments. Reviewer 1 is generally positive but has some minor issues: suggesting more references outside deep learning, renaming the term \"flow network\" to avoid confusion, removing backlinks in references, replacing arXiv with peer-reviewed sources, and ensuring code is published. The authors addressed all these points, so that's good.\n\nReviewer 2 points out some key issues. The main one is that the experiments don't compare with state-of-the-art methods like CDAN, only RPT and DANN, which are older. The authors mention in their response that they added experiments with MCD (a more recent method) and found improvements. Also, the reviewer noted that the paper's intuition about different computation amounts wasn't tested because all domains used the same layers. The authors added experiments with varying layer numbers and more flows. Another point was the figure showing no parameter sharing, which the authors clarified. The reviewer still thinks experiments need improvement but acknowledges the method's novelty and soundness.\n\nReviewer 3 is more critical. They stress the lack of comparison with SOTA UDA methods like CDAN, insufficient ablation studies, and not testing multi-source/multi-target scenarios. The authors addressed some of these by adding experiments with MCD, testing K values, and multi-source domains. However, multi-target is left for future work. The baseline results in Table 1 being lower than others is a concern, but the authors explain they used standard conditions.\n\nThe authors' responses seem to have addressed most of the reviewers' concerns. They added comparisons with more recent methods, did more ablation studies on K and flow configurations, clarified the figure, and renamed the method to avoid confusion. They also fixed the references and code issues.\n\nNow, considering top-tier standards. The paper's main contribution is a novel architecture for domain adaptation, which is technically sound. The experiments, while initially lacking, were improved in the response. The key issue was not comparing with SOTA, but the authors added MCD results, which are better. The multi-branch idea is novel and addresses a specific gap in existing methods. The experiments now seem more comprehensive, though maybe not exhaustive, but the rebuttal shows they've made efforts to strengthen their case.\n\nPotential weaknesses: The original submission didn't have SOTA comparisons, but the authors addressed that. The multi-target domain adaptation is still future work, but the paper's core idea is solid. The name change is good. The code will be available, which is a plus.\n\nOverall, the paper's contributions are novel and the experiments, after revisions, are sufficient for a top conference. The reviewers' main concerns were addressed, so I think it should be accepted.\n</think>\n\nThe paper proposes a novel approach to unsupervised domain adaptation by introducing a domain-adaptive multibranch architecture, which allows different domains to follow distinct computational paths. This addresses a key limitation of existing methods that enforce uniform processing across domains. The core idea is technically sound and offers a fresh perspective on handling domain-specific variations. \n\n**Strengths:**\n1. **Novelty and Contribution:** The multibranch architecture is a meaningful departure from standard domain adaptation techniques, which typically use shared or parallel streams with fixed operations. The ability to dynamically adapt computation paths per domain is a valuable innovation.\n2. **Technical Soundness:** The method is well-motivated, and the experiments (after revisions) demonstrate improved performance over baselines. The authors addressed reviewer concerns by adding comparisons with state-of-the-art methods (e.g., MCD) and ablation studies on critical parameters like the number of flows (K).\n3. **Practical Utility:** The framework’s flexibility to handle multiple domains and its potential extension to multi-source/multi-target scenarios are significant advantages. The authors also committed to releasing code, enhancing reproducibility.\n\n**Weaknesses and Concerns:**\n1. **Initial Experimental Gaps:** The original submission lacked comparisons with state-of-the-art methods like CDAN, but the rebuttal clarified that integrating MCD (a stronger baseline) improved results and outperformed it. While this strengthens the case, the revised experiments should be included in the final paper.\n2. **Clarity and Naming:** The term \"flow network\" was problematic due to prior usage in graph theory, but the authors renamed it to \"Domain-Adaptive Multibranch Networks,\" resolving this issue.\n3. **Ablation Studies:** While the authors added some ablation studies (e.g., varying K and flow configurations), more detailed analyses (e.g., sensitivity to hyperparameters, deeper exploration of multi-source scenarios) could further solidify the claims. However, the rebuttal demonstrates sufficient effort to address these gaps.\n\n**Reviewer Feedback Resolution:**\n- **Reviewer 1’s concerns** (naming, references, arXiv usage) were fully addressed.\n- **Reviewer 2’s critiques** (lack of SOTA comparisons, incomplete intuition testing) were mitigated by new experiments and clarifications about parameter sharing.\n- **Reviewer 3’s objections** (insufficient ablations, multi-source/multi-target testing) were partially resolved through added experiments, though multi-target remains future work. The authors’ rebuttal convincingly argues that their core contribution is valid even without exhaustive exploration of all possible configurations.\n\n**Conclusion:** The paper’s novel architecture and experimental improvements post-rebuttal meet the standards of a top-tier conference. While there are minor unresolved issues (e.g., multi-target adaptation), the core contribution is sufficiently robust and impactful. The authors have demonstrated a strong commitment to addressing reviewer concerns, and the revised experiments and clarifications strengthen the paper’s validity.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. They're proposing a deep neural approach for time-evolving graphs using temporal point processes. The key contributions seem to be handling multiple time resolutions with LSTM-based units, time gates, selection mechanisms, and attention. The application is anti-fraud detection, which is relevant. \n\nLooking at the reviewer comments, there are several points raised. Reviewer 1 mentions that the contributions are incremental, especially the use of LSTM with time gates and stacked LSTMs. They also point out that the ablation study isn't thorough enough and that a key baseline (JODIE) was missing. Reviewer 2 agrees on the lack of novelty, noting that similar approaches exist in prior work. They also question the log-likelihood formulation and suggest comparing with standard benchmarks. Reviewer 3 found the paper hard to understand, lacking diagrams and clarity on how the model handles dynamic graphs and computational efficiency.\n\nThe author responses addressed some points. They added diagrams, clarified the TDG concept, and included comparisons with JODIE. They also addressed parameter scaling and computational aspects. However, some concerns remain, like the log-likelihood correction and the parameter count difference between their model and baselines.\n\nNow, considering the standards of a top-tier conference, the paper needs to have significant novelty, thorough evaluation, and clear presentation. The main issues here are the incremental contributions and the initial lack of important baselines. The authors did add JODIE comparisons, which helps, but the core idea of combining TPP with LSTMs and attention isn't entirely new. The ablation study was improved but might still be insufficient. The clarity issues were addressed with figures, which is good.\n\nOn the positive side, the experiments show superior performance, and the application to real-world fraud detection is a plus. The authors also provided more analysis on parameters and scalability. However, the method's novelty is questionable. Top conferences usually require more groundbreaking contributions. The stacked LSTMs and time gates might be seen as combining existing ideas without enough innovation. The reviewers' points about the log-likelihood and parameter differences are valid concerns that need careful consideration.\n\nIn the end, while the paper has merits and improvements post-rebuttal, the lack of sufficient novelty and thorough analysis might tip the decision towards rejection. But the authors did address some critical issues, so maybe a borderline accept? Wait, the reviewers' main concerns were about novelty and baseline comparisons. Since they added JODIE and clarified some points, perhaps it's borderline. However, top-tier conferences are highly competitive. If the core idea isn't novel enough, even with good experiments, it might not make the cut. The authors' responses didn't fully resolve the incremental nature of the contributions. So, I think the decision leans towards reject, but I need to check again.\n\nAlternatively, maybe the combination of mechanisms (time gates, selection, attention) in the context of time-evolving graphs is a solid contribution. The experiments do show better performance, and the application is important. The clarity issues were addressed. Maybe the reviewers' points about being incremental are too harsh, and the paper's overall contribution is sufficient for acceptance. Hmm.\n\nWait, Reviewer 2 said the main contributions are not novel on their own, but the combination might be. The authors argue their TDG and DIP units are novel. If the combination is sufficiently novel and the experiments are strong, maybe it's acceptable. The rebuttal added necessary comparisons and clarifications. The log-likelihood was fixed. The parameter count difference was addressed by showing performance even with smaller parameters. \n\nConsidering all this, perhaps the paper should be accepted. The improvements in the rebuttal address some key issues, and the experiments are strong. The novelty might be borderline but sufficient for a top conference if the combination is considered a meaningful advancement. The clarity was improved with figures. So, maybe accept.\n</think>\n\nThe paper proposes a method for modeling time-evolving graphs using a temporal point process framework with recurrent neural networks (LSTMs), time gates, selection mechanisms, and attention. While the approach addresses important challenges in dynamic graph modeling, several critical issues raised by reviewers remain unresolved or only partially addressed. \n\n### Key Strengths:\n1. **Empirical Performance**: The experiments demonstrate superior performance over baselines on multiple tasks, including a real-world financial application. This is a strong practical advantage.\n2. **Novelty in Combination**: The integration of time gates, selection mechanisms, and multi-layer fusion into a temporal point process framework represents a non-trivial combination of ideas, even if individual components (e.g., LSTMs, attention) are not novel in isolation.\n3. **Scalability and Inductive Capabilities**: The method handles growing/shrinking graphs incrementally and supports new nodes, which is valuable for real-world applications like fraud detection.\n4. **Rebuttal Improvements**: The authors addressed several concerns by adding diagrams, clarifying the temporal dependency graph (TDG) concept, and including comparisons with JODIE. They also provided parameter sensitivity analyses and fixed the log-likelihood formulation error.\n\n### Key Weaknesses:\n1. **Novelty Concerns**: Reviewers highlighted that core components (e.g., LSTMs for intensity functions, temporal attention) are not novel. The authors argue their contributions (e.g., TDG, DIP units) are unique, but the justification for these innovations remains underdeveloped. The rebuttal did not sufficiently clarify how these components meaningfully advance the state of the art beyond incremental improvements.\n2. **Ablation and Analysis**: While the ablation study was expanded, it still lacks detailed analysis of key parameters (e.g., how `k` and `L` affect performance across datasets) and the theoretical justification for the selection mechanism’s design. The impact of removing critical components (e.g., time gates, residual connections) was not fully explored.\n3. **Baseline Comparisons**: Although JODIE was added post-rebuttal, the authors did not address why interaction classification was not compared with temporal baselines (e.g., training a classifier on embeddings from prior methods). This limits the completeness of the evaluation.\n4. **Clarity and Presentation**: While figures were added, the paper’s technical exposition remains dense and requires significant effort to parse. The role of parameters like `alpha` and scaling factors remains unclear, and the computational efficiency claims (e.g., parallel training) need more concrete justification.\n\n### Final Assessment:\nThe paper’s empirical results and real-world application are compelling, and the rebuttal addressed several critical issues. However, the lack of a compelling novelty argument and insufficient theoretical/empirical analysis of core contributions (e.g., stacked LSTMs, selection mechanism) remain significant drawbacks for a top-tier conference. While the method is an improvement over prior work, its incremental nature and unresolved gaps in justification prevent it from meeting the high bar for acceptance at a leading venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. The main contribution is a framework called Continuous Query Decomposition (CQD) for answering complex queries on incomplete Knowledge Graphs (KGs). They use neural link predictors and translate queries into differentiable objectives with logical operators handled via t-norms and t-conorms. The experiments show better performance than state-of-the-art methods with less training data. The authors also mention explainability and provide code and datasets.\n\nNow, looking at the reviewer comments. There are five reviewers. Let's go through each one.\n\nReviewer 1 just says \"Your comment cleared my doubts\" but that's vague. Maybe they had concerns but were addressed?\n\nReviewer 2's summary is positive, calling the method elegant and effective. They note SOTA results and explainability but suggest more discussion on EPFO queries, the 8 query types, and an example. They also ask about how KG incompleteness is handled and better context on why complex queries are important. The author response addressed these points, adding examples and clarifying the experimental setup. So Reviewer 2 seems satisfied after revisions.\n\nReviewer 3 is more technical. They question the difference between CQD and previous methods like GQE/Q2B. The author response explains that CQD decomposes queries into atomic parts using t-norms without needing to train on complex queries, unlike GQE which embeds queries. They also address timing concerns, showing that combinatorial optimization is manageable. The reviewer also asks about inference time comparisons, which the authors added in the appendix. They also mention using different link predictors, and the authors ran DistMult experiments. The reviewer's main points seem addressed, though they note the method's simplicity but acknowledge its effectiveness.\n\nReviewer 4 thinks the paper is practical, likes the data efficiency (training on 1-hop queries), but wants more clarity on data usage and timing. The authors clarified the training data part and added timing results. The reviewer also suggested an ablation with TransE, which the authors did with DistMult. They also fixed notation issues. The reviewer's concerns seem addressed, and they support acceptance.\n\nReviewer 5 is positive, noting the method's advantages over baselines in terms of data and explainability. They asked about embedding size and some technical points. The authors clarified the embedding size (rank) and addressed the technical question about variable embeddings. The reviewer's final note is strong support for acceptance.\n\nLooking at the author responses, they addressed most of the reviewers' concerns. They added experiments with DistMult, clarified notation, provided timing data, and fixed examples. The key points from reviewers were about methodology differences, data efficiency, and timing, which were all addressed.\n\nThe paper's contributions are clear: a novel method that outperforms SOTA with less data, handles complex queries (including disjunctions which some baselines don't), and offers explainability. The experiments are thorough across multiple datasets and baselines. The technical soundness is supported by the responses, especially the ablation studies and timing analysis.\n\nPotential weaknesses: Some reviewers noted the method's complexity (though the authors simplified notation), and the combinatorial method's exponential scaling with hops. However, the authors mention that in their experiments, the maximum hop length wasn't an issue and suggest possible optimizations for longer chains. The timing results show it's manageable, so it's a valid trade-off.\n\nThe paper's clarity was a minor issue, but the authors improved the presentation. The main concerns from reviewers were either addressed or deemed acceptable.\n\nGiven that all reviewers, especially after author responses, seem to support the paper, and the contributions are significant for top-tier standards, I think this should be accepted. The method is novel, effective, and addresses important challenges in KG query answering. The experiments back up the claims, and the authors have adequately addressed concerns.\n</think>\n\nThe paper proposes Continuous Query Decomposition (CQD), a novel framework for answering complex queries on incomplete knowledge graphs (KGs). The method leverages pre-trained neural link predictors to compute scores for atomic query components, combines these scores using t-norms/t-conorms for logical operations, and solves an optimization problem to identify optimal variable assignments. Key strengths include state-of-the-art (SOTA) performance on standard benchmarks with significantly less training data (only 1-hop queries), improved explainability, and compatibility with existing link predictors. The authors address reviewer concerns by clarifying technical details, providing additional experiments (e.g., with DistMult), and improving presentation. \n\n**Detailed Analysis:**\n1. **Novelty and Impact:** \n   - CQD addresses a critical gap in handling complex queries (e.g., disjunctions and existential quantifiers) without requiring large-scale training on complex queries, a limitation of prior methods like GQE and Query2Box. This reduces data dependency and computational costs, a major advantage for practical applications.\n   - The use of t-norms/t-conorms for logical operations is a principled and interpretable approach, offering clear explanations for query answers, which is a significant contribution to the field.\n\n2. **Technical Soundness:**\n   - The method is theoretically grounded and validated through rigorous experiments. The authors compare against strong baselines (GQE, Query2Box) on standard datasets (FB15k, NELL, etc.), demonstrating consistent improvements in Hits@3 (8–40% relative gains). \n   - Addressed reviewer concerns: \n     - Clarified differences from prior work (e.g., GQE’s query embedding vs. CQD’s decomposition).\n     - Added timing results showing combinatorial optimization is feasible (under 50ms per query) and manageable even for longer chains.\n     - Provided ablation studies with DistMult, confirming performance gains are not solely due to ComplEx.\n\n3. **Clarity and Presentation:**\n   - The authors improved notation and examples in response to feedback, enhancing readability. The method’s simplicity and modular design (independent of link predictors) are well-explained.\n\n4. **Weaknesses and Mitigation:**\n   - The combinatorial approach’s exponential complexity with hop length is noted, but the authors argue it is manageable for typical query lengths (≤3 hops) and suggest optimizations for longer chains.\n   - While the method’s reliance on pre-trained link predictors inherits their limitations, the authors demonstrate robustness across different models (ComplEx, DistMult).\n\n**Conclusion:** The paper presents a significant advancement in complex query answering for KGs, with strong empirical results, clear contributions, and thorough validation. The authors effectively addressed reviewer concerns, and the method’s efficiency and explainability align with top-tier conference standards.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about reducing the number of supervised synaptic updates needed for a neural network model to match the primate ventral stream. The authors use three strategies: fewer training epochs, better initialization (Weight Compression), and training only a subset of synapses. They claim these reduce the required updates by two orders of magnitude while maintaining ~80% of the original model's performance on BrainScore.\n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewer 4 is very supportive, pointing out the problem's importance and the thorough experiments. They recommend acceptance. However, Reviewers 1, 2, and 3 have significant concerns.\n\nReviewer 1 and 2 question the interpretation of BrainScore. They argue that the paper conflates high BrainScore with biological plausibility. For instance, if BrainScore=1.0 doesn't guarantee a perfect match to the brain's representations, then claiming 80% of BrainScore means 80% brain match is misleading. The authors addressed this by clarifying the terminology and adding caveats, but Reviewer 2 still feels the paper's claims are overblown. They want explicit disclaimers that the scores don't equate to actual similarity, which the authors partially addressed but maybe not strongly enough.\n\nReviewer 3 is more critical. They argue the paper lacks theoretical foundation and biological plausibility. The methods like Weight Compression are seen as arbitrary without a clear biological basis. The paper is framed as a proof-of-concept, but the reviewer thinks it's not a significant contribution. They also point out methodological details in the appendix and unclear figures. The authors responded by clarifying that their methods aren't claiming to model evolution exactly but exploring possibilities, and they provided statistical backing for Weight Compression's improvement over standard initialization. However, the reviewer's main issue about the lack of biological grounding remains.\n\nReviewer 4's positive take is crucial here. They see the problem as important and the methods as innovative. The experiments are thorough, and the results are surprising. Their only questions are about biological relevance and some technical details, which the authors addressed.\n\nThe authors' responses seem to have mitigated some concerns. They clarified the BrainScore interpretation, provided statistical validations for their methods, and acknowledged the limitations. However, the core issue raised by Reviewers 1 and 2 about the metric's meaning and Reviewer 3's critique on biological plausibility still linger. The paper's main contribution is showing that fewer updates can achieve similar performance, which is a solid technical contribution. But the biological implications are speculative, which might be a problem for a top-tier conference that values both technical merit and biological relevance.\n\nThe conference standards require high impact and rigorous validation. While the technical aspects are strong, the biological claims are under-supported. The authors frame their work as a step toward biologically plausible models, but the reviewers point out that the methods aren't grounded enough. However, the paper does advance the field by addressing a critical issue in model plausibility, even if it's a first step. The combination of three strategies leading to significant reduction in updates is a notable contribution.\n\nConsidering all this, the paper has enough technical merit and addresses an important problem. The reviewers' concerns are valid but perhaps manageable with the authors' clarifications. Since Reviewer 4 is strongly in favor and the others have mixed but not entirely damning critiques, especially after revisions, it's borderline but leans toward acceptance. The field needs such explorations even if they're not perfect, and the authors have addressed many concerns adequately.\n</think>\n\nThe paper addresses a critical problem in computational neuroscience: reconciling the success of deep neural networks (DNNs) in modeling adult primate visual processing with their biological implausibility in terms of training requirements. The authors propose three strategies to reduce the number of supervised synaptic updates needed to achieve brain-like performance, demonstrating a two-orders-of-magnitude reduction while retaining ~80% of the original model's BrainScore. The technical contributions are substantial, with thorough experiments and clear results. However, the paper faces significant critiques regarding its biological plausibility and the interpretation of BrainScore.\n\n### Key Strengths:\n1. **Technical Innovation**: The combination of reduced training, improved initialization (Weight Compression), and selective layer training is novel and effective. The results are statistically robust, with clear improvements over baselines (e.g., WC achieving 54% BrainScore vs. 43% for standard initialization).\n2. **Problem Relevance**: The focus on reducing supervised updates aligns with a major challenge in neuroscientific modeling—bridging the gap between DNN performance and biological feasibility.\n3. **Broader Impact**: The work opens avenues for future research into biologically plausible learning rules and developmental trajectories, as highlighted by Reviewer 4.\n\n### Key Weaknesses:\n1. **Biological Plausibility**: Reviewers 1, 2, and 3 question whether the methods (e.g., Weight Compression) have a clear biological basis. While the authors clarify that their goal is to explore hypotheses rather than model evolution directly, the lack of explicit biological justification weakens the neuroscientific significance.\n2. **Metric Interpretation**: Reviewers 1 and 2 emphasize that BrainScore is a proxy, not a direct measure of representational similarity. The authors revised the text to clarify this distinction but did not fully resolve concerns about overinterpreting scores as \"match to the brain.\"\n3. **Theoretical Gaps**: Reviewer 3 critiques the lack of theoretical grounding for methods like WC and the arbitrary nature of layer selection in Critical Training. While the authors provided statistical validation, the biological rationale remains underdeveloped.\n\n### Author Responses:\nThe authors addressed several critiques effectively:\n- They statistically validated WC’s superiority over standard initialization.\n- They clarified that BrainScore is a proxy and acknowledged its limitations.\n- They acknowledged the speculative nature of biological analogies while defending the work as a \"proof-of-principle.\"\n\nHowever, unresolved issues remain:\n- The biological plausibility of WC and layer selection remains unconvincing to some reviewers.\n- The paper’s framing as a step toward developmental models may overreach without stronger biological evidence.\n\n### Final Assessment:\nWhile the paper’s technical contributions are strong and the problem is important, its biological claims are speculative. However, top-tier conferences often accept work that advances technical methods while acknowledging limitations, especially when addressing foundational challenges. The authors have sufficiently addressed concerns about methodology and clarified ambiguities. The work’s potential to inspire future research in biologically plausible learning and development outweighs its current gaps. \n\n**Final Decision: Accept**"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"Deep Coherent Exploration For Continuous Control\" and it's about improving exploration in reinforcement learning, especially with deep networks. The key contributions are scaling up a previous method (Generalized Exploration from van Hoof et al. 2017) to deep RL by focusing on the last layer parameters, using a recursive inference step, and providing recipes for A2C, PPO, and SAC.\n\nLooking at the reviewer comments first. Reviewer 1 is positive, saying the paper is well-written, the on-policy results are strong, and the appendix is good. But they have concerns about the off-policy (SAC) part being less strong and maybe not as well-integrated. They suggest reframing it as an on-policy method and mention the off-policy as a limitation. They tentatively accept.\n\nReviewer 2 thinks the method is novel but points out that the main contribution isn't entirely new since it's an extension of van Hoof. The empirical results on SAC aren't as good, and since SAC is state-of-the-art, the practical significance might be limited. They also note that the experiments are only on Mujoco, which might not be enough, and some baselines underperform vanilla methods, which could indicate tuning issues. They're on the fence but leaning towards accept but with reservations.\n\nReviewer 3 mentions clarity issues in section 3.2, suggesting moving some details to the appendix. They like the theoretical ideas but note they're not novel. The figures are hard to read. They still think the scalability and evaluation are good contributions. They seem okay with acceptance.\n\nReviewer 4 points out that the method follows GE closely, so originality is a concern. The SAC results are mixed, even degrading on HalfCheetah. They want more experiments on exploration-focused tasks, not just return performance. They also note the lack of explanation on step vs trajectory exploration. They might lean towards reject but not sure.\n\nThe author response addresses some points. They improved the SAC integration, which might help. They clarify that their method is for undirected exploration, so not aiming at hard exploration tasks. They also address hyperparameters and experiment domains, defending Mujoco's diversity.\n\nNow, considering the criteria for a top-tier conference. The paper needs to be novel, significant, and well-executed. The main contribution is scaling GE to deep RL, which is a solid contribution. The on-policy results are strong, which is a plus. However, the off-policy (SAC) results are weaker, and the integration there was initially heuristic. The authors improved that, but the results still aren't as strong as on-policy. \n\nThe reviewers' main concerns are originality (since it's an extension) and the empirical impact, especially on SAC. The experiments are limited to Mujoco, but the authors argue that's sufficient for continuous control and undirected methods. The clarity issues in the paper can be fixed in the camera-ready version.\n\nTop conferences often value both theoretical contributions and empirical impact. The method's scalability and the on-policy improvements are significant. The SAC part is a bit of a letdown but not a dealbreaker if the core idea is solid. The authors addressed some reviewer points, which helps.\n\nI think the paper has enough merit. The extension to deep RL is non-trivial, and the on-policy results are compelling. The SAC issues are noted but the authors have made improvements. The clarity can be fixed. The problem of exploration is important, and the method offers a new approach. So, I would lean towards accept.\n</think>\n\nThe paper \"Deep Coherent Exploration For Continuous Control\" proposes a scalable framework for exploration in deep reinforcement learning by extending prior work on Generalized Exploration (GE) to deep policies. The method focuses on perturbing the last layer parameters of the policy network and introduces a recursive inference step to handle these latent variables efficiently. The contributions are significant in addressing the scalability of GE to deep RL, particularly for on-policy methods like A2C and PPO, where the empirical results are strong and the method outperforms baselines. However, the off-policy integration with SAC is less robust, with mixed results and heuristic components, which the authors acknowledge and attempt to address in their rebuttal. \n\n### Key Strengths:\n1. **Novelty and Scalability**: The extension of GE to deep networks by focusing on the last layer and using recursive inference is a meaningful contribution. This addresses a critical limitation of prior work (GE) that did not scale to deep policies.\n2. **Empirical Validation**: The on-policy results (A2C, PPO) are compelling, showing consistent improvements in learning speed and stability. Ablation studies and detailed derivations strengthen the method's credibility.\n3. **Theoretical Soundness**: The analytical treatment of latent variables and gradients reduces variance, which is a theoretically grounded and valuable innovation.\n4. **Reproducibility**: The appendix and code (implied by reviewer comments) support reproducibility, and the authors address clarity concerns in their rebuttal.\n\n### Key Weaknesses:\n1. **Off-Policy Limitations**: The SAC integration is weaker, with degraded performance on HalfCheetah and heuristic components. While the authors propose improvements, the results remain underwhelming compared to on-policy methods.\n2. **Originality Concerns**: The core idea builds directly on GE (van Hoof et al., 2017), and the first contribution (unifying step- and trajectory-based exploration) is inherited from prior work. The novelty lies in scaling it to deep networks, which is still a valid contribution but less groundbreaking.\n3. **Empirical Scope**: Experiments are limited to Mujoco tasks, which, while standard, may not fully demonstrate the method’s broader applicability. The lack of results on sparse-reward or directed-exploration tasks limits the demonstrated significance.\n4. **Clarity Issues**: Some sections (e.g., Section 3.2) are dense and could be more accessible, though the authors commit to revisions.\n\n### Author Rebuttals:\nThe authors address several concerns, notably improving the SAC formulation and clarifying hyperparameters. They also defend the focus on Mujoco by emphasizing the diversity of tasks and the niche of undirected exploration. These responses mitigate some concerns but do not fully resolve the SAC performance gap.\n\n### Final Judgment:\nWhile the paper has limitations in off-policy performance and originality, its core contribution—scaling GE to deep RL with strong on-policy results—is impactful for continuous control. The problem of exploration in RL is critical, and the method offers a novel, theoretically grounded approach. The improvements in on-policy methods alone justify acceptance at a top-tier conference, especially given the clarity of the presentation and the authors' commitment to addressing reviewer feedback. The SAC shortcomings are noted but do not outweigh the overall contribution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. I need to go through all the provided information carefully.\n\nFirst, the paper's title and abstract suggest they're proposing a generalized probability kernel (GPK) for discrete distributions, extending MMD and other discrepancy measures. The key contributions seem to be unifying existing methods and providing a framework for new statistics like power-MMD. But the reviewers have several criticisms.\n\nLooking at Reviewer 1's comments: They point out major issues like confusion between K and k, unclear definitions (like why it's called a kernel instead of a distance), missing proofs (like Theorem 5), and incomplete sections like simulations. The reviewer also mentions that some sections are trivial or known results. That's a big red flag because a top conference needs novel and rigorous work.\n\nReviewer 2 adds that the paper lacks experiments, has poor presentation with grammar issues, and the theoretical contributions are just rehashing existing work without showing practical utility. They question the usefulness of the proposed estimators without asymptotic analysis or experiments. Without empirical validation, especially for a method meant for two-sample tests, this is a problem. Top conferences usually require both solid theory and empirical results.\n\nReviewer 3's points include undefined notations, unclear motivation for the setting (like why discrete variables with real labels?), and technical issues like the Stein operator in a discrete context. They also note that the polynomial kernel might only work under specific conditions (l=k). The lack of proofs and rigorous definitions is another issue here. The presentation problems (typos, grammar) make it hard to follow, which is a barrier for reviewers and readers.\n\nReviewer 4 mentions confusion in notation between kernels on distributions vs samples, and questions how the method applies when distributions are unknown (since two-sample tests typically don't know p and q). The authors addressed this in their response by saying they use plugin estimators, but the original submission didn't have experiments to back this up. Also, the removal of KSD due to incorrect proofs shows some instability in their contributions.\n\nThe author response tries to address some issues: they removed the KSD part, fixed notation, added new results on polynomial GPK and power-MMD, and worked on typos. But the initial reviews were very critical, especially about missing proofs, experiments, and clarity. The revised version might have improved, but the original submission had major flaws. However, the user provided the author response, so I need to consider if the revisions sufficiently address the concerns.\n\nWait, the user's prompt says \"based on all the information provided\"—so including the author's response. The authors claim they fixed notation, added experiments (though the original reviewers said there were none), and addressed some theoretical gaps. But the original reviews pointed out that the proofs were missing (like Theorem 5's proof), and the simulations were completely missing. The author response mentions \"in this revision, we present the usage... provide unbiased estimator and convergence bounds,\" but does that include experiments? The original Reviewer 2 said there were no experiments, and the author response doesn't explicitly mention adding simulations. The response talks about theoretical results like Theorem 2 and 3, but maybe not empirical evaluation.\n\nAlso, the authors removed the KSD part due to incorrect proofs, which might have been a major part of their original contribution. Now focusing on polynomial GPK and power-MMD. But if the core issues like missing experiments and proofs aren't resolved, the paper might still be weak.\n\nTop-tier conferences require strong theoretical contributions and empirical validation. The reviewers' main points were lack of experiments, incomplete proofs, and unclear presentation. The authors addressed some notation and theoretical parts but maybe not the experiments. Since the original submission lacked experiments, and the author response doesn't clearly state they added them, that's a problem. Without empirical results, especially for a two-sample test method, it's hard to assess practical utility.\n\nAdditionally, the theoretical claims need solid proofs. If Theorem 5's proof was missing and they fixed it, that's good, but if not, it's still an issue. The authors mention revising the theory sections but it's unclear if all gaps are filled.\n\nConsidering all this, even with revisions, the paper might still have unresolved critical issues. The initial reviews were harsh, pointing out fundamental flaws. Unless the revisions completely fixed all major points (proofs, experiments, clarity), which isn't clear from the provided info, the paper might not meet the standards. Since the user's question is based on the provided info, and the author response doesn't explicitly address all reviewer concerns (like experiments), I think the decision should be to reject. Top conferences can't accept papers with missing proofs and no empirical validation, even if the ideas are interesting.\n</think>\n\nThe paper proposes a generalized probability kernel (GPK) for discrete distributions, aiming to unify existing discrepancy measures like MMD and extend them. While the idea of unifying these methods is promising, the submission suffers from significant flaws that undermine its credibility and novelty. Below is the detailed analysis:\n\n### Key Issues Identified:\n1. **Theoretical Gaps and Clarity**:\n   - **Definitions and Notation**: Reviewers highlighted critical ambiguities in core definitions (e.g., conflating kernels and distance metrics, inconsistent use of \\( K \\) vs. \\( k \\), undefined terms like \\( \\mathcal{H} \\), \\( \\Delta^* \\), and \\( \\phi \\)). These issues persist even after revisions, as the authors only partially addressed notation but did not fully clarify foundational concepts like the Stein operator in discrete settings.\n   - **Missing Proofs and Rigor**: Key proofs (e.g., Theorem 5) were absent in the original submission, and while the authors mention revising theory sections, the revised version does not explicitly resolve these gaps. The removal of the KSD section due to incorrect proofs further weakens the paper’s contributions.\n   - **Unresolved Technical Concerns**: The polynomial GPK’s validity (e.g., requiring \\( l = k \\)) and the lack of formal proof that symmetric-KDSD satisfies kernel properties remain unaddressed.\n\n2. **Empirical Evaluation**:\n   - **No Experiments**: Reviewers emphasized the absence of empirical validation, a critical flaw for a method proposed for two-sample testing. The authors’ response does not mention adding experiments, leaving the practical utility of GPK and power-MMD untested. Without empirical results, claims of applicability (e.g., two-sample tests) lack evidence.\n\n3. **Presentation and Motivation**:\n   - **Poor Writing and Grammar**: The original submission had severe grammatical and typographical errors, which the authors claim to have fixed. However, the revised version’s clarity is still questionable given lingering undefined terms and notation issues.\n   - **Motivation and Novelty**: The paper’s motivation for applying RKHS concepts to discrete distributions (e.g., labels in \\( \\mathbb{R}^k \\)) is underdeveloped. The authors cite word2vec as an example but fail to provide a rigorous justification for why such a framework is necessary or advantageous over existing methods like MMD.\n\n4. **Contributions and Impact**:\n   - **Limited Innovation**: The theoretical contributions (e.g., generalizing MMD) are framed as extensions but lack novel insights beyond rephrasing existing work. The removal of KSD due to flawed proofs further reduces the paper’s novelty.\n   - **Unanswered Questions**: The application to two-sample testing remains unclear. The authors argue that plugin estimators eliminate the need to know \\( p \\) and \\( q \\), but without experiments or asymptotic analysis, this claim is unconvincing.\n\n### Author Response Analysis:\nThe authors addressed some issues (notation, removal of flawed KSD sections, focus on polynomial GPK/power-MMD), but critical gaps persist:\n- **No Empirical Validation**: The revised version still lacks experiments, which are essential for a two-sample test method.\n- **Incomplete Theoretical Fixes**: While notation was improved, foundational proofs and definitions (e.g., Stein operator in discrete settings) remain unresolved.\n- **Unclear Motivation**: The revised discussion on discrete distributions with real-valued labels (e.g., word2vec) is insufficient to justify the framework’s relevance.\n\n### Conclusion:\nDespite the authors’ efforts to revise the paper, it fails to meet the standards of a top-tier conference due to unresolved theoretical gaps, lack of empirical validation, and unclear presentation. The core contributions lack rigor and novelty, and the practical utility of GPK remains unproven. While the ideas are intriguing, the paper requires substantial reworking beyond minor revisions to address these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms.\" The keywords are Metric Learning, Geometric Algorithms, Approximation Algorithms. The abstract mentions they're formulating the problem as an optimization to minimize violated constraints, using an FPTAS with linear time via linear programming in low dimensions. They also mention experiments and that the algorithm is parallelizable and handles adversarial noise.\n\nLooking at the reviewers' comments first. Reviewer 1 isn't an expert but thinks the method is theoretically sound and outperforms some methods. However, they point out the paper doesn't discuss state-of-the-art (SOTA) properly, like Verma and Branson (NIPS 2015) and Ye et al. (IJCAI). They also question the logic in Lemma 2.1 about adding constraints making feasible instances infeasible, and the non-monotonic running time in Figure 4. The conclusion is missing.\n\nReviewer 2 says the paper uses an LP-type problem and Har-Peled's result to get an approximation. They like the idea of applying LP-type problems to metric learning. But they note the paper doesn't mention computational hardness, defines terms like \"accuracy\" and \"combinatorial dimension\" late, has a typo in Algorithm 1, and should clarify that the FPTAS assumes fixed dimension. Also, they want more on prior work's results for minimizing constraints.\n\nReviewer 3 is more critical. They think the main novelty isn't clear. Transforming into LP doesn't seem novel, and experiments don't show accuracy improvements. They question the theoretical guarantees, whether the approach is new, and the need for approximation. They also point out confusion in the proof about adding constraints.\n\nAuthor responses address some points. They clarify that their method is the first with provable guarantees on violated constraints, using LP-type methods from geometry not applied before in metric learning. They mention prior works use error functions, while their direct minimization handles adversarial cases better (as in Figure 3). They admit the problem's complexity isn't known but suspect NP-hardness. They also fix some typos and clarify definitions.\n\nNow, evaluating the paper's strengths and weaknesses. The key contributions seem to be the first FPTAS for minimizing violated constraints with theoretical guarantees, using LP-type problems in a novel way for metric learning. The experiments show handling adversarial cases better than SOTA like ITML and LMNN. The authors address some reviewer concerns, like mentioning prior work and fixing typos.\n\nBut there are issues. Reviewer 1 and 2 noted the lack of discussion on SOTA beyond ITML and LMNN, specifically not comparing with Verma/Branson and Ye et al. The authors mention those papers but argue their setting is different (adversarial vs. i.i.d. samples). That's valid, but the paper should still discuss them to situate their work. Also, the conclusion is missing, which is a structural flaw.\n\nThe theoretical novelty is a point of contention. Reviewer 3 thinks applying LP here isn't novel, but the authors argue it's new in their context. Since they're the first to apply LP-type algorithms here, that's a valid contribution. The approximation's guarantees are a strong point, especially for adversarial inputs.\n\nThe experiments: Figure 3 shows their method handles adversarial cases better, which is a good point. However, Reviewer 3 says the accuracy isn't better, but maybe they're measuring different things. The authors clarify that prior methods use error functions, so their direct constraint minimization is different. The experiments need to better explain this distinction.\n\nTechnical soundness: The proof in Lemma 2.1 had a confusing phrasing, but authors fixed that. The FPTAS relies on fixed dimension, which they clarified. The running time non-monotonicity in Figure 4 is explained by PCA effects, which is acceptable but should be clearer in the paper.\n\nOverall, the paper has solid theoretical contributions with novel application of LP-type methods, addresses adversarial cases, and has experiments showing advantages. The weaknesses are in thorough literature review (specifically not discussing some recent works adequately), missing conclusion, and some clarity issues. However, the authors addressed many points in their response, and the core idea seems strong enough for a top conference. The novelty is there, even if some steps seem straightforward, the combination and application is new. The experiments, while maybe not showing accuracy gains in all metrics, do show robustness to adversarial cases, which is a plus.\n\nTop-tier conferences require significant contributions and clear presentation. The paper's main contribution is novel and theoretically sound, even if the experiments could be more extensive. The authors have addressed reviewer concerns sufficiently, and the remaining issues (like the missing conclusion) can be fixed in the final version. The lack of discussion on some recent works is a minor flaw compared to the overall contribution.\n\nFinal Decision: Accept\n</think>\n\nThe paper presents a novel approach to Mahalanobis metric learning by formulating the problem as an optimization task to minimize violated constraints and proposing an FPTAS using LP-type algorithms. Key strengths include:  \n1. **Theoretical Novelty**: The first work to apply LP-type problem machinery from computational geometry to metric learning, providing provable guarantees on constraint violations for adversarial inputs. This is a significant contribution, as prior methods (e.g., ITML, LMNN) use error functions rather than directly minimizing violations.  \n2. **Robustness**: Experiments (e.g., Figure 3) demonstrate superior performance on adversarial inputs compared to state-of-the-art methods, highlighting practical utility.  \n3. **Algorithmic Efficiency**: The FPTAS with nearly-linear running time and parallelizability are strong technical merits, especially for low-dimensional settings.  \n\nWeaknesses and concerns raised by reviewers include:  \n- **Lack of SOTA Discussion**: The paper initially underdiscusses recent works like Verma & Branson (NIPS 2015) and Ye et al. (IJCAI), though authors clarify these address different settings (e.g., adversarial vs. i.i.d. data).  \n- **Clarity Issues**: Ambiguities in proofs (Lemma 2.1), missing definitions (e.g., \"combinatorial dimension\"), and a missing conclusion. These are acknowledged and addressable in revisions.  \n- **Novelty Perception**: Reviewer 3 questions the novelty of applying LP techniques, but the authors persuasively argue this is the first such application in metric learning, justifying its significance.  \n\nThe authors’ responses effectively address many concerns, clarifying theoretical contributions, fixing typos, and emphasizing the adversarial robustness advantage. While minor flaws remain (e.g., missing conclusion), they are resolvable. The core contribution—a theoretically grounded, efficient algorithm with unique guarantees—is sufficiently impactful for a top-tier conference.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Disentangling Representations of Text by Masking Transformers.\" The abstract mentions using binary masks on transformer weights or hidden units to find subnetworks that encode specific features like syntax, semantics, sentiment, and genre. They compare their method to VAEs and adversarial training, claiming better performance. The experiments are on movie reviews with spurious correlations between genre and sentiment, and another task involving syntax vs. semantics.\n\nLooking at the reviewers' comments first. Reviewer 1 is concerned about the generality of the method. They ask if it's just for specific cases or more general. The author response addresses this by saying the method is applicable where robustness is needed, which is common. They also mention experiments on two different tasks, which might indicate some generality, but the reviewer still might not be fully convinced.\n\nReviewer 2 points out that the paper doesn't clearly show superiority over VAEs and adversarial methods beyond the abstract. The authors added VAE baselines in their response, which helps. But the reviewer also mentions lack of benchmark results. The authors argue their focus is on disentanglement, not SOTA on standard tasks. That's a valid point, but top conferences often look for strong empirical results. The reviewer also asks about speed and parameters. The authors clarify that masks are trained without fine-tuning BERT, which saves memory but not necessarily speed. The reviewer's main concern is impact, which might still be a hurdle.\n\nReviewer 3 notes the triplet loss limits to two factors, possible attribute leakage, increased parameters, and multiple forward passes. The authors acknowledge the two-factor limitation but say it's extendable. They also mention using an MLP probe, which might not be the strongest. The response addresses some points but not all. The WC task's representativeness is questioned, but the authors follow prior work here.\n\nReviewer 4 likes the idea but wants more domains and better comparison with finetuned models. The authors added more experiments and clarified that their method outperforms finetuned in robustness. The reviewer also points out that masking hidden units is a subset of weight masking, which the authors address. The authors also added missing citations, which is good.\n\nReviewer 5 is concerned about the limited genres (only Drama and Horror) and the figure interpretation. The authors explain their choice of genres for spurious correlations and clarify the figure. They also mention using STS for semantic tasks. The reviewer wants more downstream tasks, but the authors argue their experiments show robustness benefits.\n\nThe author responses address several points: added VAE baselines, clarified experiments, explained genre choice, and acknowledged some limitations. They also mention plans for more experiments in the camera-ready version. The core idea seems novel—using masks to find subnetworks without fine-tuning, which is computationally efficient in terms of memory. The experiments show promise in disentanglement and robustness, even if not SOTA on standard benchmarks.\n\nHowever, the reviewers' concerns about generality, scalability beyond two factors, and lack of broader benchmark results are valid. The paper's contributions are methodological, focusing on disentanglement rather than task performance. For a top-tier conference, the method needs to be impactful. The approach is innovative, and the results are convincing in the presented scenarios. The authors have addressed many concerns, and the rebuttal strengthens their case. While there are areas for improvement, the work seems solid enough for acceptance, especially given the importance of disentanglement and robustness in NLP.\n\nFinal Decision: Accept\n</think>\n\nThe paper proposes a novel method for disentangling representations in pre-trained transformers by learning binary masks over weights or hidden units. The approach aims to isolate distinct aspects of text (e.g., syntax, semantics, sentiment, genre) without fine-tuning the model, which is computationally efficient and leverages existing pre-trained models. Key strengths include:\n\n1. **Novelty and Innovation**: The idea of using masks to identify subnetworks encoding specific factors is original and addresses a critical problem in representation learning—disentanglement. This method sidesteps the need for training from scratch or adversarial techniques, offering a lightweight alternative.\n\n2. **Empirical Validation**: Experiments on sentiment/genre disentanglement and syntax/semantics separation show promising results. The authors demonstrate robustness to spurious correlations (e.g., genre-sentiment) and outperform baselines like VAEs and adversarial methods in controlled settings. The rebuttal clarifies comparisons with VAEs and addresses figure interpretation concerns.\n\n3. **Theoretical and Practical Relevance**: Disentanglement is crucial for robustness and interpretability, which are high-priority areas in NLP. The method’s ability to work with pre-trained models aligns with practical constraints of computational efficiency and resource usage.\n\n**Addressed Concerns**:  \n- The authors clarified that their method generalizes across tasks (sentiment/genre vs. syntax/semantics) and acknowledged plans to expand experiments.  \n- They addressed scalability limitations (e.g., triplet loss for two factors) by noting future work and explaining layer-specific masking.  \n- Added comparisons with VAE baselines and fixed ambiguities in figures.  \n\n**Remaining Concerns**:  \n- **Generality**: While experiments are compelling, broader domain and task validation (e.g., beyond two genres) would strengthen claims of generalizability.  \n- **Scalability**: The method’s performance on more than two factors and computational efficiency (e.g., memory vs. speed trade-offs) require further exploration.  \n- **Downstream Tasks**: While robustness is demonstrated, explicit experiments on standard downstream tasks (e.g., NLI, semantic similarity) would better highlight practical utility.  \n\nHowever, these limitations are acknowledged and addressable in future work. The core contribution—a novel, efficient approach to disentanglement in transformers—is sufficiently impactful for a top-tier conference. The method’s potential to improve robustness and reduce reliance on spurious correlations justifies its acceptance, especially given the lack of strong alternatives in this space.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about multitask learning, specifically a method called αVIL that dynamically adjusts task weights during training using model updates. The abstract mentions it's the first to use model updates for task weight estimation. The keywords are multitask learning, meta-optimization, and deep learning.\n\nLooking at the reviewers' comments first. Reviewer 1 says the paper is well-written but has major concerns: lack of theoretical justification and insufficient experimental results. The experiments don't consistently outperform baselines, and the baselines might be limited. They want more theory and better empirical validation.\n\nReviewer 2 points out that the method is ad hoc with little justification. They question the novelty since dynamic task weighting isn't new, comparing it to existing methods like DIW and meta-learning approaches (MAML, etc.). The experiments on MNIST show results within standard deviations of baselines, and NLU tasks are mixed. They also mention the need for more ablations and clarity on α vs. task weights. The results aren't convincing enough, so they suggest more work.\n\nReviewer 3 also thinks the experiments don't show significant improvement over baselines. They question the algorithm's novelty and clarity, especially the role of α. They want more discussion on the algorithm's properties and statistical significance. The results are minor and sometimes worse, leading them to suggest rejection.\n\nThe authors responded by addressing some points. They explained the α parameter and the subtraction in the algorithm. They mentioned that improvements on NLU tasks are small in general, which aligns with prior work. They also note that αVIL outperforms DIW on MNIST and is less prone to overfitting. They agree on adding theory and more experiments for the camera-ready version.\n\nNow, considering the standards of a top-tier conference. Top conferences usually require strong theoretical contributions, clear empirical validation, and significant improvements over baselines. The reviewers' main issues are the lack of theory and the marginal experimental results. The authors acknowledge these points but suggest that the method is novel in using model updates for weights and that results are better than DIW on MNIST. However, the NLU results are mixed and not statistically significant in some cases.\n\nThe novelty is a bit shaky because existing methods like DIW and meta-learning approaches are similar. The authors argue their method is different in meta-optimization approach, but Reviewer 2 points out that it's similar to MAML. The theoretical justification is missing, which is a big red flag for a top conference. Without a solid theory, the method's effectiveness isn't well-grounded.\n\nThe experiments, while done on two domains, don't consistently show strong results. The MNIST results are close to baselines, and the NLU results are mixed. The authors mention that multitask learning on NLU tasks often has small gains, but even so, the paper should show a clear advantage. The lack of statistical significance in some cases and the limited baselines (only standard multitask and DIW) might not be enough.\n\nThe reviewers' consensus seems to lean towards rejection because of the lack of theoretical backing and underwhelming results. The authors' responses address some points but don't resolve the core issues. They plan to add theory and more experiments later, but the current submission doesn't meet the high bar.\n\nConsidering all this, the paper might not be ready for a top-tier conference. The contributions are incremental without strong theoretical support, and the experiments don't clearly demonstrate superiority. The reviewers' concerns are valid and not sufficiently addressed in the current version. Therefore, the decision should be to reject.\n</think>\n\nThe paper proposes αVIL, a method for multitask learning that dynamically adjusts task weights using model updates. While the paper is well-written and the problem is relevant, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Theoretical Justification**: All reviewers highlight the lack of theoretical analysis or formal justification for the method. The authors acknowledge this gap and suggest addressing it in future work, but a top-tier conference requires a stronger theoretical foundation to validate the approach's principles and convergence guarantees.\n\n2. **Empirical Results**: \n   - The experiments show only marginal improvements over baselines (e.g., DIW) and inconsistent performance across tasks. On MNIST, results are within standard deviations of baselines, and on NLU tasks, improvements are minor or even negative. \n   - The choice of baselines is limited (e.g., standard multitask learning and DIW), and the NLU experiments lack statistical significance reporting. The authors note that multitask gains on NLU tasks are generally small, but this does not excuse the lack of clear superiority over existing methods.\n   - The MNIST setup uses only one auxiliary task, which limits the method's validation in more complex multitask scenarios. The authors' defense of this choice (e.g., as a sanity check) is insufficient for a top-tier venue.\n\n3. **Novelty and Clarity**: \n   - Reviewers question the method's novelty, noting similarities to existing approaches like meta-learning (e.g., MAML) and DIW. The authors argue that αVIL’s use of model updates for weight estimation is novel, but this distinction is not clearly articulated or sufficiently justified.\n   - The algorithm’s design (e.g., the role of α vs. task weights, the subtraction in line 11 of Algorithm 1) is unclear, and the authors’ explanations in the response do not fully resolve these ambiguities.\n\n4. **Experimental Rigor**: \n   - The NLU experiments report test results as ensembles but development results as averages, creating inconsistency. The authors acknowledge this and plan to address it but did not do so in the submission.\n   - Ablation studies and additional sanity checks (e.g., testing with random labels) are missing, weakening the credibility of the results.\n\nWhile the authors propose improvements for a camera-ready version (e.g., theory, statistical significance, ablations), the current submission does not meet the high standards of a top-tier conference. The marginal empirical gains, lack of theoretical grounding, and unresolved clarity issues outweigh the method’s potential contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning DNA folding patterns with Recurrent Neural Networks.\" The keywords include machine learning, RNNs, 3D chromatin structure, TADs, and computational biology. The abstract mentions using ML methods like linear models, gradient boosting, and RNNs (specifically bidirectional LSTM) to predict chromatin folding from epigenetic marks. The main claim is that the LSTM outperforms others, attributing this to the model's memory of sequential DNA states. They also mention identifying informative epigenetic features.\n\nNow, looking at the reviewer comments. There are three reviewers. Let's go through each one.\n\nReviewer 1 points out that the improvement might be due to the loss function (weighted MSE) rather than the LSTM itself. They suggest comparing with a vanilla LSTM and testing generalizability to other datasets or features like ATAC-seq. The reviewer also questions the choice of 11 in the loss function.\n\nReviewer 2 has several minor issues: unclear equation (K?), wording on wMSE, citation error, figure labels, and lack of discussion on why regression vs NN. Also, they ask about the huge negative training errors vs positive test errors in Figure 6.\n\nReviewer 3 is more critical. They argue the paper lacks methodological novelty, unclear data/methods description, and weak evaluation. They mention similar prior work not cited, unclear features (binary or continuous?), why bidirectional LSTM over other architectures, using the same loss for all models, feature importance analysis for LSTM, hyperparameter details, and why center hidden state instead of concatenating.\n\nThe authors responded to these points. They clarified the loss function (K is number of objects), renamed it modified wMSE, fixed citations, rephrased DNA explanation, fixed figure labels, added discussion on NN advantages, addressed generalizability by stating they focused on Drosophila and specific data types, explained the 11 comes from target value distribution, added more metrics, and addressed hyperparameters and architecture choices.\n\nNow, evaluating based on top-tier standards. Key factors: novelty, methodology, clarity, significance, and thoroughness.\n\nFirst, novelty. Reviewer 3 says it's similar to prior work and lacks methodological novelty. The authors claim no prior work predicts TAD characteristics from ChIP-seq data, but they mention some workshop papers. If those are indeed not in main conferences/journals, maybe their approach is novel in that specific application. However, the method (using LSTM) isn't novel; it's applying existing models. The key contribution is showing that sequential models (LSTM) perform better here, which is a nice insight but maybe incremental.\n\nMethodology: The authors used bidirectional LSTM, compared with linear models and GB. They addressed some reviewer points by adding more metrics and clarifying features. However, Reviewer 3 points out unclear feature representation (they clarified it's float values from ChIP-Seq integrated over 20kb bins). The loss function's 11 is explained as based on target distribution. The hyperparameters were tuned, but the response says they added info in section 4.3. The bidirectional choice is justified by DNA's double-stranded nature, but the reviewer asked why not other architectures like CNNs. The authors didn't address that directly except saying sequential order matters. Maybe a fair point, but they did compare with non-RNN models.\n\nClarity: The authors fixed some issues (equation, citations, figures). The initial paper had a sparse equation and unclear sections, but responses indicate improvements. However, the core explanation of the model's rationale (e.g., bidirectional for DNA's double-strandedness) might still be weak. The DNA's directionality argument is a bit hand-wavy; the model uses bidirectional processing of features, not the actual DNA strand direction. The reviewer pointed out that DNA contacts can be non-adjacent, so why RNN? The authors didn't fully address that, just said sequential order matters. Maybe the features are sequential, so RNN is appropriate, but the justification could be better.\n\nEvaluation: Reviewer 1 and 3 wanted more baselines and metrics. The authors added more metrics (MSE, MAE, R²) and clarified that all models used the same loss. That's good. However, the lack of testing on other datasets (like ATAC-seq) is a concern for generalizability. The authors say they focused on Drosophila and their specific data, which is a valid choice but limits broader impact. The loss function's design (modified wMSE) might be a confounder if not applied to all models, but they addressed that by using it for all.\n\nSignificance: The problem of predicting TADs from epigenetic data is important. Showing that sequential models capture structure better is a useful contribution. However, the application is niche, and the method isn't groundbreaking. For a top-tier conference, the significance might be borderline unless the results are transformative.\n\nReviewers' votes: Reviewer 3 recommended rejection due to lack of novelty and clarity. Reviewers 1 and 2 had more minor issues but didn't explicitly reject. The authors addressed many points but didn't fully resolve the methodological novelty and clarity concerns. The key issue is whether the contribution is sufficient for a top conference. Since the method isn't novel, and the application might not be sufficiently novel or impactful, plus the clarity issues, it might not meet the high bar. However, the thoroughness of the experiments and the clear improvement over baselines could tip it towards acceptance if the reviewers' concerns are adequately addressed.\n\nLooking at the author responses: They added more metrics, clarified features, addressed loss function, and literature. They also explained the bidirectional choice better. But the core issue of prior work similarity remains. If the cited workshop papers are indeed doing similar things, then the novelty is low. The authors argue those aren't in main conferences, so maybe their work is the first in a journal or main conference. That could be a point in favor.\n\nFinal decision hinges on whether the contribution is novel enough and the methodology is sound despite existing work. Since the authors addressed many concerns and clarified points, but the method lacks novelty, maybe it's a borderline case. However, top-tier conferences usually require more significant contributions. Given Reviewer 3's strong rejection stance and the lack of methodological innovation, I might lean towards reject. But if the application is novel and the results are strong, maybe accept. The abstract says they used ChIP-Seq and Hi-C, predicting TADs from epigenetic features, which might be a new angle. The bidirectional LSTM's success shows the importance of sequential features, which is a valid insight. The responses improved clarity, so maybe it's acceptable now.\n\nHmm. The reviewers' comments were addressed, but the key issues are novelty and methodological contribution. Since the method isn't new, but the application is, and they show better performance, maybe it's a marginal accept. However, top conferences often require more. Alternatively, if the prior work is indeed not in main tracks, then this could be the first, making it acceptable. The authors mention that no other work predicts TAD characteristics from ChIP-seq, so that's a unique angle. The bidirectional LSTM's use here is justified by the sequential nature of the features, even if the model isn't new. The results showing its superiority over linear models and GB is a solid contribution. The loss function's design is a bit ad-hoc but explained. The lack of testing on other data is a limitation but acceptable if the focus is on Drosophila. \n\nConsidering all, the paper has merit but may be on the edge. However, since Reviewer 3's points about lack of novelty and clarity were addressed (added lit review, clarified features, etc.), maybe it's acceptable. The final decision is a bit tough, but I think the authors have sufficiently addressed the concerns, so I'll go with accept.\n</think>\n\nThe paper presents an application of machine learning methods, particularly bidirectional LSTMs, to predict chromatin folding patterns (TADs) in Drosophila melanogaster using epigenetic features from ChIP-Seq data. While the work addresses an important problem in computational biology and demonstrates that recurrent neural networks outperform classical models, several critical issues raised by reviewers remain unresolved or only partially addressed. \n\n### Key Strengths:\n1. **Novelty of Application**: The paper claims to be the first to predict TAD characteristics (e.g., gamma transitional) directly from ChIP-Seq data without using Hi-C maps as input, which is a unique angle. This application could provide new insights into the relationship between epigenetic marks and 3D chromatin structure.\n2. **Methodological Insights**: The bidirectional LSTM’s superior performance highlights the importance of sequential dependencies in chromatin folding, a valuable contribution to the field.\n3. **Response to Feedback**: The authors addressed several technical concerns (e.g., clarifying the loss function, adding evaluation metrics, improving feature descriptions) and expanded the literature review to distinguish their work from prior studies.\n\n### Key Weaknesses:\n1. **Lack of Methodological Innovation**: The core method (bidirectional LSTM) is not novel, and the paper does not propose a fundamentally new architecture or training strategy. The improvement over baselines is attributed to the model’s sequential processing, which is a known property of RNNs.\n2. **Unclear Justification for Key Choices**: The bidirectional LSTM’s rationale (e.g., DNA’s double-stranded nature) remains weakly justified. The authors did not adequately address why a bidirectional approach is necessary over other architectures (e.g., CNNs) or why the center hidden state was chosen instead of concatenating forward/backward states.\n3. **Limited Generalizability**: The paper focuses narrowly on Drosophila and ChIP-Seq data, with no exploration of other organisms, cell types, or data modalities (e.g., ATAC-Seq). This limits broader impact.\n4. **Evaluation Limitations**: While the authors added more metrics, the comparison of loss functions across models (e.g., using the same loss for all baselines) was not fully clarified. The lack of discussion on why regression vs. classification was chosen remains unresolved.\n5. **Clarity and Rigor**: Despite revisions, the paper’s initial lack of clarity (e.g., sparse equations, vague feature descriptions) and incomplete citations of relevant prior work (e.g., workshop papers) raise concerns about rigor.\n\n### Final Decision:\nWhile the paper addresses an important problem and provides useful insights, its methodological contributions are incremental, and unresolved weaknesses in justification, generalizability, and clarity prevent it from meeting the high bar of a top-tier conference. The lack of novelty in the core method and insufficient exploration of alternative approaches or datasets further weaken its case.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by understanding the paper and the reviews. The paper is about distributed mean estimation (DME) and variance reduction, which are important in distributed machine learning, especially for things like parallel SGD. The key contribution here is a new method using lattice theory to achieve better error bounds that depend on the distance between inputs rather than their absolute norms. This is a significant improvement because in many real-world scenarios, the inputs might be large in norm but close to each other, so previous methods that relied on input norms would be inefficient.\n\nLooking at the reviewer comments, there's a mix of positive and constructive feedback. Reviewer 1 points out that the paper could benefit from discussing a straightforward iterative approach and referencing non-distributed mean estimation techniques like median of means. They also mention some minor writing issues. Reviewer 2 thinks the result is interesting but notes that some techniques aren't entirely new, though the combination is novel. They suggest a simpler algorithm using a reduction to ℓ∞ norms. Reviewer 3 praises the clarity and contributions but notes some experimental limitations (only n=2 tested) and a few typos. Reviewer 4 is overall positive, highlighting the problem's importance and the tight bounds, but points out that the practical algorithm doesn't match the theoretical bounds and some constants are hidden in O-notation.\n\nThe author responses address most of these points. They clarify the packing radius definition, explain the practical vs theoretical algorithms, and mention adding experiments for larger n in the appendix. They also plan to include the simpler algorithm suggested by Reviewer 2 and discuss the median of means approach. The experiments' small n issue is addressed by pointing to appendix figures, which they'll move some to the main text.\n\nNow, considering acceptance criteria for a top-tier conference: the problem is fundamental, the solution introduces a novel approach with lattices, provides theoretical guarantees with matching lower bounds, and has practical experiments. The reviewers' major concerns are addressed in the author responses. The iterative approach idea is acknowledged but shown not to improve asymptotic bounds. The experiments, while limited in the main text, do have more in the appendix, and the authors plan to include more. The practical vs theoretical gap is noted but explained as a trade-off, which is common in such works. The typos and minor issues are fixable.\n\nThe key strengths are the novel use of lattice theory leading to better error bounds, the theoretical analysis with lower bounds, and the practical instantiation. The weaknesses are the practical algorithm not matching the optimal bounds and limited main experiments, but these are acknowledged and addressed. Since top conferences value both theoretical contributions and practical relevance, and the paper has strong theoretical results with some experiments, it should be accepted. The authors have shown they can address the reviewers' concerns in the revision.\n</think>\n\nThe paper addresses a fundamental problem in distributed machine learning—distributed mean estimation (DME)—and introduces a novel approach leveraging lattice theory to achieve error bounds dependent on input distances rather than absolute norms. This is a significant advancement, as prior methods suffered from inefficiencies when inputs had large norms but were close to each other. The theoretical contributions include asymptotically optimal communication-to-error trade-offs, lower bounds, and practical extensions using cubic lattices. The experiments, while limited in the main text, are supplemented by appendix results for larger \\( n \\), and the authors commit to addressing this in revisions.\n\n**Strengths:**\n1. **Novelty and Impact:** The use of lattice theory to decouple error bounds from input norms is innovative and addresses a critical gap in existing DME methods. This has direct implications for distributed optimization (e.g., SGD) where gradients may be large but centered around a mean.\n2. **Theoretical Rigor:** The paper provides tight upper and lower bounds, demonstrating optimality. The reduction from \\( \\ell_2 \\) to \\( \\ell_\\infty \\) norms and the analysis of lattice properties are technically sound.\n3. **Practical Relevance:** The cubic lattice-based algorithm is computationally feasible and shows experimental improvements, even if it does not match theoretical bounds in all cases. The authors acknowledge this gap but explain it as a trade-off between optimality and practicality.\n\n**Weaknesses and Concerns:**\n1. **Practical Algorithm Limitations:** The computationally efficient algorithm does not achieve the optimal theoretical bounds, and constants in the asymptotic analysis may matter in practice. However, the authors clarify this distinction and justify focusing on asymptotic results for the theoretical version.\n2. **Experimental Scope:** Initial experiments in the main text focus on \\( n=2 \\), which is narrow. The authors address this by pointing to appendix results for \\( n=8,16 \\) and plan to include more in revisions.\n3. **Minor Technical Issues:** Some definitions (e.g., packing radius) had errors, but the authors committed to corrections. The distinction between theoretical and practical algorithms was not always clear, but this will be clarified.\n\n**Reviewer Feedback and Responses:**  \nThe authors thoroughly addressed reviewers' concerns. They acknowledged the iterative approach suggestion but showed it does not improve asymptotic bounds. They plan to incorporate the simpler \\( \\ell_\\infty \\)-based algorithm and discuss median-of-means techniques. The packing radius definition and Lemma 20’s practicality were fixed, and experimental limitations were mitigated by pointing to appendix results. These responses demonstrate a strong commitment to improving the paper.\n\n**Conclusion:**  \nThe paper’s theoretical contributions are substantial and novel, addressing a key problem in distributed learning. While practical limitations and minor experimental gaps exist, these are acknowledged and addressed in the author response. The combination of foundational theory, practical extensions, and clear paths for improvement aligns with the standards of a top-tier conference. The reviewers’ critiques are constructive but do not outweigh the paper’s merits.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about linking the cerebellum's function to decoupled neural interfaces (DNI) in deep learning, specifically addressing credit assignment and locking problems. The reviewers have mixed feedback. \n\nStarting with Reviewer 1: They think the hypothesis is intriguing and could stimulate discussion but point out clarity issues. The main concerns are about the model being presented in a feedforward setting while the results are recurrent. They also mention that the cerebellar-DNI model isn't clearly differentiated from the original DNI, and there's a need for better references to bootstrapping. The authors addressed these by adding figures and clarifying the temporal focus, so that's good.\n\nReviewer 2 is more critical. They argue the paper doesn't offer new insights, just juxtaposing existing ideas (cerebellum and DNI). They say the key points about DNI and sparsity were already known, so the combination doesn't add enough. The authors tried to counter this by adding new experiments and predictions, but I'm not sure if that's enough. The reviewer's main point is that the paper doesn't advance either field significantly.\n\nReviewer 3's main issues are lack of experimental comparisons and testable predictions. They also question the biological plausibility of gradient computation and the neuron count discrepancy. The authors added figures with predictions and addressed some points, like the Purkinje cell numbers by suggesting thalamic mediation. They also fixed citation errors. However, the core issue of how gradients are computed biologically still seems unresolved, which Reviewer 3 found critical.\n\nLooking at the author responses: They added three new figures and a section with predictions. For example, Fig. 5 links cerebellar structure to task performance, Fig. 6 shows ablation results, and Fig. 7 correlates with experimental data. They also clarified the model's differences from DNI and addressed some biological feasibility points. \n\nThe key strengths are the novel framework connecting cerebellum to DNI, the experiments showing cerebellar deficits mirroring patient data, and the new predictions. However, Reviewer 2's point about lack of new insight is a problem. The authors argue they're opening a new sub-field, which is a valid point. But does the paper's contribution meet top-tier standards? \n\nTop-tier conferences require significant novelty and impact. While the paper is well-written and addresses some reviewer points, the core critique from Reviewer 2 about not adding new insights to either neuroscience or DNI is a concern. The authors' additions might mitigate this, but if the reviewers still feel the synthesis isn't enough, it could be a rejection. However, the revised version includes more concrete predictions and experiments, which might tip it towards acceptance. \n\nThe other reviewers, especially 1 and 3, had constructive feedback that the authors addressed. The main remaining issue is whether the synthesis is novel enough. Since the authors propose a new model (CC-DNI) with biological plausibility and make testable predictions, this could be seen as a sufficient contribution. The fact that they show cerebellar-like deficits in tasks aligns with patient data, which is a strong point. \n\nFinal call: The paper has been improved with additional experiments and predictions, addressing some key concerns. While there's debate on novelty, the integration of cerebellar biology with DNI in a way that explains both motor and cognitive deficits, along with new predictions, likely meets the bar for a top conference. The reviewers' positive comments on clarity and potential impact also support acceptance.\n</think>\n\nThe paper proposes a novel framework linking the cerebellum's function to decoupled neural interfaces (DNI), suggesting that the cerebellum acts as a \"decoupling machine\" to solve credit assignment and locking problems in the brain. The authors argue that this framework bridges systems neuroscience and deep learning, offering insights into cerebellar deficits and improving model efficiency through sparse connectivity. While the reviewers raised valid concerns, the revised manuscript addresses many of these points effectively, though some limitations remain.\n\n### Key Strengths:\n1. **Novel Integration of Concepts**: The paper uniquely connects cerebellar forward/inverse models to DNI's forward/backward locking solutions, proposing a unified framework. This synthesis is a significant contribution, particularly in suggesting that the cerebellum's role extends beyond motor control to broader cognitive tasks, aligning with patient deficits observed in both domains.\n2. **Experimental Validation**: The authors demonstrate that their cerebellar-inspired models (CC-DNI and sCC-DNI) replicate cerebellar-dependent deficits in motor and cognitive tasks (e.g., target reaching, caption generation). This provides empirical support for the framework's biological relevance.\n3. **Testable Predictions**: In response to reviewer feedback, the authors added concrete predictions (e.g., correlation dynamics between cerebellar and cortical activity, task-dependent cerebellar importance, and distributed representations in cortical networks). These predictions strengthen the paper's scientific impact and provide a roadmap for future experiments.\n4. **Biological Plausibility**: The sparse connectivity (sCC-DNI) aligns with cerebellar anatomy and improves learning efficiency, suggesting that biological constraints (e.g., sparsity) may enhance computational performance—a key insight for both neuroscience and deep learning.\n\n### Key Weaknesses and Concerns:\n1. **Novelty Debate**: Reviewer 2 argued that the paper merely juxtaposes existing ideas (cerebellar models and DNI) without novel insights. However, the revised manuscript clarifies the framework's unique contributions (e.g., cerebellar deficits in cognitive tasks, sparse DNI variants) and explicitly ties predictions to experimental findings (e.g., Wagner et al., 2019). This addresses the \"no new insight\" critique, though the synthesis may still be seen as incremental by some.\n2. **Biological Feasibility**: Reviewer 3 questioned how gradients are computed and transmitted biologically. While the authors acknowledged this as future work, they provided plausible hypotheses (e.g., thalamic mediation of signals). The revised manuscript also clarifies that gradients could be replaced with activity predictions in biologically plausible models (e.g., Sacramento et al., 2018), mitigating this concern.\n3. **Comparison with Prior Work**: The original DNI model was not included as a baseline, but the authors clarified architectural differences (e.g., cerebellar-inspired unit ratios) and emphasized that CC-DNI extends DNI by incorporating biological constraints. This distinction is now clearer, though direct comparisons would strengthen the results.\n\n### Conclusion:\nThe revised manuscript significantly improves clarity, addresses reviewer concerns, and adds testable predictions and experiments. While the paper's novelty is debated, its integration of cerebellar biology with DNI principles, alignment with patient deficits, and biological plausibility make it a valuable contribution to both neuroscience and deep learning. The framework opens a new research direction, fulfilling the criteria for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about multitask learning, specifically a method called αVIL that dynamically adjusts task weights during training using model updates. The key points from the abstract are that it's novel in using parameter updates for weight estimation and claims to outperform existing methods. \n\nLooking at the reviewers' comments first. Reviewer 1 says the paper is well-written but lacks theoretical justification and the experiments aren't convincing. They mention the baselines are limited and the results aren't consistent. Reviewer 2 points out the method is ad hoc, not clearly different from existing ones like DIW or meta-learning approaches. The experiments on MNIST show small improvements within standard deviations, and NLU results are mixed. They also note the need for more analysis on weights and better presentation. Reviewer 3 is also critical, saying the algorithm isn't clearly explained (especially regarding α vs. task weights), experiments are underwhelming with minor improvements, and the setup uses only one auxiliary task in vision. They suggest rejection.\n\nThe authors responded by addressing some points: explaining the α parameter, justifying MultiMNIST choice, and mentioning they'll add more analysis in the camera-ready. They acknowledge the need for theory and more experiments but note space constraints. They also point out that improvements in NLU are tough in general, so their results are in line with the field's challenges.\n\nNow, considering top-tier standards. Top conferences usually require strong novelty, solid theoretical grounding, and significant empirical results. The paper's main contribution is using parameter updates for task weighting, which they claim is novel. However, Reviewer 2 and 3 question its novelty compared to existing methods like DIW or meta-learning. The authors argue it's different because it's a general framework using any optimizer, but the reviewers aren't convinced. \n\nThe experiments: On MNIST, the method does better on average but within standard deviations. On NLU tasks, results are mixed, sometimes worse than baselines. The authors mention that multitask learning on NLU often has small gains, so their results aren't too bad, but top conferences usually want clear, consistent improvements. The baselines are standard but maybe not the strongest possible. The lack of statistical significance in some results is a problem. \n\nTheoretical justification is missing, which Reviewers 1 and 3 point out. The authors say they'll add this in the camera-ready, but the current submission lacks it. For a top conference, that's a red flag. \n\nClarity and presentation: Reviewers noted some style issues, but the authors addressed those. The algorithm's explanation, especially α vs. weights, was confusing, but the authors clarified in their response. However, the core idea's novelty and clarity are still in question.\n\nThe main issues are: \n1. Novelty is questionable compared to existing methods (DIW, meta-learning). \n2. Empirical results are marginal and not statistically significant in many cases. \n3. Lack of theoretical backing in the current version. \n4. Experimental setup limitations (e.g., only one auxiliary task in vision). \n\nThe authors have plans to address some points (theory, more experiments) but the current submission doesn't meet the high bar. Top-tier conferences require the work to be both novel and impactful. Since the method's advantage isn't clearly established over existing approaches and the results are underwhelming, especially in key domains like NLU, it's a tough sell. \n\nThe reviewers' consensus leans towards rejection, with two suggesting rejection and one wanting major improvements. The authors' responses don't fully resolve the core issues. Without stronger empirical results or a clearer theoretical contribution, the paper might not be suitable for a top conference now. They might need more work and better experiments to justify acceptance.\n</think>\n\nThe paper proposes αVIL, a method for multitask learning that dynamically adjusts task weights using parameter updates. While the paper is well-written and the problem is relevant, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Novelty and Differentiation**: Reviewers question the method's novelty compared to existing approaches like DIW and meta-learning (e.g., MAML). The authors argue αVIL uses a general optimization framework, but the distinction remains unclear. The lack of a formal analysis or theoretical justification further weakens the contribution's uniqueness.\n\n2. **Empirical Results**: \n   - The experiments show only marginal improvements over baselines, often within statistical uncertainty (e.g., MNIST results are close to baselines, and NLU results are mixed). \n   - The NLU experiments use a limited setup (e.g., only one auxiliary task in vision), and the lack of statistical significance in key results (e.g., NLU tasks) raises concerns about the method's robustness. \n   - The authors acknowledge that multitask gains on NLU tasks are inherently small, but top-tier venues require stronger evidence of superiority over baselines in at least one domain.\n\n3. **Theoretical Gaps**: The absence of theoretical grounding or formal analysis is a significant drawback. While the authors plan to address this in a revised version, the current submission lacks the rigor expected for a top conference.\n\n4. **Clarity and Justification**: The algorithm's design (e.g., the role of α vs. task weights) is poorly explained, and the intuition behind key steps (e.g., line 11 in Algorithm 1) was initially unclear. While the authors clarified some points in their response, the core ideas remain under-explained, making the method feel ad hoc.\n\n5. **Baseline and Experimental Rigor**: The baselines are limited (e.g., standard multitask learning and DIW), and the experimental setup (e.g., single auxiliary task in vision) does not fully test the method's potential. The authors' response highlights computational constraints but does not resolve these limitations.\n\n**Conclusion**: While the paper addresses an important problem and shows promise, the current submission falls short in novelty, theoretical justification, and empirical rigor. The marginal improvements over baselines, lack of statistical significance, and unresolved questions about the method's differentiation from existing work make it insufficient for acceptance at a top-tier conference. The authors' proposed revisions (e.g., adding theory, more experiments) could strengthen the work, but these are not yet reflected in the current version.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions.\" The abstract suggests they're arguing that using language to modulate both top-down and bottom-up visual processing improves performance in referring expression tasks. Their experiments show improvements over baselines, especially with ablation studies showing both components are essential.\n\nLooking at the reviewers' comments:\n\nReviewer 1 points out that the novelty might be thin since they didn't explore many language modeling techniques. They also have some suggestions for figures and citations. The author addressed some of these, like adding citations and figures, but the main concern about novelty remains. The reviewer also asks about layer numbers and interpretation of interactions between language and vision. The authors responded by mentioning they tried different layers and are working on analysis, but maybe not enough yet.\n\nReviewer 2 is concerned about the empirical results. The model doesn't match SOTA on some datasets, suggesting possible overfitting. They also noticed discrepancies in tables and the missing reference to Mei et al. (2018). The authors explained they used that as a baseline but didn't cite it, which is a problem. The reviewer downgraded their score post-rebuttal because of this and the lack of significance tests.\n\nReviewer 3 is very positive, saying the paper is well-written, addresses an important problem, and shows clear improvements. They think it's a strong contribution.\n\nReviewer 4 is critical about conceptual motivation and significance. They want better understanding of why the model works and more analysis. The authors responded with ablation studies but the reviewer still feels the contribution isn't clear enough. They also question the originality compared to Step-ConvRNN, which the authors say is different but maybe not by much.\n\nThe author's rebuttal addresses some points but not all. They mention ablation studies and that their model's mean IoU is 60.74 with low std, but the reviewer points out inconsistencies in the tables. The authors didn't do significance tests, which is a problem for a top conference.\n\nNow, considering all this. The paper has mixed reviews. Reviewer 3 is strongly in favor, but others have valid concerns. The key issues are:\n\n1. **Novelty and Originality**: Reviewer 1 and 4 question if the approach is just a tweak of existing methods (like Step-ConvRNN). The authors argue their model allows ablation studies of top-down vs bottom-up, but if the architecture isn't sufficiently novel, that's a problem.\n\n2. **Empirical Results**: Some datasets show SOTA, others don't. The reviewer's point about overfitting and lack of statistical tests is concerning. Without significance tests, it's hard to trust the improvements are real.\n\n3. **Conceptual Contribution**: Reviewer 4 wants to know why modulating bottom-up helps. The authors did ablations but maybe not enough to explain the mechanism. The cognitive motivation is mentioned but not deeply explored.\n\n4. **Technical Issues**: Missing citations (Mei et al.) and table discrepancies hurt credibility. The authors fixed some but not all.\n\nTop-tier conferences require strong contributions, clear novelty, and solid empirical support. While the paper has some good points (ablations, some SOTA results), the unresolved issues in originality, empirical rigor, and lack of deeper analysis might tip it towards rejection. The fact that two reviewers (2 and 4) are critical, especially about significance and motivation, is a red flag. The positive review from 3 isn't enough if the other issues aren't addressed. The authors didn't fully resolve the missing reference or statistical tests, which are important for a top venue.\n\nHmm, but the authors did address some points. However, the key problem is that the model doesn't outperform others on all datasets, and the conceptual contribution isn't clearly established. Without a stronger argument for why this approach is better (beyond ablation results), it might not meet the high bar. Also, the missing reference to a key baseline (Mei et al.) suggests incomplete related work, which is a problem for a top conference.\n\nFinal call: The paper has potential but the current state doesn't meet the high standards. The empirical results are inconsistent, the novelty is questionable, and there's insufficient analysis. So, I think it should be rejected.\n</think>\n\nThe paper proposes integrating language modulation into both top-down and bottom-up visual processing for referring expression understanding, arguing that this approach improves performance over existing methods. While the work shows promise with ablation studies and some state-of-the-art results, several critical issues undermine its acceptance at a top-tier conference:\n\n1. **Novelty and Originality**:  \n   - Reviewers 1 and 4 question the originality, noting that the model may be a minor tweak of prior work (e.g., Step-ConvRNN). The authors’ defense—that their architecture allows ablation studies—is insufficient to establish a clear conceptual leap. The lack of a strong cognitive or task-based motivation for modulating low-level visual features further weakens the contribution’s significance.\n\n2. **Empirical Rigor**:  \n   - The model underperforms on key datasets (e.g., UNC testA/B, ReferIt), raising concerns about overfitting and generalizability. The absence of statistical significance tests (e.g., bootstrap analysis) undermines claims of improvement. The authors’ rebuttal cites a mean IoU of 60.74 with low variance but fails to reconcile discrepancies between tables, casting doubt on the results’ reliability.  \n   - The missing citation to Mei et al. (2018)—a critical baseline—suggests incomplete related work, further eroding credibility.\n\n3. **Conceptual and Technical Gaps**:  \n   - The paper lacks a clear explanation of *why* bottom-up modulation improves performance. While ablation studies confirm its necessity, deeper analysis (e.g., visualizing language’s effect on low-level features, error analysis) is absent. Reviewer 4’s critique of poor conceptual motivation remains unaddressed.  \n   - The authors’ response to Q2 (interpreting language-visual interactions) is vague, relying on future work rather than concrete results. This limits the paper’s contribution to the broader vision-language community.\n\n4. **Reviewer Consensus**:  \n   - While Reviewer 3 is enthusiastic, two other reviewers (2 and 4) highlight critical flaws in empirical validity and conceptual depth. The post-rebuttal downgrade by Reviewer 2 due to unresolved issues (e.g., missing references, statistical tests) further weakens the case for acceptance.\n\n**Conclusion**: The paper’s contributions are incremental, its empirical results inconsistent, and its conceptual justification underdeveloped. These shortcomings, combined with unresolved technical issues, prevent it from meeting the high standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about multitask learning with a new method called αVIL that dynamically adjusts task weights during training. The reviewers have mixed feedback. \n\nStarting with the positives: The paper is well-written and the method seems intuitive. The experiments cover both computer vision and NLP tasks, which is good. The authors address some reviewer concerns in their response, like explaining the algorithm's components and justifying their experimental setup. They also mention plans to add theoretical analysis and more experiments if accepted.\n\nNow the negatives. Reviewer 1 points out a lack of theoretical justification, which is a big issue for a top-tier conference. The experiments don't consistently outperform baselines, especially in NLU tasks where results are mixed. The baselines might not be strong enough, and the improvements are minor. Reviewer 2 mentions the method is ad hoc, not clearly different from existing ones like DIW or meta-learning approaches. The results on MNIST are close to baselines, and the NLU results don't show a clear advantage. Reviewer 3 also notes that the improvements are minor and statistically insignificant, and the algorithm's novelty is questionable. They suggest rejection because the experiments aren't strong enough.\n\nThe authors respond by acknowledging the need for more theory and experiments but argue that their method does better than DIW in some cases and is less prone to overfitting. However, the reviewers are concerned about the marginal gains and lack of solid theoretical backing. \n\nTop conferences usually require significant contributions and strong empirical evidence. While the problem is important, the method's novelty is in doubt, and the results aren't compelling enough. The authors' responses address some points but don't fully resolve the main criticisms. Without stronger theoretical grounding or more convincing experiments showing clear superiority, especially on more challenging tasks, this might not meet the high bar. \n\nHmm, but the authors did mention that in MultiMNIST, their method outperforms DIW on average. However, Reviewer 2 noted that even there, results are within a standard deviation. The NLU tasks have mixed results. The lack of statistical significance is a problem. Also, the experimental setup in MultiMNIST only uses one auxiliary task, which might not fully test the method's capabilities. The authors' response about space constraints is understandable, but maybe they should have prioritized more impactful experiments over covering multiple domains.\n\nThe theoretical gap is a major issue. Top-tier conferences often require some form of analysis, even if not full proofs. The authors plan to add this, but the current submission lacks it. Since the decision is based on the current submission, that's a strike against acceptance.\n\nConsidering all this, the paper has potential but falls short in key areas. The reviewers' consensus leans towards rejection, especially since the experiments aren't strong enough and the method's novelty is questionable. The authors' responses don't fully address the main concerns, so I think it should be rejected but encouraged to resubmit with improvements.\n</think>\n\nThe paper proposes αVIL, a method for multitask learning that dynamically adjusts task weights during training by leveraging parameter updates. While the paper is well-written and addresses important challenges in multitask learning, several critical issues raised by reviewers and reflected in the author responses suggest it falls short of the standards for a top-tier conference. \n\n### Key Strengths:\n1. **Novelty and Intuition**: The method introduces a novel approach to task weight adjustment by directly using parameter updates, which is a fresh angle compared to heuristic or gradient-based baselines.\n2. **Empirical Scope**: Experiments span computer vision (MultiMNIST) and NLU tasks, demonstrating the method's applicability across domains.\n3. **Author Responses**: The authors acknowledge limitations and propose improvements (e.g., theoretical analysis, ablations, statistical significance tests), indicating a willingness to address concerns.\n\n### Major Concerns:\n1. **Lack of Theoretical Justification**: Reviewers 1 and 3 emphasize the absence of formal analysis or theoretical grounding, which is a critical gap for a top-tier venue. The authors acknowledge this but do not provide current evidence of theoretical validity.\n2. **Marginal Empirical Gains**: \n   - On MultiMNIST, αVIL’s improvements over baselines (e.g., DIW) are small and within standard deviations, suggesting statistical insignificance.\n   - On NLU tasks, results are mixed, with no consistent superiority over baselines. The authors attribute this to the inherent difficulty of multitask learning on these tasks but do not demonstrate a clear advantage.\n   - The experimental setup for MultiMNIST uses only one auxiliary task, limiting the method’s validation in complex multitask scenarios. The authors’ response justifies this choice but does not address the narrow scope.\n3. **Novelty and Differentiation**: Reviewers 2 and 3 question whether αVIL meaningfully distinguishes itself from existing methods like DIW or meta-learning approaches (e.g., MAML). The authors argue that αVIL’s use of gradient-based meta-optimization and simplicity are advantages, but the distinction remains unclear.\n4. **Statistical Rigor**: \n   - NLU results lack standard deviations, making it hard to assess significance.\n   - The use of ensembles for test sets but not development sets introduces inconsistency, which the authors partially address but do not fully resolve.\n5. **Algorithmic Clarity**: Reviewer 2’s critique of the algorithm’s ad hoc nature (e.g., the arbitrary subtraction in Algorithm 1) and Reviewer 3’s confusion about the role of α vs. task weights highlight ambiguities in the method’s design. While the authors clarify some points, the core intuition remains underexplained.\n\n### Author Responses and Mitigation:\nThe authors acknowledge most concerns and propose fixes (e.g., adding theory, statistical tests, ablations). However, these are future plans rather than present contributions. For a top-tier conference, the current submission must stand on its own, and the unresolved gaps in theory, empirical significance, and clarity are decisive.\n\n### Conclusion:\nWhile αVIL addresses an important problem and shows promise, the current submission lacks sufficient theoretical justification, empirical robustness, and clear differentiation from existing methods. The marginal improvements over baselines and unresolved ambiguities in the algorithm’s design weaken its competitiveness for a top-tier venue. The authors’ responses indicate potential for improvement, but the paper as submitted does not meet the high bar for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper introduces Shoot Tree Search (STS), a method that modifies MCTS by expanding multiple nodes per simulation, akin to multi-step expansion. The authors claim it balances depth and breadth of search, offering a bias-variance trade-off similar to TD(n). They tested it on Sokoban and Google Research Football, showing better performance than MCTS and other baselines.\n\nNow, looking at the reviewer comments. Reviewer 1 points out that multi-step expansion isn't new, citing Soemers et al. 2016. They also question the novelty and suggest ablation studies comparing STS and MCTS without value function approximators. The reviewer also found several technical issues in the algorithms and experiments, like incorrect tree policy in MCTS (using greedy instead of UCB), unclear hyperparameters, and zero-initialization being arbitrary. They also mention that Table 2's RandomShooting-PPO underperforming PPO is unexpected.\n\nReviewer 2 thinks the paper lacks intuition on why STS is better, especially in sparse reward scenarios. They note unclear figures and missing explanations for terms like MCTS and random shooting. They also question the empirical details in the appendix figures.\n\nReviewer 3 likes the simplicity but is unconvinced about STS's superiority over MCTS with more simulations (like AlphaGo's 1600). They want more thorough comparisons and insights into underperformance in some tasks.\n\nReviewer 4 argues that combining MCTS with rollouts isn't novel, similar to AlphaGo, and the improvements are marginal. They question the necessity of STS over just tuning MCTS hyperparameters.\n\nReviewer 5 is on the fence. They see the idea as interesting but note missing related work and unclear experiment setups. After revisions, they moved towards acceptance but still have concerns about memory and prior work.\n\nThe authors responded by adding experiments addressing some points: comparing with AlphaGo-style rollouts, testing with more simulations, sparse rewards, and clarifying hyperparameters and backpropagation. They argue their method's novelty in using neural networks for value functions and the specific backpropagation approach, which prior work didn't do. They also address technical issues like hyperparameters and initialization.\n\nNow, considering all this. The main issues are novelty and thoroughness. Reviewer 1 and 4 question if STS is just a known idea applied with neural networks. The authors argue their approach differs in value function usage and backpropagation, which prior work like Soemers didn't do. The experiments in the author response, like the ablation with backpropagation, show that their method's approach matters. The added experiments with more simulations (like 500 nodes) show STS still holds up, which addresses Reviewer 3's concern.\n\nTechnical issues like the MCTS implementation using greedy instead of UCB were pointed out. The authors didn't directly address this in their response, but maybe they fixed it in revisions. The hyperparameter differences between STS and Shooting were explained as part of fair tuning, which makes sense.\n\nThe intuition part was addressed by adding more discussion in the revised version, as per Reviewer 2 and 3. The sparse reward experiments also help show STS's effectiveness.\n\nThe key is whether the contributions are novel enough. Since prior work did multi-step expansion but without integrating neural networks in this specific way, and the experiments show benefits, maybe the paper has merit. The reviewers' mixed scores, with some leaning towards acceptance after revisions, suggest that with clarifications and addressing technical points, it's borderline acceptable.\n\nHowever, top-tier conferences require significant novelty. If the core idea is just applying an existing technique (multi-step expansion) with modern tools (neural networks), but the experiments show clear improvements and the method's unique aspects (like the backpropagation and value function use) are well-explained, then it might be acceptable. The authors' responses seem to address many concerns, especially with added experiments.\n\nFinal Decision: Accept, but only if the revisions properly fix the technical issues (like MCTS implementation) and clearly distinguish from prior work. Since the authors addressed several points and the experiments show promise, and some reviewers moved towards acceptance, I think it's a marginal accept.\n</think>\n\nFinal Decision: Accept\n\n**Detailed Reasoning:**\n\n1. **Novelty and Contribution:**  \n   While multi-step expansion in MCTS is not entirely novel (as noted by Reviewers 1, 4, and 5), the authors argue that their approach differs in critical ways:  \n   - Integration with modern neural network-based value function approximation, which changes the backpropagation dynamics and enables averaging effects akin to TD(n).  \n   - Explicit control over the bias-variance trade-off through the parameter \\( H \\), which prior work (e.g., Soemers et al. 2016) did not emphasize or analyze.  \n   - Empirical validation in challenging domains (Sokoban, GRF) with results showing consistent improvements over MCTS and shooting methods.  \n   The authors’ response clarifies these distinctions and provides ablation studies (e.g., comparing backpropagation schemes) to demonstrate the method’s unique advantages.  \n\n2. **Technical Soundness and Experiments:**  \n   - **Ablation Studies:** The authors addressed Reviewer 1’s concerns by adding experiments comparing STS to AlphaGo-style rollout evaluation and sparse reward settings. These results highlight STS’s superiority in leveraging neural networks for value estimation.  \n   - **Scalability:** New experiments with larger computational budgets (e.g., 500 nodes per move in Sokoban) show STS remains competitive with MCTS, countering Reviewer 3’s skepticism about performance at scale.  \n   - **Hyperparameters and Initialization:** The authors clarified hyperparameter tuning practices and provided justification for zero-initialization, addressing Reviewer 1’s concerns.  \n\n3. **Clarity and Presentation:**  \n   - The authors acknowledged the need for better intuition and presentation. Their response indicates revisions to clarify the method’s motivation (e.g., bias-variance trade-off, macro-action interpretation) and fix ambiguities (e.g., Algorithm 5’s weight assignment, \\( N_p \\) vs. \\( C \\)).  \n   - Minor issues like missing references and unclear figures were addressed in the response, with promises to improve the revised version.  \n\n4. **Weaknesses and Remaining Concerns:**  \n   - **Prior Work:** While the authors clarified differences from Soemers et al. and Coulom’s work, the novelty could be more explicitly framed in the revised paper.  \n   - **MCTS Implementation:** Reviewer 1 noted that the MCTS baseline used a greedy policy instead of UCB. The authors did not directly address this in their response, which is a critical flaw. If unresolved, this could undermine comparisons. However, the authors’ commitment to revising the text and experiments suggests this can be fixed.  \n   - **Memory Usage:** Reviewer 5 questioned scalability, but the authors provided evidence that moderate \\( H \\) values avoid excessive memory growth.  \n\n5. **Reviewer Consensus:**  \n   - Reviewers 1 and 4 were skeptical but did not outright dismiss the work.  \n   - Reviewers 2, 3, and 5 leaned toward acceptance after revisions, with Reviewer 5 increasing their score.  \n   - The authors’ thorough responses and added experiments demonstrate a willingness to address concerns, which is critical for a top-tier venue.  \n\n**Conclusion:**  \nThe paper introduces a method with clear empirical benefits and a novel integration of multi-step expansion with neural networks. While concerns about novelty and technical details remain, the authors’ responses and revisions sufficiently address most critiques. The contributions are significant enough for a top-tier conference, provided the remaining issues (e.g., MCTS implementation) are fixed in the final version.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"GG-GAN: A Geometric Graph Generative Adversarial Network.\" The abstract mentions they're addressing graph generation from a geometric perspective, associating nodes with positions in space and connecting edges based on similarity. They claim to solve three key challenges: complex relations, isomorphic consistency, and exploiting latent distributions. Their main contribution is GG-GAN, a WGAN that's permutation equivariant and scalable for large graphs. They also mention it balances novelty and distribution modeling better than existing methods.\n\nNow, looking at the reviewers' comments. There are five reviewers, with mixed feedback. Let's break down each reviewer's points.\n\nReviewer 1 is generally positive but has concerns about the experiments. They point out that the datasets used are small (9-node graphs and 20-node artificial ones). Since there are only ~12k non-isomorphic graphs for 9 nodes, the datasets with ~10k graphs might have a lot of duplicates. This could affect model performance and the baselines. Also, the numerical evidence isn't convincing because Table 1 shows only one MMD improvement over others, and graphRNN does better in many cases. They suggest comparing with NetGAN and simpler baselines like Watts-Strogatz models.\n\nReviewer 2 has several technical concerns. They question mode collapse/diversity, which is a common GAN issue. The paper claims to model edge dependencies, but the edge sampling is independent Bernoulli, so that might not hold. Corollary 1's existence proof is seen as trivial. Proposition 1's assumption of independent node sampling might be too weak. The \"Avoiding Collision\" section is unclear. They also mention hand-crafted features might limit the model's capacity and suggest testing if z is sampled per node instead of uniquely.\n\nReviewer 3 is positive, calling the method inspiring with good theoretical support and clear writing. That's a plus.\n\nReviewer 4 is more critical. They think the contributions aren't novel enough, as existing work like graphRNN and NetGAN already handle some aspects (isomorphism, scalability). The literature review is lacking, not comparing properly with recent models like TagGen. The theoretical theorems don't seem strongly connected to the method. Experiments show limited improvement over baselines, especially missing comparisons with NetGAN. Scalability claims aren't backed with thorough comparisons, and the reasoning for GAN's speed over autoregressive models is questioned (since GANs might need O(n²) parameters). They suggest weak rejection unless addressed.\n\nReviewer 5 also has mixed feedback. They like the clarity and novelty but note experiments are on small datasets (<=20 nodes), which might not be convincing. The role of φ is confusing, and some sections need more rigor. They also mention typos and incomplete references. They ask questions about how φ is learned and its impact, and why Bernoulli sampling is used instead of differentiable methods.\n\nThe author response isn't available, so I can't consider any rebuttals.\n\nNow, considering all this for a top-tier conference. Key factors: novelty, technical soundness, experimental validation, and comparison with baselines.\n\nFirst, the paper's main claims are addressing geometric graph generation with WGAN, permutation equivariance, and scalability. The reviewers point out that some aspects (like isomorphism handling) might not be novel compared to existing methods like graphRNN. The literature review is lacking, which is a problem because it's hard to see the unique contribution without proper comparison.\n\nThe experiments are a big issue. The datasets are very small (9 nodes), which might not be representative. With only ~12k non-isomorphic graphs for 9 nodes, using a dataset of ~10k could lead to overfitting or redundancy. The baselines aren't strong enough; graphRNN and NetGAN are better in many cases, and simpler models like Watts-Strogatz weren't tested. The scalability claim isn't backed by experiments on larger graphs (they mention tens of thousands but don't show results). The MMD improvement is minimal, so the empirical evidence isn't strong.\n\nTechnical concerns: Reviewer 2 points out that edge dependencies aren't modeled since edges are independent Bernoulli. If that's true, the paper's claim about complex edge dependencies is unfounded. The theoretical parts (like Corollary 1 and Proposition 1) are criticized as trivial or not well-explained, which weakens the theoretical contribution.\n\nMode collapse and diversity are critical for GANs, and the paper doesn't address this adequately. The \"Avoiding Collision\" section is unclear, so there's a lack of justification for some design choices. The use of hand-crafted features might limit the model's expressiveness, and the z sampling method isn't well-justified.\n\nThe writing has issues too, like unclear explanations of φ and some sections needing more rigor. Typos and incomplete references are minor but detract from professionalism.\n\nConsidering all this, the paper has some novel ideas but lacks sufficient experimental validation, proper comparison with strong baselines, and clear explanations of technical aspects. The scalability claim isn't supported, and the theoretical contributions are questioned. For a top-tier conference, the experiments and novelty need to be stronger. The reviewers' concerns, especially from Reviewers 1, 2, and 4, highlight significant flaws that aren't addressed in the paper. Without a strong author response, it's hard to justify acceptance.\n\nFinal Decision: Reject\n</think>\n\nThe paper proposes GG-GAN, a geometric graph generative adversarial network designed to address challenges in graph generation, such as isomorphism consistency and scalability. While the work introduces an interesting approach with theoretical underpinnings and claims competitive performance, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Concerns:\n1. **Experimental Validation**:\n   - **Dataset Limitations**: The use of small datasets (9-node and 20-node graphs) raises concerns about overfitting and redundancy. With only ~12,000 non-isomorphic graphs for 9-node graphs, the datasets may contain significant isomorphic duplicates, which could bias results and limit generalizability. Larger-scale experiments (e.g., graphs with thousands of nodes) are missing, despite claims of scalability.\n   - **Weak Baseline Comparisons**: The experiments fail to compare with strong baselines like NetGAN and Transformer-based models (e.g., TagGen), which are relevant for scalability and graph quality. Simpler models (e.g., Watts-Strogatz) are also omitted, making it difficult to assess the method’s superiority.\n   - **Limited Performance Gains**: Table 1 shows minimal improvements over baselines (e.g., graphRNN outperforms GG-GAN in most metrics). The claimed \"novelty\" and \"modeling distribution statistics\" are not convincingly demonstrated.\n\n2. **Technical Soundness**:\n   - **Edge Dependencies**: Reviewer 2 highlights that edge sampling uses independent Bernoulli distributions, contradicting the claim of modeling complex edge dependencies. This undermines a key contribution.\n   - **Theoretical Gaps**: Corollary 1 and Proposition 1 are criticized as trivial or poorly motivated. The \"Avoiding Collision\" section lacks theoretical justification, and the role of hand-crafted features (e.g., φ) is unclear, potentially limiting the model’s expressiveness.\n   - **Mode Collapse and Diversity**: No evidence is provided to address mode collapse, a critical issue for GANs. The paper also does not explore the impact of sampling strategies (e.g., per-node vs. unique z).\n\n3. **Novelty and Literature Review**:\n   - **Overlapping Contributions**: Existing methods like graphRNN and NetGAN already address isomorphism and scalability, but the paper fails to clearly distinguish its contributions. The literature review is incomplete, omitting recent works (e.g., TagGen) and failing to articulate how GG-GAN advances the field.\n   - **Scalability Claims**: The assertion that GG-GAN is \"significantly faster\" than autoregressive models is unconvincing without runtime comparisons or justification for why GANs avoid O(n²) complexity.\n\n4. **Clarity and Rigor**:\n   - **Ambiguities in Methodology**: Sections like \"Avoiding Collision\" and the role of φ are poorly explained. Theoretical claims lack rigor (e.g., undefined terms like \"complete representation\" in Section 2.1).\n   - **Minor Issues**: Typos, incomplete references, and an unnecessary ethics section detract from professionalism.\n\n### Conclusion:\nWhile GG-GAN introduces an intriguing geometric perspective for graph generation, the paper falls short in experimental rigor, baseline comparisons, and technical clarity. The datasets are too small, the theoretical contributions are underdeveloped, and critical flaws in edge modeling and scalability remain unaddressed. For a top-tier conference, these shortcomings outweigh the potential contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about applying meta-learning theory to improve few-shot learning algorithms through regularization. The reviewers have mixed opinions. \n\nFirst, Reviewer 1 says the regularization methods aren't novel, comparing them to spectral normalization and weight decay. The authors responded by emphasizing that their approach combines two assumptions from theory, which together are necessary, and that each regularization serves a different purpose. They also added experiments showing that using just one doesn't work as well. That's a good point. But the reviewer still thinks the methods are known. Hmm.\n\nReviewer 2 points out inconsistencies between the theoretical assumptions and the practical setup, like the lack of a validation set in the equations. The authors addressed this by clarifying the theoretical vs. practical setup and adding explanations. They also mentioned that the assumptions are about the optimal predictors, which might not be controllable, but they can influence them through regularization. The experiments on synthetic data were suggested, which the authors are working on. That's a positive step but maybe not fully addressed yet.\n\nReviewer 3 is concerned about the statistical significance of the improvements and the terminology. The authors clarified that the improvements are statistically significant and adjusted the language from \"ensuring\" to \"respecting\" assumptions. They also added synthetic experiments. That helps, but the initial results might have been borderline.\n\nReviewer 4 questions the novelty again, especially the second regularization being like L2, and the applicability to complex models. The authors argue their approach targets specific parts (linear predictors) differently than general weight decay. They also mention that SVD gradients are handled by frameworks. The experiments with more baselines were added, which is good. But the reviewer still thinks the contribution isn't strong enough for a top conference.\n\nThe authors' response seems to address many points: adding experiments, clarifying differences from existing methods, and explaining their theoretical grounding. However, the main criticism is that the regularization isn't novel. The paper's contribution is more about applying theory to practice, which is a valid angle. Even if the techniques are known, combining them based on theory and showing their effectiveness in this context could be a contribution. The experiments do show improvements, and the authors have strengthened their case with additional results and clarifications.\n\nTop-tier conferences value both theoretical grounding and practical impact. The paper bridges theory and practice, which is a plus. The reviewers' concerns about novelty and experimental rigor were addressed in the response. While the methods might not be groundbreaking, the application of theory to improve meta-learning algorithms is a solid contribution. The experiments, though initially questioned, were bolstered with more data and statistical analysis. \n\nConsidering all this, the paper's unique angle in connecting theory to practice, along with the addressed concerns, makes it a borderline case. But since the authors have made efforts to strengthen their claims and the work is a step towards understanding meta-learning through theory, I think it should be accepted.\n</think>\n\nThe paper aims to bridge the gap between meta-learning theory and practice by incorporating theoretical assumptions into regularization terms for meta-learning algorithms. While reviewers raised concerns about novelty, experimental rigor, and theoretical alignment, the authors provided substantive responses addressing these points. Here's the breakdown:\n\n### Key Strengths:\n1. **Theoretical-Practical Bridge**: The paper’s core contribution is applying recent meta-learning theory to improve algorithm design, a novel angle that addresses a gap in the literature. Even if the regularization techniques (e.g., singular value and Frobenius norm penalties) are not entirely new, their specific application to enforce theoretical assumptions in meta-learning is a meaningful contribution.\n2. **Empirical Validation**: The authors demonstrated improvements over vanilla meta-learning methods on standard benchmarks. They addressed concerns about statistical significance by clarifying their experimental setup and adding results with hyperparameter tuning. The inclusion of additional baselines (e.g., Meta-Curvature) strengthens the evaluation.\n3. **Clarity and Response**: The authors thoroughly addressed reviewer critiques, clarifying distinctions from prior work (e.g., spectral normalization vs. their singular value ratio penalty) and explaining the necessity of combining both regularizations to satisfy theoretical assumptions. They also acknowledged limitations (e.g., task distribution assumptions) and proposed synthetic experiments to further validate their approach.\n\n### Key Weaknesses:\n1. **Novelty Concerns**: Reviewers questioned the novelty of the regularization techniques, noting similarities to weight decay and spectral normalization. The authors countered by emphasizing the unique goals of their approach (e.g., enforcing theoretical assumptions rather than generalization via sparsity) and provided evidence that combining both regularizations is critical for effectiveness.\n2. **Theoretical-Practical Gaps**: Reviewer 2 and 3 highlighted discrepancies between the theoretical assumptions (e.g., i.i.d. tasks) and practical settings (e.g., fixed tasks, lack of validation sets). The authors acknowledged these gaps but argued that their regularization can still \"respect\" assumptions in practice, even if they cannot be fully enforced. Synthetic experiments were proposed to better isolate the effects of their approach, which would further strengthen the paper.\n3. **Technical Details**: Reviewer 4 noted missing details on SVD gradient calculations. The authors clarified that these are handled by modern frameworks, but explicit derivations in the supplementary material would have been helpful.\n\n### Final Assessment:\nWhile the paper’s technical contributions are incremental, its primary value lies in demonstrating how meta-learning theory can guide practical algorithm design—a non-trivial task that has been under-explored. The authors addressed most critiques effectively, particularly by clarifying their approach’s unique goals and providing additional experiments. The work aligns with the conference’s focus on advancing foundational understanding and practical applications of meta-learning. The improvements over baselines, though modest, are statistically significant and demonstrate the utility of the proposed method. The theoretical insights and empirical validation justify its acceptance, especially given the growing interest in connecting meta-learning theory and practice.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Deep Ecological Inference,\" and the keywords include ecological inference, representation learning, multi-task learning, and Bayesian deep learning. The abstract mentions they're using an efficient loss function approximation for ecological inference, applying it to linear models, DNNs, and Bayesian NNs. They tested this on Maryland 2018 election data, showing improvements over benchmarks with polling data. They also talk about joint learning of multiple races and latent representations.\n\nNow, looking at the reviewer comments. There are four reviewers. Let me go through each one.\n\nReviewer 1 points out several weaknesses. The paper is hard to follow, with structural issues like not introducing Poisson binomial/multinomial losses early, unclear input data, and evaluation tasks. Also, lack of implementation details for methods and baselines. Typos and formatting issues are mentioned too. The strengths are that the problem is important, the approach is interesting, and real data evaluation.\n\nReviewer 2 thinks the paper is a solid attempt but has incomplete experiments. They mention missing baselines like standard MLM (multi-level model), which is a problem because the paper's models might not actually be better than existing methods. The experiments don't go deep enough, like not explaining why the deep MLM performs better in some aspects. Also, the linear model has the best R², which isn't discussed. Reproducibility issues: no details on train/test splits, network architecture, latent features used, etc. Figures and tables have missing data. Typos again. The conclusion is that it's preliminary due to incomplete analysis and experiments.\n\nReviewer 3 is more positive. They see the problem as important, especially for gerrymandering cases. The approximation is useful, but they want more on hyperparameters and comparisons to state-of-the-art methods. They mention a typo but overall think it's publishable given space constraints.\n\nReviewer 4 is critical. They say the approach isn't new, related work is missing (like LLP, distribution regression, etc.), and the paper isn't well-written, especially the experiments. They point out that the loss function might be equivalent to existing methods (like Collective Graphical Models). The experiments are not well-organized, and results aren't insightful. They also mention missing references and unclear figures.\n\nAuthor response isn't available, so we can't consider any rebuttals.\n\nNow, considering the standards of a top-tier conference. Top conferences usually require strong technical contributions, clear writing, thorough experiments, and proper comparison to baselines. Let's weigh the points.\n\nStrengths:\n- Addresses an important problem (ecological inference) with potential real-world impact (elections, gerrymandering).\n- Proposes combining deep learning with ecological inference, which is a novel angle.\n- Uses real-world data (Maryland elections) and shows some improvements over benchmarks.\n- The idea of joint learning and latent representations is interesting.\n\nWeaknesses:\n- Lack of clear baselines: Reviewers 2 and 4 point out that standard MLM isn't compared. Without knowing how the proposed models compare to existing methods, it's hard to claim superiority.\n- Experiments are incomplete and not reproducible. Missing details on hyperparameters, splits, and implementation. This is a big issue for reproducibility and validation.\n- Writing and structure issues: The paper is hard to follow, with unclear sections and typos. Top conferences require clear exposition.\n- Related work is insufficient. Reviewer 4 mentions missing key references (LLP, Collective Graphical Models), which might mean the contribution isn't as novel as claimed.\n- The linear model outperforming in some metrics (like R²) isn't addressed, which could undermine the proposed methods' value.\n- Figures and tables have inconsistencies, making results less clear.\n\nThe reviewers' consensus seems mixed. Reviewers 1 and 2 are critical about experiments and clarity, while 3 is more positive but acknowledges some gaps. Reviewer 4 is quite negative, pointing out lack of novelty and poor writing.\n\nTop-tier conferences usually require strong technical contributions and thorough evaluations. Here, the main issues are the lack of proper baselines (like standard MLM and other related methods), incomplete experiments, and reproducibility concerns. Without comparing to existing methods, it's hard to argue that this is a significant advancement. The writing issues also make it less accessible, which is a problem for a top venue where clarity is key.\n\nAdditionally, the fact that a simpler model (linear regression) performs better in some aspects without explanation is a red flag. The authors need to address why their complex models are better in other tasks but not in standard metrics. The experiments don't seem to provide enough evidence to support the claims made.\n\nConsidering all this, even though the problem is important and the approach has potential, the current submission falls short in methodology rigor, experimental thoroughness, and clarity. It might be more suitable for a venue with lower standards, but for a top-tier conference, these issues need to be addressed. Since the authors can't respond here, and based on the reviews, I think the paper should be rejected unless major revisions are made, which the reviewers don't indicate are possible in this version.\n</think>\n\nThe paper addresses an important problem in ecological inference, proposing the use of deep learning techniques to improve predictions of individual-level labels from aggregate data. The application to real-world election data and the exploration of multi-task learning and latent representations are promising. However, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Lack of Strong Baselines and Comparisons**: Reviewers 2 and 4 highlight the absence of comparisons to standard multi-level models (MLMs) and other established methods (e.g., LLP, Collective Graphical Models). Without demonstrating superiority over these baselines, the paper cannot convincingly argue for the novelty or utility of its proposed approach. The claim that \"increased network depth and joint learning improve accuracy\" is weakened by the lack of such comparisons.\n\n2. **Incomplete and Unreproducible Experiments**: Key experimental details (e.g., hyperparameters, train/test splits, network architectures) are missing, making reproducibility difficult. The paper also fails to explain why a simple linear model outperforms deeper models in some metrics (e.g., R²), which raises questions about the robustness of the proposed methods. Figures and tables contain inconsistencies (e.g., missing baselines in plots), further obscuring the results.\n\n3. **Clarity and Writing Issues**: The paper is poorly structured, with unclear explanations of input data, evaluation tasks, and technical details (e.g., Poisson binomial/multinomial losses introduced late). Typos and formatting errors further hinder readability, which is unacceptable for a top-tier venue.\n\n4. **Novelty Concerns**: Reviewer 4 argues that the core idea (approximating ecological inference via loss functions) may not be novel, as similar approaches exist in related fields (e.g., Collective Graphical Models). The paper does not adequately contextualize its contributions within existing literature, weakening its technical significance.\n\n5. **Incomplete Analysis**: The paper conjectures about model behavior (e.g., \"inductive biases facilitate estimation of interactions\") without quantitative evidence. Stronger analysis, such as ablation studies or interpretability experiments, would be necessary to validate these claims.\n\nWhile the problem domain is impactful and the proposed methods have potential, the current submission lacks the rigor, clarity, and thoroughness required for a top-tier conference. The absence of critical baselines, incomplete experiments, and poor writing collectively prevent the paper from meeting the high standards of such venues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let me start by going through all the information provided. The paper is about using CycleGAN for automatic music accompaniment in the Mel-frequency domain. The reviewers have given mixed feedback, so I need to weigh the strengths and weaknesses carefully.\n\nFirst, the title was criticized as misleading because it mentions \"Automatic Music Production,\" which is broader than just accompaniment. The authors addressed this by keeping the title but adding clarifications in the abstract and introduction. They argue that accompaniment is part of production, but I think the title still might be an issue since production includes mixing/mastering. However, their response shows they tried to clarify the scope, so maybe that's acceptable.\n\nThe abstract had some claims about being the first to treat audio as images, but reviewers pointed out that's common in MIR. The authors added references to existing work, which should help. They also rephrased some parts to avoid overstatements. That's good, but the initial overstatement was a problem.\n\nIn the introduction, the authors didn't motivate the shortcomings of their approach, like source separation issues. They addressed this by adding details about Demucs and its limitations. They also mentioned computational cost without quantifying it, but they clarified that it's about parallelization vs autoregressive methods. That seems better now.\n\nRelated work was lacking symbolic domain references. The authors added those, which is necessary. The claim about raw audio being better than symbolic was too strong, so they softened it by discussing pros and cons. That's an improvement.\n\nMethod section had gaps on Demucs and CycleGAN details. The authors provided more info on Demucs's time-equivariance and architecture, and added a section on CycleGAN specifics. They also addressed phase retrieval, which was a major concern. Including code upon acceptance should help reproducibility.\n\nExperiments: The datasets' details were missing, but authors added song IDs and dataset specifics. The evaluation metrics (STOI, FID) were questioned, but they explained their choice as predictive features for human judgment. The human evaluation had issues with subjectivity and expert background, but they added correlation matrices and clarified the binarization rationale. They also addressed the test set uniformity assumption by clarifying they used random samples from songs.\n\nReviewer 2 wanted more experiments like drums2bass, but authors explained that's not the goal and CycleGAN does it anyway. They also mentioned not having space for all variants. The evaluation depth was improved with inter-annotator agreement.\n\nReviewer 3's points on source separation impact and phase retrieval were addressed. The authors added explanations about Demucs performance and phase reconstruction. They also fixed some technical details like windowing and quantization.\n\nOverall, the paper has significant improvements post-rebuttal. They addressed most technical concerns, clarified their contributions, and fixed methodological gaps. The title is still a bit broad but maybe acceptable after changes. The experiments are more thorough now, and the evaluation, while subjective, is better justified. The main issues were either fixed or sufficiently explained. Since top-tier conferences require strong contributions and thoroughness, and given the authors' responses, I think it's borderline but leans towards acceptance with the revisions. However, some reviewers suggested major revisions and resubmit, but since they've addressed most points, maybe it's acceptable now.\n</think>\n\nThe paper proposes a framework for automatic music accompaniment using CycleGAN in the Mel-frequency domain, addressing a niche area within music generation. While the initial submission had several critical shortcomings, the authors' responses demonstrate significant effort to address reviewer concerns. Below is the evaluation breakdown:\n\n### Strengths:\n1. **Novelty and Relevance**: The approach of treating music as images (Mel-spectrograms) for unpaired translation via CycleGAN is a valid contribution to the field of music arrangement, particularly in the audio domain. The problem of automatic accompaniment is understudied compared to symbolic approaches, making this work relevant.\n2. **Technical Rigor Post-Rebuttal**: The authors addressed key methodological gaps (e.g., details on Demucs, phase retrieval, CycleGAN architecture, and dataset specifics). They clarified ambiguities in the source separation process and provided additional experimental details (e.g., song IDs, windowing functions, and quantization steps).\n3. **Evaluation Improvements**: The revised evaluation now includes inter-annotator agreement metrics, clearer descriptions of subjective criteria, and explanations for binarization choices. While subjective evaluation remains challenging, the authors justified their approach and acknowledged limitations.\n4. **Reproducibility**: The commitment to release code and dataset details (e.g., FMA subset IDs) enhances reproducibility, addressing a major concern raised by Reviewer 2.\n\n### Weaknesses and Remaining Concerns:\n1. **Title and Scope**: The title \"Automatic Music Production\" remains misleading, as the work focuses narrowly on accompaniment. While the authors defended their choice, the title overstates the contribution. However, the revised abstract and introduction better clarify the scope.\n2. **Source Separation Impact**: While the authors manually verified Demucs' performance on FMA, they did not quantitatively assess contamination or compare results with smaller datasets (e.g., MusDB-only training). This leaves uncertainty about the robustness of their approach.\n3. **Phase Retrieval and Audio Quality**: Though phase reconstruction details were added, the perceptual artifacts in generated audio (noted by Reviewer 3) remain unresolved. The authors acknowledge this but did not propose solutions.\n4. **Limited Task Scope**: The experiments focus on bass→drums and voice→full arrangement, excluding reverse tasks (e.g., drums→bass). While justified, this limits generalizability and real-world applicability (e.g., typical recording workflows start with drums).\n\n### Conclusion:\nThe paper’s core idea is innovative and addresses a gap in audio-based music arrangement. The authors have sufficiently addressed most technical and methodological critiques, and the revised manuscript clarifies ambiguities. While the title and scope remain slightly overreaching, the improvements in rigor, evaluation, and reproducibility meet the standards of a top-tier conference. The unresolved issues (e.g., source separation impact, phase artifacts) are noted but do not outweigh the contribution’s potential impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is titled PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning. The abstract mentions combining imitation learning (IL) and meta-RL to address the issue of requiring large amounts of interaction data for new tasks. They propose PERIL, which uses demonstrations to precondition exploration policies, improving adaptation rates. The key claims are better performance over baselines like PEARL and MetaIL, handling sparse rewards, and zero-shot adaptation.\n\nNow, looking at the reviewer comments. There are four reviewers, each with their own points. Let me go through each one.\n\nReviewer 1 is generally positive but has several concerns. They like the idea of using demos for meta-RL in sparse reward settings. The empirical results show improvement over baselines, which is good. However, the method is complex with many loss terms (six?), which makes it hard to follow. The notation and math are inconsistent, variable name clashes, and parts of the objective aren't clearly explained. They also question the necessity of all components and suggest ablations. The reviewer mentions that the problem formulation and solution don't align, as the problem states POMDP with changing state but the method assumes fixed z. They suggest either fixing the problem setup or the algorithm. Also, the experiments lack error bars and some baselines (like PEARL with demos). The reviewer's final score isn't given, but the comments are mixed.\n\nReviewer 2 points out several issues. They note that the related work is incomplete, missing key papers like Ghasemipour et al. and others. The method section has unclear explanations, especially around equation 1 and the variational encoder. They question the novelty compared to Yu et al. and criticize the lack of comparisons to prior works that combine meta-IL and meta-RL (Mendonca et al., Zhou et al.). The experiments don't compare to standard BC baselines, and there's confusion about reward types (sparse vs dense). The reviewer also mentions unclear terminology and notation, making the paper hard to parse. They give a 3/5 for quality, 2/5 for clarity, etc., and recommend rejection.\n\nReviewer 3 is more critical. They note that the paper makes many claims without sufficient substantiation. The experiments lack statistical significance (no multiple seeds, confidence intervals). The method is complex, and the experiments don't address high-dimensional observations (like images), which the introduction mentions as a limitation of prior work. They also point out that the \"zero-shot\" claim might not hold since the method requires some interaction. The reviewer mentions the method is too similar to PEARL and Yu et al. without clear differentiation. They give a 2/5 for significance and recommend rejection.\n\nReviewer 4 also has concerns. They mention the strong assumption of having an expert policy conditioned on z, which might not be realistic. Clarity issues in problem formulation and notation (like undefined losses in the algorithm). Related work is lacking, and experiments don't test tasks requiring sophisticated exploration. They initially recommend rejection but suggest possible improvements in rebuttal.\n\nPutting this together, the main issues across reviewers are:\n\n1. **Clarity and Presentation**: Multiple reviewers point out confusing notation, inconsistent math, and lack of clear explanations. The method is complex with many loss terms, and the paper doesn't sufficiently explain each component's necessity. The problem setup vs. algorithm inconsistency is a problem.\n\n2. **Experimental Rigor**: Lack of proper baselines (e.g., PEARL with demos, Mendonca et al.), insufficient statistical analysis (no error bars, few seeds), and unclear experimental setups (sparse vs dense rewards). The \"zero-shot\" claim is questionable.\n\n3. **Originality and Related Work**: The method is seen as an extension of existing works (PEARL, Yu et al.) without enough differentiation. The related work section is incomplete, missing key references.\n\n4. **Assumptions and Claims**: The assumption of expert policies conditioned on z is unrealistic. The experiments don't fully support all claims, especially regarding zero-shot learning and high-dimensional observations.\n\n5. **Complexity**: The method's complexity might be a barrier, and without ablations, it's hard to see which parts are essential.\n\nOn the positive side, the idea is promising, and the empirical results show improvement over baselines. However, the reviewers' concerns about clarity, rigor, and originality are significant for a top-tier conference. Top conferences require not just good ideas but also thorough execution, clear presentation, and proper contextualization within existing literature.\n\nThe reviewers' scores (especially Reviewers 2, 3, and 4) lean towards rejection. While Reviewer 1 is somewhat positive, their concerns about complexity and clarity are substantial. The paper's current state doesn't meet the high standards required for acceptance without major revisions, which might not be feasible in the rebuttal period.\n\nTherefore, considering all factors, the paper should be rejected. The authors need to address the clarity issues, strengthen experiments with proper baselines and statistical tests, better differentiate their work from prior methods, and validate their claims more thoroughly.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\nThe paper proposes PERIL, a method combining imitation learning (IL) and meta-reinforcement learning (meta-RL) to improve task adaptation. While the idea of leveraging demonstrations to enhance meta-RL in sparse-reward settings is promising, the paper falls short in several critical areas for a top-tier conference:\n\n1. **Clarity and Presentation**:\n   - **Notation and Formulation**: Multiple reviewers highlighted inconsistent notation (e.g., variable name clashes, undefined losses like \\(L_{mi}\\) and \\(L_{KL}\\)), unclear explanations of key equations (e.g., Equation 1), and a mismatch between the problem formulation (POMDP with dynamic state \\(z\\)) and the algorithm (fixed \\(z\\)). This undermines reproducibility and understanding.\n   - **Complexity**: The method’s numerous loss terms (six or more) and lack of clear ablations make it difficult to discern which components are essential. The paper does not sufficiently justify the necessity of each term, leaving reviewers questioning whether a simpler approach could achieve similar results.\n\n2. **Experimental Rigor**:\n   - **Baselines and Comparisons**: The paper fails to compare against critical baselines like PEARL with demonstrations, Mendonca et al., or Zhou et al., which are directly relevant to combining meta-IL and meta-RL. The absence of standard BC baselines and noisy BC explanations weakens the empirical claims.\n   - **Statistical Significance**: Experiments lack error bars, multiple seeds, and statistical tests, making it hard to assess the significance of results. The \"zero-shot\" claim is questionable since the method requires some interaction data.\n   - **Task Realism**: Experiments do not address high-dimensional observations (e.g., images), despite the introduction citing this as a limitation of prior work. The use of SAC-generated \"expert\" demonstrations may overstate the method’s real-world applicability.\n\n3. **Originality and Related Work**:\n   - **Overlapping Prior Work**: The method is seen as an extension of PEARL and Yu et al. without clear differentiation. Key contributions (e.g., auxiliary losses) are not sufficiently contextualized against existing methods, and related work sections omit critical references (e.g., Ghasemipour et al., Wang et al.).\n   - **Assumptions**: The requirement for an expert policy conditioned on the latent \\(z\\) is unrealistic, as \\(z\\) is learned by PERIL itself, creating a circular dependency.\n\n4. **Claims and Validation**:\n   - **Unsubstantiated Claims**: The paper makes broad claims about zero-shot learning, multi-family adaptation, and applicability to robotics without sufficient evidence. For instance, experiments on 3D tasks are limited, and the \"zero-shot\" performance is not clearly defined.\n   - **Lack of Ablations**: Without ablations isolating the impact of each loss term (e.g., \\(L_{aux}\\), \\(L_{bc}\\)), it is impossible to assess the method’s core innovations.\n\n5. **Technical Soundness**:\n   - **Algorithmic Gaps**: The problem formulation assumes a POMDP with dynamic state \\(z\\), but the algorithm treats \\(z\\) as fixed. This inconsistency requires clarification (e.g., revising the problem setup or algorithm).\n   - **Reward Confusion**: The use of dense rewards during training (for the critic) versus sparse rewards at test time is ambiguously described, complicating interpretation.\n\n### Conclusion:\nWhile PERIL shows potential in combining IL and meta-RL, the paper’s lack of clarity, incomplete experiments, insufficient comparisons, and unresolved technical ambiguities prevent it from meeting the high standards of a top-tier conference. The reviewers’ concerns—particularly around reproducibility, rigor, and originality—are significant enough to warrant rejection. The authors would need to address these issues through substantial revisions, including clearer explanations, stronger baselines, statistical validation, and better contextualization of contributions, which likely exceed the scope of a rebuttal."
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper on Block Minifloat (BM) should be accepted to a top-tier conference. Let me start by going through all the information provided.\n\nFirst, the paper's title and abstract. They propose BM as a new minifloat format for training DNNs with 4-8 bits for weights, activations, and gradients. The key idea is adding an exponent bias as an optimizable field, allowing smaller exponents and more efficient hardware. They claim better efficiency than FP8 and FP32 in terms of area and energy, with good accuracy on ResNet-18 on ImageNet. \n\nLooking at the reviewers' comments. Reviewer 1 had some questions about novelty, especially regarding the shared exponent bias not being new, but the combination with smaller exponents and Kulisch accumulators might be. They also asked about hardware details like denorm handling and accumulator impact on convergence. The author addressed these by explaining the exponent bias alignment and providing more equations. \n\nReviewer 2 thought the contribution was okay but noted that BM is a simple extension of existing BFP. However, the hardware implementation with Kulisch accumulators was a strong point. They were concerned about the different formats for forward and backward passes increasing hardware overhead, but the authors clarified that the overhead was accounted for in synthesis. The reviewer raised the score to accept after that. \n\nReviewer 3 pointed out clarity issues in the math and tables, but the authors addressed those by adding equations and fixing tables. They also compared to Flexpoint, which the authors differentiated by noting BM's smaller bit sizes and different exponent handling. \n\nThe author responses seem to have addressed most of the reviewers' concerns. The main points are:\n\n1. **Novelty**: BM extends BFP by using shared exponent bias, allowing smaller exponents. The hardware implementation with Kulisch accumulators is a key contribution, especially the efficiency gains. While the core idea isn't entirely new, the combination and hardware evaluation are solid.\n\n2. **Hardware Evaluation**: The synthesis results showing 4x smaller area and 2.3x less energy than FP8 are compelling. The authors clarified that the overhead for multiple formats was considered, which was a major concern. \n\n3. **Clarity and Experiments**: The authors fixed notation issues, added equations, and clarified tables. They tested on ResNet-18 and VGG-16, though not ResNet-50. However, they explained that smaller networks are more relevant for embedded systems, which is a valid point.\n\n4. **Potential Issues**: The use of different formats for forward/backward passes was a concern, but the authors showed that the hardware overhead was accounted for. The Kulisch accumulator's practicality was addressed by linking it to INT8-like efficiency.\n\n5. **Comparison to Prior Work**: They differentiated from Flexpoint and others, showing BM's advantages in bit size and exponent handling.\n\nConsidering top-tier standards, the paper has a clear contribution in hardware-efficient training with BM. The experiments, while not on the largest models, are thorough enough. The hardware synthesis adds significant value over typical quantization papers. The authors addressed most reviewer concerns, and the revised version should resolve the clarity issues. The novelty might be incremental, but the hardware results and thorough evaluation justify acceptance. \n\nFinal Decision: Accept\n</think>\n\nThe paper introduces Block Minifloat (BM), a novel floating-point representation for training deep neural networks with 4-8 bits, emphasizing hardware efficiency. The core contributions are (1) extending block floating-point by introducing an optimizable exponent bias to reduce exponent bits, (2) leveraging Kulisch accumulators for efficient dot-product computation, and (3) demonstrating significant hardware efficiency gains (4.1× smaller area, 2.3× lower energy vs. FP8) with minimal accuracy loss. \n\n### Key Strengths:\n1. **Hardware Innovation**: The integration of Kulisch accumulators with BM formats is a compelling contribution. The synthesis results showing dramatic reductions in area and energy compared to FP8/FP32 are strong evidence of practical value, especially for embedded or low-power training scenarios. This goes beyond typical quantization papers by providing rigorous hardware metrics.\n2. **Novelty in Design Space Exploration**: While the shared exponent concept is not entirely new, the authors systematically explore how reducing exponent bits enables denser hardware (e.g., INT8-like complexity) and validate this with experiments. The hybrid format approach for forward/backward passes, though initially concerning, is clarified to have manageable overhead.\n3. **Comprehensive Evaluation**: The paper tests BM across multiple models (ResNet-18, VGG-16) and datasets, addresses denormal numbers, and provides detailed hardware component breakdowns. The comparison to prior work (e.g., Flexpoint) is thorough, highlighting BM’s smaller bitwidth and simpler exponent update mechanism.\n\n### Addressed Concerns:\n- **Reviewer 1’s Questions**: The authors clarified the role of exponent bias alignment and provided additional equations (e.g., Equation 3) to formalize the dynamic range optimization. They also explained denormal handling and accumulator design in revised sections.\n- **Reviewer 2’s Hardware Overhead Issue**: The authors demonstrated that supporting multiple formats (e.g., BM8’s (2e,5m)/(4e,3m)) does not incur prohibitive hardware costs, as synthesis results already account for necessary multipliers and logic.\n- **Clarity and Notation Issues**: The revised draft fixes ambiguities in equations, tables, and notation (e.g., defining columns in Table 2, adding BM7 to tables), improving readability.\n\n### Remaining Considerations:\n- **Lack of Larger Models**: While ResNet-50 experiments were omitted due to computational constraints, the authors justified focusing on smaller networks for embedded relevance. This is acceptable given the paper’s hardware-centric focus.\n- **Kulisch Accumulator Practicality**: The authors persuasively linked the accumulator’s error-free property to FP32’s sufficiency for training, justifying their approach without requiring full precision.\n\n### Conclusion:\nThe paper advances the state of the art in low-precision training hardware by combining algorithmic innovations (exponent bias optimization) with rigorous hardware evaluation. Despite incremental contributions relative to prior work, the synthesis results and clear methodology elevate its impact. The authors effectively addressed reviewer concerns, and the revised version strengthens clarity and rigor. This work aligns with top-tier conference standards for novel, impactful contributions to efficient DNN training.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about improving uncertainty estimates for deep image classifiers on out-of-distribution (OOD) data using a post-hoc calibration method with outlier exposure. The reviewers have mixed opinions, so I need to weigh their points carefully.\n\nStarting with the strengths: The problem is important, especially since models are often overconfident on OOD data. The method uses contrast-corrupted data for calibration and shows improvements over benchmarks like Ovadia et al. 2019. Reviewers 2, 3, and 4 note consistent improvements and the simplicity of the approach. The authors addressed some concerns in their response, adding experiments on SVHN and other OOD datasets, which helps.\n\nNow the criticisms. Reviewer 1's main issue was that the method assumes the unknown distribution is from a known set, which they felt was a fundamental flaw. But the authors clarified that they use a different corruption (contrast) in calibration and test on other corruptions, so it's not assuming prior knowledge of possible distributions. That seems to address the concern. Also, they added experiments on completely OOD datasets like SVHN, which should mitigate that point.\n\nReviewer 2 pointed out unclear motivation for the multiple image method and lack of experiments beyond corrupted data. The authors added more analysis and experiments on other OOD cases, which helps. The method's intuition is better explained now, though some notation issues remain but are clarified.\n\nReviewer 3 found the paper unclear, especially with terms like p_max and the choice of contrast. The authors defined p_max and explained the contrast choice in the appendix. They also addressed notation and provided more experiments, which should improve clarity. However, the method's generalization to other corruptions is now supported by added tables and figures.\n\nReviewer 4 wanted more analysis on why the method works across different corruptions. The authors added figures and discussed the calibration set choices, which should help. The method's performance on entirely OOD data (SVHN, etc.) is a positive addition.\n\nThe key remaining issue from Reviewer 1 is about baselines. They mentioned that existing methods under distribution shift could be applied but might not perform well with single images. The authors didn't mention adding those baselines, but their rebuttal focused on addressing the distribution assumption. Since the paper's method is post-hoc and simple, maybe the baselines are already covered in Ovadia's work, which they improved upon. The authors did add comparisons using non-corrupted validation sets as a baseline, showing their approach's necessity.\n\nOverall, the authors addressed most concerns through revisions, especially clarifying the method's assumptions and adding experiments. The method's simplicity and effectiveness on a range of OOD cases (including entirely different datasets) are strong points. While there are minor clarity issues, the core contributions seem valid and the improvements significant. The problem is important, and the solution offers a practical post-hoc method without requiring retraining. The initial criticism about distribution assumptions was clarified, so the paper meets the standards for a top-tier conference.\n</think>\n\nThe paper addresses the critical issue of improving uncertainty estimation for deep image classifiers on out-of-distribution (OOD) data, a problem of significant practical importance. The proposed method leverages outlier exposure via contrast-corrupted calibration sets to recalibrate model confidences, demonstrating consistent improvements over prior work on benchmark datasets like CIFAR-10C and ImageNet-C. Key strengths include its simplicity, post-hoc applicability, and effectiveness across diverse corruptions and even entirely OOD datasets (e.g., SVHN for CIFAR-10 models). The authors addressed several reviewer concerns through revisions, including clarifying notation, explaining the choice of contrast corruption, and adding experiments on realistic OOD scenarios (e.g., MNIST → Fashion-MNIST). \n\nWhile initial critiques highlighted potential flaws (e.g., assumptions about known distributions), the authors clarified that their method does not require prior knowledge of test corruptions—instead, it generalizes to unseen corruptions by leveraging a single calibration corruption (e.g., contrast). This distinction aligns with the problem of calibration under distribution shift rather than unsupervised domain adaptation, resolving the core criticism. Remaining concerns about baseline comparisons are partially mitigated by the inclusion of non-corrupted validation baselines and the focus on post-hoc methods. Notational and clarity issues were also improved, though minor ambiguities persist. \n\nThe method’s empirical success, combined with its practicality and theoretical grounding in statistical calibration, positions it as a meaningful contribution to the field. While not revolutionary, it advances the state-of-the-art in a challenging area and provides actionable insights for improving model reliability in real-world applications.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract mention a new Reduction Algorithm for Deep ReLU Networks based on regularization theory. The key contributions seem to be a method to reduce neurons significantly while maintaining function accuracy. The experiments are on toy data and MNIST, which are mentioned by reviewers as insufficient.\n\nLooking at the reviewer comments, there's a lot of criticism. Reviewer 1 says the presentation needs improvement and that experiments should be on more complex datasets and compared to state-of-the-art. Reviewer 2 points out poor writing, vague contributions, and weak experiments (only toy and MNIST without proper comparisons). Reviewer 3 is harsh, noting reliance on unpublished work, similarity to existing architectures like Highway Networks, and insufficient experiments (suggests CIFAR). They also mention typos and lack of statistical significance. Reviewer 4 highlights clarity issues, missing comparisons with baselines, and questions about the theory's rigor. Reviewer 5 says the technique lacks motivation, experiments are insufficient, and parts can't be verified due to unpublished work. Reviewer 6 also criticizes the writing, mentions the method isn't novel (like removing dead ReLUs), and points out undefined terms like \"P-FUNCTIONAL.\"\n\nThe authors' response addresses some points: they agree on clarity issues, mention they need to publish other work first, and explain their contributions as theoretical insights and an algorithm approximating ideal pruning. They clarify their architecture's differences from ResNets and explain their clustering approach. They also mention planning to add more experiments and comparisons but acknowledge current shortcomings.\n\nNow, considering top-tier standards, the key factors are novelty, rigor, experimental validation, and clarity. The paper's main issues seem to be:\n\n1. **Insufficient Experiments**: All reviewers note that experiments are on simple datasets (toy and MNIST) without comparison to state-of-the-art pruning methods. Top conferences require strong empirical validation on standard benchmarks like CIFAR-10/100, ImageNet, etc., especially for methods claiming efficiency.\n\n2. **Lack of Novelty**: Reviewers point out that parts of the method (like removing dead neurons or combining always-active ones) are not novel. The authors argue their theoretical foundation is new, but since it's tied to unpublished work, reviewers can't verify it, which is a problem for reproducibility and trust in the claims.\n\n3. **Clarity and Presentation**: Multiple reviewers mention poor writing, unclear explanations, and missing definitions. For a top conference, the paper needs to be well-written and accessible, especially since the theoretical contributions are central here.\n\n4. **Theoretical Gaps**: Reliance on an unpublished paper for key theory is a red flag. Without being able to reference that work, the current submission's theoretical underpinnings are hard to assess. Also, some steps in the algorithm (like the clustering in step 3) lack clear derivation or motivation, making the method seem heuristic rather than theoretically grounded.\n\n5. **Baseline Comparisons**: The absence of comparisons with existing pruning techniques (e.g., magnitude-based, lottery ticket hypothesis, or more recent methods) undermines the paper's contribution. Without showing superiority or unique benefits over existing methods, it's hard to justify acceptance.\n\nThe authors' response addresses some points but doesn't resolve the core issues. They mention adding experiments against TensorFlow's pruning but haven't done so yet. The theoretical claims still depend on unpublished work, which isn't available for review. While the authors emphasize their contributions, the current submission doesn't meet the standards for a top conference due to the above issues.\n\nIn conclusion, despite potential theoretical insights, the paper's current state—with weak experiments, lack of novelty in practical steps, unclear presentation, and reliance on unpublished work—doesn't meet the bar for acceptance at a top-tier conference. The authors need to significantly improve the experiments, clarify their contributions, and ensure their theoretical foundations are accessible and verified.\n</think>\n\nFinal Decision: Reject\n\nThe paper proposes a neuron pruning method for ReLU networks based on regularization theory, but it falls short of the standards required for a top-tier conference. Key issues include:\n\n1. **Insufficient Experiments**: The experiments are limited to toy datasets and MNIST, lacking validation on more complex benchmarks like CIFAR or ImageNet. There is no comparison with state-of-the-art pruning techniques (e.g., magnitude-based, Hessian-based, or structured pruning methods), making it impossible to assess the method's superiority or practical utility.\n\n2. **Novelty Concerns**: Core steps of the algorithm (e.g., removing dead neurons or combining always-active ones) are not novel, and the theoretical contributions rely heavily on an unpublished work that reviewers cannot verify. While the authors emphasize their theoretical framework, the current submission does not provide sufficient rigor or clarity to establish novelty.\n\n3. **Clarity and Presentation**: The paper suffers from poor organization, undefined terms (e.g., \"P-functional\"), and unclear explanations of critical steps (e.g., the clustering algorithm in Section 4.3). The writing is riddled with typos and grammatical errors, hindering comprehension.\n\n4. **Theoretical Gaps**: The reliance on an unpublished paper undermines the credibility of the theoretical claims. Key derivations and motivations (e.g., the vector representation for clustering) are inadequately explained, leaving reviewers unable to verify the method's validity or understand its foundational principles.\n\n5. **Lack of Baseline Comparisons**: Without demonstrating performance against existing pruning methods, the paper fails to establish its practical relevance. The authors' promise to add comparisons post-review does not address the current submission's deficiencies.\n\nWhile the authors acknowledge these issues and propose improvements, the current version lacks the experimental depth, theoretical clarity, and novelty required for acceptance at a top-tier conference. The paper would need substantial revisions—including rigorous experiments on standard benchmarks, clearer explanations of the method, and independent validation of its theoretical claims—to be reconsidered."
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning.\" The abstract mentions they're using an information-theoretic approach to handle both aleatoric and epistemic uncertainty in continuous action spaces, which is a good angle. They compare against SAC and DSAC and claim better performance on Mujoco. \n\nLooking at the reviewer comments, there are both positive and critical points. Reviewer 1 points out that the presentation is unclear, especially regarding how policies are parameterized and the adaptive exploration over time. They also mention that experiments are preliminary without enough ablation studies. Reviewer 2 notes that the paper is well-written but needs more details on hyperparameters and the target policy. They also mention that DSAC sometimes performs similarly or better, which is a concern. \n\nReviewer 3 is critical, noting that the improvement over DSAC isn't significant except in a couple of cases. They also question the horizon setting and suggest ablation studies on using only one type of uncertainty. Reviewer 4 criticizes the writing as haphazard and the lack of ablations to validate key claims, plus the limited baselines. Reviewer 5 thinks the idea is novel but the paper is hard to follow, with unclear notation and insufficient evaluation, especially in sparse-reward settings.\n\nThe author responses address some points. They added ablation studies on hyperparameters and the effect of distinguishing uncertainties. They clarified some equations and notation, moved some algorithms to the main text where possible, and addressed the horizon issue by stating it's for efficiency. They also compared with OAC and provided more details on sparse environments.\n\nNow, considering the standards of a top-tier conference, the paper needs to have clear contributions, solid methodology, thorough experiments, and good presentation. The main contribution here is combining information-theoretic exploration with distributional RL to handle both uncertainties. The method seems novel, but the reviewers raised several issues.\n\nThe writing was a big problem initially, with confusing notation and unclear explanations. The authors addressed some of this by revising equations and clarifying sections, but if the core sections like 4.1 were still unclear after revisions, that's a red flag. The experiments showed only marginal improvements over DSAC except in specific cases, which weakens the impact. The ablation studies added in the response help, but the original experiments were called preliminary. Also, the comparison with OAC and other methods like VIME was addressed, but the authors argued their method is model-free, which is valid but doesn't fully counter the need for broader comparisons.\n\nThe horizon setting and lack of exploration in sparse environments were concerns. The authors explained the horizon choice and provided more sparse task results, which is good. The mutual information formulation's clarity was improved, but if the notation issues remain, it might still be a barrier.\n\nTop conferences require strong empirical validation. The fact that MQES only shows significant gains in some sparse tasks and not others might limit its general applicability. The method's novelty is there, but the experiments need to better demonstrate its superiority, especially in challenging scenarios where exploration is critical.\n\nConsidering all this, the paper has potential but the current version, even with author responses, might still have unresolved clarity issues and insufficient empirical support. The marginal gains over existing methods, except in specific cases, might not meet the high bar for acceptance. However, if the revisions addressed the major concerns effectively, maybe it's borderline. But given the multiple critical reviews pointing to presentation flaws and limited empirical impact, I'm leaning towards rejection unless the authors can convincingly fix everything, which the responses might not fully assure.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Novelty and Contribution**: The paper proposes MQES, an information-theoretic exploration method that distinguishes between aleatoric and epistemic uncertainty in continuous RL. This is a novel angle, but the empirical improvements over baselines like DSAC are marginal except in specific sparse environments. The contribution is incremental and lacks clear evidence of broad superiority.\n\n2. **Clarity and Presentation**: Multiple reviewers highlighted significant issues with the writing, notation, and logical flow. While the authors addressed some points (e.g., rephrasing equations and clarifying hyperparameters), critical sections like the derivation of MQES (Section 4.1) remain unclear. The lack of Algorithm 2 in the main paper and unresolved notation ambiguities (e.g., mutual information definitions) hinder reproducibility and understanding.\n\n3. **Empirical Evaluation**: \n   - **Baselines**: The comparison with DSAC and SAC is insufficient. While the authors added OAC, they did not address VIME or other intrinsic motivation methods (e.g., DORA), weakening the novelty claim.\n   - **Ablations**: The added ablation studies on hyperparameters and uncertainty types are a step forward, but the original experiments were called \"preliminary\" and lacked statistical rigor (e.g., P-values for significance).\n   - **Sparse Tasks**: While MQES shows improvements in sparse environments, the results are limited to a few tasks. The horizon setting (100 vs. 1000) remains unconvincing without justification for its impact on exploration.\n\n4. **Theoretical Soundness**: The mutual information formulation and its connection to exploration are not clearly justified. The authors revised equations but did not fully resolve ambiguities (e.g., the role of $\\pi^*$ in deterministic environments). The claim that MQES avoids over-exploration is under-supported empirically.\n\n5. **Broader Impact**: The method’s reliance on Q*-guided exploration ties it to external rewards, limiting its utility in sparse-reward settings. While the authors argue this is addressed via epistemic uncertainty, the experiments do not conclusively demonstrate this.\n\n### Conclusion:\nWhile MQES introduces an interesting idea, the paper’s unresolved clarity issues, limited empirical validation, and marginal improvements over baselines fall short of the high bar for a top-tier conference. The authors’ responses address some concerns but do not sufficiently elevate the work to meet the required standards."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract suggest they're proposing a new algorithm to reduce the number of neurons in ReLU networks using regularization theory. The keywords include things like reduction, compression, and pruning, so it's about network pruning. The abstract mentions experiments showing significant neuron reduction with minimal accuracy loss. \n\nLooking at the reviewer comments, there's a mix of positive and negative points. Reviewer 1 initially had concerns but retracted some after author responses. They still want better presentation and more experiments compared to state-of-the-art. Reviewer 2 criticizes the writing quality, vague contributions, and insufficient experiments (only toy and MNIST). Reviewer 3 is more negative, pointing out reliance on unpublished work, similarity to existing architectures (like highway networks), and weak experiments. They also mention typos and lack of statistical significance. Reviewer 4 notes clarity issues, insufficient comparisons, and questions about the theory's basis. Reviewer 5 says the method lacks motivation, experiments are insufficient, and parts can't be verified due to unpublished work. Reviewer 6 also mentions poor writing, referencing unpublished work, and that parts of the method aren't novel (like removing dead ReLUs).\n\nThe author responses address some points, like agreeing on clarity issues and planning more experiments. They emphasize their theoretical contributions and differences from existing methods, but acknowledge the paper's current shortcomings in presentation and experiments.\n\nNow, considering top-tier conference standards, which require strong contributions, clear presentation, solid experiments, and thorough comparisons. The main issues here are:\n\n1. **Theoretical Basis**: The paper heavily relies on an unpublished work, making it hard for reviewers to verify the claims. Top conferences usually require all cited works to be accessible, especially foundational ones. The authors mention this is a concurrent submission but not to ICLR, so it's still an issue.\n\n2. **Novelty**: Reviewers point out that parts of the method (like removing dead neurons or combining always-active ones) might not be novel. The clustering step is the main theoretical contribution, but without clear derivation or comparison to existing methods (like other clustering-based pruning techniques), it's hard to assess its novelty.\n\n3. **Experiments**: The experiments are on toy datasets and MNIST, which are too simple for a top conference. They lack comparisons with state-of-the-art pruning methods (e.g., magnitude-based, Hessian-based pruning). The authors mention they plan to add comparisons with TensorFlow's default pruning, but the current submission doesn't have that. Also, the experiments don't show statistical significance (e.g., standard errors), which is a red flag.\n\n4. **Clarity and Presentation**: Multiple reviewers note poor writing, unclear explanations, and missing definitions. The method's steps aren't well-motivated, and the theoretical parts are too vague. Even after author responses, the presentation issues remain significant.\n\n5. **Technical Soundness**: Some parts of the method (like the clustering formula) have unclear derivations and potential typos. The reviewers question the validity of certain steps, like how combining neurons doesn't change the function, which the authors tried to explain but it's still not clear.\n\n6. **Contribution Strength**: The main contributions are theoretical, but without strong empirical validation on complex datasets (like CIFAR or ImageNet), it's hard to argue practical impact. The authors claim their method is better in theory but need to back this up with experiments.\n\nConsidering all this, the paper has potential but falls short in several critical areas for a top conference. The reliance on unpublished work undermines the theoretical foundation, the experiments are insufficient, and the presentation is too unclear. While the authors acknowledge these issues and suggest improvements, the current submission doesn't meet the standards. Top conferences require polished work with solid experiments and clear exposition. Without addressing these points, especially the experiments and accessibility of the theory, it's unlikely to be accepted.\n</think>\n\nFinal Decision: Reject\n\nThe paper proposes a neuron pruning algorithm for ReLU networks based on regularization theory, but it falls short of the standards required for a top-tier conference. Key issues include:\n\n1. **Theoretical Foundations**: Heavy reliance on an unpublished work makes critical aspects of the method unverifiable. Top-tier conferences require foundational claims to be grounded in accessible literature, and the lack of a citable reference undermines trust in the theoretical basis.\n\n2. **Novelty and Clarity**: While the authors emphasize theoretical contributions, reviewers highlight that parts of the method (e.g., removing dead neurons, combining always-active neurons) are not novel. The clustering step, though theoretically motivated, lacks clear derivation and comparison to existing pruning techniques, making its novelty unclear. The presentation is marred by poor writing, missing definitions, and insufficient motivation for key steps.\n\n3. **Empirical Validation**: Experiments are limited to toy datasets and MNIST, which are insufficient to demonstrate practical impact. The absence of comparisons to state-of-the-art pruning methods (e.g., magnitude-based, Hessian-based) and lack of statistical rigor (e.g., standard errors) weaken the empirical claims. The authors mention plans to add comparisons but the current submission does not include them.\n\n4. **Technical Soundness**: Concerns about the validity of certain steps (e.g., clustering formulas, handling of vector representations) remain unresolved. Reviewers question whether the method truly preserves the learned function as claimed, and the authors’ explanations are insufficient to address these doubts.\n\n5. **Presentation Quality**: The paper suffers from significant clarity issues, including unclear explanations of core concepts, inconsistent notation, and poor organization. Even after revisions, the authors acknowledge the need for substantial reworking, which is beyond the scope of a conference submission.\n\nWhile the authors identify potential improvements and theoretical insights, the current submission lacks the rigor, clarity, and empirical depth required for acceptance at a top-tier venue. Addressing these issues would necessitate a complete overhaul of both the theoretical exposition and experimental evaluation, which is not feasible within the conference timeline."
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Training independent subnetworks for robust prediction,\" and the main idea is using a MIMO architecture to train multiple subnetworks within a single model. The key claim is that this approach achieves the benefits of ensembles without the extra compute, except for a 1% parameter increase. The experiments show improvements in accuracy, NLL, and calibration on several datasets.\n\nLooking at the reviewer comments first. Reviewer 1 points out that while the parameter increase is minimal, the authors still mention \"for free,\" which might be misleading. Also, they want more details on where the 1% comes from and experiments on more datasets like OpenImages. \n\nReviewer 2 thinks the experiments are thorough and the method is strong, but notes that the approach isn't entirely novel compared to existing MIMO methods like BatchEnsemble. They also want more theoretical backing for why the subnetworks become independent. \n\nReviewer 3 finds the idea clever but is concerned about the lack of theory and practical limitations, like needing small M. They also question the training setup where inputs are mixed and then tested with identical copies. \n\nReviewer 4 is critical of originality, pointing out similar multi-branch architectures exist and that the related work section is lacking. They also argue that the computational savings might be deceptive because the batch size effectively gets reduced. The clarity is good but there are some formatting issues.\n\nThe author responses address some points. They clarified the parameter increase comes from first and last layers, added FLOPs numbers, and explained the independence through empirical analysis. They also addressed the batching concerns, showing that inference time is comparable. However, they didn't fully address all originality concerns, just pointing out differences from specific works.\n\nNow, considering the standards of a top-tier conference. The paper has strong empirical results and a novel method, but there are valid concerns about originality and theoretical grounding. The reviewers' main issues are:\n\n1. Originality: Reviewers 2, 3, and 4 mention that similar approaches exist (BatchEnsemble, multi-branch architectures). The authors argue their use of MIMO with multiple inputs is novel, but the related work section might not sufficiently distinguish their approach from prior art. The authors added some comparisons but maybe not enough.\n\n2. Theoretical basis: Multiple reviewers (2,3) want more explanation on why the subnetworks become independent. The authors provided empirical evidence but acknowledge that theory is future work. For a top conference, some theoretical insights might be expected, even if not full proofs.\n\n3. Practical limitations: The method requires M copies of inputs at test time, which could affect batch processing. Reviewer 4 points out that the batch size effectively shrinks, which might not be ideal in practice. The authors addressed this by explaining how batching works, but the concern about computational costs remains.\n\n4. Parameter increase: Even 1% is a trade-off. The authors clarified where it comes from, but the \"for free\" claim might still be misleading if the parameter increase is non-trivial in some architectures.\n\nStrengths: The method is simple, shows significant improvements, and the experiments are thorough. The idea of achieving ensemble benefits without extra compute is compelling.\n\nWeaknesses: Originality is a concern, and the lack of deeper theoretical analysis might limit the contribution's depth. The practical aspects like batch handling and parameter increase need clearer explanations.\n\nConsidering all this, the paper has a solid contribution but faces valid criticisms about originality and thoroughness. Top conferences usually require high originality and strong theoretical backing. The authors addressed some points but might not have sufficiently addressed the originality concerns. The method's novelty is borderline, and without more distinctiveness from existing work, it might not meet the bar. However, the empirical results are strong, and the problem addressed is important. \n\nThe reviewers' scores are mixed. Reviewer 2 and 3 are positive but have reservations. Reviewer 4 is more negative on originality and practicality. The authors did improve the paper based on feedback, but some issues remain. \n\nIn the end, the key question is whether the contribution is novel enough and addresses the concerns sufficiently. Since the authors didn't fully resolve the originality issues and the theoretical gaps, and given that top conferences are highly competitive, I think the paper is on the edge. However, the strong empirical results and the clever idea might tip it towards acceptance if the reviewers' scores average to a borderline accept. But looking at the reviewers' comments, especially Reviewer 4's lower score on originality and significance, it might lean towards rejection. Alternatively, maybe a revise decision, but since the user asks to choose between accept/reject, I'll have to decide.\n\nWait, the user says to conclude with Accept or Reject. Given that some reviewers are positive but others are critical on originality and practicality, and the authors addressed some but not all points, perhaps the final decision is to reject. But the paper does have significant empirical results. Hmm. Maybe the borderline case leans towards accept if the conference allows, but given the originality concerns, perhaps reject. Alternatively, the authors clarified some points, so maybe accept. \n\nLooking at the author responses, they addressed the parameter increase and FLOPs, which were concerns. They also explained the independence through empirical analysis. The main unresolved issues are originality and related work. If the related work section was insufficient, that's a problem. The authors added some comparisons but maybe not enough. Since Reviewer 4 pointed out missing citations, that's a hit. \n\nIn top conferences, originality is crucial. If the method isn't sufficiently novel compared to existing multi-branch approaches, it might not pass. The authors argue their use of multiple inputs is novel, but if similar ideas exist (like BatchEnsemble), then it's a weaker contribution. \n\nFinal call: The paper has strong empirical results but faces significant originality concerns. Without a stronger case for novelty and better handling of related work, it might not meet the high bar. So I think the decision should be to reject.\n</think>\n\nThe paper proposes a novel approach called MIMO to train independent subnetworks within a single model, achieving ensemble-like robustness without significant parameter or compute overhead. The method shows strong empirical results across multiple benchmarks, demonstrating improvements in accuracy, calibration, and robustness. However, several critical concerns arise:\n\n1. **Originality**: While the authors argue their use of multiple inputs to enforce independence is novel, reviewers highlight similarities to existing methods like BatchEnsemble and multi-branch architectures. The related work section is insufficiently detailed, and the authors’ rebuttal only partially addresses these comparisons. The core idea of using MIMO for subnetwork independence may not be sufficiently distinct from prior work, weakening its novelty.\n\n2. **Theoretical Grounding**: Multiple reviewers note the lack of theoretical explanations for why subnetworks become independent. The authors provide empirical evidence (e.g., loss landscapes, activation analysis) but acknowledge that deeper theoretical insights are future work. For a top-tier conference, some theoretical justification or analysis (even heuristic) would strengthen the contribution.\n\n3. **Practical Limitations**: The method’s reliance on replicating inputs at test time reduces effective batch size, which could impact real-world scalability. While the authors clarify batching mechanics, the trade-off between parameter increases (even 1%) and computational costs remains unresolved. The \"for free\" claim is partially undermined by these considerations.\n\n4. **Clarity and Rigor**: Reviewer 4 points out formatting issues and a lack of FLOP/MAC reporting, though the authors addressed some of these in their rebuttal. The paper’s structure (e.g., placing related work after experiments) may also hinder clarity for readers unfamiliar with the field.\n\n**Strengths**: The method is simple, effective, and offers a compelling solution to reducing inference latency for ensembles. The thorough experimental evaluation and clear presentation are significant strengths.\n\n**Weaknesses**: The lack of originality relative to existing multi-branch methods and insufficient theoretical grounding are critical drawbacks. While the empirical results are strong, top-tier conferences typically require more substantial innovation and deeper analysis to justify acceptance.\n\n**Conclusion**: The paper’s contributions are valuable but fall short of the originality and theoretical rigor expected at a top-tier conference. The unresolved concerns about novelty and practical limitations, combined with the competition for limited acceptance slots, tip the decision toward rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is titled \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design.\" The main contributions seem to be a large dataset of over a million slides, a pre-training approach using a transformer model (CanvasEmb), and evaluations on two tasks: element role labeling and image captioning. The reviewers have given their feedback, and the authors have responded to some points.\n\nFirst, looking at the strengths. The dataset is a big plus—over a million slides is impressive. Previous work used much smaller datasets, so this is a significant contribution. The use of a transformer-based model for layout representation is novel in this context, and pre-training is a hot topic inspired by NLP successes. The authors also mention achieving state-of-the-art results on their proposed tasks, which is a good sign.\n\nNow the weaknesses. Reviewer 1 points out that the dataset details are lacking. How were the slides parsed? Are the properties fully automatic? The role labeling dataset's relation to the pretraining data isn't clear. Also, the evaluation tasks might not fit the standard pretraining-finetuning framework. The experiments could be more systematic—like testing geometric and color properties, and more qualitative results. Layout retrieval's manual scoring might be subjective, and baselines like LayoutGAN or Lee et al. weren't compared.\n\nReviewer 2 mentions the method's technical sparsity. The transformer details aren't well explained, and there's a lack of comparison to prior works like Cao et al. (SIGGRAPH 2019) and others. The related work section is weak, not clearly differentiating their approach. They also question why a GNN wasn't considered, as it might be more efficient. The image captioning task is actually a binary classification, which might not be as challenging as presented.\n\nReviewer 3 agrees on the dataset's value but criticizes the evaluation. The baselines are too simple (decision trees), and the tasks might not be complex enough. The layout auto-completion and retrieval evaluations are simplified compared to prior work. They suggest harder tasks and stronger baselines. Also, the transformer's necessity over simpler models isn't justified.\n\nThe author responses address some points. They plan to add more baselines, clarify the dataset details, and improve evaluations. They mention the average elements per slide (6, max 20) and that role labels aren't part of the pretraining data. They also note that font properties matter beyond geometry. However, some issues like GNN comparisons and more detailed model descriptions remain to be addressed in revisions.\n\nConsidering top-tier standards, the paper has strong points but also critical gaps. The dataset is a major contribution, but without sufficient details, it's hard to assess its quality. The method's novelty is there, but the lack of thorough comparisons and a weak evaluation section are red flags. The authors have a plan to address some issues, but the current submission might not meet the high bar yet. However, since they can revise, maybe it's a borderline case. But top conferences often require strong evaluations and clear comparisons. The current paper's experiments are limited, and the model's advantages over simpler methods (like GNNs or CNNs) aren't demonstrated. The reviewers' concerns about the evaluation's thoroughness and baseline comparisons are valid. Without stronger evidence of superiority and better experimental design, it might not be ready. But the dataset and approach are promising. Hmm.\n\nWait, the authors did mention they'll add more baselines and improve evaluations. However, the decision is based on the current submission. Since the current version lacks key comparisons and has evaluation shortcomings, even with the dataset, it might not be acceptable yet. Top conferences require rigorous evaluations. The reviewers' points about missing baselines (like LayoutGAN, Cao et al.), insufficient task diversity, and unclear dataset details are significant. The authors' responses indicate they can improve, but the paper as is doesn't meet the standards. Therefore, I think it should be rejected but given a chance to resubmit if they address all points thoroughly. But the final decision here is based on the current version. So, probably reject.\n</think>\n\nThe paper proposes CANVASEMB, a transformer-based model for learning layout representations through large-scale pre-training on a novel dataset of over one million slides. While the contributions are notable—particularly the dataset and the application of pre-training to layout representation—the current submission falls short of top-tier conference standards due to critical weaknesses in evaluation, methodology clarity, and comparisons to prior work. Here’s the breakdown:\n\n### Strengths:\n1. **Dataset Contribution**: The large-scale slide dataset is a significant contribution, addressing the scarcity of layout data in prior work. This alone could be impactful if properly contextualized and validated.\n2. **Novel Approach**: The use of a transformer architecture for layout representation is innovative and aligns with trends in self-supervised learning.\n3. **Potential Applications**: The proposed tasks (e.g., element role labeling, layout retrieval) demonstrate real-world relevance in graphic design.\n\n### Weaknesses:\n1. **Evaluation Shortcomings**:\n   - **Incomplete Tasks**: The evaluation focuses on limited tasks (binary classification for \"image captioning,\" simplified auto-completion). More complex tasks (e.g., geometry/color prediction, full layout generation) and qualitative visualizations are missing.\n   - **Weak Baselines**: Comparisons are limited to a decision-tree baseline. Key prior works like LayoutGAN, Cao et al. (SIGGRAPH 2019), and READ (CVPR 2020) are not adequately addressed, weakening the paper’s novelty and impact claims.\n   - **Layout Retrieval**: The manual scoring lacks rigor, and there is no systematic evaluation framework.\n\n2. **Methodological Clarity**:\n   - **Transformer Details**: The model architecture and attention mechanisms are under-described, making reproducibility challenging.\n   - **Dataset Ambiguity**: Critical details (e.g., parsing method, element distribution, role labeling process) are omitted, raising concerns about dataset quality and pre-training/fine-tuning separation.\n\n3. **Lack of Motivation**:\n   - The choice of transformer over simpler architectures (e.g., GNNs) is unexplained. Without evidence of superiority over GNNs or CNNs, the model’s necessity is unclear.\n   - The related work section fails to clearly differentiate the approach from prior methods, particularly those focused on layout generation or representation.\n\n4. **Technical Gaps**:\n   - The \"pre-training\" framework does not strictly follow the standard unsupervised-supervised paradigm, as downstream tasks overlap with pre-training objectives. This undermines the validity of the approach.\n   - The role labeling dataset’s relationship to the pre-training data is unclear, risking data leakage or biased evaluation.\n\n### Author Responses:\nThe authors acknowledge many of these issues and propose improvements (e.g., adding baselines, refining evaluations, clarifying dataset details). However, the current submission does not yet meet the required rigor. While the revisions are promising, the paper as presented lacks the thoroughness expected of top-tier venues.\n\n### Final Decision: Reject\n\nThe paper’s contributions are promising but insufficiently validated and contextualized. The evaluation is underdeveloped, comparisons to prior work are incomplete, and critical methodological details are missing. While the authors’ proposed revisions address many concerns, the current version does not meet the high standards of a top-tier conference. Acceptance would require substantial reworking of the experiments, baselines, and clarity of presentation."
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Programmable 3D snapshot microscopy with Fourier convolutional networks.\" The abstract mentions using Fourier-based neural networks to handle non-local optical encodings in microscopy, which existing UNet architectures can't do well. The key contributions seem to be applying Fourier convolutional networks (FourierNets) for end-to-end optimization of optical encoders and better reconstruction compared to UNets, especially in scenarios requiring global context.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Let's break down each reviewer's input:\n\nReviewer 1 improved their score to 6 (marginally above acceptance) after the author response. They mention the evaluation could be better but acknowledge the difficulty in getting real data. They're okay with the lensless imaging experiment despite some implementation differences.\n\nReviewer 2 also accepts the paper after the response, so no major issues here.\n\nReviewer 3's main concern is that the ML ideas aren't novel since Rippel et al. (2015) already did similar work with Fourier networks. They think the paper would fit better in an application-specific venue but are open to ICLR if the application is deemed important enough. The authors addressed some points by citing Rippel but argue their application is novel.\n\nReviewer 4 points out that the technical novelty is low compared to Rippel, and the paper lacks experiments showing the superiority of their approach over Rippel's. They also want more details on implementation (like how real-valued outputs are maintained and parameter counts). However, they acknowledge the good writing and results. Their final recommendation is to accept if the paper addresses the Rippel comparison and resource details.\n\nReviewer 5 is more critical. They think the evaluation is insufficient (only simulated data, one type of volume), and the UNet's poor performance isn't adequately explained. They also note inconsistencies in Table 3 and feel the lensless imaging part is tangential. Their recommendation is weak reject.\n\nReviewer 6 likes the idea but questions the necessity of 10^6 pixels and the domain gap between simulation and real-world. Still, they think it's acceptable with some improvements.\n\nReviewer 7 thinks the method isn't novel in ML terms but the application is solid. They mention existing work like Rippel and positional encodings but find the pipeline well-executed.\n\nAuthor responses addressed some points: they added citations to Rippel and others, clarified the real-valued outputs, provided more details on parameters and training times, and defended the application's relevance to ICLR's scope (neuroscience, computational biology).\n\nNow, considering the criteria for a top-tier conference like ICLR, which values both technical novelty and impact. The paper's main contributions are in applying FourierNets to a specific imaging problem, showing better performance than UNets in their setup. However, the technical novelty is questioned because Rippel's work already used Fourier convolutions. The authors argue their application is novel, which might be valid, but the reviewers (especially 3 and 4) feel the ML contribution is incremental.\n\nThe empirical results are on simulated data, which some reviewers (5,6) find a weakness. The authors justify this by stating that PSF optimization is simulation-based, but real-world validation is still a gap. The lensless imaging experiment adds credibility but is a different application.\n\nThe writing and experiments are thorough, with a good limitations section. The authors addressed many reviewer concerns in their response, but some points remain, like the comparison with Rippel's approach and the exact impact of global vs. local filters.\n\nTop conferences often require a balance between innovation and significance. Here, the application is important (3D microscopy for neuroscience), and the method shows clear improvements. However, the ML technique isn't groundbreaking but is a clever adaptation. The lack of real data might be a hurdle, but the authors argue simulation is sufficient for their claims.\n\nConsidering all this, the reviewers' scores are mostly in the accept range (6s) except Reviewer 5's weak reject. The authors have addressed many issues, and the application's significance might tip the balance. The technical novelty is marginal, but the problem's importance and the method's effectiveness in a niche area could justify acceptance, especially if ICLR's scope includes such applications.\n</think>\n\nThe paper proposes the use of Fourier convolutional networks (FourierNets) to address challenges in 3D snapshot microscopy, where non-local optical encodings require global context for reconstruction. While the technical novelty of FourierNets is limited due to prior work (e.g., Rippel et al., 2015), the authors demonstrate their effectiveness in a novel application domain (programmable microscopy) and provide empirical evidence of superior performance over UNets in both end-to-end optimization and reconstruction tasks. Key strengths include thorough simulations, a well-written manuscript, and a focus on a high-impact application (neuroscience/biological imaging). However, the work faces criticism for incremental ML contributions, lack of real-world validation, and insufficient comparison with prior Fourier-based methods. The authors addressed several concerns (e.g., citations, implementation details), and reviewers generally acknowledged the application’s significance despite technical limitations. While the technical novelty is marginal, the problem’s importance and the method’s demonstrated efficacy in a niche but impactful area align with ICLR’s scope for application-driven contributions. The majority of reviewers (excluding one weak reject) lean toward acceptance after revisions.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Global Node Attentions via Adaptive Spectral Filters.\" The abstract mentions that they're addressing the limitation of GNNs assuming local homophily, which isn't always the case, especially in disassortative graphs. Their solution is a model with a global self-attention mechanism using learnable spectral filters. They claim it outperforms baselines on disassortative graphs and is comparable on assortative ones. \n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewer 1 is skeptical about the novelty and computational complexity. They argue that ChevNet can already handle global information with a large K and that the paper's claims about complexity might be incorrect. Reviewer 2 likes the paper's clarity and empirical results but defers due to lack of theoretical results. Reviewer 3 points out scalability issues and limited dataset sizes, especially for disassortative graphs. Reviewer 4 also questions novelty, mentions complexity concerns, and notes that the tested datasets are small. \n\nThe author responses address some points. They clarify that their method uses learnable spectral filters, which they argue ChevNet can't do because it's limited by fixed polynomial filters. They also mention computational complexity using Chebyshev approximations to keep it manageable, O(m*|E|), and provided runtime comparisons showing their model is faster than some baselines. They added experiments on a larger dataset (Chameleon) and addressed sparsification techniques to improve efficiency.\n\nNow, considering the standards of a top-tier conference, the key factors are novelty, technical soundness, empirical validation, and clarity. \n\nNovelty: The authors claim their main contribution is the adaptive spectral filters allowing global attention without assuming homophily. Reviewers 1 and 4 question the novelty, pointing out that non-local attention and spectral methods exist (like [1] and ChevNet). The authors counter by emphasizing the learnable aspect and the multi-head attention using different spectral filters, which they say ChevNet can't do. However, the reviewers remain unconvinced, especially since ChevNet's polynomial filters can theoretically capture distant nodes. The authors' rebuttal about ChevNet's fixed filters vs their adaptive ones might be a valid point, but the reviewers still see limited novelty. \n\nTechnical Soundness: The complexity argument is crucial. The authors use Chebyshev approximations to reduce computation, claiming O(m*|E|). Reviewer 1 and 4 are skeptical because eigen decomposition is O(N^3), but the authors mention using Chebyshev polynomials which avoid full eigen decomposition, similar to existing methods. Their runtime results show competitiveness, but the datasets tested are still small (Chameleon has 2k nodes). The scalability for larger graphs isn't fully demonstrated, which is a concern for a top conference expecting robustness.\n\nEmpirical Validation: The experiments show improvement on disassortative graphs, which is the paper's focus. However, the datasets are small, and the authors added Chameleon but not much larger ones like Actor (7k nodes). The synthetic graphs with controllable parameters are planned but not yet included. The lack of experiments on larger disassortative graphs and other tasks (like link prediction) weakens the validation. \n\nClarity: Reviewer 2 found it well-written, which is a plus. The rebuttal addressed some points, but the core issues of novelty and scalability remain.\n\nTop-tier conferences require significant contributions and thorough evaluation. The paper's main idea is interesting but may not be sufficiently novel. The technical claims about complexity need more convincing evidence, especially for larger graphs. The experiments, while showing promise, are limited in dataset size and types. The authors did address some points in the rebuttal, but the reviewers' concerns about overfitting in ChevNet and the non-local attention's prior existence (like in [1]) haven't been fully resolved. \n\nConsidering all this, the paper has merit but falls short in novelty and scalability validation. The reviewers' consensus seems to lean towards rejection, especially since Reviewer 1 and 4, who are more critical technically, are not convinced. The positive aspects (clarity, empirical results on some datasets) aren't enough to overcome the novelty and scalability issues at a top conference level.\n</think>\n\nThe paper proposes a novel approach to address the limitations of graph neural networks (GNNs) in handling disassortative graphs by introducing a global self-attention mechanism based on adaptive spectral filters. While the idea of leveraging spectral methods and attention for non-local interactions is promising, several critical issues undermine its acceptance at a top-tier conference:\n\n### Key Strengths:\n1. **Motivation and Problem Relevance**: The focus on improving GNN performance on disassortative graphs is timely and addresses a recognized limitation of existing methods.\n2. **Empirical Results**: The model shows strong performance on node classification tasks, particularly on disassortative datasets, and the experiments are thorough in comparing with baselines.\n3. **Clarity and Presentation**: The paper is well-written and accessible, as noted by Reviewer 2.\n\n### Major Concerns:\n1. **Novelty and Contribution**:\n   - **Spectral Filters vs. Existing Work**: Reviewers 1 and 4 argue that the core idea of non-local attention and spectral filters (e.g., ChevNet, [1]) is not sufficiently novel. The authors counter that their learnable spectral filters and multi-head attention provide adaptivity lacking in prior work. However, the reviewers remain unconvinced, noting that ChevNet’s polynomial filters can theoretically capture distant nodes and that non-local attention has been explored in [1]. The distinction between \"learnable\" vs. \"fixed\" filters is nuanced but insufficient to justify a top-tier contribution.\n   - **Lack of Theoretical Insights**: The absence of theoretical results (e.g., generalization bounds, expressiveness analysis) weakens the paper’s depth, as highlighted by Reviewer 2.\n\n2. **Technical Soundness and Scalability**:\n   - **Complexity Claims**: The authors claim linear complexity via Chebyshev approximations, but Reviewers 1 and 4 question this, noting that eigen decomposition (even via Chebyshev) requires O(N^2) operations for dense graphs. The rebuttal clarifies the use of sparse matrix operations, but the provided runtime results (e.g., on Chameleon) still show slower performance than GAT and comparable results to Geom-GCN. The scalability for very large graphs (e.g., millions of nodes) remains unaddressed.\n   - **Sparsification and Efficiency**: While sparsification techniques improve runtime, the authors admit that computational complexity remains tied to the graph’s edge count. This limits practical applicability to large-scale graphs, a critical drawback for real-world impact.\n\n3. **Empirical Validation**:\n   - **Dataset Limitations**: The experiments rely on small disassortative datasets (e.g., Chameleon has ~2k nodes). Larger datasets like Actor (7k nodes) are mentioned but not fully evaluated. Synthetic graphs with controllable parameters (e.g., β) are planned but not yet included, leaving gaps in validating the model’s robustness.\n   - **Task Scope**: Node classification is the sole evaluation task, despite reviewers urging exploration of other tasks (e.g., link prediction). This limits the paper’s broader impact.\n\n### Conclusion:\nWhile the paper addresses an important problem and shows promising results on specific datasets, its contributions are incremental relative to existing work, and scalability concerns and lack of theoretical grounding prevent it from meeting the high bar of a top-tier conference. The reviewers’ critiques on novelty and technical rigor are compelling, and the rebuttal does not fully resolve these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject the paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" for a top-tier conference like ICLR. Let me go through all the provided information step by step.\n\nFirst, the paper's main contributions are an automated platform for multi-objective optimization using Bayesian methods, with a novel strategy called Believer-Penalizer (BP) for asynchronous batch experiments, and a user-friendly GUI. The authors emphasize usability for non-experts and modularity for researchers.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 found the tool useful but questioned the performance and novelty, giving a 5 (on a scale where 5 might be borderline). Reviewer 2 and 3 were more critical, pointing out that the methodology isn't novel enough, the platform is just an assembly of existing techniques, and the BP strategy lacks theoretical backing. Reviewer 4 noted that while the system is well-engineered, the empirical results are overstated and the technical novelty is low. Reviewer 5 was more positive, highlighting the problem's importance and the platform's potential but also mentioned minor issues.\n\nThe author responses addressed some points, like expanding support to more objectives and clarifying the BP strategy's empirical robustness. They also emphasized the engineering contributions and the GUI's importance for non-coders.\n\nNow, considering the standards of a top-tier conference, which usually require significant technical contributions and strong empirical results. The reviewers' consensus seems to be that the technical novelty is marginal. The BP method is a straightforward combination of existing strategies (KB and LP) without strong theoretical justification. The platform's main selling points are its usability and modularity, which are important but might not meet the high bar for a top conference's methodological contributions.\n\nThe empirical results are mixed. Figures 5 and 4 show that BP isn't always the best and AutoOED underperforms in some benchmarks. The authors argue for robustness, but top conferences often require clear superiority or significant improvements over baselines. The comparison with other platforms also isn't overwhelmingly in AutoOED's favor.\n\nAdditionally, there were issues with the paper's claims, like initially limiting to 2-3 objectives and not citing prior work. The authors addressed some of these in their response, but the core critique about the lack of novel methodology remains.\n\nWhile the platform's engineering effort is commendable and the GUI is a practical contribution, top-tier conferences typically prioritize groundbreaking research over well-engineered tools unless the tool introduces fundamentally new methods. Since the reviewers pointed out that the technical contributions are minor and the experiments don't strongly support the claims, this might not meet the bar.\n\nHowever, the problem domain (multi-objective experimental design) is important, and the platform could have real-world impact. But ICLR's standards might require more in terms of methodological innovation. The authors' responses didn't add enough new technical contributions beyond clarifications and minor improvements.\n\nWeighing all this, the lack of strong technical novelty and underwhelming empirical results, despite the practical utility, likely tip the decision towards rejection. The reviewers' scores (mostly 2-3 on technical novelty) and the consensus that it's more engineering than scientific method contribute to this.\n</think>\n\nThe paper presents AutoOED, a platform for multi-objective optimization with a novel asynchronous strategy (Believer-Penalizer, BP) and a user-friendly GUI. While the platform addresses important usability gaps and offers modular design for researchers, its technical contributions are limited. The BP strategy combines existing methods (KB and LP) without theoretical justification, and reviewers noted its empirical performance is mixed (not always superior, sometimes underperforming). The platform’s engineering effort is notable, but top-tier conferences prioritize significant methodological innovations. The authors’ responses clarified some points but did not resolve core critiques about novelty or empirical strength. Competing tools already support similar or broader functionalities, and AutoOED’s performance claims are overstated. While the problem domain is valuable, the paper’s contributions fall short of the high bar for technical novelty and empirical impact expected at ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about addressing the \"greedy learning\" issue in multi-modal DNNs where models rely too much on one modality. The authors propose a method called balanced multi-modal learning to fix this. Now, I need to evaluate if it's good enough for a top-tier conference.\n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 1 and 2 are critical, pointing out that the experiments aren't strong enough. Reviewer 2 mentions inconsistencies in Table 1 between versions, which is a red flag. They also say the claims aren't well-supported. Reviewer 4 is even harsher, saying the main claims aren't validated and the experiments are weak, leading them to recommend rejection. \n\nOn the other hand, Reviewer 5 and 7 are more positive. Reviewer 5 thinks the empirical results are strong but wants clearer explanations on some methods. Reviewer 7 suggests a weak accept, noting the problem is interesting but the experiments lack comparisons with SOTA. \n\nThe author responses addressed some points, like adding RUBi results and hyperparameter studies. But Reviewer 2 still questions the validity of changes in Table 1 without explanation. Also, several reviewers point out the lack of theoretical analysis and insufficient comparison with related work. The datasets used are all visual, which might not cover natural multimodal scenarios like audio-visual, as noted by Reviewers 3, 6, and 7. \n\nThe key issues here are the weak experimental validation, especially on more natural datasets, and the lack of solid theoretical backing. The authors added some comparisons but the main claims about the greedy nature affecting generalization aren't fully proven. The method's effectiveness isn't clearly shown across diverse datasets. Also, the hyperparameters and some method details were unclear, which the authors tried to fix but might not be enough. \n\nTop-tier conferences require strong validation and novel contributions. While the problem is interesting, the current evidence isn't convincing enough. The experiments are limited, and the method doesn't outperform others significantly. The theoretical gaps and reviewer concerns about the core claims being unsupported are critical. \n\nSo, despite some positive aspects, the paper's shortcomings in validation and theory, along with the mixed reviews leaning towards rejection from key reviewers, make me think it's not ready for a top conference yet.\n</think>\n\nThe paper addresses an important problem in multi-modal learning—greedy reliance on a single modality—and proposes a method to mitigate this issue. However, several critical weaknesses undermine its suitability for a top-tier conference:\n\n1. **Weak Experimental Validation**: \n   - Reviewers 2, 4, and 6 highlight that key claims (e.g., the link between greedy learning and generalization performance) are not strongly supported. Table 1’s inconsistent results across versions and the lack of statistical significance in ModelNet40 and NVGesture datasets weaken credibility.\n   - The Colored MNIST experiments, while showing large gains, are criticized as artificially biased and not representative of standard multi-modal settings. The absence of results on natural multimodal datasets (e.g., audio-visual) further limits generalizability.\n\n2. **Theoretical Gaps**:\n   - Reviewers 2, 4, and 6 note a lack of theoretical justification for the greedy learning hypothesis or the proposed method. The authors acknowledge this is an empirical study but fail to provide rigorous analysis or connections between conditional learning speed and utilization rates.\n\n3. **Methodological Clarity and Baselines**:\n   - The rebalancing step and conditional learning speed definitions are poorly explained (Reviewers 3, 5, 6). While the authors revised the appendix, the main text remains unclear.\n   - Comparisons with prior work (e.g., RUBi, adaptive learning rate methods) are insufficient. The added RUBi results show comparable performance to the proposed method, undermining its novelty and superiority.\n\n4. **Hyperparameter and Dataset Limitations**:\n   - The choice of hyperparameters (e.g., α, Q) lacks systematic analysis, and the authors only partially address this in revisions (Reviewer 6, 7).\n   - All datasets use visual modalities, limiting the evaluation of cross-modal scenarios (e.g., audio-visual) that are critical for validating the method’s broader applicability.\n\n5. **Reviewer Consensus**:\n   - While some reviewers (5, 7) find the problem interesting and the empirical results promising, the majority (Reviewers 2, 4, 6) argue the claims are not sufficiently validated. Reviewer 4 explicitly recommends rejection due to weak evidence and lack of theoretical grounding.\n\n**Conclusion**: The paper identifies a relevant problem but falls short in rigorously validating its claims, providing theoretical justification, and demonstrating clear superiority over existing methods. The experimental setup’s limitations and unresolved methodological ambiguities prevent it from meeting the high standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about applying boosting techniques to reinforcement learning (RL) in Markov decision processes (MDPs), aiming for algorithms whose complexity doesn't depend on the number of states. The key contributions seem to be using a non-convex Frank-Wolfe method and gradient boosting to handle the non-convexity of the value function, leading to sample complexity and runtime bounds independent of the state space. \n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 and 3 are more positive, with 1 wanting a rewrite for clarity and 3 giving a weak accept. Reviewers 2,4,5,6 point out issues like lack of novelty compared to Hazan & Singh 2021, insufficient experiments, notation overload, and clarity problems. Reviewer 4 and 5 especially mention the paper is hard to read with undefined terms and notation issues. \n\nThe authors responded by acknowledging the need for better notation and explanations, but they didn't address all points. For example, they mention adding a notation list and more intuition, which could help. However, the core concerns about incremental novelty and lack of experiments remain. Reviewer 2 says the technical novelty is limited and similar to prior work, which the authors counter by pointing out their non-convex approach vs. convex in Hazan & Singh. That's a valid point, but does it make the contribution significant enough for a top-tier conference?\n\nThe paper's theoretical results are solid, but the lack of experiments is a hit. Reviewer 2 and 6 suggest experiments would strengthen it, and the authors say they're beyond the scope. For a top conference, even theoretical papers often have some empirical validation. Also, the clarity issues are significant. Multiple reviewers mention the dense notation and lack of intuition, which could hinder broader impact. \n\nThe authors' response addresses some points but not all. They didn't provide examples of weak learners, which Reviewer 6 asked for. The technical novelty is there but perhaps not groundbreaking. The problem of state-independent complexity is important, but the paper's approach might be too similar to existing methods. \n\nConsidering all this, the paper has merit but needs major revisions. However, top-tier conferences usually require both strong novelty and clarity. Since the reviewers' consensus leans towards the paper needing significant improvements in clarity and having incremental contributions, it might not meet the high bar. The authors' responses don't fully resolve the key issues raised, especially regarding novelty and experiments. \n\nHmm, but Reviewer 3 gave a weak accept. Maybe the core idea is good but needs better presentation. However, the majority of reviewers (especially 2,4,5,6) have serious concerns. The final decision hinges on whether the contributions are significant enough despite the flaws. Since the technical novelty is somewhat there but incremental, and clarity is a major barrier, I think it's a borderline case. But given that top conferences are highly competitive, and the paper's current state has too many issues, I'd lean towards reject unless the authors can make substantial revisions. Since the decision is based on the current submission, the answer should be reject.\n</think>\n\nThe paper proposes a boosting approach for reinforcement learning in MDPs, aiming to achieve sample complexity and runtime bounds independent of the number of states. The core technical contribution involves adapting non-convex optimization techniques (e.g., Frank-Wolfe) to handle the non-convexity of the value function, which is a notable advancement over prior work that often assumes convexity. However, the paper faces significant challenges in terms of novelty, clarity, and practical validation:\n\n### Strengths:\n1. **Technical Contribution**: The use of non-convex Frank-Wolfe and gradient boosting to address the non-convexity of the value function is a meaningful step forward, particularly compared to Hazan & Singh 2021, which relies on convexity. This distinction is highlighted in the author response and is a valid point of novelty.\n2. **Theoretical Guarantees**: The paper provides rigorous sample complexity and runtime bounds that avoid explicit dependence on the state space, a critical property for large-scale RL problems.\n3. **Relevance**: The problem of scalable RL without structural assumptions is important, and the boosting framework aligns with trends in leveraging weak learners for complex tasks.\n\n### Weaknesses:\n1. **Novelty Concerns**: Reviewers 2 and 3 note that the approach is incremental compared to existing boosting techniques (e.g., Hazan & Singh 2021). While the non-convex adaptation is a step forward, the overall framework is seen as an extension rather than a paradigm shift.\n2. **Clarity and Presentation**: Multiple reviewers (4, 5, 6) criticize the paper’s dense notation, undefined terms, and lack of intuition. The algorithm and theoretical results are obscured by poor exposition, making it difficult to grasp the core ideas or assess practical feasibility.\n3. **Lack of Empirical Validation**: Despite theoretical claims, the paper provides no experiments. Reviewers 2 and 6 emphasize that even basic empirical validation would strengthen the work, especially for a top-tier conference where empirical grounding is often expected, even in theoretical papers.\n4. **Weak Learner Assumptions**: The weak learner assumption (optimizing linear functions over policies) is restrictive and not well-motivated for complex policy classes like neural networks. The authors acknowledge this but do not provide examples or practical justification, leaving gaps in applicability.\n5. **Unresolved Technical Questions**: Reviewers raised specific concerns about undefined notation (e.g., \\(\\mathbb{\\Lambda}\\), \\(d_0\\)), algorithmic steps (e.g., \\(\\eta_2\\)), and assumptions (e.g., distribution mismatch \\(C_\\infty\\)), which the authors only partially addressed in their response.\n\n### Author Responses:\nThe authors acknowledge the need for better notation lists, intuition, and clarity but do not fully address critical issues like the lack of experiments or the incremental nature of their contributions. Their defense of the weak learner assumption and non-convexity distinction is valid but insufficient to overcome broader concerns about novelty and presentation.\n\n### Final Assessment:\nWhile the paper introduces a technically sound approach to boosting in RL and addresses non-convexity, its incremental nature, lack of empirical validation, and significant clarity issues hinder its impact. Top-tier conferences demand both strong novelty and polished presentation. The current submission falls short in these areas, particularly given the consensus among multiple reviewers (2, 4, 5, 6) that the paper is not yet ready for publication. The authors’ responses do not sufficiently resolve these concerns, leaving the work in a borderline but ultimately rejectable state.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about understanding whether neural networks generalize because of their architecture or the optimization process (gradient descent). The authors argue both play a role. They developed new tools to analyze the NNGP posterior's average test error and found that gradient descent can improve generalization by selecting functions with larger margins.\n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewer 1 thinks the theoretical results on NNGP are interesting but the experiments are weak and section 5 isn't a contribution. They want it removed and suggest a weak reject. Reviewer 2 had initial issues but softened their stance, though still sees the paper as borderline. They mention presentation issues and lack of comparisons with other architectures. Reviewer 3 says it's premature, not significant enough, and doesn't apply to CNNs. Reviewer 4 criticizes the empirical evaluation and margin claims, saying they're not rigorous. Reviewer 5 points out that the margin claims in section 5 aren't supported by the loss functions used (they used squared loss instead of exponential) and the implicit bias argument is shaky. Reviewer 6 thinks the paper overpromises, especially on the GD analysis, which is underdeveloped. Reviewer 7 is concerned about the use of the Nero optimizer instead of standard GD/SGD and the limited experiments. They want more ablations and comparisons. Reviewer 8 says the PAC-Bayes bound is incremental and the GD analysis isn't novel. Reviewer 9 likes the ideas but wants better experiments on more architectures and validation beyond simple datasets.\n\nThe authors responded by clarifying that their margin claims are based on the loss functions used and that their work isn't claiming GD maximizes margin universally but rather shows control over it via specific losses. They also mention that their bounds are non-vacuous, which is a plus, but reviewers point out that the experiments are limited, especially with Nero and not standard optimizers. The key issues seem to be the margin analysis being under-supported, the experimental setup using non-standard methods (Nero), and the lack of broader validation across architectures and datasets.\n\nThe paper's strengths include novel theoretical contributions on NNGP bounds and insights into the interplay between architecture and optimization. However, the empirical validation is seen as insufficient, and the margin claims have technical issues. The use of Nero complicates the interpretation of GD's role. Since top-tier conferences require strong empirical validation and clear novel contributions, the current state might not meet those standards. The reviewers' consensus leans towards the paper being borderline but needing more work. Since the authors didn't address all concerns (like Nero's impact, CNN comparisons), it's safer to recommend rejection for now.\n</think>\n\nThe paper presents an analysis of the roles of neural network architecture and gradient descent (GD) in generalization, introducing new tools to bound and estimate the average test error of NNGP posteriors. While the theoretical contributions on NNGP bounds are acknowledged as novel and non-vacuous, several critical issues undermine the paper's overall impact and rigor:\n\n1. **Margin Claims and Loss Function Mismatch**: The margin analysis in Section 5 relies on claims that GD maximizes margin, but reviewers note this is unsupported for the squared loss used (as opposed to exponential loss in prior work). The authors clarify that margin control is explicit via α-scaling, but this does not address the implicit bias argument, which remains underdeveloped. Reviewers highlight a lack of formal connections to GD's trajectory and confusion in terminology.\n\n2. **Empirical Weaknesses**: \n   - **Nero Optimizer**: The use of Nero (a non-standard optimizer with norm constraints) complicates attributing results to GD itself. Reviewers argue this choice limits generalizability and conflates optimization effects with architecture bias.\n   - **Limited Experiments**: The experiments focus on MLPs and simple datasets, with no comparisons to CNNs or broader architectures. The gap between NNGP bounds and GD performance is not rigorously explained, and the bounds themselves are acknowledged as loose and impractical.\n\n3. **Presentation and Clarity**: \n   - The paper overstates GD's role in margin maximization, conflating it with explicit regularization via loss design. Reviewers note the terminology around \"implicit bias\" is misleading, as the analysis often reduces to explicit margin control.\n   - Theoretical contributions are incremental (e.g., building on Valle-Pérez et al. 2019) and lack comparisons to prior work, weakening their significance.\n\n4. **Technical Gaps**: \n   - The margin analysis for GD is not formally grounded in the NTK or trajectory-based frameworks, and the analogy to GP concentration on the mean is problematic in cases like CNNs where NTK means underperform.\n   - The PAC-Bayes bounds, while non-vacuous, are not compared to existing methods, and their practical utility is questioned.\n\n**Conclusion**: While the paper offers interesting theoretical insights into NNGP generalization and margin effects, its empirical validation is insufficient, its margin claims are technically flawed, and its reliance on non-standard methods (e.g., Nero) undermines broader conclusions. The reviewers' consensus—particularly on the paper’s premature state, lack of significance beyond MLPs, and unresolved ambiguities—suggests it falls short of the high bar for top-tier conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about analyzing Feedback Alignment (FA) in deep linear networks. The main contributions are convergence guarantees for both continuous and discrete dynamics, and the discovery of implicit anti-regularization under certain initializations. The reviewers have mixed opinions. \n\nStarting with the strengths: Reviewer 3 is very positive, highlighting the clear writing, rigorous proofs, and the interesting implicit anti-regularization phenomenon. They mention that the analysis extends to deep networks and that experiments align well with theory. The authors also addressed some concerns in their response, like adding figures and clarifying spectral initialization.\n\nBut the other reviewers have significant reservations. Reviewer 1 and 2 point out that the analysis relies heavily on diagonalizable matrices and spectral initialization, which might not reflect real-world scenarios. Reviewer 4 is concerned that spectral initialization isn't commonly used and that the results might not apply to non-spectral cases. They also mention the lack of experiments beyond the restrictive assumptions. \n\nThe authors responded by saying they have experiments without spectral initialization (Fig 2) and that convergence holds for any initialization in the appendix. They also added figures to help visualize the dynamics. However, the key issue is whether the theoretical results are meaningful beyond their restrictive assumptions. \n\nThe paper's novelty is debated. Reviewer 3 thinks it's significant, but others say it's marginal. The technical contributions are correct but might not be groundbreaking. The empirical work is limited, as most experiments stick to the diagonalizable setup. \n\nThe main question is whether the theoretical insights, despite their assumptions, contribute meaningfully to understanding FA. The implicit anti-regularization is a novel finding, even if under specific conditions. The analysis of FA's dynamics in linear networks, while not directly applicable to nonlinear cases, provides foundational understanding. Since top-tier conferences value theoretical contributions even if applied to simplified models (like linear networks), this might be acceptable. \n\nHowever, the reliance on spectral initialization and diagonalizable matrices is a big limitation. The authors argue that similar dynamics occur with small random initializations, citing prior work. Their response addresses some points but doesn't fully bridge the gap to practical scenarios. Still, the paper's theoretical rigor and novel insights into FA's behavior, especially the anti-regularization, seem valuable. \n\nConsidering all this, the paper's theoretical analysis, despite its assumptions, offers new understanding of FA. The clear writing and correct proofs support acceptance. The limitations are noted but don't negate the contribution's significance in the context of existing literature on linear networks. So, I think it should be accepted.\n</think>\n\nThe paper presents a theoretical analysis of the Feedback Alignment (FA) algorithm for deep linear networks, focusing on convergence guarantees and implicit regularization phenomena. While the reviewers highlight both strengths and weaknesses, the key considerations for acceptance at a top-tier conference are as follows:\n\n### Strengths:\n1. **Novel Contributions**: \n   - The discovery of **implicit anti-regularization** under specific initializations is a novel and theoretically interesting phenomenon, particularly as it contrasts with the implicit regularization observed in gradient descent (GD). This insight contributes to understanding FA's behavior and its differences from backpropagation.\n   - The paper provides rigorous convergence proofs for FA in both continuous and discrete dynamics, extending prior work on linear networks. The analysis for deep networks, while restricted to diagonalizable matrices, is technically sound and fills a gap in FA theory.\n\n2. **Clarity and Rigor**: \n   - The paper is well-written, with clear proofs and a structured presentation. Reviewer 3 explicitly praises its clarity and the thoroughness of its analysis.\n   - The authors addressed reviewer concerns by adding visualizations (e.g., phase-space plots) and clarifying assumptions (e.g., spectral initialization limitations).\n\n3. **Empirical Validation**: \n   - Experiments align with theoretical results, even when relaxing assumptions (e.g., non-spectral initializations in Figure 2). This demonstrates practical relevance beyond purely theoretical constructs.\n\n### Weaknesses:\n1. **Limitations of Assumptions**:\n   - The analysis heavily relies on **spectral initialization** and diagonalizable matrices, which are restrictive and not representative of standard training practices (e.g., random Gaussian initialization). Reviewers 1, 2, and 4 question the practical significance of results under these assumptions.\n   - The authors acknowledge these limitations but argue that their framework provides foundational insights and that similar dynamics may emerge in non-spectral cases (citing prior work on linear networks). However, the gap between theory and practice remains a concern.\n\n2. **Scope and Generalizability**:\n   - The focus on **linear networks** limits direct applicability to nonlinear architectures, where FA is often studied empirically. While linear networks are a standard starting point for theoretical analysis, the paper does not address how its findings might extend to nonlinear settings (as noted by Reviewer 2).\n   - The implicit anti-regularization phenomenon is observed only under specific initializations, raising questions about its relevance to common use cases. The authors argue that such phenomena are still theoretically valuable, even if not universally observed.\n\n3. **Comparisons with GD**:\n   - While the paper highlights FA's advantages over GD (e.g., faster convergence in experiments), the theoretical analysis does not directly compare FA and GD under equivalent conditions. Reviewer 1 notes that FA's dynamics are simpler than GD's, making such comparisons non-trivial.\n\n### Balancing the Evaluation:\n- **Theoretical Value**: The paper advances the theoretical understanding of FA, particularly in linear networks. The implicit anti-regularization is a novel contribution that could inspire further work on FA's dynamics and initialization strategies.\n- **Practical Relevance**: While the assumptions are restrictive, the experiments and theoretical framework provide a stepping stone for future studies. The authors' response clarifies that their analysis is foundational and not intended to fully bridge the gap to nonlinear networks or standard initialization practices.\n- **Conference Standards**: Top-tier conferences often accept papers with strong theoretical contributions even if applied to simplified models (e.g., linear networks). The paper’s rigorous analysis and novel insights meet this standard, despite its limitations.\n\n### Final Decision:\nThe paper’s theoretical contributions, particularly the discovery of implicit anti-regularization and convergence guarantees for FA in linear networks, are significant enough to warrant acceptance. While the assumptions are restrictive, the work provides a critical foundation for understanding FA’s behavior and opens avenues for future research. The clear writing, correct proofs, and alignment with experiments further strengthen its case.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about Target Propagation (TP) via Regularized Inversion. The authors present a simplified version of TP using regularized inversion of layers, aiming to make it implementable in differentiable programming frameworks. They compare it to Backpropagation (BP) in terms of computational complexity and apply it to RNNs for sequence modeling. The abstract mentions that experiments show regularization's importance in TP.\n\nNow, looking at the reviewers' comments. There are four reviewers, each with their own points. Let me break them down one by one.\n\nReviewer 1 is not very enthusiastic. They point out that the proposed TP method requires weight transport, which limits its appeal compared to other TP variants that don't. This is a big deal because weight transport is a known issue in TP, making it less efficient for hardware or distributed systems. Also, the computational cost is higher than BP, even if it's better for long sequences. The reviewer is concerned about the lack of theoretical guarantees that TP's descent directions are better than BP's gradients. The experiments aren't convincing enough, especially since the author isn't an expert in sequence learning. They also have several technical questions about the method's mathematical foundations, like the 0th vs 1st order approximation, handling multiple minimizers with ReLU, and vanishing/exploding gradients. The reviewer thinks the contributions are only marginally novel and the correctness is a 3 out of 5.\n\nReviewer 2 is more positive. They acknowledge the algorithm's implementability via autograd and the computational cost analysis. They mention a theoretical guarantee (Lemma 3.1) that bounds the difference between TP and BP. However, they note that the experiments have high variance in loss and accuracy compared to BP, which might mean it's harder to analyze theoretically. They point out a typo but overall think the paper is clear. The technical novelty is still only marginally significant, but empirical contributions are somewhat new. Correctness is higher here (4/5), but the significance scores are still low.\n\nReviewer 3 is on the fence. They like the clear presentation and the idea of exploring TP, but the experiments are a problem. The accuracy on CIFAR-10 is really low (under 20%), which is concerning. They suggest that maybe the baselines or the model setup isn't strong enough. Also, the paper claims the method works for general neural networks, but experiments are only on RNNs. The reviewer is worried about generalization and scalability. They lean towards rejection but are open to author feedback. Their scores are similar to others: correctness 3, novelty 2-3.\n\nReviewer 4 thinks the paper is well-written and the algorithm is straightforward. They like the theory part but feel it's not fully explored. The lemma comparing TP and BP directions is too loose, and the connection to Gauss-Newton isn't fully explained. Empirically, the experiments are too limited. They want more comparisons with existing methods like Hessian-Free optimization and better analysis of why TP works better in some cases. The reviewer thinks the paper needs more theoretical and empirical depth. Their scores are technical novelty 3 and empirical 2, with correctness 3.\n\nThe author didn't respond, so we can't consider any rebuttals.\n\nNow, considering all this. The main issues are:\n\n1. **Weight Transport**: Reviewer 1 points out that requiring weight transport is a major drawback. Since many TP variants aim to avoid this, this method's limitation reduces its appeal as an alternative to BP.\n\n2. **Computational Cost**: Higher than BP, even if it's better for long sequences. For top-tier conferences, efficiency is a big factor, especially if it's not better in all cases.\n\n3. **Theoretical Gaps**: Lack of strong theoretical guarantees. The lemma mentioned is seen as too loose, and the connection to Gauss-Newton isn't fully explored. Without clear theoretical backing, it's hard to justify the method's superiority.\n\n4. **Empirical Weaknesses**: Experiments are limited. The CIFAR-10 results are poor, and there's no comparison with strong baselines. High variance in results makes it seem unreliable. The focus on RNNs without broader testing limits the significance.\n\n5. **Novelty**: Most reviewers agree the contributions are only marginally novel. The method builds on existing TP work but doesn't break new ground enough for a top conference.\n\n6. **Technical Concerns**: Questions about handling ReLU activations, vanishing/exploding gradients, and the order of approximation (0th vs 1st) need addressing. These could undermine the method's validity.\n\nOn the positive side, the method is implementable via autograd, and there's some theoretical groundwork. But the negatives seem to outweigh these. The lack of strong empirical results and theoretical backing, combined with the method's limitations (weight transport, cost), make it less compelling. Top conferences usually require significant contributions and robust validation. Since the reviewers are mostly critical (especially on novelty and empirical strength), and the authors didn't address these points, it's likely the paper doesn't meet the bar.\n\nFinal Decision: Reject\n</think>\n\nThe paper proposes a variant of Target Propagation (TP) using regularized inversion of layer Jacobians, aiming to simplify implementation and apply it to recurrent neural networks (RNNs). While the approach is presented as a computationally tractable alternative to backpropagation (BP), several critical issues undermine its potential for acceptance at a top-tier conference:\n\n### Key Strengths:\n1. **Implementability**: The method is straightforward to implement in differentiable programming frameworks, leveraging autograd systems.\n2. **Theoretical Insights**: A lemma bounds the difference between TP and BP directions, and the connection to Gauss-Newton methods is noted, though underdeveloped.\n3. **Empirical Focus**: The paper explores TP for RNNs with long sequences, a niche area where BP struggles, and highlights the importance of regularization in TP.\n\n### Major Weaknesses:\n1. **Lack of Novelty**: Reviewers consistently note that the contributions are only marginally novel. The method builds on prior TP work (e.g., Lee et al., 2015; Meulemans et al., 2020) without significant conceptual advances.\n2. **Weight Transport Requirement**: Unlike other TP variants, the proposed method requires weight transport, a key limitation that reduces its appeal for neuromorphic hardware or distributed systems. This negates one of TP’s primary motivations as an alternative to BP.\n3. **Computational Overhead**: The algorithm is strictly costlier than BP, even if the gap narrows for long sequences. For top-tier venues, efficiency is critical, and the paper does not convincingly argue for scenarios where this trade-off is justified.\n4. **Empirical Weaknesses**:\n   - **Poor Baselines**: Experiments on CIFAR-10 show accuracies below 20%, suggesting either flawed implementation or inadequate model design. Competing RNN baselines are not rigorously compared.\n   - **High Variance**: Training loss and accuracy exhibit significant variability compared to BP, raising concerns about reliability.\n   - **Limited Scope**: Experiments focus narrowly on RNNs, with no exploration of other architectures (e.g., CNNs), weakening claims of general applicability.\n5. **Theoretical Gaps**:\n   - The lemma comparing TP and BP directions is deemed too loose to justify superiority.\n   - The relationship to Gauss-Newton methods is under-explored, and the necessity of regularization (to avoid divergence) contradicts prior interpretations of TP as a Gauss-Newton approximation.\n   - Open questions remain about handling non-invertible activations (e.g., ReLU) and vanishing/exploding gradients in TP.\n6. **Clarity and Rigor**: Reviewer 1’s technical questions (e.g., 0th vs. 1st order approximations, multiple minimizers) remain unanswered, suggesting potential flaws in the method’s mathematical foundations.\n\n### Conclusion:\nWhile the paper addresses an interesting problem and provides some theoretical groundwork, its limitations in novelty, empirical robustness, computational efficiency, and theoretical rigor outweigh its contributions. The requirement for weight transport, lack of strong empirical validation, and failure to address critical reviewer concerns (e.g., comparison with existing methods like Hessian-Free optimization) further diminish its competitiveness for a top-tier conference. The authors’ inability to respond to these critiques exacerbates the concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving diffusion models' sampling efficiency by optimizing samplers with differentiable objectives. The key contributions are DDSS and GGDM. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 was initially concerned about the method's distinction from adversarial training but was satisfied after the rebuttal, raising their score. They still had questions about the significance but acknowledged improvements. \n\nReviewer 2 had major concerns about theoretical grounding and the choice of optimizing perceptual loss over ELBO. They worried the method might not be optimal and the model's expressiveness. However, after the rebuttal, they raised their score to 6, though they still rated technical significance as 3. \n\nReviewer 3 found the approach interesting with good empirical results but noted clarity issues and the need for more datasets. They were convinced by the rebuttal, especially the KID loss connection. \n\nReviewer 4 was critical about the lack of theoretical guarantees, the dependency on datasets, and the need for more high-res experiments. They downgraded technical novelty but the rebuttal addressed some points, leading to a score increase. \n\nReviewer 5 pointed out clarity issues and the need for better explanations but acknowledged the method's novelty and empirical success. They wanted more on larger datasets but were satisfied post-rebuttal. \n\nThe authors addressed several points: clarified the loss as KID, provided proofs, and added experiments. The main strengths are the empirical results showing better FID scores with fewer steps, which is a significant practical improvement. The method is compatible with existing models without retraining, which is a plus. \n\nWeaknesses include the lack of theoretical guarantees beyond the proof sketch, dependency on perceptual metrics, and some dataset limitations. However, the rebuttal strengthened the theoretical basis by linking to KID and MMD, which helps. \n\nTop-tier conferences require strong technical contributions and clear significance. While some reviewers felt the method builds on prior work (like DDIM), the empirical gains and the novel approach of optimizing samplers via differentiable objectives seem impactful. The clarity improvements and addressing theoretical concerns in the rebuttal should mitigate some criticisms. \n\nConsidering all this, the paper's empirical contributions and the addressed concerns tip the balance towards acceptance, especially since the conference values practical improvements in efficiency for diffusion models.\n</think>\n\nThe paper introduces DDSS and GGDM to accelerate diffusion models' sampling while maintaining high quality. Key strengths include strong empirical results (e.g., FID scores on LSUN with 10 steps) and compatibility with pre-trained models without retraining. The authors addressed reviewer concerns by clarifying the loss function (linking it to KID/MMD), providing formal proofs, and improving theoretical grounding. While weaknesses remain—such as dependency on perceptual metrics and limited high-resolution datasets—the rebuttal strengthened the method's justification and empirical validation. The problem is important, and the contributions advance the field by enabling faster inference without sacrificing quality. Though some reviewers noted incremental aspects, the practical impact and novel optimization approach justify acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract suggest it's about understanding adversarial examples from a frequency perspective. The authors argue that adversarial examples aren't strictly high or low frequency but depend on the dataset. They also propose some frequency-based adversarial training methods and explain the accuracy-robustness trade-off.\n\nLooking at the reviewer comments, there's a mix of positive and negative feedback. Let's break down each reviewer's points:\n\nReviewer 1 initially had concerns but then lowered their score because the authors didn't address other reviewers' points well. They mention that the paper should consider universal adversarial perturbations (UAPs) to make it more unique. The authors responded by saying UAPs are beyond their current scope but agreed to add a discussion in related work. They also mentioned conflicting results with another paper, suggesting it's a nuanced area needing more research. That seems okay, but maybe the lack of UAP analysis is a gap.\n\nReviewer 2 had several issues. They questioned the math differences between DCT and Fourier transforms, which the authors clarified. The main problem was the lack of clear contributions and the methods not showing better performance. The authors tried to clarify their contributions in the rebuttal, but the reviewer still wasn't convinced. The methods don't improve benchmarks, which might be a problem for a top conference. Also, the reviewer pointed out that the lambda parameter in TRADES is similar to their approach, which the authors addressed by saying their work is more about analysis than improving benchmarks. But if the methods don't offer better performance, the contribution might be too marginal.\n\nReviewer 3 was critical. They pointed out that the main conclusion (dataset dependence) isn't new and has been mentioned in prior work. The experiments are limited to CIFAR and ImageNet, not simpler datasets like MNIST. They also noted that only PGD attacks were used, and the paper overlaps with concurrent work. The authors added more datasets and experiments in the rebuttal, but the reviewer still thinks the contribution is incremental. The technical novelty is low, so this is a big issue for a top-tier conference.\n\nReviewer 4 was positive, appreciating the experiments and practical insights. They only mentioned minor issues like figure placement and grammar. Their support is important, but the other reviewers' criticisms might outweigh this.\n\nReviewer 5 was confused by the contributions and thought the paper didn't clearly state its novel aspects. The authors tried to clarify in the rebuttal, but the reviewer still wanted more. If the contributions aren't clear, that's a red flag. The reviewer even considered rejecting it unless clarified, which the authors did, but maybe not enough.\n\nReviewer 6 was positive, noting the insights and experiments. They suggested more experiments on different datasets and architectures, which the authors addressed by adding supplementary material. The reviewer recommended acceptance, which is a plus.\n\nThe authors' responses addressed some points but not all. They clarified contributions, added experiments, and discussed related work. However, some key issues remain: overlapping with prior work, limited datasets, and methods not improving benchmarks. The technical novelty is considered marginal by some reviewers.\n\nTop-tier conferences usually require significant contributions and clear novelty. The paper's main idea isn't entirely new, as several reviewers pointed out. The experiments are thorough but maybe not enough to justify acceptance if the core idea isn't novel. The authors' rebuttal tried to highlight their unique aspects, like focusing on adversarial noise frequencies vs. image frequencies, but it's unclear if that's sufficient.\n\nThe fact that some reviewers (like R3 and R5) found the contributions too incremental and not clearly explained is concerning. The paper might need major revisions, but the conference decision is now. Since the reviewers are split, but the negative points about novelty and clarity are strong, especially from R3 and R5, I think the paper might not meet the high bar for acceptance. However, R4 and R6's positive feedback and the authors' efforts in rebuttal could tip it. But given the consensus on lack of novelty and clarity, I'm leaning towards reject.\n</think>\n\nThe paper presents an analysis of adversarial examples through a frequency perspective, arguing that their properties are dataset-dependent rather than strictly high- or low-frequency. While the work offers some novel insights and thorough experiments, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Lack of Novelty**: Multiple reviewers highlighted that the core conclusion (dataset dependence of adversarial example frequencies) is not novel, as prior works (e.g., Tsuzuku et al., 2019; Bernhard et al., 2021) have already questioned the high-frequency misconception and observed similar trends. The paper adds incremental evidence but does not fundamentally shift the field’s understanding.\n\n2. **Weak Contributions**: The proposed methods (frequency-constrained adversarial training) do not improve benchmark performance (e.g., clean/robust accuracy) compared to existing approaches like TRADES. The authors frame this as intentional, focusing on analysis over performance, but top-tier venues typically require stronger technical or empirical contributions.\n\n3. **Insufficient Empirical Validation**: While the authors added experiments on additional datasets (e.g., MNIST) and architectures (e.g., ViT) in the rebuttal, the original submission lacked diversity in datasets (e.g., no MNIST/SVHN) and attack types (e.g., no CW/AutoAttack). This limits the generality of their claims.\n\n4. **Clarity and Presentation**: Reviewers consistently noted unclear contributions and poor structure. The introduction buried novel ideas under excessive related work, and key claims (e.g., noise gradient analysis) were not sufficiently justified. The rebuttal improved clarity but did not fully address these concerns.\n\n5. **Overlap with Concurrent Work**: The paper’s conclusions overlap significantly with concurrent work (e.g., Bernhard et al., 2021), and the authors’ distinction (focusing on perturbations vs. images) is subtle and not strongly emphasized.\n\n**Counterpoints**: The work does offer interesting empirical observations and a fresh angle on adversarial robustness, as noted by R4 and R6. The supplementary material adds depth, and the authors addressed some concerns in the rebuttal. However, these positives are outweighed by the lack of novelty, insufficient validation, and unclear contributions.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper in question is about the optimization landscape of regularized two-layer ReLU networks. The title mentions an exact characterization of optimal solutions via convex optimization with cone constraints. The abstract lists several contributions, including showing that all global optima can be found via a convex program, characterizing the optimal solutions, and providing algorithms for checking optimality and constructing paths to minima. \n\nLooking at the reviewer comments, there's a mix of positive and some critical points. Reviewer 1 points out some technical issues like typos and inconsistencies in equations, but still rates the technical novelty as 3 (somewhat new) and correctness as 3 (minor issues). Reviewer 2 is very positive, calling it the best among submissions, and gives correctness a 4. However, they mention missing references and the need for extensions to other activations and deeper networks. Reviewer 3 also has some concerns about novelty compared to prior work by Pilanci and others, and suggests experiments would strengthen it. They note some typos and unclear parts. Reviewer 4 is strongly in favor, giving a clear accept, emphasizing the significance even if it's limited to two layers.\n\nThe author responses address the typos and clarify some technical points, like the cone definitions and the inclusion of bias terms. They also mention plans to include more references and discuss extensions in future work.\n\nNow, considering the standards of a top-tier conference, the key factors are novelty, significance, correctness, and clarity. The paper's main contribution is extending Pilanci and Ergen's work by fully characterizing all global optima via convex programs, which is a solid theoretical advance. The fact that Reviewer 4, who seems to have deep expertise, strongly endorses it as a \"clear accept\" is a big plus. The other reviewers, while noting limitations (like single-layer focus and some technical issues), don't find fatal flaws. The typos and minor inconsistencies can be fixed in the final version as the authors have addressed them.\n\nThe paper's theoretical contributions are significant, especially providing a framework that doesn't rely on duality, which is a novel angle. The polynomial-time algorithm for checking optimality and the path construction are practical contributions. The lack of experiments is a minor point since it's a theoretical paper, but the reviewers acknowledge that. The limitations (e.g., two-layer networks) are noted, but top conferences often accept papers with clear theoretical advances even if they don't cover all cases. The authors have also addressed the concerns about prior work by clarifying their contributions versus Pilanci et al.\n\nOverall, the strengths in theoretical novelty and the positive endorsements from reviewers 2 and 4, along with the authors' thorough responses, tip the balance towards acceptance. The minor issues can be resolved in the camera-ready version.\n</think>\n\nThe paper presents a significant theoretical contribution to understanding the optimization landscape of regularized two-layer ReLU networks. Its core innovation lies in characterizing **all global optima** via a convex program with cone constraints, extending prior work by Pilanci and Ergen (2020) and avoiding duality-based approaches. The contributions—such as the polynomial-time optimality check, explicit construction of paths to global minima, and the absence of spurious valleys under sufficient overparameterization—are novel and impactful for the field. \n\n### Key Strengths:\n1. **Theoretical Depth**: The paper provides a complete characterization of global optima and stationary points, which is a major advance in understanding non-convex neural network optimization. This work goes beyond prior results by explicitly linking the non-convex problem to a convex program and resolving ambiguities in prior formulations (e.g., clarifying cone definitions and minimal networks).\n2. **Novel Framework**: The avoidance of duality-based methods and the focus on direct convex analysis offer a fresh perspective, distinguishing it from earlier work. The framework’s ability to construct *all* optimal networks and analyze their invariances is particularly valuable.\n3. **Algorithmic Contributions**: The polynomial-time algorithm for verifying global optimality and the explicit path construction are concrete tools that could influence future research and practical optimization strategies.\n4. **Reviewer Consensus**: While some reviewers noted limitations (e.g., single-layer focus, missing references), **Reviewer 4’s strong endorsement** (\"clear accept\") and the overall agreement on the paper’s significance are compelling. The technical rigor is affirmed by Reviewer 2’s correctness score of 4 and the authors’ thorough rebuttal addressing concerns.\n\n### Weaknesses and Limitations:\n1. **Scope Limitations**: The analysis is restricted to two-layer ReLU networks, and extensions to deeper networks or other activations are left for future work. However, this is a common limitation in foundational theoretical work, and the authors acknowledge it while providing plausible pathways for future research.\n2. **Technical Gaps**: Minor issues in equations (e.g., typos in summation bounds, cone definitions) were identified but are fixable. The lack of experiments is noted, but this is acceptable for a purely theoretical paper.\n3. **Prior Work Overlap**: Reviewers 1 and 3 highlighted overlaps with Pilanci and Ergen’s prior work. However, the authors clarify their contributions (e.g., full characterization of *all* optima vs. prior work’s focus on a single solution), which justifies the novelty.\n\n### Conclusion:\nThe paper’s theoretical contributions are substantial and align with the high standards of top-tier conferences. While it has limitations (e.g., scope to two-layer networks), these are offset by its depth, clarity, and potential to inspire future work. The authors’ responses adequately address concerns, and the technical flaws are minor. The paper’s significance in advancing the understanding of neural network optimization landscapes justifies its acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality.\" The abstract mentions they introduce DDSS and GGDM, aiming to reduce the number of inference steps needed for diffusion models while maintaining high sample quality. They report significant improvements in FID scores with fewer steps compared to DDPM/DDIM baselines. \n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Let's break it down by each reviewer:\n\nReviewer 1 initially had concerns about the method's novelty and theoretical grounding but was satisfied after the rebuttal, raising their score to 6. They mention that Theorem 1 clarified the relationship with DDPMs, which helped. However, they still had questions about whether the improvements were due to the non-Markovian structure or the KID minimization, but the authors addressed some of these.\n\nReviewer 2 was more critical. They pointed out that the method's motivation wasn't clear, especially regarding why they chose to minimize KID instead of maximizing ELBO. They also questioned the expressiveness of the model and whether the non-Markovian structure was justified. The reviewer initially wanted to reject but raised their score to 6 after the rebuttal, noting the improved clarity and theorem. However, they still had concerns about the method's theoretical justification and the empirical results on larger datasets.\n\nReviewer 3 found the approach interesting and the results compelling, especially the sample quality in fewer steps. They noted the method's simplicity and the impressive results but mentioned the need for more datasets. They were positive overall, giving a score of 4 in correctness and 3 in significance.\n\nReviewer 4 was skeptical about the theoretical grounding, the objective function's hand-crafted nature, and the lack of results on high-res datasets like LSUN. They also questioned the proof's formality. Their scores were lower (3 or 2 in some categories), but after the rebuttal, they kept their score, noting the lack of larger datasets but acknowledging improvements.\n\nReviewer 5 appreciated the idea but wanted better clarity and more experiments on larger datasets. They found the method novel but the paper's clarity lacking. After the rebuttal, they were satisfied with some changes but still wanted more on larger datasets.\n\nThe authors addressed several points in their response. They renamed GGDP to GGDM, clarified the loss function's relation to KID, provided more theoretical grounding with Theorem 1's formal proof, and added experiments showing that optimizing KID with different kernels works well. They also addressed concerns about overfitting and likelihoods, explaining that sample quality isn't always tied to likelihood. They mentioned adding more datasets for the camera-ready version.\n\nNow, considering all this:\n\nStrengths:\n- The method achieves strong empirical results, especially in FID scores with fewer steps, which is a significant practical improvement.\n- The approach is novel, combining non-Markovian samplers with gradient-based optimization of parameters using perceptual losses.\n- The authors addressed many theoretical concerns with Theorem 1 and clarified the loss function's motivation via KID.\n- The technical contributions, like using gradient rematerialization, are solid.\n\nWeaknesses:\n- Some reviewers were concerned about the theoretical justification, especially regarding why the perceptual loss is better than ELBO. The authors did address this by linking to prior work showing FID and likelihood can be decoupled.\n- Limited datasets in the main paper (though authors promised more in camera-ready).\n- The method's computational cost during optimization (gradient rematerialization) might be a practical issue, but the paper's focus is on inference efficiency, so that's a trade-off.\n\nThe rebuttal seems to have addressed many of the initial concerns, especially around theoretical grounding and loss function motivation. The empirical results are strong, and the problem addressed (speeding up diffusion models) is important. The fact that multiple reviewers raised their scores post-rebuttal suggests the authors effectively addressed key issues.\n\nHowever, there's still a concern about the method's applicability to high-resolution datasets. But since the authors committed to including more datasets in the final version, that might be mitigated. The theoretical aspects were clarified, and the method's novelty is there.\n\nConsidering top-tier standards, the paper's contributions are significant enough, especially given the empirical improvements and the novel approach to optimizing samplers. The remaining issues are either addressed or minor compared to the overall contribution.\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is titled \"The weighted mean trick – optimization strategies for robustness\" and claims to show that minimizing a weighted mean can optimize higher-order moments of the loss distribution, leading to better robustness. The reviewers have given mixed feedback, so I need to weigh their points carefully.\n\nFirst, looking at the abstract and the problem statement, the authors are connecting weighted mean minimization to higher moments like variance, skewness, and kurtosis. They mention that this can tighten bounds on the loss mean and improve robustness against outliers. They also note that their approach might lead to non-convex objectives but explore convexity preservation. The experiments compare their method to other robust loss functions, showing similar performance with a stronger theoretical basis.\n\nNow, the reviewer comments. Let's start with Reviewer 1. They mention that the choice of λ_i isn't theoretically justified, so the empirical improvement might just be due to tuning. They suggest focusing on negative weights and separating variance penalization. This is a valid concern because if the improvement is just from hyperparameter tuning, the method's novelty is questionable.\n\nReviewer 2 points out a critical issue with the algorithm. The proposed method uses only part of the gradient when minimizing the weighted loss. The reviewer explains that the gradient should involve two terms, but the algorithm only uses one. This means the algorithm might not actually be minimizing the intended objective, which is a fundamental flaw. The authors responded by saying they intended to modify the empirical distribution, but if the gradient is incorrect, the optimization might not work as claimed. This is a significant problem because it undermines the theoretical basis.\n\nReviewer 3 echoes the same concern as Reviewer 2 about the gradient computation. They also mention that the convexity properties discussed in Section 2 aren't maintained in Section 3, breaking coherence. The authors tried to address this by clarifying notations but didn't resolve the gradient issue. The lack of convergence guarantees for Algorithm 1 is another red flag. Without knowing if the algorithm converges, the practical utility is in doubt.\n\nReviewer 4 is critical of the theoretical contributions, noting that the paper's insights are confused by considering both positive and negative λ, which have opposite effects. They also point out that the weights depend on unknown expectations and that the number of hyperparameters increases with higher moments. The authors didn't fully address the theoretical gaps, and the comparison to existing robust methods like Trimmed Mean or MoM isn't adequately explored. The reviewer's low scores on technical novelty and significance suggest the work isn't groundbreaking.\n\nReviewer 5's main issue is that the theoretical results are too straightforward once dependencies on θ are ignored. They also question the validity of Lemma 2 when weights depend on θ. The authors didn't clarify this, so the theoretical claims might be flawed. The algorithm's disconnect from the objective function is another point, making the method's effectiveness uncertain.\n\nReviewer 6 points out specific issues like unclear notation (e.g., \"convex objective ℓ\"), lack of concentration analysis for weighted samples, and the impact of clipping weights. The authors addressed some notations but didn't fully resolve the theoretical concerns. The paper's poor writing and lack of clarity further hinder its acceptance.\n\nReviewer 7 is more positive, seeing potential in the observation that weighted means relate to higher moments. They acknowledge the work's interest but note the theorems are straightforward and applications are lacking. The authors added experiments and cited more work, but the core issues remain.\n\nThe authors' responses mainly address notational issues and clarify some points but don't fix the gradient problem or provide convergence proofs. They mention future work on robust mean estimation but didn't revise the algorithm's theoretical underpinnings in time.\n\nPutting this together: The core issue is the algorithm's gradient computation. If the method isn't actually minimizing the intended objective, the results are unreliable. Multiple reviewers (2,3,5) highlight this, and the authors didn't adequately address it. The theoretical contributions are seen as incremental or flawed, and the empirical results might be due to hyperparameter tuning rather than the method's inherent merit. The paper's coherence and clarity are also problematic, with unresolved notational and conceptual issues.\n\nGiven that top-tier conferences require strong theoretical foundations and clear empirical validation, the unresolved gradient issue and lack of convergence guarantees are deal-breakers. The paper's contributions don't seem novel enough to outweigh these flaws. Therefore, it should be rejected.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Core Algorithmic Flaw**: Multiple reviewers (2, 3, 5) highlighted a critical issue with Algorithm 1: it does not properly compute the gradient of the weighted loss objective. The algorithm omits a key term in the gradient derivation, undermining its theoretical validity. The authors’ response did not resolve this, leaving the algorithm’s alignment with the proposed objective in doubt. This flaw invalidates the method’s foundational claims.\n\n2. **Theoretical Weaknesses**: \n   - The paper’s theoretical contributions (e.g., Theorem 1, Lemma 2) are criticized as \"basic computations\" (Reviewer 4) or \"straightforward\" (Reviewer 5) once dependencies on parameters are ignored. \n   - The lack of convergence guarantees for Algorithm 1 (noted by Reviewers 3 and 4) further weakens its practical utility.\n   - The inconsistent treatment of convexity across sections (e.g., Section 2 vs. Section 3) breaks coherence, and the justification for negative λ values is unclear (Reviewers 1, 4).\n\n3. **Empirical Concerns**: \n   - Empirical improvements may stem from hyperparameter tuning rather than the method’s inherent merit (Reviewer 1). \n   - The paper does not adequately compare to existing robust methods like Trimmed Mean or MoM (Reviewer 4), limiting its novelty.\n\n4. **Clarity and Notation**: \n   - Confusing notation (e.g., \"convex objective ℓ\" in Lemma 2) and poor writing (Reviewer 6, 7) hinder readability. \n   - Despite revisions, key ambiguities (e.g., the role of λ, the computational graph) remain unresolved.\n\n5. **Novelty and Significance**: \n   - Reviewers 4 and 5 judged the contributions as \"marginal\" or \"not novel,\" with technical insights already present in prior work (e.g., Duchi & Namkoong, 2019). \n   - The paper’s theoretical claims are incremental and lack a compelling motivation for penalizing higher-order moments beyond variance.\n\n### Conclusion:\nThe unresolved algorithmic flaw, weak theoretical grounding, and lack of compelling empirical or novel contributions make this paper unsuitable for a top-tier conference. The authors’ responses did not sufficiently address core criticisms, and the remaining issues significantly outweigh the paper’s potential contributions."
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving adversarial robustness through a new adversarial training method that uses sample reweighting with a bilevel optimization approach inspired by MAML. The reviewers have mixed opinions, so I need to weigh their points carefully.\n\nStarting with the strengths: The paper introduces a novel method combining MAML with adversarial training, which is a fresh angle. The experiments show improvements over some baselines, especially in clean and robust accuracy. The authors addressed some concerns in their response, like adding experiments on adaptive attacks and clarifying parts of their methodology.\n\nNow the major concerns. Reviewer 1 points out that the approach is a direct adaptation of MAML, which limits novelty. They also stress the lack of evaluation against adaptive attacks, which is critical for adversarial robustness. The authors did run some preliminary tests, but maybe not comprehensive enough. The performance gains are marginal, especially under AutoAttack, and they didn't compare to SOTA like AWP. Using older datasets like MNIST might not be strong enough for a top-tier conference.\n\nOther reviewers echo these points. Reviewer 6 mentions that BiLAW alone isn't competitive with methods like TRADES and the gains might come from combining with others. The technical novelty is considered marginal by several reviewers, which is a problem for a top venue. The empirical results, while positive, don't beat the best methods and the ablation studies could be better.\n\nOn the flip side, some reviewers (like 4,7,8,9,10) lean towards acceptance, citing the method's novelty in applying MAML to adversarial training and decent empirical results. They acknowledge the limitations but think the contribution is worth publishing.\n\nThe key issues here are novelty and thoroughness of evaluation. The method isn't groundbreaking since it's an adaptation of existing techniques. The lack of strong SOTA comparisons and not addressing adaptive attacks fully are red flags. While the authors did some experiments on adaptive attacks, the results aren't compelling enough. Also, the marginal gains and reliance on older datasets might not meet the high bar of a top conference.\n\nConsidering all this, the paper has some merit but falls short in innovation and rigorous evaluation. The reviewers' consensus leans towards rejection due to these shortcomings, even though there are positive aspects. The authors need more work to strengthen the novelty and experiments before it's ready for a top-tier venue.\n</think>\n\nThe paper proposes a novel adversarial training framework called BiLAW, which integrates sample reweighting via a bilevel optimization approach inspired by MAML. While the method introduces an interesting application of meta-learning to adversarial robustness and shows marginal improvements over some baselines, several critical issues undermine its readiness for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty in Application**: The combination of MAML-style bilevel optimization with adversarial training for sample reweighting is a fresh angle, particularly in leveraging a parametric function to map multi-class margins to weights.\n2. **Empirical Validation**: The paper demonstrates consistent, albeit modest, improvements in clean and robust accuracy compared to heuristic reweighting methods like GAIRAT and MAIL. The use of AutoAttack and other strong adversaries adds credibility.\n3. **Clarity and Organization**: The paper is well-written, and the methodology is logically structured.\n\n### Major Concerns:\n1. **Technical Novelty**: Multiple reviewers (1, 2, 6, 10) note that the core idea is a direct adaptation of MAML and meta-weight networks to adversarial training, lacking significant innovation. The multi-class margin concept, while intuitive, is not rigorously justified theoretically, and prior work on margins (e.g., in boosting) is under-explored.\n2. **Incomplete Evaluation**:\n   - **Adaptive Attacks**: Reviewer 1 and others emphasize that the method’s reliance on a learned reweighting module necessitates evaluation against adaptive attacks that exploit this module. While the authors provided preliminary results, these are limited and inconclusive (e.g., adaptive attacks only slightly underperform standard PGD). A top-tier venue requires thorough analysis of such scenarios.\n   - **SOTA Baselines**: The paper fails to compare against state-of-the-art methods like AWP, and results on CIFAR-100 are omitted. The improvements over TRADES are marginal under AutoAttack, and the reliance on small architectures (e.g., MNIST/F-MNIST) weakens the empirical significance.\n3. **Ablation and Justification**: The input encoding for the reweighting network (e.g., multi-class margin vs. raw data/activations) lacks rigorous justification. While ablations exist in the appendix, reviewers (3, 9) argue they do not address critical questions about the superiority of margin-based inputs.\n4. **Performance Trade-offs**: Reviewer 6 highlights that BiLAW’s gains often depend on combining with TRADES, raising questions about its standalone efficacy. The trade-off between clean and robust accuracy is not fully explored, and the method does not outperform SOTA in absolute terms.\n\n### Author Responses:\nThe authors addressed some concerns (e.g., adaptive attacks, typos), but key issues remain unresolved. The adaptive attack experiments are preliminary and insufficient to confirm robustness. The lack of comparisons to AWP and SOTA baselines on modern datasets (e.g., CIFAR-100) persists, and theoretical justifications for the multi-class margin remain underdeveloped.\n\n### Conclusion:\nWhile the paper contributes a novel angle to adversarial robustness, its technical novelty is incremental, and the evaluation is incomplete. The marginal performance gains, reliance on outdated benchmarks, and failure to address adaptive attacks and SOTA comparisons fall short of the high bar for a top-tier conference. The reviewers’ consensus, particularly among those emphasizing rigor (Reviewers 1, 3, 6), leans toward rejection unless these issues are resolved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is titled \"Sample and Computation Redistribution for Efficient Face Detection.\" The main contributions are two methods: Computation Redistribution (CR) and Sample Redistribution (SR). They claim these improve face detection efficiency, especially for small faces in low-res images. The experiments show state-of-the-art results on WIDER FACE, with their SCRFD-34GF model outperforming TinaFace by 4.78% AP on the hard set while being 3x faster. The code is available, which is a plus.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Let's break down each reviewer's input:\n\nReviewer 1 and 2 are very brief, but 1 says \"The author has solved my problem,\" which is positive. Reviewer 2 and 3 mention that authors addressed their questions, so maybe initial concerns were resolved.\n\nReviewer 4 points out strengths like addressing small faces and effective ablation studies. However, they criticize the search strategy as straightforward and not interesting, and lack of comparison with network search methods. They rate technical novelty as 2 (marginal) and empirical as 3 (somewhat new). Correctness is 3 (minor issues).\n\nReviewer 5 is more positive, giving correctness 4, but technical novelty 3. They note the method's effectiveness for small faces but limited application beyond scale ranges. The author addressed their concerns about figures and comparisons.\n\nReviewer 6 is positive, giving high scores (4s) but asks about NAS comparisons and whether evolutionary methods could do the same. The author added comparisons with evolutionary methods in the response, showing their method outperforms them. The reviewer thinks it's acceptable despite questions.\n\nReviewer 7 mentions missing figures and details, but the authors added those. They also pointed out that cross-dataset comparisons were lacking, but the authors provided more data showing their method surpasses others. However, technical novelty was rated 2 here.\n\nAuthor responses addressed several issues: added figures, algorithms, clarified sample distribution, and included more comparisons with NAS methods like evolutionary and BFBox. They also clarified the search space and methodology differences from other NAS approaches.\n\nNow, considering the criteria for a top-tier conference. The paper's main strengths are strong empirical results on a well-known dataset, clear ablation studies, and practical improvements (speed vs accuracy). The methods, while perhaps not groundbreaking in innovation, are effective and provide a useful contribution to efficient face detection, especially for small faces. The authors addressed many of the reviewers' concerns, like adding comparisons with NAS methods and improving the paper's clarity.\n\nThe weaknesses include the search strategy being seen as straightforward, and some reviewers feeling the technical novelty is marginal. However, the empirical results are compelling, and the problem addressed (efficient detection, especially for small faces) is important. The fact that they outperform existing methods significantly (like TinaFace) is a strong point.\n\nTop conferences often value both technical novelty and strong empirical results. While the technical novelty might be on the lower side according to some reviewers, the practical impact and the thorough experiments might tip the balance. The authors have also strengthened the paper by addressing the NAS comparison issues, which was a key concern.\n\nI think the paper should be accepted. The improvements and empirical results are significant enough for a top conference, even if the methods aren't entirely novel. The problem is relevant, and the solutions are effective and well-evaluated.\n</think>\n\nThe paper proposes two methods, Computation Redistribution (CR) and Sample Redistribution (SR), to address the challenge of efficient face detection, particularly for small faces in low-resolution images. The SCRFD framework achieves state-of-the-art performance on the WIDER FACE dataset, outperforming prior methods like TinaFace by a significant margin (4.78% AP on the hard set) while maintaining computational efficiency. Key strengths include strong empirical results, clear ablation studies demonstrating the effectiveness of CR and SR, and practical relevance for real-world applications (e.g., mobile devices). The authors also addressed reviewer concerns by adding figures, algorithm descriptions, and comparisons with NAS methods like evolutionary search and BFBox, which strengthened the paper's technical and empirical rigor.\n\nHowever, some reviewers noted limitations: the search strategy is relatively straightforward, comparisons with prior NAS methods were initially inadequate, and the technical novelty is marginal. Despite these critiques, the authors provided thorough responses, including additional experiments and clarifications, which mitigated these concerns. The problem of efficient face detection for small faces is well-motivated, and the results are compelling enough to justify acceptance at a top-tier conference, even if the contributions are incremental. The practical impact and thorough evaluation outweigh the modest technical novelty.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum.\" The abstract mentions that they analyze SGDm under covariate shift, showing it can be unstable and diverge due to resonance, using ODE approximations and Floquet theory. They also do empirical tests beyond their theoretical assumptions.\n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Let's break down each reviewer's points:\n\nReviewer 1 didn't add much beyond acknowledging the response. \n\nReviewer 2 is critical. They question the technical novelty, arguing that the proofs rely on existing work (like Faragó et al.) and that the claimed \"technical novelty\" is weak. They also point out that the intuition for why resonance occurs at specific frequencies isn't clear. Additionally, they're concerned that the theory ignores stochasticity (the \"S\" in SGD), which might make the results less applicable. However, the author response addresses some of these points, like showing experiments with stochasticity still align with theory.\n\nReviewer 3 thinks the problem is important but the contributions are limited. They mention that Proposition 1 and Theorem 1 are straightforward applications of existing methods (Muehlebach & Jordan, Halanay), and the presentation could be better. They also question if the analysis is of SGDm or just its expected version. They note that resonance might only occur in edge cases (high momentum, small noise), making it less practical. The author response tried to clarify the connection between theory and experiments, and addressed the stochasticity concern with additional experiments.\n\nReviewer 4 is positive, finding the paper well-written and the empirical results valuable, though they can't assess the math. They recommend acceptance.\n\nReviewer 5 thinks the paper is important but the theory could be improved. They suggest analyzing the discrete dynamics directly and providing more concrete examples. They also note that the empirical results in non-linear settings might not be due to the same mechanism. The authors addressed some of these by adding intuition and experiments.\n\nReviewer 6 is generally positive but wants more real-world examples and analysis of other optimizers like Adam. They also point out that the practical relevance might be limited if real data doesn't have clear oscillations. The authors mention future work on real datasets and experiments with Adam.\n\nThe author responses address several points: softening novelty claims, adding intuition, clarifying stochastic experiments, and acknowledging limitations. They argue that the connection between ML and resonance theory is novel, even if the math isn't new.\n\nNow, considering the criteria for a top-tier conference. The paper's main contributions are identifying a resonance phenomenon in SGDm under covariate shift, using ODE analysis and empirical validation. The theoretical part builds on existing math but applies it to a new context (ML optimization under covariate shift). The empirical results extend beyond the linear case to neural networks and other optimizers, showing the phenomenon persists.\n\nThe criticisms about technical novelty are valid—some proofs use existing techniques. However, applying these to show a new phenomenon in ML is a contribution. The authors clarify that the novelty is in connecting resonance theory to ML, which is a valid point. The presentation issues (like unclear intuition) were addressed in the response with added explanations and experiments.\n\nThe stochasticity concern is addressed by showing that even with single samples, the resonance still occurs, aligning with theory. The reviewers who were critical (2 and 3) have some valid points but the authors have responded adequately. The positive reviews (4,5,6) highlight the importance and thoroughness of the work.\n\nTop conferences often value novel insights and clear contributions even if the techniques aren't entirely new. The paper's exploration of a previously unnoticed failure mode in SGDm under non-iid data is significant. The empirical validation across different settings strengthens the case. While there are limitations (like the linear case assumptions), the broader impact and potential for future work are substantial.\n\nConsidering all this, the paper's contributions, despite some technical limitations, are novel and important enough for a top-tier conference. The authors have addressed key concerns, and the work opens up new avenues for understanding optimization dynamics under non-iid conditions.\n</think>\n\nThe paper investigates the stability of stochastic gradient descent with momentum (SGDm) under covariate shift, a non-i.i.d. setting common in real-world applications like continual learning and reinforcement learning. It identifies a resonance phenomenon where periodic covariate shifts can cause exponential divergence of SGDm, leveraging tools from Floquet theory and parametric oscillators. The theoretical analysis is complemented by empirical validation across linear models, neural networks, and other optimizers, demonstrating the robustness of the resonance effect beyond idealized assumptions. \n\n### Key Strengths:\n1. **Novel Phenomenon Identification**: The paper is the first to connect covariate shift to resonance-driven divergence in optimization, a previously underexplored failure mode. This bridges ML optimization and nonlinear dynamics, offering a new lens to understand SGDm behavior under non-i.i.d. data.\n2. **Theoretical Foundation**: By modeling SGDm as a time-varying ODE and applying Floquet theory, the work provides a rigorous framework to analyze divergence conditions. While relying on existing mathematical tools, the application to ML is novel and impactful.\n3. **Empirical Rigor**: Experiments extend beyond theoretical assumptions (e.g., nonlinear models, Adam optimizer) and show resonance persists even with stochasticity, strengthening the practical relevance of the findings.\n4. **Clarity and Structure**: The paper is well-written, systematically moving from hypothesis to analysis to validation, as highlighted by Reviewer 4.\n\n### Criticisms Addressed:\n- **Technical Novelty Concerns**: While reviewers (2, 3) questioned the novelty of the mathematical techniques, the authors clarify that the core contribution lies in applying these tools to ML, a novel and valuable interdisciplinary connection. The resonance mechanism itself is a novel insight for the ML community.\n- **Stochasticity Critique**: The authors demonstrate that even with full stochasticity (single-sample updates), resonance aligns with theoretical predictions, addressing concerns about ignoring the \"S\" in SGD.\n- **Presentation Gaps**: The authors added intuitive explanations (e.g., the \"ball and bowl\" analogy) and clarified ambiguities in the theoretical discussion, improving accessibility.\n\n### Limitations:\n- **Scope of Theoretical Results**: The primary analysis is limited to linear models with periodic covariate shifts. However, the empirical results show the phenomenon generalizes, and the authors acknowledge this as a foundation for future work.\n- **Practical Relevance**: Reviewers (3, 6) note resonance might occur in edge cases (e.g., high momentum). The authors respond by highlighting scenarios where sharp spectral peaks (e.g., sensor data) could trigger resonance and emphasize the theoretical contribution as foundational.\n\n### Conclusion:\nThe paper’s identification of resonance-driven divergence in SGDm under covariate shift is a significant contribution to optimization theory and practice. While the technical novelty is incremental in mathematical terms, the interdisciplinary application and empirical validation make it a valuable addition to the literature. The authors have adequately addressed critical concerns, and the work’s potential to inform robust optimization strategies in non-i.i.d. settings justifies its acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about TimeVAE, a VAE-based approach for generating multivariate time series data. The authors claim it has advantages like interpretability, domain knowledge incorporation, and faster training compared to GANs. \n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 mentions the paper is well-written and has an interesting approach with classical time series components. However, they point out weaknesses like lack of hyperparameter details and ablation studies on interpretability. They also note confusing results in Table 1 where higher training data leads to worse performance, which is counterintuitive. The reviewer thinks the empirical evaluation is insufficient and the paper isn't ready for publication.\n\nReviewer 2 says the paper is well-written but questions the technical novelty. They argue that the model isn't much different from a standard VAE applied to time series, especially since it uses convolutions instead of recurrent layers. They also mention that TimeGAN has additional loss functions which TimeVAE lacks, making it unclear why TimeVAE is better. The reviewer concludes the contributions aren't significant enough for acceptance.\n\nReviewer 3 acknowledges the strengths like the general-purpose approach and clear writing but criticizes the literature review for lacking context on related models like state-space models. They point out methodological issues, such as using a single global latent variable instead of per-time-step variables, and experimental flaws like not using well-known datasets or evaluating interpretability. The experiments also don't compare to autoregressive models, and the discriminator score isn't compared to standard metrics like FID. They recommend major revisions.\n\nReviewer 4 thinks the paper is well-written but the contributions are limited. They note that interpretability isn't discussed in results and the performance isn't better than SOTA except on one dataset. They question the practical contribution to the ML community.\n\nThe authors didn't respond, so there's no rebuttal to address these points. \n\nNow, considering the standards of a top-tier conference like ICLR, the paper needs to have significant technical novelty, solid empirical validation, and address reviewer concerns. The main issues here are:\n\n1. **Technical Novelty**: Reviewers 2 and 3 question how much the model differs from existing VAEs. The decoder's components add some structure, but without clear innovation over prior work (like GP-VAE mentioned by Reviewer 1), it's marginal.\n\n2. **Empirical Evaluation**: Multiple reviewers point out insufficient experiments. The lack of ablation studies on hyperparameters and interpretability undermines the claims. The datasets used aren't standard, making comparisons hard. The performance on some datasets isn't better than SOTA, and the Table 1 results are confusing.\n\n3. **Interpretability**: A key contribution isn't properly evaluated. The paper claims interpretability but doesn't provide experiments to back it up, which several reviewers noted.\n\n4. **Methodological Flaws**: Using a global latent variable instead of per-time-step might limit the model's effectiveness. The VAE loss isn't well-explained, and the discriminator score isn't compared to established metrics.\n\n5. **Literature Review**: The background is lacking, not discussing related work thoroughly, which affects the paper's context and significance.\n\nGiven these points, while the paper has some merits like clear writing and a good idea, the lack of thorough evaluation, insufficient novelty, and unaddressed methodological issues are critical. Top conferences require strong contributions and rigorous validation. Since the reviewers (especially 1,2,3) argue the paper isn't ready without major revisions, and the authors didn't respond to address these, it's likely better suited for a minor revision elsewhere rather than acceptance at a top-tier conference.\n</think>\n\nThe paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" proposes a VAE-based architecture for generating synthetic time-series data, emphasizing interpretability, domain knowledge integration, and reduced training times. While the paper is well-written and introduces an interesting approach to incorporating classical time-series components (e.g., trends and seasonality), several critical issues undermine its suitability for acceptance at a top-tier conference like ICLR:\n\n### Key Strengths:\n1. **Interpretability and Domain Knowledge**: The inclusion of interpretable decoder blocks for trends and seasonality is a novel angle, which could be valuable in applications requiring transparency.\n2. **Empirical Performance**: The paper shows competitive results on similarity and next-step prediction tasks compared to some state-of-the-art methods, particularly on the \"sine\" dataset.\n3. **Clarity and Writing**: The paper is well-structured and accessible, making its technical ideas clear.\n\n### Major Weaknesses:\n1. **Technical Novelty**: \n   - Reviewers 2 and 3 argue that the core architecture (e.g., using convolutional layers and a global latent variable) is not sufficiently distinct from existing VAEs or other generative models (e.g., TimeGAN). The decoder’s \"classical\" components are not rigorously compared to prior work like GP-VAE (Fortuin et al., 2020), weakening claims of novelty.\n   - The use of a single global latent variable for the entire sequence is criticized as limiting, contrasting with state-space models that use per-time-step latent variables.\n\n2. **Empirical Evaluation**:\n   - **Ablation Studies and Hyperparameters**: Reviewers 1 and 3 highlight the absence of ablation studies to validate the interpretability claims or hyperparameter selection processes for the decoder components. This undermines the paper’s key selling points.\n   - **Confusing Results**: Table 1’s results (worse performance with full training data) are unexplained and counterintuitive, raising concerns about experimental rigor.\n   - **Lack of Standard Benchmarks**: The use of non-standard datasets and failure to compare against probabilistic autoregressive models or established metrics (e.g., FID scores) weakens the empirical significance.\n   - **Interpretability Evaluation**: The paper claims interpretability but provides no experiments or analysis to demonstrate it, contradicting its own stated contributions.\n\n3. **Methodological Flaws**:\n   - The VAE loss function is presented without sufficient derivation or justification for its weighting terms.\n   - The discriminator score metric is inadequately explained, and its comparison to standard metrics is missing.\n   - The reliance on post-hoc LSTM models for next-step prediction instead of integrating predictive capabilities into the VAE design is criticized as suboptimal.\n\n4. **Literature Review**:\n   - The paper lacks depth in discussing related work (e.g., state-space models, particle filters), leading to an incomplete contextualization of contributions.\n   - Claims like \"autoregressive models are deterministic\" are technically incorrect, reflecting imprecise understanding of prior work.\n\n5. **Practical Impact**:\n   - Reviewer 4 questions the practical relevance of the method, noting that synthetic data generators must address specific real-world challenges, which the paper does not demonstrate.\n\n### Conclusion:\nWhile the paper introduces an interesting idea and shows some promising results, the lack of rigorous empirical validation, insufficient technical novelty, and unresolved methodological issues (e.g., interpretability evaluation, hyperparameter analysis, and dataset choices) significantly weaken its case for acceptance at a top-tier conference. The reviewers’ consensus (especially Reviewers 1, 2, and 3) that the paper requires major revisions before publication aligns with the observation that the contributions are marginal compared to existing work. Without addressing these concerns, the paper does not meet the high standards of ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. I need to go through all the provided information carefully.\n\nFirst, the paper's title and abstract. The main idea is about a new benchmark for compositional learning using meta-referential games and a Symbolic Continuous Stimulus (SCS) representation. The goal is to assess agents' ability to generalize compositionally, which is important for human-AI collaboration. \n\nLooking at the reviewer comments, there's a mix of positive and negative points. Reviewer 1 mentions the idea is interesting but the paper is hard to read and the experiments aren't sufficient. They point out that the SCS isn't clearly explained and the experiments lack baseline comparisons. Reviewer 2 also finds the paper unclear, with missing details on the benchmark and SCS. They note that OHE performs better in some experiments, which undermines the SCS's value. Reviewer 3 is more critical, saying the contributions aren't novel enough and the SCS's utility is limited. They also mention the paper's clarity issues. Reviewer 4 thinks the work is ambitious but needs more development, especially in evaluating against existing benchmarks and clarifying the SCS's advantages over continuous representations.\n\nThe authors responded by acknowledging the need for improvements but didn't provide specific rebuttals to address the technical concerns. \n\nNow, considering top-tier standards, the paper needs to have significant contributions, clear explanations, and solid empirical validation. The reviewers consistently point out that the paper's clarity is a major issue. The SCS isn't well-explained, and the benchmark isn't detailed enough. The experiments are underwhelming, with baselines not properly compared and results barely above chance. Technical novelty is considered marginal, and the empirical results don't strongly support the claims. Reviewer 4 even says the main claims aren't well-supported. \n\nWhile the problem of compositional learning is important, the current submission doesn't meet the required standards for a top conference. The lack of clarity, insufficient experiments, and marginal contributions are deal-breakers. The authors might improve it, but as it stands, it's not ready for acceptance.\n</think>\n\nThe paper proposes a novel benchmark and representation (SCS) to study compositional learning in artificial agents, addressing an important challenge in AI. However, the reviewers' critiques reveal critical shortcomings that undermine its readiness for a top-tier conference:\n\n1. **Clarity and Explanation**: Multiple reviewers (1, 2, 3, 4) highlight significant readability issues. The SCS representation is inadequately explained, key concepts like \"positional disentanglement\" and \"systematic generalization\" are undefined, and the benchmark’s structure and novelty are poorly articulated. The abstract and introduction lack a clear problem statement, and figures are illegible. These issues hinder reproducibility and broader impact.\n\n2. **Technical and Empirical Weaknesses**: \n   - The SCS’s advantages over alternatives (e.g., one-hot encoding) are not convincingly demonstrated. Reviewer 2 notes that OHE outperforms SCS in experiments, and the authors do not address why SCS is preferable in practical scenarios. \n   - Experiments are underpowered: limited baselines (no comparison to continuous representations), insufficient seeds (only 2 for key results), and no ablation studies. The results (e.g., 29% accuracy vs. 25% chance) are marginally better than random, raising questions about the task’s difficulty or the models’ limitations.\n   - The benchmark’s novelty is contested. Reviewer 3 argues it lacks external utility beyond the paper’s narrow setup, and Reviewer 4 notes it does not engage with existing compositional benchmarks like SCAN.\n\n3. **Novelty and Significance**: Reviewers 1–4 rate technical and empirical significance as marginal. The SCS is seen as a niche solution with unclear real-world applicability, and the meta-referential game setup does not sufficiently distinguish itself from prior work (e.g., referential games in language grounding). The failure of baselines (e.g., DNC vs. LSTM) may reflect hyperparameter issues rather than inherent task difficulty.\n\n4. **Literature Engagement**: Reviewer 4 points out the paper does not adequately situate itself within decades of research on compositionality (e.g., Chalmers, Smolensky) or recent benchmarks like gSCAN. This weakens its theoretical contributions.\n\n5. **Author Response**: The authors’ reply is generic and does not address specific technical concerns (e.g., SCS vs. continuous representations, experiment reproducibility). Without concrete rebuttals or plans to strengthen these areas, confidence in the work’s validity remains low.\n\n**Conclusion**: While the problem of compositional learning is valuable, the paper’s lack of clarity, underwhelming empirical results, and marginal technical contributions fall short of top-tier standards. The reviewers’ consensus (especially Reviewer 3 and 4’s rejection recommendations) underscores that the work requires substantial revision to clarify its ideas, validate its methods rigorously, and demonstrate broader relevance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title and abstract suggest it's about overparameterization in recursive value estimation for reinforcement learning. The key contributions seem to be analyzing how different algorithms (TD, FVI, RM) behave in an overparameterized linear setting, showing they converge to different fixed points, and proposing regularizers for deep models based on this analysis. The experiments show improved stability and performance.\n\nNow, looking at the reviewer comments. There are 10 reviewers here. Let's break down their feedback.\n\nReviewer 1 and 4 are positive, saying their concerns were addressed. Reviewer 2 wants more credit to existing work and to avoid strong claims about deep RL. Reviewer 3 points out that a solution (equation 71) is already known and needs proper citation. They also mention that the connection to infinite-width neural nets isn't addressing their concern about practical deep RL models not being that wide. \n\nReviewer 6 has several points. They note that the matrix decomposition assumption wasn't justified, and the paper needs more content like asymptotic results or experiments with wider networks. They also question the generalization error bound's explosiveness, but the authors clarified that in overparameterization, d scales with n, so it's okay. However, they still feel the paper isn't enough to change their rating. \n\nReviewer 7 is excited about the topic but critical of the paper's scholarship. They point out that Theorem 2 is similar to existing work without proper credit, Theorem 4 is a known result from convex optimization (like the minimum norm solution), and the proof has errors. They also question the connection to deep RL since the last layer's features might not be underdetermined. Experiments are seen as toyish and not impactful. Despite this, they lean towards acceptance.\n\nReviewer 8 mentions the dataset assumption (independent samples vs. trajectories) and asks about more complex environments. They increased their score but lowered confidence. \n\nReviewer 9 is more critical, pointing out errors in equations, incorrect proofs (like Hoeffding's inequality misuse), and confusion about overparameterization's definition. They think the technical contributions are weak and need better writing. \n\nReviewer 10 thinks the linear case is too restrictive and wants extensions to nonlinear models. They also question the significance and suggest discussing NTK. \n\nThe authors addressed some points, like adding references and clarifying the matrix decomposition. They also explained the connection to NTK for nonlinear models. However, some issues remain, like existing work not being cited properly and the empirical sections being weak.\n\nNow, considering the standards of a top-tier conference. The paper's theoretical contributions seem novel in the overparameterized linear RL setting, but there's overlap with prior work that needs better acknowledgment. The empirical results are in toy environments, which might not be sufficient for a top venue. Reviewers 7 and 9 raised valid concerns about scholarship and correctness. The authors addressed some, but others like the misuse of Hoeffding's inequality and equation errors need fixing. \n\nThe paper's main strength is the theoretical analysis in the overparameterized regime, which is a hot topic. However, the lack of strong empirical validation and potential oversights in existing literature could be issues. The reviewers' mixed feedback shows some see merit but want improvements. Since the authors have addressed some concerns and the topic is relevant, but there are unresolved issues (like proper citations and empirical rigor), I'm on the fence. \n\nTop conferences require high novelty and solid contributions. The paper's theoretical part has contributions but needs better scholarship. The experiments are underwhelming. Given that some reviewers (like 7 and 10) think the linear case is too restrictive and the empirical impact is low, maybe it's borderline. However, the problem addressed is important, and the authors have made efforts to address concerns. \n\nFinal Decision: Accept, but with the caveat that the authors must thoroughly revise the citations, fix the noted errors, and possibly strengthen empirical sections. Since the reviewers' scores are mostly in the accept range (especially after rebuttal), and the topic is timely, it's a marginal accept.\n</think>\n\nThe paper presents a theoretical analysis of recursive value estimation algorithms (TD, FVI, RM) in the overparameterized linear regime, highlighting differences in their fixed points and proposing regularizers for deep models. While the topic is timely and the theoretical contributions are novel in the overparameterized linear setting, several critical issues emerge from the reviews:\n\n1. **Scholarship and Prior Work**: Reviewers 2, 3, and 7 note insufficient acknowledgment of existing results (e.g., Theorem 4 being a known minimum-norm solution, equation 71’s solution already established). The authors addressed some concerns but must ensure proper citations and clarify contributions relative to prior work.\n\n2. **Technical Soundness**: Reviewer 9 identifies errors in proofs (e.g., misuse of Hoeffding’s inequality, equation typos) and questions the validity of overparameterization assumptions. While some issues were addressed, unresolved gaps remain, particularly in proof rigor and empirical methodology (e.g., independent vs. trajectory-based datasets).\n\n3. **Empirical Weakness**: Multiple reviewers (7, 8, 10) criticize the experiments as toyish and insufficiently motivating. The empirical results, while supportive, lack depth and real-world relevance, weakening the practical significance of the proposed regularizers.\n\n4. **Broader Relevance**: Reviewers 3, 7, and 10 question the connection to deep RL, noting that overparameterized linear models may not fully capture neural network behavior. The authors’ NTK-based extension is promising but underdeveloped, requiring clearer links to deep learning practice.\n\n**Strengths**: The paper advances understanding of algorithmic behavior in overparameterized regimes, a critical area in modern RL. The theoretical framework unifying different algorithms’ implicit biases is insightful, and the proposed regularizers show empirical promise.\n\n**Weaknesses**: The scholarship gaps, empirical limitations, and unresolved technical issues (e.g., proof errors) undermine confidence in the work’s rigor and broader impact. While the topic is important, the execution falls short of top-tier standards without significant revisions.\n\n**Conclusion**: The paper’s contributions are non-trivial but lack the polish, thoroughness, and empirical validation expected at a top conference. While the topic is timely and the core ideas are promising, the unresolved issues in scholarship, technical soundness, and empirical rigor tip the decision toward rejection unless major revisions are made. However, given the reviewers’ mixed but generally supportive post-rebuttal scores and the paper’s potential, a conditional accept is plausible. Yet, strict adherence to top-tier standards requires a stronger demonstration of novelty, correctness, and empirical relevance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about epoch-wise double descent in neural networks, using a linear teacher-student model. The reviewers have mixed feedback. \n\nFirst, looking at the strengths. Reviewer 1 is positive, saying it's a valuable contribution, clearly written, and the theory is well-motivated. They mention the work shows that epoch-wise double descent can be explained through feature learning at different scales, which is insightful. The empirical validation with ResNet18 on CIFAR10 is a plus. The main issues they raised are about extending to more than two scales (triple descent?), differences between epoch-wise and model-wise double descent, and the peak's interpretation. The authors addressed these by adding experiments for triple descent and discussing the differences in the revised version.\n\nReviewer 2 points out connections to prior work on implicit regularization (like Ridge) and HDLSS settings. The authors added discussions on these connections and referenced relevant papers. They also provided a colab notebook to show the link between training time and regularization. That seems to handle the concerns.\n\nReviewer 3 had more technical concerns: typos, notation issues, and whether the contributions are novel enough. The authors fixed typos, clarified notation, and added appendices with detailed derivations. They also addressed the novelty by comparing with Heckel & Yilmaz and Stephenson & Lee, showing their approach offers closed-form expressions and a different data model. The connection to deep networks via NTK was strengthened with new experiments, which helps bridge theory and practice.\n\nThe technical novelty and significance were rated 3 by Reviewers 1 and 2, but 2 by Reviewer 3. However, the authors' responses seem to have addressed the novelty concerns by highlighting their unique contributions like the time-dependent distribution and closed-form analysis. The empirical work also supports their claims.\n\nThe main issues were addressed in the author response: typos fixed, connections to prior work clarified, experiments added for triple descent, and better explanations of the model's applicability to real networks. The reviewers' concerns about technical depth and connections were met with thorough revisions.\n\nConsidering top-tier standards, the paper provides a novel theoretical framework explaining epoch-wise double descent, validated empirically. The contributions, while building on prior work, offer new insights and analysis methods. The revisions have strengthened the paper's rigor and clarity. The typos and minor issues were resolved, and the authors engaged constructively with feedback.\n\nFinal Decision: Accept\n</think>\n\nThe paper investigates epoch-wise double descent in neural networks through a linear teacher-student model, offering theoretical insights and empirical validation. Key strengths include its clear exposition, analytical tractability, and alignment with observations in deep networks like ResNet18. The authors address reviewer concerns by clarifying connections to prior work (e.g., implicit regularization, HDLSS settings), fixing technical issues (typos, notation), and enhancing the theoretical framework (e.g., time-dependent distributions, closed-form expressions). They also strengthen the link to realistic networks via NTK experiments and address questions about multi-scale features and triple descent. While some reviewers initially questioned novelty and technical depth, the revisions demonstrate sufficient advancements, such as extending analyses to transient dynamics and providing detailed derivations. The paper’s contributions—bridging theory and practice, explaining double descent via feature-scale interactions, and offering tools for further study—align with top-tier conference standards despite building on existing foundations.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about AutoOED, an automated experimental design platform using Bayesian optimization for multi-objective problems. The reviewers have mixed opinions. Let me go through each part step by step.\n\nFirst, the title and abstract mention it's a platform with data and time-efficient MOBO. The key contributions are the modular framework, the Believer-Penalizer (BP) strategy for asynchronous batch experiments, a GUI, and real-world automation. \n\nLooking at the reviewer comments, there's a split. Reviewer 1 thinks it's a good tool but questions the performance and novelty. They mention Figure 5 shows mixed results and the methods aren't very novel. They gave a 5 but leaning towards rejection. \n\nReviewer 2 is critical, saying it's more of a software manual than a research paper. They point out the platform's limitations, like only handling 2-3 objectives and not citing a related arXiv paper. They rate technical novelty as low. \n\nReviewer 3 also mentions it's engineering-focused, lacking theoretical backing for BP. They give low novelty scores. \n\nReviewer 4 acknowledges the engineering effort but says the methodological contributions are minor. They criticize the empirical results as overstated, noting AutoOED underperforms in some benchmarks. \n\nReviewer 5 is more positive, seeing the problem as important and the platform as useful, but notes minor issues. \n\nAuthor responses address some points: they added support for more objectives, clarified the BP strategy, and defended the platform's usability. They argue that engineering contributions are valid for ICLR and that their modular design and first asynchronous MOBO implementation are unique. \n\nNow, considering top-tier conference standards. ICLR does accept software contributions, but they need to have significant technical or methodological contributions. The main novel part is BP and asynchronous MOBO, but reviewers point out BP's performance isn't consistently better and the method isn't theoretically grounded. The platform's usability is a plus, but the technical novelty seems marginal. \n\nThe empirical results are mixed, with AutoOED not clearly outperforming others. The authors mention it's robust but the figures show underperformance in some cases. The lack of theoretical analysis for BP is a downside. \n\nThe paper's main strength is the platform's usability and modular design, which is valuable. However, the reviewers' consensus on low technical novelty and overstated empirical results might tip the balance. Since top conferences require significant contributions, and the main innovations are incremental, I think it's a borderline case but leaning towards rejection. The authors tried to address concerns, but the core issues about novelty and empirical impact remain.\n</think>\n\nThe paper presents AutoOED, a platform for multi-objective optimization with a focus on usability and asynchronous batch processing. While the platform addresses important practical challenges in experimental design and offers a user-friendly interface, the reviewers' critiques highlight critical limitations that undermine its suitability for a top-tier conference like ICLR. \n\n### Key Strengths:\n1. **Usability and Accessibility**: The GUI and modular design are significant contributions for non-expert users, addressing a gap in existing tools.\n2. **First Asynchronous MOBO Implementation**: AutoOED is noted as the first platform to integrate asynchronous multi-objective Bayesian optimization (MOBO), which is a novel integration.\n3. **Real-World Application**: The demonstration of real-world hardware integration shows practical relevance.\n\n### Major Concerns:\n1. **Technical Novelty**: \n   - The core methodological contribution (Believer-Penalizer, BP) is described as a \"straightforward\" combination of existing strategies (KB and LP) without rigorous theoretical justification. Reviewers question its novelty and robustness, noting that BP does not consistently outperform baselines (e.g., underperforming on DTLZ3).\n   - The platform primarily implements existing MOBO algorithms, with only minor innovations (e.g., asynchronous MOBO). While this is useful, it falls short of the \"significant\" novelty expected at ICLR.\n\n2. **Empirical Evaluation**:\n   - The results are mixed: AutoOED underperforms on 6/12 benchmarks and only clearly outperforms others in 4 cases. The authors overstate BP's performance by claiming it \"consistently outperforms\" others, which reviewers dispute.\n   - The real-world experiment is limited to a \"proof-of-concept\" comparison with random search, lacking deeper validation against alternative methods.\n\n3. **Methodological Gaps**:\n   - The BP strategy lacks theoretical grounding, and the authors acknowledge this limitation. \n   - The platform’s initial restriction to 2–3 objectives (later addressed in the response) reflects a scalability issue, though the fix is post-hoc and not empirically validated in the paper.\n\n4. **Overlapping Work**:\n   - Reviewer 2 points out an uncited arXiv paper with the same name and topic, raising concerns about novelty and prior work acknowledgment.\n\n5. **Conference Fit**:\n   - While ICLR accepts software contributions, they must demonstrate **both** technical innovation and empirical impact. The platform’s engineering contributions are valuable but insufficient to offset the lack of methodological breakthroughs.\n\n### Author Responses:\nThe authors addressed some concerns (e.g., expanding objective support, clarifying BP’s motivation), but key issues remain:\n- BP’s empirical performance is still inconsistent.\n- Theoretical gaps persist, and the platform’s contributions are framed as incremental rather than transformative.\n- The revised claims about BP’s performance are more tempered, but the paper still emphasizes it as a \"state-of-the-art\" strategy, which reviewers dispute.\n\n### Conclusion:\nWhile AutoOED is a useful tool for practitioners, its technical contributions are marginal, and the empirical results are overstated. The platform’s value lies in engineering and usability, but top-tier conferences require stronger methodological or theoretical advances. The reviewers’ consensus on low novelty and insufficient empirical rigor, coupled with the lack of a transformative idea, makes this paper borderline but ultimately unsuitable for acceptance at ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about using insights from neuroscience, specifically the thalamocortical architecture, to improve RNNs for hierarchical motor control tasks. The main goal is to allow RNNs to chain together learned motor motifs without interference and with zero-shot generalization. \n\nLooking at the reviewers' comments, there's a mix of positive and negative feedback. Reviewer 1 is still on the fence but not convinced. Reviewer 2 points out that the proposed resetting mechanism might not be the most biologically plausible and questions the novelty. They also mention that the contributions are only marginally significant. Reviewer 3 is critical about the lack of testing alternative hypotheses and the small scale of experiments. They argue that the models are too toy-like and not relevant to broader ML. Reviewer 4 is more positive, noting the simplicity and reproducibility but admits low confidence due to unfamiliarity with the literature. Reviewer 5 is harsh, pointing out unclear writing, lack of formal problem definition, and the existence of a trivial solution (resetting to a fixed state) that wasn't adequately addressed.\n\nThe authors responded by addressing some points: they clarified the advantages over reset strategies, added figures showing interpolation benefits, and discussed noise robustness. They also tried to improve the task description and address continual learning metrics. However, some concerns remain, like the scale of experiments and the comparison to alternatives.\n\nKey issues to consider for a top-tier conference: \n\n1. **Novelty and Significance**: Reviewers 2, 3, and 5 question the technical and empirical significance. The method might not be novel enough, especially if similar approaches exist. The problem's relevance to broader ML is under debate, given the small network size and synthetic tasks.\n\n2. **Experimental Rigor**: Reviewer 3 and 5 highlight that alternatives weren't sufficiently tested. The authors added some experiments but maybe not enough to convince. The task's trivial solution (resetting) wasn't adequately addressed in the original submission, though the response tries to counter that.\n\n3. **Clarity and Presentation**: Multiple reviewers (3,5) mention poor writing and unclear problem/task descriptions. The authors made improvements but might not have fixed all issues.\n\n4. **Biological Plausibility**: Reviewer 2 and 3 question if the mechanism aligns well with neuroscience. The authors tried to clarify but might still have gaps.\n\n5. **Scalability**: The small network size (300 units, 10 motifs) raises concerns about applicability. The authors argue this is sufficient for their task but top conferences often expect more scalable solutions.\n\nThe authors' responses addressed some points but didn't fully resolve major concerns. For instance, while they added experiments on reset strategies, the core issue of the task's trivial solution and the lack of broader validation remains. The writing improvements might help but were a significant drawback. The biological angle is there but not strongly validated against alternatives.\n\nConsidering top-tier standards, the paper needs strong novelty, significant impact, and rigorous experiments. The mixed reviews, especially the critical ones from Reviewers 3 and 5, suggest that the paper's contributions might not meet the high bar. The lack of thorough exploration of alternatives and the small-scale experiments are deal-breakers for a top conference. Even with revisions, the reviewers' lingering doubts about significance and methodology might not be overcome without more substantial changes.\n</think>\n\nThe paper proposes a method inspired by the thalamocortical architecture to enhance RNNs for hierarchical motor control, addressing challenges in continual learning and zero-shot motif chaining. While the work introduces a biologically motivated inductive bias and demonstrates some improvements over baseline methods, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Novelty and Significance**:  \n   - Reviewers 2, 3, and 5 question the technical and empirical significance of the contributions. The proposed \"resetting mechanism\" is criticized as computationally non-optimal and biologically implausible compared to existing alternatives (e.g., basal ganglia-driven disinhibition). The method’s novelty is further diminished by the lack of clear differentiation from prior work (e.g., Schuessler et al. 2020).  \n   - The task’s trivial solution (hard resets to a fixed state) was not adequately addressed in the original submission, though the authors added clarifications. However, the reviewers remain unconvinced of the necessity of the proposed inductive bias over simpler alternatives.\n\n2. **Experimental Rigor**:  \n   - Reviewers 3 and 5 highlight insufficient testing of alternative approaches (e.g., training on end states, noise-injected resets). While the authors added experiments, the scale of the experiments (300 units, 10 motifs) is deemed too small to generalize to real-world or larger-scale problems. The lack of validation on naturalistic datasets or tasks further limits relevance to broader ML.  \n   - The continual learning setup is poorly formalized, and key metrics (e.g., interference quantification) are inadequately explained, weakening the empirical claims.\n\n3. **Clarity and Presentation**:  \n   - Multiple reviewers (3, 5) note significant writing issues, including unclear problem definitions, fragmented task descriptions, and poor organization. The authors made improvements but did not fully resolve these concerns, which hinder reproducibility and accessibility.\n\n4. **Biological Plausibility**:  \n   - Reviewers 2 and 3 question the alignment of the proposed mechanism with neuroscience (e.g., biasing to the origin vs. observed preparatory activity). The authors’ response clarifies some points but does not fully address discrepancies with biological evidence (e.g., Susillo et al. 2015).\n\n5. **Scalability and Relevance**:  \n   - The small network size and synthetic task raise doubts about the method’s practical utility. While the authors argue that scalability is not the focus, top-tier conferences typically expect contributions with broader applicability or foundational insights. The problem’s narrow scope and lack of comparison to state-of-the-art continual learning methods further weaken its impact.\n\n**Conclusion**: Despite addressing some reviewer concerns, the paper’s marginal novelty, insufficient experimental validation, and unresolved clarity issues fail to meet the high standards of a top-tier conference. The lack of rigorous comparison to alternatives and the triviality of the proposed solution in the task’s context are particularly damning. While the biological inspiration is intriguing, the work does not yet demonstrate sufficient technical or empirical advancement to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about PEARL, a method for differentially private data synthesis using generative models. The reviewers have given their feedback, and the authors have responded. I need to decide if it should be accepted to a top-tier conference.\n\nStarting with the strengths. Reviewer 1 says the approach is novel, well-motivated, and the math is clear. They mention it outperforms DP-MERF and DP-GAN in experiments. The theoretical guarantees are a plus. But they point out the lack of thorough benchmarking—only a few datasets and not comparing to PATE-GAN or using something like CIFAR-10 for images. They also suggest moving Algorithm 1 to the main paper.\n\nReviewer 2 had some confusion about the privacy budget allocation and sensitivity calculations. The authors addressed these in their response, explaining the k factor and clarifying the privacy budget split. They also added more baselines like DPCGAN and another dataset (Credit). The reviewer still had minor issues but thought the approach was nice.\n\nReviewer 3 was more critical. They wanted more related work, better explanations on privacy implementation, more datasets, and baselines like PrivSyn. The authors added references, explained the one-shot sampling and privacy guarantees, included the Credit dataset, and DPCGAN. They also did a complexity analysis and added more evaluation tasks like MMD. However, the reviewer mentioned the paper's editorial quality was inconsistent and needed more \"realistic\" experiments. They gave a conditional recommendation for acceptance if these are fixed.\n\nLooking at the author responses: They addressed most points. Added datasets, more baselines, clarified technical aspects, and fixed some explanations. The main concerns were about benchmarking and clarity. The authors seem to have made efforts to include more experiments and references. The remaining issues might be minor, like the algorithm placement and some editorial fixes.\n\nTop-tier conferences usually require strong novelty, thorough experiments, and clear presentation. The paper's technical novelty is there, as it's a new framework with theoretical backing. The empirical results were initially limited but expanded in the rebuttal. The main weaknesses were addressed by adding more datasets and baselines. The privacy explanations were clarified, which is crucial for a DP method. The lack of CIFAR-10 might be a point, but the authors explained it's challenging for DP methods generally. \n\nThe reviewers' scores were mostly 3s and 4s, with Reviewer 3 suggesting a 5 but open to a 6 with fixes. The authors have made the necessary changes. The paper's contributions are significant enough, and the revisions seem to handle the major concerns. The editorial issues can be fixed in the camera-ready version. \n\nSo, balancing the pros and cons, the paper meets the standards after addressing the feedback. It's a solid contribution with necessary improvements made. I think it should be accepted.\n</think>\n\nThe paper proposes PEARL, a novel framework for differentially private data synthesis using generative models. The approach addresses a critical problem in privacy-preserving machine learning by offering a \"one-shot\" privacy mechanism that avoids the iterative privacy degradation seen in gradient sanitization methods. The theoretical grounding and empirical results on multiple datasets demonstrate its effectiveness compared to existing methods like DP-MERF and DP-GAN. Key strengths include clear motivation, a well-structured mathematical formulation, and the inclusion of theoretical guarantees. \n\nHowever, initial concerns were raised about benchmarking breadth (e.g., limited datasets, missing baselines like PATE-GAN and PrivSyn), lack of clarity in privacy implementation details, and insufficient complexity analysis. The authors addressed these points effectively in their rebuttal: they added new datasets (e.g., Credit), incorporated additional baselines (e.g., DPCGAN), clarified the privacy budget allocation and sensitivity calculations, provided computational complexity analysis, and expanded evaluation tasks (e.g., MMD, range queries). While minor issues like editorial inconsistencies and the absence of CIFAR-10 experiments remain, these do not fundamentally undermine the paper’s contributions. The revised experiments and clarifications sufficiently strengthen the empirical and theoretical foundations, aligning the work with top-tier standards.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about using tropical geometry and Hausdorff distances of zonotopes for neural network compression, specifically for ReLU networks with one hidden layer. The main contributions are a theoretical bound on approximation error based on Hausdorff distance and a compression algorithm using K-means. \n\nLooking at the reviews, there's a mix of positive and critical points. Reviewer 1 points out that the experiments are on small networks and old architectures like LeNet, and the abstract overstates the results by claiming competitiveness with modern methods when they only compared to ThiNet 2017, which isn't state-of-the-art. They also mention the method is limited to fully-connected layers but tested on CNNs, which might be confusing. The theoretical part is seen as solid and novel, but the empirical part is weak.\n\nReviewer 2 thinks the work is novel but the experiments are on small datasets and not aligned well with ICLR's focus. They also note the supplementary material issue but the authors fixed that. Reviewer 3 is concerned about the bounds getting worse with fewer weights, which is counterintuitive, and the lack of comparison with more recent methods like Alfarra 2020. They also mention unclear parts like \"deepNN\" and the need for better experimental validation on modern architectures. Reviewer 4 likes the novelty but wants more reproducibility details like pseudo-code and code, which the authors addressed by providing code.\n\nThe authors responded by clarifying their scope is theoretical and proof-of-concept, not aiming to beat SOTA. They adjusted the abstract to mention ReLU and revised the claims about performance. They added a new theorem to better frame their contribution. They also explained that ThiNet is a structured pruning SOTA and their method is in that category, so comparing to ThiNet is valid. They addressed some of the code and clarity issues.\n\nNow, considering top-tier conference standards. The theoretical contribution seems significant and novel, especially connecting Hausdorff distance to approximation error. That's a strong point. However, the empirical validation is limited: small datasets, old architectures, and not competing with the latest methods. The reviewers pointed out that the experiments don't scale to modern networks like ResNet or Transformers, which limits practical impact. The abstract's original claims were overreaching, but the authors revised that. The method's applicability is restricted to fully-connected layers, which are less common in modern CNNs, so the utility might be niche.\n\nThe conference values both theory and empirical rigor. While the theory is solid, the experiments are not sufficient to demonstrate practical relevance beyond proof-of-concept. However, since the authors frame it as a theoretical first step, maybe that's acceptable. But top conferences often expect stronger empirical sections, even for theory-heavy papers. The reviewers' consensus seems to lean towards acceptance because of the novel theory, but some hesitation due to empirical shortcomings. The authors addressed many concerns in their response, so maybe the paper can be accepted with the caveat that the empirical part is limited but the theory is a good contribution. \n\nFinal Decision: Accept, but with the understanding that the empirical evaluation is preliminary and the authors should clearly state limitations. However, since the reviewers' scores were mostly 3 (minor issues) and the technical novelty is high, it's borderline. Wait, looking at the scores: technical novelty was 4 from R1 and 3 from others. The empirical novelty was lower. The main issue is whether the conference would accept a paper with strong theory but weak experiments. Since the authors clarified the scope and the theory is novel, maybe it's a yes.\n</think>\n\nThe paper presents a novel theoretical framework using tropical geometry to analyze neural network approximation, specifically linking the approximation error to the Hausdorff distance between tropical zonotopes. This theoretical contribution is mathematically rigorous and introduces a fresh perspective for understanding neural network compression, which is a significant and novel advance in the field. The proposed algorithms (Zonotope K-means and Neural Path K-means) are grounded in this theory and show promise in experiments, particularly in outperforming a tropical baseline and achieving competitive results with ThiNet, a structured pruning method. \n\nHowever, the empirical evaluation has notable limitations. The experiments are conducted on small datasets (MNIST, Fashion-MNIST, CIFAR-10/100) and outdated architectures (LeNet, VGG), with no tests on modern networks like ResNet or Vision Transformers. While the authors clarify that their focus is on proof-of-concept validation and theoretical grounding, the abstract originally overstated the results by claiming competitiveness with \"modern pruning techniques,\" which was corrected in the response. The restriction to fully-connected layers further limits practical relevance, as modern CNNs rely heavily on convolutional layers. Additionally, the bounds in Propositions 4 and 5 exhibit counterintuitive behavior (worsening with fewer weights), suggesting they may be loose, though the authors acknowledge this as a first step toward tighter bounds.\n\nThe reviewers’ concerns about empirical rigor and scope are valid, but the theoretical contribution is sufficiently novel and impactful to warrant acceptance. The authors addressed key issues (e.g., clarifying the method’s scope, revising the abstract, and providing code), and the work’s potential to inspire future research in tropical geometry for neural networks aligns with the conference’s focus on foundational advances. While the empirical section is limited, it is framed appropriately as a proof-of-concept, and the theoretical framework is a meaningful step forward.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"Natural Language Descriptions of Deep Visual Features.\" The abstract describes MILAN, a method to automatically generate natural language descriptions for neurons in deep networks based on their activation patterns. The applications include analysis, auditing, and editing of models. The reviewers have given their comments, and the authors have responded to some points.\n\nLooking at the reviewers' scores, all of them gave correctness a 4 or 3, which is good. Technical and empirical novelty are mostly 3s, meaning the contributions are significant but some aspects exist in prior work. The main strengths highlighted are the method's novelty in generating compositional descriptions, the thorough testing across architectures and tasks, and the practical applications like auditing and editing models. The authors addressed several concerns in their response, like adding more examples, clarifying the dataset, and discussing limitations.\n\nNow, the key points to consider for a top-tier conference:\n\n1. **Novelty and Contribution**: The paper introduces MILAN, which seems to be a novel approach to neuron interpretation. While some parts might build on prior work (like NetDissect), the combination of mutual information optimization and natural language generation for open-ended descriptions is a step forward. The applications (auditing, editing) add practical value, which is a plus.\n\n2. **Technical Soundness**: The method is well-explained, and the experiments are extensive. The reviewers noted that the claims are well-supported. The authors addressed concerns about the dataset (MILANNOTATIONS) by adding more details and examples. They also compared against image captioning baselines, showing MILAN's superiority.\n\n3. **Empirical Results**: The results show higher agreement with human annotations compared to baselines, and the applications (like removing text-sensitive neurons) demonstrate real-world utility. The experiments across different models and tasks (classification, generation, unsupervised learning) strengthen the case for generalizability.\n\n4. **Weaknesses and Limitations**: Reviewers pointed out some issues. The inter-annotator agreement in the dataset is lower than the model's performance, but the authors explained this as different annotators focusing on different aspects. The dataset's potential biases are acknowledged, and the authors plan to address this in future work. The method's scope is limited to visual features, which the authors clarified by adjusting the title. The failure modes are discussed, and the authors added more examples in the appendix.\n\n5. **Author Responses**: The authors addressed most concerns effectively. They added figures, clarified methodology, and provided empirical comparisons with captioning models. They also acknowledged limitations and future work, showing a proactive approach to improving the method.\n\n6. **Potential Impact**: The ability to describe neurons with natural language could significantly aid in model interpretability, especially for auditing and editing. This is a hot topic in AI ethics and model understanding, so the paper's contributions are timely and impactful.\n\nConsidering all this, the paper seems to meet the standards of a top-tier conference. The method is novel, technically sound, and has clear applications. The authors have addressed most reviewer concerns, and the remaining limitations are acknowledged. While there's room for improvement (like scaling to other modalities or addressing bias), the current work is solid and presents a meaningful advancement. The applications demonstrated are compelling, and the thorough testing across multiple scenarios adds credibility.\n\nI don't see any major flaws that would warrant rejection. The minor issues (like typos, some dataset limitations) are addressed or noted, and the overall contribution is strong enough for acceptance.\n</think>\n\nThe paper introduces MILAN, a novel method for generating natural language descriptions of neurons in deep visual models by maximizing mutual information between descriptions and neuron-activated image regions. The approach is technically sound, with clear motivation and thorough empirical validation across multiple architectures, datasets, and tasks. Key strengths include:\n\n1. **Technical Innovation**: MILAN represents a meaningful advancement in neuron interpretation by enabling open-ended, compositional descriptions that capture categorical, relational, and logical structures. While building on prior work (e.g., NetDissect), the integration of mutual information optimization and language generation for fine-grained descriptions is novel and impactful.\n\n2. **Empirical Rigor**: The experiments demonstrate strong performance compared to baselines, with high agreement to human annotations and successful applications in auditing (e.g., detecting face-sensitive neurons in anonymized datasets) and editing (improving robustness by removing spurious text features). Results generalize across diverse models and tasks, supporting the method's utility.\n\n3. **Practical Applications**: The three showcased applications (analysis, auditing, editing) highlight MILAN's potential to address critical challenges in model interpretability and controllability. The ability to surface and manipulate neurons based on linguistic descriptions is particularly compelling for ethical AI and bias mitigation.\n\n4. **Clarity and Reproducibility**: The paper is well-written, with sufficient technical details and open-source code/data. The authors addressed reviewer concerns about dataset quality, failure modes, and methodology through clarifications and supplementary material.\n\n**Limitations and Concerns**: \n- The inter-annotator agreement in the MILANNOTATIONS dataset is lower than model performance, but the authors explain this as reflecting differing human perspectives rather than model flaws.\n- The method is currently limited to visual models, though the authors clarify this scope and suggest future extensions.\n- Some failure modes (e.g., vague or incorrect descriptions) are acknowledged, with plans to explore improvements in future work.\n\n**Author Responses**: The authors effectively addressed reviewer critiques, adding dataset statistics, failure examples, and comparisons to image captioning baselines. They also clarified methodology and limitations, demonstrating a commitment to transparency and improvement.\n\n**Conclusion**: The paper meets the standards of a top-tier conference. Its technical contributions, empirical robustness, and practical applications outweigh minor limitations. The method advances the field of model interpretability and provides actionable tools for improving model behavior, aligning with current priorities in AI research.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is titled \"Variational Neural Cellular Automata\" and proposes a generative model inspired by biological cellular processes. The key points from the abstract are that it's a probabilistic model, uses local communication, and shows some robustness to perturbations. The experiments show it can generate diverse outputs but lags behind state-of-the-art in performance.\n\nLooking at the reviewer comments, there's a mix of opinions. Reviewer 1 and 4 are leaning towards rejection due to underwhelming results, especially on datasets like CelebA where likelihoods are much lower than SOTA. They also mention the need to average samples to get reasonable results, which is a downside. However, Reviewer 3 and 5 are in favor of acceptance, with 5 raising their score after the rebuttal. Reviewer 2 points out that some ideas might not be novel, like the connection to resnets with constant weights, but sees value in bridging that gap. \n\nThe authors addressed several points in their response. They added experiments on damage recovery, latent space analysis, and clarified architectural choices. They also acknowledged the lack of SOTA performance but emphasized the model's unique properties like self-organization and robustness. \n\nTechnical novelty is a concern. Reviewers 2, 6, and 7 note that parts of the work might not be novel, especially the connection to existing architectures like resnets. However, the authors argue that their main contribution is showing NCAs can model data distributions through morphogenesis, which they claim is novel. The rebuttal adds experiments that highlight the VNCA's ability to recover from damage, which a standard VAE can't do as effectively. \n\nEmpirical results are mixed. While MNIST results are good, performance on more complex datasets is lacking. The authors admit this but stress the model's unique traits. The robustness to perturbations and the self-organizing aspect are seen as contributions, even if not SOTA in generation quality. \n\nThe writing and reproducibility are praised by some reviewers. The paper is well-written and honest about its shortcomings, which is a plus. The authors also committed to releasing code, which is good for reproducibility.\n\nConsidering top-tier standards, the key factors are novelty, significance, and empirical validation. The VNCA introduces a new approach with interesting properties (self-organization, robustness) that aren't present in standard models. Even if it's not the best in likelihoods, the unique capabilities might justify its acceptance, especially if it opens new research directions. The rebuttal strengthened the paper by addressing some criticisms and adding experiments, which might tip the balance.\n\nHowever, some reviewers argue the technical novelty is marginal. But since the paper is the first to apply NCAs in a proper generative model framework with a VAE, that could be considered a meaningful step. The robustness experiments post-rebuttal add value, showing capabilities beyond standard models. \n\nIn conclusion, despite the performance gaps, the paper's novel approach and unique contributions to generative models through NCAs, along with the thorough rebuttal addressing concerns, make it a borderline case. But since top conferences do accept papers with incremental contributions if they open new avenues, I think it should be accepted.\n</think>\n\nThe paper proposes the Variational Neural Cellular Automata (VNCA), a generative model inspired by biological cellular processes. While its performance lags behind state-of-the-art (SOTA) methods in likelihood and sample quality, it introduces a novel framework that combines neural cellular automata (NCA) with variational inference. Key considerations include:\n\n### Strengths:\n1. **Novelty and Conceptual Contribution**: \n   - The VNCA is the first proper probabilistic generative model based on NCAs, bridging the gap between self-organizing systems and generative modeling. While some architectural elements (e.g., weight-sharing residual blocks) have precedents, the integration of NCAs into a VAE framework and the focus on morphogenesis and robustness to perturbations are novel.\n   - The rebuttal clarifies that the VNCA learns a single function iteratively (unlike standard deep networks), enabling unique properties like damage recovery and self-organization, which are demonstrated in new experiments (e.g., Section 3.3).\n\n2. **Robustness and Unique Properties**:\n   - The VNCA exhibits inherent robustness to early-stage perturbations, a feature not typically prioritized in SOTA generative models. The added damage-recovery experiments show its ability to stabilize around data distributions, even when trained without explicit damage exposure. This capability distinguishes it from standard VAEs and highlights its potential for modeling systems with emergent properties.\n\n3. **Clarity and Reproducibility**:\n   - The paper is well-written, transparent about limitations, and includes thorough experiments. The authors addressed reviewer concerns by adding latent space analysis, clarifying architectural choices, and releasing code, enhancing reproducibility.\n\n4. **Broader Impact**:\n   - The work contributes to an under-explored area (NCAs in generative modeling) and opens avenues for future research, such as local training of NCAs or applications in robust generative systems.\n\n### Weaknesses:\n1. **Empirical Performance**:\n   - The VNCA underperforms SOTA in likelihood and sample quality on complex datasets like CelebA. While it reconstructs well on MNIST, its utility for high-dimensional data is limited. However, the authors acknowledge these shortcomings and frame the work as a foundational step rather than a competitive model.\n\n2. **Technical Overlap**:\n   - Reviewers noted similarities to existing architectures (e.g., weight-shared residual networks, neural ODEs). However, the VNCA’s focus on local communication, iterative morphogenesis, and emergent behavior differentiates it from prior work, even if some components are not entirely novel.\n\n3. **Motivation and Scope**:\n   - The paper’s motivation could be clearer. While the VNCA introduces architectural constraints (local communication, iterative growth), the practical benefits beyond theoretical curiosity are underdeveloped. However, the damage-recovery experiments and latent space analysis in the rebuttal partially address this by highlighting unique capabilities.\n\n### Rebuttal Impact:\nThe authors significantly strengthened the paper by:\n- Adding experiments on damage recovery and stability (Section 3.3), demonstrating the VNCA’s ability to learn stable attractors and outperform standard VAEs in this context.\n- Clarifying architectural choices and hyperparameter selection.\n- Addressing concerns about novelty by emphasizing the VNCA’s iterative function application and local communication, distinct from standard deep networks.\n\n### Conclusion:\nWhile the VNCA does not match SOTA in generative performance, its novel integration of NCAs into a probabilistic framework, unique robustness properties, and potential for exploring emergent behavior in generative models justify its acceptance. The work is a valuable contribution to the growing interest in self-organizing systems and provides a reproducible baseline for future research. The rebuttal effectively addressed key criticisms, particularly by highlighting the VNCA’s distinct capabilities and adding meaningful experiments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving online knowledge distillation by creating diverse teacher ensembles through label prior shifts and importance sampling. The reviewers have mixed opinions. \n\nStarting with the strengths: The authors propose a method to induce diversity among teachers using label prior shifts, which they handle via importance sampling. They also introduce a post-compensation softmax and aggregation strategy. The experiments show improvements in error rates and calibration, which are important metrics. Reviewer 3 and parts of others acknowledge the thorough empirical analysis and clear writing. The authors addressed some concerns in their response, like adding comparisons with more recent methods and fixing typos.\n\nNow the weaknesses. Reviewers 1, 2, 4, and 5 point out issues. The main criticisms are around novelty. The technical approach combines existing techniques (label prior shifts, importance sampling, PC-Softmax) without a strong claim of innovation. Reviewers question if the method is just a combination of prior work. The empirical results, while showing improvements, are marginal against some baselines (like PCL on ImageNet only 0.14% better). Also, some comparisons were done post-submission, which might not be as rigorous as peer-reviewed results. Reviewer 2 mentions inconsistencies in reproducing MVCL's results, which is a red flag. The novelty is considered \"marginal\" by most reviewers, which is a big issue for a top-tier conference. \n\nThe authors' responses tried to address these by adding more experiments and clarifying the setup. They argue their method is novel in applying these techniques to online KD, but the reviewers still see it as incremental. The fact that the main contribution is combining existing ideas without significant theoretical advancement might not meet the high bar for top conferences. Also, the minor improvements over strong baselines might not be impactful enough. \n\nConsidering the conference standards, top-tier venues usually require substantial novelty and significant improvements. Here, the method's incremental nature and marginal gains, along with reproducibility concerns, might not suffice. Even though the experiments are thorough, the lack of a groundbreaking idea and the reviewers' consensus on low novelty tip the scale towards rejection. \n\nFinal Decision: Reject\n</think>\n\nThe paper proposes an approach to enhance online knowledge distillation by creating diverse teacher ensembles through label prior shifts and importance sampling, combined with a post-compensation softmax aggregation strategy. While the method shows improvements in error rates and calibration metrics, several critical issues undermine its acceptance at a top-tier conference:\n\n1. **Novelty Concerns**:  \n   - Reviewers consistently highlight that the contributions are incremental, combining existing techniques (e.g., label prior shifts, importance sampling, PC-Softmax) without a clear, novel theoretical advancement. The authors frame their work as novel in the context of online KD, but reviewers argue that the core ideas lack sufficient innovation compared to prior art.  \n   - The technical novelty is deemed \"marginal\" by multiple reviewers, which is a significant barrier for top-tier venues requiring high innovation thresholds.\n\n2. **Empirical Marginal Gains**:  \n   - The improvements over strong baselines (e.g., PCL, L-MCL, MVCL) are small. For instance, on ImageNet, the top-1 error improvement over PCL is only 0.14%, and on CIFAR-10/100, the gains are modest compared to existing methods.  \n   - Some comparisons rely on post-submission experiments or third-party implementations, raising concerns about reproducibility and fairness. Reviewer 2 explicitly questions the validity of the MVCL results, noting inconsistencies with the original work.\n\n3. **Scope Limitations**:  \n   - The method is specialized to online KD with peer networks, and while the authors suggest extensions to offline settings, these are not empirically validated. The narrow applicability limits broader impact.  \n   - The aggregation strategy (model averaging) is criticized as naive, and the authors’ response does not fully address why weighted averaging or alternative strategies were not explored.\n\n4. **Reviewers’ Consensus**:  \n   - Most reviewers (1, 2, 4, 5) rate technical and empirical significance as \"marginal,\" with Reviewer 2 and 5 explicitly advocating rejection. Only Reviewer 3 provides a cautiously positive assessment but acknowledges the lack of novelty.  \n   - The authors’ responses address some concerns (e.g., typos, additional experiments), but they fail to resolve core criticisms about novelty and significance.\n\n**Conclusion**: While the paper is well-written and the experiments are thorough, the incremental nature of the contributions, marginal empirical gains, and unresolved reproducibility concerns fall short of the high bar for acceptance at a top-tier conference. The method does not demonstrate sufficient novelty or transformative impact to justify publication in this venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about combining Transformers with MCTS for offline RL, specifically on SameGame. The reviewers have some mixed but mostly critical feedback. Let me go through each point step by step.\n\nFirst, the title and abstract sound promising. They mention a novel combination of two established methods, which could be interesting. But the reviewers point out several issues. \n\nLooking at Reviewer 1's comments: The main strengths are the idea of addressing exploration in Transformers and the novelty of combining them with MCTS. But the weaknesses are significant. The writing is unclear, which is a problem because even if the idea is good, the paper needs to communicate it well. The experiments are limited to SameGame, which is considered a toy problem, and they didn't compare against relevant baselines like Decision Transformers or AlphaZero. That's a big issue because without proper benchmarks, it's hard to see the real impact. Also, the MCTS baseline might not be fair since it doesn't use learned policies like in AlphaGo, making the comparison less meaningful. \n\nReviewer 2 is more critical about novelty. They argue that the approach is just replacing a ResNet with a Transformer in an AlphaZero-like setup, which isn't novel. The experiments are unfair because they don't control for computation time and don't compare against other MCTS+NN methods. The minor issues like typos and lack of code also hurt reproducibility. \n\nReviewer 3 echoes similar points: limited novelty, experiments only on SameGame without standard benchmarks, and insufficient comparisons. They also mention that the method is a straightforward combination without showing why Transformer is better than other architectures. The lack of standardized test sets and proper baselines weakens the claims.\n\nThe authors didn't respond, which might mean they couldn't address these concerns. \n\nNow, considering top-tier conference standards, the paper needs strong empirical validation and clear presentation. The key issues here are the lack of proper baselines (like AlphaZero, Decision Transformers), the limited domain (SameGame isn't as impactful as Atari or Go), and unclear writing. The novelty is marginal since combining MCTS with neural nets isn't new, and using a Transformer here doesn't show a significant leap. The experiments don't convincingly demonstrate superiority over existing methods in more standard environments. \n\nEven if the idea has potential, the current submission doesn't meet the bar for a top conference. The reviewers' consensus is that it's not ready for acceptance, though Reviewer 1 thinks it could be improved. However, given the current state, the flaws in experiments and presentation are too critical. \n\nSo, the decision should be to reject unless major revisions are made, but since the authors can't respond here, and the conference requires the paper to be ready as is, rejection is appropriate.\n</think>\n\nThe paper proposes MCTransformer, a framework combining Transformers with MCTS for offline reinforcement learning, evaluated on SameGame. While the idea of integrating these methods is novel and has potential, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty**: The combination of Transformers and MCTS in an actor-critic setup is a novel contribution, with potential for future work.\n2. **Empirical Results**: The paper shows improved performance over standalone Transformer and MCTS baselines on SameGame.\n\n### Major Weaknesses:\n1. **Lack of Empirical Rigor**:\n   - **Limited Domain**: Experiments are confined to SameGame, a relatively simple and niche environment. Top-tier conferences require evaluation on standard benchmarks (e.g., Atari, MiniGo, or MuJoCo) to demonstrate broader impact.\n   - **Incomplete Baselines**: The paper fails to compare against critical baselines like Decision Transformers, AlphaZero, or MuZero, which use neural networks in MCTS. This makes it impossible to assess the method’s superiority over existing state-of-the-art approaches.\n   - **Unfair Comparisons**: The MCTS baseline does not use learned policies or value functions (as in AlphaZero), rendering comparisons misleading. Additionally, the evaluation does not control for computational resources (e.g., simulation counts vs. time limits).\n\n2. **Clarity and Presentation**:\n   - The paper’s exposition is unclear, with poorly explained figures (e.g., small fonts, undefined terms like \\(rtg_\\pi(s_t)\\)) and ambiguous descriptions of the MCTS setup.\n   - The term \"Transformer\" is overused ambiguously, conflating the architecture with specific RL frameworks like Decision Transformers.\n\n3. **Novelty Concerns**:\n   - Reviewers argue that the core idea (combining MCTS with neural networks) is not novel. The work resembles AlphaZero/AlphaGo with a Transformer replacing the ResNet, offering minimal technical innovation.\n   - The paper does not justify why a Transformer is better suited for this task compared to other architectures (e.g., CNNs or MLPs).\n\n4. **Reproducibility**:\n   - The authors do not provide source code, and the experimental setup lacks details (e.g., how the Transformer is trained, rollout policy specifics, and hyperparameters). While the method is conceptually similar to AlphaZero, the lack of code and precise descriptions hinders reproducibility.\n\n### Conclusion:\nWhile the paper introduces an interesting idea, its execution falls short of top-tier standards. The limited experimental scope, incomplete baselines, and unclear presentation prevent a convincing demonstration of the method’s significance. The lack of novelty relative to existing MCTS+NN frameworks (e.g., AlphaZero) further weakens its contribution. These issues, combined with the absence of an author response to address reviewer concerns, indicate the paper is not yet ready for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through the paper and the reviews to figure out whether it should be accepted. The paper is about Behavior Proximal Policy Optimization (BPPO) for offline RL. The main idea is that online on-policy algorithms like PPO can naturally handle offline RL because their inherent conservatism prevents overestimation of out-of-distribution actions. They modify PPO to use importance sampling ratios based on the current policy and introduce a clipping decay. The experiments show it outperforms state-of-the-art methods on D4RL.\n\nLooking at the reviews, there are both strengths and weaknesses highlighted. Reviewer 1 points out some theoretical contributions but notes issues with clarity, lack of experiments addressing overestimation, and concerns about the soundness of the method. They also question if BPPO is truly an online method or just an off-policy variant. Reviewer 2 mentions that parts of the theory aren't novel, referencing prior work like GePPO, and is concerned about the advantage approximation's soundness. They also note missing ablations and comparisons with more recent baselines. Reviewer 3 is more positive but asks about advantage estimation accuracy and algorithm steps. Reviewer 4 is critical of the empirical results, saying improvements are minor and the method isn't significantly better than existing baselines like TD3+BC. Reviewer 5 also questions the assumptions and empirical significance, especially on sparse reward tasks.\n\nThe authors addressed some points in their response. They revised Assumption 1 to use total variational divergence, added experiments on Antmaze, and provided more ablations. They argue that BPPO's performance is better on harder tasks like Adroit and Antmaze, and that it's simpler than CQL. However, some concerns remain, like the theoretical soundness of the advantage replacement and whether the improvements are substantial enough.\n\nNow, considering the criteria for a top-tier conference: the paper needs to have novel contributions, solid theoretical backing, and strong empirical results. The theoretical part has some overlap with existing work (GePPO), which reviewers pointed out. The authors addressed this by modifying assumptions but didn't fully resolve the novelty concern. The empirical results show improvements, especially on certain tasks, but some reviewers argue these are marginal compared to existing methods. The method's simplicity is a plus, but the lack of clear superiority over simpler baselines like TD3+BC is a downside.\n\nThe clarity issues in the original paper were addressed in the response, which helps. However, the theoretical soundness of the advantage approximation remains a point of contention. The authors acknowledge it loosens the bound but argue practical benefits. The experiments added in the rebuttal, like on Antmaze, do show better performance, which strengthens their case.\n\nBalancing all factors: The paper has a novel angle in applying on-policy methods to offline RL, with some theoretical grounding. The empirical results, especially on harder tasks, are compelling. However, the overlap with prior work and the minor improvements over some baselines might not meet the high bar of a top conference. The authors' responses addressed some concerns but didn't fully resolve the theoretical soundness or novelty issues. The experiments, while improved, might not be sufficient to claim a major breakthrough.\n\nHmm, but the authors did show significant gains on Adroit and Antmaze, which are important benchmarks. The simplicity of BPPO compared to complex methods like CQL is a plus. Maybe the combination of a fresh theoretical perspective and strong empirical results on key tasks tips it towards acceptance, especially if the novelty is deemed sufficient after revisions.\n</think>\n\nThe paper proposes Behavior Proximal Policy Optimization (BPPO), an offline RL method leveraging on-policy algorithms' inherent conservatism to address overestimation. While the theoretical framework extends prior work (e.g., GePPO) and faces novelty concerns, the empirical results show notable performance gains on challenging tasks like Adroit and Antmaze. Key points include:\n\n1. **Theoretical Contributions**: The analysis of offline monotonic policy improvement is novel but overlaps with existing off-policy methods. The authors revised Assumption 1 and addressed reviewer concerns, though theoretical soundness of the advantage replacement remains debated.\n2. **Empirical Results**: BPPO outperforms SOTA on complex benchmarks (e.g., +61.7% on Adroit, +24.6% on Antmaze), demonstrating practical utility. Added experiments in the rebuttal strengthen this case.\n3. **Simplicity**: BPPO’s design is simpler than complex alternatives like CQL, aligning with the \"simplicity + effectiveness\" criterion for impactful methods.\n4. **Weaknesses**: Overlap with prior work, minor improvements over some baselines (e.g., on Gym), and unresolved concerns about theoretical guarantees limit its significance.\n\n**Final Decision: Accept**  \nThe combination of a fresh theoretical angle, strong empirical results on critical benchmarks, and methodological simplicity meets the bar for a top-tier conference, despite identified limitations. The authors’ revisions and clarifications sufficiently address key concerns, particularly on empirical robustness and assumptions."
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new contrastive learning objective to reduce text degeneration in language models. The reviewers have mixed but mostly critical feedback. Let me go through each point step by step.\n\nFirst, the title and abstract suggest that the method improves on unlikelihood training (UL) by addressing its limitations. But Reviewer 1 points out that the core idea of penalizing previous tokens isn't new—it's similar to UL. The authors even misrepresented UL's equations, which is a problem. That's a big hit on novelty. Reviewer 3 also mentions lack of novelty, saying the idea is very similar to UL. So the main contribution might not be novel enough for a top conference.\n\nNext, the experiments. Reviewer 1 and 2 both criticize the experiments as insufficient. The paper uses GPT2-small and wikitext-103, which might not be strong enough baselines. Reviewer 2 notes that larger models (like bigger GPT2 versions) aren't tested, and the metrics used (like perplexity) might not be reliable indicators. They also mention that the paper doesn't test on tasks like summarization or machine translation, even though the authors claim it could help there. That's a gap. Reviewer 4 adds that human evaluations didn't show statistical significance over UL-TS, which weakens the claims.\n\nTechnical novelty is another issue. Reviewer 1 says the method is incremental, just building on UL. The contrastive approach isn't entirely new, and the key difference (using M previous tokens vs all in UL) isn't sufficiently justified. Reviewer 4 suggests experiments comparing using all tokens vs M would help, but they weren't done. Without clear evidence that this difference matters, the method's advantage isn't proven.\n\nClarity is good across the board, which is a plus, but not enough to overcome other issues. Reproducibility is okay since code is provided, but that doesn't address the lack of novelty or experiments.\n\nThe main strengths are the intuitive idea and some positive results, but the reviewers agree the experiments are limited and the method isn't novel enough. Top-tier conferences require significant contributions and thorough validation. Since the paper's improvements over UL aren't clearly demonstrated and the experiments don't cover important scenarios, it might not meet the bar.\n\nHmm, but the authors didn't respond, so we can't see if they addressed these points. Given the reviewers' consensus on low novelty and insufficient experiments, especially from multiple reviewers, I think the paper should be rejected. It's incremental without enough evidence of superiority in critical areas.\n</think>\n\nThe paper proposes a contrastive token (CT) learning objective to address text degeneration in autoregressive language models by penalizing previously generated tokens. While the idea is intuitive and the paper is well-written, several critical issues undermine its suitability for a top-tier conference:\n\n### **Novelty Concerns**\n- **Lack of Innovation**: Reviewers 1, 2, and 3 highlight that the core idea of penalizing repetitive tokens is not novel. The method is framed as an incremental improvement over unlikelihood training (UL), but the authors misrepresented UL’s formulation (e.g., omitting UL’s joint consideration of positive and negative tokens in Equation 4). This misrepresentation weakens the paper’s claims of novelty and fairness to prior work.\n- **Marginal Technical Contribution**: The distinction between CT and UL (e.g., using M previous tokens vs. all tokens in UL) is not rigorously justified. Reviewer 4 notes that experiments comparing CT with UL using equivalent negative token sets (e.g., all vs. M tokens) are missing, leaving the significance of this difference unproven.\n\n### **Experimental Weaknesses**\n- **Limited Baselines and Tasks**: The experiments focus on small models (e.g., GPT2-small) and limited tasks (wikitext-103, dialogue generation). Reviewers 1 and 2 argue that larger models (e.g., GPT2-large) and critical downstream tasks like machine translation or summarization—explicitly mentioned in the abstract—are not tested, undermining claims of broad applicability.\n- **Questionable Metrics**: The reliance on perplexity and simple diversity metrics (dist-1, uniq-1) is criticized. Reviewer 2 notes that low perplexity in LMs like GPT2 can indicate degenerate behavior, and prior work uses higher-order n-gram diversity metrics (dist-2/3/4). The absence of these metrics and task-specific evaluations (e.g., ROUGE for summarization) weakens the empirical validation.\n- **Human Evaluation Flaws**: Reviewer 4 points out that human evaluations (Table 3) lack statistical significance compared to UL-TS, conflicting with claims of improved fluency and reduced repetition.\n\n### **Technical and Empirical Significance**\n- **Unclear Practical Impact**: While the method is simple to implement, its benefits over UL or cross-entropy are not convincingly demonstrated. Reviewer 4 notes that repetition is less of an issue in modern, larger models, and the paper provides no evidence that CT improves performance in such settings.\n- **Failure to Address Key Criticisms**: The authors did not respond to reviewers’ concerns, leaving unresolved issues like the misrepresentation of UL, insufficient experiments, and lack of statistical rigor.\n\n### **Conclusion**\nThe paper’s contributions are incremental, its experiments are underwhelming for a top-tier venue, and its novelty is overstated. While the idea is intuitive and the implementation is reproducible, the lack of rigorous validation, novel technical insights, and broader empirical evaluation fail to meet the high standards of a conference like ICLR. The reviewers’ consensus on thin novelty and insufficient experiments strongly supports rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about the order of Batch Normalization (BN) and activation functions, specifically when using bounded activations like Tanh. The main claim is that swapping BN to after the activation (Swap model) gives better performance compared to the conventional order (BN before activation) when using bounded activations. They attribute this to asymmetric saturation and increased sparsity.\n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 points out that the paper needs more figures for VGG16_11's skewness and clarifies some questions about Sigmoid. Reviewer 2 mentions weaknesses like unclear connections between asymmetric saturation and generalization, issues with sparsity metrics, and limited applicability since they exclude residual networks. They also note that the Swap model with Tanh doesn't beat ReLU. Reviewer 3 thinks it's borderline, highlighting incomplete metrics and practical limitations. Reviewer 4 wants more justification and analysis on training dynamics and architecture effects. Reviewer 5 has concerns about rigor in claims, experimental setup, and reproducibility.\n\nThe authors addressed some points in their response. They clarified the center of the function, provided code details, and adjusted the model for CIFAR-100 by modifying VGG16. They also added experiments on ResNet and discussed sparsity vs. saturation. However, some issues remain, like the Swap model not surpassing ReLU and the limited scope excluding residual connections.\n\nThe paper's main strengths are the novel observation about the Swap order's performance with bounded activations and the analysis of saturation and sparsity. However, the weaknesses include insufficient theoretical grounding, limited practical impact (since ReLU still outperforms), and unclear connections between their metrics and performance. The reviewers' scores are mostly in the middle range (correctness 2-3, novelty 2-3), suggesting the work is interesting but not groundbreaking. The authors made some improvements in the rebuttal, but key concerns about generalizability and rigor remain.\n\nConsidering top-tier conference standards, the paper needs stronger evidence linking their findings to generalization, better handling of residual networks, and clearer explanations. Since the Swap model doesn't beat ReLU and the analysis is limited, the contributions might be marginal. The reviewers' mixed but mostly cautious feedback, especially the borderline recommendation from Reviewer 3, suggests it's a tough call. However, the lack of significant practical benefit and unresolved methodological issues might tip the decision towards rejection.\n</think>\n\nThe paper presents an interesting empirical observation that swapping the order of batch normalization (BN) and bounded activation functions (e.g., Tanh) improves performance, attributed to asymmetric saturation and sparsity. However, several critical issues undermine its acceptance at a top-tier conference:\n\n1. **Lack of Practical Impact**: The Swap model with bounded activations (e.g., Tanh) still underperforms the conventional ReLU-based models, limiting its real-world utility. The authors do not convincingly argue why this approach is preferable in practice.\n\n2. **Incomplete Analysis and Rigor**: \n   - The connection between asymmetric saturation and generalization is weakly supported. Reviewers highlight inconsistencies in layer-wise saturation/sparsity trends (e.g., deeper layers in VGG16) and lack of theoretical grounding for why asymmetric saturation improves performance.\n   - The exclusion of residual networks (due to design constraints with Tanh) limits the work’s relevance to modern architectures. While the authors tested ResNet20, the results are marginal and not emphasized.\n   - The saturation metric is critiqued as incomplete, and the sparsity metric’s relationship to performance is not clearly established.\n\n3. **Methodological Limitations**: \n   - The experiments focus on VGG-like architectures and small datasets (e.g., CIFAR-100), raising questions about scalability and generalizability to larger models or tasks.\n   - The revised VGG16_11 model (trimmed for CIFAR-100) was not adequately discussed in the original submission, indicating rushed adjustments.\n\n4. **Clarity and Reproducibility**: \n   - Key terms like \"center of the function\" and acronyms (e.g., NWDBN) were ambiguously defined initially.\n   - While the authors clarified code details in the rebuttal, the paper’s original presentation of metrics (saturation, skewness, sparsity) was unclear.\n\n5. **Novelty and Significance**: \n   - The core observation (Swap order benefits bounded activations) is novel but incremental. The analysis of saturation and sparsity adds limited new insights beyond prior work on BN and activation functions.\n   - The authors do not address why ReLU’s dominance persists despite their findings, weakening the paper’s broader impact.\n\n**Strengths** include the clear empirical setup, the identification of asymmetric saturation as a phenomenon, and the potential niche utility of the Swap order for bounded activations. However, the unresolved methodological flaws, lack of practical benefit, and insufficient theoretical grounding prevent the work from meeting the high bar of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's start by going through all the information provided. The paper is about the convergence analysis of gradient descent (GD) for deep linear neural networks, focusing on sharp rates and the effect of initialization and network depth. The title mentions \"sharp convergence analysis,\" and the abstract highlights that the convergence rate doesn't depend on initialization type and that depth doesn't affect the optimal rate when the network is sufficiently wide.\n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Reviewers 2, 4, and part of 3 seem to support acceptance, while others like Reviewer 1 and 5 have concerns about novelty. Reviewer 6 initially wanted to reject but is open to accepting if issues are addressed. The authors have provided a detailed response addressing several points, like adding a new section for insights, clarifying overclaims in the title and abstract, and discussing the necessity of overparameterization.\n\nFirst, the main strengths: The paper extends previous work on linear networks by considering general loss functions beyond the squared loss, different initializations, and varying layer widths. The key contribution in Theorem 3.3 about the trajectory similarity to convex optimization seems novel, as noted by Reviewer 3. The analysis showing depth independence under sufficient width is also a solid contribution. The authors addressed concerns about overclaims by revising the title and abstract, which is good.\n\nNow, the concerns. Reviewer 1 and 3 question the novelty of extending the loss function and initialization. The authors argue that handling low-rank X and using semi-norms is non-trivial, and the trajectory analysis is new. Reviewer 5 thinks the extension to general losses is routine, but the trajectory part might still be novel. The technical challenges in handling different widths and initializations are addressed in the author response, which might alleviate some novelty concerns.\n\nReviewer 6's points about lack of discussion on overparameterization's role and the necessity of width were addressed by adding Section 4, which explains why overparameterization avoids bad saddles and why width matters. The authors also clarified dependencies of constants on hyperparameters, which helps with reproducibility.\n\nThe correctness scores are mostly high (4), except Reviewer 6 gave a 3, but they mentioned minor issues. The authors fixed some typos and figure font sizes, which is good. The technical novelty is rated 3 by most reviewers (significant but some prior work exists), except Reviewer 5 who gave 2. However, the trajectory analysis and insights into overparameterization's role seem to tip it towards acceptable novelty for a top conference.\n\nConsidering that top-tier conferences value both solid technical contributions and novel insights, the paper's extension to general losses, new trajectory results, and deeper understanding of overparameterization's role meet these criteria. The authors have adequately addressed most concerns through revisions. While some parts build on prior work, the added insights and extensions justify acceptance. Reviewer 6's initial rejection was due to lack of discussion, which the authors fixed. The majority of reviewers (after revisions) lean towards acceptance, so I think it should be accepted.\n</think>\n\nThe paper under review presents a convergence analysis of gradient descent (GD) for deep linear neural networks, focusing on sharp convergence rates and the impact of initialization and network depth. The key contributions include extending prior work to general strongly convex and smooth loss functions, analyzing multiple initialization schemes (Gaussian, orthogonal, balanced), and demonstrating that convergence rates are independent of initialization and depth when the network is sufficiently wide. The authors also show that the trajectory of GD for deep linear networks closely follows that of the convex counterpart, a novel insight highlighted in Theorem 3.3.\n\n### Strengths:\n1. **Novel Contributions**: The trajectory analysis (Theorem 3.3) is novel and provides valuable insights into how GD behaves in non-convex settings, aligning with convex optimization dynamics. This addresses a gap in prior work, which focused more on convergence rates than trajectory behavior.\n2. **Technical Depth**: The extension to general loss functions (beyond squared loss) and varying layer widths requires handling low-rank data matrices and semi-norms, which the authors argue introduces non-trivial technical challenges. The use of concentration inequalities for beta distributions in analyzing new initialization schemes also adds technical merit.\n3. **Addressing Reviewer Concerns**: The authors revised the title and abstract to avoid overclaims, added Section 4 to clarify insights (e.g., overparameterization avoiding bad saddles, width requirements), and addressed typos and figure issues. These revisions directly tackle concerns raised by Reviewers 1, 3, 4, and 6.\n4. **Significance**: The results align with empirical observations that deep networks trained with GD perform well despite non-convexity. The depth-independence result under overparameterization is a meaningful theoretical contribution.\n\n### Weaknesses and Concerns:\n1. **Novelty Debate**: Some reviewers (e.g., Reviewers 1, 5) questioned the novelty of extending prior work (e.g., Du & Hu 2019) to general losses and initialization schemes. However, the authors persuasively argue that handling low-rank data and the trajectory analysis are non-trivial extensions. While some techniques build on prior work, the combination and novel insights justify the contribution for a top-tier conference.\n2. **Local vs. Global Analysis**: Reviewer 4 noted the analysis assumes an initial loss not too far from the optimum. The authors clarified that this is not a locality constraint but a requirement on the minimum width, which is standard in overparameterization analyses.\n3. **Empirical Gaps**: The paper is purely theoretical. While this is acceptable for a top conference, some reviewers (e.g., Reviewer 6) noted a lack of intuitive explanations. The added Section 4 partially mitigates this by providing high-level insights.\n\n### Final Assessment:\nThe paper makes solid contributions to the theoretical understanding of GD in deep linear networks, particularly in extending prior results to broader loss functions and initialization schemes, and introducing novel trajectory analysis. The authors effectively addressed reviewer concerns through revisions, clarifying technical challenges and improving exposition. While some aspects build on existing work, the depth of analysis and novel insights (e.g., trajectory alignment with convex optimization, overparameterization's role) meet the standards of a top-tier conference. The majority of reviewers (after revisions) support acceptance, and the remaining concerns are adequately addressed.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Detecting Out-of-Distribution Data with Semi-supervised Graph “Feature” Networks\". The abstract mentions using graph structures and topological properties to detect OOD data, achieving high AUROC scores on LSUN. The keywords are missing, which might be an oversight but not a major issue.\n\nLooking at the reviewer comments, there are four reviewers. Let me go through each one's points.\n\nReviewer 1 points out several weaknesses. The paper uses vague terms like \"common sense\" without proper definition. The connection between the summary and the actual implementation is unclear. There are grammatical errors and the feature extraction process isn't detailed enough. The experiments compare to SoTA by citing another work but don't reproduce those methods for a fair comparison. Also, the impact of object detection accuracy on the model isn't analyzed. The reviewer suggests more detailed examples, better experiments, and clearer descriptions. They give a correctness score of 3 (minor issues) and technical novelty 3 (significant but some prior work exists), empirical novelty 2 (marginal).\n\nReviewer 2 is harsher. They say the paper is not well-prepared, with confused notation (Near/Far OOD differing between intro and Table 1), unclear motivation, and no literature review on OOD detection. The experiments are not convincing because they don't compare to state-of-the-art methods, only graph embedding techniques. Only LSUN dataset used. They recommend a strong reject. Correctness is 1 (claims not supported), novelty and significance both 1 (not novel or significant).\n\nReviewer 3 mentions the method lacks novelty as it combines existing techniques. No comparison with state-of-the-art, poor writing with missing details (like detector architecture), and no ablation studies. They think reproducibility is okay but analysis is lacking. Correctness 2 (some claims incorrect), novelty 2 (marginal).\n\nReviewer 4 acknowledges the idea's novelty in using semantic graphs for interpretable OOD detection but criticizes reliance on a pre-trained detector which might not cover all in-distribution data. Experiments are insufficient: no comparison with other OOD methods, limited datasets, no ablation studies, and poor writing with many glitches. They also note missing related work on common OOD techniques. Correctness 3, technical novelty 3, empirical 2.\n\nAuthor response is missing, so no rebuttal to address these points.\n\nNow, considering top-tier standards, the key issues are:\n\n1. **Lack of comparison with state-of-the-art**: Multiple reviewers (2,3,4) note that the paper doesn't compare with established OOD detection methods. The authors only compare with graph embedding techniques, which isn't sufficient. For a top conference, showing competitive performance against current best methods is essential.\n\n2. **Poor experimental validation**: Only using LSUN dataset is a problem. Top-tier papers usually test on multiple datasets. Also, no ablation studies to show the impact of different components (like hyperparameters, graph construction steps).\n\n3. **Writing and clarity issues**: Grammar errors, unclear explanations, missing details in methodology (like how the graph is built, what object detection model is used, handling of OOD in detection phase). Figure 1 isn't explanatory enough. These hinder reproducibility and understanding.\n\n4. **Novelty concerns**: Reviewers 2,3,4 suggest the method isn't novel enough. It combines existing techniques (object detection, graph embeddings, OOD classifiers) without significant innovation. The idea of using semantic graphs might be novel but not groundbreaking, and the execution lacks depth.\n\n5. **Critical assumptions not addressed**: The reliance on object detection accuracy is a flaw. If the detector fails on OOD data, the graph might be flawed, but the paper doesn't analyze this. This weakens the method's robustness claims.\n\n6. **Related work shortcomings**: Not discussing major OOD detection approaches (like distance-based, logit-based methods) shows a lack of thorough literature review, which is expected at top conferences.\n\nOn the positive side, the idea of using semantic graphs for OOD detection is interesting (Reviewer 4's strength), and the results on LSUN are strong. However, the method's novelty is marginal, and the experiments are insufficient to support the claims for a top venue.\n\nGiven that three out of four reviewers (especially Reviewer 2) recommend rejection, and the critical flaws in experiments and writing, I think this paper doesn't meet the high standards required. The authors need to address the methodological shortcomings, provide better comparisons, and improve clarity before it's suitable for a top conference.\n</think>\n\nThe paper proposes a method for OOD detection using graph structures derived from object detection outputs, achieving high AUROC scores on LSUN. However, several critical issues undermine its suitability for a top-tier conference:\n\n1. **Insufficient Experimental Rigor**: \n   - The paper fails to compare with state-of-the-art OOD detection methods (e.g., SSD, energy-based methods), instead comparing only to graph embedding techniques. This weakens claims of competitiveness.\n   - Limited dataset usage (only LSUN) and lack of ablation studies (e.g., hyperparameters, graph construction impact) make results unpersuasive for a top venue.\n\n2. **Novelty and Originality**: \n   - The method combines existing techniques (object detection, graph embeddings, OOD classifiers) without significant innovation. Reviewers note marginal novelty, with the core idea of semantic graphs being insufficiently developed or contextualized against prior work.\n\n3. **Clarity and Reproducibility**: \n   - Poor writing, grammatical errors, and missing details (e.g., object detection model specifics, graph construction steps, Figure 1 explanation) hinder reproducibility and understanding. The lack of a formal inference pipeline and incomplete related work section further reduce its quality.\n\n4. **Critical Assumptions**: \n   - The method’s reliance on accurate object detection for OOD data is unaddressed. Since object detectors may fail on OOD inputs, this creates a vulnerability not analyzed in the paper.\n\n5. **Reviewer Consensus**: \n   - Three reviewers (especially Reviewer 2) strongly recommend rejection due to poor preparation, lack of comparisons, and insufficient novelty. While one reviewer acknowledges potential promise, the majority consensus highlights unmet standards for a top-tier conference.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper is about improving adversarial robustness by applying more regularizations to less robust samples. The title and abstract suggest a novel adversarial training algorithm with both theoretical and empirical contributions. \n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Reviewers 2, 6, 7, 12, and 14 seem more positive, while others like Reviewers 3, 5, 9, 10, 11, 12, 13, 15, and 16 have significant concerns. \n\nFirst, the theoretical contribution is a point of contention. Reviewers 5, 9, 10, 11, 12, and 16 argue that Theorem 1 isn't a meaningful contribution. They point out that it's an extension of TRADES' bound to multi-class but with loose inequalities and unclear definitions of z(x). The authors tried to clarify z(x) as a function in the response, but some reviewers still find the math unclear and the bound too loose. This is a big issue because a top-tier conference requires strong theoretical backing.\n\nEmpirically, the results show marginal improvements over existing methods like HAT, especially when using extra data. Reviewers 8 and 15 note that the gains are small and question if the baselines were properly tuned. The authors added experiments with gradient accumulation to match HAT's setup, showing slight improvements, but the differences are minimal. The experiments on larger datasets like CIFAR-100 or ImageNet aren't mentioned, which is a limitation since small datasets might not generalize.\n\nClarity and rigor are also problematic. Reviewers 10, 11, and 12 highlight issues with the definition of z(x) and the mathematical rigor of the theorem. The authors tried to address this but the confusion remains. For a top conference, clarity is crucial, and unresolved ambiguities could hinder reproducibility.\n\nThe paper's novelty is another concern. Reviewers mention similarities to MART and TRADES, with the main idea being a weighted regularization. While the authors argue it's an improvement, the marginal gains and existing methods' proximity reduce its novelty.\n\nOn the positive side, the method is theoretically inspired, and the experiments are extensive. The authors addressed some concerns by adding more experiments and clarifications. However, the core theoretical contribution's weakness and the small empirical gains might not meet the high bar of a top-tier conference.\n\nConsidering all factors: the theoretical contribution is debated but not strongly novel or rigorous enough, empirical results show only marginal improvements, and there are unresolved clarity issues. While the paper has merits, the critical flaws in theory and experiments might tip the decision towards rejection unless the authors can significantly address these in revisions. Since the rebuttal didn't fully resolve the theoretical concerns and the improvements are small, I lean towards rejection.\n</think>\n\nThe paper proposes a novel adversarial training method (ARoW) aimed at improving adversarial robustness by applying stronger regularization to less robust samples. While the work has some strengths, several critical issues undermine its acceptance at a top-tier conference:\n\n### Theoretical Contributions:\n- **Weak Novelty and Rigor**: Multiple reviewers (5, 9, 10, 11, 12, 16) argue that Theorem 1 is not a meaningful theoretical advance. It is seen as a trivial extension of TRADES' bound to multi-class classification with loose inequalities and poorly defined terms (e.g., \\( z(x) \\)). The authors attempted clarifications but failed to resolve ambiguities in the mathematical definitions (e.g., non-uniqueness of \\( z(x) \\)), leaving the theoretical contribution in doubt. Reviewers 5 and 16 explicitly state the theorem is \"meaningless\" and \"artificially created,\" which is a severe critique for a theory-driven paper.\n\n### Empirical Contributions:\n- **Marginal Improvements**: While ARoW shows consistent but small gains over baselines like HAT and TRADES, the improvements are often minimal (e.g., 0.1–0.2% in robust accuracy). Reviewers 8, 15, and 16 note that these gains are insufficient to justify the method’s novelty, especially when using extra data. The experiments on larger datasets (e.g., CIFAR-100, ImageNet) are absent, limiting generalizability.\n- **Baseline Comparisons**: Concerns about fair comparisons persist. For instance, HAT’s reported results with extra data in Table 3 are close to ARoW’s, and the authors’ rebuttal only partially addresses discrepancies with prior work (e.g., gradient accumulation experiments added in the appendix).\n\n### Clarity and Reproducibility:\n- **Mathematical Ambiguities**: Reviewers 10, 11, and 12 highlight unresolved issues in the definition of \\( z(x) \\) and the derivation of bounds. The authors’ clarifications (e.g., redefining \\( \\mathcal{Z} \\)) do not fully address the confusion, risking reproducibility and theoretical soundness.\n- **Writing and Presentation**: While the paper is generally well-written, minor issues (e.g., inconsistent baseline comparisons, omitted details in figures) detract from its polish.\n\n### Strengths:\n- The method is theoretically motivated and aligns with prior adversarial training frameworks.\n- Empirical results are extensive, including ablation studies and fairness analyses.\n- The authors addressed some reviewer concerns in the rebuttal (e.g., gradient accumulation experiments).\n\n### Final Assessment:\nThe paper’s core theoretical contribution is insufficiently novel and rigorously grounded, and the empirical gains are marginal. While the problem of adversarial robustness is important, the work does not meet the high bar for a top-tier conference, where contributions must be both theoretically significant and empirically impactful. The unresolved ambiguities in the theory and the lack of compelling experimental advantages weaken its case for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's main contribution is exploring the effect of swapping the order of Batch Normalization (BN) and activation functions, especially with bounded activations like Tanh. They found that putting BN after the activation (Swap) improves performance compared to the conventional order when using bounded activations. They attribute this to asymmetric saturation and induced sparsity. \n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 points out that the paper needs more figures (like skewness for VGG16_11) and clarifications on why sigmoid isn't considered. Reviewer 2 mentions weaknesses like unclear connections between asymmetric saturation and generalization, issues with sparsity metrics, and limited applicability since it doesn't outperform ReLU. Reviewer 3 thinks it's borderline, noting the interesting finding but insufficient practical impact. Reviewer 4 wants more analysis on training dynamics and whether the swap helps beyond image classification. Reviewer 5 has concerns about rigor in claims, experimental setup (like residual connections), and reproducibility.\n\nThe authors addressed some points in their response. For example, they clarified the center of the function, provided code details, and adjusted the model for CIFAR-100. They also added experiments with ResNet and discussed why Tanh might not work well with residuals. However, some issues remain, like the lack of superiority over ReLU and the limited scope excluding residual networks.\n\nNow, considering the standards of a top-tier conference, the paper needs to have significant contributions, clear methodology, and thorough analysis. The key findings are interesting—showing that Swap order with bounded activations can outperform conventional setups. However, the reviewers highlight several issues: \n\n1. **Lack of Practical Impact**: The Swap models with Tanh still underperform ReLU-based models. This limits the practical significance unless there's a specific use case where bounded activations are preferred. \n2. **Limited Scope**: The analysis excludes architectures with residual connections, which are widely used. The authors mention some experiments with ResNet but the results aren't as strong, and the main experiments are on VGG-like networks. \n3. **Clarity and Rigor**: Some claims about saturation and sparsity aren't well-supported. The connection between asymmetric saturation and generalization isn't fully convincing. The metrics (saturation, skewness, sparsity) need better definitions and validation. \n4. **Reproducibility**: Initially, the code lacked clear documentation for key metrics, though the authors clarified this in their response. \n\nOn the positive side, the paper presents a novel observation about the interaction between BN and bounded activations, which is a solid contribution. The experiments are extensive with multiple activation functions and architectures. The authors addressed some reviewer concerns, but not all. \n\nThe main issue is the lack of a strong practical advantage over existing methods (ReLU) and the limited generalizability. While the phenomenon is interesting, without showing that it can be applied to more common architectures (like ResNets) or achieve comparable performance to ReLU, the impact is diminished. The reviewers' concerns about the analysis not being fully convincing and the paper's technical novelty being marginal also weigh against acceptance. \n\nHowever, the paper does contribute a new insight into BN and activation ordering, which could be valuable for the community. The authors have made improvements in the rebuttal, but the remaining weaknesses might not meet the high bar of a top conference. \n\nFinal Decision: Reject\n</think>\n\nThe paper investigates the impact of swapping the order of Batch Normalization (BN) and bounded activation functions (e.g., Tanh), demonstrating that placing BN after the activation (\"Swap\" order) improves performance compared to the conventional order. Key findings include the role of asymmetric saturation and induced sparsity in enhancing generalization. However, several critical issues undermine its acceptance at a top-tier conference:\n\n1. **Limited Practical Impact**: While the Swap order improves performance for bounded activations, it still underperforms ReLU-based models. The authors do not demonstrate scenarios where bounded activations with the Swap order outperform ReLU, limiting real-world applicability.\n\n2. **Scope and Generalizability**: The analysis excludes architectures with residual connections, which are widely used in modern networks. Though the authors mention limited experiments with ResNet, the results are not compelling, and the core experiments focus on VGG-like networks. This restricts the paper’s relevance to contemporary practices.\n\n3. **Clarity and Rigor**: \n   - The connection between asymmetric saturation and generalization is not fully convincing. Reviewers highlight ambiguities in metrics (e.g., saturation, sparsity) and the lack of a clear causal link to performance gains.\n   - Claims about saturation being \"counterintuitive\" (e.g., beneficial in final training stages) are not rigorously supported, and the authors rely on prior work that may not align with their context.\n   - The definition of \"center of the function\" and other terms were initially unclear, though clarified in the rebuttal.\n\n4. **Empirical Limitations**: \n   - The experiments on CIFAR-100 with VGG16 revealed inconsistent saturation patterns across layers, attributed to model overparameterization. The revised analysis using a trimmed VGG16_11 partially addresses this but does not fully resolve concerns about layer-wise consistency.\n   - The Swap order’s performance does not surpass ReLU, weakening its empirical significance.\n\n5. **Technical Novelty**: While the observation about the Swap order with bounded activations is novel, the analysis lacks depth. The contributions are incremental, and the paper’s conclusions are constrained by the limited scope and unresolved methodological issues.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about collaborative adversarial training (CAT) where two models are trained together using different adversarial methods. The main idea is that since different methods have different strengths and weaknesses, combining them can improve robustness. The experiments show SOTA on CIFAR-10 under Auto-Attack.\n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 says the idea is interesting but points out lack of novelty compared to ensemble methods and missing recent baselines. They also mention typos and the need for theoretical analysis. Reviewer 2 is more critical, saying the method isn't novel enough and the experiments aren't strong enough. They also note that training two models doubles the cost, which is a downside. Reviewer 3 is similar, pointing out that the method's novelty is low, and they want more experiments on larger datasets and different architectures. Reviewer 4 thinks it's somewhat novel but wants more ablations and comparisons with stronger baselines like AWP.\n\nThe authors didn't respond, which might be an issue if they could have addressed some points. The main strengths are the SOTA results and the clear motivation. But the weaknesses are significant: lack of novelty, computational cost, missing important baselines (like ensemble methods mentioned in reviews), and theoretical backing. Also, the experiments are only on CIFAR-10 and 100, which might not be enough for a top conference. The reviewers' consensus seems to lean towards the method not being novel enough and the experiments not thorough enough. The computational cost is a practical concern. Since top-tier conferences require high novelty and strong empirical validation, this might not meet the bar. Even though the results are good, the lack of addressing prior work and the method's incremental nature could lead to rejection. But some reviewers gave it a 3/4 on significance. Hmm, but the majority of the reviews (especially 2 and 3) are more negative. Final decision probably reject.\n</think>\n\nThe paper proposes Collaborative Adversarial Training (CAT), which trains two adversarially robust models jointly to improve robustness by leveraging their complementary strengths. While the method achieves state-of-the-art results on CIFAR-10 and CIFAR-100 under Auto-Attack, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Empirical Results**: The method achieves new SOTA robustness on CIFAR-10 without additional data, which is a strong empirical contribution.\n2. **Clarity**: The paper is well-written and easy to follow.\n3. **Novel Motivation**: The observation that different adversarial training methods make distinct errors is insightful and provides a plausible rationale for collaboration.\n\n### Major Weaknesses:\n1. **Lack of Novelty**: \n   - Reviewers highlight that the core idea of combining adversarial defenses (e.g., ensembles, knowledge fusion) is not novel. Prior work like ensemble adversarial training [1, 2] and related methods [3, 4] are omitted, weakening the contribution’s originality.\n   - The proposed method is framed as a straightforward extension of existing techniques (e.g., knowledge distillation between models trained with different strategies).\n\n2. **Empirical Gaps**:\n   - **Missing Baselines**: The paper does not compare against recent state-of-the-art methods like Adversarial Weight Perturbation (AWP) or modern ensemble approaches, making it unclear if the gains are truly significant.\n   - **Limited Scope**: Experiments are confined to small datasets (CIFAR-10/100) and architectures. Larger-scale validation (e.g., ImageNet) and diverse model architectures (e.g., VGG, MobileNet) are absent, limiting generalizability.\n   - **Computational Cost**: Training two models doubles computational and memory requirements, a significant drawback for practical deployment. The authors do not address this trade-off adequately.\n\n3. **Theoretical and Methodological Gaps**:\n   - No theoretical analysis explains *why* collaboration improves robustness or how it differs from existing ensemble methods.\n   - Key ablations (e.g., using the same method for both models, impact of more than two defenses, or integrating with AWP) are missing, weakening the method’s credibility.\n   - The paper fails to analyze whether prediction discrepancies between models (e.g., TRADES vs. AT) are larger than those within the same method (e.g., different random seeds), which is critical to justifying the approach.\n\n4. **Writing and Reproducibility**:\n   - Typos and missing references (e.g., undefined \"ALP\" abbreviation) reduce clarity.\n   - The absence of an author response leaves unresolved concerns about comparisons to prior work and experimental rigor.\n\n### Reviewer Consensus:\n- **Reviewers 2 and 3** rate the technical and empirical significance as marginal (scores of 2/4), citing low novelty and insufficient experiments.\n- **Reviewers 1 and 4** acknowledge the method’s potential but stress the lack of theoretical grounding, missing baselines, and computational inefficiency.\n- All reviewers agree that the paper under-cites prior work on ensemble adversarial training and knowledge fusion, undermining its contribution.\n\n### Conclusion:\nWhile the empirical results are impressive, the method’s incremental nature, lack of theoretical justification, and failure to engage with recent baselines and ensemble methods significantly weaken its case for acceptance at a top-tier conference. The computational cost and limited experimental scope further diminish its practical and academic impact. Addressing these issues in a revised version would be necessary for reconsideration, but as currently presented, the paper does not meet the bar for a top conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about maintaining diverse playing styles in self-play for reinforcement learning. The main idea is using a bi-objective optimization model to balance skill and diversity. The reviewers have given mixed feedback. \n\nStarting with the strengths: The problem they're addressing is important because self-play can lead to agents that aren't well-generalized. The approach seems novel in combining evolutionary algorithms with bi-objective optimization. The experiments in Pong and Justice Online show some promising results, especially against other baselines. \n\nBut the weaknesses are significant. Reviewer 1 points out that they didn't cite Lanctot et al.'s work on multiagent RL algorithms like Fictitious Play, which are population-based self-play methods. That's a big oversight because those methods could be relevant baselines. Also, the experimental setup in Table 3 is flawed because the pool heavily favors EMOGI and BiO agents, giving them an unfair advantage. This might undermine the results' validity.\n\nReviewer 2 mentions that reducing playing style to a single scalar is a major simplification. While the authors might have done this for simplicity, it's a limitation they didn't adequately address. The lack of confidence intervals in the tables and the potential cherry-picking in Figure 2 are also issues. Plus, there's confusion about some notation and terminology, like \"1G frames\" and PPT vs. PBT.\n\nReviewer 3 highlights the absence of quality-diversity algorithms like MAP-Elites as baselines. This is a critical point because those methods are directly related to diversity in RL. The paper only uses two domains, one of which is simple (Pong), so the results might not be generalizable. Also, the experiments don't compare against domain randomization, a common technique to improve generalization, which weakens the empirical analysis.\n\nThe technical novelty is considered marginal by two reviewers, which is a problem for a top-tier conference. The method builds on existing multi-objective and evolutionary approaches without enough innovation. The empirical results, while positive, are not robust enough due to the limited baselines and experimental setups.\n\nThe authors didn't respond, so we can't assume they addressed these issues. The combination of missing key baselines, flawed experiments, and limited novelty suggests the paper isn't up to the standards of a top conference. Even though the problem is important, the current submission has too many unresolved weaknesses.\n</think>\n\nThe paper proposes a method to maintain diverse playing styles in self-play through a bi-objective optimization framework, aiming to improve policy generalization. While the problem is important and the approach has some empirical promise, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Problem Relevance**: The focus on diversity in self-play is timely, as over-specialization in self-play can limit generalization.\n2. **Empirical Results**: The method shows competitive performance in Pong and Justice Online, particularly against some baselines.\n3. **Clarity**: The paper is generally well-written and the approach is conceptually straightforward.\n\n### Major Weaknesses:\n1. **Missing Baselines**:\n   - **Multiagent RL Frameworks**: Reviewer 1 highlights the absence of key prior work like Lanctot et al.'s (2017) multiagent RL algorithms (e.g., Fictitious Play), which are population-based self-play methods. These should have been included as baselines.\n   - **Quality-Diversity Algorithms**: Reviewer 3 emphasizes the lack of comparisons to quality-diversity methods like MAP-Elites, which are directly relevant to diversity-driven optimization in RL. This omission weakens the novelty and significance of the contribution.\n   - **Domain Randomization**: The paper does not compare against domain randomization (a standard technique to improve generalization), making it unclear whether the proposed method offers unique advantages.\n\n2. **Flawed Experimental Design**:\n   - **Biased Evaluation in Table 3**: The experimental setup for Table 3 heavily overrepresents EMOGI and BiO agents in the opponent pool (30 each), giving them an unfair advantage. This design flaw risks inflating the proposed method’s performance.\n   - **Limited Domains**: Only two domains are tested (one simple, one proprietary). Pong’s simplicity limits the generality of the results, while Justice Online’s closed-source nature complicates reproducibility and broader validation.\n\n3. **Technical and Conceptual Limitations**:\n   - **Simplification of Playing Style**: Reducing playing style to a single scalar (aggressive/defensive) is a major limitation. The authors do not adequately address this simplification’s potential shortcomings or discuss how it might fail in higher-dimensional style spaces (as noted by Reviewer 2).\n   - **Notational and Explanatory Gaps**: Ambiguities in equations (e.g., Equation 4’s unclear notation) and undefined terms (e.g., \"reward weights\") reduce clarity. The lack of confidence intervals in results (e.g., Table 1/2) undermines statistical rigor.\n\n4. **Novelty and Significance**:\n   - **Marginal Innovation**: Reviewers 1 and 3 note that combining multi-objective optimization with RL is not novel. The proposed method’s adaptation of NSGA-II to self-play is incremental, with limited technical or empirical breakthroughs.\n   - **Unclear Advantages Over Existing Work**: The paper does not convincingly demonstrate superiority over AlphaStar (which explicitly aims for diversity) or other population-based methods like PBT. The experiments’ design flaws further obscure this comparison.\n\n### Conclusion:\nWhile the paper addresses an important problem and shows some promise, its failure to engage with critical prior work, flawed experimental design, and lack of robust empirical validation against relevant baselines significantly weaken its contributions. The technical novelty is marginal, and the unresolved methodological issues raise doubts about the reliability of the results. For a top-tier conference, these shortcomings outweigh the paper’s strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about regression with label differential privacy (DP). The main idea is using a prior distribution of labels to create an optimal randomization mechanism, specifically \"randomized response on bins.\" They claim this is better than existing methods like Laplace or Exponential mechanisms. The experiments show their method works well.\n\nLooking at the reviewers' comments. Reviewer 1 initially had an issue with conflating regression and classification but then retracted that after the author's clarification. That's a positive sign. Reviewer 2 was concerned about a fair comparison but agreed to include the results in the appendix. The authors addressed this by adding an appendix with the comparison, which is good.\n\nReviewer 3 thinks the method is interesting and practical, but points out that it doesn't compare against a specific label-DP method (Ghazi et al. 2021). The author responded by noting their method is more general and handles regression losses better. The reviewer also mentions the paper is clearly written and has potential impact. However, they gave a 3 on technical novelty, suggesting some overlap with prior work.\n\nReviewer 4 is more positive, giving an 8/10. They note the method is novel and optimal, but point out a weakness in not considering the second term in the loss equation (the Bayes error part). The authors acknowledged this as a future direction. Also, the reviewer questioned if the number of bins could be smaller than labels, which the authors empirically observed but lack a formal proof. Still, the reviewer's high score is a plus.\n\nReviewer 5 says the paper is well-written and provides experiments but notes limited technical novelty compared to Ghazi et al. They mention the method isn't technically novel but is a generalization. The reviewer's technical novelty score is 2, which is low. However, they do acknowledge the optimality proof and empirical results. The main issue here is the lack of significant novelty beyond existing work.\n\nReviewer 6 thinks the problem is interesting but points out mapping continuous labels to finite bins as a weakness. The authors addressed this by extending their theorem to continuous intervals. The reviewer also noted some typos but overall found it an interesting problem.\n\nAuthor responses addressed most concerns. They added comparisons against Ghazi et al., showing better performance, but noted it's an unfair comparison since Ghazi's method is for classification. They clarified the difference between regression and classification losses, which seems valid. The technical points about the LP solution and dynamic programming being novel were highlighted, which might counter the novelty concerns.\n\nNow, considering the criteria for a top-tier conference. The paper's main strengths are the clear writing, empirical validation, and theoretical proof of optimality. The method is a generalization to regression, which is a solid contribution. However, some reviewers (like 3 and 5) question the novelty, especially compared to Ghazi et al. But the authors argue their approach handles regression losses differently and the solution structure is novel. The experiments show better performance, which is a practical advantage.\n\nThe weaknesses include not comparing directly with all relevant label-DP methods (though they did add some in the appendix), and the theoretical consideration of the Bayes error term. But the authors addressed these by adding discussions and experiments. The technical novelty is borderline but seems sufficient for a top conference if the problem is well-motivated and the solution is a clear step forward.\n\nTop conferences often look for significant contributions, but sometimes incremental work with strong empirical results and clear theoretical backing can get in. Here, the problem is well-defined, the solution is novel in extending to regression with optimality proofs, and the experiments back it up. The main criticism is on novelty, but since they handle a different setting (regression vs classification), it's a valid extension. The fact that Reviewer 4 gave a high score and others found it clear and impactful leans towards acceptance.\n\nI think the paper should be accepted. The contributions, while building on prior work, provide a novel approach for regression with label DP, which is a distinct problem from classification. The experiments and theoretical results are solid, and the authors addressed reviewer concerns effectively.\n</think>\n\nThe paper proposes a novel approach to achieving label differential privacy (DP) in regression tasks by introducing a \"randomized response on bins\" mechanism. This method is theoretically grounded, with proofs of optimality under a given regression loss function, and supported by empirical evaluations demonstrating superior performance compared to baseline DP methods. Key strengths include clear writing, a well-motivated problem, and practical utility. The authors address reviewer concerns effectively, such as adding comparisons against prior work (e.g., Ghazi et al., 2021) in the appendix and clarifying the distinction between regression and classification tasks. While some reviewers question the technical novelty relative to existing label-DP methods, the paper extends these approaches to regression—a distinct and understudied setting—with a unique solution structure (e.g., dynamic programming for bin optimization). Theoretical limitations (e.g., not addressing the Bayes error term in full) are acknowledged as future work, and empirical results are compelling. The contributions, though incremental in some aspects, are sufficiently novel and impactful for a top-tier conference, particularly given the lack of prior work on regression-specific label DP mechanisms.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new validation protocol called PVP, aiming to solve the problem of splitting data into train/validation sets, which causes a trade-off between model performance and reliable evaluation. The reviewers have mixed opinions but mostly leaning towards rejection. Let me go through each point carefully.\n\nFirst, the problem they're addressing is valid—validation set splitting is a common issue. The proposed solution uses data augmentation and a sampling method to create a validation set that doesn't take away from the training data. That's an interesting approach. But the reviewers point out several issues.\n\nReviewer 1 mentions that the experiments don't clearly show how PVP helps in model selection (like hyperparameter tuning), which is a key use of validation sets. The experiments might only show performance on fixed models, which isn't sufficient. Also, the clarity of the experiments section is a problem. Tables 3 and 4's metrics aren't well-defined, leading to confusion. The reviewer also questions the robustness of PVP to different data augmentations and the choice of weighting in their method. Plus, they wonder if it works beyond classification tasks.\n\nReviewer 2 is concerned about the empirical evaluation using small datasets like CIFAR-LT instead of larger ones like ImageNet. They also want theoretical backing for why PVP works, not just empirical results. Without that, the claims might not hold up.\n\nReviewer 3 likes the problem's significance but points out major issues in experiments. They want more real-world or industrial setups, like using pretrained models and handling noisy labels. Also, ablation studies on hyperparameters and statistical significance in tables are missing. They also suggest an alternative approach that combines the proximal set with the original training data, which the authors didn't explore.\n\nReviewer 4 is harsher, saying the method lacks theoretical justification. They argue that the metrics used (like test-validation gap) don't prove the validation set is representative of the true data distribution. The core issue here is whether the augmented data actually reflects the real test distribution, which isn't addressed.\n\nThe authors didn't respond, so we can't see if they addressed these points. \n\nPutting this together: The paper's main strengths are addressing a real problem and proposing a novel method. However, the experiments are insufficient. They don't show how PVP aids in model selection, use small datasets, lack ablation studies, and don't validate across different scenarios (like regression or noisy data). The theoretical basis is missing, which is a big red flag for a top-tier conference. The clarity issues in the experiments make it hard to assess the claims properly. \n\nTop conferences require strong empirical validation and theoretical grounding. Since the reviewers point out critical flaws in both areas, and the authors didn't address them, the paper probably doesn't meet the standards. The method's novelty isn't enough if it's not rigorously tested and justified.\n</think>\n\nThe paper proposes the Proximal Validation Protocol (PVP) to address the trade-off between training data usage and validation set quality. While the problem is important and the approach is novel, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Empirical Evaluation Shortcomings**: \n   - **Lack of Model Selection Evidence**: The experiments focus on fixed models but fail to demonstrate that PVP improves hyperparameter tuning or model selection, which is the primary purpose of validation sets. This gap weakens the practical relevance of the claims.\n   - **Small Datasets**: The use of small datasets (e.g., CIFAR-LT instead of ImageNet, limited text datasets) raises concerns about scalability and real-world applicability. Industrial scenarios, such as fine-tuning pretrained models or handling noisy labels, are not adequately addressed.\n   - **Incomplete Ablations**: The method’s sensitivity to data augmentation choices, sampling strategies (e.g., the non-standard weighting in Equation 6), and hyperparameters (e.g., proximal set size) is not rigorously tested. Without ablation studies, it is unclear whether the results are robust or specific to the tested configurations.\n\n2. **Theoretical Gaps**: \n   - The paper lacks theoretical justification for why the augmented and sampled proximal set aligns with the true data distribution. Reviewer 4’s critique highlights that the test-validation gap metric does not confirm representativeness of the validation set, a foundational requirement for reliable evaluation. Without theory or stronger empirical evidence, the claims about PVP’s effectiveness remain speculative.\n\n3. **Clarity and Presentation Issues**: \n   - Ambiguities in the experiments (e.g., undefined metrics like \"deterioration,\" unclear table interpretations) hinder reproducibility and evaluation. The writing contains stylistic issues (e.g., informal language) that distract from the technical content.\n   - The method’s applicability beyond classification tasks (e.g., regression, unsupervised learning) is unexplored, limiting its broader significance.\n\n4. **Statistical Rigor**: \n   - Statistical significance is not reported for key results (e.g., Table 1 and 2), making it difficult to assess whether observed improvements are meaningful. This is particularly problematic for a paper focused on validation protocols, where reliability is paramount.\n\nWhile the problem is significant and the idea is novel, the current submission falls short in empirical rigor, theoretical grounding, and clarity. These deficiencies, coupled with the lack of an author response to address reviewer concerns, suggest the work is not yet mature for a top-tier conference. The method’s potential is intriguing, but the paper does not meet the high standards required for acceptance without substantial revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Functional Risk Minimization,\" and the keywords are learning framework, theory, meta-learning, supervised learning. The abstract mentions breaking the classic assumption of data coming from a single function with noise, instead modeling each data point with its own function. They call this Functional Risk Minimization (FRM) and claim it subsumes ERM for many loss functions, provides better performance in small experiments, and relates to memorization in over-parameterized models.\n\nLooking at the reviewer comments, there are three reviewers. Let's parse each one.\n\nReviewer 1 is quite critical. They mention strengths like the novel approach to noise modeling and promising empirical results in linear regression. But the weaknesses are significant. The main issues are the lack of clear derivation connecting the algorithm to the theory, scalability concerns (since the algorithm uses Hessian approximations which might not scale), and no formal guarantees on why FRM outperforms ERM. They also point out clarity issues in the presentation, especially around terminology like ERM and FRM. The reviewer's final summary says the paper has promising ideas but can't be accepted due to gaps in connecting the algorithm to theory, lack of basic theory, and clarity issues. Their correctness score is 2 (some claims not well-supported), technical novelty is 4 (significant and new), empirical is 3 (somewhat new).\n\nReviewer 2 thinks the idea is interesting and well-written but has some confusion. They question whether the model is just a hierarchical Bayesian approach and why test-time uses a single theta when each data point has its own function. They also note that the functional generative model wasn't evaluated experimentally. Their summary is positive but wants more clarity on the Bayesian connection and experimental validation. Correctness is 3 (minor issues), technical and empirical novelty both 3 (somewhat new).\n\nReviewer 3 is more positive, saying it's marginally above the acceptance line. They acknowledge the high novelty and quality but note computational costs and the need to explain why FRM is crucial over ERM. Their summary suggests acceptance. Correctness 3, technical novelty 4, empirical 3.\n\nThe author response addresses several points. They clarify the Bayesian vs hierarchical model distinction, explain the test-time use of a single theta, and mention new experiments with CNNs on MNIST variants showing FRM's scalability and benefits in structured data. They also address scalability by explaining Hessian approximations via vector products and mention that the approximation is valid for over-parameterized models. They also provide more theoretical grounding, like FRM subsuming ERM for common losses.\n\nNow, considering all this. The paper's main contributions are novel, as per the reviewers, especially the framework challenging traditional ERM assumptions. The authors have addressed some scalability concerns with new experiments and clarified parts of the theory. However, Reviewer 1's key points about the lack of formal guarantees (why FRM works, theoretical underpinnings) and clarity in the presentation remain. The authors mention experiments showing FRM's effectiveness in structured data, which is a plus. The new experiments added in the response (like the VAE on MNIST variants) help with empirical validation, which Reviewer 2 pointed out was missing.\n\nBut the theoretical gaps are still there. The authors say they can't guarantee low loss unless data fits FGM assumptions, but that's a big caveat. The algorithm's derivation was unclear, but they clarified equations 7 and 8. The Hessian issue is addressed via vector products, which is a valid approach, so scalability might be manageable. The terminology issues (like ERM vs FRM) were addressed in the response, which should improve clarity.\n\nTop-tier conferences usually require strong theoretical foundations, clear presentation, and empirical validation. The paper has high novelty and potential impact, which is a big plus. The weaknesses are in the theoretical rigor and some clarity, but the authors have made efforts to address these. The new experiments added in the response strengthen the empirical case. The scalability concerns were partially alleviated with the Hessian approach and the CNN experiments.\n\nConsidering that the authors have provided responses that address some major points (like scalability and Bayesian confusion), and the paper's novel framework is seen as impactful by some reviewers, I think it's on the borderline. However, the lack of formal guarantees and theoretical proofs might still be a hurdle. But since the conference is top-tier, they might value the novel idea and potential over the current gaps, especially if the authors commit to improving clarity and theory in the final version. The fact that the authors have already added experiments and clarifications in their response suggests they can address the remaining issues.\n\nFinal Decision: Accept, but with the caveat that the authors need to make the requested clarifications and ensure the theoretical connections are more rigorously presented. Since the reviewers' main concerns were addressed in the response, and the paper's novelty and potential are high, it's worth accepting for presentation and further development.\n</think>\n\nThe paper introduces a novel framework, Functional Risk Minimization (FRM), which challenges traditional Empirical Risk Minimization (ERM) by modeling each data point as arising from its own function. This approach is theoretically intriguing and has the potential to address limitations of ERM in structured noise scenarios. The authors provide empirical results in regression, reinforcement learning, and new experiments with CNNs on MNIST variants, demonstrating FRM's scalability and benefits in structured data. Key strengths include high technical novelty, potential impact on understanding over-parameterized models, and the framework's broad applicability across ML paradigms.\n\nHowever, critical concerns remain. Reviewer 1 highlights significant gaps in theoretical rigor: the lack of formal guarantees (e.g., why FRM outperforms ERM, convergence properties), unclear derivations of core equations (7 and 8), and scalability challenges due to Hessian computations. While the authors address scalability via Hessian-vector products and new experiments, the theoretical underpinnings remain underdeveloped. Clarity issues, such as ambiguous terminology (e.g., ERM vs. FRM) and the relationship between FGM and Bayesian models, were partially addressed in the response but require further refinement. Reviewer 2’s confusion about test-time usage of a single parameter and the lack of prior FGM experiments were also mitigated by the authors’ clarifications and added results.\n\nDespite these weaknesses, the paper’s novel framework and empirical evidence suggest transformative potential, particularly in aligning with modern deep learning’s over-parameterized regime. The authors’ responses demonstrate a commitment to addressing scalability and clarity, and the new experiments strengthen the empirical case. While the theoretical gaps are notable, top-tier conferences often prioritize groundbreaking ideas with clear pathways for future work over fully polished results. The paper’s high novelty, potential impact, and partial resolution of reviewer concerns tip the balance toward acceptance, provided the authors further clarify derivations, strengthen theoretical arguments, and improve presentation in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving protein-protein interaction (PPI) prediction using pretrained structure embeddings. The main idea is to use structural information from OmegaFold, which is a pretrained model, and then feed those embeddings into a GNN for PPI prediction. \n\nLooking at the reviewers' comments, there's a common theme here. Reviewer 1 points out that the method isn't very novel because they're just using existing embeddings (OmegaFold) and standard GNNs like SAGE or GAT. The novelty is low since they didn't develop a new structure pretraining method. Also, there's an issue with how they handle variable-length embeddings via mean pooling, which might lose important local structural info. The results in tables 2 and 3 are inconsistent, which is a problem. Plus, some minor issues like typos and unclear tables.\n\nReviewer 2 says the approach is simple—just adding existing embeddings to GNNs. The improvement is only about 5%, which isn't huge. They mention lack of novelty and that it doesn't spark new ideas. Correctness is okay but the contributions are marginal.\n\nReviewer 3 notes clarity issues. They're confused about how AUC is measured here and the setup of the binary classification. The empirical results show only marginal gains and don't compare to state-of-the-art methods like dMaSIF. The comparisons are weak, only against GraphSAGE and GAT, which might not be the best baselines.\n\nReviewer 4 is more critical. They point out that the baselines are too weak—like using a simple ResNet on sequences isn't a strong comparison. Missing comparisons to AlphaFold-Multimer, which is a big deal in PPI prediction. Also, the GNN part isn't justified well. Why use a GNN over a pairwise model? The negative sampling in training isn't clearly explained. The model's design advantage isn't supported by results, and the paper spends too much time on OmegaFold which is just a pretrained model they use without changes.\n\nThe authors didn't respond, so we can't see any rebuttals addressing these points. \n\nPutting this together: The main issues are lack of novelty, weak baselines, unclear methodology, and marginal improvements. The reviewers consistently rate technical and empirical significance low. Even though the problem is important, the contributions don't meet the bar for a top-tier conference. The method is a straightforward combination of existing techniques without significant innovation. The experiments don't compare to the latest SOTA, which is a red flag. Also, the inconsistencies in results and unclear presentation add to the concerns. \n\nSo, despite the problem's importance, the paper's shortcomings in novelty, empirical validation, and methodology make it not suitable for acceptance at a top conference like ICLR.\n</think>\n\nThe paper proposes using pretrained structural embeddings from OmegaFold combined with graph neural networks (GNNs) for protein-protein interaction (PPI) prediction. While the motivation is valid—structural information is underutilized in PPI tasks—the reviewers consistently highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n### Key Issues:\n1. **Novelty and Technical Contribution**:\n   - The method lacks innovation. It directly adopts OmegaFold’s pre-trained embeddings and uses standard GNN architectures (e.g., GraphSAGE, GAT) without meaningful architectural or methodological advancements. Reviewers note this as a \"simple combination\" of existing techniques (Reviewer 2) and \"limited contribution\" (Reviewer 1).\n   - The core idea—using structural embeddings for PPI—is not novel, and the paper does not propose a new pretraining method for structures (Reviewer 1, 3, 4).\n\n2. **Empirical Weaknesses**:\n   - **Weak Baselines**: The paper compares only to basic GNN variants (GraphSAGE, GAT) and a simple ResNet on sequences, ignoring state-of-the-art PPI methods like dMaSIF (Reviewer 3, 4). This makes it impossible to assess the method’s true competitiveness.\n   - **Marginal Improvements**: The reported gains (e.g., ~5% improvement) are small and inconsistent across datasets (Reviewer 1, 3). The lack of confidence intervals and proper statistical validation further weakens the empirical claims (Reviewer 3).\n   - **Missing Comparisons**: No comparison to AlphaFold-Multimer, a prominent PPI prediction tool, is a major oversight (Reviewer 4).\n\n3. **Methodological Concerns**:\n   - **Handling of Structural Embeddings**: The use of mean pooling to create fixed-length embeddings risks losing critical local structural information critical for interactions (Reviewer 1).\n   - **GNN Necessity**: The GNN’s role is unclear. The task could be framed as pairwise interaction prediction without a graph, yet the paper does not justify the GNN’s added value (Reviewer 4).\n   - **Negative Sampling**: The training and evaluation setup (e.g., how negative edges are sampled) is poorly explained, raising questions about experimental rigor (Reviewer 4).\n\n4. **Clarity and Presentation**:\n   - The paper suffers from unclear explanations (e.g., how AUC is computed for PPI classification) and poorly formatted tables without confidence intervals (Reviewer 3).\n   - Overemphasis on OmegaFold’s details distracts from the paper’s core contributions (Reviewer 4).\n\n### Strengths:\n- The problem of leveraging structural embeddings for PPI is important and timely.\n- The transferability of embeddings across species shows promise but is underexplored.\n\n### Conclusion:\nWhile the paper addresses a relevant problem, its lack of technical novelty, weak empirical validation, and failure to engage with state-of-the-art baselines render it insufficient for a top-tier conference like ICLR. The method is a straightforward extension of existing techniques without meaningful innovation, and the experiments do not convincingly demonstrate significant progress in the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper on data leakage in tabular federated learning. Let me start by going through all the information provided.\n\nThe paper's title and abstract mention that they're addressing data leakage in federated learning (FL) specifically for tabular data, which is different from the image and NLP domains where such attacks have been studied before. The key challenges they identify are the mixed discrete-continuous nature of tabular data, high variance in reconstructions, and difficulty in assessing quality. Their solution, TabLeak, uses a softmax structural prior, pooled ensembling, and an entropy measure for quality assessment. The experiments show improvements over baselines, especially on the Adult dataset with a 10% boost in accuracy at batch size 32.\n\nNow, looking at the reviewer comments. Reviewer 1 is critical about the paper's writing quality, calling it poorly written with grammar issues and lack of clarity. They also say the methods are standard and not novel. However, they acknowledge the topic's importance. Their technical novelty and significance ratings are low (2), and correctness is 3 (minor issues). \n\nReviewer 2 points out that the technical contributions are weak. They mention that softmax relaxations and ensemble techniques are common, and the baselines are weak. They also suggest that more sophisticated optimization methods exist (like Gumbel trick or branch-and-bound) which the authors didn't consider. Their technical and empirical novelty scores are even lower (1), but they give correctness a 4. \n\nReviewer 3 is more positive. They acknowledge the problem's novelty for tabular data and the thorough experiments. They have some concerns about the claims regarding the challenges (like the mix causing higher variance) and suggest some improvements. Their technical novelty is 3 (somewhat new) and empirical is 4 (significant and new). Correctness is 3.\n\nThe author responses address several points. They provided additional experiments to validate the variance issue with mixed features, explained why Gumbel-Softmax isn't better, and clarified some methodological choices. They also fixed notational inconsistencies and added more details in the appendix.\n\nSo, the main points of contention are novelty and writing quality. Reviewer 1 and 2 think the methods are not novel enough, using standard techniques. Reviewer 3 thinks it's novel in applying these to tabular data and the problem itself is new. The authors have addressed some of the concerns with additional experiments and clarifications.\n\nThe paper's contribution seems to be the first comprehensive attack on tabular FL data, addressing specific challenges. Even if the techniques are not entirely new, their adaptation and validation in a new domain (tabular) could be significant. The experiments are thorough, showing improvements over baselines, which is a strong point. The writing issues were noted, but the authors have made efforts to clarify and fix them, especially in the rebuttal.\n\nTop-tier conferences often value novel applications and thorough empirical validation even if the techniques are not groundbreaking. The problem of data leakage in tabular FL is important, especially since such data is used in high-stakes areas like finance and law. The authors' rebuttal strengthens their case by addressing reviewer concerns with additional experiments and explanations.\n\nConsidering all this, the paper's novelty in applying existing methods to a new and important problem, along with strong empirical results, might outweigh the criticism about method novelty. The writing issues seem manageable, especially if the authors can polish the text as suggested. The reviewers' scores are mixed, but the empirical significance (Reviewer 3's 4) and the problem's relevance tip the balance towards acceptance.\n</think>\n\nThe paper addresses the critical issue of data leakage in federated learning (FL) for tabular data, a domain with unique challenges compared to images or text. While the proposed method (TabLeak) leverages existing techniques like softmax relaxation and ensembling, its novelty lies in adapting these to the tabular setting, which involves mixed discrete-continuous features and structured data. The authors demonstrate empirical effectiveness through thorough experiments, showing significant improvements over baselines (e.g., 10% accuracy gain on Adult dataset). Key strengths include:  \n1. **First-of-its-kind work**: The paper is the first to systematically study and propose a reconstruction attack for tabular FL, addressing domain-specific challenges (e.g., mixed data types, entropy-based quality assessment).  \n2. **Empirical rigor**: The experiments are extensive, with ablation studies, synthetic datasets, and validation of claims (e.g., variance reduction via pooling). The rebuttal further strengthens these with additional analyses (e.g., synthetic datasets confirming mixed-type variance).  \n3. **Practical significance**: Tabular data is central to high-stakes applications (e.g., finance, healthcare), making this vulnerability critical to expose.  \n\n**Weaknesses**:  \n- **Novelty debates**: Reviewers 1 and 2 argue that the methods (softmax, ensembling) are standard, but Reviewer 3 acknowledges the novelty of applying them to tabular data and the problem’s uniqueness. The authors’ rebuttal clarifies that prior work (e.g., text attacks) does not directly apply due to differences in data structure and model architectures.  \n- **Writing concerns**: While the initial draft had clarity issues, the authors addressed notational inconsistencies and grammatical errors in the rebuttal, suggesting these can be resolved.  \n\n**Key rebuttal improvements**:  \n- Validated the claim about mixed-type variance with synthetic experiments.  \n- Demonstrated that Gumbel-Softmax underperforms softmax in this context.  \n- Added experiments on network size and linear models, addressing reviewer critiques.  \n\n**Conclusion**: The paper’s empirical contributions and problem relevance outweigh the methodological incrementalism. Top-tier conferences often prioritize impactful applications and thorough validation, even if techniques are adapted from prior work. The authors have sufficiently addressed concerns about novelty and rigor in their rebuttal.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper is about protein sequence and structure co-design using an equivariant translation approach. The key claims are that their method, ProtSeed, outperforms existing methods in terms of both speed and accuracy, with a one-shot update of amino acids, which reduces inference time. \n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewers 1, 2, 3, and 4 seem to have some concerns but are generally positive, especially after the author responses. Reviewer 1 is satisfied and even increased their score. Reviewer 3 pointed out the need for clearer equations for SeqIPA, which the authors addressed by adding them. Reviewer 4 had some confusion about the model's capabilities but the authors clarified that they can't generate structure conditioned on sequence in prior work, which was a point of clarification.\n\nHowever, Reviewers 5, 6, and 7 have more significant concerns. Reviewer 5 mentions several weaknesses: lack of hyperparameter details, no code release, unclear relevance of some tasks, discrepancies in prior work results, and the low AAR scores. They also question the functional evaluation, noting that structural metrics might not reflect real-world functionality. The authors addressed some of these by adding ablation studies and promising code release upon acceptance, but the hyperparameters and training details are still missing in the current submission. \n\nReviewer 6 is more critical, giving lower scores for correctness and novelty. They argue that the method isn't as novel as claimed, pointing out that parts are derived from AlphaFold2. They also question the decoder's handling of predicted amino acids and the lack of clear SeqIPA equations, which the authors addressed in their response. However, the reviewer still feels the contributions are marginal. The training cost and lack of ablation studies were concerns, but the authors provided training speed comparisons and added ablation studies in the appendix. \n\nReviewer 7 is positive about the empirical results and benchmarking but notes the lack of comparisons with newer models like ESM-IF and the absence of ablation studies. The authors added ablation studies, which should address part of this. \n\nReviewer 8 is supportive, seeing the method as a good contribution despite similarities to prior work. \n\nThe authors' responses seem to have addressed many of the reviewers' points, especially regarding clarity, ablation studies, and code release. They added details on SeqIPA, clarified claims about prior work, and provided training cost info. However, some issues remain, like the lack of hyperparameters in the main paper and the AAR scores being low. \n\nThe key points for acceptance at a top-tier conference would be the method's novelty, empirical validation, and reproducibility. The paper shows strong empirical results, addresses reviewer concerns through ablations and clarifications, and promises code release. The technical novelty is somewhat limited (as noted by some reviewers), but the problem addressed is important, and the speed improvement is a significant practical contribution. \n\nThe main remaining issues are the missing hyperparameters and training details in the main paper, which could hinder reproducibility. However, the authors mentioned adding a reproducibility statement and plan to release code and data. The low AAR scores are a concern, but the authors might argue that their method still outperforms others in this metric. \n\nConsidering all this, the paper's contributions seem sufficient for a top-tier conference, especially with the improvements and clarifications made. The reviewers' scores, especially after author responses, lean towards acceptance, with only a couple of minor unresolved issues that might be fixable in the final version.\n</think>\n\nThe paper proposes ProtSeed, a novel approach for protein sequence and structure co-design that addresses computational inefficiencies of existing methods (e.g., autoregressive and diffusion models) through iterative, one-shot updates. The method achieves strong empirical results across multiple benchmarks, with faster inference and improved metrics over prior work. Key strengths include clear motivation, strong empirical validation, and technical contributions such as the trigonometry-aware encoder and SeqIPA module. \n\n**Strengths:**\n1. **Empirical Performance:** The paper demonstrates significant improvements over state-of-the-art baselines in terms of PPL, RMSD, and AAR, with notable speedups in inference time. These results are critical for practical applications in protein design.\n2. **Technical Novelty:** While some components (e.g., IPA modules) draw from AlphaFold2, the integration of iterative, cross-conditioned sequence-structure updates and the one-shot translation mechanism represent meaningful advancements for co-design tasks.\n3. **Clarity and Reproducibility:** The authors addressed reviewer concerns by adding ablation studies, clarifying methodological details (e.g., SeqIPA equations), and committing to code/data release. These steps improve transparency and reproducibility.\n\n**Weaknesses:**\n1. **Novelty Concerns:** Reviewers noted that parts of the method (e.g., IPA adaptations) are derivative of prior work, and the co-design claim is nuanced (e.g., Jin et al. 2021 can be adapted with effort). However, the iterative cross-conditioning framework and one-shot updates are distinct contributions.\n2. **Hyperparameters and Training Details:** The main paper lacks specific hyperparameter and training details, which could hinder reproducibility. While the authors provided some training cost data, full details are still pending.\n3. **Functional Evaluation:** Metrics like RMSD and AAR may not fully capture functional utility. The authors acknowledged this but did not provide functional benchmarks (e.g., binding affinity), which is a limitation for real-world impact.\n\n**Reviewer Feedback:**\n- **Positive Outcomes:** Most reviewers (1–4, 7–8) supported acceptance after author clarifications. Reviewers 5 and 6 raised valid concerns but did not outright reject the work. The authors addressed many issues (e.g., ablations, SeqIPA equations, code release).\n- **Remaining Concerns:** The lack of hyperparameters in the main text and the low AAR scores (≤40%) remain unresolved. However, the authors’ commitment to releasing code and the strong empirical results mitigate these concerns.\n\n**Conclusion:** The paper’s technical contributions, empirical rigor, and clear improvements over prior work align with the standards of a top-tier conference. While there are valid critiques about novelty and functional validation, the method’s practical benefits (speed, accuracy) and thorough revisions in response to reviews justify acceptance. The unresolved issues (e.g., hyperparameters) are minor and can be addressed in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about online black-box adaptation to label-shift when there's also conditional-shift. The reviewers have given mixed feedback. Let me start by going through each reviewer's comments and the author responses to see the strengths and weaknesses.\n\nFirst, Reviewer 1 points out several issues. The main problem is the lack of a systematic approach to the distribution shifts considered. They mention that the experiments are modest and don't cover enough scenarios, making it hard to draw general conclusions. Also, there are undefined terms and unclear sections, like the Bayesian part. The authors addressed some of these by adding more background and clarifying notation, but the core issue of limited empirical scope remains. The reviewer thinks the paper needs improvements but isn't ready yet.\n\nReviewer 2 says the heuristics don't show significant improvements over the original method. For example, in some datasets, the proposed methods perform similarly or worse. They also note that Heuristic 3 is a common practice, so not novel. The authors tried to justify their heuristics but the empirical results still don't show strong gains. The reviewer is skeptical about the novelty and effectiveness.\n\nReviewer 3 is more critical. They mention unclear writing, especially in the algorithm design and notation. The validation set's role is confusing, and they question the necessity of comparing to a baseline using the validation set directly. The authors added some comparisons but the core issues about the method's reliance on label-shift assumptions and lack of theoretical backing remain. The reviewer suggests rejection.\n\nReviewer 4 emphasizes the lack of self-contained explanations and theoretical justifications. The Bayesian section is unclear, and the paper feels preliminary. The authors tried to clarify but the technical depth and reproducibility are still lacking. The reviewer thinks it's not ready for a top conference.\n\nLooking at the author responses, they addressed some clarity issues and added more details. They also included new experiments comparing to an oracle baseline. However, the main criticisms about the limited empirical scope, marginal improvements, and lack of theoretical grounding persist. The authors acknowledge these but argue that the paper is an initial exploration. \n\nThe problem the paper addresses is important—label-shift with conditional-shift is a real-world issue. But the contributions are mostly heuristic extensions without strong theoretical backing. The experiments don't conclusively show significant benefits, and the methods are incremental. Top-tier conferences usually require stronger contributions, clear experiments, and thorough analysis. \n\nConsidering all this, the paper's empirical results are underwhelming, the methodology isn't novel enough, and there are unresolved clarity issues. While the problem is relevant, the current work doesn't meet the high standards needed for acceptance. The reviewers' consensus leans towards rejection, especially since the authors couldn't address all major concerns sufficiently.\n</think>\n\nThe paper addresses an important problem of adapting models to label-shift in the presence of conditional-shift, which is relevant for real-world deployments. However, the reviewers' critiques highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n1. **Empirical Limitations**: \n   - The experiments are described as modest in scope, lacking sufficient diversity in shifts, datasets, and baselines. Key comparisons (e.g., with offline methods like CORAL or optimal fixed classifiers) are missing, making it hard to assess the proposed methods' advantages.\n   - Results show only marginal improvements over existing methods (e.g., Wu et al. 2021), with some datasets showing no benefit or even worse performance. The authors acknowledge these limitations and frame the work as exploratory, but top-tier venues typically require stronger empirical validation.\n\n2. **Methodological Weaknesses**:\n   - The proposed heuristics (e.g., OOD validation, confusion matrix adjustments) are ad-hoc and lack theoretical justification. Reviewers note that these ideas are either common practices (e.g., adding scalars to matrices) or extensions that do not fundamentally address the problem’s challenges.\n   - The Bayesian framework introduced in Section 4 is poorly motivated and unclear, with undefined notation and limited applicability beyond toy examples.\n\n3. **Clarity and Reproducibility**:\n   - The paper suffers from poor writing, undefined terms (e.g., \"conditional shift\"), and inconsistent notation (e.g., conflicting uses of \\( f \\)). Despite author responses, the Bayesian section and algorithm descriptions remain confusing.\n   - Reproducibility is hindered by missing implementation details (e.g., surrogate loss specifics, optimizer choices) and lack of code.\n\n4. **Novelty and Significance**:\n   - Contributions are incremental. The extension to regression and handling non-invertible confusion matrices are noted but underdeveloped. The core idea of using OOD validation is not novel and lacks a clear rationale for its effectiveness in non-label-shift scenarios.\n   - The problem is framed ambiguously (e.g., conflating \"conditional shift\" with general distribution shifts), weakening its theoretical grounding.\n\nWhile the problem is well-motivated and the paper’s honesty about limitations is commendable, the combination of underwhelming empirical results, lack of theoretical depth, and poor presentation makes it insufficient for a top-tier conference. The reviewers’ consensus, particularly their emphasis on the work’s preliminary nature and lack of significant contributions, aligns with this conclusion.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about using binary labels as an auxiliary task in multi-task learning for image classification. The main idea is that representing class labels as binary vectors instead of one-hot might help models learn better. The reviewers have given mixed feedback. \n\nStarting with the strengths: Reviewer 1 and 3 mention the novelty of the approach, especially since label representations are understudied. The experiments show some improvements in accuracy, especially on certain datasets and with limited data. The method is simple to implement as an auxiliary task, which is a plus.\n\nBut the weaknesses are significant. Reviewer 1 points out that the experimental section is weak. They didn't compare against enough baselines, like dense labels or other multi-task methods mentioned in related work. Also, the clarity issues are a problem—terms like \"Metabalance\" aren't explained, and the method isn't clearly described. Reproducibility is an issue too because key details are missing.\n\nReviewer 2 is harsher, saying the claims aren't well-supported and the experiments are insufficient. They also note notation errors and inconsistencies in the math, like the example with N=4 leading to a wrong number of groups. That's a red flag because it suggests possible sloppiness in methodology.\n\nReviewer 3 thinks the contribution is marginal. The improvements are small, and there's no deep analysis of why the method works. They also mention missing related work on label coding, which the authors didn't cite. That's a problem because it shows the novelty might not be there.\n\nReviewer 4 highlights the lack of technical depth. The multi-task setup is basic, and the paper doesn't explain why binary labels help. Also, experiments are on smaller datasets like CIFAR-100 and not ImageNet, which might limit the significance. Plus, implementation details are missing, making reproducibility hard.\n\nThe authors didn't respond, which might mean they didn't address these concerns. The key issues here are the lack of thorough experiments, unclear explanations, and insufficient related work. While the idea is somewhat novel, the execution and validation aren't strong enough for a top-tier conference. The improvements are modest, and without better analysis or more convincing results, it's a borderline case. But given the multiple reviewers pointing out critical flaws in experiments and clarity, I think it's safer to reject unless major revisions are done, which the authors haven't had a chance to fix here.\n</think>\n\nThe paper proposes an interesting idea of using binary label representations as an auxiliary task in multi-task learning to improve model performance. While there are some strengths, such as the novelty of the approach and modest empirical improvements on certain datasets, several critical weaknesses undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty**: The use of randomly generated binary labels as an auxiliary task is a novel contribution, particularly in the understudied area of label representation.\n2. **Empirical Results**: The method shows improvements in accuracy on specific datasets (e.g., FGVC-Aircraft, Stanford Cars) and in data-limited scenarios, suggesting potential practical utility.\n3. **Simplicity**: The approach is straightforward to implement as an extension to existing architectures.\n\n### Major Weaknesses:\n1. **Clarity and Reproducibility**:\n   - The paper suffers from poor clarity, with undefined terms (e.g., \"Metabalance\"), ambiguous descriptions of the binary label generation process, and missing implementation details (e.g., optimizer, learning rate schedules).\n   - Reproducibility is hindered by incomplete descriptions of experimental setups and hyperparameters (e.g., task weighting strategies).\n   - Figures are low-resolution, and equations contain errors (e.g., summations instead of averages in loss functions).\n\n2. **Weak Experimental Rigor**:\n   - **Baselines and Comparisons**: The paper fails to compare against relevant baselines (e.g., dense labels, label smoothing, or multi-task methods using semantic groupings). It also does not evaluate against state-of-the-art (SOTA) methods on standard benchmarks like ImageNet.\n   - **Analysis**: There is no systematic analysis of why the method works (e.g., ablation studies on binary label generation, sensitivity to random seeds, or the role of task weighting). The authors do not explore scenarios like distribution shifts or robustness, which are critical for a top-tier submission.\n   - **Dataset Scope**: Experiments are limited to smaller or specialized datasets (e.g., CIFAR-100, Caltech101), with no results on large-scale benchmarks like ImageNet. This limits the generalizability of the claims.\n\n3. **Technical Depth and Related Work**:\n   - The paper overlooks critical related work on label representations (e.g., label smoothing, error-correcting output codes) and unsupervised meta-learning methods that use random label groupings. This undermines the novelty and contextualization of the contribution.\n   - The motivation for binary labels is underdeveloped. The authors do not clearly articulate the problem they are solving (e.g., why random binary groupings improve performance) or explore semantic alternatives to random groupings (e.g., using class hierarchies).\n\n4. **Marginal Impact**:\n   - The improvements reported are small (e.g., 1-2% accuracy gains on some datasets) and inconsistent across tasks. For a top-tier conference, stronger empirical results or deeper theoretical insights are typically required.\n   - The method’s applicability is limited to multi-class classification, with no exploration of multilabel or other modalities.\n\n### Reviewer Consensus:\n- **Reviewers 1, 2, and 4** highlight critical flaws in experimental rigor, reproducibility, and clarity. Reviewer 2 explicitly states the claims are unsupported, while Reviewer 4 questions the technical significance.\n- **Reviewer 3** acknowledges the novelty but deems the contribution marginal due to small gains and lack of analysis.\n- The absence of an author response to address these concerns further weakens the submission’s case.\n\n### Final Decision:\nWhile the core idea has potential, the paper falls short in critical areas: experimental thoroughness, clarity, and engagement with related work. The modest empirical gains and lack of deeper analysis fail to meet the high bar for a top-tier conference. Addressing these issues would require substantial revisions beyond the current submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about Scrunch, a method for privacy-preserving representation learning in MLaaS. The goal is to prevent sensitive info leakage while maintaining task accuracy. The reviewers have some strong points against it.\n\nFirst, looking at Reviewer 1. They mention the idea isn't new—splitting the network into client and server parts isn't novel. The experiments are limited to two datasets and only one baseline (Shredder). They also point out that comparing with FHE approaches would be important but missing. The novelty is incremental, and the paper has some grammatical issues. The reviewer thinks it's not up to ICLR's standards because it's too incremental and the experiments aren't thorough enough.\n\nReviewer 2's comments are more critical. They point out serious flaws in the threat model. The method requires server access during training, which might allow the server to reconstruct the encoder via model inversion attacks. Also, the server could manipulate the loss function (like turning off center loss), undermining privacy. The technical approach has ambiguities, like confusing intra and inter-class distances. The experiments are on simple models (LeNet, VGG) and datasets (MNIST, CelebA), which might not be representative. The MNIST task isn't clearly defined, and the private task choice (smiling vs. gender) might not be the most challenging. The reviewer also notes that the method is similar to split learning but doesn't cite it, suggesting lack of awareness of prior work. The conclusion is that the technical flaws make the results unconvincing.\n\nReviewer 3 adds that the security model isn't well-defined. They question how the features can still leak info, like in time-series data from wearables. Also, for NLP, frequent words might be deducible from the encoded vectors. The authors didn't address encryption beyond the encoder, so the method might not be sufficient. The reviewer thinks it's a good idea but not enough for real-world privacy.\n\nAll reviewers agree on the lack of novelty and insufficient experiments. Reviewer 2's points about the threat model being flawed are particularly damning because if the server can subvert the privacy measures during training, the whole approach is compromised. The empirical evaluation is too limited, using shallow networks and not challenging scenarios. The paper also doesn't compare with important baselines like FHE or split learning, which are relevant. The technical errors (like confusing intra/inter-class distances) and unclear problem definitions (MNIST's private task) hurt credibility. The authors didn't respond, so they couldn't address these points. \n\nConsidering top-tier standards, the paper needs strong novelty, thorough experiments, and solid technical soundness. Here, the method is incremental, the experiments are weak, and there are fundamental flaws in the threat model. Without addressing these, it's unlikely to meet the bar. So, the decision should be to reject.\n</think>\n\nThe paper proposes a privacy-preserving representation learning method called Scrunch for MLaaS, aiming to prevent sensitive attribute inference while maintaining task accuracy. However, the reviewers' critiques highlight critical issues that undermine its suitability for a top-tier conference:\n\n1. **Technical Flaws in Threat Model**:  \n   Reviewer 2 identifies severe conceptual flaws. The approach assumes the server (which controls the loss function and training process) acts honestly, but the server could subvert privacy guarantees by manipulating the loss (e.g., disabling center loss) or reconstructing the encoder via model inversion attacks. This invalidates the core privacy claims, as the server retains full access to critical components (e.g., class labels, centers) during training. The paper fails to address how the server’s adversarial behavior is mitigated, rendering the proposed method’s privacy guarantees untenable under realistic threat models.\n\n2. **Lack of Novelty and Incremental Contribution**:  \n   Reviewers 1 and 2 note that splitting neural networks into client/server components (a key idea here) is not novel (e.g., split learning is unmentioned). The addition of center loss to control information flow is incremental, with no clear \"aha\" moment or foundational innovation. The paper does not sufficiently distinguish itself from prior work like Shredder or FHE-based methods, which are not adequately compared.\n\n3. **Weak Empirical Evaluation**:  \n   The experiments are limited to two shallow architectures (LeNet-5, VGG-16) and datasets (MNIST, CelebA). The MNIST experiments lack clarity on the private task, and the CelebA scenario (gender vs. smiling) is not representative of real-world privacy risks (e.g., identity inference). The absence of comparisons with stronger baselines (e.g., FHE, split learning) and modern architectures (e.g., Vision Transformers) weakens the empirical significance. The choice of hyperparameters (e.g., lambda) is under-explored, leaving critical design decisions unvalidated.\n\n4. **Clarity and Technical Errors**:  \n   Reviewer 2 highlights ambiguities in the paper’s technical descriptions (e.g., confusing intra/inter-class distances in the loss function), which obscure the method’s design. Reviewer 3 questions the lack of a formal security model and the feasibility of the approach for non-image domains (e.g., NLP or time-series data), where encoded features might still leak sensitive patterns.\n\n5. **Author Response**:  \n   The absence of an author response prevents addressing these critiques, leaving unresolved concerns about the method’s validity and rigor.\n\n**Conclusion**: The paper’s conceptual flaws, incremental contributions, and insufficient empirical validation render it unsuitable for a top-tier conference like ICLR. The threat model’s vulnerabilities and lack of meaningful comparisons with state-of-the-art methods further diminish its impact. Without substantial revisions to address these issues, the work does not meet the standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper introduces DeepSeaProbLog, an extension of existing neural-symbolic systems like DeepProbLog, to handle both discrete and continuous variables. The key contributions are the language semantics, an implementation with inference and learning, and experiments. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewers 1, 3, 4, and 7 seem to acknowledge the novelty and significance but point out issues with empirical evaluation and clarity. Reviewers 2, 5, 6, and parts of others are more critical, especially about the empirical sections being too limited and the work being incremental.\n\nFirst, the technical novelty. The authors claim they're extending to continuous variables, which is a clear gap in existing NPLP systems. Reviewers 3 and 4 agree this is a significant contribution, though some like Reviewer 6 question the novelty, saying techniques like reparameterization are known. However, the authors' response emphasizes their specific integration of WMI with differentiability, which hasn't been done before. That seems important.\n\nEmpirical evaluation is a big concern. Reviewers 2, 3, 5, and 6 note that the experiments are on toy tasks and lack comparisons in pure discrete settings. The authors address this by mentioning they used real data in one experiment and that in discrete cases, their system aligns with DeepProbLog. But the main experiments don't compare against existing baselines in discrete settings, which weakens the empirical significance. The MNIST subtraction example is seen as contrived by some reviewers, though the authors argue it's sufficient to show the method's flexibility.\n\nClarity and reproducibility: Some reviewers (like 6 and 7) found the paper unclear, especially on handling discrete variables and semantics. The authors added more details in their response, which might help, but the main text was initially lacking. The implementation will be released, which is good for reproducibility.\n\nScalability and performance trade-offs: Reviewers 4 and 6 point out that the approach uses sampling for continuous variables, which might be less efficient. The authors mention that exact methods are used for discrete parts and that they combine with existing techniques, but scalability isn't thoroughly tested. This is a potential downside but perhaps acceptable for a first work.\n\nThe key points for acceptance at a top conference would be the technical novelty and significance. The extension to continuous variables is a solid contribution, and the integration with differentiable WMI seems novel. However, the empirical evaluation is weak. Top conferences often require strong empirical validation, especially for a new method. The authors' responses address some concerns but the experiments still seem limited. \n\nOn the other hand, the paper's theoretical contributions are strong, and the problem they're addressing is important. The reviewers' scores on technical novelty are mostly 3 or 4, which suggests it's significant. The empirical novelty is lower, but maybe acceptable if the theory is strong enough.\n\nConsidering all this, the paper has enough technical merit and addresses an important gap. The empirical section could be better, but it's a starting point. Since the authors are addressing some issues in their response and the core idea is novel, I think it should be accepted with the hope that future work will expand the experiments. However, some reviewers (like 5 and 6) think it's too incremental. But given that the extension to continuous is a clear step forward, I lean towards accept.\n</think>\n\nThe paper introduces DeepSeaProbLog, a neural probabilistic logic programming framework that extends existing systems like DeepProbLog to handle both discrete and continuous variables. This addresses a critical gap in neural-symbolic AI, as prior work was limited to discrete domains. The technical contributions include novel semantics linking the language to weighted model integration (WMI), a differentiable implementation of WMI, and theoretical results on relaxation-based gradients. These innovations are significant and novel, particularly the integration of differentiable WMI, which reviewers acknowledge as a meaningful advancement.\n\nHowever, the empirical evaluation is a major weakness. The experiments focus on toy tasks (e.g., MNIST subtraction) and lack comparisons in purely discrete settings, where DeepSeaProbLog should theoretically match or outperform existing methods like DeepProbLog. While the authors address some concerns by mentioning real-world data in one experiment and clarifying discrete variable handling, the overall evaluation remains underdeveloped. Reviewers also highlight issues with clarity, particularly around discrete variable semantics and implementation details, though the authors’ responses suggest improvements.\n\nDespite these flaws, the core technical contributions are strong and address an important problem. The extension to continuous domains is non-trivial and fills a notable void in the literature. While the empirical validation could be stronger, the theoretical underpinnings and potential impact justify acceptance at a top-tier conference. The authors’ commitment to releasing code and addressing reviewer feedback further supports this decision.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about the average-case performance of no-regret learning dynamics, specifically q-replicator dynamics (QRD), in potential games. The main contributions seem to be analyzing how these dynamics converge to Nash equilibria and evaluating their average performance by considering both the regions of attraction and the quality of the equilibria reached. The authors focus on 2x2 coordination games and provide some theoretical results and experiments comparing gradient descent (GD) and standard replicator dynamics (RD).\n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 mentions that while the problem is important, the results are partial and the paper doesn't fully meet the expectations set by the abstract. They also point out some minor issues like citation formatting and terminology. Reviewer 2 is more critical, arguing that the work is incremental compared to PP16, lacks sufficient discussion of prior work, and the results are limited to 2x2 cases. They initially wanted to reject but were convinced to raise their score after the authors addressed PP16 comparisons. Reviewer 3 also notes the limited scope, especially the partial results in 2x2 games and lack of formal proofs for general cases, but acknowledges the problem's importance. Reviewer 4 is more positive, seeing the problem as significant but notes the restricted game class and suggests the work might be better suited for a more specialized conference like EC.\n\nThe authors addressed several points in their response: they clarified the abstract, fixed citation formatting, and added comparisons with PP16. They argue their work extends PP16 by considering a broader class of games and comparing different dynamics, which PP16 didn't do. They also mention initiating a new line of research on average performance beyond convergence.\n\nNow, considering top-tier conference standards, the key factors are novelty, significance, and the depth of contributions. The paper's main issue seems to be that while the problem is important, the results are quite limited (2x2 games) and build on prior work (PP16) without sufficiently distinguishing their contributions. However, the authors have made efforts to clarify their differences and expand the scope beyond PP16, especially in comparing different dynamics and showing some general results for potential games. The theoretical results, though partial, do provide a foundation for future work, which the authors emphasize as initiating a new direction. The empirical analysis and framework they propose might still be valuable for the community, even if the results are preliminary.\n\nOn the other hand, the reviewers' concerns about incremental nature and lack of generalization are valid. Top conferences usually expect more substantial contributions. However, since some reviewers (like Reviewer 4) still recommend acceptance after revisions and others raised their scores, and given that the problem is recognized as important, the paper might still be borderline. The authors' response addressed several criticisms, which could tip the balance towards acceptance, especially if the conference values novel directions even with partial results. The fact that the technical content is correct and the problem is acknowledged as significant might outweigh the limitations in scope for a top-tier venue that values innovative research directions.\n</think>\n\nThe paper addresses an important problem in game theory and learning dynamics by exploring the average performance of no-regret algorithms beyond mere convergence guarantees. While the contributions are theoretically sound and the problem is recognized as significant, the work has notable limitations. The core results are restricted to 2x2 coordination games, and the analysis remains partial, with conjectures and empirical evidence filling gaps where formal proofs are lacking. Reviewers raised concerns about the incremental nature of the results compared to prior work (e.g., PP16), insufficient discussion of prior contributions, and the narrow scope of the findings. However, the authors addressed several critiques in their response, clarifying their contributions relative to PP16, improving the abstract, and fixing formatting issues. They also emphasized the paper’s role in initiating a new research direction focused on average-case performance, which reviewers acknowledged as valuable. While the results are preliminary and the significance is debated, the problem’s importance, the framework proposed, and the potential for future work align with the exploratory nature of top-tier conferences. The revisions and clarifications sufficiently address critical gaps, and the problem’s relevance to the community tips the balance toward acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new aggregation method for federated learning called FedPA, aiming to address both client drift and period drift. The reviewers have given mixed feedback. \n\nStarting with the strengths: Reviewers 1 and 2 mention that the paper identifies a new problem, period drift, which is a good contribution. The idea of using meta-learning for aggregation is novel, though some say it's only marginally so. The experiments show better performance on some datasets, which is a plus.\n\nNow the weaknesses. Reviewer 1 points out that the paper doesn't clearly distinguish between client and period drift, both in theory and experiments. That's a big issue because the problem they're solving isn't well-defined. Also, the aggregation uses a proxy dataset, which limits applicability and is similar to existing methods like FedET and DS-FL. They didn't compare with those, which is a problem. The experiments are on simple datasets (EMNIST and MovieLens) and models, so maybe not strong enough for a top conference. Plus, no code provided, which hurts reproducibility.\n\nReviewer 2 says the paper lacks a deep discussion on why meta-learning is better than other approaches like temporal regularization. The dynamic system analogy isn't helpful. Also, the architecture choice (two neural networks) isn't justified. They mention another related work (Jothimurugesan 2022) that should be compared. The technical novelty is okay but not high, and the writing needs polishing.\n\nReviewer 3 is concerned about the proxy dataset assumption and privacy issues since the server needs client model updates. The method seems ad-hoc, and the connection between period drift and FedPA isn't clear. They want more ablation studies and better experimental results (like lower accuracy on FEMNIST compared to prior work). Also, hyperparameters aren't well-explained, and code isn't open-sourced.\n\nReviewer 4 notes some typos and suggests testing on more datasets like LEAF. They also want to know the server's computational overhead, which is important for scalability in FL. The explanation of Equation 7 is unclear, and code availability is again an issue.\n\nOverall, the main issues are: \n1. The concept of period drift isn't well-defined or justified with experiments.\n2. Lack of comparisons with similar methods (FedET, DS-FL, Jothimurugesan 2022).\n3. The proxy dataset requirement limits practicality and hasn't been analyzed in ablation studies.\n4. Experiments are on small datasets and models, not showing strong results.\n5. Writing quality and reproducibility (no code) are problematic.\n6. The method's advantages over simpler approaches (like scalar weights) aren't demonstrated.\n\nThe paper has some novel ideas but the execution and validation are lacking. The reviewers' consensus seems to lean towards rejection because the contributions aren't significant enough, experiments aren't robust, and there are unresolved issues in the approach. Even though some see potential, the current state doesn't meet top-tier standards.\n</think>\n\nThe paper proposes FedPA, a meta-learning-based aggregation method for federated learning (FL) to address both client drift and a newly identified \"period drift.\" While the idea of tackling period drift is novel and the experiments show some improvements over baselines, several critical issues undermine its acceptance at a top-tier conference:\n\n### Key Strengths:\n1. **Novel Problem Identification**: The concept of \"period drift\" (non-IID data distribution shifts across communication rounds) is a novel contribution, potentially highlighting an under-explored aspect of FL.\n2. **Technical Approach**: The use of a parameterized aggregator via meta-learning introduces a fresh angle compared to traditional aggregation methods like FedAvg.\n3. **Empirical Results**: The method shows competitive performance on EMNIST and MovieLens, suggesting potential utility in certain scenarios.\n\n### Major Weaknesses:\n1. **Lack of Clarity and Justification**:\n   - The definition and theoretical grounding of \"period drift\" are vague. Reviewers 1 and 4 note that the paper fails to rigorously distinguish period drift from existing client drift or demonstrate its distinct impact through experiments.\n   - The motivation for using meta-learning over simpler alternatives (e.g., temporal regularization or scalar weighting) is under-explained. Reviewer 2 and 3 question the necessity of neural networks for aggregation, suggesting ad-hoc design choices.\n\n2. **Weak Experimental Rigor**:\n   - **Baselines and Comparisons**: The paper does not compare with closely related works like FedET, DS-FL, or Jothimurugesan et al. (2022), which address similar problems. This omission undermines the novelty and significance of FedPA.\n   - **Dataset and Model Limitations**: Experiments are limited to small-scale datasets (EMNIST, MovieLens) and simple models. Performance on more challenging datasets (e.g., CIFAR-100, Stack Overflow) is absent, raising doubts about scalability and generalizability.\n   - **Ablation Studies**: No ablation studies analyze the impact of proxy dataset distribution, aggregator architecture, or hyperparameters (e.g., learning rates, communication rounds). This limits understanding of FedPA’s robustness and design choices.\n\n3. **Practical and Ethical Concerns**:\n   - **Proxy Dataset Assumption**: The reliance on a proxy dataset for training the aggregator introduces a strong assumption that may not hold in real-world FL scenarios. Privacy risks arise if the server must access client model updates (contrary to FL’s privacy principles like SecAgg or differential privacy).\n   - **Reproducibility**: No code is provided, and hyperparameters (e.g., communication rounds, learning rates) are not justified, making replication difficult.\n\n4. **Writing and Technical Quality**:\n   - The paper suffers from grammatical errors, inconsistent citations, and unclear explanations (e.g., Equation 7’s \"dense()\" operator). Reviewer 4 notes duplicated text in the introduction.\n   - The dynamic systems analogy in Section 3 is deemed irrelevant by Reviewer 2, suggesting unnecessary complexity.\n\n### Conclusion:\nWhile FedPA introduces an interesting direction for addressing period drift, the paper’s lack of rigorous theoretical grounding, incomplete experimental validation, and failure to engage with relevant prior work significantly weaken its contributions. The unresolved practical concerns (e.g., proxy dataset dependency, privacy risks) and insufficient reproducibility further diminish its appeal for a top-tier conference. The reviewers’ consensus, particularly on technical and empirical marginality, aligns with a rejection decision.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether the paper \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" should be accepted or rejected for a top-tier conference like ICLR. Let me start by going through all the provided information step by step.\n\nFirst, the paper's abstract mentions that RuDar is a new dataset for precipitation nowcasting, which includes 3D radar echo observations, geographical data, and covers multiple climate zones over two years. The authors evaluated several baselines and discussed uncertainty quantification. The dataset is publicly available, which is a plus.\n\nLooking at the reviewer comments, there's a mix of positive and negative points. Let me break down each reviewer's feedback.\n\nReviewer 1 says the dataset is attractive but notes that the methods are low-cost and questions how heavy models like transformers would perform. They also mention limited domain relevance and novelty. The technical and empirical significance are rated low (2/4), but the paper is clear and reproducible.\n\nReviewer 2 appreciates the dataset's dimensions and coverage but criticizes the baselines as too naive, lack of experiments for other ML tasks, insufficient related work, and the use of MSE as the only metric. They suggest submitting to a journal instead. Correctness is 3/4, technical significance 3, empirical 2.\n\nReviewer 3 points out that the dataset isn't novel enough, lacks methodological innovations, and the uncertainty study is simple. They argue that while the dataset is useful, it's too similar to existing ones for an ICLR publication. Correctness is 4, technical significance 2, empirical 3.\n\nThe author responses addressed some points. They added Earthformer results in the rebuttal, acknowledged the need for more experiments on other ML tasks but said it's out of scope. They also explained the metrics and dataset limitations.\n\nNow, considering the criteria for a top-tier conference. The dataset's novelty is a key factor here. The authors claim it adds 3D data and orography, which existing datasets lack. However, reviewers 2 and 3 note that similar datasets exist, and the technical novelty is low. The main contribution is the dataset, but the reviewers feel it's not groundbreaking enough for ICLR, which typically requires more methodological innovation.\n\nThe baselines are criticized as too basic. The authors added Earthformer results, but even with that, the paper's focus is on the dataset and benchmarking existing methods. The lack of ablation studies and deeper analysis on other tasks (like anomaly detection) is a drawback. The evaluation metrics could be better, but the authors defended their choices.\n\nThe dataset's size and coverage are significant, which is a plus. The fact that it's publicly available is a strong point. However, the reviewers' consensus on low technical and empirical significance is concerning. The paper might be more suitable for a journal where the dataset's utility can be highlighted without needing the same level of novel methods.\n\nThe authors argue that the dataset's combination of variables (precipitation, reflectivity, velocity) is unique. But if existing datasets already cover some aspects, the incremental contribution might not meet the bar for a top conference. The paper's main weakness is the lack of methodological innovation, as all reviewers pointed out. The experiments are standard, and the uncertainty analysis is basic.\n\nIn top-tier venues, datasets often come with new methods or a significant shift in approach. Here, the dataset is a solid contribution but perhaps not transformative enough. The authors might have a better chance if they can highlight unique aspects more clearly or if the dataset's impact is proven through more advanced experiments. However, given the current state, the reviewers' concerns about novelty and significance are valid.\n\nFinal Decision: Reject\n</think>\n\nThe paper introduces the RuDar dataset, which adds 3D radar measurements and geographical data for precipitation nowcasting, addressing gaps in existing datasets. However, the reviewers consistently highlight critical limitations. While the dataset is well-constructed and publicly available, its technical and empirical novelty is deemed marginal. Reviewers note that similar datasets exist, and the dataset's coarse spatial resolution and lack of methodological innovation (e.g., no novel models or advanced uncertainty analysis) weaken its significance for a top-tier conference like ICLR. The baselines are considered basic, and the evaluation lacks depth (e.g., limited metrics, no cross-domain experiments). Though the authors addressed some points (e.g., adding Earthformer results), the core issue of insufficient novelty persists. The dataset’s utility is acknowledged but deemed better suited for a journal rather than a conference requiring higher innovation thresholds.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference based on the provided information. Let me start by going through all the details carefully.\n\nFirst, the paper's title and abstract. The topic is permuted linear regression and phase transitions using message passing. The abstract mentions they identify phase transition thresholds precisely, which is a step beyond previous work that only looked at convergence rates without constants. They also mention both cases where the signal B is known (oracle) and unknown (non-oracle). The experiments align with theory, which is good.\n\nNow, looking at the reviewer comments. There are five reviewers, each with their own take. Let's break them down one by one.\n\nReviewer 1 is critical. They say the paper is interesting but poorly written. The main issues are incremental results and lack of clarity. They point out that the paper isn't self-contained, relying too much on prior works like Semerjian and Zhang et al. The notation is heavy and confusing, especially for someone not familiar with those papers. They suggest reorganizing or submitting to a journal instead. The technical contributions are there but the presentation is a problem. The reviewer also notes several typos and unclear sections, like the MP section needing a rewrite. They give correctness a 3 (minor issues) and technical novelty a 3 (somewhat new but aspects exist in prior work). But the overall recommendation is reject because of the writing and organization.\n\nReviewer 2 is also negative. They mention the main contribution is the non-oracle threshold, but the oracle case is just a warm-up and not practically meaningful. The analysis for the oracle case is a corollary of existing theorems. The partial permutation part isn't well-motivated. Writing is messy, non-rigorous. They point out specific issues like Assumption 1 needing justification, unclear steps from Theorem 2 to Proposition 2, and formatting in tables. They also note the code isn't provided. Correctness is 2 (some claims incorrect), technical novelty 2 (marginal). Summary is reject.\n\nReviewer 3 is more positive. They think the paper has an interesting result on phase transitions but mentions typos and standard techniques. The methods are inspired by prior work. Clarity is okay but needs a grammar check. They give correctness 4 (all claims correct) and technical novelty 3. However, they have low confidence because they're not an expert in message passing. The recommendation is a bit uncertain but leans towards accept? Wait, the summary says \"Overall, I think this is an interesting paper...\" but the final decision isn't clear. Wait, looking back, the summary says \"Overall, I think this is an interesting paper that attempts to predict the relationships... Unfortunately I am not extremely familiar with the literature... my confidence is low.\" The final decision here isn't explicit, but the score might be a 6 (since the summary says \"I think this is an interesting paper\") but the confidence is low. Hmm, but the user's summary shows the reviewer's final decision as \"Overall, I think this is an interesting paper...\" but the scores are correctness 4, technical novelty 3. The user's summary says \"summary_of_the_review: Overall, I think this is an interesting paper...\" so maybe the recommendation is borderline but leans towards accept? Not sure, but the user's data shows the reviewer's final decision is \"Overall, I think this is an interesting paper...\" but the scores are mixed. Wait, the user's data for Reviewer 3's \"summary_of_the_review\" says \"Overall, I think this is an interesting paper that attempts to predict the relationships... Unfortunately I am not extremely familiar with the literature surrounding message passing algorithms, and so my confidence is low.\" The final decision isn't explicitly stated, but the scores are correctness 4, technical novelty 3. The user's data shows the reviewer's \"correctness: 4\", \"technical_novelty_and_significance: 3\", \"empirical_novelty_and_significance: 3\". So maybe the reviewer is leaning towards accept but with reservations.\n\nReviewer 4 is more positive. They like the analysis and the precise thresholds, which are novel. They mention the work is well-structured but needs clarity improvements. They point out the code isn't available, which affects reproducibility. They have several questions about the methodology and presentation. The technical contributions are good but some aspects are from prior work. They give correctness 3 (minor issues), technical novelty 3. The final decision is dependent on code release but overall positive. The summary says \"My score is due to the nice findings... My main concerns are about the lack of clarity and reproducibility. The present grade is dependent on the future release of the code.\" So maybe a conditional accept, but since it's a conference, they might still recommend accept with revisions.\n\nReviewer 5 is very negative. They find the writing terrible, making the paper inaccessible. They list many issues like unclear definitions, misuse of terms like \"framework\", and contradictions in claims (precise vs approximate). They argue the empirical vs theoretical results don't align because theory is asymptotic. They give correctness 1 (main claims incorrect), technical novelty 3. The summary is a strong reject, suggesting major revisions and resubmission elsewhere.\n\nNow, aggregating all this. The majority of reviewers (Reviewers 1, 2, 5) are against acceptance, with strong criticisms on writing, clarity, and incremental contributions. Reviewers 3 and 4 are more positive but with reservations. However, Reviewer 5's harsh critique on accessibility and correctness (score 1) is a big red flag. The paper's main issues seem to be:\n\n1. **Clarity and Writing**: Multiple reviewers point out the paper is poorly written, not self-contained, with heavy notation and lack of explanations. This is critical for a conference where accessibility is key.\n\n2. **Novelty**: Reviewers 1 and 2 suggest the contributions are incremental. The oracle case is a corollary of existing work, and the non-oracle uses approximations that might not be novel enough.\n\n3. **Technical Soundness**: Reviewer 5 says the main claims are incorrect, and others note minor issues. The empirical vs theoretical alignment is questioned (Reviewer 5's point about asymptotic vs finite samples).\n\n4. **Reproducibility**: Lack of code is mentioned by Reviewers 2 and 4, which is a problem for top conferences.\n\n5. **Organization**: The paper's structure is confusing, relying too much on prior works without sufficient explanation.\n\nEven though some reviewers see merit (like precise thresholds and analysis), the consensus on the writing and clarity is severe. Top-tier conferences require both strong contributions and clear exposition. The paper's current state, as per the reviews, doesn't meet these standards. The technical novelty is somewhat there but not groundbreaking, and the writing issues are too significant to overlook.\n\nTherefore, the decision should be to reject. The authors might need to significantly rework the paper, perhaps for a journal, where length and clarity can be improved, and then resubmit to a conference after addressing all these points.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Clarity and Writing Issues**: Multiple reviewers (1, 2, 5) emphasize that the paper is poorly written, overly reliant on prior works (e.g., Semerjian et al., Zhang et al.), and lacks self-contained explanations. Notations are heavy and confusing, making it inaccessible to readers unfamiliar with the cited literature. Typos, grammatical errors, and unclear sections (e.g., message passing derivations) further hinder readability. These issues are critical for a top-tier conference, where clarity and accessibility are paramount.\n\n2. **Novelty and Incremental Contributions**: \n   - Reviewers 1 and 2 highlight that the oracle case analysis is a corollary of existing results (e.g., Theorem 1 in prior work), and the non-oracle case relies on approximations (e.g., Taylor expansions) that are not sufficiently novel. \n   - The partial permutation recovery is poorly motivated, and the empirical claims (e.g., \"performance almost matches full recovery\") are contradicted by figures (e.g., Figure 3 shows significant SNR gaps). \n   - While the precise phase transition thresholds are a contribution, they are deemed incremental given prior work on convergence rates and phase transitions in similar problems.\n\n3. **Technical Soundness and Empirical Validation**:\n   - Reviewer 5 argues that the paper’s claims about empirical alignment with theoretical predictions are flawed because asymptotic results do not directly apply to finite-sample experiments. \n   - Reviewer 2 notes that the non-oracle threshold is only \"approximately identified,\" undermining the paper’s emphasis on \"precise\" thresholds. \n   - The lack of code (noted by Reviewers 2 and 4) hinders reproducibility, a key requirement for top conferences.\n\n4. **Structural and Organizational Flaws**:\n   - The paper’s structure is disjointed, with redundant sections (e.g., Section 3.1 overlapping with Semerjian et al.) and unclear connections between the oracle and non-oracle cases. \n   - Key terms (e.g., \"Ener\" in Table 2) and assumptions (e.g., Assumption 1) are undefined or poorly justified, further complicating comprehension.\n\n5. **Reviewer Consensus**: \n   - Four of five reviewers (1, 2, 4, 5) explicitly recommend rejection or strongly caution against acceptance. \n   - Even the more positive reviewers (3 and 4) acknowledge significant flaws in clarity, reproducibility, and technical rigor. \n   - Reviewer 5’s harsh critique of the paper’s accessibility and correctness (assigning a correctness score of 1) underscores fundamental issues that cannot be overlooked.\n\n### Conclusion:\nWhile the paper addresses an important problem and contributes to understanding phase transitions in permuted linear regression, its current form is not suitable for a top-tier conference. The writing, clarity, and organizational issues are severe enough to obscure the technical contributions, and the novelty is incremental relative to existing work. The authors are encouraged to revise the paper significantly, address the reviewers’ concerns, and resubmit to a journal or conference after substantial improvements."
    }
  ]
}