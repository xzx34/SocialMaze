{
  "summary": {
    "model": "qwen-2.5-72B",
    "total_papers": 100,
    "correct_predictions": 64,
    "accuracy": 64.0,
    "confusion_matrix": [
      [
        41,
        35
      ],
      [
        1,
        23
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 53.94736842105263,
        "precision": 97.61904761904762
      },
      "Accept": {
        "total": 24,
        "recall": 95.83333333333334,
        "precision": 39.6551724137931
      }
    }
  },
  "results": [
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Quality of Writing and Clarity\nThe paper is severely lacking in clarity and coherence. Both Reviewer 2 and Reviewer 3 highlight significant grammatical issues and incomprehensible passages. The abstract, which is crucial for summarizing the research, is poorly written and difficult to understand. This makes it challenging for readers to grasp the main contributions and methodology of the paper. Clear and professional writing is essential for any academic publication, especially in a top-tier conference.\n\n#### Lack of References\nThe paper does not include any references, which is a major oversight. References are crucial for situating the research within the existing literature, providing context, and demonstrating the authors' understanding of the field. The absence of references suggests a lack of thorough literature review and a failure to acknowledge prior work, which is a fundamental aspect of academic research.\n\n#### Methodological Clarity and Robustness\nThe proposed method is not clearly explained, and the experiments are not well-documented. Reviewer 1 and Reviewer 3 both mention that the method is difficult to understand and that the algorithm boxes are hard to follow. This lack of clarity in methodology and experimental setup makes it impossible to evaluate the robustness and effectiveness of the proposed approach. In a top-tier conference, the methodology must be well-defined and the results must be reproducible.\n\n#### Novelty and Contribution\nWhile the authors mention the idea of a \"meta-tree\" for improving Monte Carlo Tree Search (MCTS) in reinforcement learning, the novelty and contribution of this idea are not clearly articulated. The author response indicates that similar ideas have been found in the authors' country, but no international papers have been found. This suggests that the idea might not be entirely novel, and the authors need to provide a more thorough literature review to establish the uniqueness of their contribution.\n\n#### Author Response\nThe author response acknowledges the need for improvement in writing skills and the lack of references. While the authors show a willingness to improve, the current state of the paper is not suitable for publication in a top-tier conference. The response also indicates that the authors are aware of similar ideas in their country, which further emphasizes the need for a comprehensive literature review.\n\n### Final Decision\nFinal Decision: Reject\n\nThe paper is poorly written, lacks references, and does not clearly explain the methodology or experimental results. These issues are significant and would require substantial revisions to meet the standards of a top-tier conference. The authors are encouraged to improve their writing skills, conduct a thorough literature review, and clearly articulate their methodology and contributions before resubmitting the paper."
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel problem in the field of singing voice generation: generating singing voices without pre-assigned scores and lyrics. This is a significant contribution as it opens up new possibilities for creative and generative applications in music. The authors explore three different schemes: free singer, accompanied singer, and solo singer, each with its own unique challenges and potential applications.\n\n#### Methodology\nThe authors have developed a pipeline that includes data preparation, adversarial networks for audio generation, and customized metrics for evaluation. They have also provided a detailed description of their neural network architecture and the training process. The use of GANs and the adaptation of BEGAN for sequence generation are well-explained and justified.\n\n#### Evaluation\nThe initial evaluation was criticized for being weak and lacking context. However, the authors have made significant improvements in response to the reviewers' comments:\n1. **Baseline Comparisons**: The authors have added two baseline methods (Sinsy and Synthesizer V) to their evaluation, which provides a better context for assessing the performance of their models.\n2. **Objective Metrics**: They have expanded the objective metrics to include the training data and the baselines, making the results more interpretable.\n3. **Subjective Evaluation**: The authors have conducted additional MOS studies to evaluate the sound quality and expression of the generated singing voices, which complements the objective metrics.\n\n#### Data and Training\nThe authors have clarified the details of their training data, including the genres and the process of data preparation. They have also explained why they chose to use their in-house data and the reasons for the choices made in the data splits.\n\n#### Motivation and Scope\nThe authors have expanded the introduction to provide clearer motivations and potential applications of their work. They have also addressed the limitations and the scope of their research, which helps in understanding the context and the significance of their contributions.\n\n#### Literature Review\nThe literature review has been improved to include more neural network-based methods, which provides a better context for the proposed work.\n\n#### Minor Issues\nThe authors have addressed several minor issues and typos pointed out by the reviewers, which improves the overall clarity and readability of the paper.\n\n### Final Decision\nGiven the significant improvements in the evaluation, the clarity of the methodology, and the expanded motivations and scope, the paper is now well-positioned for publication in a top-tier conference. The novel problem, the detailed methodology, and the thorough evaluation make it a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel approach to learning hierarchical and disentangled representations using a progressive learning strategy. The key contributions are:\n1. **Progressive Learning Strategy**: The model starts with the most abstract representations and progressively adds new layers to capture more detailed information.\n2. **Hierarchical Representation**: The model uses a ladder architecture to capture representations at different levels of abstraction.\n3. **New Disentanglement Metric**: The authors propose a new metric, MIG-sup, to complement existing disentanglement metrics.\n\nThe novelty of the approach is significant, as it combines progressive learning with hierarchical representation learning, which is a fresh direction in the field of disentangled representations.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to follow. The motivation and methodology are clearly explained, and the experiments are well-documented. However, there are a few areas where the clarity could be improved:\n- **Purpose Clarification**: The authors need to clearly state whether the primary focus is on hierarchical representation or disentanglement. The current presentation sometimes conflates these two concepts.\n- **Formalization**: Some terms, such as \"z from different abstraction,\" need to be better formalized.\n\n#### Experiments and Results\nThe experiments are thorough and provide both qualitative and quantitative evidence of the model's effectiveness:\n- **Quantitative Results**: The authors use multiple disentanglement metrics and show improvements over baseline models on benchmark datasets.\n- **Qualitative Results**: The generated images and latent traversals are convincing and demonstrate the model's ability to learn disentangled representations.\n- **Ablation Studies**: The authors have conducted ablation studies to investigate the effects of different implementation strategies, which adds robustness to the results.\n\nHowever, there are a few concerns:\n- **Comparison with VLAE**: The initial results on MNIST and 3DShapes showed some weaknesses compared to VLAE, but the authors have addressed these concerns with additional experiments and provided a clearer comparison.\n- **Robustness**: The authors should further investigate the robustness of the model to hyperparameters, particularly the beta parameter.\n\n#### Addressing Reviewer Comments\nThe authors have responded well to the reviewer comments:\n- **Additional Experiments**: They have conducted additional experiments to address the concerns about the first layer's training and the information flow.\n- **Clarifications**: The authors have clarified the purpose of the paper and the relationship between hierarchical representation and disentanglement.\n- **Ablation Studies**: They have added ablation studies to investigate the effects of different implementation strategies.\n\n#### Overall Evaluation\nThe paper presents a novel and well-motivated approach to learning hierarchical and disentangled representations. The experiments are thorough, and the authors have addressed the reviewer comments effectively. While there are a few areas for improvement, the overall quality of the paper is strong.\n\n### Final Decision\nFinal Decision: Accept"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces NORML, a meta-learning framework that uses an LSTM-based meta-learner to perform neuron-wise optimization on a learner network. The key claim is that NORML can scale to large networks and handle a large number of update steps more effectively than existing methods like MAML and the Meta-Learner LSTM (Ravi & Larochelle).\n\nHowever, the novelty of the proposed method is questionable:\n1. **Comparison to Ravi & Larochelle**: The paper claims that the Meta-Learner LSTM requires an enormous number of parameters, which is not accurate. Ravi & Larochelle use parameter sharing to keep the number of meta-learner parameters constant relative to the learner network size. This misunderstanding undermines the paper's claim of significant improvement.\n2. **Incremental Improvement**: The proposed method is a relatively incremental variation of existing work. The use of neuron-wise optimization is an interesting idea, but it does not represent a substantial leap in the field.\n\n#### Experimental Evaluation\nThe experimental results are not convincing:\n1. **Mini-ImageNet**: The paper builds on the method of Sun et al. but does not show results from Sun et al. for comparison. This makes it difficult to evaluate the effectiveness of NORML.\n2. **Omniglot**: The experiment is performed on a fully-connected network, which limits the applicability of the method. The paper does not use a convolutional network, which is the standard for Omniglot, making the results less meaningful.\n3. **Comparison to MAML**: The paper claims that NORML can handle a large number of inner-loop updates better than MAML, but this is not validated experimentally. The experiments involve very few inner-loop steps, which does not support the claim.\n\n#### Writing and Presentation\nThe paper has significant issues with writing and presentation:\n1. **Grammar and Style**: The paper contains numerous grammatical errors and awkward phrasing, which detracts from the clarity and professionalism of the work.\n2. **Citations**: The citations are inconsistent and sometimes incorrect, which can lead to confusion.\n3. **Clarity of Ideas**: The flow of ideas is not well-structured, making it difficult to follow the arguments and contributions.\n\n#### Technical Soundness\nThe technical soundness of the paper is questionable:\n1. **Inaccurate Descriptions**: The paper contains several inaccurate descriptions of prior work, which can mislead readers.\n2. **Missing Baselines**: The paper lacks important baselines, such as a version of MAML with per-hidden-unit and per-inner-loop-step learning rates, and MAML++.\n3. **Limited Applicability**: The method is only tested on fully-connected networks, which limits its applicability to real-world problems where convolutional networks are more common.\n\n### Final Decision\nGiven the limited novelty, unconvincing experimental results, significant issues with writing and presentation, and technical inaccuracies, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Contribution and Novelty:**\nThe paper proposes two regularizers to improve the training of certifiably robust neural networks using convex relaxations. The key idea is to minimize the gap between the optimal value of the original non-convex problem and its convex relaxation. This is a novel approach that addresses a significant issue in the field of adversarial robustness, where the gap between certifiable and empirical robustness is often large.\n\n#### **2. Technical Soundness:**\n- **Reviewer 1** raises several technical concerns, particularly about the clarity and correctness of the theoretical analysis. The confusion around the optimal convex relaxation and the conditions under which the relaxation is tight is a significant issue. The authors have attempted to address these concerns in their response, but the clarity of the theoretical sections remains a concern.\n- **Reviewer 2** points out that the paper is often unclear about whether it is referring to the unrelaxed original problem or the tightest convex relaxation. The authors have clarified this in their response, but the original presentation was indeed confusing.\n- **Reviewer 3** notes some presentation issues and the limited dataset used (MNIST). The authors have addressed the presentation issues and added results on CIFAR10, which is a more challenging dataset.\n\n#### **3. Experimental Results:**\n- The experimental results are promising, showing that the proposed regularizers lead to tighter certification bounds and improved robustness compared to non-regularized baselines. The authors have added state-of-the-art results on both MNIST and CIFAR10, which strengthens the empirical validation of their approach.\n- However, the experiments could be further strengthened by including more baselines and datasets. The inclusion of results from Gowal et al. (2019) in Table 1 is a positive step, but more comprehensive comparisons would be beneficial.\n\n#### **4. Clarity and Presentation:**\n- The paper has some clarity issues, particularly in the theoretical sections. The authors have made efforts to improve the presentation, but the original version was quite confusing. The revised version should be more clear, but it is important to ensure that the theoretical sections are well-explained and easy to follow.\n- The authors have addressed the presentation issues noted by Reviewer 3, and the addition of an illustrative example in the appendix is a positive step.\n\n#### **5. Impact and Relevance:**\n- The problem of improving certified robustness is highly relevant and important in the field of machine learning, particularly for applications where robustness to adversarial attacks is crucial.\n- The proposed regularizers have the potential to significantly improve the state of the art in certified robustness, making the paper valuable for the community.\n\n### Final Decision\nDespite the initial technical and clarity issues, the authors have made substantial revisions to address the concerns raised by the reviewers. The experimental results are strong and demonstrate the effectiveness of the proposed regularizers. The problem addressed is highly relevant, and the paper has the potential to make a significant impact in the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Innovative Contribution**:\n   - The paper proposes a novel model, ROOTS, which extends the GQN framework to provide object-oriented 3D scene representation. This is a significant contribution to the field of 3D scene understanding and representation learning.\n   - The qualitative results are impressive, demonstrating the model's ability to decompose scenes into objects, handle occlusions, and generate new scenes by swapping objects.\n\n2. **Technical Soundness**:\n   - The method is technically sound and well-motivated. The use of a 3D grid for object representation and the hierarchical structure (3D global-scene level and 2D local-image level) are innovative and well-explained in the revised version.\n\n3. **Potential Impact**:\n   - The ability to learn object-oriented 3D representations unsupervised is a valuable contribution that could have significant implications for various applications, including robotics, augmented reality, and 3D scene understanding.\n\n#### Negative Aspects:\n1. **Clarity and Readability**:\n   - The initial version of the paper was poorly written, with numerous typos, inconsistent notation, and unclear explanations. While the authors have made significant improvements in the revised version, the paper still requires further refinement to ensure clarity and readability.\n\n2. **Experimental Rigor**:\n   - The experiments are somewhat weak and lack depth. The paper does not provide a comprehensive comparison with GQN, and the ablation studies are missing. This makes it difficult to assess the specific contributions of each component of the model.\n   - The claims about the model's performance are not always supported by the experimental results. For example, the claim that ROOTS has clearer generations than GQN is not substantiated by the provided figures.\n\n3. **Related Work**:\n   - The paper lacks a thorough comparison with existing work in 3D representation learning and unsupervised 3D object detection. The authors have added more references in the revised version, but further improvements are needed to position the work more clearly within the existing literature.\n\n4. **Complexity and Explanation**:\n   - The method is complex, and the authors need to provide a clearer and more detailed explanation of the model's components and how they interact. The revised version has made some improvements, but the paper still requires more clarity in the method section.\n\n### Final Decision\nGiven the significant contributions and the potential impact of the proposed model, the paper has the potential to be a valuable addition to the field. However, the current version of the paper still has several issues that need to be addressed, particularly in terms of clarity, experimental rigor, and thoroughness in related work.\n\nFinal Decision: [Reject]\n\nThe authors are encouraged to thoroughly revise the paper, addressing the issues raised by the reviewers, and resubmit for consideration in a future conference or journal."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\n1. **SGD for GMMs**: The paper proposes using Stochastic Gradient Descent (SGD) for training Gaussian Mixture Models (GMMs) in high-dimensional spaces. This is a novel approach compared to the traditional Expectation-Maximization (EM) algorithm, which is computationally expensive and memory-intensive for large datasets.\n2. **Max-Component Approximation**: The authors introduce a max-component approximation to the log-likelihood to improve numerical stability. While this is a reasonable approach, it is not entirely novel, as similar techniques (like log-sum-exp) are standard in numerical computations.\n3. **Regularization and Annealing**: The paper introduces a regularizer and an annealing mechanism to prevent convergence to pathological local minima. This is a significant contribution, as it addresses a common issue in GMM training.\n4. **Local Principal Directions**: The authors propose a method to reduce the memory footprint of covariance matrices by using local principal directions. This is a practical and innovative approach, especially for high-dimensional data.\n\n#### Technical Soundness\n1. **Numerical Stability**: The max-component approximation is claimed to improve numerical stability. However, the authors do not provide a clear comparison with the log-sum-exp trick, which is a standard method for handling numerical instabilities.\n2. **Regularization and Annealing**: The motivation for the regularizer and the gridding approach is somewhat unclear. The authors should provide more theoretical justification for these techniques.\n3. **Covariance Matrices**: The method primarily uses diagonal covariance matrices, which is a limitation. The authors should demonstrate the effectiveness of the local principal directions approach with more complex covariance structures.\n4. **Experimental Evaluation**: The experiments are conducted on relatively low-dimensional datasets (28x28 and 32x32x3). While these are standard image datasets, they do not fully represent the challenges of high-dimensional spaces. The authors should provide more comprehensive experiments on higher-dimensional datasets to validate their claims.\n\n#### Clarity and Presentation\n1. **Clarity**: The paper is generally well-written and clear. However, there are several points of confusion, such as the motivation for the max-component approximation and the details of the regularizer.\n2. **Presentation**: The paper could benefit from more detailed explanations and better organization. For example, the repeated paragraph in the experimental section should be fixed.\n\n#### Relevance and Impact\n1. **Relevance**: The topic of training GMMs in high-dimensional spaces is relevant to the field of machine learning, especially for applications involving large datasets.\n2. **Impact**: The proposed method has the potential to improve the efficiency and scalability of GMM training. However, the impact is limited by the lack of comprehensive experiments and theoretical guarantees.\n\n### Final Decision\nGiven the novel contributions, practical significance, and the potential impact of the proposed method, the paper has merit. However, the technical soundness and experimental evaluation need to be strengthened. The authors should address the reviewers' concerns and provide more comprehensive experiments on higher-dimensional datasets.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel training strategy, MuPPET, which employs a multi-precision approach to reduce training time and energy consumption while maintaining accuracy. The key innovation is the precision-switching mechanism that dynamically decides when to increase the precision based on gradient diversity. This is a significant contribution to the field of low-precision training, as it pushes the boundaries of using fixed-point representations in training large-scale CNNs.\n\n#### Technical Soundness\nThe technical approach is well-founded, leveraging the concept of gradient diversity to determine the optimal switching points. However, the reviewers have raised several valid points that need to be addressed:\n1. **Heuristic Nature**: The switching mechanism is somewhat heuristic, and the paper could benefit from a more detailed explanation of the reasoning behind it. The authors have addressed this in their response by adding more details and clarifications, but it remains a point of concern.\n2. **Algorithm Details**: The precision-switching policy and the quantization scheme need to be more clearly defined. The authors have updated the equations and provided more details, but the clarity can still be improved.\n3. **Experimental Setup**: The choice of quantized precision levels (8-, 12-, 14-, and 16-bit) is not well-justified. The authors have addressed this by explaining that these levels were empirically chosen, but a more systematic approach would strengthen the paper.\n\n#### Experimental Results\nThe experimental results show that MuPPET can achieve the same accuracy as full-precision training with an average training-time speedup of 1.28× across different networks and datasets. However, the results are not consistently superior to state-of-the-art methods:\n1. **AlexNet and ResNet**: The Top-1 validation accuracy on ImageNet for AlexNet and ResNet is slightly lower than the FP32 baseline. The authors argue that a <1% difference is not significant, but this needs to be clearly demonstrated and discussed.\n2. **GoogLeNet**: The +4.55% improvement in validation accuracy is significant and highlights the potential of MuPPET.\n3. **Training and Validation Curves**: The inclusion of training and validation curves in the appendix is a positive step, but these should be discussed in the main text to provide a more comprehensive analysis.\n\n#### Clarity and Presentation\nThe paper is generally well-written, but there are several areas that need improvement:\n1. **Introduction**: The introduction should be more concise and clearly state the novel contributions of the paper. The authors have addressed this in their response.\n2. **Figures and Equations**: The figures and equations need to be more readable and clearly explained. The authors have added larger-scale figures to the appendix, but the main text should also be improved.\n3. **Typos and Grammar**: There are some minor typos and grammatical errors that need to be corrected.\n\n#### Relevance and Impact\nThe topic of low-precision training is highly relevant to the deep learning community, and the proposed method has the potential to significantly reduce training time and energy consumption. The paper's contribution is valuable, but it needs to be more robust and clearly presented.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper presents a novel and valuable contribution to the field of low-precision training. While there are some areas that need improvement, the authors have addressed the major concerns raised by the reviewers, and the experimental results are promising. The paper should be accepted with the condition that the authors further refine the clarity and technical details as suggested."
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Theoretical Contributions**:\n   - The paper provides a rigorous theoretical foundation for the intuition that \"hard\" negative samples are \"better\" through Theorem 2. This is a significant contribution as it formalizes and proves a widely accepted but previously unrigorously defined concept.\n   - The authors also provide a clear and well-motivated scalar measure of the signal-to-noise ratio, which is a valuable contribution to the understanding of negative sampling.\n\n2. **Practical Impact**:\n   - The proposed method significantly reduces training time by an order of magnitude compared to several competitive baselines, which is a substantial practical improvement.\n   - The adversarial sampling mechanism is simple and efficient, with a computational cost that scales logarithmically with the number of classes, making it highly scalable.\n\n3. **Clarity and Presentation**:\n   - The paper is well-written and accessible, even to non-experts, as noted by Reviewer 1. The derivations and explanations are clear and didactic, which is a strength for a top-tier conference.\n\n#### Negative Aspects:\n1. **Lack of Comprehensive Baselines**:\n   - The paper does not compare against recent state-of-the-art methods like Slice [1] and DiSMEC [2], which are important for a complete evaluation. This omission weakens the experimental section and the overall impact of the paper.\n   - The authors acknowledge this in their response and promise to include these comparisons in the final version, but the current lack of these comparisons is a significant drawback.\n\n2. **Limited Dataset Evaluation**:\n   - The paper only evaluates on two datasets (Wikipedia-500K and Amazon-670K). While these are well-established benchmarks, the evaluation is limited. The authors have committed to running experiments on the EURLex dataset, which will provide a more comprehensive comparison.\n   - The performance on tail-labels and metrics other than accuracy (e.g., MacroF1) are not evaluated, which is important for understanding the robustness of the method.\n\n3. **Comparison with Full Softmax**:\n   - The paper does not compare the proposed method with full softmax loss, which is a critical baseline. The authors have committed to running these experiments on smaller datasets, but the current lack of this comparison is a limitation.\n\n4. **Relevance to Multi-Label Setting**:\n   - The paper focuses on single-label classification and does not explore the multi-label setting. While the authors argue that single-label classification is of practical relevance, the lack of exploration in the multi-label setting limits the paper's applicability to a broader range of problems.\n\n#### Author Response:\n- The authors have addressed many of the concerns raised by the reviewers, particularly the need to include more relevant baselines and to run additional experiments. They have committed to including these in the final version of the paper.\n- The authors have also clarified the theoretical contributions and the differences between their method and related work, which strengthens the paper's position.\n\n### Final Decision\nGiven the significant theoretical contributions, practical impact, and the authors' commitment to addressing the reviewers' concerns, the paper has the potential to make a valuable contribution to the field. However, the current lack of comprehensive baselines and limited dataset evaluation are significant issues that need to be addressed.\n\nFinal Decision: [Accept]"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Problem Setting and Novelty**\nThe paper addresses the problem of out-of-distribution (OOD) detection in neural networks, specifically under the constraints that no OOD samples are available for validation and the model parameters should not be retrained. This is a relevant and practical problem, especially in safety-critical applications where the test distribution is unknown and retraining is not feasible.\n\n**Strengths:**\n- **Clear Problem Setting:** The constraints are well-defined and justified.\n- **Novel Approach:** The use of the Normalized Compression Distance (NCD) to capture spatial patterns in feature maps is a novel idea.\n- **Practical Motivation:** The method aims to make OOD detection more practical by avoiding the need for OOD samples or retraining, which is a significant contribution.\n\n#### **2. Methodology**\nThe proposed method, MALCOM, uses both global average pooling and spatial pattern extraction to identify OOD samples. The authors introduce a similarity metric based on NCD to capture shared patterns between sequences.\n\n**Strengths:**\n- **Innovative Use of NCD:** The use of NCD to capture spatial patterns is a creative and potentially effective approach.\n- **Ablation Study:** The authors provide an ablation study to show the effectiveness of different components of their method.\n\n**Weaknesses:**\n- **Comparison with Baselines:** The initial comparison with baselines was not entirely fair, as the authors did not use adversarial samples for validation, which is a common practice in the literature.\n- **Theoretical Guarantees:** The paper lacks theoretical guarantees showing the exhaustiveness of the proposed method in detecting all possible OOD examples.\n\n#### **3. Experiments and Results**\nThe authors conducted experiments on several benchmarks and neural network architectures to evaluate their method.\n\n**Strengths:**\n- **Comprehensive Evaluation:** The experiments cover a wide range of datasets and architectures.\n- **Additional Experiments:** The authors conducted additional experiments using adversarial samples for validation and introduced a variant of their method, MALCOM++, which shows improved performance.\n\n**Weaknesses:**\n- **Fair Comparison:** The initial comparison with baselines was not entirely fair, as noted by the reviewers. The authors have addressed this in the rebuttal by including additional experiments.\n- **Complex Settings:** The experiments could have been done in more complex settings, such as safety-critical applications like autonomous driving, to better demonstrate the practical significance of the method.\n\n#### **4. Writing and Presentation**\nThe paper is generally well-written and structured, but there are some areas for improvement.\n\n**Strengths:**\n- **Clear Explanation:** The method is explained clearly, and the motivation is well-articulated.\n- **Figures and Tables:** The addition of Figure 1 and the tables in the appendices helps to clarify the method and results.\n\n**Weaknesses:**\n- **Proofreading:** The paper could benefit from additional proofreading to correct grammatical issues and typos.\n- **Related Work:** The paper could cite more related work, especially on the use of compression principles in image classification.\n\n### Final Decision\nThe paper addresses a relevant and practical problem in OOD detection with a novel approach. The authors have addressed the main concerns raised by the reviewers, particularly the unfair comparison with baselines, by conducting additional experiments and providing a more comprehensive evaluation. However, the paper could benefit from further theoretical guarantees and more complex experimental settings to fully demonstrate the practical significance of the method.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Theoretical Contributions**:\n   - The paper provides a comprehensive theoretical analysis of the impact of activation functions on the training of overparametrized neural networks. This is a significant contribution to the field of deep learning theory.\n   - The authors distinguish between non-smooth and smooth activation functions, providing clear conditions under which the minimum eigenvalue of the Gram matrix is large or small. This distinction is crucial for understanding the convergence properties of different activation functions.\n   - The paper builds on and extends previous work, such as Du et al. (2019a) and Allen-Zhu et al. (2019), by providing more detailed and quantitative bounds.\n\n2. **Empirical Validation**:\n   - The authors include empirical experiments on both synthetic data and the CIFAR10 dataset. The synthetic data experiments align well with the theoretical findings, which is a strong point.\n   - The experiments on CIFAR10, while showing some gaps between theory and practice, still provide valuable insights and highlight areas for further research.\n\n3. **Clarity and Organization**:\n   - The paper is well-written and well-referenced. The main results are clearly stated, and the appendix provides detailed proofs.\n   - The authors have addressed the reviewers' comments and made improvements to the paper, such as adding a table of contents to the appendix and clarifying certain assumptions.\n\n#### Weaknesses:\n1. **Length and Readability**:\n   - The appendix is quite long, which can make the paper difficult to navigate for a general audience. However, the authors have taken steps to mitigate this issue by adding a table of contents and improving section names.\n   - The paper could benefit from a more focused presentation, especially in the main text, to make it more accessible to a broader audience.\n\n2. **Gaps in Empirical Results**:\n   - The experiments on CIFAR10 show some discrepancies between theory and practice, which could be a point of concern. However, the authors acknowledge this and discuss the limitations, which is a strength.\n\n3. **Minor Technical Issues**:\n   - There are a few minor technical issues and typos that the authors have addressed in the revised version. These are not major concerns but should be corrected to improve the overall quality of the paper.\n\n### Final Decision\n\nGiven the significant theoretical contributions, the alignment of empirical results with theory, and the authors' efforts to address the reviewers' comments, the paper should be accepted for publication. The weaknesses, while present, are not substantial enough to outweigh the strengths.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" makes a significant contribution to the understanding of the representational power of deep neural networks (DNNs). It addresses a fundamental question in deep learning and approximation theory by connecting the depth-width trade-offs of ReLU networks to Sharkovsky's Theorem from dynamical systems. This connection is novel and provides a deeper understanding of why certain functions are difficult to represent with shallow networks.\n\n#### Technical Soundness\nThe technical content of the paper is strong. The proofs are well-written and appear to be correct, as noted by Reviewer 2. The application of Sharkovsky's Theorem to derive depth-width trade-offs is both clever and well-executed. The paper also addresses an open question posed by Telgarsky, which adds to its significance.\n\n#### Clarity and Exposition\nThe paper is well-written and easy to follow. Reviewer 2 found the motivating examples and the exposition of Sharkovsky's Theorem to be particularly enjoyable. The authors have also taken the feedback from Reviewer 3 seriously and have restructured the paper to make the main results and contributions more prominent. This improvement in structure enhances the clarity and readability of the paper.\n\n#### Empirical Validation\nThe authors have addressed the empirical validation question raised by Reviewer 2. They have performed experiments on a synthetic dataset generated by triangle functions and have plotted the classification error as a function of depth. This empirical evidence helps to contextualize the theoretical results and provides a practical understanding of the depth-width trade-offs.\n\n#### Practical Relevance\nWhile the paper is primarily theoretical, it provides a natural property (periodic points) that can be used to understand the complexity of functions and the benefits of depth in DNNs. The authors acknowledge that assessing the period of a function in practice is a challenging problem but argue that their results are still valuable for understanding the representational power of DNNs. They also provide examples from physics where complex dynamics require deep networks, which adds to the practical relevance of their work.\n\n#### Addressing Reviewer Comments\nThe authors have responded to the reviewer comments in a thorough and thoughtful manner. They have restructured the paper, provided additional examples and intuition, and included empirical results. These changes have addressed the main concerns raised by the reviewers and have improved the overall quality of the paper.\n\n### Final Decision\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Problem Relevance**: The problem of learning reusable skills from weakly supervised data is highly relevant and important in the field of robot learning and control. The ability to segment trajectories into meaningful sub-tasks without detailed supervision can significantly reduce the burden on human annotators and make the learning process more scalable.\n2. **Methodological Contribution**: The paper proposes a novel approach inspired by multiple instance learning (MIL) to segment trajectories into sub-skills. The use of log-sum-exp pooling is a reasonable choice and has shown some empirical success in the experiments.\n3. **Experimental Setup**: The paper evaluates the method across a variety of environments, including both simulation and real-world robots, which demonstrates the versatility of the approach.\n\n#### Weaknesses of the Paper\n1. **Limited Scope and Depth**:\n   - **Experimental Results**: The classification results are relatively poor, with validation accuracies ranging from 35% to 60%. This raises questions about the effectiveness of the method in practical scenarios.\n   - **Baseline Comparisons**: The paper lacks thorough comparisons with relevant baselines, particularly fully-supervised methods and unsupervised clustering methods. This makes it difficult to assess the true value of the weakly-supervised approach.\n   - **Behavioral Cloning**: The behavioral cloning results are underwhelming, with a success rate of only 50% on the hierarchical control task. This suggests that the learned skills may not be robust or generalizable enough.\n2. **Theoretical and Practical Limitations**:\n   - **Dataset Requirements**: The paper does not adequately address the limitations of the dataset and the requirements for skill occurrences. The reviewer's example of skill sequences highlights a significant issue that the paper does not fully address.\n   - **Ambiguity in Skill Definition**: The paper acknowledges the ambiguity in skill definition and the challenges in classifying certain timesteps. However, this ambiguity is not fully explored or quantified, which affects the interpretability of the results.\n3. **Clarity and Detail**:\n   - **Experimental Setup**: The experimental setup is not clearly described, making it difficult to reproduce the results. Details such as the number of demonstrations per task/skill, architectural choices, and hyper-parameters are lacking.\n   - **Post-Processing**: The use of Gaussian smoothing is mentioned but not thoroughly explained, and its impact on the results is not quantified.\n\n#### Author Responses\n1. **Addressing Reviewer Concerns**:\n   - The authors provide some clarifications regarding the comparison with Shiarlis et al. (2018) and the use of Gaussian smoothing. However, these responses do not fully address the deeper issues raised by the reviewers.\n   - The authors acknowledge the limitations of the dataset and the ambiguity in skill definition but do not provide a comprehensive analysis or theoretical bounds to support their claims.\n   - The authors promise to add comparisons with unsupervised clustering methods and a fully-supervised oracle baseline, which could strengthen the paper.\n\n### Final Decision\nGiven the significant weaknesses in the experimental results, the lack of thorough baseline comparisons, and the limited theoretical and practical analysis, the paper does not meet the standards of a top-tier conference. While the problem is important and the method is novel, the current state of the paper does not provide sufficient evidence to support its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Novel Contribution**: The paper proposes a novel method, AE-GAN+sr, which combines an auto-encoder and a GAN to purify adversarial examples. The method introduces an encoder-assisted search in the latent space to find the closest natural reconstruction, which is a significant improvement over previous methods like Defense-GAN.\n2. **Computational Efficiency**: The proposed method is computationally more efficient compared to Defense-GAN, achieving comparable or better performance with fewer iterations and a single starting point.\n3. **Experimental Results**: The authors provide extensive experimental results on MNIST and Fashion-MNIST, showing that their method is robust against various adversarial attacks. They also include results on the CelebA dataset, demonstrating the method's effectiveness on more complex datasets.\n4. **Comparisons with State-of-the-Art**: The authors have included comparisons with state-of-the-art methods like PGD adversarial training, which is a strong baseline. The results show that AE-GAN+sr performs well, especially on white-box attacks.\n5. **Robustness Analysis**: The authors have conducted experiments to evaluate the method's robustness against BPDA attacks and l2_attack, showing that it can defend against these attacks to a significant extent.\n\n#### Weaknesses of the Paper\n1. **Incremental Contribution**: Some reviewers argue that the contribution is incremental, especially in comparison to Defense-GAN. While the method is more efficient, the core idea of using an auto-encoder and GAN for adversarial defense is not entirely novel.\n2. **Complexity and Clarity**: The paper has some issues with clarity and writing. There are typos and confusing phrases that make the paper hard to follow. The authors have addressed some of these issues in their response, but further improvements are needed.\n3. **Lack of CIFAR-10 Results**: The paper lacks results on more complex datasets like CIFAR-10, which is a standard benchmark for evaluating adversarial defense methods. The authors have promised to include these results in the future, but their absence is a significant gap in the current submission.\n4. **Detection Mechanism**: The detection mechanism used in the method is susceptible to white-box attacks. While the authors have provided additional experiments to address this concern, the detection mechanism's reliability under strong white-box attacks remains a concern.\n5. **Latent Space Distribution**: The authors have not provided a strong assurance that the latent distribution F(x) from the data and the prior p(z) will be similar. This is a critical aspect of the method's effectiveness, and the lack of a clear solution to this problem is a weakness.\n\n### Final Decision\nDespite the paper's strengths, the weaknesses, particularly the incremental nature of the contribution, the lack of results on CIFAR-10, and the issues with clarity and writing, are significant. The paper would benefit from further revisions to address these concerns and provide a more comprehensive evaluation.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\nThe paper introduces a modular framework for coordinating multiple agents by first learning diverse sub-skills for each agent and then using a meta-policy to coordinate these skills. The key novelty lies in the combination of skill behavior diversification and the coordination of these skills in a multi-agent setting. This approach is well-motivated and addresses a significant challenge in multi-agent reinforcement learning (MARL), where coordination of pre-learned skills is crucial for solving complex tasks.\n\n#### **2. Methodology:**\nThe method is well-explained and logically structured. The use of DIAYN (Diversity is All You Need) for skill diversification and a meta-policy for coordination is a sound approach. The authors provide a clear explanation of how the framework fits together, although some notational inconsistencies and a lack of clarity in Figure 2 were noted by the reviewers. The authors have addressed these issues in their response, which is a positive step.\n\n#### **3. Empirical Evaluation:**\nThe empirical results are strong and demonstrate the effectiveness of the proposed method in solving challenging multi-agent tasks. The tasks chosen (e.g., picking up a long bar, placing a block inside a container, and pushing a box with two ant agents) are relevant and challenging. The method outperforms the baselines, which is a significant achievement. However, the high variance in performance and the need for more seeds to improve the robustness of the results were noted by the reviewers. The authors have addressed these concerns by running more seeds and applying moving averages to the plots, which is a positive improvement.\n\n#### **4. Baselines and Comparisons:**\nThe authors have included a decentralized policy with a shared critic as a baseline, which is a fair comparison. However, the reviewers suggested adding a DIAYN-only baseline to evaluate the importance of predefined subtasks. The authors have added this baseline and shown that it fails to solve the tasks, which strengthens the argument for the importance of domain expert knowledge in complex tasks. This addition is valuable and enhances the paper.\n\n#### **5. Related Work and Discussion:**\nThe authors have provided a thorough discussion of related work, addressing the reviewers' concerns about the lack of in-depth discussion on alternative multi-agent methods. They have clearly explained why their approach is different and why it is necessary to tackle the coordination problem between multiple agents with pre-learned skills. This discussion is well-reasoned and adds depth to the paper.\n\n#### **6. Clarity and Presentation:**\nThe paper is generally well-written and well-organized. The authors have addressed the notational inconsistencies and improved the clarity of Figure 2, which should make the paper more accessible to readers. The addition of more detailed results and plots (e.g., success rates) in the supplementary material is also a positive step.\n\n### Final Decision\nGiven the strong empirical results, the novelty of the approach, the thorough discussion of related work, and the authors' responsiveness to the reviewers' concerns, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Relevance and Novelty**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning, specifically focusing on Topologically Associating Domains (TADs) in Drosophila melanogaster.\n   - The use of bidirectional LSTM RNNs to leverage the sequential nature of DNA is a novel approach in this context.\n\n2. **Methodological Soundness**:\n   - The authors compare their LSTM model with several other machine learning models (linear models with regularization, Gradient Boosting, and vanilla LSTM) and demonstrate that the bidirectional LSTM outperforms these models.\n   - The weighted Mean Squared Error (wMSE) is used as a loss function, which is a reasonable choice given the nature of the problem.\n\n3. **Data and Preprocessing**:\n   - The paper uses Hi-C and ChIP-Seq data, which are standard and relevant datasets for this type of research.\n   - The preprocessing steps are described, and the resolution of the Hi-C map (20 kb) is justified.\n\n4. **Feature Importance**:\n   - The authors provide insights into the importance of different epigenetic features, which is valuable for understanding the biological significance of the model's predictions.\n\n#### Weaknesses of the Paper:\n1. **Generalizability**:\n   - The paper lacks an evaluation of the model's performance on other datasets or organisms, which limits its generalizability.\n   - The specific choice of the wMSE (with the value 11) is not well-justified and may not be generalizable to other datasets.\n\n2. **Comparative Analysis**:\n   - The comparison with other models (linear models, Gradient Boosting) is not entirely fair, as the same loss function (wMSE) was not used for all models.\n   - The authors should have included more baseline models, such as fully connected or convolutional networks, to provide a more comprehensive comparison.\n\n3. **Clarity and Presentation**:\n   - The paper has some issues with clarity and presentation, as noted by the reviewers. For example, the equation for wMSE was not clearly defined, and the description of the data and methods could be more detailed.\n   - The discussion about the choice of regression models and neural networks could be expanded to provide a clearer rationale for the model selection.\n\n4. **Literature Review**:\n   - The literature review is somewhat limited. The authors should cite and discuss more related works, especially those that have used similar approaches or datasets.\n\n#### Author Responses:\n- The authors have addressed many of the reviewers' comments, such as clarifying the wMSE, expanding the literature review, and adding more evaluation metrics.\n- However, some issues remain, particularly regarding the generalizability of the model and the fairness of the comparison with other models.\n\n### Final Decision\nGiven the strengths of the paper, particularly its novel approach and methodological soundness, and the authors' efforts to address the reviewers' comments, the paper has the potential to make a valuable contribution to the field. However, the weaknesses in generalizability and comparative analysis need to be addressed to ensure the robustness and reliability of the results.\n\nFinal Decision: **Accept** with the condition that the authors:\n1. Conduct additional experiments to evaluate the model's performance on other datasets or organisms.\n2. Use the same loss function (wMSE) for all models to ensure a fair comparison.\n3. Provide a more detailed and clear description of the data and methods.\n4. Expand the literature review to include more related works."
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution**\n- **Novelty**: The paper introduces a novel approach to multi-task learning by modeling multi-level task dependencies, which is a significant contribution. The authors claim that their method can capture both general task dependencies and data-specific task dependencies, which is particularly useful for hierarchical data like text and graphs.\n- **Comparison to Prior Work**: The authors have addressed the concerns raised by the reviewers regarding the relationship with prior work, particularly Taskonomy [1]. They have clarified that their method can handle both general and data-specific task dependencies, which is a key distinction from Taskonomy. They also highlight the efficiency of their method in terms of parameter complexity, which is a significant improvement over [2].\n\n#### **2. Clarity and Presentation**\n- **Grammar and Readability**: The paper has been revised to address grammatical issues and improve readability. However, some minor issues may still persist, which could be addressed in the final version.\n- **Clarity of Concepts**: The authors have provided more detailed explanations of the multi-level task dependency and the position-wise mutual attention mechanism. The motivation for these concepts is clearer, and the distinction from previous work is better articulated.\n\n#### **3. Experimental Evaluation**\n- **Datasets and Baselines**: The authors have conducted experiments on several public datasets and compared their method with state-of-the-art approaches. They have also added a baseline that does not involve attention to demonstrate the importance of the attention mechanism.\n- **Results**: The results show significant improvements over existing methods, particularly in the low-labeled ratio setting. This is a strong point in favor of the paper, as it demonstrates the practical utility of the proposed method.\n\n#### **4. Technical Soundness**\n- **Methodology**: The proposed method is technically sound and well-motivated. The decomposition method to reduce parameter complexity is a valuable contribution.\n- **Mathematical Formulation**: The mathematical formulation is clear, and the authors have addressed the notation issues raised by the reviewers.\n\n#### **5. Addressing Reviewer Concerns**\n- **Reviewer 1**: The authors have addressed the grammatical issues and provided more clarity on the distinction between general and data-specific task dependencies.\n- **Reviewer 2**: The authors have expanded the motivation for the attention mechanism and provided more details on the mappings used in the model.\n- **Reviewer 3**: The authors have clarified the relationship with Taskonomy and addressed the concerns about the number of model parameters.\n- **Reviewer 4**: The authors have provided a clear explanation of the multi-level task dependency and addressed the concerns about the experimental setup.\n\n### Final Decision\nGiven the significant contributions, the clarity of the revised paper, and the strong experimental results, the paper should be accepted for publication at a top-tier conference. The authors have effectively addressed the concerns raised by the reviewers, and the paper is now in a much-improved state.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Contributions and Novelty\nThe paper claims to provide the first result on the global convergence of stochastic gradient descent-ascent (SGD) for training Wasserstein GANs (WGANs) with a one-layer generator. This is a significant theoretical contribution, especially given the complexity of the min-max optimization problem in GANs. The authors justify their choice of a quadratic discriminator by arguing that it simplifies the dynamics while maintaining sufficient discriminating power for the one-layer generator. They also demonstrate that their approach achieves a nearly optimal sample complexity.\n\n#### Reviewer Concerns\n1. **Simplification of Discriminator**:\n   - **Reviewer 1 and 2**: Both reviewers express concerns about the simplicity of the quadratic discriminator, arguing that it is not representative of the discriminators used in practical WGANs. They suggest that the analysis should consider more complex discriminators, such as two-layer ReLU networks.\n   - **Author Response**: The authors argue that the quadratic discriminator is sufficient for the one-layer generator and that using a more complex discriminator would increase the sample complexity. They also mention that the quadratic discriminator can be treated as a Lipschitz constraint for bounded data sets.\n\n2. **Relevance to Practical WGANs**:\n   - **Reviewer 1 and 2**: The reviewers question the practical relevance of the results, given the simplified setting.\n   - **Author Response**: The authors emphasize that their goal is to provide a theoretical foundation for understanding the dynamics of SGD in GANs, and they argue that their choice of discriminator is a strength, not a weakness.\n\n3. **Terminology and Clarity**:\n   - **Reviewer 2**: The reviewer points out potential confusion in the terminology used to describe the one-layer generator.\n   - **Author Response**: The authors clarify that by \"one-layer generator,\" they mean a linear model with an elementwise monotonic transform, and they reference prior work that uses similar terminology.\n\n#### Strengths\n- **Theoretical Contribution**: The paper provides a novel and rigorous analysis of the convergence of SGD in the GAN setting, which is a significant theoretical advance.\n- **Sample Complexity**: The authors demonstrate that their approach achieves a nearly optimal sample complexity, which is an important practical consideration.\n- **Clarity and Rigor**: The paper is well-written, and the theoretical analysis appears to be sound and complete.\n\n#### Weaknesses\n- **Simplified Setting**: The use of a quadratic discriminator and a one-layer generator is a significant simplification that may limit the practical relevance of the results.\n- **Generalizability**: The results may not be easily extendable to more complex discriminators and generators, which are more commonly used in practice.\n\n### Final Decision\nWhile the paper makes a significant theoretical contribution to the understanding of the convergence of SGD in GANs, the simplified setting and the use of a quadratic discriminator raise concerns about the practical relevance of the results. However, the theoretical insights and the rigorous analysis are valuable and could serve as a foundation for future work on more complex settings.\n\nGiven the importance of the theoretical contribution and the soundness of the analysis, I believe the paper should be accepted, but with a strong recommendation for the authors to address the concerns about the practical relevance and to provide more context on how their results can be extended to more complex settings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\nThe paper introduces a modular framework for coordinating multiple agents by leveraging pre-learned diverse skills. The key contributions are:\n- **Skill Behavior Diversification:** The method uses a maximum entropy objective to learn diverse behaviors for each skill, which can be controlled via a latent conditioning vector.\n- **Modular Framework:** The framework first trains sub-skills independently and then learns to coordinate these skills using a meta-policy.\n- **Empirical Results:** The method is evaluated on challenging multi-agent tasks, demonstrating its effectiveness in coordinating complex skills.\n\nThe novelty lies in the combination of skill diversification and modular coordination, which is a significant step forward in multi-agent reinforcement learning (MARL). The problem of coordinating pre-learned skills is not well-addressed in the literature, making this work a valuable contribution.\n\n#### **2. Technical Soundness:**\n- **Methodology:** The method is well-defined and technically sound. The use of a maximum entropy objective for skill diversification and the modular framework for coordination are well-motivated and logically consistent.\n- **Experiments:** The experiments are comprehensive and cover a range of challenging tasks. The results show that the proposed method outperforms baselines, which is a strong indicator of its effectiveness.\n- **Baselines:** The authors have addressed the reviewers' concerns by adding additional baselines (centralized SBD and decentralized SBD) to demonstrate the importance of pre-defined subtasks. This strengthens the empirical evaluation.\n\n#### **3. Clarity and Presentation:**\n- **Notations and Diagrams:** The notations were initially inconsistent and confusing, but the authors have revised the paper to address these issues. The updated Figure 2 provides a better understanding of the framework.\n- **Explanations:** The paper is generally well-written and easy to follow. The authors have provided detailed responses to the reviewers' questions, which should be incorporated into the final version to improve clarity.\n\n#### **4. Impact and Relevance:**\n- **Impact:** The proposed method has the potential to significantly impact the field of MARL, particularly in scenarios where pre-learned skills need to be coordinated. The ability to leverage diverse behaviors and adapt them for complex tasks is a valuable contribution.\n- **Relevance:** The paper is highly relevant to top-tier conferences in machine learning and robotics, as it addresses a challenging and important problem in multi-agent systems.\n\n#### **5. Addressing Reviewer Concerns:**\n- **High Variance in Performance:** The authors have addressed this concern by running more seeds and using moving averages in the plots, which improves the reliability of the results.\n- **Fixed Horizon \\( T_{low} \\):** The authors have provided a detailed analysis of the design choice and conducted additional experiments to show the effect of different \\( T_{low} \\) values. This addresses the concern about the meta-policy treating skills as primitive actions.\n- **DIAYN Only Baseline:** The authors have added a DIAYN only baseline to demonstrate the importance of pre-defined subtasks, which strengthens the empirical evaluation.\n- **Related Work:** The authors have provided a more in-depth discussion of related multi-agent methods, which improves the context and relevance of the paper.\n\n### Final Decision\nGiven the novel contributions, technical soundness, comprehensive experiments, and the authors' thorough responses to reviewer concerns, this paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Innovative Approach**: The paper introduces a novel method, Differentiable Hebbian Consolidation (DHC), which combines Hebbian learning with synaptic plasticity to address catastrophic forgetting in continual learning. This approach is well-motivated and draws from both computational and neurobiological principles, making it a refreshing addition to the field.\n\n2. **Theoretical Foundation**: The method is grounded in the Complementary Learning Systems (CLS) theory, which is a well-established framework in cognitive science. The authors provide a clear and detailed explanation of how their method aligns with CLS principles, making the paper accessible to both machine learning and cognitive science communities.\n\n3. **Empirical Evaluation**: The authors evaluate their method on a variety of benchmarks, including Permuted MNIST, Split MNIST, and a 5-vision dataset mixture. They also introduce an imbalanced variant of Permuted MNIST, which adds a new dimension to the evaluation. The results show that DHC outperforms simple baselines and, when combined with other task-specific synaptic consolidation methods, achieves competitive performance.\n\n4. **Clarity and Presentation**: The paper is well-written and structured. The authors provide clear explanations of the method, including equations and a comparison with standard softmax. They also address the reviewers' concerns in a detailed and thoughtful manner, demonstrating a strong commitment to improving the quality of the paper.\n\n#### Weaknesses of the Paper:\n1. **Comparative Performance**: While DHC shows improvement over simple baselines, it does not consistently outperform more sophisticated methods like Synaptic Intelligence (SI) on all benchmarks. This is a significant concern, as the paper aims to introduce a new method that should demonstrate clear advantages over existing approaches.\n\n2. **Limited Scope of Experiments**: The authors have primarily evaluated their method on vision datasets, which is a common practice in the field. However, the reviewers have pointed out the need for more sophisticated experiments, such as evaluating the method on more complex datasets like CIFAR-100 or ILSVRC, or in reinforcement learning settings. The authors have addressed this to some extent by adding experiments on Split CIFAR-10/100, but more extensive evaluations would strengthen the paper.\n\n3. **Hyperparameter Sensitivity**: The authors mention that DHC introduces a hyperparameter, the initial value of the Hebbian learning rate \\(\\eta\\), which is a learned parameter. While they provide a sensitivity analysis, it would be beneficial to see a more detailed exploration of how this hyperparameter affects the performance of the model.\n\n4. **Comparisons to CLS-Based Approaches**: The reviewers have raised concerns about the lack of comparisons to other CLS-based approaches. The authors have addressed this by providing a more detailed explanation and comparing DHC to the Hebbian Softmax layer, but more comprehensive comparisons would be valuable.\n\n### Final Decision\n\nDespite the weaknesses, the paper introduces a novel and well-motivated method that addresses an important problem in continual learning. The authors have made significant efforts to address the reviewers' concerns and have provided additional experiments and clarifications. The method shows promise and contributes to the ongoing research in the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Writing Quality**\n- **Reviewer Comments**: All three reviewers have highlighted significant issues with the clarity and writing quality of the paper. The abstract and the main text are described as poorly written, with grammatical issues and incomprehensible passages. Reviewer 2 and Reviewer 3 specifically mention that they could not understand the paper despite spending considerable time on it.\n- **Author Response**: The author acknowledges the poor writing quality and expresses a desire to improve. However, the current state of the paper is not suitable for a top-tier conference.\n\n#### 2. **Content and Contribution**\n- **Reviewer Comments**: Reviewer 1 and Reviewer 3 note that the paper lacks a clear explanation of its contributions and how it improves upon existing methods like AlphaZero. The proposed method is not well-defined, and the experiments are limited to a small problem (tic-tac-toe), which does not provide strong evidence of the method's effectiveness.\n- **Author Response**: The author mentions finding a similar idea in their country but not internationally. This suggests that the idea might not be entirely novel, and the lack of references makes it difficult to assess the contribution.\n\n#### 3. **References and Related Work**\n- **Reviewer Comments**: All reviewers point out the absence of references, which is a significant issue for a research paper. The lack of references makes it impossible to evaluate the paper's contribution in the context of existing work.\n- **Author Response**: The author explains that they did not know how to use a .bib file and removed all references. This is a critical oversight that needs to be addressed before the paper can be considered for publication.\n\n#### 4. **Experimental Results and Robustness**\n- **Reviewer Comments**: The experiments are limited to a small problem (tic-tac-toe), and there is no evidence of the method's robustness or performance on more complex problems. Reviewer 1 mentions that the paper does not provide guarantees for the stability and final performance of the learning process.\n- **Author Response**: The author acknowledges the limitations and expresses a desire to conduct more research and improve the paper.\n\n### Final Decision\nGiven the significant issues with clarity, writing quality, lack of references, and limited experimental results, the paper is not ready for publication in a top-tier conference. The author's willingness to improve is commendable, but the current state of the paper does not meet the standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel approach to unsupervised domain adaptation (UDA) by using a domain-adaptive multibranch network. The key idea is to allow different domains to undergo different sequences of operations, which is a significant departure from the standard practice of using the same sequence of operations for all domains. This flexibility is a strong contribution to the field, as it addresses the issue of information asymmetry between domains, which is a common challenge in UDA.\n\n#### Technical Soundness\nThe method is technically sound and well-motivated. The authors provide a clear explanation of the architecture and the rationale behind it. The use of adaptive computation graphs and learnable weights for different domains is a sensible approach. The paper also includes a detailed description of the training process and the loss functions used, which are well-explained and logically consistent.\n\n#### Experimental Evaluation\nThe experimental results are generally convincing, but there are some areas that need improvement:\n1. **Comparison with State-of-the-Art Methods**: The paper primarily compares its method with RPT and DANN, which are not state-of-the-art (SOTA) methods. While the authors have provided additional results using Maximum Classifier Discrepancy (MCD) in their response, these results should be included in the final version of the paper to strengthen the experimental evaluation.\n2. **Ablation Studies and Hyperparameter Sensitivity**: The authors have addressed some of the reviewer's concerns by including additional experiments, such as varying the number of parallel flows and different configurations of computational units. However, more detailed ablation studies and hyperparameter sensitivity analyses would further solidify the experimental results.\n3. **Multi-Source and Multi-Target Adaptation**: The authors have included some results for multi-source domain adaptation, but a more comprehensive evaluation, especially for multi-target adaptation, would be beneficial. This is particularly important given the potential of the proposed method to handle multiple domains simultaneously.\n\n#### Clarity and Presentation\nThe paper is well-written and easy to follow. The authors have addressed the minor issues raised by the reviewers, such as renaming the approach to avoid name clashes and removing back links in the references. The inclusion of additional related work from outside the deep learning domain is a positive step.\n\n#### Code Availability\nThe authors have committed to making the code publicly available, which is a significant plus for reproducibility and further research.\n\n### Final Decision\nDespite the need for some additional experiments and a more comprehensive comparison with SOTA methods, the paper presents a novel and technically sound approach to unsupervised domain adaptation. The authors have addressed most of the reviewer's concerns and have shown promising results. The inclusion of additional experiments and a more thorough comparison with SOTA methods in the final version of the paper would further strengthen the submission.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\nThe paper introduces a novel approach to learning Mahalanobis metric spaces by formulating the problem as an optimization task to minimize the number of violated similarity/dissimilarity constraints. The key contribution is the development of a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time for fixed ambient dimensions. This is achieved by leveraging tools from the theory of linear programming in low dimensions.\n\nThe authors claim that their algorithm is the first to provide a provable near-optimal guarantee on the number of violated constraints for arbitrary (adversarial) inputs. This is a significant theoretical advance, as it addresses a highly non-convex constraint problem that previous methods, such as ITML and LMNN, do not handle as effectively.\n\n#### Technical Soundness\nThe paper is technically sound and well-grounded in the theory of linear programming and approximation algorithms. The authors provide a clear and rigorous proof of their main results, including the FPTAS and the combinatorial dimension. The experimental results on synthetic and real-world data sets support the theoretical claims, demonstrating the algorithm's effectiveness and robustness to adversarial noise.\n\n#### Comparison to State-of-the-Art\nThe authors compare their method to ITML and LMNN, which are widely used and cited methods in the field. They also discuss the relevance of recent works by Verma and Branson, and Ye et al., explaining how their approach differs in terms of the problem setting and the guarantees provided. This comparison is thorough and helps to position the paper within the broader context of metric learning.\n\n#### Clarity and Presentation\nThe paper is generally well-written and organized. However, there are a few areas where the presentation could be improved:\n- The proof of Lemma 2.1 is confusing and needs to be rephrased.\n- The computational hardness of the problem should be discussed more clearly.\n- The definition of combinatorial dimension should be included.\n- The function call in Algorithm 1 should be corrected.\n- The conclusion is missing and should be added.\n\n#### Experimental Results\nThe experimental results are compelling and demonstrate the effectiveness of the proposed algorithm. The authors show that their method performs favorably in the presence of adversarial noise, which is a significant advantage over existing methods. However, the running time curve in Figure 4 is not monotonic, and the authors provide a reasonable explanation for this behavior.\n\n#### Addressing Reviewer Comments\nThe authors have addressed the reviewer comments thoroughly and provided clear responses to the specific points raised. They have acknowledged the need for improvements in the presentation and have committed to making the necessary changes in the final version of the paper.\n\n### Final Decision\nGiven the significant theoretical contribution, the technical soundness, and the compelling experimental results, the paper should be accepted for publication at a top-tier conference. The authors have addressed the reviewer comments effectively, and the paper has the potential to advance the field of metric learning.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Novelty and Generalization**:\n   - The paper proposes a generalized probability kernel (GPK) that extends existing discrepancy measures like MMD and KSD. This is a significant contribution as it unifies and generalizes these measures.\n   - The introduction of power-MMD as a specific instance of GPK is a novel and potentially useful extension of MMD.\n\n2. **Theoretical Contributions**:\n   - The paper provides a framework for defining and estimating GPK, including unbiased estimators and convergence bounds.\n   - The authors have addressed some of the major issues raised by the reviewers, such as the confusion between different notations and the removal of the KSD section due to incorrect proofs.\n\n3. **Potential Impact**:\n   - The work connects the fields of discrete distribution-property estimation and kernel-based hypothesis testing, which could open up new research directions and applications.\n\n#### Negative Aspects:\n1. **Clarity and Presentation**:\n   - The initial version of the paper had numerous typos, grammatical errors, and unclear notations, which made it difficult to follow. While the authors have made efforts to improve this in the revised version, the clarity and presentation still need significant improvement.\n   - The notation issues, such as the confusion between \\( K \\) and \\( k \\), and the inconsistent use of symbols, need to be thoroughly addressed.\n\n2. **Theoretical Soundness**:\n   - The initial version had several theoretical issues, such as the incorrect proof of Theorem 7 and the lack of a clear connection between the proposed kernels and existing measures.\n   - The revised version has addressed some of these issues, but the theoretical soundness and completeness of the paper still need to be verified.\n\n3. **Experimental Evaluation**:\n   - The initial version lacked any experimental evaluation, which is a significant weakness. The revised version should include empirical results to validate the proposed methods and demonstrate their effectiveness in practical scenarios.\n   - The lack of experimental results makes it difficult to assess the practical utility and performance of the proposed GPK and power-MMD.\n\n4. **Motivation and Relevance**:\n   - The authors need to provide a clearer motivation for why the proposed GPK and power-MMD are useful and how they compare to existing methods. The practical relevance and potential applications should be better articulated.\n\n### Final Decision\nGiven the significant improvements in the revised version, particularly in addressing the major issues raised by the reviewers, the paper shows promise. However, the lack of experimental evaluation and the need for further clarity and theoretical soundness are major concerns. The paper would benefit from a major revision to address these issues before it can be considered for publication in a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\nThe paper introduces a deep neural approach to model continuous time-evolving graphs using a temporal point process (TPP) framework. The key contributions include:\n1. **Temporal Dependency Graph (TDG)**: A novel concept to capture complex dependencies among time-evolving interactions.\n2. **Graph-Structured LSTM with Time Gates (DIP units)**: An extension of LSTM to handle irregular time intervals in dynamic graphs.\n3. **Selection Mechanism**: A method to weight important interactions in k-hop history subgraphs.\n4. **Fusion Mechanism**: A technique to integrate information from multiple layers to capture multiple time resolutions.\n\nWhile the individual components (LSTM, attention, and stacking) are not entirely novel, the combination and application to time-evolving graphs are innovative. The authors provide a detailed explanation of how these components work together to address the challenges of dynamic graph modeling.\n\n#### Technical Soundness\nThe paper is technically sound in its approach and methodology. The use of TPP to model interaction dynamics is well-justified, and the introduction of time gates in LSTM is a significant improvement over traditional RNNs for handling irregular time intervals. The selection and fusion mechanisms are well-designed to capture the temporal and structural dependencies in the graph.\n\nHowever, there are some concerns:\n1. **Log-likelihood Function**: The initial formulation of the log-likelihood function was incorrect, but the authors have corrected it in the response.\n2. **Ablation Studies**: The ablation studies provided are somewhat limited. More detailed analysis, such as the effect of different k and L values, would strengthen the paper.\n3. **Comparison with JODIE**: The authors have added a comparison with JODIE, which is a significant improvement. However, a more detailed discussion of the differences and performance comparisons would be beneficial.\n\n#### Experimental Results\nThe experimental results are promising, showing superior performance on interaction prediction and classification tasks. The authors have included a real-world financial application, which adds practical relevance. However, the results could be further strengthened by:\n1. **Parameter Analysis**: More detailed analysis of the impact of different parameters (k, L, embedding size) on performance.\n2. **Comparative Analysis**: A more detailed comparison with other state-of-the-art methods, including a discussion of why the proposed method outperforms them.\n\n#### Clarity and Presentation\nThe paper is generally well-written, but there are areas for improvement:\n1. **Notation Consistency**: The authors should use consistent notation throughout the paper.\n2. **Figures and Diagrams**: The addition of an overview diagram (Figure 2) and a detailed illustration of the selection and fusion mechanisms (Figure 3) is helpful but could be further improved for clarity.\n3. **Discussion Section**: The authors should add a discussion section to provide insights into the performance of the DIP model and why it outperforms other methods.\n\n### Final Decision\nDespite the concerns mentioned, the paper presents a novel and technically sound approach to modeling continuous time-evolving graphs. The contributions are significant, and the experimental results are promising. The authors have addressed most of the reviewer comments and have made substantial improvements to the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Relevance and Novelty**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning. This is a relevant and timely topic given the increasing availability of epigenetic data.\n   - The use of bidirectional LSTM RNNs to predict chromatin folding patterns from epigenetic marks is a novel approach, especially in the context of Drosophila melanogaster.\n\n2. **Methodological Soundness**:\n   - The authors compare their LSTM model with several other machine learning models (linear models with regularization, Gradient Boosting) and show that the LSTM model outperforms the others.\n   - The weighted Mean Squared Error (wMSE) loss function is introduced to improve the model's performance, and the authors provide a clear explanation of its motivation and implementation.\n\n3. **Biological Insights**:\n   - The paper identifies informative epigenetic features that contribute to the prediction of chromatin folding patterns, providing biological insights into the mechanisms of TAD formation.\n\n4. **Clarity and Presentation**:\n   - The paper is generally well-written and structured, making it easy to follow the methodology and results.\n\n#### Weaknesses of the Paper:\n1. **Generalizability**:\n   - The authors acknowledge that the model is specific to Drosophila melanogaster and may not generalize to other organisms or cell types. This is a valid concern, and the paper could benefit from a discussion on the potential for generalization or the limitations of the model.\n\n2. **Comparison with Baseline Models**:\n   - The reviewers raised concerns about the fairness of the comparison between the LSTM model and other models. Specifically, the use of the wMSE loss function for the LSTM model but not for other models could bias the results. The authors have addressed this by using the same loss function for all models, but this should be clearly stated and verified in the results.\n\n3. **Feature Representation**:\n   - The description of the input features and their representation is somewhat vague. The authors have provided more details in their response, but this information should be clearly stated in the paper to ensure reproducibility.\n\n4. **Evaluation Metrics**:\n   - The paper could benefit from a broader set of evaluation metrics. The authors have added additional metrics (MSE, MAE, R^2) in response to the reviewers' comments, which is a positive step.\n\n5. **Literature Review**:\n   - The literature review is somewhat limited. The authors have expanded it in their response, but it would be beneficial to include more relevant works, especially those that use similar approaches or datasets.\n\n6. **Biological Interpretation**:\n   - The biological interpretation of the results could be more detailed. The authors should provide a more comprehensive discussion of the biological significance of the identified features and the implications of their findings.\n\n#### Addressing Reviewer Comments:\n- **Reviewer 1**: The authors have addressed the concerns about the wMSE loss function and the generalizability of the model. They have also provided more details about the feature representation and the biological significance of the results.\n- **Reviewer 2**: The authors have clarified the wMSE equation, fixed the citation, and improved the clarity of the paper. They have also expanded the discussion on the choice of models and the biological interpretation of the results.\n- **Reviewer 3**: The authors have expanded the literature review, provided more details about the input features, and added a broader set of evaluation metrics. They have also addressed the concerns about the choice of the biLSTM model and the feature importance analysis.\n\n### Final Decision\nGiven the strengths of the paper, the authors' thorough responses to the reviewers' comments, and the improvements made, the paper should be accepted for publication. However, the authors should ensure that the final version of the paper addresses the remaining concerns and provides a more comprehensive discussion of the biological significance of their findings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\nThe paper introduces a novel framework, Continuous Query Decomposition (CQD), for answering complex queries on incomplete Knowledge Graphs (KGs). The key contributions are:\n- **Efficient Query Answering:** The method translates complex queries into an end-to-end differentiable objective, leveraging pre-trained neural link predictors.\n- **Explainability:** The model provides insights into the intermediate solutions for each query atom, enhancing interpretability.\n- **State-of-the-Art Results:** The proposed approach outperforms existing methods, achieving significant improvements in accuracy with less training data.\n\n#### **2. Technical Soundness:**\n- **Methodology:** The paper presents a clear and well-structured methodology. The use of t-norms and t-conorms for logical operations is innovative and well-justified.\n- **Optimization Techniques:** The paper explores both gradient-based and combinatorial optimization methods, providing a comprehensive evaluation.\n- **Neural Link Predictors:** The authors demonstrate the effectiveness of their approach with different neural link predictors, including ComplEx and DistMult, showing robustness and flexibility.\n\n#### **3. Experimental Evaluation:**\n- **Datasets and Benchmarks:** The experiments are conducted on standard datasets (FB15k, FB15k-237, NELL), ensuring comparability with existing work.\n- **Performance Metrics:** The paper reports significant improvements in Hits@3, a widely used metric for evaluating query answering systems.\n- **Ablation Studies:** The authors include ablation studies to analyze the impact of different components, such as the choice of neural link predictor and embedding size.\n\n#### **4. Clarity and Presentation:**\n- **Writing Quality:** The paper is generally well-written and easy to follow. The authors provide clear explanations and visual intuitions.\n- **Notation and Formalism:** While the notation is consistent, some reviewers noted that it could be simplified. The authors have addressed this in the updated version.\n- **Figures and Tables:** The figures and tables are well-presented, but some minor issues (e.g., the direction of edges in Figure 1) have been corrected.\n\n#### **5. Addressing Reviewer Concerns:**\n- **Reviewer 1:** The author's response addressed the reviewer's concerns.\n- **Reviewer 2:** The authors provided additional insights into the query types and the handling of KG incompleteness.\n- **Reviewer 3:** The authors conducted additional experiments with DistMult and provided timing results, addressing the reviewer's concerns about the choice of neural link predictor and inference time.\n- **Reviewer 4:** The authors clarified the use of 1-hop queries for training and provided more details on the embedding size.\n- **Reviewer 5:** The authors addressed the technical and presentation issues, including the use of the sigmoid function and the evaluation protocol.\n\n### Final Decision\nThe paper presents a novel and effective method for answering complex queries on incomplete KGs. The methodology is sound, the experiments are comprehensive, and the results are state-of-the-art. The authors have addressed the reviewers' concerns and provided additional experiments to strengthen their claims. The paper is well-written and makes a significant contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Summary of the Paper**\nThe paper introduces \"Deep Coherent Exploration,\" a method for improving exploration in deep reinforcement learning (RL) algorithms for continuous control tasks. The method generalizes step-based and trajectory-based exploration by modeling the last layer parameters of the policy network as latent variables and using a recursive inference step for policy updates. The authors evaluate their method on A2C, PPO, and SAC, showing improvements in performance on several MuJoCo tasks.\n\n#### **Strengths**\n1. **Novelty and Contribution**:\n   - The paper builds on the work of van Hoof et al. (2017) but extends it to deep RL methods, which is a significant contribution.\n   - The method is well-motivated and addresses an important problem in RL, particularly in continuous control tasks.\n   - The authors provide a clear and detailed explanation of the method, including mathematical derivations and algorithmic details.\n\n2. **Empirical Results**:\n   - The empirical results for on-policy methods (A2C and PPO) are strong and demonstrate the effectiveness of the proposed method.\n   - The ablation studies are insightful and help to understand the contributions of different components of the method.\n\n3. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow, with a clear introduction and related work section.\n   - The detailed appendix and mathematical derivations make the method reproducible.\n\n#### **Weaknesses**\n1. **Off-Policy Methods (SAC)**:\n   - The integration of the method with SAC is more heuristic and less theoretically grounded compared to the on-policy methods.\n   - The empirical results for SAC are less convincing, with some tasks showing degradation in performance.\n   - The authors have addressed this concern in their response by proposing a new formulation for Coherent-SAC, but the results are not yet available.\n\n2. **Experimental Scope**:\n   - The experiments are limited to MuJoCo environments, which, while diverse, might not be sufficient to fully validate the method's generalizability.\n   - Additional experiments on more complex or different domains would strengthen the paper.\n\n3. **Hyperparameter Sensitivity**:\n   - The performance of the baselines (NoisyNet-A2C/PPO and PSNE-A2C/PPO) is sometimes worse than the vanilla methods, which raises questions about hyperparameter tuning.\n   - The authors acknowledge this and provide some explanations, but more detailed tuning and analysis would be beneficial.\n\n4. **Presentation and Clarity**:\n   - Some sections, particularly Section 3.2, are dense and could benefit from more high-level descriptions or being moved to the appendix.\n   - The font size in the figures is too small and should be increased for better readability.\n\n#### **Author Response**\n- The authors have addressed many of the concerns raised by the reviewers, particularly regarding the integration of SAC and the clarity of the presentation.\n- They have proposed a new formulation for Coherent-SAC and are working on the empirical results, which, if positive, would significantly strengthen the paper.\n- The authors have also committed to improving the presentation and addressing other minor issues.\n\n### Final Decision\nGiven the significant contributions of the paper, the strong empirical results for on-policy methods, and the authors' responsiveness to the reviewers' concerns, I believe the paper should be accepted for publication. However, the authors should be required to include the new results for Coherent-SAC and address the remaining issues in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Novelty and Relevance**:\n   - The paper introduces a novel approach to disentangling representations in pre-trained transformers by learning binary masks over weights or hidden units. This is a significant contribution to the field of representation learning and disentanglement, which is a critical area for improving model robustness and interpretability.\n   - The method is innovative in that it leverages pre-trained models without fine-tuning, which is computationally efficient and practical.\n\n2. **Experimental Design**:\n   - The experiments are well-designed and cover multiple aspects of disentanglement, including sentiment/genre classification and syntax/semantics disentanglement.\n   - The use of spurious correlations to evaluate robustness is a strong point, as it addresses a real-world issue in NLP models.\n\n3. **Comparative Analysis**:\n   - The paper compares the proposed method with several baselines, including VAEs and adversarial training, and shows competitive or better performance in most cases.\n   - The results are supported by both quantitative metrics and qualitative visualizations, providing a comprehensive evaluation.\n\n4. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The method is clearly explained, and the experimental setup is well-documented.\n\n#### Weaknesses and Concerns\n\n1. **Generalizability**:\n   - The experimental setup is somewhat limited, focusing on specific genres (Drama and Horror) and a specific domain (movie reviews). While the authors provide a rationale for this choice, more diverse experiments would strengthen the paper's claims.\n   - The paper could benefit from additional experiments on other domains (e.g., books, music) and more labels to demonstrate the method's generalizability.\n\n2. **Comparative Analysis**:\n   - The comparison with VAEs in the sentiment/genre experiment was initially missing but has been added in the author response. However, the paper could still benefit from a more comprehensive comparison with other disentanglement methods.\n   - The performance on semantic tasks (Figure 4) is not significantly better than the fine-tuned baseline, which raises questions about the method's effectiveness in certain scenarios.\n\n3. **Technical Details**:\n   - The paper lacks some specific details about the implementation, such as which layers are masked and the exact configuration of the masks. These details are important for reproducibility and further research.\n   - The authors could provide more intuition about the learned masks, such as the sparsity and overlap between masks for different attributes.\n\n4. **Impact and Broader Implications**:\n   - While the paper demonstrates the method's effectiveness in specific tasks, it could benefit from a more detailed discussion of the broader implications and potential applications of disentangled representations in NLP.\n   - The paper could also explore the limitations and potential challenges of the proposed method, such as the computational cost and the robustness to different types of spurious correlations.\n\n### Final Decision\n\nDespite the concerns and areas for improvement, the paper presents a novel and significant contribution to the field of disentanglement in NLP. The method is innovative, the experiments are well-designed, and the results are promising. The authors have addressed many of the reviewers' concerns in their response, and the paper has the potential to inspire further research in this area.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, for dynamic task weighting in multitask learning. The method leverages task-specific updates of the model parameters to adjust task weights during training, which is a unique and intuitive approach.\n   - The paper is well-written and easy to follow, with clear motivation and a well-structured presentation of the methodology.\n\n2. **Empirical Evaluation**:\n   - The authors provide empirical evaluations on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method.\n   - The results on MultiMNIST show that $\\alpha$VIL outperforms other methods in terms of mean performance, which is a positive sign.\n\n3. **Comparative Analysis**:\n   - The paper compares $\\alpha$VIL with standard multitask learning, single-task training, and a strong baseline (DIW). The comparison is fair and provides a clear context for the performance of $\\alpha$VIL.\n\n#### Weaknesses of the Paper\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation. The reviewers have pointed out the need for a formal analysis or theoretical justification of the proposed methodology. This is a significant gap that needs to be addressed.\n\n2. **Empirical Results**:\n   - The experimental results on NLU tasks are mixed and not consistently superior to the baselines. The improvements are marginal, and in some cases, the performance is even lower than the baselines.\n   - The lack of statistical significance testing in the results further weakens the empirical validation of the method.\n\n3. **Experimental Setup**:\n   - The choice of datasets and tasks, particularly the use of MultiMNIST with only one auxiliary task, is somewhat limited. More diverse and complex datasets with multiple auxiliary tasks would provide a stronger validation of the method.\n   - The authors should provide more ablation studies and sanity checks to demonstrate the robustness and effectiveness of $\\alpha$VIL.\n\n4. **Algorithmic Clarity**:\n   - Some aspects of the algorithm, such as the subtraction of 1 from the alpha parameters, are not clearly explained. The authors should provide a more detailed and intuitive explanation of these steps.\n\n#### Author Response\n- The authors have acknowledged the weaknesses and provided some clarifications and justifications. They have promised to address the theoretical and empirical gaps in the camera-ready version.\n- The authors have also provided additional context for the choice of datasets and tasks, which helps to understand the experimental setup better.\n\n### Final Decision\nWhile the paper introduces a novel and intuitive method for dynamic task weighting in multitask learning, the lack of strong theoretical justification and the mixed empirical results on NLU tasks are significant concerns. The authors have acknowledged these issues and have committed to addressing them in the camera-ready version. However, given the standards of a top-tier conference, the paper in its current form does not meet the threshold for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Addressing a Significant Problem**:\n   - The paper addresses a critical issue in computational neuroscience: the discrepancy between the large number of supervised updates required by deep neural networks (DNNs) and the biological plausibility of such extensive supervised learning in the primate visual system. This is a significant and underexplored problem.\n\n2. **Innovative Methods**:\n   - The authors propose three complementary strategies to reduce the number of supervised updates: (1) reducing the number of training epochs and images, (2) improving the random distribution of synaptic connectivity, and (3) training only a subset of model synapses. These strategies are novel and thought-provoking, especially in the context of biological plausibility.\n\n3. **Thorough Experiments**:\n   - The experiments are well-designed and thorough. The authors provide detailed results and statistical analyses to support their claims. The combination of these strategies results in a significant reduction in the number of supervised updates while maintaining a high match to the primate ventral stream.\n\n4. **Relevance to the Field**:\n   - The findings are relevant to both computational neuroscience and machine learning. They offer insights into how DNNs can be made more biologically plausible and efficient, which is a valuable contribution to the field.\n\n5. **Clarity and Writing**:\n   - The paper is well-written and clearly structured. The authors provide a clear motivation for their work and a detailed explanation of their methods and results.\n\n#### Weaknesses of the Paper\n\n1. **Interpretation of Brain-Score**:\n   - The primary concern raised by the reviewers is the interpretation of the Brain-Score metric. The reviewers argue that a high Brain-Score does not necessarily imply a high biological match. The authors have acknowledged this concern and plan to update the language to make it clear that the Brain-Score is a limited proxy for biological similarity. However, this remains a significant limitation.\n\n2. **Biological Plausibility of Weight Compression**:\n   - The weight compression (WC) method, while effective, is not biologically plausible. The authors acknowledge this and clarify that WC is used to explore the hypothesis that evolution might have found better initialization strategies. However, this method's biological relevance is still questionable.\n\n3. **Assumptions and Theoretical Foundation**:\n   - The paper makes several assumptions without providing a strong theoretical foundation. For example, the assumption that the primate visual system develops primarily through supervised learning is not well-supported by biological evidence. The authors have addressed this by clarifying their framework and commitments, but the assumptions still need to be critically evaluated.\n\n4. **Generalization and Robustness**:\n   - The generalization of the results to new datasets and models is not fully explored. While the authors provide some evidence of generalization, more extensive validation is needed to ensure the robustness of their findings.\n\n#### Addressing Reviewer Concerns\n\n1. **Brain-Score Interpretation**:\n   - The authors have committed to updating the language to make it clear that the Brain-Score is a limited proxy for biological similarity. They also plan to include additional analyses to support the generalization of the Brain-Score benchmarks. This is a positive step, but the concern remains that the Brain-Score might not be a reliable metric for biological similarity.\n\n2. **Biological Plausibility**:\n   - The authors have clarified that the weight compression method is not a biologically plausible model of post-natal development but rather an exploration of alternative initialization strategies. They also plan to discuss related works that address other aspects of biological learning. This helps to contextualize their approach but does not fully address the biological plausibility concern.\n\n3. **Theoretical Foundation**:\n   - The authors have expanded the related work section to discuss approaches addressing other ANN shortcomings of biological learning. They also plan to make their partial approach of improving models more clear. This is a step in the right direction, but the theoretical foundation of their assumptions still needs to be strengthened.\n\n#### Final Evaluation\n\nThe paper makes a significant contribution to the field by addressing a critical problem and proposing innovative methods to reduce the number of supervised updates in DNNs. The experiments are thorough, and the results are promising. However, the interpretation of the Brain-Score metric and the biological plausibility of the weight compression method remain significant concerns. The authors have addressed these concerns to some extent, but the issues are not fully resolved.\n\n### Final Decision: Accept\n\nDespite the weaknesses, the paper's strengths and the significance of the problem it addresses make it a valuable contribution to the field. The authors have shown a willingness to address the concerns raised by the reviewers, and the paper has the potential to stimulate further research and discussion. Therefore, I recommend acceptance with the condition that the authors address the remaining concerns in the final version of the paper."
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **1. Problem Relevance and Novelty**\nThe paper addresses a fundamental problem in distributed machine learning: distributed mean estimation (DME). This problem is crucial for many applications, including distributed stochastic gradient descent (SGD). The authors introduce a novel approach using lattice theory to achieve better error bounds that depend on the distance between inputs rather than their norms. This is a significant improvement over previous methods, especially in scenarios where the inputs are concentrated around the mean but have large norms.\n\n#### **2. Technical Soundness and Contributions**\n- **Main Contribution**: The paper's main contribution is the development of a lattice-based quantization method that allows for efficient communication while maintaining high accuracy. The authors provide both theoretical and practical algorithms, with the theoretical version achieving optimal bounds and the practical version being computationally feasible.\n- **Theoretical Results**: The paper provides matching upper and lower bounds for the communication to error trade-off, which is a strong theoretical contribution. The use of lattice theory to achieve these bounds is novel and well-motivated.\n- **Practical Algorithms**: The practical algorithms, while not achieving the optimal theoretical bounds, are straightforward to implement and show practical improvements over existing methods. The authors also provide a clear explanation of the trade-offs involved.\n\n#### **3. Experimental Evaluation**\n- **Experiments**: The experimental results are provided to validate the practicality of the proposed methods. While the experiments are primarily conducted with \\( n = 2 \\) machines, the authors do include some results for larger \\( n \\) in the appendix. The experiments show that the proposed method outperforms existing methods in terms of communication efficiency and accuracy.\n- **Limitations**: The experiments could be more extensive, especially for larger \\( n \\). However, the authors acknowledge this limitation and plan to add more multi-machine experiments in the revision.\n\n#### **4. Clarity and Presentation**\n- **Writing Quality**: The paper is generally well-written and well-organized. The problem is clearly defined, and the main ideas are well-explained. The authors provide a good overview of related work and motivate their approach effectively.\n- **Minor Issues**: There are a few minor issues with the writing, such as typos and some unclear statements. These are noted in the reviewer comments and can be easily addressed in the revision.\n\n#### **5. Reviewer Feedback and Author Responses**\n- **Reviewer Comments**: The reviewers provide constructive feedback, highlighting both the strengths and weaknesses of the paper. The main concerns are the practicality of the algorithms, the need for more extensive experiments, and some minor technical issues.\n- **Author Responses**: The authors respond to the reviewer comments thoughtfully and provide detailed explanations for their choices. They also commit to making the necessary revisions to address the reviewers' concerns, including adding more multi-machine experiments and clarifying the technical details.\n\n### Final Decision\nGiven the significance of the problem, the novelty and soundness of the proposed approach, and the authors' commitment to addressing the reviewers' concerns, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper proposes a novel approach to integrating language and visual processing by modulating both bottom-up and top-down visual pathways with referring expressions. This is a significant contribution because it challenges the common practice of using language only to direct top-down attention over high-level visual features. The authors argue that language should influence the entire visual processing pipeline, from pixels to high-level features, which is a fresh perspective in the field of multimodal tasks.\n\n#### Experimental Results\nThe experimental results show that the proposed model achieves state-of-the-art or competitive results on several referring expression datasets. The ablation studies demonstrate that both top-down and bottom-up modulation are essential for the model's performance. However, the results on some metrics (e.g., UNC testA/testB, UNC+ testB, and ReferIt) are not as strong as those on others, which raises concerns about the model's robustness and generalizability. The authors attribute this to the lack of over-tuning, but this explanation is not entirely convincing without further statistical significance tests.\n\n#### Quality and Significance\nThe paper is well-written and the model is well-described. The authors provide a clear motivation for their approach, drawing on cognitive science literature to support their hypothesis. However, the lack of a detailed error analysis and the absence of statistical significance tests are significant drawbacks. These issues make it difficult to fully trust the empirical results and understand the exact contribution of the model.\n\n#### Originality\nThe model is innovative in its approach to integrating language throughout the visual pathway. While the authors acknowledge the influence of previous work (e.g., Step-ConvRNN), they clearly differentiate their model by focusing on both top-down and bottom-up modulation. The cognitive science references add a unique dimension to the work, making it more than just a technical improvement.\n\n#### Addressing Reviewer Concerns\nThe authors have addressed some of the reviewer comments, such as adding figures and citations, and providing additional experimental details. However, they have not fully addressed the concerns about the statistical significance of their results and the lack of a detailed error analysis. These issues are critical for a top-tier conference, where the quality and robustness of empirical results are paramount.\n\n### Final Decision\nDespite the paper's novel contribution and well-motivated approach, the lack of statistical significance tests and a detailed error analysis are significant weaknesses. These issues raise doubts about the robustness and generalizability of the model's performance. Therefore, I recommend rejecting the paper for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces GG-GAN, a geometric graph generative adversarial network, which aims to address key challenges in graph generation, such as modeling complex relations, isomorphic graphs, and fully exploiting the latent distribution. The authors claim that GG-GAN is permutation equivariant and scales well to generate large graphs. While the idea of using GANs for graph generation is not entirely novel, the specific geometric approach and the theoretical support provided are promising.\n\n#### Theoretical Support\nThe paper includes several theorems and propositions to support the proposed method. However, some reviewers have pointed out that the theoretical analysis is weak and the connection between the theorems and the practical implementation is not well-established. For instance, the existence of distribution \\(\\mathcal{D}_x\\) in Corollary 1 is considered naive, and the necessity of the collision avoidance mechanism is not well-justified.\n\n#### Experimental Evaluation\nThe experimental section is a significant area of concern. The datasets used are small, with graphs having only 9 or 20 vertices. This limits the ability to evaluate the scalability and effectiveness of the proposed method on larger graphs. Additionally, the comparison with baselines is not convincing. The results in Table 1 show that GG-GAN outperforms other methods in only a few metrics, and the inclusion of other relevant baselines such as NetGAN and simpler network models is missing. The lack of experiments on medium and large graphs and the limited improvement over baselines raise questions about the practical utility of GG-GAN.\n\n#### Mode Collapse and Generation Diversity\nOne of the critical issues in GAN-based models is mode collapse, where the generator fails to produce diverse samples. The paper does not provide sufficient evidence to show that GG-GAN avoids this problem. This is a significant concern, as mode collapse can severely limit the effectiveness of the model in generating diverse and realistic graphs.\n\n#### Literature Review and Comparison\nThe paper lacks a comprehensive literature review and a detailed comparison with existing methods. The authors do not clearly explain the key innovations of GG-GAN over previous work, particularly in terms of isomorphism, scalability, and expressive power. This makes it difficult to assess the unique contributions of the paper.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to follow. The authors provide a clear explanation of the method and the theoretical background. However, some sections, such as the introduction of \\(\\phi\\) and the collision avoidance mechanism, are not well-explained and could benefit from more detailed justification.\n\n### Final Decision\nGiven the concerns raised by the reviewers, particularly the limited experimental evaluation, the lack of convincing results, and the weak theoretical support, the paper does not meet the standards of a top-tier conference. While the idea is interesting and the method has potential, the current presentation and evaluation are not sufficient to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper proposes a novel framework that integrates the cerebellum's role in the brain with the concept of decoupled neural interfaces (DNI) from deep learning. This integration is a significant contribution because it bridges two distinct fields—neuroscience and deep learning—offering a new perspective on how the cerebellum might solve the credit assignment problem in the brain. The authors argue that the cerebellum acts as a decoupling mechanism, allowing the neocortex to learn more efficiently by approximating temporal feedback signals. This is a compelling and novel hypothesis that has the potential to stimulate further research and discussion in both communities.\n\n#### Clarity and Presentation\nThe paper is generally well-written and structured. The authors have made a concerted effort to address the reviewers' comments, particularly by adding new figures and a new section that highlight specific predictions and experimental findings. The revised manuscript is clearer and more focused, especially in explaining the temporal feedback problem and the use of bootstrapping in the model. The addition of new figures (Fig. 5, 6, and 7) and the extended discussion on predictions are valuable contributions that strengthen the paper.\n\n#### Experimental Validation\nThe authors have conducted a range of experiments to validate their model, including motor reaching tasks, sequential MNIST, and caption generation. These experiments demonstrate the model's ability to learn effectively and provide insights into the cerebellum's role in both motor and cognitive tasks. The ablation studies and correlation analyses are particularly useful in showing how the cerebellum's contributions change over the course of learning. These results are consistent with existing experimental findings, which adds credibility to the model.\n\n#### Scientific Impact\nThe paper's scientific impact is significant. By proposing a new framework that integrates the cerebellum with DNI, the authors open up new avenues for research in both neuroscience and deep learning. The model's predictions, such as the cerebellum's importance in pattern recognition tasks and its role in decorrelating neural activity, are testable and can be compared with experimental data. This interplay between theory and experiment is crucial for advancing our understanding of the brain.\n\n#### Addressing Reviewer Concerns\nThe authors have effectively addressed the concerns raised by the reviewers. They have clarified the use of bootstrapping, the focus on temporal feedback, and the differences between their model and the original DNI. The new figures and predictions provide a stronger foundation for the paper's claims. The minor issues with references have been corrected, and the paper is now more accurate and precise in its citations.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper presents a novel and well-supported hypothesis that bridges neuroscience and deep learning. The authors have addressed the reviewers' concerns effectively, and the revised manuscript is clear, well-structured, and scientifically impactful. The model's predictions and experimental validations provide a strong basis for further research, making this paper a valuable contribution to the field."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This is a unique approach that has not been explored in the same way before.\n   - The method is well-motivated and intuitive, addressing the challenge of estimating the positive or negative influence of auxiliary tasks on the target task.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and a detailed explanation of their methodology.\n   - The experimental setup is well-documented, and the results are presented in a structured manner.\n\n3. **Empirical Evaluation**:\n   - The paper evaluates the proposed method on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating its versatility.\n   - The authors compare their method against strong baselines, including standard multitask learning and Discriminative Importance Weighting (DIW).\n\n#### Weaknesses of the Paper\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation. The authors do not provide a formal analysis or theoretical justification for the proposed methodology, which is a significant concern for a top-tier conference.\n   - The reviewers have pointed out that the method is somewhat ad hoc, and the intuition behind certain steps (e.g., subtracting 1 from all weights) is not well-explained.\n\n2. **Empirical Results**:\n   - The experimental results are not as convincing as they could be. While the method outperforms baselines on MultiMNIST, the improvements on NLU tasks are marginal and sometimes not statistically significant.\n   - The authors acknowledge that improvements in NLU tasks are generally small, but this does not fully address the concern that the method does not consistently outperform baselines.\n\n3. **Experimental Setup**:\n   - The choice of datasets and tasks is somewhat limited. For example, the MultiMNIST dataset only has one auxiliary task, which does not fully demonstrate the method's capability in a multi-task setup with multiple auxiliary tasks.\n   - The authors could benefit from additional ablation studies and more thorough experiments to validate the effectiveness of their method.\n\n4. **Comparisons and Baselines**:\n   - The baselines chosen are appropriate, but the authors could benefit from comparing their method against a wider range of existing dynamic task weighting methods to better establish its novelty and effectiveness.\n\n#### Author Response\n- The authors have addressed some of the reviewers' concerns, particularly regarding the intuition behind certain steps in the algorithm and the choice of datasets.\n- They have also provided additional context for the experimental results and acknowledged the limitations of their method.\n- However, the authors have not fully addressed the need for a theoretical justification or provided additional experiments to strengthen their empirical results.\n\n### Final Decision\nGiven the novel and intuitive approach, the well-written presentation, and the initial promising results on MultiMNIST, the paper has potential. However, the lack of a strong theoretical foundation and the marginal improvements on NLU tasks are significant concerns. The paper would benefit from additional theoretical analysis and more thorough empirical validation.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Technical Novelty and Contribution**\n- **Novelty**: The paper introduces Shoot Tree Search (STS), a method that extends Monte Carlo Tree Search (MCTS) by allowing multi-step expansion. While the idea of multi-step expansion is not entirely new (as noted by Reviewer 1 and Reviewer 5), the authors argue that their approach is novel in the context of modern reinforcement learning with neural network value function approximators. The authors provide a thorough comparison with existing methods and highlight the unique aspects of STS, such as the \"mega-backpropagation\" and the integration with neural network value functions.\n- **Contribution**: The paper provides a clear and detailed explanation of the STS algorithm, including pseudocode and a theoretical analysis. The authors also conduct extensive experiments on challenging domains (Sokoban and Google Research Football) and demonstrate that STS outperforms MCTS and other baselines. The ablation studies and additional experiments provided in the author response further strengthen the claims.\n\n#### **Experimental Evaluation**\n- **Quality and Thoroughness**: The experimental setup is well-designed, and the results are convincing. The authors compare STS with MCTS, random shooting, and other baselines on multiple tasks. The experiments are conducted with a reasonable number of seeds and provide statistical significance. The authors also address the reviewer's concerns by running additional experiments, such as comparing STS with AlphaGo-style leaf evaluation and varying the number of simulations.\n- **Reproducibility**: The paper provides enough details to reproduce the experiments, including hyperparameters and training procedures. The authors also provide links to additional results and graphs, which is a positive aspect.\n\n#### **Presentation and Clarity**\n- **Writing Quality**: The paper is generally well-written and easy to follow. However, there are some areas that need improvement, as noted by the reviewers. For example, the definition of CALCULATE_TARGET in Algorithm 2 is missing, and the explanation of the trade-off between depth and breadth in the search is not as intuitive as it could be. The authors have addressed some of these issues in their response, and they plan to make further improvements in the revised version.\n- **Figures and Tables**: The figures and tables are generally clear, but some of them (e.g., Figure 7 and 8) need to be improved for better clarity. The authors have acknowledged this and plan to make the necessary changes.\n\n#### **Addressing Reviewer Concerns**\n- **Technical Novelty**: The authors have addressed the concerns about the novelty of the multi-step expansion by providing a detailed comparison with existing works and highlighting the unique aspects of STS. They have also run additional experiments to demonstrate the effectiveness of STS in different settings.\n- **Experimental Details**: The authors have provided additional experimental results and clarified the experimental setup, particularly regarding the number of passes and the memory usage. They have also addressed the concerns about the hyperparameters and the initialization of the value network.\n- **Intuition and Discussion**: The authors have provided more intuition and discussion about the advantages of STS, particularly in terms of building a more efficient search tree and reducing bias in value function estimates. They have also addressed the question about the performance of STS in sparse reward settings.\n\n### Final Decision\nBased on the detailed analysis, the paper presents a novel and well-supported method that extends MCTS with multi-step expansion. The experimental evaluation is thorough and convincing, and the authors have addressed the reviewer concerns effectively. While there are some areas that need improvement, the overall quality of the paper is high, and it meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Problem Relevance**: The paper addresses a significant and practical problem in ecological inference, which is crucial for various applications, including political science and election analysis.\n2. **Methodological Innovation**: The paper introduces an efficient approximation to the loss function for ecological inference, enabling the use of linear models, deep neural networks, and Bayesian neural networks. This is a novel and valuable contribution.\n3. **Real-World Application**: The authors apply their methods to real-world data from the Maryland 2018 midterm elections, demonstrating the practical utility of their approach.\n4. **Multi-Task Learning**: The paper shows that joint learning across multiple races improves the accuracy of ecological inference, which is a significant finding.\n5. **Latent Representations**: The paper demonstrates that learned latent representations outperform raw covariates for leave-one-out prediction, which is an important result.\n\n#### Weaknesses\n1. **Clarity and Structure**: The paper is not easy to follow. The structure could be improved to make it more accessible. Key concepts and methods are not introduced early enough, making it difficult for readers to understand the contributions.\n2. **Experimental Details**: The paper lacks detailed information on the implementation of the proposed methods and baselines. This hampers reproducibility and makes it difficult to verify the results.\n3. **Baseline Comparisons**: The paper does not compare the proposed deep multi-level model (MLM) with a standard MLM, which is crucial for isolating the effects of deep learning.\n4. **Analysis Depth**: The analysis is incomplete and unconvincing. Claims about the behavior of the deep MLM are not well-supported with quantitative evidence.\n5. **Typos and Writing Issues**: The paper contains numerous typos and minor writing problems, which detract from its overall quality.\n6. **Related Work**: The paper does not adequately cite related work, particularly in the area of learning with label proportions (LLP) and other related problems in machine learning.\n7. **Synthetic Data**: The paper could benefit from experiments on synthetic data to validate the proposed methods under controlled conditions.\n8. **Hyperparameter Optimization**: The paper does not provide guidance on hyperparameter optimization, which is important for practical applications.\n\n### Final Decision\nGiven the significant strengths of the paper, including its relevance, methodological innovation, and real-world application, it has the potential to make a valuable contribution to the field. However, the weaknesses in clarity, experimental details, and analysis depth are substantial and need to be addressed. The paper would benefit from a thorough revision to improve its structure, provide more detailed experimental information, and include a more comprehensive analysis.\n\nFinal Decision: **Reject**\n\nThe paper should be rejected in its current form but with the strong recommendation for a major revision. The authors should address the issues outlined in the review to strengthen the paper and make it suitable for publication in a top-tier conference."
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper aims to bridge the gap between theoretical insights and practical applications in meta-learning, specifically focusing on few-shot learning. The main contributions are:\n1. **Theoretical Integration**: The paper reviews recent theoretical advances in meta-learning and proposes regularization terms based on these theories to improve the generalization of meta-learning algorithms.\n2. **Empirical Validation**: The authors conduct experiments on classic few-shot classification benchmarks to validate the effectiveness of the proposed regularization terms.\n\nHowever, the novelty of the proposed regularization terms is questioned by several reviewers. Reviewer 1 points out that the regularization methods are similar to existing techniques like spectral normalization and weight decay. Reviewer 4 also raises concerns about the novelty of the second regularization term, suggesting it is equivalent to \\(L_2\\) weight decay.\n\n#### Technical Soundness\nThe paper is well-organized and clearly written, which is a positive aspect. However, there are several technical concerns:\n1. **Assumptions and Theoretical Consistency**: Reviewer 2 highlights that the theoretical setting in the paper is not fully consistent with the few-shot learning setting, particularly regarding the task distribution and the absence of a validation set in the theoretical formulation.\n2. **Algorithmic Differences**: Reviewer 2 also notes that the meta-learning loss in the paper differs from popular meta-learning objectives, such as MAML, which does not optimize the classifier to convergence.\n3. **Experimental Rigor**: Reviewer 1 and Reviewer 3 express concerns about the statistical significance of the improvements and the need for more recent and diverse baselines. Reviewer 3 also suggests conducting synthetic experiments to better understand the impact of the proposed regularization terms.\n\n#### Author Response\nThe authors have addressed some of the concerns in their response:\n1. **Novelty**: They argue that the regularization terms are fundamentally different from existing techniques and provide additional experiments to support this claim.\n2. **Statistical Significance**: They clarify that the improvements are statistically significant and provide additional experiments with hyperparameter tuning.\n3. **Theoretical Assumptions**: They explain that the assumptions are for the true optimal predictors and that the meta-learning process should lead to a consistent estimation of these predictors.\n\nHowever, the responses do not fully address all the concerns:\n1. **Novelty**: The arguments about the differences from existing techniques are not entirely convincing, especially given the similarities noted by the reviewers.\n2. **Experimental Rigor**: While the authors provide additional experiments, the need for more recent and diverse baselines remains unaddressed.\n3. **Theoretical Consistency**: The authors acknowledge the differences between the theoretical setting and the practical implementation but do not provide a thorough discussion on how these differences might impact the results.\n\n### Final Decision\nGiven the concerns about the novelty of the proposed regularization terms, the need for more recent and diverse baselines, and the technical consistency between the theoretical and practical settings, the paper does not meet the high standards expected for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This is a unique approach that has not been explored in the same way before.\n   - The method is well-motivated and intuitive, addressing the challenge of estimating the positive or negative influence of auxiliary tasks on the target task.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and a detailed explanation of their methodology.\n   - The experimental setup is well-documented, and the results are presented in a structured manner.\n\n3. **Empirical Evaluation**:\n   - The paper includes experiments on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method.\n   - The authors compare their method against strong baselines, including standard multitask learning and Discriminative Importance Weighting (DIW).\n\n#### Weaknesses of the Paper\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation. The authors do not provide a formal analysis or theoretical justification for the proposed method, which is a significant gap.\n   - This lack of theoretical grounding makes it difficult to understand why the method works and under what conditions it is expected to be effective.\n\n2. **Empirical Results**:\n   - The empirical results are not as strong as they could be. While the method outperforms baselines on MultiMNIST, the improvements on NLU tasks are marginal and sometimes not statistically significant.\n   - The results on NLU tasks are mixed, and the method does not consistently outperform the baselines. This raises questions about the robustness and generalizability of the proposed method.\n\n3. **Experimental Design**:\n   - The choice of datasets and experimental setup is somewhat limited. The MultiMNIST dataset is artificial and may not fully capture the complexity of real-world multitask learning scenarios.\n   - The NLU tasks are from the GLUE and SuperGLUE benchmarks, but the results are not as compelling as they could be. More diverse and challenging datasets could provide a better evaluation of the method's effectiveness.\n\n4. **Ablation Studies and Analysis**:\n   - The paper could benefit from more ablation studies and a deeper analysis of the learned task weights. Understanding how the method behaves under different conditions and why it makes certain decisions would strengthen the paper.\n   - The authors mention that they have additional experiments and analysis that were cut due to page limits. Including these in the camera-ready version would be beneficial.\n\n#### Author Response\n- The authors address some of the reviewers' concerns, particularly regarding the intuition behind the method and the choice of datasets.\n- They acknowledge the need for more theoretical justification and additional experiments, and commit to including these in the camera-ready version.\n- The authors provide a reasonable explanation for the choice of MultiMNIST and the behavior of task weights, but more detailed analysis and justification would still be beneficial.\n\n### Final Decision\nGiven the novel and intuitive approach, the well-written presentation, and the authors' commitment to addressing the weaknesses in the camera-ready version, the paper has the potential to make a valuable contribution to the field. However, the lack of strong theoretical justification and the marginal empirical results are significant concerns that need to be addressed.\n\nFinal Decision: [Reject]\n\nThe paper should be rejected for the current submission but with the strong recommendation that the authors address the identified issues and resubmit a revised version for future consideration."
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in reinforcement learning (RL) and imitation learning (IL) by combining meta-RL and IL to improve sample efficiency and adaptability to new tasks.\n   - The motivation for using demonstrations to guide exploration in sparse reward settings is well-justified and aligns with practical needs in real-world applications.\n\n2. **Empirical Results:**\n   - The proposed method, PERIL, outperforms baselines in several benchmark tasks, demonstrating its effectiveness.\n   - The visualizations and t-SNE plots provide useful insights into the latent space and the method's ability to generalize to unseen tasks.\n\n3. **Methodological Contributions:**\n   - The paper introduces a novel approach that integrates demonstrations into the meta-RL framework, which is a valuable contribution to the field.\n   - The use of auxiliary losses and mutual information terms to improve the latent space representation is a promising direction.\n\n#### **Weaknesses:**\n1. **Clarity and Notation:**\n   - The paper suffers from significant clarity issues, particularly in the notation and mathematical formulation. There are inconsistencies, variable name clashes, and some parts of the objective function are not properly introduced or explained.\n   - The problem formulation and the proposed solution do not always align, leading to confusion. For example, the distinction between the latent variable \\( z \\) and the true state \\( s \\) is not clearly made.\n\n2. **Complexity and Ablation Studies:**\n   - The method is quite complex, with multiple loss terms and components. The paper does not provide a clear explanation of why each component is necessary or how they interact.\n   - The lack of ablation studies makes it difficult to understand which parts of the method are responsible for the observed performance improvements.\n\n3. **Experimental Design:**\n   - The experiments lack multiple seeds and statistical significance tests, which are crucial for validating the results.\n   - The comparison to other methods that combine meta-IL and meta-RL (e.g., Mendonca et al., Zhou et al.) is missing, which is a significant flaw in the experimental section.\n   - The claim of zero-shot learning is not well-supported, as the method still requires some samples from the target environment.\n\n4. **Related Work:**\n   - The treatment of prior work is limited and does not adequately acknowledge many relevant methods in context-based meta-RL and adaptive RL.\n   - The paper does not provide a clear comparison to closely related works like PEARL and meta-IL, making it difficult to understand the novelty and contributions.\n\n5. **Assumptions and Realism:**\n   - The assumption that an expert distribution \\( p_{\\pi_E}(\\tau \\mid z) \\) is available is strong and may not be realistic in many practical scenarios.\n   - The tasks used in the experiments do not require sophisticated exploration, which limits the generalizability of the results.\n\n### Final Decision\nGiven the significant weaknesses in clarity, experimental design, and the lack of ablation studies, the paper is not ready for publication in a top-tier conference. While the problem addressed is important and the empirical results are promising, the paper needs substantial revisions to address the identified issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Title and Abstract\n- **Title**: The title \"Automatic Music Production Using Generative Adversarial Networks\" is indeed misleading. The paper focuses on automatic music accompaniment, which is a specific subset of music production. The authors have acknowledged this and provided a justification for keeping the title, but it remains a significant issue. A more accurate title would be \"Automatic Music Accompaniment Using Generative Adversarial Networks.\"\n- **Abstract**: The abstract has been revised to clarify the scope and motivation of the work. However, the claim that they are the first to treat music audio as images is not accurate, as this is a common practice in many MIR tasks. The authors have addressed this by adding relevant citations.\n\n#### Introduction\n- **Motivation**: The introduction has been revised to better motivate the need for automatic accompaniment. The authors have added details about the limitations and advantages of the waveform/frequency domain and the symbolic domain.\n- **Demucs Algorithm**: The authors have added more details about the Demucs algorithm, including its limitations and potential artifacts.\n- **Computational Cost**: The claim about low computational cost has been clarified. The authors now explain that their method is fully parallelizable and does not require the lengthy autoregressive mechanisms used by other models.\n\n#### Related Works\n- **Symbolic Domain**: The authors have added relevant citations to the literature on music generation in the symbolic domain. This improves the literature review and provides a more comprehensive context for their work.\n- **Raw Audio Representation**: The authors have added a balanced discussion about the advantages and disadvantages of raw audio representation compared to symbolic representation.\n\n#### Method\n- **Demucs Algorithm**: The authors have provided more details about the Demucs algorithm, including its architecture and potential artifacts.\n- **CycleGAN**: A dedicated section has been added to describe the CycleGAN used in the paper, including its architecture, training details, and hyper-parameters.\n- **Mel Scale**: A relevant citation has been added to explain the Mel scale.\n\n#### Experiments\n- **Dataset Selection**: The authors have provided more details about the selection of the pop music subset and the number of songs used from the MusDB18 dataset.\n- **Evaluation Metrics**: The authors have clarified the choice of the four attributes (quality, euphony, coherence, and intelligibility) and the features (STOI, FID) used for evaluation. They have also added a correlation matrix for the annotators and rephrased the section to better explain their intentions.\n- **Phase Retrieval**: The authors have added a detailed explanation of the phase retrieval and time domain signal reconstruction methods.\n\n#### Minor Comments\n- **Text Clarity**: The authors have addressed several minor comments to improve the clarity and readability of the text.\n- **Reproducibility**: The authors have committed to releasing the code and providing all necessary details to reproduce the findings.\n\n### Final Decision\nThe authors have addressed most of the significant issues raised by the reviewers. The paper now provides a more accurate and comprehensive description of the work, including the limitations and details of the methods used. However, the misleading title remains a concern, and the authors should consider revising it to better reflect the specific focus of the paper.\n\nFinal Decision: Accept\n\nThe paper should be accepted for publication, but the authors should consider revising the title to \"Automatic Music Accompaniment Using Generative Adversarial Networks\" to accurately reflect the scope of their work."
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution:**\n- **Block Minifloat (BM) Representation:** The paper introduces a new representation for training DNNs with low precision (4-8 bits). The key innovation is the use of a shared exponent bias, which allows for smaller exponent bits and more efficient hardware implementation. This is a significant contribution, especially in the context of specialized hardware for DNN training.\n- **Kulisch Accumulator:** The authors propose using a Kulisch accumulator for minifloat dot products, which is a novel and efficient approach. This is particularly important for reducing the hardware overhead and power consumption.\n\n#### **2. Clarity and Presentation:**\n- **Clarity:** The paper is generally well-written, and the idea is clearly presented. However, there are several areas where the clarity can be improved:\n  - **Exponent Bias and Kulisch Accumulator:** The explanation of why smaller exponents are critical for Kulisch accumulation is not sufficiently detailed. The authors have addressed this in the revised draft, but it could still benefit from more mathematical and intuitive explanations.\n  - **Equation 6:** The derivation and intuition behind Equation 6 are not well-explained. The revised draft has improved this, but it remains a point of confusion.\n  - **Tables and Results:** The tables and results are somewhat confusing, particularly the use of multiple formats in the same experiment. The authors have addressed this in the revised draft, but it still requires careful reading to understand.\n\n#### **3. Experimental Evaluation:**\n- **Hardware Evaluation:** The hardware evaluation is thorough and provides impressive results. The authors have synthesized a 4x4 systolic array multiplier using BM units and reported area and power savings. This is a strong contribution and adds credibility to the proposed method.\n- **Software Simulations:** The software simulations are comprehensive, covering a range of models and datasets. However, the inclusion of larger networks like ResNet50 on ImageNet would have strengthened the paper. The authors explain that the training time was a bottleneck, but this is a limitation that should be acknowledged.\n\n#### **4. Addressing Reviewer Comments:**\n- **Reviewer 1:** The authors have addressed most of the reviewer's comments, particularly regarding the explanation of the exponent bias and Kulisch accumulator. However, the explanation of the hardware overhead for denormal numbers and the impact on model convergence could be more detailed.\n- **Reviewer 2:** The authors have addressed the major issue regarding the overhead of supporting multiple formats in hardware. The revised draft clarifies that the (6e, 9) format is only used for weight gradients and does not involve matrix multiplication. The minor issues regarding power modeling and the inclusion of log-BM have also been addressed.\n- **Reviewer 3:** The authors have improved the explanation of the minifloat distribution alignment and the derivation of Equation 6. The revised draft also clarifies the notation and the use of multiple formats in the experiments.\n\n### Final Decision\nThe paper introduces a novel and efficient representation for training DNNs with low precision, backed by thorough hardware and software evaluations. While there are some areas where the clarity and detail could be improved, the authors have made significant revisions to address the reviewer comments. The contributions are substantial and the results are impressive.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Innovative Approach:**\n   - The paper introduces a novel method (MIMO) to train multiple subnetworks within a single model, which can improve robustness and uncertainty estimation without significantly increasing the number of parameters or computational costs.\n   - The approach is simple and can be evaluated in a single forward pass, which is a significant advantage over traditional ensemble methods that require multiple forward passes.\n\n2. **Thorough Experimental Evaluation:**\n   - The authors provide a comprehensive evaluation of their method on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet) and their out-of-distribution variants.\n   - The experiments cover a wide range of metrics, including accuracy, negative log-likelihood, and calibration error, demonstrating the method's effectiveness.\n\n3. **Empirical Analysis:**\n   - The paper includes detailed empirical analysis to support the claim that the subnetworks behave independently. This includes loss landscape analysis, t-SNE plots, and conditional variance analysis of activations.\n   - The authors also provide a theoretical analysis in the regression case, showing that MIMO can leverage excess capacity in the network.\n\n4. **Clarity and Presentation:**\n   - The paper is well-written and easy to follow, with clear explanations and well-structured sections.\n   - The method framework is explained with a clear figure (Figure 1), and the training methods are detailed, including techniques like input repetition.\n\n#### **Weaknesses:**\n1. **Originality:**\n   - While the MIMO approach is novel, the paper could benefit from a more detailed comparison with related work, particularly with other multi-branch architectures like BatchEnsemble.\n   - The authors acknowledge that the independence of subnetworks is an empirical observation and leave a deeper theoretical understanding for future work.\n\n2. **Computational Costs:**\n   - The authors claim that MIMO's computational cost is equivalent to a standard deep neural network at test time, but this claim could be better supported with more detailed measurements, such as FLOPs.\n   - The concern about the batch size being implicitly decreased by replicating the same input M times is valid and should be addressed more clearly.\n\n3. **Dataset Diversity:**\n   - The paper primarily evaluates the method on image classification benchmarks. While these are standard datasets, additional experiments on more diverse datasets (e.g., OpenImages) would strengthen the paper's impact.\n\n4. **Theoretical Understanding:**\n   - The paper lacks a deeper theoretical analysis of why and how the subnetworks become independent. While the empirical evidence is strong, a theoretical explanation would make the paper more robust.\n\n#### **Author Response:**\n- The authors have addressed many of the reviewers' concerns, including clarifying the source of the 1% increase in model parameters, providing FLOPs measurements, and explaining the independence of subnetworks.\n- They have also expanded on the relationship between MIMO and BatchEnsemble and provided additional theoretical insights in the regression case.\n- However, the authors could further address the concern about the batch size and provide more detailed comparisons with related work.\n\n### Final Decision\nDespite the weaknesses, the paper presents a novel and effective method for improving robustness and uncertainty estimation in neural networks. The thorough experimental evaluation and empirical analysis support the claims, and the authors have addressed many of the reviewers' concerns in their response. The paper's contributions are significant and align with the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Problem and Contribution\nThe paper addresses the important problem of calibrating predictive uncertainties for out-of-distribution (OOD) data in deep image classifiers. This is a critical issue in many real-world applications where models need to provide reliable confidence estimates, especially when the test data distribution differs from the training data distribution. The authors propose a post-hoc calibration method that uses outlier exposure to improve the calibration of model probabilities on OOD data.\n\n#### Strengths\n1. **Relevance and Importance**: The problem of OOD calibration is highly relevant and important in practical applications.\n2. **Novelty**: The proposed method is novel and builds on existing work in a meaningful way. It uses a specific type of corruption (contrast) to calibrate the model, which then generalizes to other types of corruptions.\n3. **Effectiveness**: The method shows consistent improvements in calibration metrics (ECE) across various types of corruptions and datasets, including CIFAR-10 and ImageNet.\n4. **Simplicity and Efficiency**: The method is simple and computationally efficient, making it practical for real-world applications.\n5. **Generalization**: The method works well even when the test data is completely OOD, such as SVHN for CIFAR-10 and Fashion-MNIST for MNIST.\n\n#### Weaknesses and Addressed Concerns\n1. **Assumptions and Generalization**: Initially, there were concerns about the method's assumptions and its ability to generalize to different types of corruptions. The authors have addressed these concerns by providing additional experiments and clarifications:\n   - They have added results for a variety of corruptions, including translations and rotations, and shown that the method performs well.\n   - They have evaluated the method on completely OOD datasets (SVHN, Fashion-MNIST, Not-MNIST) and demonstrated its effectiveness.\n2. **Clarity and Terminology**: The paper's terminology and notation were initially unclear. The authors have made significant improvements in the revised version, providing clearer definitions and explanations.\n3. **Baseline Comparisons**: The authors have added a baseline comparison using non-corrupted data for calibration, showing that the use of corrupted data is necessary for better calibration.\n4. **Related Work**: The authors have included recent related work and provided a more comprehensive discussion of the literature.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Initially had significant concerns about the method's assumptions and generalization. After the authors' revisions, the reviewer has updated their score, but still has some reservations about the baselines.\n- **Reviewer 2**: Found the method simple and effective, with some suggestions for additional experiments and clarifications. The reviewer's concerns were largely addressed in the revised version.\n- **Reviewer 3**: Had concerns about the clarity and generalization of the method. The revised version addressed these concerns, and the reviewer increased their rating.\n- **Reviewer 4**: Found the method interesting and effective, with some suggestions for additional analysis. The reviewer's concerns were addressed, and they increased their rating.\n\n### Final Decision\nGiven the importance of the problem, the novelty and effectiveness of the proposed method, and the thorough addressing of initial concerns, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Clarity and Presentation**\n- **Clarity**: The paper suffers from significant issues in clarity and presentation. The authors reference an unpublished work extensively, which makes it difficult to verify the theoretical foundations. The notation and definitions are often unclear, and important concepts are relegated to footnotes, making the main text hard to follow.\n- **Presentation**: The paper is poorly organized and hard to read. The motivation for the pruning steps is not well-explained, and the theoretical results are not presented as rigorous theorems. The experimental section is vague and lacks detail, making it hard to reproduce the results.\n\n#### 2. **Theoretical Contributions**\n- **Novelty**: The authors claim to have a novel algorithm for pruning ReLU networks based on a theoretical framework. However, the theory is not well-explained, and the connection between the theory and the algorithm is not clear. The authors also mention that the theory is based on an unpublished work, which makes it impossible to verify the claims.\n- **Originality**: While the idea of combining neurons based on their vector representation is interesting, the authors do not provide a clear derivation or motivation for this approach. The method of combining neurons that are always active is not novel and is implicit in the literature on linear regions of ReLU networks.\n\n#### 3. **Experimental Evaluation**\n- **Scope**: The experiments are limited to toy datasets and MNIST, which are not sufficient to validate the effectiveness of the proposed method. The authors do not compare their method to state-of-the-art pruning techniques, making it difficult to assess the practical value of their approach.\n- **Quality**: The experimental results are not convincing. The authors do not provide statistical significance measures, and the results are not compared to existing methods. The experiments lack depth and breadth, and the authors do not explore the impact of hyperparameters on the performance of their algorithm.\n\n#### 4. **Author Responses**\n- **Clarity**: The authors acknowledge the need for improvement in clarity and presentation. They provide some additional explanations and clarifications, but these are not sufficient to address the major issues in the paper.\n- **Experiments**: The authors promise to add more experiments and comparisons to state-of-the-art methods. However, these improvements are not yet reflected in the current version of the paper.\n- **Theoretical Foundation**: The authors provide some additional details about their theoretical framework, but the lack of a published reference and the unclear derivation of the algorithm remain significant issues.\n\n### Final Decision\nGiven the significant issues in clarity, presentation, and experimental evaluation, and the lack of a clear and verifiable theoretical foundation, the paper is not ready for publication in a top-tier conference. The authors should address the major issues and provide a more rigorous and well-organized presentation of their work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Clarity and Presentation**\nThe paper suffers from significant issues in clarity and presentation. Multiple reviewers have pointed out that the paper is hard to read, poorly organized, and contains numerous typos and grammatical errors. The authors have acknowledged these issues and have promised to improve the presentation. However, the current state of the paper makes it difficult for readers to follow the arguments and understand the contributions.\n\n#### **2. Theoretical Foundation**\nThe paper heavily relies on an unpublished work by the authors, which is not available for review. This makes it impossible to verify the theoretical claims and the correctness of the proposed algorithm. The authors have provided some explanations in their response, but the lack of a published reference or a detailed derivation in the current paper is a major concern. The theory behind the algorithm, while potentially interesting, cannot be fully evaluated without access to the referenced work.\n\n#### **3. Novelty and Contribution**\nThe reviewers have raised concerns about the novelty of the proposed method. Steps 1 and 2 of the algorithm, which involve removing dead neurons and combining always active neurons, are not particularly novel and are similar to existing techniques in the literature. The third step, which involves clustering neurons based on their vector representation, is more mathematically interesting, but the authors have not provided a clear derivation or motivation for this step. The authors claim that their method is theoretically grounded, but this cannot be verified without the referenced unpublished work.\n\n#### **4. Experimental Evaluation**\nThe experimental evaluation is limited and not convincing. The authors have only conducted experiments on toy datasets and MNIST, which are not complex enough to showcase the effectiveness of the proposed method. The experiments lack comparisons with state-of-the-art pruning techniques, making it difficult to assess the practical value of the method. The authors have promised to add more experiments and comparisons, but the current results are insufficient for a top-tier conference.\n\n#### **5. Author Response**\nThe authors have acknowledged the issues raised by the reviewers and have provided some clarifications. They have promised to improve the presentation, add more experiments, and provide a clearer explanation of the theory. However, these improvements are not yet reflected in the current version of the paper. The authors' response suggests that the paper has the potential to be improved, but the current state is not ready for publication.\n\n### Final Decision\nGiven the significant issues in clarity, the lack of a verifiable theoretical foundation, the limited experimental evaluation, and the need for substantial improvements, the paper should be rejected for publication at this time. The authors are encouraged to address the reviewers' comments, improve the presentation, and provide a more comprehensive evaluation before resubmitting the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Novelty and Contribution**:\n   - The paper introduces a novel exploration strategy, Max-Q Entropy Search (MQES), which is designed to handle both epistemic and aleatoric uncertainties in continuous reinforcement learning (RL) environments.\n   - The method is theoretically grounded in information theory and is designed to be more efficient and stable compared to existing methods.\n\n2. **Empirical Evaluation**:\n   - The authors provide empirical evaluations on Mujoco environments, showing that MQES outperforms state-of-the-art algorithms like SAC and DSAC, especially in sparse-reward settings.\n   - The paper includes comparisons with relevant baselines and provides some ablation studies to understand the impact of different components of the method.\n\n3. **Clarity and Presentation**:\n   - The paper is generally well-written and structured, with a clear introduction and motivation for the proposed method.\n   - The authors have addressed many of the reviewer comments and have made significant improvements to the clarity and rigor of the theoretical derivations.\n\n#### Weaknesses of the Paper\n\n1. **Clarity and Notation**:\n   - Despite the improvements, some parts of the paper, particularly in Section 4.1, remain confusing and lack clarity. The notation and terminology used are not always well-explained, which can make it difficult for readers to follow the derivations.\n   - The mutual information and posterior probability concepts are not clearly defined, leading to confusion in the equations.\n\n2. **Empirical Evaluation**:\n   - The empirical evaluation, while showing promising results, is still somewhat limited. The authors have not provided a comprehensive comparison with a wider range of baselines, including methods like VIME and OAC.\n   - The experiments are conducted with a limited number of seeds (5), which may not be sufficient to draw strong conclusions about the significance of the results.\n   - The choice of a shorter horizon (100 steps) is not well-justified, and the authors should provide more detailed comparisons with the standard horizon (1000 steps).\n\n3. **Theoretical Justification**:\n   - The theoretical derivations, while interesting, lack some rigor. The authors have not provided detailed derivations for some key equations, such as the mutual information and posterior probability.\n   - The impact of hyper-parameters on the performance of MQES is not thoroughly discussed, and the ablation studies are limited.\n\n4. **Generalizability**:\n   - The paper does not fully address the limitations of the method in very sparse-reward settings, where exploration is particularly challenging. The authors claim that MQES can handle sparse-reward settings, but this is not convincingly demonstrated in the experiments.\n\n### Final Decision\n\nDespite the novel and interesting contributions of the paper, the current version has several significant issues that need to be addressed before it can be considered ready for publication in a top-tier conference. The clarity and rigor of the theoretical derivations, the comprehensiveness of the empirical evaluation, and the thoroughness of the ablation studies are all areas that require improvement.\n\nFinal Decision: Reject\n\nHowever, the authors are encouraged to revise the paper based on the detailed feedback provided by the reviewers and resubmit it for consideration in the future."
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Dataset Contribution**: The paper contributes a large-scale dataset of over one million slides, which is a significant resource for the community. This dataset can be a valuable asset for future research in layout representation and graphic design.\n2. **Innovative Approach**: The use of a Transformer-based model for layout representation is novel and aligns with recent trends in deep learning, particularly in self-supervised pre-training. This approach has the potential to generalize well to various downstream tasks.\n3. **Downstream Tasks**: The paper introduces two novel tasks (element role labeling and image captioning) and demonstrates state-of-the-art performance on these tasks. This shows the effectiveness of the proposed model in practical applications.\n4. **Clarity and Writing**: The paper is well-written and easy to follow, which is important for the dissemination of research findings.\n\n#### Weaknesses:\n1. **Evaluation**: The evaluation is a significant weakness. The paper lacks comparisons with relevant baselines, particularly those from recent works like Cao et al. (SIGGRAPH 2019), Li et al. (ICLR 2019), and READ (CVPR 2020). This makes it difficult to assess the true novelty and effectiveness of the proposed method.\n2. **Dataset Details**: The paper does not provide sufficient details about the dataset, such as the distribution of elements, templates, and the completeness of properties. This information is crucial for understanding the richness of the dataset and the appropriateness of the model.\n3. **Task Complexity**: The proposed tasks (element role labeling and image captioning) are relatively simple and do not fully demonstrate the capabilities of the model. More complex tasks, such as layout auto-completion and layout retrieval, are mentioned but not thoroughly evaluated.\n4. **Model Details**: The paper lacks a detailed explanation of the Transformer model and the attention mechanism, which are essential for understanding the technical contributions.\n5. **Baseline Comparisons**: The paper only compares the proposed model with a gradient-boosted decision tree, which is a relatively simple baseline. Comparisons with more sophisticated models, such as Graph Neural Networks, are missing.\n\n#### Author Response:\nThe authors have acknowledged the weaknesses and have committed to addressing them in the next version. They plan to:\n- Add more complex tasks and baselines.\n- Provide more dataset details.\n- Refine the evaluation methods.\n- Clarify the differences from related works.\n- Add more details about the Transformer model.\n\n### Final Decision\nGiven the significant contributions of the dataset and the innovative approach, but considering the current weaknesses in evaluation and model details, the paper has the potential to be a strong contribution if the authors address the identified issues. Therefore, I recommend a **conditional acceptance** with the expectation that the authors will thoroughly address the reviewers' comments and improve the paper accordingly.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\n- **Novelty**: The paper introduces a novel approach to GNNs by incorporating global self-attention mechanisms via adaptive spectral filters. This is a significant contribution because it addresses the limitation of existing GNNs, which often assume local homophily. The authors argue that their method can generalize well to both assortative and disassortative graphs, which is a valuable advancement in the field.\n- **Contribution**: The paper provides a well-motivated and theoretically grounded approach to handling heterophilic graphs. The use of adaptive spectral filters and global attention is a unique and innovative solution. The authors also provide a clear comparison with existing methods, demonstrating the effectiveness of their approach.\n\n#### Technical Soundness\n- **Methodology**: The proposed method is technically sound and well-explained. The use of Chebyshev polynomials to approximate the graph wavelet transform is a practical and efficient approach. The authors address the computational complexity concerns by introducing sparsification techniques, which is a crucial aspect for scaling the model to larger graphs.\n- **Experiments**: The experimental setup is thorough and well-designed. The authors evaluate their model on six benchmark datasets, including both assortative and disassortative graphs. The results show that the proposed model outperforms state-of-the-art baselines on disassortative graphs and performs comparably on assortative graphs. The addition of runtime comparisons and the inclusion of larger datasets (e.g., Chameleon) in the revised paper further strengthens the experimental validation.\n\n#### Clarity and Presentation\n- **Clarity**: The paper is well-written and easy to understand. The authors provide clear explanations of the technical details and the motivation behind their approach. The figures and tables are well-presented and add value to the paper.\n- **Presentation**: The paper is well-organized, and the flow of ideas is logical. The authors address the reviewers' comments effectively in their response, providing additional details and experimental results to clarify their points.\n\n#### Addressing Reviewer Concerns\n- **Reviewer 1**: The authors address the concerns about the novelty and complexity of their method. They provide a detailed explanation of how their approach differs from ChevNet and other methods, and they clarify the computational complexity of their model.\n- **Reviewer 2**: The reviewer liked the paper and found it clear and well-explained. The lack of theoretical results is noted, but the reviewer defers to other reviewers for a more detailed evaluation.\n- **Reviewer 3**: The authors address the concerns about scalability and the need for more experiments on larger disassortative graphs. They provide additional experimental results and promise to include more results in future revisions.\n- **Reviewer 4**: The authors address the concerns about novelty and computational complexity. They provide a detailed explanation of the differences between their method and existing approaches and clarify the computational complexity of their model.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper presents a novel and significant contribution to the field of graph neural networks, particularly in handling heterophilic graphs. The methodology is sound, the experiments are thorough, and the paper is well-written and well-presented. The authors have effectively addressed the reviewers' concerns, and the additional experimental results and clarifications strengthen the paper. Therefore, the paper should be accepted for publication at a top-tier conference."
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n- **Technical Novelty**: The paper introduces the use of Fourier convolutional networks (FourierNets) for 3D snapshot microscopy, which is a novel application of Fourier-domain convolutions. However, the core idea of using Fourier-domain convolutions is not entirely new, as it has been explored in previous works such as Rippel et al. (2015). The authors have acknowledged this and provided citations, but the technical novelty is somewhat limited.\n- **Significance**: The paper addresses a significant challenge in 3D snapshot microscopy by enabling the optimization of highly non-local optical encoders. This is a crucial step forward in the field of computational imaging, particularly for applications requiring fast volumetric imaging. The proposed FourierNets outperform UNets in both simulation and real-world lensless imaging experiments, demonstrating their effectiveness.\n\n#### Empirical Novelty and Significance\n- **Empirical Novelty**: The paper provides a thorough evaluation of the proposed FourierNets on both simulated and real-world datasets. The experiments are well-designed and demonstrate the superiority of FourierNets over UNets in handling highly non-local optical encodings. The lensless imaging results further validate the general utility of FourierNets.\n- **Significance**: The empirical results are significant, especially in the context of 3D snapshot microscopy and lensless imaging. The paper provides a unique dataset of high-resolution confocal ground truth images of zebrafish, which is valuable for the research community.\n\n#### Strengths\n- **Well-Written and Clear**: The paper is well-written and easy to understand, with clear explanations of the FourierNet architecture and the experimental setup.\n- **Thorough Evaluation**: The authors provide a comprehensive evaluation of their method, including comparisons with state-of-the-art methods and a detailed limitations section.\n- **Relevance to ICLR**: The paper aligns well with the ICLR call for papers, particularly in the areas of computational imaging and neuroscience. The integration of machine learning with computational optics is a promising and relevant research direction.\n\n#### Weaknesses\n- **Limited Real-World Experiments**: The main application (3D snapshot microscopy) is evaluated primarily on simulated data. While the authors provide a detailed explanation for this, real-world validation would strengthen the paper.\n- **Comparison with Rippel et al.**: The paper could benefit from a more detailed comparison with Rippel et al. (2015) to clearly demonstrate the specific improvements and innovations in the proposed FourierNets.\n- **Implementation Details**: Some implementation details, such as the memory requirements and training times, could be more clearly explained to ensure a fair comparison with other methods.\n\n### Final Decision\nDespite the limitations, the paper makes a significant contribution to the field of computational imaging by introducing FourierNets for 3D snapshot microscopy. The empirical results are strong, and the paper is well-written and relevant to the ICLR community. The authors have addressed the reviewers' concerns and provided additional details to improve the clarity and robustness of their work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Technical Novelty and Significance\n- **Reviewer 1** and **Reviewer 3** both point out that the technical novelty and significance of the paper are only marginally significant. They argue that the proposed algorithm lacks the key features that make target propagation (TP) attractive, such as the ability to avoid weight transport and the potential for decentralized optimization.\n- **Reviewer 2** and **Reviewer 4** find the algorithm to be novel and straightforward to implement, but they also note that the theoretical guarantees and empirical results are not strong enough to fully support the claims.\n\n#### Empirical Novelty and Significance\n- **Reviewer 1** and **Reviewer 3** are particularly critical of the empirical results, noting that the experiments are not convincing and that the performance on realistic datasets is weak.\n- **Reviewer 2** and **Reviewer 4** acknowledge the empirical results but suggest that more extensive and detailed experiments are needed to fully validate the claims.\n\n#### Correctness\n- **Reviewer 1** and **Reviewer 3** raise several questions and concerns about the correctness of the claims, particularly regarding the relationship between TP and backpropagation (BP) and the handling of multiple minimizers.\n- **Reviewer 2** and **Reviewer 4** generally find the claims to be correct but suggest that more detailed theoretical analysis is needed.\n\n#### Clarity and Structure\n- **Reviewer 2** and **Reviewer 4** praise the clarity and structure of the paper, noting that it is easy to read and well-organized.\n- **Reviewer 1** and **Reviewer 3** also find the paper clear but suggest that some sections need more detailed explanation and discussion.\n\n### Summary of Reviewer Feedback\n- **Strengths:**\n  - The proposed algorithm is straightforward to implement in a differentiable programming framework.\n  - The paper is well-structured and clear.\n  - The algorithm shows some promise in improving optimization for RNNs with long sequences.\n\n- **Weaknesses:**\n  - The technical novelty and significance are only marginally significant.\n  - The empirical results are not convincing and need more extensive validation.\n  - The theoretical analysis is not fully developed, and some claims require more detailed justification.\n  - The paper lacks a comprehensive comparison with other optimization methods for RNNs.\n\n### Final Decision\nGiven the mixed feedback from the reviewers, the paper has some strengths but also significant weaknesses. The technical novelty and empirical significance are not strong enough to justify acceptance at a top-tier conference. The theoretical analysis and empirical validation need substantial improvement to fully support the claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novelty and Significance:**\n   - The paper introduces a novel approach to reinforcement learning (RL) by applying boosting techniques, which is a significant contribution to the field. The idea of converting weak learners into strong policies is interesting and has not been extensively explored in the RL context.\n   - The theoretical analysis is solid, and the paper provides a clear algorithm with provable guarantees. The sample complexity and running time bounds are polynomial in the natural parameters of the problem, which is a strong theoretical result.\n\n2. **Technical Contributions:**\n   - The use of a non-convex variant of the Frank-Wolfe method to overcome the non-convexity of the value function is a significant technical contribution. This approach is novel and leverages recent advances in gradient boosting.\n   - The paper addresses the challenge of large state spaces by ensuring that the complexity does not explicitly depend on the number of states, which is a valuable result for practical applications.\n\n3. **Relevance:**\n   - The problem of efficient RL in large state spaces is highly relevant and challenging. The proposed method has the potential to be useful in a wide range of applications, from robotics to game playing.\n\n#### **Weaknesses:**\n1. **Clarity and Readability:**\n   - The paper is dense and notation-heavy, making it difficult to follow. Many notations are undefined or poorly explained, which hinders the reader's understanding. The authors have acknowledged this issue and promised to add a notation list and improve clarity in the final version.\n   - The paper lacks motivation and intuition for many definitions and algorithmic steps. More supporting text and explanations would significantly improve the readability and accessibility of the paper.\n\n2. **Empirical Evaluation:**\n   - The paper is primarily theoretical and lacks empirical validation. While the authors argue that the focus is on theoretical guarantees, some initial experiments would provide valuable insights into the practical performance of the proposed algorithm. This is a common expectation for papers in top-tier conferences, even if the primary focus is theoretical.\n\n3. **Comparison with Prior Work:**\n   - The paper does not adequately discuss the differences between designing boosted contextual bandits and boosted RL. This comparison is important to highlight the novelty and challenges specific to the RL setting.\n   - The paper could benefit from a more comprehensive discussion of related work, particularly in state compression techniques and other methods for handling large state spaces.\n\n4. **Practicality:**\n   - The practicality of the proposed algorithm is questionable. The authors need to address how the algorithm can be implemented in practice, especially concerning the computation of certain quantities and the verification of assumptions.\n\n#### **Author Responses:**\n- The authors have acknowledged the issues raised by the reviewers and have promised to improve the clarity, add a notation list, and provide more intuition and motivation. These changes are essential and will significantly enhance the paper.\n- The authors have also addressed the practicality concerns and provided some context for the assumptions made in the paper. However, they have not provided concrete examples of weak learners or empirical validation, which are still missing.\n\n### Final Decision\nGiven the significant theoretical contributions and the potential impact of the proposed method, the paper has strong merit. However, the current version has several issues that need to be addressed to meet the standards of a top-tier conference. The authors have committed to making substantial improvements, which should address the main concerns.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions:**\n   - The paper provides a rigorous theoretical analysis of the Feedback Alignment (FA) algorithm for deep linear networks. It offers convergence guarantees for both continuous and discrete dynamics, which is a significant contribution to the field.\n   - The authors introduce the concept of implicit anti-regularization, which is a novel and interesting phenomenon. This adds to the understanding of the dynamics of FA and could have broader implications for other learning algorithms.\n\n2. **Clarity and Structure:**\n   - The paper is well-structured and clearly written. The authors are transparent about their assumptions and provide detailed proofs.\n   - The numerical experiments support the theoretical findings and provide insights into the behavior of FA under different initializations.\n\n3. **Relevance:**\n   - The study of linear networks is a crucial starting point for understanding more complex non-linear networks. The paper builds on existing literature and contributes to the growing body of work on FA and implicit regularization.\n\n#### Weaknesses of the Paper\n\n1. **Assumptions and Generalizability:**\n   - The analysis is heavily dependent on the assumption that the data matrices, FA matrices, and initial weight matrices can be diagonalized simultaneously. This assumption may oversimplify the problem and limit the generalizability of the results.\n   - The convergence results are primarily derived under spectral initialization, which is a restrictive condition. While the authors provide some experiments with non-spectral initialization, more extensive empirical validation is needed to demonstrate the robustness of the findings.\n\n2. **Comparison with Gradient Descent (GD):**\n   - The paper does not provide a comprehensive comparison with GD, which is a critical benchmark. While the authors argue that their results are a first step towards understanding FA, a more detailed comparison would strengthen the paper's impact.\n\n3. **Practical Relevance:**\n   - The practical relevance of the findings is somewhat limited by the focus on linear networks and the specific initialization schemes. The authors acknowledge this gap and suggest future work, but the current results may not be directly applicable to real-world non-linear networks.\n\n#### Reviewer Feedback\n\n- **Reviewer 1 and Reviewer 4** raise valid concerns about the assumptions and the generalizability of the results. They suggest that the one-dimensional dynamics and spectral initialization may oversimplify the problem and limit the significance of the findings.\n- **Reviewer 2** points out that the paper does not explore the alignment of forward and backward weights, which is a key characteristic of FA. They also suggest that the results are limited to linear networks and may not directly translate to non-linear settings.\n- **Reviewer 3** is more positive, finding the paper well-written and the results novel and interesting. They recommend acceptance, noting the paper's contributions to the understanding of FA and implicit regularization.\n\n### Final Decision\n\nDespite the valid concerns raised by the reviewers, the paper makes significant theoretical contributions to the understanding of the Feedback Alignment algorithm. The introduction of implicit anti-regularization is a novel and interesting finding that adds to the field's knowledge. The paper is well-structured and clearly written, and the numerical experiments support the theoretical results.\n\nHowever, the assumptions and the focus on linear networks with specific initializations do limit the generalizability of the findings. The authors should address these limitations in future work and provide more extensive empirical validation to demonstrate the practical relevance of their results.\n\nGiven the significance of the theoretical contributions and the clarity of the paper, I recommend acceptance with the understanding that the authors will address the limitations in future work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Theoretical Contributions**:\n   - The paper introduces a novel PAC-Bayes bound that is non-vacuous and analytically tractable, which is a significant theoretical contribution.\n   - The authors provide an unbiased estimator for the NNGP’s generalization error, which is a valuable tool for understanding the implicit bias of neural architectures.\n   - The paper offers a clear and insightful discussion on the role of margin in generalization, which is a crucial aspect of deep learning.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow, with clear explanations and intuitive insights.\n   - The authors provide a detailed and careful analysis of the behavior of infinite width networks and finite width networks trained by gradient descent.\n\n3. **Empirical Validation**:\n   - The experiments on different widths of MLPs and the comparison with NNGP are interesting and provide valuable insights.\n   - The discussion of margin and its impact on generalization is supported by empirical evidence, which is a strong point.\n\n#### Weaknesses of the Paper\n1. **Limited Scope**:\n   - The analysis is primarily focused on MLPs, and the results may not generalize to more complex architectures like CNNs or ResNets. This is a significant limitation, as the paper claims to provide insights into the generalization of neural networks.\n   - The use of the \"Nero\" optimizer, which is a relatively new and specific algorithm, raises questions about the generalizability of the results to other optimization methods like SGD or vanilla GD.\n\n2. **Experimental Design**:\n   - The experimental setup is somewhat limited and specific. The authors should provide more comprehensive experiments across different datasets, architectures, and optimization methods to strengthen their claims.\n   - The comparison with existing literature, particularly works like Valle-Pérez et al. (2019), is not thorough enough. The authors should provide a more detailed comparison to demonstrate the novelty and significance of their results.\n\n3. **Clarity and Terminology**:\n   - The language surrounding the \"implicit bias of gradient descent\" is somewhat misleading and could be clarified. The authors should be more precise in their claims and avoid attributing specific behaviors to gradient descent without sufficient theoretical support.\n   - The discussion of margin could be more rigorous and formally presented, especially in the context of different loss functions and their impact on margin.\n\n4. **Bound Tightness**:\n   - The bounds derived in the paper are not as tight as they could be, and the gap between the bounds and empirical performance is significant. The authors should provide a more detailed analysis of why this gap exists and how it can be reduced.\n\n### Final Decision\nDespite the significant theoretical contributions and clear presentation, the paper has several limitations that need to be addressed. The experimental design is too specific, and the results may not generalize to more complex architectures and optimization methods. Additionally, the claims about the implicit bias of gradient descent and the role of margin need to be more rigorously supported.\n\nGiven these considerations, the paper should be **rejected** for publication in its current form. However, the authors are encouraged to address the reviewers' feedback and resubmit a revised version with more comprehensive experiments and a clearer, more precise discussion of their claims.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Motivation and Problem Relevance**:\n   - The problem of multi-objective optimization in experimental design is well-motivated and relevant to various scientific and engineering fields.\n   - The platform addresses a significant gap in the availability of user-friendly tools for non-experts in coding and machine learning.\n\n2. **Engineering Contributions**:\n   - **User-Friendly Design**: The platform is designed with a graphical user interface (GUI) that makes it accessible to users with little or no coding experience. This is a significant contribution as it lowers the barrier to entry for using advanced optimization techniques.\n   - **Modular Design**: The modular framework allows for easy integration and testing of new algorithms, which is valuable for researchers in the field.\n   - **Asynchronous Optimization**: The introduction of the Believer-Penalizer (BP) strategy for asynchronous optimization is a novel contribution, especially in the context of multi-objective optimization.\n\n3. **Scientific Contributions**:\n   - **Believer-Penalizer (BP)**: While the idea is relatively simple, it addresses the limitations of existing methods (Kriging Believer and Local Penalization) and provides a robust approach. The authors have identified and addressed failure cases, which is a valuable contribution.\n   - **Empirical Evaluations**: The paper includes extensive empirical evaluations on 12 benchmark problems, demonstrating the robust performance of the platform. The ablation studies and comparisons with other platforms provide a comprehensive assessment of the platform's capabilities.\n\n4. **Real-World Applications**:\n   - The platform is demonstrated to be capable of controlling and guiding real-world hardware experiments, which is a practical and significant application.\n\n#### Negative Aspects:\n1. **Technical Novelty**:\n   - The technical novelty of the BP strategy is limited. While it is a useful contribution, it is not a groundbreaking innovation.\n   - The platform primarily implements existing techniques, and the BP strategy is a straightforward combination of existing methods.\n\n2. **Empirical Results**:\n   - The empirical results show mixed performance. While the platform performs well on some benchmarks, it underperforms on others. The claims of outperforming other methods are overstated and have been revised in the author response.\n   - The real-world example is more of a proof of concept and lacks a strong connection to a real-world benchmark.\n\n3. **Scope and Limitations**:\n   - The platform was initially limited to 2 or 3 objectives, which is a significant limitation. However, the authors have addressed this in the response by incorporating support for single-objective and many-objective optimization.\n   - The theoretical insights behind the BP strategy are not well-supported, and the authors acknowledge this in their response.\n\n4. **Comparisons and Citations**:\n   - The paper could benefit from more comprehensive comparisons with other existing platforms and a more thorough literature review.\n\n### Final Decision\nDespite the limitations and the relatively limited technical novelty, the paper makes significant contributions in terms of user-friendliness, modular design, and practical applications. The platform addresses a real need in the scientific and engineering communities and has the potential to lower the barrier to entry for using advanced optimization techniques. The authors have also addressed many of the reviewers' concerns in their response, particularly regarding the support for a wider range of objectives and the revision of empirical claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Frequency-Based Understanding of Adversarial Examples**:\n   - The paper provides a comprehensive and novel analysis of adversarial examples from a frequency perspective, which is a significant contribution to the field. The authors show that adversarial examples are neither strictly high-frequency nor low-frequency but are dataset-dependent. This is a valuable insight that challenges the common misconception and provides a more nuanced understanding of adversarial robustness.\n\n2. **Empirical and Theoretical Support**:\n   - The paper is supported by extensive empirical experiments and theoretical analysis. The authors have conducted experiments on multiple datasets (CIFAR-10, ImageNet, MNIST, Fashion-MNIST) and different model architectures (ResNet18, VGG16, DenseNet-121, ViT), which strengthens the validity of their claims.\n\n3. **Novel Contributions**:\n   - The paper introduces a new measure of noise gradient to study the frequency properties of adversarial examples. It also proposes a method for adversarial training with frequency constraints, which provides a new tool for controlling the accuracy vs. robustness trade-off. These contributions are novel and offer practical insights for the adversarial training community.\n\n4. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors have provided extensive supplementary material to support their claims, which is a positive aspect. The experimental design is well-thought-out, and the results are clearly presented.\n\n#### Weaknesses of the Paper:\n1. **Overlap with Prior Work**:\n   - Some reviewers have pointed out that the paper's contributions overlap with existing works, particularly in the frequency analysis of adversarial examples. However, the authors have addressed this concern by highlighting the differences in their approach and the unique insights provided by their analysis.\n\n2. **Scope and Generalization**:\n   - The paper primarily focuses on natural images and standard datasets. While this is a common and important domain, the authors could benefit from extending their analysis to other types of images (e.g., radar signals, vibration signals) to provide a more comprehensive understanding of the frequency properties of adversarial examples.\n\n3. **Clarity of Contributions**:\n   - Some reviewers have noted that the contributions of the paper are not clearly highlighted in the introduction. The authors have addressed this in their response by providing a clearer enumeration of their contributions. However, this should be reflected in the final version of the paper to ensure clarity for the readers.\n\n4. **Minor Issues**:\n   - There are minor grammatical errors and issues with figure placement, which the authors have committed to addressing. These are minor and can be easily fixed.\n\n### Final Decision\nDespite the minor weaknesses and overlaps with prior work, the paper provides significant and novel contributions to the field of adversarial robustness. The comprehensive analysis, empirical support, and practical insights make it a valuable addition to the literature. The authors have adequately addressed the concerns raised by the reviewers, and the paper is well-written and well-supported.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Relevance and Interest**: The paper addresses a relevant and interesting problem in multi-modal deep learning, specifically the issue of greedy learning where models tend to rely on one modality while under-utilizing others. This is a significant issue in the field, and the topic is worthy of exploration.\n2. **Empirical Validation**: The authors provide empirical evidence to support their hypothesis through experiments on multiple datasets (Colored MNIST, Princeton ModelNet40, and NVIDIA Dynamic Hand Gesture Dataset). They introduce metrics such as conditional utilization rate and conditional learning speed to quantify the imbalance in modality utilization.\n3. **Proposed Solution**: The paper proposes a balanced multi-modal learning algorithm to address the greedy learning issue. The algorithm is designed to control the training speeds of different modalities, ensuring more balanced utilization.\n4. **Clarity and Structure**: The paper is well-structured and easy to follow. The problem is clearly defined, and the proposed method is explained in a logical manner.\n\n#### Weaknesses of the Paper\n1. **Lack of Theoretical Support**: While the empirical results are strong, the paper lacks a solid theoretical foundation to support the greedy learner hypothesis. The authors acknowledge this and suggest that a theoretical explanation will come after empirical understanding, but this is a significant gap in the current work.\n2. **Inconsistent Results**: Some reviewers noted inconsistencies in the experimental results, particularly in Table 1, where the results between the vanilla and guided methods are not statistically significantly different in some datasets (ModelNet40, NVGesture-scratch, and NVGesture-pretrained). This raises questions about the robustness of the proposed method.\n3. **Limited Comparison with Prior Work**: The paper does not compare the proposed method with other methods that address similar issues, such as RUBi and other techniques for balancing learning rates between modalities. This limits the evaluation of the method's novelty and effectiveness.\n4. **Focus on Visual Modalities**: The experiments are primarily conducted on datasets with visual modalities. The paper could benefit from including experiments with more natural modalities, such as audio and visual, to demonstrate the generalizability of the proposed method.\n5. **Hyperparameter Sensitivity**: The authors did not provide a thorough analysis of the sensitivity of the proposed method to hyperparameters, such as the imbalance tolerance parameter \\(\\alpha\\) and the re-balancing window size \\(Q\\). This is crucial for understanding the robustness and practicality of the method.\n\n#### Author Responses\n1. **Addressing Reviewer Concerns**: The authors have made efforts to address some of the reviewer concerns, such as fixing typos, augmenting related works, and providing additional experiments and analyses. However, some issues, such as the lack of theoretical support and the need for more comprehensive comparisons with prior work, remain unaddressed.\n2. **Additional Experiments**: The authors have provided additional experiments, including the implementation of RUBi on NVGesture-scratch, which shows that their method performs similarly to RUBi. This is a step in the right direction but does not fully address the need for a more comprehensive comparison.\n\n### Final Decision\nGiven the strengths of the paper in addressing a relevant problem and providing empirical validation, but considering the significant weaknesses in theoretical support, inconsistent results, and limited comparison with prior work, the paper does not meet the high standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions**:\n   - The paper provides a novel and comprehensive analysis of the optimization landscape for two-layer ReLU neural networks with L2 regularization.\n   - It extends the results of Pilanci and Ergen (2020) by characterizing all global optima via a convex optimization program, which is a significant theoretical advance.\n   - The paper introduces the concepts of minimal and nearly minimal neural networks, which provide a clear structure for understanding the set of optimal solutions.\n   - It provides a polynomial-time algorithm for checking if a neural network is a global minimum, which is a practical and valuable contribution.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and organized, making the technical content accessible and easy to follow.\n   - The authors provide clear and detailed proofs, which are essential for a theoretical paper of this nature.\n\n3. **Relevance and Impact**:\n   - The results have the potential to inspire further research on the optimization landscape of neural networks, particularly in the context of convexity.\n   - The paper addresses a fundamental problem in deep learning theory, which is highly relevant to the community.\n\n#### Weaknesses of the Paper\n\n1. **Scope**:\n   - The analysis is limited to two-layer ReLU networks, which is a significant limitation. While the authors acknowledge this and suggest future work, the current scope is narrow.\n   - The extension to deeper networks and other activation functions is not fully explored, which is a common criticism from the reviewers.\n\n2. **Technical Issues**:\n   - There are some minor technical issues and typos that need to be addressed, as pointed out by the reviewers. These issues, while not major, could affect the clarity and correctness of the paper.\n   - The positive homogeneity of the ReLU activation function is a key assumption, and the authors should provide more discussion on how this assumption affects the results and whether similar results can be obtained for other activation functions.\n\n3. **Empirical Validation**:\n   - The paper lacks empirical validation, which is a common expectation for papers in machine learning. While the theoretical contributions are strong, some experimental results could strengthen the paper's impact.\n\n#### Reviewer Feedback\n\n- **Reviewer 1**:\n  - Positive about the novel extensions and important theoretical contributions.\n  - Concerns about the scope (single-hidden layer) and some technical issues.\n  - Suggests addressing the extension to multiple layers and other activation functions.\n\n- **Reviewer 2**:\n  - Highly positive, considering it the best among the reviewed papers.\n  - Praises the clean and elegant construction of the mapping and the potential for further exploration.\n  - Suggests discussing the extension to other activation functions and deep networks.\n  - Recommends acceptance.\n\n- **Reviewer 3**:\n  - Positive about the interesting observations and practical ways for constructing optimal neural networks.\n  - Suggests comparing the results with previous work and providing experimental validations.\n  - Recommends addressing the extension to deep networks.\n\n- **Reviewer 4**:\n  - Highly positive, considering it a clear accept.\n  - Praises the complete characterization of global minima and the algorithm for testing optimality.\n  - Suggests addressing the extension to deep networks and other activation functions.\n\n### Final Decision\n\nGiven the significant theoretical contributions, the clarity and organization of the paper, and the positive feedback from the reviewers, the paper should be accepted. The limitations and technical issues can be addressed in the final version, and the lack of empirical validation is a minor concern given the strong theoretical results.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Problem Relevance and Importance**:\n   - The paper addresses a significant issue in diffusion models: the high computational cost of generating high-quality samples. Reducing the number of inference steps while maintaining or improving sample quality is a crucial problem in the field of generative models.\n\n2. **Novelty and Technical Contribution**:\n   - The introduction of Generalized Gaussian Diffusion Models (GGDM) and the Differentiable Diffusion Sampler Search (DDSS) method is novel. The use of non-Markovian samplers and the optimization of perceptual losses to improve sample quality in fewer steps is a significant technical contribution.\n\n3. **Empirical Results**:\n   - The paper demonstrates strong empirical results on multiple datasets, including CIFAR-10 and ImageNet 64x64. The FID scores are significantly improved with fewer inference steps compared to existing methods like DDPM and DDIM.\n\n4. **Clarity and Exposition**:\n   - The authors have made significant improvements in the clarity of the paper, addressing many of the reviewers' concerns. They have provided a clearer motivation for the perceptual loss and established a connection with the Kernel Inception Distance (KID).\n\n5. **Theoretical Grounding**:\n   - The authors have provided a more rigorous theoretical foundation for their method, including a formal proof of Theorem 1 and a detailed explanation of the reverse process and ELBO.\n\n#### Weaknesses of the Paper\n\n1. **Theoretical Guarantees**:\n   - While the authors have improved the theoretical grounding, some reviewers still express concerns about the lack of guarantees for the distribution matching. The use of perceptual losses, while empirically effective, might not have the same theoretical guarantees as optimizing the ELBO.\n\n2. **Generalization to Larger Datasets**:\n   - The paper primarily focuses on smaller datasets like CIFAR-10 and ImageNet 64x64. While the authors promise to include results on larger datasets in the camera-ready version, the current results are limited in scope.\n\n3. **Practicality and Scalability**:\n   - The computational cost of the DDSS optimization process is a concern. The use of gradient rematerialization, while effective, might be computationally expensive, especially for larger datasets.\n\n4. **Clarity and Notation**:\n   - Despite improvements, some reviewers still find the notation and clarity of the paper to be a bit challenging. The double subscripts and the complexity of the equations might make the paper difficult to follow for some readers.\n\n### Final Decision\n\nGiven the significant contributions of the paper, the strong empirical results, and the improvements made in response to reviewer feedback, the paper should be accepted for publication. However, the authors should address the remaining concerns about theoretical guarantees and generalization to larger datasets in the camera-ready version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Theoretical Insights**:\n   - The paper provides a clear theoretical framework for understanding how minimizing a weighted mean can optimize higher-order moments of the loss distribution.\n   - Theorems 1 and 3 are well-structured and provide a solid foundation for the proposed method.\n   - Lemma 2 and Lemma 4 offer insights into the convexity properties of the weighted mean trick, which is crucial for practical optimization.\n\n2. **Empirical Validation**:\n   - The paper includes experimental results that demonstrate the effectiveness of the weighted mean trick in improving robustness against outliers.\n   - The experiments are well-designed and show that the proposed method performs similarly to other specialized robust loss functions.\n\n3. **Clarity and Notation**:\n   - The authors have made significant efforts to improve the clarity of the paper, including the addition of a new section on notation and the revision of theoretical results.\n   - The paper is generally well-written and easy to follow, despite some minor issues.\n\n4. **Relevance**:\n   - The problem of robustness in machine learning is highly relevant, and the proposed method offers a new perspective on how to achieve it.\n   - The paper builds on and extends existing work in the field, such as Duchi & Namkoong (2019) and Li et al. (2021).\n\n#### Weaknesses:\n1. **Theoretical Gaps**:\n   - The paper does not fully address the issue of how the weights depend on the parameters $\\theta$, which is a significant concern raised by Reviewer 2 and Reviewer 5.\n   - The convergence guarantees for Algorithm 1 are not rigorously established, and the explanation provided is somewhat informal.\n\n2. **Empirical Limitations**:\n   - The experiments are limited in scope and do not explore the full range of potential applications of the weighted mean trick.\n   - The paper does not compare the proposed method to more recent or advanced robust optimization techniques, which could provide a more comprehensive evaluation.\n\n3. **Clarity and Notation**:\n   - Despite the improvements, some notational issues and unclear statements remain, as pointed out by Reviewer 4 and Reviewer 6.\n   - The paper could benefit from a more detailed discussion of the practical implications of the theoretical results.\n\n4. **Robustness and Generalization**:\n   - The paper does not fully address the robustness of the method in the presence of heavy-tailed distributions or high levels of contamination, which are important considerations in real-world applications.\n\n### Final Decision\nDespite the strengths of the paper, the theoretical gaps and empirical limitations are significant enough to warrant further work. The paper provides a valuable contribution to the field, but it is not yet ready for publication in a top-tier conference without addressing these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Novel Insight and Problem Setting**:\n   - The paper addresses an important and underexplored area: the behavior of SGD with momentum (SGDm) under non-iid sampling, specifically covariate shift. This is particularly relevant for applications like continual learning and reinforcement learning.\n   - The phenomenon of resonance-driven divergence is a novel and interesting observation. It provides a new perspective on the stability of optimization algorithms under non-iid conditions.\n\n2. **Theoretical Contributions**:\n   - The paper formulates SGDm under covariate shift as a parametric oscillator, which is a novel connection between optimization and nonlinear dynamics.\n   - The use of Floquet theory to analyze the stability of the system is a significant theoretical contribution. While the proof techniques may rely on existing results, the application to this specific problem is novel.\n\n3. **Empirical Validation**:\n   - The empirical results are comprehensive and well-designed. They start with a simple linear setting and progressively relax assumptions to include more complex scenarios, such as non-linear models and different optimizers.\n   - The experiments show good alignment with theoretical predictions, even in the presence of stochasticity, which strengthens the validity of the theoretical findings.\n\n4. **Clarity and Structure**:\n   - The paper is well-written and follows a clear scientific format: phenomenon → hypothesis → analysis. This makes the paper easy to follow and understand.\n\n#### Weaknesses of the Paper\n\n1. **Technical Novelty**:\n   - Some reviewers have raised concerns about the technical novelty of the proof techniques. While the application of these techniques to the specific problem is novel, the techniques themselves are not new.\n   - The paper could benefit from a more nuanced discussion of the technical contributions, acknowledging the reliance on existing results and emphasizing the novel application.\n\n2. **Presentation and Intuition**:\n   - The presentation of Theorem 1 and the explanation of resonance could be improved. The current explanation is somewhat abstract and could benefit from more concrete examples and intuitive explanations.\n   - The \"Example\" section could be expanded to provide more detailed insights into the resonance behavior.\n\n3. **Relevance to Practical Settings**:\n   - While the paper provides a strong theoretical foundation, the practical relevance of the findings could be better demonstrated. The synthetic data used in the experiments is designed to have specific frequency characteristics, which may not be representative of real-world datasets.\n   - The paper could benefit from experiments on real-world datasets or more complex environments to show the practical implications of the resonance phenomenon.\n\n4. **Stochasticity in SGDm**:\n   - The analysis is based on expected gradients, which removes the stochasticity inherent in SGDm. While the authors have provided additional experiments to show that the results hold in the stochastic setting, this is a valid concern that should be addressed more thoroughly.\n\n### Final Decision\n\nDespite the weaknesses, the paper makes a significant contribution to the understanding of SGDm under non-iid sampling. The novel insight into resonance-driven divergence, the comprehensive theoretical analysis, and the well-designed empirical validation make the paper a valuable addition to the literature. The concerns raised by the reviewers can be addressed in the camera-ready version to strengthen the paper further.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Well-Written and Clear**: The paper is well-written and clearly conveys its main points and contributions. This is a significant strength as it ensures that the ideas are accessible to the readers.\n2. **Interesting Approach**: The use of VAEs for time series generation, particularly with the inclusion of interpretable components, is an interesting and novel approach. The idea of incorporating domain knowledge into the generative model is valuable and can be highly advantageous in certain applications.\n3. **Empirical Evaluation**: The authors provide a thorough empirical evaluation on four different datasets, comparing their method to several state-of-the-art data generation methods. The results show that the proposed TimeVAE performs well in terms of similarity and predictability.\n\n#### Weaknesses of the Paper\n1. **Lack of Hyperparameter Tuning and Model Selection**: The introduced decoder building blocks introduce new hyperparameters, but the paper does not provide details on how to best select these hyperparameters or perform model selection. This is a significant gap in the methodology.\n2. **Limited Evaluation of Interpretability**: While interpretability is claimed as one of the main contributions, the paper does not provide any empirical evidence or experiments to support this claim. An ablation study or specific experiments to demonstrate the interpretability of the model would strengthen the paper.\n3. **Comparison to Related Work**: The paper does not adequately compare its method to other relevant works, such as GP-VAE (Fortuin et al., 2020). Including a broader set of benchmarks and discussing the relationship between time-series imputation and next-step prediction would provide a more comprehensive evaluation.\n4. **Technical Clarity and Precision**: The paper contains some imprecise or non-standard claims, such as the categorization of generative models and the explanation of VAEs. These need to be clarified to ensure the technical soundness of the paper.\n5. **Experimental Methodology**: The experiments are conducted on synthetic or novel datasets, which makes it difficult to compare the proposed method to other works in the literature. Including well-known datasets and more extensive experiments would enhance the robustness of the results.\n6. **Limited Contribution to the Field**: The overall contribution of the paper is limited, as the proposed method does not seem to introduce substantial originality or significant improvements over existing methods. The lack of a clear advantage over state-of-the-art methods, especially in terms of interpretability, is a major concern.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, the following points are critical in making the final decision:\n\n- **Technical Soundness**: The paper has some technical issues that need to be addressed, particularly in the areas of hyperparameter tuning, model selection, and interpretability evaluation.\n- **Empirical Evaluation**: While the empirical evaluation is thorough, it lacks the depth and breadth needed to convincingly demonstrate the superiority of the proposed method.\n- **Contribution to the Field**: The paper's contributions are not sufficiently novel or significant to warrant acceptance at a top-tier conference.\n\nFinal Decision: **Reject**\n\nThe paper has several areas that need improvement and does not meet the high standards required for publication at a top-tier conference. Addressing the identified weaknesses and providing a more comprehensive evaluation would be necessary for future submissions."
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Effective Methods:**\n   - The proposed Computation Redistribution (CR) and Sample Redistribution (SR) methods are shown to be effective in improving face detection accuracy, especially for small faces.\n   - The ablation studies and comparative experiments demonstrate significant improvements over the baseline and other state-of-the-art methods.\n\n2. **State-of-the-Art Performance:**\n   - The proposed SCRFD-34GF outperforms the best competitor, TinaFace, by 4.78% (AP at hard set) while being more than 3× faster on GPUs with VGA-resolution images.\n   - The method achieves state-of-the-art performance on the WIDER FACE dataset and other benchmarks like AFW, Pascal, and FDDB.\n\n3. **Practicality:**\n   - The methods are practical and can be applied to real-world applications, such as face detection on mobile devices, where computational efficiency is crucial.\n\n4. **Detailed Experiments:**\n   - The paper includes extensive experiments and ablation studies, providing a thorough evaluation of the proposed methods.\n\n#### **Weaknesses:**\n1. **Search Strategy:**\n   - The search strategies (CR) are described as straightforward and not particularly novel. This is a common criticism from multiple reviewers.\n   - The paper lacks a comparison with other well-known network search methods, such as evolutionary algorithms, which could provide a more comprehensive evaluation of the proposed approach.\n\n2. **Limited Scope:**\n   - The methods are primarily effective for detecting small faces and may have limited applications in other object detection tasks.\n   - The paper could benefit from a more detailed discussion on the generalizability of the proposed methods to other object detection tasks.\n\n3. **Clarity and Detail:**\n   - Some reviewers noted that the paper lacks a figure to illustrate the overall framework and an algorithm to describe the computation redistribution method.\n   - The details of the sample distribution and the search space are not clearly explained, which could be improved in the revised version.\n\n#### **Reviewer Feedback:**\n- **Positive Feedback:**\n  - Reviewers generally agree that the proposed methods are effective and achieve state-of-the-art performance.\n  - The paper is well-supported by extensive experiments and ablation studies.\n  - The practicality and real-world applicability of the methods are highlighted.\n\n- **Negative Feedback:**\n  - The search strategies are described as straightforward and not particularly novel.\n  - The lack of comparison with other network search methods is a significant concern.\n  - Some details and clarity issues need to be addressed in the revised version.\n\n### Final Decision\nDespite the criticisms regarding the straightforward nature of the search strategies and the need for more detailed comparisons, the paper demonstrates significant improvements in face detection accuracy and efficiency. The methods are well-supported by extensive experiments and are practical for real-world applications. The authors have also addressed many of the reviewer's concerns in their responses, providing additional details and comparisons.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Readability**\n- **Reviewer 1 and 2**: Both reviewers found the paper difficult to follow and lacking in clarity. The introduction and problem statement are not well-explained, and key concepts are not defined. The paper needs more concrete examples and a clearer explanation of the proposed benchmark and the SCS representation.\n- **Reviewer 3 and 4**: These reviewers also pointed out that the paper is unclear in several places, particularly regarding the SCS representation and the experimental setup. The lack of detail in the model architecture and training regimes is a significant issue.\n\n#### 2. **Technical Novelty and Significance**\n- **Reviewer 1 and 2**: The contributions are described as marginally significant or novel. The SCS representation is an interesting idea, but its utility and applicability outside the paper's limited setting are not well-justified.\n- **Reviewer 3 and 4**: These reviewers also noted that the SCS representation's novelty and significance are limited. The paper does not provide a compelling case for why SCS is better than other representations, such as fully continuous representations.\n\n#### 3. **Empirical Novelty and Significance**\n- **Reviewer 1 and 2**: The experiments are not sufficient to support the claimed contributions. The paper lacks ablation studies and comparisons with other representations, such as one-hot encodings. The results are not robust, with only two seeds used for some experiments.\n- **Reviewer 3 and 4**: These reviewers also pointed out the lack of empirical rigor. The results are barely above chance performance, and the paper does not demonstrate the value of the SCS representation on existing benchmarks for compositional generalization.\n\n#### 4. **Correctness**\n- **Reviewer 1 and 2**: Some of the paper's claims have minor issues and require small changes to be correct. The paper lacks sufficient detail and clarity, which affects the correctness of the claims.\n- **Reviewer 3 and 4**: These reviewers noted that the main claims of the paper are incorrect or not well-supported by theory or empirical results. The poor performance of the models and the lack of ablation studies further undermine the correctness of the paper.\n\n#### 5. **Engagement with Existing Literature**\n- **Reviewer 4**: The paper needs better engagement with the existing literature on compositional generalization. The authors should situate their work within the broader context of research on discrete vs. continuous representations and compositionality.\n\n### Final Decision\nGiven the significant issues with clarity, technical novelty, empirical rigor, and engagement with the existing literature, the paper needs substantial revisions to meet the standards of a top-tier conference. The current version of the paper does not provide a compelling case for its contributions and lacks the necessary detail and robustness in its experiments.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Problem Relevance**: The paper addresses a significant problem in diffusion models, which is the high computational cost associated with generating high-quality samples. Reducing the number of inference steps while maintaining or improving sample quality is a crucial challenge in the field.\n\n2. **Novelty and Technical Contribution**:\n   - **Generalized Gaussian Diffusion Models (GGDM)**: The introduction of a non-Markovian family of samplers (GGDM) is a novel contribution. This family of models allows for more flexible sampling processes, which can potentially lead to better sample quality with fewer steps.\n   - **Differentiable Diffusion Sampler Search (DDSS)**: The method of optimizing the parameters of the sampler by differentiating through the sample quality scores is innovative. The use of the reparametrization trick and gradient rematerialization to handle the computational challenges is well-executed.\n\n3. **Empirical Results**:\n   - The paper demonstrates strong empirical results on several datasets, including CIFAR-10 and ImageNet 64x64. The FID scores are significantly improved with fewer inference steps compared to existing methods like DDPM and DDIM.\n   - The use of the Kernel Inception Distance (KID) as the perceptual loss is well-motivated and theoretically grounded, addressing the concerns raised by the reviewers about the lack of theoretical guarantees.\n\n4. **Clarity and Presentation**:\n   - The authors have made significant improvements in the clarity of the paper, addressing the concerns raised by the reviewers. The revised version is more coherent and easier to follow, with better explanations of the technical details and the motivation behind the choices made.\n\n#### Weaknesses of the Paper\n\n1. **Theoretical Grounding**:\n   - While the authors have addressed the concerns about the theoretical grounding of the perceptual loss by connecting it to the KID, there is still a need for more detailed analysis of the statistical properties of the GGDM and the implications of optimizing the KID. This is particularly important to understand the potential limitations and robustness of the method.\n\n2. **Scalability and Practicality**:\n   - The computational cost of the DDSS optimization process is a concern, especially for larger datasets and higher-resolution images. The authors mention that the method is computationally intensive but argue that the gains in sample quality justify the cost. However, more detailed experiments on larger datasets (e.g., LSUN, ImageNet 256x256) would strengthen the practical relevance of the method.\n\n3. **Comparison to Baselines**:\n   - The paper could benefit from a more comprehensive comparison to other recent methods that aim to improve the efficiency of diffusion models. While the results on CIFAR-10 and ImageNet 64x64 are strong, additional experiments on more challenging datasets would provide a more complete picture of the method's performance.\n\n4. **Clarity and Exposition**:\n   - Despite the improvements, the paper could still benefit from further refinement in terms of clarity and exposition. Some of the technical details, such as the double subscripts in the equations and the parametrization of the sampler, could be explained more clearly to make the paper more accessible to a broader audience.\n\n### Final Decision\n\nGiven the significant contributions of the paper, the strong empirical results, and the improvements made in response to the reviewers' feedback, the paper should be accepted for publication. However, the authors should continue to address the remaining concerns, particularly regarding the theoretical grounding and the practicality of the method on larger datasets.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Novelty in Adversarial Training**:\n   - The paper introduces a novel approach to adversarial training by learning to reweight training samples using a bilevel optimization framework. This is a significant contribution as it moves beyond heuristic reweighting methods and leverages meta-learning techniques.\n   - The use of multi-class margins as input to the reweighting network is a novel and intuitive approach that captures more information about the sample's position relative to decision boundaries.\n\n2. **Methodological Soundness**:\n   - The paper is well-written and the methodology is clearly explained. The use of MAML for reweighting is a sound approach, and the authors provide a clear derivation of the bilevel optimization problem.\n   - The authors have performed extensive experiments on benchmark datasets (MNIST, CIFAR-10, CIFAR-100) and compared their method with several baselines, including state-of-the-art techniques like TRADES and AWP.\n\n3. **Empirical Results**:\n   - The empirical results show that the proposed method (BiLAW) improves both clean and robust accuracy compared to related techniques. The results are consistent across different datasets and attack types, including strong adversaries like AutoAttack.\n   - The authors have provided ablation studies to demonstrate the effectiveness of different components of their method, such as the multi-class margin and the parametric reweighting function.\n\n#### Weaknesses:\n1. **Limited Novelty**:\n   - While the paper introduces a novel approach, the core idea of using bilevel optimization for reweighting is not entirely new. The authors acknowledge this by comparing their method to MAML and other meta-learning techniques.\n   - The novelty is primarily in the application of these techniques to adversarial training and the specific formulation of the multi-class margin.\n\n2. **Evaluation Concerns**:\n   - **Adaptive Attacks**: Several reviewers have raised concerns about the evaluation of the method against adaptive attacks, where the attacker has knowledge of the reweighting network. The authors have provided preliminary results showing that even with oracle knowledge of the weighting network, the adaptive attack does not significantly improve the attack's effectiveness. However, a more extensive analysis is needed to fully address this concern.\n   - **Comparison with SOTA**: The paper claims to outperform state-of-the-art baselines, but some reviewers have pointed out that the comparison is not entirely fair. The authors have provided additional results comparing BiLAW-TRADES to TRADES, showing that BiLAW can achieve better robustness with similar clean accuracy. However, the comparison with other recent SOTA methods (e.g., AWP) is still limited.\n\n3. **Theoretical Justification**:\n   - The paper lacks a deeper theoretical justification for why the multi-class margin is a better input to the reweighting network compared to other potential inputs. The authors have provided some ablation studies, but a more rigorous analysis would strengthen the paper.\n\n4. **Computational Cost**:\n   - The computational cost of training the reweighting network is not thoroughly discussed. While the authors mention that the method can be trained within a reasonable number of epochs, a more detailed comparison with other methods in terms of training time and computational resources would be beneficial.\n\n### Final Decision\nDespite the limitations, the paper presents a novel and well-motivated approach to adversarial training that shows promising empirical results. The method is sound, and the authors have addressed many of the reviewers' concerns with additional experiments and clarifications. However, the paper could benefit from a more thorough evaluation against adaptive attacks and a deeper theoretical justification for the multi-class margin.\n\nGiven the strengths and the potential impact of the work, I recommend acceptance with the condition that the authors address the remaining concerns in the final version of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Theoretical Contribution:**\n   - The paper provides a novel analysis of recursive value estimation in the overparameterized linear setting, which is a significant and timely topic in reinforcement learning (RL).\n   - The authors show that classical updates like TD, FVI, and RM converge to different fixed points in the overparameterized regime, which is a non-trivial and important finding.\n   - The unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to different constraints is a valuable contribution.\n\n2. **Practical Implications:**\n   - The paper introduces two regularizers that improve the stability and performance of TD and FVI in deep RL settings, which is a practical and useful contribution.\n   - The empirical results demonstrate the effectiveness of these regularizers, providing evidence of their utility.\n\n3. **Clarity and Presentation:**\n   - The paper is generally well-written and easy to follow, with clear explanations of the theoretical results and their implications.\n   - The authors have addressed many of the reviewers' concerns in their responses, showing a willingness to improve the paper.\n\n#### **Weaknesses of the Paper:**\n1. **Theoretical Rigor:**\n   - Some reviewers have pointed out issues with the mathematical derivations, particularly in the proofs of Theorem 4 and the matrix decomposition in the appendix. These issues need to be addressed to ensure the correctness of the results.\n   - The connection to deep RL is not fully convincing, as the overparameterized linear setting is not a direct analog of deep neural networks used in practice.\n\n2. **Empirical Evaluation:**\n   - The empirical evaluation is limited to relatively simple environments, which may not fully demonstrate the practical impact of the proposed regularizers.\n   - The experiments could be more extensive and include more complex environments to validate the practical utility of the theoretical findings.\n\n3. **Comparative Analysis:**\n   - The paper could benefit from a more detailed comparison with existing work, particularly in the context of overparameterized models and deep RL.\n   - The authors should provide a clearer explanation of the pros and cons of overparameterization compared to underparameterization.\n\n4. **Technical Novelty:**\n   - Some of the results, such as the least norm solution in Theorem 4, are not entirely novel and have been derived in other contexts. The authors should give proper credit to existing work and clearly highlight the novel aspects of their contributions.\n\n### Final Decision\n\nDespite the weaknesses, the paper makes a significant theoretical contribution to the understanding of overparameterized linear value estimation in RL. The practical implications of the proposed regularizers are also valuable. The authors have shown a willingness to address the reviewers' concerns, which is a positive sign.\n\nHowever, the issues with the mathematical derivations and the limited empirical evaluation are significant and need to be addressed before the paper can be considered for publication.\n\n**Final Decision: Reject**\n\nThe paper should be rejected for publication in its current form due to the identified issues with theoretical rigor and empirical evaluation. However, the authors are encouraged to address these issues and resubmit the paper for consideration in a future conference or journal."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Technical Novelty**: The paper introduces the Believer-Penalizer (BP) strategy for asynchronous multi-objective Bayesian optimization (MOBO). While the idea is straightforward, it addresses a practical issue in MOBO by combining Kriging Believer (KB) and Local Penalization (LP) to handle uncertainty in predictions. However, the novelty is somewhat limited, as the method is a combination of existing techniques.\n- **Scientific Contribution**: The paper's primary scientific contribution is the introduction of asynchronous MOBO and the empirical evaluation of the BP strategy. The authors claim that this is the first empirical evaluation of asynchronous MOBO, which is a significant contribution to the field. However, the theoretical underpinnings of BP are not well-developed, and the paper lacks a rigorous theoretical analysis.\n- **Engineering Contribution**: The platform itself is well-engineered and user-friendly, with a modular design that allows for easy integration of new algorithms. The graphical user interface (GUI) is a significant advantage for users with limited coding experience. The platform supports a wide range of data types and optimization scenarios, making it a valuable tool for practical applications.\n\n#### **2. Empirical Evaluation**\n- **Benchmarking**: The empirical results are mixed. The paper claims that BP outperforms other methods, but the results in Figure 4 and Figure 5 show that BP is not consistently the best. The authors have updated the paper to reflect that BP is a robust approach that performs well on average, which is a more accurate representation.\n- **Real-World Application**: The real-world example demonstrates the platform's ability to control and guide hardware experiments, which is a valuable practical application. However, the comparison to random search is not very strong, and the benchmark could be more rigorous.\n\n#### **3. Clarity and Presentation**\n- **Writing and Structure**: The paper is well-written and well-structured, making it easy to follow. The authors have addressed many of the minor issues pointed out by the reviewers, such as typos and redundant specifications.\n- **Documentation and Accessibility**: The authors have provided a video introduction and executable files on their website, which enhances the accessibility of the platform. The GUI and user-friendly design are significant advantages for general users.\n\n#### **4. Addressing Reviewer Concerns**\n- **Reviewer 1**: The reviewer is still not convinced by the overall performance of AutoOED and the novelty of the BP strategy. The authors have addressed these concerns by updating the paper to reflect that BP is a robust approach and by providing more extensive comparisons.\n- **Reviewer 2**: The reviewer is concerned about the limited methodological contribution and the misleading claim about the number of objectives. The authors have addressed these concerns by implementing single-objective and many-objective optimization and clarifying the scope of the platform.\n- **Reviewer 3**: The reviewer is concerned about the lack of theoretical proof for BP and the limited novelty of the method. The authors have provided more intuitive explanations and empirical evidence to support the robustness of BP.\n- **Reviewer 4**: The reviewer is concerned about the empirical results and the lack of comparison to other platforms. The authors have updated the paper with more extensive comparisons and clarified the performance claims.\n- **Reviewer 5**: The reviewer is generally positive about the paper but points out minor issues. The authors have addressed these issues and provided more updated materials.\n\n### Final Decision\nWhile the paper has some limitations in terms of theoretical novelty and empirical robustness, it makes significant contributions in terms of engineering and practical application. The platform is well-designed, user-friendly, and has the potential to bridge the gap between research and practical application in multi-objective optimization. The introduction of asynchronous MOBO and the empirical evaluation of the BP strategy are valuable contributions to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in the field of continual learning and motor control, which is highly relevant for both machine learning and neuroscience. The ability to learn and chain motor motifs without interference is a crucial aspect of both biological and artificial systems.\n\n2. **Biological Inspiration:**\n   - The authors draw inspiration from the thalamocortical circuit, which is a well-studied biological system. This biological grounding adds a layer of realism and potential applicability to the proposed RNN architecture.\n\n3. **Technical Contributions:**\n   - The paper introduces a novel preparatory module that enhances the robustness and accuracy of RNNs in generating and chaining motor motifs. The module is designed to address the specific challenges of zero-shot transfer and out-of-distribution generalization.\n\n4. **Empirical Validation:**\n   - The authors provide empirical evidence to support their claims, including comparisons with baseline architectures and demonstrations of the preparatory module's effectiveness in various scenarios.\n\n#### **Weaknesses of the Paper:**\n1. **Technical Novelty and Significance:**\n   - While the preparatory module is a novel contribution, the reviewers raise concerns about its novelty and significance. The module's effectiveness is not convincingly demonstrated compared to simpler alternatives, such as hard resets or training with end states.\n\n2. **Experimental Design:**\n   - The experimental setup is criticized for being too small-scale and synthetic. The networks are relatively small (300 units), and the number of motifs (10) is limited. This raises questions about the generalizability and relevance of the results to more complex and realistic scenarios.\n\n3. **Alternative Hypotheses:**\n   - The authors do not adequately test or rule out alternative hypotheses, such as training the RNNs to generalize from end states or using a consistent reset state. This lack of thoroughness diminishes the strength of their claims.\n\n4. **Presentation and Clarity:**\n   - The paper is criticized for its writing and presentation. The description of the problem and the experimental setup is unclear, and the paper lacks a clear formalization of the task and optimization problem. This makes it difficult for readers to understand and replicate the experiments.\n\n5. **Trivial Solutions:**\n   - The reviewers point out that the task described in the paper admits a trivial solution, such as a hard reset to a fixed state. The authors need to demonstrate that their approach is superior to these trivial solutions, which they have not convincingly done.\n\n#### **Author Response:**\n- The authors have made efforts to address the reviewers' concerns, including providing more evidence and clarifications. However, the responses do not fully address the fundamental issues raised, particularly regarding the scale of the experiments and the testing of alternative hypotheses.\n\n### Final Decision\nGiven the significant weaknesses in the paper, particularly the lack of thorough experimental validation, the small scale of the experiments, and the inadequate testing of alternative hypotheses, the paper does not meet the standards of a top-tier conference. While the problem addressed is relevant and the biological inspiration is interesting, the technical and empirical contributions are not sufficiently novel or robust to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Theoretical Contribution:**\n   - The paper provides a clear and well-motivated theoretical framework for understanding epoch-wise double descent in a linear teacher-student model. The use of the replica method to derive closed-form expressions for the generalization error is a significant technical contribution.\n   - The paper successfully captures the phenomenon of epoch-wise double descent, which is a less studied aspect of double descent, and provides a clear mechanism for its occurrence.\n\n2. **Empirical Validation:**\n   - The authors validate their theoretical findings with numerical experiments, showing a good match between the analytical predictions and empirical results. This adds credibility to their theoretical claims.\n   - The connection to real-world deep neural networks (ResNet18 on CIFAR-10) is demonstrated, suggesting that the insights from the simple model may generalize to more complex settings.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and the results are neatly presented. The authors have made significant efforts to address the reviewers' comments, improving the clarity and precision of the text.\n\n#### **Weaknesses:**\n1. **Technical Novelty:**\n   - While the paper builds on existing work, some reviewers have raised concerns about the novelty of the technical contributions. The use of the replica method and the connection to ridge regularization are well-established in the literature.\n   - The authors have addressed these concerns by providing a detailed comparison with related work and highlighting the specific advancements in their approach. However, the incremental nature of the contributions may be a point of contention.\n\n2. **Empirical Depth:**\n   - The empirical validation is primarily based on a linear model and a single deep network (ResNet18 on CIFAR-10). While the results are promising, a more extensive empirical evaluation with a variety of deep networks and datasets would strengthen the paper.\n\n3. **Typos and Notational Issues:**\n   - The paper contains several typos and notational inconsistencies that can distract the reader and make the calculations harder to follow. The authors have addressed many of these issues in the revised version, but a thorough proofreading is still recommended.\n\n4. **Connection to Real-World Applications:**\n   - The authors have made an effort to connect their findings to real-world deep networks using the Neural Tangent Kernel (NTK) approximation. However, the connection could be further strengthened with more detailed experiments and analysis.\n\n### Final Decision\n\nDespite the concerns raised by the reviewers, the paper makes a valuable contribution to the understanding of epoch-wise double descent in a well-motivated and theoretically sound manner. The authors have addressed the major issues and provided additional experiments and discussions to strengthen their claims. The paper is well-written and the results are significant enough to warrant publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Well-Written and Clear:**\n   - The paper is well-written and easy to follow, with clear technical details and a logical structure.\n   - The authors provide sufficient information for reproducibility, including data, code, and trained models.\n\n2. **Innovative Methodology:**\n   - The introduction of MILAN (Mutual Information-Guided Linguistic Annotation of Neurons) is a novel and well-motivated approach to interpreting deep learning models.\n   - The method is designed to generate natural language descriptions of neurons, which can capture a wide range of features, from low-level to high-level.\n\n3. **Comprehensive Evaluation:**\n   - The paper includes a thorough evaluation of MILAN across different model architectures, datasets, and tasks.\n   - The results show that MILAN outperforms baseline methods in generating descriptions that align with human annotations.\n\n4. **Practical Applications:**\n   - The paper demonstrates three practical applications of MILAN: analysis, auditing, and editing.\n   - These applications highlight the potential of MILAN to improve model understanding and robustness.\n\n5. **Dataset Contribution:**\n   - The creation of the MILANNOTATIONS dataset is a significant contribution, providing a valuable resource for future research.\n\n#### **Weaknesses:**\n1. **Inter-Annotator Agreement:**\n   - The inter-annotator agreement in the MILANNOTATIONS dataset is relatively low, which could affect the quality of the training data.\n   - The authors should provide more details on how they handle this inconsistency during model training.\n\n2. **Generalizability:**\n   - While the paper shows that MILAN can generalize to different vision models, it is limited to image datasets (ImageNet and Places365).\n   - The authors acknowledge that MILAN may not generalize well to non-vision tasks or more diverse vision datasets without additional data collection.\n\n3. **Baseline Comparison:**\n   - The paper could benefit from a more comprehensive comparison with off-the-shelf image captioning models, which could provide a stronger baseline.\n   - The authors have started to address this in their response, but more detailed results would strengthen the paper.\n\n4. **Failure Modes:**\n   - The paper mentions failure modes but does not delve deeply into them. More discussion and analysis of these failure cases would provide valuable insights for future improvements.\n\n5. **Ethical Considerations:**\n   - The paper touches on ethical considerations but could benefit from a more detailed discussion of potential biases in the dataset and the implications of using MILAN for auditing and editing models.\n\n#### **Reviewer Feedback:**\n- **Reviewer 1 and 2:**\n  - Both reviewers found the paper well-written and technically sound.\n  - They provided constructive feedback on the inter-annotator agreement, potential improvements, and the need for more details on the dataset and failure modes.\n\n- **Reviewer 3:**\n  - The reviewer appreciated the thorough analysis and practical applications but suggested further characterization of the limits of the approach.\n  - They also pointed out the need for more detailed descriptions of the generated outputs and the dataset.\n\n- **Reviewer 4:**\n  - The reviewer, while not a domain expert in computer vision, found the paper strong and well-motivated.\n  - They raised important questions about the generalizability of the method to other tasks and datasets, the performance of off-the-shelf captioning models, and the potential for more impactful applications.\n\n#### **Author Response:**\n- The authors have addressed many of the reviewer comments effectively, including changing the title to better reflect the scope, adding more details on failure modes, and providing additional experiments and dataset statistics.\n- They have also committed to further experiments and improvements, which will strengthen the paper.\n\n### Final Decision\nGiven the strengths of the paper, the thorough evaluation, and the practical applications, combined with the authors' responsiveness to reviewer feedback, I believe this paper should be accepted for publication. The weaknesses identified can be addressed in future work or in the final version of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Theoretical Contribution**: The paper provides a novel theoretical framework for neural network approximation using tropical geometry. The main theorem (now a proposition) and the new theorem that bounds the approximation error based on the Hausdorff distance of zonotopes are significant contributions. This work is a first step towards a purely geometrical interpretation of neural network approximation, which is a valuable and novel direction in the field.\n\n2. **Algorithmic Contribution**: The paper proposes two algorithms (Zonotope K-means and Neural Path K-means) for compressing fully connected layers of ReLU-activated neural networks. These algorithms are theoretically grounded and have been empirically evaluated.\n\n3. **Clarity and Accessibility**: The paper is well-written and accessible to non-domain experts. The background on tropical geometry is well-explained, and the illustrations are helpful in understanding the methods.\n\n4. **Empirical Evaluation**: The empirical results, while limited to smaller datasets and architectures, provide a proof-of-concept that the proposed methods work as expected. The comparison with relevant baselines (Smyrnis & Maragos, ThiNet) is informative and shows that the methods can achieve competitive performance.\n\n#### Weaknesses\n1. **Limited Empirical Scope**: The experiments are primarily conducted on smaller datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and older architectures (LeNet, VGG). While the authors justify this as a proof-of-concept, it would be more convincing to see the methods applied to modern architectures like ResNets or Vision Transformers. This limitation reduces the practical impact of the work.\n\n2. **Comparison with State-of-the-Art**: The paper compares the proposed methods with older techniques (ThiNet, L1 structured pruning) and a related tropical method (Smyrnis & Maragos). However, it does not compare with more recent and advanced pruning techniques, which are necessary to demonstrate the practical relevance of the proposed methods.\n\n3. **Complexity Analysis**: The paper lacks a detailed analysis of the computational and memory complexity of the proposed algorithms. While the authors provide some information in the author response, this should be clearly stated in the paper.\n\n4. **Overclaiming**: The abstract and some parts of the paper overclaim the performance of the proposed methods. The authors have addressed this in the revised version, but it is important to ensure that the claims are well-supported by the empirical results.\n\n5. **Reproducibility**: The code is provided as supplementary material, but more detailed information on the experimental setup and pseudo-code for the zonotope generators would enhance reproducibility.\n\n#### Addressing Reviewer Comments\n- **Reviewer 1**: The reviewer's concerns about the experimental scope and overclaiming have been addressed in the revised version. The paper now clearly states the limitations and focuses on the theoretical contribution.\n- **Reviewer 2**: The reviewer's concerns about the relevance to ICLR and the limited empirical scope are valid. The authors have provided a summary and justification for the scope of their work.\n- **Reviewer 3**: The reviewer's questions about the bounds and the comparison with Alfarra et al. have been addressed. The authors have also clarified the experimental setup and provided more details on the complexity.\n- **Reviewer 4**: The reviewer's suggestions for improving the clarity and reproducibility have been addressed. The authors have provided more details on the experimental setup and the code.\n\n### Final Decision\nDespite the limitations in the empirical scope and the need for more detailed complexity analysis, the paper makes a significant theoretical contribution to the field of neural network approximation using tropical geometry. The novel algorithms and the theoretical bounds are valuable and provide a new perspective on neural network compression. The authors have addressed the major concerns raised by the reviewers and have made the paper more accessible and clear.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Novelty and Motivation**:\n   - The paper introduces a novel framework, PEARL, for synthesizing data using deep generative models with differential privacy. The approach is well-motivated and addresses a significant problem in the field of data synthesis.\n   - The use of characteristic functions and adversarial re-weighting is a unique and interesting approach that adds to the novelty of the paper.\n\n2. **Theoretical Guarantees**:\n   - The paper provides theoretical guarantees for the performance of the proposed framework, which is a strong point. The mathematical formulation is clear and easy to follow.\n\n3. **Empirical Evaluation**:\n   - The empirical results show that PEARL outperforms other methods at reasonable levels of privacy. The authors have added more datasets and baselines in the rebuttal, which strengthens the empirical evaluation.\n\n4. **Writing and Clarity**:\n   - The paper is well-written and easy to understand. The authors have addressed many of the reviewers' comments and made the necessary clarifications in the rebuttal.\n\n#### Weaknesses of the Paper\n\n1. **Benchmarking**:\n   - While the authors have added more datasets and baselines in the rebuttal, the benchmarking is still somewhat limited. For tabular data, it would be beneficial to compare PEARL with more advanced baselines like PATE-GAN and CTGAN. For image data, evaluating on more complex datasets like CIFAR-10 would provide a more comprehensive comparison.\n\n2. **Privacy Budget and Sensitivity Calculation**:\n   - The sensitivity calculation and the auxiliary information release are not entirely clear. The authors have provided some clarifications in the rebuttal, but further explanation and detailed derivations would be helpful.\n\n3. **Complexity Analysis**:\n   - The computational complexity analysis is provided in the rebuttal, but it would be better to include this in the main paper to provide a complete picture of the method's efficiency.\n\n4. **Additional Evaluation Tasks**:\n   - The authors have added more evaluation tasks like marginal release, range query, and MMD, which is a positive step. However, more tasks and a broader range of evaluations would further strengthen the paper.\n\n#### Reviewer Feedback and Author Responses\n\n- **Reviewer 1**:\n  - The reviewer is generally positive about the paper but suggests more thorough benchmarking and moving Algorithm 1 to the main paper. The authors have addressed these points in the rebuttal by adding more datasets and baselines and providing a high-level description of the algorithm.\n\n- **Reviewer 2**:\n  - The reviewer has several specific questions about the auxiliary information release, sensitivity calculation, and performance saturation. The authors have provided detailed responses and clarifications, which address most of the concerns.\n\n- **Reviewer 3**:\n  - The reviewer points out the need for more related work, justifications for novelty claims, and additional benchmark datasets and baselines. The authors have added the missing references, more datasets, and baselines, and provided a computational complexity analysis.\n\n### Final Decision\n\nWhile the paper has some weaknesses, the authors have made significant efforts to address the reviewers' comments and improve the paper. The novel approach, theoretical guarantees, and empirical results are strong points that justify acceptance. However, the paper would benefit from further improvements in benchmarking and clarity.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Theoretical Contribution:**\n   - The paper provides a novel theoretical analysis of offline monotonic policy improvement, which is a significant contribution to the field of offline reinforcement learning.\n   - The authors derive a lower bound on the true performance that depends on additional terms related to the offline dataset, which is a non-trivial result.\n\n2. **Empirical Contribution:**\n   - The proposed method, Behavior Proximal Policy Optimization (BPPO), is simple to implement and achieves competitive results on the D4RL benchmark.\n   - The method outperforms state-of-the-art algorithms on more challenging tasks such as Adroit, Antmaze, and Kitchen, which are important for demonstrating the practical utility of the method.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and the presentation of the proposed algorithm is clear. The authors have addressed many of the reviewers' concerns in their rebuttal, particularly regarding the theoretical assumptions and experimental setup.\n\n#### **Weaknesses:**\n1. **Theoretical Soundness:**\n   - The initial assumption 1 was considered too strong by multiple reviewers. The authors have addressed this by replacing it with a more reasonable assumption using total variational divergence, which is a significant improvement.\n   - The replacement of \\(A_{\\pi_k}\\) with \\(A_{\\pi_\\beta}\\) is a practical choice but loosens the theoretical bounds. The authors have justified this trade-off between theoretical soundness and practical performance, but it remains a point of concern.\n\n2. **Experimental Robustness:**\n   - The performance on the Gym locomotion tasks is only slightly better or worse compared to baselines, which is not a strong selling point. However, the significant improvements on more challenging tasks (Adroit, Antmaze, Kitchen) are compelling.\n   - The authors have added necessary ablation studies and experiments on sparse reward settings (Antmaze), which strengthens the empirical evaluation.\n\n3. **Comparison with Baselines:**\n   - The comparison with other state-of-the-art methods (CQL, TD3+BC) is thorough, and the authors have provided additional results in the rebuttal to address the reviewers' concerns.\n   - The authors have also clarified the simplicity of implementation, which is a practical advantage of BPPO.\n\n4. **Novelty:**\n   - While the theoretical results are novel, there is some overlap with prior work, particularly in the context of off-policy monotonic policy improvement. The authors have addressed this by highlighting the specific contributions and differences from existing methods.\n\n### Final Decision\nGiven the significant theoretical and empirical contributions, the clarity of the presentation, and the authors' responsiveness to the reviewers' concerns, the paper should be accepted for publication. The improvements in the latest submission, particularly the modification of assumption 1 and the additional experimental results, have addressed the major weaknesses identified by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Novel Idea**: The paper introduces a novel framework, MCTransformer, which combines Monte-Carlo Tree Search (MCTS) with Transformers. This combination is a promising direction in the field of reinforcement learning, especially for offline settings.\n2. **Potential Impact**: The idea of using Transformers to enhance MCTS has the potential to improve the exploration and evaluation phases, which are critical in reinforcement learning tasks.\n3. **Initial Results**: The initial results on the SameGame domain show that MCTransformer outperforms both Transformer-only and MCTS-only solutions, which is a positive indication of the framework's effectiveness.\n\n#### Weaknesses:\n1. **Limited Novelty**: The method is a combination of existing techniques (MCTS and Transformers) and does not introduce significant new components or algorithms. This is a common criticism from all reviewers.\n2. **Limited Evaluation**: The evaluation is conducted on a single, non-standard domain (SameGame), which is relatively toy-like. The lack of comparisons to well-established benchmarks (e.g., MiniGo, Atari) and other relevant methods (e.g., DecisionTransformers, Online DTs, AlphaZero) makes it difficult to assess the generalizability and robustness of the proposed method.\n3. **Unfair Comparisons**: The comparisons with MCTS-based solutions are not fair, as the MCTS baseline does not use a learned prior policy or value function, which is a standard practice in MCTS-based methods.\n4. **Clarity and Presentation**: The paper lacks clarity in exposition, with several typos, grammatical errors, and unclear explanations. The figures and tables need better captions and explanations.\n5. **Reproducibility**: The paper does not provide the source code, which is crucial for reproducibility. While the method is similar to AlphaZero, the lack of code makes it harder for other researchers to reproduce the results.\n\n#### Reviewer Feedback:\n- **Reviewer 1**: Highlights the lack of clarity, limited evaluation, and unfair comparisons. Suggests improvements in the writing and additional experiments.\n- **Reviewer 2**: Criticizes the limited novelty and unfair comparisons. Points out the need for more comprehensive experiments and comparisons.\n- **Reviewer 3**: Emphasizes the need for more comparisons with other methods and standardized test sets. Also points out the lack of clarity and presentation issues.\n\n### Final Decision\nGiven the significant weaknesses in the paper, particularly the limited novelty, unfair comparisons, and lack of comprehensive evaluation, the paper is not ready for acceptance in its current form. However, the idea itself is promising and could be a strong contribution with further improvements.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Clarity and Reproducibility:**\n   - The paper is well-written and clearly explains the concepts and experimental setup.\n   - The authors have addressed most of the reviewers' comments and have made significant improvements, including adding new experiments and clarifications.\n   - The authors have committed to making the code publicly available, which is crucial for reproducibility.\n\n2. **Novelty and Contribution:**\n   - The paper introduces a novel approach by combining variational autoencoders with neural cellular automata (NCA), which is a significant contribution to the field.\n   - The VNCA is the first proper generative model using NCAs, and the authors have evaluated it fairly against state-of-the-art methods.\n   - The paper highlights the unique properties of NCAs, such as local interaction and self-organization, which are valuable for understanding and developing generative models.\n\n3. **Experimental Results:**\n   - The authors have conducted experiments on multiple datasets (MNIST, Noto Emoji, and CelebA) and have shown that the VNCA can learn to reconstruct images well.\n   - The paper includes new experiments on damage recovery and latent space analysis, which provide additional insights into the capabilities of the VNCA.\n   - The authors have been transparent about the limitations of their model, which is a strength in itself.\n\n#### **Weaknesses:**\n1. **Underwhelming Results:**\n   - The experimental results, particularly on CelebA, are not as strong as those of state-of-the-art generative models.\n   - The sample quality and likelihoods are lower compared to other methods, which is a significant drawback.\n   - The need to average samples to produce reasonable results is a limitation.\n\n2. **Technical Novelty:**\n   - Some reviewers argue that the main insight of the paper (interpreting deep convolutional networks as NCAs) is not novel and has been previously discussed in the literature.\n   - The VNCA architecture can be seen as a deep convolutional network with weight sharing, which raises questions about the true novelty of the approach.\n\n3. **Motivation and Practical Relevance:**\n   - The motivation for using NCAs in generative modeling is not fully clear. The authors need to better articulate the unique advantages of NCAs over other generative models.\n   - The practical relevance of the VNCA, especially in terms of real-world applications, is not well-established.\n\n#### **Reviewer Feedback:**\n- **Positive Feedback:**\n  - Reviewers 3, 5, and 7 have raised their scores and are in favor of accepting the paper, citing the paper's clarity, novel contributions, and interesting experimental results.\n  - Reviewer 5, in particular, has raised their score from \"6: marginally above the acceptance threshold\" to \"8: accept, good paper\" after the authors addressed their comments.\n\n- **Negative Feedback:**\n  - Reviewers 1, 2, 4, and 6 are more critical of the paper, citing underwhelming experimental results, lack of technical novelty, and unclear motivation.\n  - Reviewer 6, in particular, argues that the main insight of the paper is not novel and that the contributions are marginal.\n\n### Final Decision\n\nGiven the mixed feedback from the reviewers, the strengths and weaknesses of the paper, and the significant improvements made by the authors in response to the initial reviews, I believe the paper should be accepted for publication. The novel combination of variational autoencoders and neural cellular automata, along with the transparent and thorough evaluation, makes a valuable contribution to the field. While the experimental results are not state-of-the-art, the paper provides a solid foundation for future research and is well-written and reproducible.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Clarity and Structure:**\n   - The paper is well-written and structured, making it easy to follow. The motivation and problem statement are clear.\n   - The figures, especially Figure 1, provide a good summary of the work.\n\n2. **Novelty:**\n   - The paper introduces a novel approach to online knowledge distillation by using a label prior shift to induce diversity among the teachers. This is a unique contribution, as it is the first work to the best of the authors' knowledge to use this technique in this context.\n   - The use of post-compensated Softmax (PC-Softmax) to correct the likelihoods of the specialized teachers is another novel aspect.\n\n3. **Empirical Analysis:**\n   - The empirical analysis is thorough, with extensive experiments on multiple datasets (CIFAR-10, CIFAR-100, and ImageNet).\n   - The ablation studies are well-designed and provide insights into the effectiveness of each component of the proposed method.\n\n4. **Comparisons:**\n   - The authors have conducted additional experiments to compare their method with recent state-of-the-art methods (PCL, L-MCL, and CGL). The results show that their method outperforms these baselines in terms of accuracy, expected calibration error (ECE), and negative log-likelihood (NLL).\n\n#### **Weaknesses of the Paper:**\n1. **Specialized Setup:**\n   - The setup is specialized to online distillation and peer networks. While the authors argue that the method can be applied to offline distillation and non-peer networks, this is not thoroughly demonstrated in the paper.\n   - The authors should provide more evidence or experiments to show the applicability of their method in these other setups.\n\n2. **Baseline Selection:**\n   - The choice of baselines is not well-explained. The authors should provide a more detailed justification for the selection of the baselines and why they are appropriate for comparison.\n   - The authors should also compare their method with more recent works, such as CGL and L-MCL, to strengthen their claims.\n\n3. **Technical Novelty:**\n   - While the combination of label prior shift and importance sampling is novel in the context of online knowledge distillation, the individual components (label prior shift, importance sampling, and PC-Softmax) are not new. The authors should clearly explain how their combination is novel and why it is effective.\n\n4. **Typos and Minor Issues:**\n   - The paper contains several typos and minor issues that need to be addressed. These can be distracting and should be corrected before publication.\n\n5. **Weighting of Teachers:**\n   - The authors average the predictions of the teachers without considering the labels they were trained on. This is a naive approach, and the authors should explore weighted averaging based on the labels to potentially improve performance.\n\n#### **Author Response:**\n- The authors have addressed many of the reviewers' concerns and conducted additional experiments to compare their method with recent state-of-the-art methods.\n- They have also clarified the writing and corrected the typos.\n- However, they have not fully addressed the issue of the specialized setup and the weighting of teachers.\n\n### Final Decision\nDespite the weaknesses, the paper presents a novel and well-motivated approach to online knowledge distillation. The empirical results are strong, and the authors have addressed many of the reviewers' concerns. However, the paper could benefit from further experiments to demonstrate the applicability of the method in other setups and a more detailed explanation of the baseline selection.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Clarity and Quality**\n- **Clarity**: The paper is generally well-written and easy to follow, as noted by all reviewers. The idea is intuitive and the motivation is clear.\n- **Quality**: The quality of the paper is somewhat compromised by the issues raised by the reviewers, particularly regarding the novelty and the experimental setup.\n\n#### **Novelty**\n- **Technical Novelty**: The paper proposes a contrastive token learning objective, which is a new approach. However, the novelty is thin, as it builds heavily on unlikelihood training (UL). Reviewer 1 points out that the authors misrepresent UL by not fully acknowledging its consideration of both positive and negative tokens. This is a significant issue that undermines the novelty claim.\n- **Empirical Novelty**: The experiments are limited in scope. The use of GPT2-small and the wikitext-103 dataset is not convincing, especially given the current standards in language modeling. Reviewer 2 and Reviewer 4 both highlight the need for experiments with larger models and more diverse datasets. The metrics used (perplexity, dist-1) are also not comprehensive enough to fully evaluate the effectiveness of the proposed method.\n\n#### **Experimental Setup and Results**\n- **Experimental Setup**: The experimental setup is weak. The use of GPT2-small and the wikitext-103 dataset is not representative of the current state of the art. Reviewer 2 and Reviewer 4 both suggest that experiments with larger models and more diverse datasets are necessary to validate the claims.\n- **Results**: The results show some improvement in reducing repetition and improving diversity, but the metrics used are not robust. Perplexity is not a reliable metric for evaluating text generation quality, and the dist-1 metric is not standard. The human evaluation is also not statistically significant, which further weakens the claims.\n\n#### **Reproducibility**\n- **Reproducibility**: The authors have provided code and a Google Colab notebook, which is a positive aspect. However, the limited experimental setup and the issues with the metrics make it difficult to fully reproduce and validate the results.\n\n#### **Overall Evaluation**\n- **Strengths**:\n  - The paper is well-written and easy to follow.\n  - The idea is intuitive and the motivation is clear.\n  - The proposed method is simple to implement.\n- **Weaknesses**:\n  - The novelty is thin and the paper misrepresents the contributions of unlikelihood training.\n  - The experimental setup is limited and not representative of the current state of the art.\n  - The metrics used are not robust and the human evaluation is not statistically significant.\n\n### Final Decision\nGiven the thin novelty, the limited and unconvincing experimental setup, and the issues with the metrics used, the paper does not meet the standards of a top-tier conference. Therefore, I recommend rejecting the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novel Idea:** The paper introduces an interesting approach to OOD detection by leveraging graph structures and topological properties. This is a novel direction that could potentially lead to more interpretable and robust OOD detection methods.\n2. **High Performance:** The reported AUROC scores (97.95% for far-OOD and 98.79% for near-OOD) are competitive with state-of-the-art techniques, which is a strong point.\n3. **Human-Interpretable Concepts:** The focus on human-interpretable concepts and the potential for human-in-the-loop machine learning is a valuable contribution, especially in safety-critical applications.\n\n#### **Weaknesses:**\n1. **Clarity and Writing:** The paper suffers from poor writing and clarity issues. Reviewers have pointed out numerous grammatical mistakes and unclear descriptions, which make it difficult to follow the methodology and results.\n2. **Lack of Rigorous Validation:** The paper lacks a comprehensive comparison with state-of-the-art OOD detection methods. The authors only compare their method with graph embedding algorithms, which is not sufficient to establish the superiority of their approach.\n3. **Assumptions and Robustness:** The method relies heavily on the accuracy of the pre-trained object detection network. The paper does not provide an analysis of how the performance of the object detector impacts the OOD detection results, which is a critical detail.\n4. **Limited Experiments:** The experiments are conducted only on the LSUN dataset, which is not extensive enough to validate the generalizability of the method. Additional experiments on other datasets (e.g., CIFAR-10/100, ImageNet) are necessary.\n5. **Ablation Studies:** The paper lacks ablation studies to understand the impact of different components of the method, such as the choice of graph embedding algorithms and hyperparameters.\n6. **Motivation and Intuition:** The motivation and intuition behind the method are not well-supported or rigorously demonstrated. The paper uses terms like \"common sense\" and \"child cognition\" without providing strong references or empirical evidence.\n\n#### **Technical and Empirical Novelty:**\n- **Technical Novelty:** The idea of using graph structures and topological properties for OOD detection is novel, but the method itself is a combination of existing techniques (pre-trained object detection, graph embedding, and classification). The novelty is marginal.\n- **Empirical Novelty:** The empirical results are promising but limited. The lack of comparison with state-of-the-art methods and the limited dataset scope reduce the significance of the empirical contributions.\n\n#### **Reproducibility:**\n- The paper provides an open dataset for experimental results, but the algorithm description is not detailed enough to ensure reproducibility. More detailed descriptions and examples are needed.\n\n### Final Decision\nGiven the significant weaknesses in clarity, rigorous validation, and experimental scope, the paper does not meet the standards of a top-tier conference. While the idea is interesting and the initial results are promising, the paper requires substantial revisions to address the reviewers' concerns and strengthen its contributions.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Convergence Analysis for Overparameterized Models:**\n   - The paper provides a sharp convergence analysis for gradient descent (GD) on overparameterized deep linear neural networks. This is a significant contribution as it addresses a fundamental question in deep learning: why overparameterized models can be efficiently optimized.\n   \n2. **Generalization to Different Loss Functions:**\n   - The paper extends the analysis to general strongly convex and smooth loss functions, which is a step beyond the typical squared loss considered in previous works. This generalization is valuable as it broadens the applicability of the results.\n\n3. **Insights on Trajectories:**\n   - Theorem 3.3, which shows that the trajectories of GD on deep linear networks stay close to the trajectories of the corresponding convex problem, is a novel and insightful result. This observation provides a deeper understanding of the optimization dynamics in deep linear networks.\n\n4. **Clarity and Readability:**\n   - The paper is well-written and easy to follow. The notations are clear, and the results are presented in a logical and coherent manner.\n\n#### **Weaknesses:**\n1. **Novelty Concerns:**\n   - Several reviewers have raised concerns about the novelty of the paper. While the extension to general strongly convex and smooth loss functions is a valuable contribution, the techniques used are similar to those in previous works (e.g., Du and Hu, 2019). The authors have addressed this by highlighting the technical challenges and extensions, but the novelty is still somewhat limited.\n\n2. **Overparametrization Focus:**\n   - The paper focuses on overparameterized models, and the results do not apply to models with small widths. The title and abstract should be revised to avoid overclaiming, as suggested by Reviewer 6.\n\n3. **Insufficient Discussion:**\n   - The paper lacks a detailed discussion on why overparametrization is essential for achieving good convergence rates. The authors have added a new section 4 to address this, but more in-depth analysis and intuitive explanations would strengthen the paper.\n\n4. **Local Convergence Analysis:**\n   - The convergence analysis is local, requiring the initial objective function to be close to the minimum value. This is a limitation that should be clearly stated and discussed in the paper.\n\n5. **Minor Issues:**\n   - There are a few minor issues with the paper, such as typos and unclear statements, which the authors have addressed in the revised version.\n\n### Final Decision\n\nDespite the concerns raised by the reviewers, the paper provides valuable insights into the convergence of gradient descent on overparameterized deep linear neural networks. The novel results on trajectory closeness and the generalization to different loss functions are significant contributions. The authors have also made efforts to address the reviewers' concerns by adding a new section and clarifying the technical challenges.\n\nHowever, the paper could benefit from further refinement, particularly in terms of providing more intuitive explanations and addressing the limitations of the local convergence analysis.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Theoretical Contributions:**\n- **Theorem 1 and Comparison to TRADES:**\n  - Theorem 1 is an extension of the results from TRADES to the multi-classification problem. While the extension is relatively straightforward, it does provide a new bound for multi-class problems, which is a novel contribution.\n  - However, the bound is loose, and the practical implications of this bound are not clearly demonstrated. The authors have acknowledged this and revised the paper to focus on multi-class problems, but the theoretical contribution is still somewhat limited.\n  - The discrepancy between the theoretical definition of \\( z(x) \\) and its practical implementation using KL divergence is a valid concern. The authors have provided a justification for this discrepancy, but it remains a point of criticism.\n\n#### **2. Empirical Contributions:**\n- **Performance on Standard Datasets:**\n  - The proposed method, ARoW, shows consistent improvements over existing methods on standard datasets like CIFAR-10, F-MNIST, and SVHN. The improvements are marginal but consistent, which is a positive aspect.\n  - The authors have provided ablation studies and additional experiments to support their claims, which adds to the empirical robustness of the paper.\n- **Performance with Extra Data:**\n  - The results with extra data are less convincing. The improvements over HAT are marginal, and the authors have acknowledged this. However, they have provided additional results using gradient accumulation, which show slight improvements.\n  - The authors have also highlighted other advantages of ARoW, such as ease of implementation and improved fairness, which are important considerations.\n\n#### **3. Clarity and Reproducibility:**\n- **Clarity:**\n  - The paper is well-written and clearly presents the method and the results. The authors have addressed the concerns raised by the reviewers and provided additional details in the rebuttal.\n- **Reproducibility:**\n  - The authors have provided code and implementation details, which should make the results reproducible. The additional experiments and clarifications in the rebuttal further support this.\n\n#### **4. Novelty and Significance:**\n- **Technical Novelty:**\n  - The modification to the TRADES loss is small but relevant. The theoretical justification for the proposed method is sound, and the empirical results support the claims.\n- **Empirical Significance:**\n  - The improvements over existing methods are marginal but consistent. The additional experiments and ablation studies provide a comprehensive evaluation of the method.\n\n### Final Decision\nGiven the detailed analysis, the paper has several strengths, including a well-motivated and theoretically justified method, consistent empirical improvements, and clear presentation. However, the theoretical contribution is somewhat limited, and the improvements over existing methods are marginal, especially with extra data.\n\nDespite these limitations, the paper provides valuable insights and contributions to the field of adversarial robustness. The authors have addressed the concerns raised by the reviewers and provided additional experiments to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Interesting Observation**: The paper presents a novel and interesting observation that swapping the order of batch normalization (BN) and bounded activation functions (like Tanh) can significantly improve performance. This is a valuable contribution to the understanding of neural network architecture design.\n2. **Clear Hypothesis and Analysis**: The authors provide a clear hypothesis and conduct experiments to support their claims. They introduce metrics to measure saturation, skewness, and sparsity, which are well-defined and useful for their analysis.\n3. **Empirical Validation**: The paper includes extensive empirical validation across multiple datasets and architectures, which strengthens the credibility of the findings.\n\n#### Weaknesses:\n1. **Clarity and Rigor**: Several claims and explanations are not clearly defined or rigorously supported. For example, the concept of \"center of the function\" and the relationship between saturation and sparsity need more precise definitions and justifications.\n2. **Generalizability**: The analysis is limited to VGG-like architectures and does not extend to more modern architectures like ResNets. This limits the practical applicability of the findings.\n3. **Contradictory Observations**: Some observations, such as the relationship between saturation and sparsity, are contradictory and require further clarification. The authors need to address these inconsistencies to strengthen their arguments.\n4. **Comparison with ReLU**: The paper does not convincingly demonstrate that the Swap model with bounded activation functions outperforms the Convention model with ReLU, which is a widely used activation function. This limits the practical significance of the findings.\n5. **Experimental Setup**: The experimental setup and analysis have some gaps. For instance, the authors need to provide more detailed information on the training dynamics and convergence to rule out the possibility that the performance improvement is due to accelerated training rather than the proposed mechanism.\n\n#### Reviewer Feedback:\n- **Reviewer 1**: Suggests additional figures and a more detailed analysis of the skewness in different layers. This feedback is valuable and should be addressed to strengthen the paper.\n- **Reviewer 2**: Points out several issues with the clarity and rigor of the claims, the limited coverage of the analysis, and the practical significance of the findings. These are significant concerns that need to be addressed.\n- **Reviewer 3**: Highlights the need for a more comprehensive analysis and the potential limitations of the findings. The reviewer's concerns about the generalizability and practical significance are valid.\n- **Reviewer 4**: Questions the robustness of the claims and suggests further investigations to validate the findings. The reviewer's suggestions for additional experiments are valuable.\n- **Reviewer 5**: Raises several concerns about the clarity and rigor of the claims, the experimental setup, and the practical significance of the findings. The reviewer's feedback is detailed and provides a comprehensive list of issues that need to be addressed.\n\n### Final Decision\nWhile the paper presents an interesting and novel observation, the current version has several significant weaknesses that need to be addressed to meet the standards of a top-tier conference. The claims and analysis need to be more rigorous, and the practical significance of the findings needs to be better demonstrated. Therefore, the paper should be rejected for publication in its current form, but the authors are encouraged to revise and resubmit after addressing the reviewers' feedback.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Interesting Observation**: The paper presents a novel and interesting observation that the order of batch normalization (BN) and activation functions can significantly impact the performance of neural networks, especially when using bounded activation functions like Tanh.\n2. **Clear Hypothesis and Experiments**: The authors clearly state their hypothesis and provide a series of experiments to support their claims. They introduce metrics to measure saturation, skewness, and sparsity, which are well-defined and useful for their analysis.\n3. **Reproducibility**: The authors provide code and detailed training procedures, which enhances the reproducibility of their results.\n\n#### Weaknesses:\n1. **Clarity and Rigor**:\n   - **Saturation and Sparsity**: The relationship between saturation and sparsity is not clearly explained. The claim that higher saturation leads to higher sparsity is counterintuitive and requires more rigorous justification.\n   - **Generalization vs. Accelerated Training**: The authors need to provide more evidence to support the claim that the improved performance is due to better generalization rather than accelerated training. The addition of training and test loss curves in the appendix is a step in the right direction, but more detailed analysis is needed.\n   - **Residual Connections**: The explanation for why Tanh cannot be used in residual connections is not clear. The authors should provide a more detailed and rigorous explanation or consider including experiments with residual connections to validate their claims.\n\n2. **Empirical Analysis**:\n   - **Limited Coverage**: The analysis is limited to VGG-like architectures and does not cover more modern architectures like ResNets. The authors should extend their experiments to a broader range of architectures to strengthen their claims.\n   - **Comparison with ReLU**: The Swap model with bounded activation functions does not outperform the Convention model with ReLU. This limits the practical significance of the findings, as ReLU is widely used and performs well in many tasks.\n\n3. **Technical Novelty**:\n   - **Saturation Metric**: The saturation metric is not complete enough. The authors should address the reviewer's concern that the metric can be close to 1 even if the values are centered around 0.\n   - **Asymmetric Saturation**: The authors need to provide more evidence to support the claim that asymmetric saturation is beneficial. They should consider creating symmetric saturation in conventional architectures to see if it enhances performance.\n\n4. **Rebuttal**:\n   - **Additional Experiments**: The authors have addressed some of the reviewer's concerns by providing additional experiments and clarifications. However, some issues, such as the relationship between saturation and sparsity, still require more detailed explanations.\n\n### Final Decision\nDespite the interesting and novel observation, the paper has several weaknesses that need to be addressed to strengthen the claims and improve the clarity of the arguments. The empirical analysis is limited, and the practical significance of the findings is not fully demonstrated. Therefore, the paper should be **rejected** for publication at a top-tier conference but with the encouragement to revise and resubmit after addressing the identified issues.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novel Concept**: The idea of collaborative adversarial training (CAT) is innovative and addresses a gap in the literature by leveraging the complementary robustness of different adversarial training methods.\n2. **Empirical Results**: The paper demonstrates state-of-the-art robustness on CIFAR-10 and CIFAR-100 under the Auto-Attack benchmark, which is a strong empirical validation of the method's effectiveness.\n3. **Clarity and Readability**: The paper is well-written and easy to follow, making it accessible to a broad audience.\n\n#### Weaknesses of the Paper\n1. **Computational Cost**: The proposed method requires training two models simultaneously, which doubles the computational cost and memory overhead. This is a significant drawback, especially given the already high computational requirements of adversarial training.\n2. **Lack of Theoretical Analysis**: The paper does not provide a theoretical analysis to explain why the proposed method works. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms and potential limitations.\n3. **Limited Baselines and Comparisons**: The paper does not compare the proposed method with more recent and stronger baselines, such as ensemble adversarial training methods. This limits the robustness of the empirical results and the novelty of the contribution.\n4. **Typos and Missing Citations**: The paper contains several typos and fails to cite relevant prior work, which affects its academic rigor and completeness.\n5. **Limited Dataset and Architecture Evaluation**: The experiments are conducted only on CIFAR-10 and CIFAR-100, which are relatively small datasets. Larger datasets like ImageNet and a wider range of network architectures should be included to fully validate the method's effectiveness.\n6. **Ablation Studies**: The paper lacks detailed ablation studies, such as comparing the performance of CAT with different combinations of adversarial training methods and evaluating the impact of using more than two defenses.\n\n#### Reviewer Feedback\n- **Reviewer 1** and **Reviewer 4** provide balanced feedback, acknowledging the strengths of the paper while pointing out the need for more detailed comparisons and theoretical analysis.\n- **Reviewer 2** and **Reviewer 3** are more critical, highlighting the lack of novelty, the high computational cost, and the need for more comprehensive experiments and ablation studies.\n\n### Final Decision\nGiven the significant computational cost, the lack of theoretical analysis, and the limited empirical comparisons, the paper does not meet the high standards of a top-tier conference. While the idea is novel and the empirical results are promising, the paper needs substantial improvements to address the identified weaknesses.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novelty and Potential Impact:**\n   - The paper introduces a novel framework, Functional Risk Minimization (FRM), which challenges the traditional assumptions of data generation in machine learning. This is a significant contribution as it offers a new perspective on how to model and handle noise in data.\n   - The idea of associating each data point with its own function is innovative and could lead to more realistic noise models, especially in scenarios where data variations are structured.\n\n2. **Theoretical Foundations:**\n   - The paper provides a theoretical derivation of FRM and shows how it subsumes Empirical Risk Minimization (ERM) for common loss functions. This theoretical grounding is strong and adds to the paper's credibility.\n   - The authors have also provided a clear connection between FRM and the over-parameterized regime, which is a relevant and timely topic in modern deep learning.\n\n3. **Empirical Results:**\n   - The initial experiments, while small in scale, demonstrate the potential benefits of FRM over ERM in regression and reinforcement learning tasks. The new experiments with CNNs and VAEs on MNIST variants further support the claims and show that FRM can be applied to more complex models and datasets.\n   - The results on structured variations (colored and translated MNIST) are particularly compelling, as they highlight the strengths of FRM in capturing structured noise.\n\n4. **Scalability and Practicality:**\n   - The authors have addressed the scalability concerns by providing an approximation (Eq. 9) that is both accurate and scalable. They have also shown that FRM can be applied to real neural networks like VAEs, which is a significant step forward.\n   - The potential for FRM to enable more efficient training of neural networks by implicitly representing larger models is an exciting direction for future research.\n\n#### **Weaknesses:**\n1. **Clarity and Presentation:**\n   - The paper has some clarity issues, particularly in the distinction between the FGM modeling assumption and the FRM learning objective. The authors have acknowledged this and have made improvements in the response, but further refinement is needed.\n   - The use of \"ERM\" in a non-standard way can be confusing, and the authors should clarify this in the final version of the paper.\n\n2. **Theoretical Guarantees:**\n   - While the paper provides some theoretical insights, it lacks formal arguments showing why or when FRM will outperform ERM. The authors have addressed this to some extent in the response, but more rigorous theoretical analysis would strengthen the paper.\n\n3. **Experimental Scope:**\n   - The initial experiments were small in scale, which was a concern. However, the new experiments with CNNs and VAEs on MNIST variants address this issue and provide a broader range of empirical evidence.\n   - The paper could benefit from more extensive experiments on larger and more diverse datasets to further validate the claims.\n\n4. **Scalability Concerns:**\n   - The pure version of FRM (Eq. 7) is not scalable, and the approximation (Eq. 9) is necessary for practical applications. The authors have addressed this in the response, but it remains a limitation that should be clearly communicated in the paper.\n\n### Final Decision\nDespite the weaknesses, the paper presents a novel and promising framework that has the potential to significantly impact the field of machine learning. The theoretical foundations are strong, and the empirical results, while initially small in scale, have been expanded to include more complex models and datasets. The authors have also addressed many of the concerns raised by the reviewers, particularly regarding scalability and clarity.\n\nGiven the high novelty, potential impact, and the improvements made in the response, I believe this paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance and Importance:**\n   - The paper addresses a significant challenge in reinforcement learning (RL) and game AI: maintaining a diverse set of high-skill agents through self-play. This is a relevant and important problem, especially in complex games where diverse playing styles can lead to better generalization and robustness.\n\n2. **Methodological Contribution:**\n   - The proposed bi-objective optimization model is a novel approach to maintaining diversity in playing styles while optimizing for skill level. The use of a meta bi-objective model to ensure that high-level agents with diverse playing styles are more likely to be incomparable is a creative and interesting idea.\n\n3. **Empirical Evaluation:**\n   - The paper provides empirical results on two different games: Pong and Justice Online. The results show that the proposed method can learn a well-generalized policy and maintain a diverse set of high-level agents. The experiments are well-designed and provide insights into the effectiveness of the proposed approach.\n\n4. **Clarity and Readability:**\n   - The paper is generally well-written and easy to follow. The authors provide a clear explanation of the problem, the proposed method, and the experimental setup. The figures and tables are well-presented and help in understanding the results.\n\n#### **Weaknesses of the Paper:**\n1. **Literature Review and Baselines:**\n   - **Missing Important Literature:**\n     - The paper does not discuss the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al. (2017), which is a significant work in the field of multiagent RL. This omission is a major weakness, as the paper should be compared against relevant baselines from this work.\n   - **Quality-Diversity Algorithms:**\n     - The paper does not compare the proposed method against quality-diversity algorithms, such as MAP-Elites, which are widely used for maintaining diversity in RL. This comparison is crucial to understand the novelty and effectiveness of the proposed method.\n\n2. **Experimental Design:**\n   - **Imbalanced Opponent Pool:**\n     - The experimental design in Table 3 is problematic because the pool of opponents is heavily biased towards the proposed method (BiO) and EMOGI. This gives an unfair advantage to these methods and makes it difficult to assess their true performance.\n   - **Limited Domains:**\n     - The paper only evaluates the proposed method on two games: Pong and Justice Online. While these are good choices, the results would be more robust if the method were tested on a broader range of games or environments. This would help to demonstrate the generalizability of the approach.\n\n3. **Technical Details and Clarity:**\n   - **Playing Style Quantification:**\n     - The reduction of playing style to a single scalar (0 for defensive, 1 for aggressive) is a significant simplification. While this might work well in the tested environments, it is a limitation that should be acknowledged and discussed. The paper should also explore the possibility of using a more complex representation of playing style.\n   - **Confidence Intervals and Statistical Significance:**\n     - The paper does not provide confidence intervals or statistical significance tests for the reported results. This makes it difficult to assess the reliability and significance of the empirical findings.\n\n4. **Scalability and Robustness:**\n   - **High-Dimensional Playstyles:**\n     - The paper does not discuss how the proposed method scales to games with high-dimensional playstyles. This is an important consideration, as the complexity of the problem increases with the dimensionality of the playing style.\n   - **Domain Randomization:**\n     - The paper does not compare the proposed method against domain randomization, which is a common technique to improve generalization in RL. This comparison would provide valuable insights into the relative strengths and weaknesses of the proposed method.\n\n### Final Decision\n\nGiven the strengths and weaknesses of the paper, the following points are critical in making the final decision:\n\n- **Strengths:**\n  - The problem addressed is important and relevant.\n  - The proposed method is novel and shows promising results.\n  - The paper is well-written and easy to follow.\n\n- **Weaknesses:**\n  - Missing important literature and baselines.\n  - Imbalanced experimental design.\n  - Limited domains and lack of scalability analysis.\n  - Insufficient technical details and statistical significance.\n\nWhile the paper has several strengths, the weaknesses, particularly the missing important literature and the imbalanced experimental design, are significant enough to warrant a rejection. The paper would benefit from addressing these issues and resubmitting for a more thorough evaluation.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Technical Novelty and Significance:**\n- **Strengths:**\n  - The paper introduces a novel mechanism for label differential privacy (DP) in regression tasks, which is a significant contribution. The proposed \"randomized response on bins\" (RR-on-Bins) mechanism is theoretically optimal under a given regression loss function.\n  - The authors provide a clear and rigorous proof of the optimality of their mechanism, which is a strong theoretical contribution.\n  - The paper extends the concept of label DP from classification to regression, which is a valuable generalization.\n  - The empirical evaluation on real datasets demonstrates the effectiveness of the proposed method, showing that it outperforms other common DP methods.\n\n- **Weaknesses:**\n  - Some reviewers (e.g., Reviewer 3 and Reviewer 5) point out that the technical novelty is somewhat limited, as aspects of the contributions exist in prior work. However, the extension to regression and the specific mechanism for achieving optimality are novel.\n  - The paper could benefit from a more detailed comparison with other label-DP methods, particularly those designed for regression. The authors have addressed this in their response, but it would be stronger if included in the main text.\n\n#### **2. Empirical Novelty and Significance:**\n- **Strengths:**\n  - The empirical evaluation is thorough and conducted on real-world datasets, which adds credibility to the claims.\n  - The results show that the proposed method outperforms other DP methods, which is a strong empirical validation of the theoretical claims.\n  - The authors have included additional experiments in response to reviewer comments, which demonstrates their commitment to addressing feedback and improving the paper.\n\n- **Weaknesses:**\n  - The comparison with other label-DP methods, particularly those designed for regression, is not as comprehensive as it could be. While the authors have provided some results in their response, a more detailed comparison in the main text would strengthen the paper.\n\n#### **3. Clarity and Quality:**\n- **Strengths:**\n  - The paper is well-written and easy to follow, which is a significant strength.\n  - The authors have addressed reviewer comments and made revisions to improve the clarity and completeness of the paper.\n  - The proofs and derivations are well-structured and clearly presented.\n\n- **Weaknesses:**\n  - There are some minor issues with the claims and statements, as noted by Reviewer 1 and Reviewer 6. These issues are minor and can be addressed with small changes.\n  - The paper could benefit from a more detailed discussion of the practical implications and limitations of the proposed method.\n\n#### **4. Correctness:**\n- **Strengths:**\n  - The claims and statements are well-supported and correct, as noted by Reviewer 4 and Reviewer 5.\n  - The authors have provided a detailed proof of the optimality of their mechanism, which is a strong theoretical foundation.\n\n- **Weaknesses:**\n  - Some reviewers (e.g., Reviewer 3) note that the proofs were not checked in detail, which suggests that a thorough review of the proofs would be beneficial.\n\n### Final Decision\n\nGiven the significant technical and empirical contributions, the thorough evaluation, and the clear writing, the paper should be accepted for publication. However, the authors should address the minor issues and incorporate the additional comparisons and discussions suggested by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Problem and Motivation\nThe paper addresses a significant and practical problem in machine learning: the trade-off between using more data for training and having a reliable validation set for model evaluation. This issue is particularly relevant in real-world applications where data is often limited. The motivation is clear and well-articulated, and the problem is indeed underexplored in the literature.\n\n#### Novelty and Contribution\nThe proposed Proximal Validation Protocol (PVP) is a novel approach that leverages data augmentation and a distribution-preserving sampling algorithm to create a validation set without splitting the training data. This idea is innovative and has the potential to improve the efficiency and reliability of model evaluation in practical settings.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to understand. However, there are several issues with clarity and presentation that need to be addressed:\n- **Reviewer 1** points out that the paper uses some unusual language and terminology that could be improved for scientific clarity.\n- **Reviewer 3** suggests that the paper could benefit from more detailed explanations and better readability in the experimental section.\n\n#### Empirical Evaluation\nThe empirical evaluation is a critical aspect of the paper, and it has several shortcomings:\n- **Reviewer 1** and **Reviewer 3** both highlight the need for more thorough empirical evaluation. The experiments should demonstrate that PVP not only improves model evaluation but also leads to better model selection and hyperparameter tuning.\n- **Reviewer 2** and **Reviewer 3** note that the datasets used are relatively small and may not fully support the claims. Larger and more diverse datasets, including real-world and industrial datasets, should be included.\n- **Reviewer 4** criticizes the lack of theoretical justification for the metrics used to evaluate the validation set. The test-validation gap and variance metrics need to be more rigorously defined and justified.\n\n#### Theoretical Justification\n- **Reviewer 4** points out that the paper lacks a theoretical basis for why the proximal set is representative of the data-generating distribution. This is a significant concern, as the effectiveness of the validation set is crucial for reliable model evaluation.\n\n#### Addressing Reviewer Concerns\n- **Model Selection and Hyperparameter Tuning**: The paper should explicitly demonstrate that PVP leads to better model selection and hyperparameter tuning, which is a key aspect of validation set usage.\n- **Dataset Diversity**: The experiments should be extended to larger and more diverse datasets, including real-world and industrial scenarios.\n- **Theoretical Justification**: The paper should provide a more rigorous theoretical analysis to support the claims about the representativeness of the proximal validation set.\n- **Ablation Studies**: More ablation studies should be conducted to understand the impact of different hyperparameters and choices in the PVP method.\n\n### Final Decision\nGiven the significant issues with empirical evaluation and theoretical justification, and the need for more thorough and diverse experiments, the paper in its current form is not ready for publication in a top-tier conference. However, the problem addressed is important, and the proposed method has potential. The authors should address the reviewer concerns and resubmit a revised version.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Motivation and Relevance**\nThe paper addresses the important problem of predicting protein-protein interactions (PPIs), which is crucial for understanding cellular behavior and functionality. The motivation for using structural embeddings is well-founded, as it leverages the physical binding properties of proteins, which are often ignored in sequence and network-based methods.\n\n#### **Methodology and Novelty**\n1. **Pretrained Structural Embeddings**: The authors use pretrained embeddings from OmegaFold, a state-of-the-art protein structure prediction model. While the use of OmegaFold embeddings is a reasonable choice, the novelty is limited because the embeddings are directly adopted without significant modifications.\n2. **Graph Neural Networks (GNNs)**: The paper proposes using GNNs to predict PPIs based on these structural embeddings. However, the novelty here is also limited, as the GNN architecture is not significantly different from standard models like GraphSAGE and GAT.\n3. **Transfer Learning**: The authors demonstrate the transferability of PPI prediction models across different species, which is a valuable contribution. However, this is more of an application of the method rather than a novel contribution.\n\n#### **Empirical Evaluation**\n1. **Datasets**: The paper uses two datasets (antibody-target interaction and membrane protein interaction) to validate the method. However, the choice of baselines is weak. The authors only compare their method to GraphSAGE and GAT, which are not necessarily the state-of-the-art (SOTA) methods for PPI prediction. Other relevant methods, such as dMaSIF and AlphaFold-Multimer, are not considered.\n2. **Results**: The empirical results show marginal improvements over the baselines. The lack of confidence intervals and the inconsistency in results across different datasets (e.g., PSE4PPI-SAGE and PSE4PPI-GAT on ATI and H-PPI datasets) raise concerns about the robustness and reliability of the findings.\n3. **Negative Sampling**: The method of negative sampling during training and evaluation is not clearly described, which is crucial for the reproducibility and validity of the results.\n\n#### **Clarity and Presentation**\n1. **Clarity**: The paper has some clarity issues. For example, the AUC metric and the binary classification problem setup are not clearly explained. The tables are not in a digestible format, and the lack of confidence intervals makes it difficult to interpret the results.\n2. **Writing**: The paper could benefit from additional proofreading to correct typos and grammatical mistakes.\n\n#### **Reviewer Comments**\n1. **Reviewer 1**: Points out the limited novelty and contribution, the potential issues with mean pooling, and the inconsistency in results. Suggests minor improvements in clarity and presentation.\n2. **Reviewer 2**: Emphasizes the limited novelty and the lack of significant modifications to existing methods.\n3. **Reviewer 3**: Highlights the lack of clarity in the presentation, the marginal improvements in results, and the weak baseline choices.\n4. **Reviewer 4**: Criticizes the weak baseline choices, the unclear model design, and the lack of comparison to more relevant methods like AlphaFold-Multimer.\n\n### Final Decision\nGiven the limited novelty, weak empirical evaluation, and clarity issues, the paper does not meet the standards of a top-tier conference. While the motivation is strong, the technical and empirical contributions are not significant enough to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Novel Approach**: The paper introduces a novel method for protein sequence and structure co-design using a trigonometry-aware encoder and a roto-translation equivariant decoder. This approach is innovative and addresses a significant challenge in the field of protein engineering.\n2. **Empirical Performance**: The method, ProtSeed, demonstrates strong empirical performance across multiple tasks, outperforming state-of-the-art baselines in terms of key metrics such as PPL, RMSD, and AAR.\n3. **Iterative Refinement**: The iterative refinement process, which updates both sequence and structure in one shot, is a significant improvement over autoregressive and diffusion models, reducing inference time and improving efficiency.\n4. **Clear Motivation**: The paper provides a clear motivation for the problem and a well-structured introduction to the related work, making it accessible to a broad audience.\n\n#### Weaknesses:\n1. **Hyperparameters and Training Details**: The paper lacks specific hyperparameter and training details, which are crucial for reproducibility. This is a significant concern for a top-tier conference.\n2. **Discrepancies with Prior Work**: There are discrepancies in the reported results compared to previous work, particularly with the Diffusion model. The authors need to address these discrepancies to ensure the validity of their claims.\n3. **Functional Evaluation**: The paper primarily focuses on structural metrics (PPL, RMSD, AAR) but lacks functional evaluations, which are essential for assessing the practical utility of the designed proteins.\n4. **Generalizability**: The claim that the method is the \"first protein sequence-structure co-design\" is somewhat misleading. While the method is novel, it is not the first to tackle this problem, and the authors should clarify the specific contributions and limitations.\n5. **Context Features**: The requirement for context features (single feature \\(m\\) and pair feature \\(z\\)) may limit the practical applicability of the method in scenarios where such features are not readily available.\n6. **Ablation Studies**: The paper lacks comprehensive ablation studies to demonstrate the contributions of each component of the model. This is important for understanding the effectiveness of the proposed method.\n\n#### Reviewer Feedback:\n- **Positive Feedback**: Reviewers generally acknowledge the novelty and empirical performance of the method. They appreciate the clear motivation and well-structured related work.\n- **Constructive Criticism**: Reviewers raise valid concerns about the lack of hyperparameters, training details, and functional evaluations. They also point out the need for clearer claims and more comprehensive ablation studies.\n\n### Final Decision\nGiven the strengths of the paper, particularly its novel approach and strong empirical performance, it has the potential to make a significant contribution to the field. However, the weaknesses, especially the lack of hyperparameters, training details, and functional evaluations, are significant and need to be addressed to meet the standards of a top-tier conference.\n\n**Final Decision: Accept**\n\nThe authors should address the following points in the final version:\n1. **Include hyperparameters and training details** to ensure reproducibility.\n2. **Address discrepancies with prior work** and provide a clear explanation for any differences in reported results.\n3. **Conduct functional evaluations** to assess the practical utility of the designed proteins.\n4. **Clarify claims** regarding the novelty and generalizability of the method.\n5. **Add comprehensive ablation studies** to demonstrate the contributions of each component of the model.\n6. **Discuss the practical implications** of requiring context features and provide more details on how to handle scenarios where these features are not readily available."
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Problem Relevance and Motivation\nThe paper addresses a significant and relevant problem in machine learning: adapting models to distribution shifts, particularly in the context of online learning and black-box models. The problem of label shift and conditional shift is well-motivated, and the authors provide a clear rationale for why this is important in real-world applications. This is a strong point in favor of the paper.\n\n#### Technical Contributions\nThe paper proposes several heuristics to improve the performance of existing label shift adaptation methods in the presence of conditional shift. These heuristics include:\n1. Using an OOD validation set for hyper-parameter tuning.\n2. Adding scaling hyper-parameters to the loss function.\n3. Handling non-invertible confusion matrices by using an identity matrix or adding a tunable scalar to the diagonal.\n\nWhile these heuristics are practical and potentially useful, they lack theoretical justification. The authors acknowledge this limitation and position their work as an empirical study. However, the lack of theoretical support and the ad-hoc nature of some heuristics (e.g., the identity matrix approximation) are significant weaknesses.\n\n#### Empirical Evaluation\nThe empirical evaluation is a mixed bag. The authors conduct experiments on synthetic and realistic datasets, including both classification and regression tasks. However, the scope of the experiments is limited, and the results are not consistently strong. For example:\n- In some cases, the proposed methods show marginal improvements over the baseline.\n- In other cases, the performance is comparable or even worse than the baseline.\n- The use of an OOD validation set is shown to be beneficial in some scenarios but not in others.\n\nThe authors are transparent about the limitations of their empirical findings, which is commendable. However, the modest and tentative nature of the results limits the paper's impact.\n\n#### Clarity and Reproducibility\nThe paper is generally well-organized and easy to follow. However, there are several issues with clarity and reproducibility:\n- Some key definitions and notations are missing or unclear (e.g., \"conditional shift,\" method definitions, Bayesian notation).\n- The experimental setup and details are not fully described, making it difficult to reproduce the results.\n- The authors have addressed some of these issues in their response, but the paper could benefit from further clarification and additional details.\n\n#### Novelty and Significance\nThe paper's contributions are marginal in terms of technical novelty and significance. The heuristics proposed are practical but not groundbreaking. The empirical study is a valuable contribution, but the limited scope and modest results reduce its impact.\n\n### Final Decision\nGiven the mixed nature of the empirical results, the lack of theoretical justification, and the limited scope of the experiments, the paper does not meet the high standards of a top-tier conference. While the problem is relevant and the authors have made a good effort, the contributions are not significant enough to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **Strengths of the Paper:**\n1. **Novel Idea**: The paper introduces a novel concept of using Binary Labels as an auxiliary task in multi-task learning. This idea is interesting and has the potential to improve the performance of classification models.\n2. **Empirical Results**: The authors show that their approach can improve accuracy on several datasets, including Cifar-100, Caltech101, Sun397, FGVC-Aircraft, and Stanford Cars. They also demonstrate improvements in data efficiency.\n3. **Simplicity**: The proposed method is relatively simple to implement and can be easily added to existing neural network architectures.\n\n#### **Weaknesses of the Paper:**\n1. **Lack of Thorough Analysis**: The paper lacks a detailed analysis of why the Binary Labels improve performance. There is no clear explanation of the underlying mechanisms or the conditions under which this approach is most effective.\n2. **Limited Experimental Scope**: The experimental section is weak. The authors do not compare their method against a wide range of baselines, and the datasets used are relatively small. The paper would benefit from experiments on larger benchmarks like ImageNet.\n3. **Clarity Issues**: The paper has several clarity issues. For example, the generation of Binary Labels is not clearly explained, and the use of Metabalance for multi-task learning is not well-defined. The paper also contains some minor typos and notational errors.\n4. **Reproducibility**: The paper lacks sufficient details to reproduce the results. Important implementation details such as optimizer, learning rate, data preprocessing, and batch size are missing.\n5. **Related Work**: The authors do not thoroughly discuss related work, particularly in the areas of label smoothing, unsupervised meta-learning, and multi-class label coding. This makes it difficult to assess the novelty of the proposed approach.\n6. **Small Improvements**: In some cases, the performance improvements are marginal, and the authors do not provide a clear understanding of when and why the proposed method is effective.\n\n#### **Reviewer Comments:**\n- **Reviewer 1**: Highlights the novelty of the idea but points out the need for more thorough experiments, analyses, and clarity. The reviewer also suggests comparing against additional baselines and exploring different datasets and scenarios.\n- **Reviewer 2**: Criticizes the paper for poor notation, calculation errors, and insufficient experimental results. The reviewer also points out that the claim about the lack of research on labels is not well-supported.\n- **Reviewer 3**: Finds the method simple and effective but notes that the contribution might not be significant enough due to small empirical improvements and the lack of further analysis.\n- **Reviewer 4**: Echoes the need for more detailed explanations and experiments, particularly on larger benchmarks. The reviewer also suggests analyzing the weighting between the two tasks and discussing the properties modeled by Binary Labels.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, and the feedback from the reviewers, the paper does not meet the high standards required for a top-tier conference. While the idea of using Binary Labels as an auxiliary task is novel and has some empirical support, the paper lacks the depth of analysis, thorough experimental validation, and clarity needed to justify acceptance. The authors should address the identified issues and conduct more comprehensive experiments before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Technical Contributions and Novelty**\n- **Strengths:**\n  - The paper addresses an important problem in permuted linear regression, specifically focusing on phase transition thresholds.\n  - The authors provide a precise identification of phase transition thresholds, which is a novel contribution compared to previous works that only considered convergence rates.\n  - The use of message passing algorithms and the connection to branching random walk processes is technically sound and well-explained in some sections.\n\n- **Weaknesses:**\n  - The incremental nature of the result is a concern. The paper heavily relies on prior work, particularly Semerjian et al. 2020 and Zhang et al. 2020, and the novelty lies primarily in the approximation used to compute the phase transition in the non-oracle setting.\n  - The assumption of a known signal matrix in the oracle case is unrealistic and limits the practical significance of the results.\n  - The partial permutation recovery section lacks clear motivation and practical justification.\n\n#### 2. **Clarity and Quality of Writing**\n- **Strengths:**\n  - The introduction provides a broad overview of the field and the problem.\n  - Some sections, particularly the theoretical analysis in Section 4, are well-explained and provide a clear understanding of the methods used.\n\n- **Weaknesses:**\n  - The paper is notationally heavy and requires a deep understanding of prior work, making it difficult for readers unfamiliar with the field.\n  - The organization of the paper is poor, with sections and equations often not clearly introduced or explained.\n  - There are numerous typos and grammatical errors throughout the paper, which detract from its clarity and readability.\n\n#### 3. **Empirical Results and Reproducibility**\n- **Strengths:**\n  - The paper includes numerical experiments that support the theoretical predictions, and the empirical phase transition points are well-aligned with the theoretical predictions.\n\n- **Weaknesses:**\n  - The code for the numerical results is not publicly available, which affects the reproducibility of the results.\n  - The empirical results are limited to finite samples, and the theoretical results are asymptotic, which creates a gap between practice and theory.\n\n#### 4. **Correctness and Support for Claims**\n- **Strengths:**\n  - The theoretical results are well-supported by the analysis and the connection to branching random walk processes.\n  - The numerical experiments provide empirical validation of the theoretical predictions.\n\n- **Weaknesses:**\n  - Some claims, such as the precision of the phase transition thresholds and the performance of the partial permutation estimator, are not well-supported by the results.\n  - The paper lacks a clear discussion of the limitations and assumptions, particularly the assumption of additive Gaussian noise and the unrealistic assumption of a known signal matrix.\n\n### Final Decision\nGiven the significant issues with clarity, organization, and the incremental nature of the results, the paper is not ready for publication in its current state. While the technical contributions are interesting, they are not sufficiently novel or well-presented to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Novelty**: The paper addresses a novel and important problem—data leakage in federated learning for tabular data. While there has been significant work on data leakage in image and NLP domains, the unique challenges posed by tabular data (mixed discrete-continuous features, structured data) have not been thoroughly explored. The authors propose a method, TabLeak, which combines a softmax structural prior, pooled ensembling, and an entropy measure to address these challenges. The use of softmax for continuous relaxation and pooling for variance reduction, while not entirely novel, is effectively applied to a new domain.\n- **Significance**: The paper's findings are significant because they highlight the vulnerability of federated learning on tabular data, which is often used in high-stakes applications like finance and healthcare. The empirical results show a 10% improvement over baselines on the Adult dataset, which is a substantial improvement in this domain.\n\n#### **2. Technical Quality and Correctness**\n- **Technical Quality**: The paper is technically sound. The authors provide a clear and detailed explanation of the method, including the mathematical formulations and the rationale behind each component. The experimental setup is well-designed, and the results are robust. The authors have addressed the reviewers' concerns by conducting additional experiments and providing more detailed justifications.\n- **Correctness**: The claims and statements are well-supported by the experimental results. The authors have addressed the issues raised by Reviewer 1 regarding the clarity and grammar of the paper, and the issues raised by Reviewer 2 regarding the novelty of the methods. The experiments are well-documented, and the source code is available, ensuring reproducibility.\n\n#### **3. Clarity and Presentation**\n- **Clarity**: The paper is well-written and easy to follow, as noted by Reviewer 3. The authors have addressed the minor typos and notational inconsistencies pointed out by Reviewer 3. The introduction and background sections provide a clear context for the problem, and the method section is well-structured.\n- **Presentation**: The paper is well-organized, with a logical flow of ideas. The figures and tables are clear and informative. The authors have provided detailed explanations and justifications for their choices, making the paper accessible to both experts and non-experts in the field.\n\n#### **4. Empirical Evaluation**\n- **Empirical Evaluation**: The empirical evaluation is thorough and convincing. The authors have conducted a comprehensive set of experiments on four popular tabular datasets, including ablation studies and comparisons with baselines. The results demonstrate the effectiveness of TabLeak, particularly in improving reconstruction accuracy for categorical features. The authors have also addressed the concerns raised by Reviewer 3 regarding the impact of the mix of continuous and discrete features on the variance of the reconstruction problem.\n\n#### **5. Addressing Reviewer Concerns**\n- **Reviewer 1**: The authors have addressed the concerns regarding the clarity and grammar of the paper. They have also provided additional experiments to support their claims about the variance reduction and the effectiveness of the pooling method.\n- **Reviewer 2**: The authors have addressed the concerns regarding the novelty of the methods by providing a detailed comparison with existing techniques and demonstrating the effectiveness of their approach in the tabular data setting.\n- **Reviewer 3**: The authors have addressed the detailed comments and questions raised by Reviewer 3, including the impact of the mix of continuous and discrete features, the justification for median pooling, and the impact of the network size. They have also provided additional experiments and clarifications to support their claims.\n\n### Final Decision\nBased on the detailed analysis, the paper should be accepted for publication at a top-tier conference. The paper addresses a novel and important problem, provides a technically sound and well-presented solution, and demonstrates the effectiveness of the proposed method through thorough empirical evaluation. The authors have also effectively addressed the concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Technical Novelty and Significance**\n- **Novelty**: The paper introduces a method called \"Scrunch\" for privacy-preserving representation learning in MLaaS settings. The core idea of splitting the neural network and using an additional loss function (Center Loss) to control information leakage is interesting. However, as noted by Reviewer 1, the idea of splitting networks is not new, and the novelty is incremental.\n- **Significance**: The significance of the work is limited. Reviewer 2 points out that the proposed approach is severely flawed, particularly in terms of the threat model and the practicality of the method. The server can easily recreate the client's encoder, and the privacy guarantees during the training phase are questionable.\n\n#### 2. **Empirical Novelty and Significance**\n- **Experimental Setup**: The experimental evaluation is limited to two datasets (MNIST and CelebA) and two models (LeNet-5 and VGG-16). Reviewer 1 and Reviewer 2 both suggest that the experiments are not comprehensive enough. More datasets and models, especially more sophisticated ones like ResNets and DenseNets, should be considered.\n- **Evaluation Metrics**: The evaluation metrics are interesting, but the results are not convincing. Reviewer 2 points out that the private task for MNIST is not well-defined, and the evaluation on CelebA is not challenging enough. The paper lacks experiments on more complex and realistic scenarios.\n\n#### 3. **Clarity and Quality**\n- **Clarity**: The paper is generally clear and easy to follow, as noted by Reviewer 2. However, there are grammatical errors and ambiguous statements that affect the clarity.\n- **Quality**: The quality of the work is poor, as highlighted by Reviewer 2. The technical flaws and unconvincing results significantly impact the overall quality.\n\n#### 4. **Correctness**\n- **Claims and Support**: Several of the paper's claims are incorrect or not well-supported. Reviewer 2 points out that the server can easily recreate the client's encoder, and the privacy guarantees are not robust. Reviewer 3 also notes that the security model is not well-defined, and the method is risky for information leakage.\n\n#### 5. **Reproducibility**\n- **Reproducibility**: The paper seems to be fairly reproducible, as the authors mention the hyperparameter settings used. However, details such as optimizers and the number of training epochs are missing, which could affect reproducibility.\n\n### Final Decision\nGiven the incremental nature of the novelty, the limited and unconvincing experimental evaluation, the technical flaws, and the lack of a well-defined security model, the paper does not meet the standards of a top-tier conference like ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Motivation and Problem Relevance**:\n   - The paper addresses a significant and challenging problem in the field of learning in games, specifically the convergence to good equilibria. This is a crucial issue that has not been extensively explored in the literature, making the problem highly relevant.\n\n2. **Theoretical Contributions**:\n   - The paper provides a solid theoretical foundation by proving pointwise convergence of q-replicator dynamics to Nash equilibria in a broader class of games compared to previous works.\n   - The introduction of average performance metrics and the quantification of these metrics for specific dynamics (gradient descent and standard replicator) in 2x2 symmetric coordination games is a novel and valuable contribution.\n\n3. **Empirical Analysis**:\n   - The empirical results and visualizations provide a clear and intuitive understanding of the regions of attraction and the performance of different dynamics. This is particularly useful for validating the theoretical findings and for extending the results to other dynamics in the QRD class.\n\n4. **Clarity and Writing**:\n   - The paper is well-written and the problem is well-motivated. The authors have made efforts to address the reviewers' comments, improving the clarity and accuracy of the paper.\n\n#### Weaknesses of the Paper\n1. **Scope and Generalizability**:\n   - The main results are limited to 2x2 symmetric coordination games, which is a relatively restricted setting. While the authors argue that this is a necessary first step, the lack of results for more general games is a significant limitation.\n   - The empirical results and visualizations are promising, but the paper does not provide a clear path for extending the analysis to higher-dimensional games or more complex settings.\n\n2. **Comparison with Prior Work**:\n   - The paper's relationship with prior work, particularly PP16, was initially not well-explained. The authors have since added a paragraph to clarify the similarities and differences, but the initial lack of clear comparison could have led to misunderstandings about the novelty of the results.\n\n3. **Technical Depth**:\n   - While the mathematical analysis is involved and spans various concepts, the results for 2x2 games are somewhat incremental compared to PP16. The paper could benefit from more extensive theoretical results or a clearer demonstration of the significance of the findings in a broader context.\n\n#### Reviewer Feedback and Author Responses\n- **Reviewer 1**: The author's response addressed the concerns about the abstract and the use of \"whenever\" in Theorem 4.4. The clarification about the monotonicity of stable manifolds and regions of attraction was well-explained.\n- **Reviewer 2**: The author's response provided a detailed comparison with PP16, which helped to clarify the novelty and significance of the results. The added paragraph in the paper is a positive step.\n- **Reviewer 3**: The author's response addressed the concerns about the scope of the results and the clarity of the paper. The rephrasing of Theorem 4.4 and the clarification of notation are improvements.\n- **Reviewer 4**: The author's response was well-received, and the reviewer recommended acceptance. The reviewer's comments about the importance of the problem and the clarity of the paper are strong points in favor of acceptance.\n\n### Final Decision\nGiven the significant contributions in terms of motivation, theoretical analysis, and empirical results, and the authors' efforts to address the reviewers' concerns, the paper should be accepted for publication. However, the authors should continue to work on extending the results to more general games and providing a clearer path for future research.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Identification of Period Drift**: The paper introduces the concept of \"period drift\" in federated learning, which is a novel and important observation. This concept addresses the heterogeneity in data distributions across different communication rounds, which is a significant issue in federated learning.\n2. **Meta-Learning Approach**: The proposed method, FedPA, uses a meta-learning framework to learn an adaptive aggregation strategy. This approach is innovative and has the potential to improve the performance of federated learning models.\n3. **Experimental Results**: The paper provides experimental results on two datasets (EMNIST and MovieLens) that demonstrate the effectiveness of FedPA compared to baseline methods.\n\n#### Weaknesses:\n1. **Lack of Theoretical Justification**: The paper does not provide a strong theoretical foundation for the concept of period drift or why the proposed meta-learning approach is particularly suited to address it. This lack of theoretical support weakens the paper's claims.\n2. **Proxy Dataset Assumption**: The use of a proxy dataset is a strong assumption and limits the practical applicability of the method. The paper does not adequately address the challenges and limitations associated with this assumption.\n3. **Ablation Studies**: The paper lacks comprehensive ablation studies to understand the impact of different components of the proposed method, such as the data distribution of the proxy dataset and the architecture of the neural networks used for aggregation.\n4. **Comparison with Related Work**: The paper does not compare FedPA with other relevant methods, such as FedET and DS-FL, which are similar in their use of proxy datasets. This comparison is crucial to justify the effectiveness and novelty of FedPA.\n5. **Experimental Settings**: The experimental settings are not strong enough. The datasets and models used are relatively simple, and the paper could benefit from including more challenging datasets (e.g., CIFAR-100 and Stack Overflow) to demonstrate the robustness of the method.\n6. **Clarity and Quality**: The paper has several grammatical and formatting issues, which affect its readability and overall quality. The figures are also poorly formatted, making it difficult to interpret the results.\n7. **Reproducibility**: The paper does not provide code or detailed implementation details, which makes it challenging for other researchers to reproduce the results.\n\n#### Reviewer Feedback:\n- **Reviewer 1**: Points out the lack of discrimination between client drift and period drift, the use of a proxy dataset, and the need for more comprehensive experiments and ablation studies.\n- **Reviewer 2**: Highlights the need for a more in-depth discussion on the advantages of the meta-learning framework and the potential use of regularization-based approaches. Also, suggests comparing FedPA with related work.\n- **Reviewer 3**: Expresses concerns about the ad-hoc nature of the method, the need for more ablation studies, and the experimental performance on FEMNIST. Also, questions the practicality of the proxy dataset assumption.\n- **Reviewer 4**: Points out several minor issues with the writing and formatting, and suggests including more datasets and quantifying the server overhead.\n\n### Final Decision\nGiven the significant weaknesses in theoretical justification, experimental settings, and reproducibility, as well as the need for more comprehensive comparisons with related work, the paper does not meet the standards of a top-tier conference. While the concept of period drift and the meta-learning approach are interesting, the paper requires substantial improvements to address the reviewers' concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### **Strengths of the Paper:**\n1. **Novelty and Significance:**\n   - **DeepSeaProbLog** is a significant extension of DeepProbLog, addressing a critical limitation by supporting both discrete and continuous random variables. This is a substantial contribution to the field of neural-symbolic AI, as it allows for more expressive and realistic modeling of real-world problems.\n   - The integration of weighted model integration (WMI) and the development of a differentiable WMI approach are novel and important contributions. The theoretical proofs (Propositions 4.1 and 4.2) provide a solid foundation for the approach.\n   - The implementation of DeepSeaProbLog using TensorFlow Probability is a practical and valuable contribution, bridging the gap between discrete, symbolic methods and continuous, approximative ones.\n\n2. **Empirical Evaluation:**\n   - The paper provides a range of experiments that demonstrate the capabilities of DeepSeaProbLog, including a neural-symbolic object detection task, a variational auto-encoder with a difference constraint, and a neural hybrid Bayesian network. These experiments showcase the flexibility and data efficiency of the proposed approach.\n   - The use of real meteorological data in one of the experiments adds credibility to the empirical evaluation.\n\n3. **Clarity and Quality:**\n   - The paper is well-written and the technical content is sound. The authors have provided detailed responses to reviewer comments, addressing many of the concerns raised.\n   - The paper includes clear definitions and examples, making it accessible to readers who may not be familiar with the underlying concepts.\n\n#### **Weaknesses of the Paper:**\n1. **Incrementality Concerns:**\n   - Some reviewers have raised concerns about the incremental nature of the work, suggesting that it is a small extension of DeepProbLog. However, the authors have convincingly argued that the support for continuous variables and the differentiable WMI approach are significant contributions that go beyond a simple extension.\n   - The authors have also provided a detailed list of their contributions, which helps to clarify the novelty and significance of the work.\n\n2. **Empirical Evaluation:**\n   - While the experiments are well-designed, some reviewers have suggested that the empirical evaluation could be more extensive. For example, additional benchmarks and comparisons with other methods in the purely discrete setting would strengthen the paper.\n   - The MNIST subtraction experiment, while interesting, could benefit from more detailed ablation studies and a broader range of tasks.\n\n3. **Technical Details:**\n   - Some reviewers have pointed out that the paper could benefit from more detailed explanations of certain technical aspects, such as the handling of discrete random variables and the differentiation through WMI. The authors have addressed these concerns in their responses and have committed to adding more details to the paper.\n\n4. **Scalability:**\n   - The scalability of the implementation is a valid concern, as the problems expressible in DeepSeaProbLog fall into the #P-hard complexity class. However, the authors have acknowledged this and have discussed potential directions for future work to address computational hardness.\n\n### Final Decision\nGiven the significant contributions of the paper, the solid theoretical foundation, and the practical implementation, the paper should be accepted for publication. The authors have addressed the major concerns raised by the reviewers and have provided a compelling case for the novelty and significance of their work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Dataset Quality and Uniqueness**:\n   - The RuDar dataset is comprehensive and includes 3D radar echo observations, which is a significant improvement over existing datasets that often provide only single-level or aggregated data.\n   - The dataset covers multiple climate zones and a large temporal span (2019-2021), making it valuable for a wide range of precipitation nowcasting tasks.\n   - The inclusion of orography information adds another layer of complexity and realism to the dataset.\n\n2. **Baseline Evaluation**:\n   - The authors evaluate several baselines, including optical flow and neural network models, which provides a solid foundation for comparing different approaches.\n   - The addition of Earthformer results, even if limited to intensity inputs, shows the authors' commitment to evaluating state-of-the-art models.\n\n3. **Clarity and Reproducibility**:\n   - The paper is well-written and easy to understand, with clear descriptions of the dataset and evaluation methods.\n   - Both the dataset and code are publicly available, which enhances the reproducibility of the work.\n\n#### Weaknesses:\n1. **Methodological Novelty**:\n   - The paper primarily focuses on the dataset and baseline evaluations, with no new methodological contributions. This is a significant limitation for a top-tier conference like ICLR, which typically values novel methods and techniques.\n   - The uncertainty estimation is relatively simple and could benefit from more detailed analysis.\n\n2. **Baseline Methods**:\n   - The baseline methods used are relatively low-cost, and the authors acknowledge this in their response. While they have added Earthformer results, a more comprehensive evaluation with a variety of state-of-the-art models would strengthen the paper.\n\n3. **Cross-Dataset Evaluation**:\n   - The paper lacks cross-dataset evaluation, which would help in understanding the generalizability of the models and the dataset. This is a valid point raised by Reviewer 2.\n\n4. **Evaluation Metrics**:\n   - The use of MSE and F1 measure is appropriate, but additional metrics (e.g., IoU, CSI) could provide a more comprehensive evaluation of the models' performance.\n\n#### Additional Considerations:\n- **Community Impact**:\n  - The dataset has the potential to be a valuable resource for the weather forecasting and machine learning communities, especially for precipitation nowcasting tasks.\n  - The authors' commitment to making the dataset and code publicly available is commendable and aligns with the open science principles.\n\n- **Future Work**:\n  - The authors have outlined several interesting directions for future research, including domain adaptation, anomaly detection, and uncertainty estimation. These could be valuable contributions in their own right.\n\n### Final Decision\nGiven the strengths of the dataset and the potential impact on the community, but considering the limitations in methodological novelty and the need for more comprehensive evaluations, the paper is better suited for a more specialized venue or a journal that focuses on datasets and benchmarks.\n\nFinal Decision: Reject"
    }
  ]
}