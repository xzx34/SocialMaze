{
  "summary": {
    "model": "deepseek-r1",
    "total_papers": 100,
    "correct_predictions": 82,
    "accuracy": 82.0,
    "confusion_matrix": [
      [
        59,
        17
      ],
      [
        1,
        23
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 77.63157894736842,
        "precision": 98.33333333333333
      },
      "Accept": {
        "total": 24,
        "recall": 95.83333333333334,
        "precision": 57.49999999999999
      }
    }
  },
  "results": [
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes ROOTS, a generative model for unsupervised 3D scene decomposition and rendering. The key contributions are object-oriented 3D representations, hierarchical decomposition, and maintaining performance compared to GQN. The reviewers have a mix of positive and negative feedback.\n\nReviewer 1 initially rejected due to writing issues but upgraded to a 6 after revisions. They pointed out incorrect claims about being the first unsupervised model, notation inconsistencies, and some methodological questions. The authors addressed these by revising the paper, fixing claims, and clarifying notation.\n\nReviewer 2 found the idea important but criticized unclear experiments and claims not supported by figures. The authors added more experiments in the appendix, clarified figures, and addressed the need for better comparisons. However, the experiments still seem a bit weak, especially the lack of ablation studies.\n\nReviewer 3 noted the lack of proper citations and comparisons with existing work, and the absence of ablation studies. The authors added more related work and some experiments but didn't fully address the ablation issue. The core contribution is seen as minor over existing GQN and 3D works.\n\nReviewer 4 had major concerns about method complexity, weak experiments, and broad claims. They felt the paper was hard to follow and experiments didn't sufficiently validate the claims. The authors improved clarity but didn't fully resolve the experimental weaknesses, like comparing with GQN's object detection or showing scalability.\n\nThe authors' response included significant revisions: improving clarity, adding experiments on generalization and composition, expanding related work, and fixing notation. However, some issues remain. The experiments still lack ablation studies to show which components contribute to the results. The comparison with GQN on object detection isn't done, and the benefits of the hierarchical structure aren't empirically validated. The claims about being the first unsupervised model were adjusted, but positioning within existing literature might still be insufficient.\n\nThe AC's post-rebuttal update leans towards rejection due to weak experiments and remaining clarity issues. The authors' revisions improved readability but didn't fully address experimental gaps. The lack of ablation studies and insufficient empirical validation of the method's components are critical. While the qualitative results are impressive, the quantitative results are marginal, and the paper's claims aren't fully substantiated. Given the high standards of a top-tier conference, the remaining weaknesses in experimental rigor and thorough validation might justify rejection despite the revisions.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a novel approach to unsupervised 3D scene decomposition with promising qualitative results, but critical shortcomings in experimental validation and clarity persist despite revisions. While the authors addressed many issues (e.g., improving notation, expanding related work, and adding experiments), core weaknesses remain:  \n1. **Insufficient Empirical Validation**:  \n   - **Ablation Studies**: The lack of ablation studies leaves the contribution of individual components (e.g., hierarchical representation, 3D grid) unverified. Without isolating these elements, it is unclear whether the improvements stem from architectural choices or increased model capacity.  \n   - **Generalization Claims**: Experiments on generalization (e.g., varying object counts) show mixed results, and comparisons to GQN’s limitations in object-centric tasks are not empirically demonstrated (e.g., object detection metrics for GQN are absent).  \n   - **Quantitative Results**: Marginal improvements in MSE and NLL over GQN do not strongly support the claimed advantages, and the absence of precision-recall curves or AP metrics weakens the object detection analysis.  \n\n2. **Clarity and Positioning**:  \n   - While readability improved, the method’s complexity and occasional inconsistencies (e.g., undefined terms like STN-INV) still hinder reproducibility.  \n   - The paper’s positioning within the broader 3D vision literature remains incomplete. Though citations were added, the discussion does not sufficiently differentiate ROOTS from prior work (e.g., 3D-R2N2, DeepVoxels) in terms of unsupervised object discovery versus supervised reconstruction.  \n\n3. **Overstated Claims**:  \n   - The revised title (“Object-Oriented Representation of 3D Scenes”) remains overly broad, and claims about being the “first” unsupervised model for 3D object identification, though softened, still risk misrepresentation given existing unsupervised voxel segmentation methods.  \n\nWhile the qualitative results (e.g., object swapping, viewpoint invariance) are compelling, the paper does not meet the rigor expected at a top-tier conference. The unresolved experimental gaps and lingering clarity issues outweigh the methodological novelty, warranting rejection."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The title suggests that they're using Monte Carlo Tree Search (MCTS) to refine MCTS agents, which sounds a bit meta. The abstract mentions improving the coordination of episode generation by treating the entire system as a game tree search. They claim their method handles the exploration-exploitation trade-off better than AlphaZero, with experiments on a small problem showing robust performance.\n\nNow, the reviewers are all in agreement that the paper is poorly written. Reviewer 1 points out the lack of references and calls it \"less than half-baked.\" Reviewer 2 couldn't understand the abstract and mentions grammatical issues throughout. Reviewer 3 also notes the poor writing, undefined symbols, and lack of clarity in the algorithm. The author's response is apologetic, mentioning they removed references because they didn't know how to use a .bib file and that they found a similar idea in their country but not internationally. They acknowledge the writing issues and hope to improve.\n\nFirst, the writing quality is a major issue. If the paper is incomprehensible, it's impossible to evaluate the technical merits. Even if the idea has potential, the presentation is so poor that the contribution isn't clear. Top-tier conferences require clarity and rigor. The lack of references is another red flag; not citing prior work like AlphaZero properly undermines the paper's credibility. The author's response about not knowing how to use a .bib file is concerning—references are a basic academic requirement.\n\nThe technical contribution is also questionable. The experiments are on a small problem (tic-tac-toe?), which might not demonstrate the method's effectiveness in more complex scenarios. The reviewers didn't get into the technical details because the writing was too poor, so the actual innovation is unclear. The author mentions a \"meta-tree\" idea found locally but hasn't connected it to existing international literature, which is a gap.\n\nThe author's response doesn't adequately address the issues. They apologize but don't provide a plan to fix the references or improve the writing. The mention of a similar idea in their country but not internationally suggests possible issues with novelty if it's already been published elsewhere, even if not in English.\n\nIn top-tier conferences, even a novel idea needs to be presented clearly, with proper citations, and validated through rigorous experiments. This paper fails on presentation, scholarly rigor, and possibly novelty. The reviewers' unanimous negative feedback, especially regarding clarity and references, makes it hard to justify acceptance. The author needs to significantly revise the paper, improve the writing, add references, and validate the method on more substantial problems before it's suitable for publication.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper suffers from critical flaws that render it unsuitable for publication in a top-tier conference:  \n\n1. **Severe Writing and Clarity Issues:**  \n   All three reviewers highlight the paper’s incomprehensibility due to pervasive grammatical errors, undefined notation, and poorly structured explanations. The abstract and technical content are described as unintelligible, which undermines the ability to assess the method’s novelty or validity. Top-tier conferences demand clarity to ensure scholarly rigor and reproducibility.  \n\n2. **Lack of Scholarly Foundation:**  \n   The absence of references (attributed by the author to unfamiliarity with .bib files) is a fundamental failure to contextualize the work within existing literature. For instance, AlphaZero and related MCTS/RL research are central to the paper’s claims but are not properly cited, violating academic standards.  \n\n3. **Unclear Technical Contribution:**  \n   While the idea of using MCTS to refine MCTS agents is intriguing, the paper fails to articulate how this approach differs from prior work (e.g., meta-MCTS frameworks) or why it constitutes a meaningful advance. The experiments on a “small problem” (likely tic-tac-toe) lack the scale or comparison to established baselines needed to demonstrate robustness or superiority over AlphaZero.  \n\n4. **Author Response Deficiencies:**  \n   The author’s response acknowledges weaknesses but does not propose concrete revisions to address them. The admission of a similar “meta-tree” idea in their local literature raises concerns about novelty, especially if this work has already been explored (even in non-English publications).  \n\n**Conclusion:**  \nWhile the core idea might have potential, the paper’s current form is irreparably flawed in presentation, scholarship, and technical evaluation. Significant revisions—including rewriting for clarity, adding references, and rigorous experimentation—are required before it could be reconsidered."
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper presents a progressive learning approach for hierarchical disentanglement in VAEs. The authors claim that their method improves disentanglement by growing the network's capacity incrementally. The reviewers had several concerns, but the authors addressed many in their response.\n\nReviewer 1 initially pointed out issues with the training of the first layer compared to VLAE and requested mutual information comparisons. The authors added experiments showing that their model achieves a clearer information hierarchy, which seems to address this. The mutual information metrics now show that the layers in pro-VLAE have a descending order of information, supporting their claims.\n\nReviewer 2 was concerned about the paper's focus and the practicality of the new metric. The authors clarified that their main contribution is the progressive learning strategy, not just hierarchical representation. They also differentiated their metric from existing ones, explaining that it doesn't require a regressor, which addresses the hyperparameter concern. However, the metric still needs ground truth, which is a common limitation in the field, so that's acceptable. The comparison to [3] was addressed by highlighting architectural differences in capacity growth, which seems valid.\n\nReviewer 3 found the idea fresh and well-supported but wanted more quantitative info on information flow. The authors provided additional experiments with different layer configurations, showing consistent information flow patterns. This strengthens their argument about hierarchical disentanglement.\n\nReviewer 4 suggested more thorough comparisons with VLAE. The authors included mutual information metrics and ablation studies on implementation strategies like fade-in, showing improved training stability. They also addressed the inactive latent dimensions by testing with fewer dimensions, showing desired outcomes when factors are fully captured.\n\nThe authors' responses to the hyperparameter sensitivity (beta) acknowledge that high beta might hurt reconstruction but show their method outperforms baselines across most tested values. The ablation studies on fade-in and pre-trained KL penalties demonstrate the strategies' effectiveness in training stability.\n\nPotential weaknesses: The paper's novelty is somewhat incremental over VLAE, but the progressive approach and the new metric add value. The experiments are thorough, and the rebuttal addressed most concerns. However, the practical impact of the new metric is limited by the need for ground truth, which is a common issue but worth noting.\n\nOverall, the authors made significant improvements based on reviews, added necessary experiments, and clarified their contributions. The results support their claims, and the approach seems promising for disentanglement research. The paper now meets the standards of a top-tier conference.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper introduces a novel progressive learning strategy for hierarchical disentanglement in VAEs, addressing a significant challenge in representation learning. The authors effectively responded to reviewer concerns by:  \n1. **Conducting Additional Experiments:** They provided quantitative evidence of mutual information across layers, demonstrating clearer hierarchical information flow compared to VLAE. This addressed concerns about layer-wise training efficacy.  \n2. **Clarifying Contributions:** The distinction between progressive learning (their core contribution) and hierarchical representation was clarified, emphasizing the incremental growth of network capacity as orthogonal to prior work like [3].  \n3. **Validating Metrics:** The proposed MIG-sup metric, while requiring ground truth (a common limitation), was justified as a supplement to existing metrics, with comparisons showing its utility.  \n4. **Ablation Studies:** Training stability experiments (e.g., fade-in and pre-trained KL penalties) and comparisons with VLAE under varying configurations strengthened the method’s robustness.  \n5. **Addressing Information Flow:** Quantitative results on information allocation across layers (e.g., descending mutual information) validated the progressive learning mechanism.  \n\nWhile the paper’s novelty is incremental over VLAE, the progressive approach and empirical rigor meet top-tier standards. The rebuttal resolved critical concerns, and the results demonstrate improved disentanglement with hierarchical control. Minor weaknesses (e.g., metric limitations) are acknowledged but do not outweigh the contributions."
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about improving the training of certifiably robust models using convex relaxations. The authors propose two regularizers to tighten the bounds between certifiable and empirical robustness. The reviewers had a mix of positive and negative feedback, and the authors provided a detailed response addressing many of the concerns.\n\nStarting with Reviewer 1's comments: They pointed out several technical issues, like confusion in sections 4.1 and 4.3, and the need for more baselines like Gowal et al. The authors responded by restructuring the paper, adding an illustrative example in the appendix, and including new experiments comparing with IBP and CROWN-IBP, showing improved results. They also addressed the concern about ambiguous ReLUs not necessarily needing to be eliminated, which is a good point. However, there's still a question about whether the theoretical analysis is sufficiently clear, even after revisions.\n\nReviewer 2 was confused about the comparison between the original problem and its convex relaxation, and whether the regularizer's mechanism is justified. The authors clarified that their approach isn't a new relaxation method but a way to improve existing ones. They added more experimental results, including state-of-the-art accuracies on MNIST and CIFAR10. However, the reviewer's concern about the unclear theoretical justification might still linger, even with the added example. The authors' response about the feasibility of δ₀* being within the norm ball is a valid counterpoint, but Reviewer 2's confusion suggests the explanation might not be fully intuitive.\n\nReviewer 3 criticized the use of MNIST and presentation issues. The authors added CIFAR10 results, which strengthens the empirical validation. They also fixed some typos. The dataset concern is partially addressed, but more datasets would have been better. Still, CIFAR10 is a step up from MNIST in complexity.\n\nThe authors' response was thorough. They restructured the paper, added experiments, and provided theoretical clarifications. Their new results showing SOTA on CIFAR10 are compelling. However, some theoretical aspects remain complex, and the reviewers' initial confusion might indicate that the paper's clarity, even after revisions, could be a hurdle for readers. The technical depth is there, but the presentation might still be challenging for those not deeply familiar with the specific convex relaxation methods discussed.\n\nConsidering the strengths: The proposed regularizers do improve certified robustness, as shown in experiments. The addition of CIFAR10 results and comparisons with strong baselines like CROWN-IBP adds credibility. The weaknesses include some remaining clarity issues in the theoretical sections and limited datasets. However, the empirical results are strong enough for a top-tier conference, especially since the authors addressed the major concerns through revisions. The paper's contribution of a novel regularization approach that enhances existing methods is valuable. While not perfect, the revisions and responses likely push it towards acceptance.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper addresses a significant challenge in certified robustness by proposing novel regularizers to tighten the gap between empirical and certifiable robustness. While initial reviews raised valid concerns about theoretical clarity, experimental baselines, and dataset scope, the authors made substantial revisions:  \n1. **Theoretical Improvements:** They clarified the relationship between convex relaxations (e.g., Fast-Lin vs. optimal layer-wise relaxations) and added an illustrative example demonstrating that tight bounds can coexist with ambiguous ReLUs, addressing Reviewer 1’s concerns about overly restrictive assumptions.  \n2. **Empirical Strengthening:** The authors expanded experiments to include state-of-the-art baselines (e.g., CROWN-IBP, IBP) and achieved SOTA certified accuracies on CIFAR-10 (e.g., 70.2% at ε=2/255), directly responding to Reviewer 1 and 2’s requests for stronger comparisons.  \n3. **Clarity and Scope:** While the theoretical sections remain complex, the authors restructured the paper and added appendices to improve readability. Reviewer 3’s dataset concern was partially mitigated by adding CIFAR-10 results, though more datasets would strengthen generality.  \n\n**Remaining Weaknesses:**  \n- The theoretical discussion, though improved, may still challenge non-expert readers.  \n- Limited to MNIST and CIFAR-10; broader evaluation would enhance impact.  \n\n**Conclusion:**  \nThe paper’s empirical advancements and responsiveness to critiques justify acceptance. Its core contribution—a practical method to improve certified robustness via regularization—aligns with top-tier conference standards despite minor limitations."
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper is about generating singing voices without using pre-assigned scores or lyrics, which is a novel approach compared to traditional singing voice synthesis. They use GANs and propose three different generation schemes. The authors also developed their own dataset and evaluation metrics.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 is concerned about the evaluation metrics not capturing the essence of singing quality. They mention that vocalness and average pitch might not be sufficient. Reviewer 2 points out the lack of motivation for the three tasks and the absence of comparisons with existing methods. They also question the dataset's clarity and suggest using MIR-1k. Reviewer 3 criticizes the evaluation methods, especially the use of CREPE for pitch tracking and the vocalness metric. Reviewer 4 highlights issues with justifying the architecture choices and the lack of baselines. Reviewer 5 suggests comparing with MIDI-based methods and questions the choice of working with raw waveforms.\n\nThe authors responded by adding two baseline methods (Sinsy and Synthesizer V), revising their evaluation metrics, clarifying the dataset, and expanding the literature review. They also addressed specific technical concerns, like modifying the vocalness calculation and including more experimental results.\n\nHowever, despite these revisions, the core issues remain. The evaluation metrics, even after adjustments, might still not fully capture the quality of singing. The comparison with baselines is a step forward, but the paper still lacks a strong justification for why their approach is better than MIDI-based methods. The dataset clarification helps, but the reliance on in-house data and source separation might introduce biases. The motivation for the three tasks is somewhat addressed but not thoroughly convincing.\n\nThe paper's novelty in tackling a new problem is a strength, but the execution and validation are lacking. Top-tier conferences require not just novelty but also rigorous validation and clear superiority over existing approaches. The authors made efforts to improve, but the reviewers' concerns about evaluation, baselines, and motivation still leave significant doubts about the paper's contribution and validity.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nWhile the paper introduces a novel task of score/lyrics-free singing voice generation and demonstrates technical effort in pipeline design, several critical issues remain unresolved despite author revisions:  \n\n1. **Evaluation Insufficiency:** The proposed metrics (Vocalness, Matchness) and subjective MOS studies lack convincing validation for assessing singing quality. Even with revised metrics, concerns persist about their ability to capture musically meaningful aspects of singing (e.g., melodic coherence, expressiveness). Comparisons to baselines (Sinsy, Synthesizer V) are added but inadequately contextualized—these systems are designed for score-conditioned synthesis, making the comparison apples-to-oranges.  \n\n2. **Motivation & Task Justification:** The three proposed generation schemes (free, accompanied, solo) are not sufficiently motivated. The paper fails to articulate why these specific settings are meaningful or how they advance the field beyond incremental engineering. The \"solo singer\" task, which generates chords first, partially contradicts the core premise of being score-free.  \n\n3. **Architectural Justification:** Key design choices (e.g., BEGAN adaptation, GRU/dilated convolutions) lack ablation studies or rigorous comparison to alternatives (e.g., PatchGAN). Reviewer 4’s criticism about hand-wavy arguments remains valid despite added appendix experiments.  \n\n4. **Dataset Limitations:** Heavy reliance on in-house Jazz/piano data and source separation introduces domain bias. The authors’ dismissal of MIR-1k (due to piano scarcity) is unconvincing, as alternative conditioning strategies could have been explored.  \n\n5. **Missing Baselines:** No comparison to MIDI-based melody generation followed by vocoder synthesis—a natural baseline for unconditional singing generation—undermines claims of superiority for the raw waveform approach.  \n\nWhile the work is pioneering, its contributions are overshadowed by methodological gaps and insufficient validation. Top-tier conferences demand both novelty and rigor; here, the latter falls short."
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about improving extreme classification using adversarial softmax approximation. The main idea is using an adversarial model (a decision tree) to sample hard negatives more efficiently, which reduces gradient variance and speeds up training. The reviewers have mixed but mostly positive feedback. \n\nReviewer 1 is positive, finds the paper educative and recommends acceptance. Reviewer 2 points out some related work that's missing, like [3] and [4], which use non-uniform sampling but in a different context (sampled softmax vs. negative sampling). They also mention experiments could compare with full softmax on smaller datasets. Reviewer 3 is more critical, noting missing SOTA comparisons (like Slice, DiSMEC) and lack of evaluation on tail labels and multi-label settings. The authors responded by clarifying the distinction between their method and sampled softmax, adding some references, and agreeing to run more experiments on EURLex.\n\nHmm. The key strengths here are the theoretical contribution (Theorem 2) which formalizes the intuition about hard negatives, and the practical efficiency gains shown. The adversarial sampling method is log(C) cost, which is a big deal for extreme classification. The experiments show significant speed improvements over baselines. However, the weaknesses are the limited experimental validation compared to recent SOTA methods and not addressing multi-label scenarios. \n\nThe authors argue that their focus is on negative sampling, not other methods like DiSMEC, which is a valid point if their contribution is specifically within the negative sampling approach. But Reviewer 3 is right that comparing to SOTA methods even outside negative sampling could show where the method stands overall. The authors did agree to add EURLex experiments but didn't have results by the rebuttal. \n\nThe theoretical contribution seems solid. The rebuttal clarifies the difference between their approach and prior work, especially [3] and [4], which is important. They also address the confusion between negative sampling and sampled softmax, which might have been a misunderstanding by the reviewer. \n\nHowever, the lack of comparison with methods like Slice and DiSMEC is a concern. If those methods are significantly better in accuracy, even if slower, the paper's contribution might be more about efficiency than overall performance. But the authors do show their method is faster, which is valuable. \n\nThe authors also mention they'll add EURLex results, which would help show the approximation gap. But since those aren't in the current submission, does that weaken the paper? Maybe, but the core results on the two large datasets are still there. \n\nConsidering the theoretical contribution, the practical efficiency gains, and the clear rebuttal addressing some concerns, I think the paper has merit. However, the experimental section could be stronger with more comparisons. But given that the focus is on improving negative sampling rather than beating all SOTA methods, and the theoretical analysis is novel, it might be acceptable. The reviewers' concerns were partially addressed, and the authors made efforts to include more references and clarify their contributions. \n\nFinal decision: Accept, but maybe with a recommendation to include those experiments in the final version. Since the conference process usually allows for camera-ready revisions, and the authors have committed to adding some experiments, it's reasonable to accept.\n</think>\n\n**Reasoning:**\n\nThe paper presents a novel adversarial sampling method for extreme classification, offering theoretical contributions and practical efficiency gains. Here's the analysis:\n\n1. **Theoretical Contribution**: The paper's main strength is Theorem 2, which rigorously formalizes the intuition that adversarial (hard) negative sampling optimizes the gradient signal-to-noise ratio. This is a significant theoretical advancement over prior heuristic approaches to negative sampling.\n\n2. **Methodological Clarity**: The adversarial sampling mechanism is simple, efficient (logarithmic cost in classes), and deterministic, requiring minimal hyperparameter tuning. The distinction between \"negative sampling\" (unbiased) and \"sampled softmax\" (biased) is clarified in the rebuttal, addressing concerns about prior work ([3], [4]).\n\n3. **Experimental Results**: \n   - The experiments demonstrate a 10x speedup over baselines on large datasets (Wikipedia-500K, Amazon-670K), validating practical utility.\n   - However, comparisons to SOTA methods like Slice and DiSMEC are missing. While the authors argue their focus is on negative sampling (not full softmax alternatives), omitting these benchmarks limits the broader impact assessment.\n   - The lack of tail-label evaluation (e.g., Macro-F1) and multi-label experiments weakens the analysis of real-world applicability.\n\n4. **Rebuttal Response**: \n   - The authors addressed key concerns by clarifying distinctions from prior work, fixing typos, and committing to EURLex experiments (though results were pending). \n   - They defended their scope (single-label, negative sampling) but acknowledged the need for broader related work discussion.\n\n5. **Reviewer Consensus**: \n   - Reviewer 1 strongly supports acceptance for clarity and technical merit.\n   - Reviewer 2 highlights missing comparisons but acknowledges contributions.\n   - Reviewer 3 critiques incomplete experiments but recognizes the method’s novelty.\n\n**Conclusion**: While the experimental validation could be more comprehensive, the paper’s theoretical rigor, efficiency gains, and rebuttal adjustments justify acceptance. The authors’ commitment to additional experiments (e.g., EURLex) in the final version mitigates some concerns. The work advances the understanding of negative sampling in extreme classification and offers a practical, theoretically grounded solution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes MuPPET, a method to speed up CNN training by switching between different fixed-point precisions based on gradient diversity. The main idea is to use lower precision when possible to save computation time and energy, then switch to higher precision when needed to maintain accuracy. The authors claim a 1.28x speedup on average without loss in accuracy compared to full-precision training.\n\nReviewer 1 found the paper interesting but had concerns about the motivation behind the gradient diversity metric and the switching mechanism. They felt the explanation was heuristic and wanted more details on why co-aligned gradients indicate it's time to switch. Also, there were some typos and questions about the AlexNet justification.\n\nReviewer 2 acknowledged the novel approach but pointed out several issues. They mentioned that the method doesn't actually choose the quantization level dynamically but has fixed levels. The setting of precision levels (8-, 12-, 14-, 16-bit) was confusing, and the presentation of the switching policy was unclear. The results weren't significantly better than existing methods, and training/validation curves were missing. There were also minor issues like typos.\n\nReviewer 3 recommended weak reject because the dynamic switching wasn't shown to be necessary. They questioned the impact of gradient diversity and the parameters used. The quantization details were unclear, and some parts of the paper needed better explanation. Minor issues included readability of figures and unclear terms.\n\nThe authors responded by clarifying that the core contribution is the adaptive switching for runtime benefits, not that dynamic switching is necessary. They added experiments showing MuPPET outperforms random or borrowed switching points. They provided more details on quantization, addressed hyperparameter choices, and included training/validation curves in the appendix. They fixed typos and clarified sections as per the reviewers' suggestions.\n\nNow, evaluating the paper's strengths: The idea of using multiple fixed-point precisions and a dynamic switching mechanism based on gradient diversity is novel. The approach is shown to work across different networks and datasets, which is a plus. The speedup achieved without accuracy loss is meaningful, especially for practical applications. The authors addressed most of the reviewers' concerns by adding clarifications, experiments, and figures.\n\nWeaknesses: The initial submission had significant clarity issues, especially around the switching mechanism and quantization details. Even after revisions, some aspects might still be a bit heuristic. The improvement over existing methods (like mixed-precision training) isn't groundbreaking, though the 1.28x speedup is non-trivial. The necessity of dynamic switching versus a fixed schedule is still somewhat in question, though the authors added experiments to support it.\n\nThe reviewers' concerns were mostly addressed, but some doubts remain. For a top-tier conference, the paper needs to have both technical soundness and clear novelty/impact. The method works, but the theoretical underpinnings of the switching mechanism could be stronger. However, the practical results and the thorough revisions might tip the balance towards acceptance, especially given the potential for real-world applications in efficient training.\n</think>\n\n**Reasoning:**\n\nThe paper presents MuPPET, a multi-precision training strategy that dynamically switches between fixed-point precisions to accelerate CNN training while maintaining accuracy. The approach is novel in leveraging gradient diversity across epochs to determine precision transitions, achieving a 1.28× average speedup on ImageNet-trained models. However, reviewers raised valid concerns about clarity, justification of the switching mechanism, and comparative performance, which the authors addressed in their response.\n\n**Strengths:**\n1. **Novelty:** MuPPET advances quantized training by incorporating multiple fixed-point precisions and a dynamic switching mechanism, extending beyond existing mixed-precision methods (e.g., FP16/FP32). This addresses an under-explored area in low-precision training.\n2. **Practical Impact:** The method achieves speedups on modern GPUs without accuracy loss, which is valuable for real-world applications. The integration with hardware-level capabilities (e.g., NVIDIA Turing’s 8-bit support) strengthens its relevance.\n3. **Empirical Validation:** Experiments on AlexNet, ResNet18, and GoogLeNet demonstrate generalizability. The authors added ablation studies (e.g., comparing MuPPET to fixed schedules) to justify the necessity of dynamic switching.\n4. **Revisions:** The authors addressed most concerns: clarifying the gradient diversity rationale, quantization details, and hyperparameter choices; adding training/validation curves; and improving readability.\n\n**Weaknesses:**\n1. **Theoretical Justification:** While the gradient diversity metric is motivated by prior work, the connection to precision switching remains somewhat heuristic. The argument that co-aligned gradients indicate quantization-induced information loss is plausible but not rigorously proven.\n2. **Incremental Improvement:** The 1.28× speedup, while meaningful, is modest compared to state-of-the-art mixed-precision training. The gains stem from extreme quantization (8-bit fixed-point), but the practical benefit depends on hardware support.\n3. **Clarity Issues:** Despite revisions, some algorithmic details (e.g., the role of hyperparameters like γ) remain under-explained. The fixed progression of precision levels (8→12→14→16-bit) is empirically justified but lacks a theoretical basis.\n\n**Conclusion:**\nThe paper’s strengths—novelty, practical relevance, and thorough revisions—outweigh its weaknesses. While the theoretical grounding of the switching mechanism could be deeper, the empirical results and hardware-aware design justify acceptance at a top-tier conference. The work opens avenues for efficient training with extreme quantization, aligning with the community’s focus on scalable deep learning.\n\n**Final Decision:** Accept"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes NORML, a meta-learning approach using an LSTM to optimize neuron-wise updates. The main claims are around scalability and effectiveness compared to existing methods like MAML and the Meta-Learner LSTM by Ravi & Larochelle.\n\nReviewer 1 points out that the comparison with Ravi & Larochelle's work is flawed. They mention that the authors incorrectly claim that Ravi & Larochelle's method requires a massive number of parameters, but actually, they use parameter sharing, so their meta-learner's parameters don't scale with the learner's size. The reviewer also notes that the experiments don't properly compare with Sun et al., and the claims about handling many inner-loop steps aren't validated since the experiments use few steps.\n\nReviewer 2 criticizes the writing quality, pointing out grammatical errors and unclear sections. They also question the technical novelty and some of the theoretical claims, like the explanation of MAML's limitations. The reviewer seems to think the paper's presentation is below the standard for top conferences.\n\nReviewer 3 highlights limited novelty, missing baselines, and inaccurate descriptions of prior work. They mention that the comparison with Meta-Learner LSTM isn't fair because of different architectures and pretraining. Also, the method's applicability is limited to fully connected networks, which isn't common in few-shot learning. The reviewer also points out inaccuracies in how prior works are described, like the use of separate LSTMs per parameter in Andrychowicz et al., which isn't the case.\n\nThe authors didn't provide a response, so there's no rebuttal to these points. \n\nLooking at the key issues: \n\n1. **Novelty and Contribution**: The main contribution seems incremental. The approach modifies existing LSTM-based meta-learners but doesn't clearly demonstrate a significant improvement over them. The reviewers argue that the comparison with Ravi & Larochelle isn't fair, and the technical novelty is minimal.\n\n2. **Technical Validity**: The experiments are criticized for not comparing properly with existing methods (like Sun et al. and Meta-Learner LSTM under the same conditions). The claims about handling more inner-loop steps aren't backed by experiments. Also, the method's limitation to fully connected layers reduces its applicability.\n\n3. **Writing and Clarity**: The paper has grammatical errors and unclear explanations, which Reviewer 2 finds problematic for a top-tier conference. Incorrect citations and misstatements about prior work (like the parameter scaling in Ravi & Larochelle) further weaken the paper's credibility.\n\n4. **Reproducibility and Rigor**: Missing confidence intervals in results, lack of comparison with key baselines (MAML++), and potential inaccuracies in methodology (like separate LSTMs per layer) raise concerns about the rigor of the experiments.\n\nGiven that all three reviewers have significant concerns, especially regarding the novelty, technical validity, and presentation, and the authors didn't address these in a response, the paper likely doesn't meet the high standards of a top-tier conference. The issues with experimental comparisons and the limited scope of the method's applicability are major drawbacks. Even though the idea might have some merit, the current presentation and validation aren't sufficient for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces NORML, a novel meta-learning framework using an LSTM for neuron-wise optimization. While the idea of addressing scalability in meta-learning is relevant, the submission suffers from critical flaws that undermine its suitability for a top-tier conference:  \n\n1. **Insufficient Novelty and Misrepresentation of Prior Work**:  \n   - The technical contribution is incremental compared to Ravi & Larochelle’s Meta-Learner LSTM. The authors incorrectly claim that prior work requires \"an enormous number of parameters,\" whereas Ravi & Larochelle use parameter sharing to keep the meta-learner’s size constant. This misrepresentation weakens the paper’s motivation.  \n   - The proposed method’s distinction (e.g., layer-wise LSTMs) is not clearly justified against existing approaches, and the claimed advantages (e.g., scalability) lack empirical validation.  \n\n2. **Weak Experimental Validation**:  \n   - **Missing Baselines**: Key comparisons with Meta-Learner LSTM and MAML++ are absent. For instance, experiments on Mini-ImageNet do not compare with Sun et al.’s results under equivalent settings, and Omniglot experiments use non-standard architectures (fully connected networks instead of convolutional ones).  \n   - **Unsupported Claims**: The assertion that NORML handles long inner-loop updates better than MAML is untested, as experiments use few update steps.  \n   - **Limited Applicability**: Restricting NORML to fully connected networks reduces its practical relevance, as few-shot learning typically relies on convolutional architectures.  \n\n3. **Technical and Presentation Issues**:  \n   - The paper contains grammatical errors, citation inaccuracies (e.g., Rajeswaran 2019), and unclear explanations (e.g., ambiguous claims about MAML’s limitations). These issues hinder readability and professionalism.  \n   - Theoretical claims (e.g., gradient flow through inner loops) lack formal analysis or ablation studies.  \n\n4. **Lack of Author Response**:  \n   - The absence of a rebuttal leaves critical reviewer concerns unaddressed, such as clarifying novelty, fixing experimental comparisons, and correcting misstatements about prior work.  \n\nWhile the scalability focus is promising, the paper’s technical shortcomings, weak empirical validation, and presentation flaws make it unsuitable for acceptance at a top-tier venue. Significant revisions and rigorous comparisons are needed to establish its contributions."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes using SGD to train GMMs in high-dimensional spaces, which is a classic problem. They introduce three main ideas: an upper bound on the log-likelihood for numerical stability, a regularizer to avoid bad local minima, and a method to enforce constraints in SGD. They also have a TensorFlow implementation and experiments on image datasets.\n\nReviewer 1 thinks the work is incremental and questions the authors' understanding of online EM. They criticize the numerical stability approach, the regularization method, and the experimental evaluation, especially the datasets not being high-dimensional enough. Reviewer 2 has concerns about the novelty, technical details (like covariance matrix constraints), and the experimental setup's clarity. They also mention missing comparisons with EM and K-means. Reviewer 3 finds the work serious but notes that the contributions are limited, especially with diagonal covariances, and the practical advantages over EM aren't clear.\n\nThe authors responded by clarifying their focus on online settings where data isn't pre-available, hence K-means isn't feasible. They addressed some technical points, like renaming \"regularization\" to \"annealing\" and explaining the covariance structure. However, they didn't fully resolve all issues, like the experimental comparisons and the handling of high-dimensional data.\n\nNow, considering the reviews and responses: The main issues are the incremental nature of the work, lack of strong empirical results, and unclear advantages over existing methods. The authors' focus on online learning is a valid point, but the experiments don't convincingly show superiority in that scenario. The technical contributions, like the max-component approximation, are questioned for their necessity compared to existing tricks. The regularization/annealing method is novel but not thoroughly justified. The experiments on datasets like MNIST (28x28) might not be high-dimensional enough, and SVHN (32x32x3) is better but still not extremely high-dimensional. The lack of runtime comparisons with EM is a problem, as the main claimed advantage of SGD is efficiency. Also, the covariance structure experiments are limited to diagonal matrices, which is a simpler case.\n\nFor a top-tier conference, the paper needs strong novelty, technical soundness, and empirical validation. While the authors addressed some points, the remaining issues—especially the incremental contribution and insufficient experiments—make it hard to justify acceptance. The work might be more suitable for a workshop or a lower-tier venue where incremental but practical contributions are valued.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper proposes an SGD-based approach for training GMMs in high-dimensional spaces, addressing numerical stability, regularization, and covariance constraints. While the work is technically sound and addresses a practical challenge, it falls short of the standards for a top-tier conference due to several critical weaknesses:\n\n1. **Incremental Contribution and Novelty:**  \n   - The core ideas (e.g., max-component approximation, annealing-based regularization) are either insufficiently differentiated from existing techniques (e.g., log-sum-exp tricks, online EM) or lack theoretical justification. For instance, the max-component approximation is argued to improve numerical stability, but reviewers noted that standard log-domain techniques could achieve similar stability without sacrificing the full likelihood. The regularization/annealing method, while effective empirically, is heuristic and lacks formal analysis or comparison to hierarchical priors.  \n   - The connection to SOMs (Appendix B) is underdeveloped and does not clearly advance the understanding of GMM training.\n\n2. **Technical Concerns:**  \n   - **Covariance Modeling:** The proposed low-rank covariance approximation (Section 3.4) is inadequately explained and validated. Experiments focus on diagonal covariances, which are computationally trivial, and fail to demonstrate the utility of the principal directions approach in realistic scenarios.  \n   - **Gradient Stability and Optimization:** The non-differentiability of the max operation (Section 3.1) and the ad-hoc annealing schedule (Section 3.3) raise questions about optimization robustness. While the authors argue these are manageable (e.g., akin to ReLU non-differentiability), no formal guarantees or ablation studies are provided.  \n\n3. **Weak Empirical Evaluation:**  \n   - **Datasets:** The experiments use MNIST (784D) and SVHN (3072D), but reviewers rightly questioned whether these qualify as \"very high-dimensional\" in the context of GMMs, where even 100D is challenging. No experiments on truly high-dimensional data (e.g., >10k dimensions) or non-image domains are shown.  \n   - **Baselines:** Comparisons to EM+K-means initialization are incomplete. The authors emphasize online settings but do not evaluate streaming or non-stationary data scenarios. Runtime/memory efficiency claims (a key SGD advantage) lack empirical validation.  \n   - **Reproducibility:** Critical details (e.g., test vs. train log-likelihood, hyperparameters, dataset splits) are missing or unclear, undermining confidence in the results.\n\n4. **Author Responses:**  \n   While the authors addressed some concerns (e.g., clarifying the annealing analogy, fixing notation), they did not fully resolve major issues:  \n   - No new experiments were added to validate the principal directions method or high-dimensional performance.  \n   - The online learning motivation remains speculative without empirical support (e.g., streaming data experiments).  \n   - Theoretical gaps (e.g., convergence guarantees, gradient analysis at degenerate solutions) persist.  \n\n**Conclusion:**  \nThe paper presents a practical implementation of SGD for GMMs but does not meet the bar for a top-tier conference. The contributions are incremental, the technical claims lack sufficient theoretical or empirical support, and the evaluation is incomplete. While the work could be valuable as a tutorial or engineering contribution, it does not advance the state of the art in a meaningful way."
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a new method called MALCOM for out-of-distribution (OOD) detection using the Normalized Compression Distance (NCD). The key claims are that MALCOM doesn't require OOD samples for validation or model retraining, making it more practical. The reviewers have a mix of concerns and some positive feedback.\n\nReviewer 1's main issues are about the comparison with prior work, specifically the Mahalanobis method. They argue that the authors didn't compare against the version of Mahalanobis that uses adversarial samples for validation, which would fit the same constraints. The authors responded by adding experiments with MALCOM++, a variant using weighted sums validated on adversarial samples, which outperformed Mahalanobis. However, Reviewer 1 still questions if the main contribution (NCD) is effective or if it's just minor tweaks. The authors provided an ablation study in the appendix showing that their method works better when combining global average pooling and their new approach, especially in lower layers. But Reviewer 1 remains unconvinced, pointing out that the performance gains might come from incorrect implementations of prior work.\n\nReviewer 2 is concerned about the strict constraints (no OOD samples, no retraining) and whether they're justified. They want comparisons with methods that don't have these constraints to show the method's relative standing. The authors added comparisons in the appendix (Tables 4 and 5) where MALCOM++ performs well even when OOD samples are available. However, Reviewer 2 still feels the justification for the constraints isn't strong enough and that the problem's importance isn't clearly established. The authors argue that using OOD samples for validation is impractical since you can't know the test distribution, but Reviewer 2 might not find this fully convincing without real-world use cases.\n\nReviewer 3 questions the significance and theoretical guarantees. They mention the lack of experiments in complex, safety-critical scenarios like autonomous driving. The authors acknowledge this as future work but didn't add such experiments. Also, the theoretical foundation of NCD's effectiveness isn't deeply explored, which leaves some doubts about the method's robustness.\n\nThe authors made efforts to address the reviews by adding experiments, ablation studies, and clarifying their constraints. However, key issues remain:\n\n1. **Comparison with Prior Work**: Even with the new experiments, there's skepticism about whether the Mahalanobis method was correctly implemented. If the original Mahalanobis results are much higher, the gains from MALCOM might be due to implementation flaws rather than true innovation. The authors included original vs. replicated numbers in the appendix, but this needs to be clear in the main paper to avoid misleading comparisons.\n\n2. **Justification of Constraints**: While the authors argue that using OOD samples for validation is impractical, they don't provide strong real-world scenarios where both constraints (no OOD samples and no retraining) are absolute. Without this, the problem's relevance is still in question.\n\n3. **Ablation and Component Effectiveness**: The ablation study is in the appendix, which Reviewer 1 points out as a weakness. The main paper should highlight these results to show the contribution of each component. The authors mention that combining their method with global average pooling is key, but the core innovation's standalone effectiveness isn't clear.\n\n4. **Theoretical Underpinnings**: The paper lacks a theoretical analysis of why NCD works better for OOD detection. Without this, the method's reliability in diverse scenarios is uncertain, especially as Reviewer 3 notes the absence of guarantees.\n\n5. **Writing and Presentation**: Reviewer 2 mentions grammatical issues and missing citations. The authors addressed some, but unclear if all are fixed. The first sentence of the abstract is still awkward, which could affect readability.\n\nConsidering the strengths: the method is novel, the problem is relevant to AI safety, and the authors did extensive experiments. However, the weaknesses in comparison fairness, justification of constraints, and theoretical depth are significant. Top-tier conferences require both technical soundness and clear contributions. The reviewers' concerns, especially about the comparison with prior work and the method's core effectiveness, suggest that the paper isn't yet ready for acceptance. The additional experiments in the appendix are a step forward, but key issues remain unresolved or not sufficiently highlighted in the main text. Therefore, a weak reject seems appropriate, allowing the authors to address these points more thoroughly in a revision.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel approach for out-of-distribution (OOD) detection using the Normalized Compression Distance (NCD) and addresses a practical problem setting where no OOD validation data or model retraining is allowed. While the idea is innovative and the authors have conducted extensive experiments, several critical issues raised by reviewers remain unresolved, making the paper unsuitable for acceptance at a top-tier conference in its current form:\n\n1. **Unfair Comparisons and Implementation Concerns**:  \n   - Reviewer 1 highlights that the comparison with the Mahalanobis method is flawed. The authors compared their method to a suboptimal version of Mahalanobis (vanilla or concatenated features) rather than the adversarial-validated variant, which adheres to the same constraints. Although the authors added experiments with MALCOM++ (a variant using adversarial validation), these results are relegated to the appendix, and the main paper’s comparison remains incomplete.  \n   - The replication of prior work’s results (e.g., Mahalanobis) shows significantly lower performance than originally reported, raising concerns about implementation validity. While the authors acknowledge this, the main paper does not transparently juxtapose original vs. replicated results, weakening the credibility of their claims.\n\n2. **Weak Justification for Constraints**:  \n   - Reviewer 2 questions the necessity of the strict constraints (no OOD validation, no retraining). The authors argue that OOD validation assumes prior knowledge of the test distribution, which is impractical. However, they fail to provide concrete use cases where both constraints are simultaneously critical. Comparisons with methods that relax these constraints (e.g., using adversarial samples for validation) are buried in appendices, diminishing the paper’s impact.\n\n3. **Limited Ablation and Component Analysis**:  \n   - The effectiveness of the proposed NCD-based pooling is not conclusively demonstrated. Reviewer 1 notes that performance gains might stem from minor tweaks (e.g., concatenation) rather than the core NCD mechanism. The ablation study in Appendix C is insufficiently highlighted in the main text, leaving doubts about the method’s novelty.\n\n4. **Theoretical and Practical Gaps**:  \n   - Reviewer 3 critiques the lack of theoretical guarantees and experiments in safety-critical scenarios (e.g., autonomous driving). The authors defer this to future work, but the omission weakens the paper’s relevance to real-world applications. Additionally, the connection between NCD and OOD detection lacks rigorous theoretical grounding.\n\n5. **Presentation Issues**:  \n   - Grammatical errors (e.g., the abstract’s first sentence) and missing citations (e.g., foundational work on compression in image tasks) persist, affecting readability and scholarly rigor.\n\n**Conclusion**:  \nWhile the paper introduces a promising idea, the unresolved methodological concerns, incomplete comparisons, and insufficient justification for constraints outweigh its strengths. The work requires significant revisions to address these issues before being suitable for publication at a top-tier venue."
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me start by going through all the information provided. The paper in question is about the effect of activation functions on training overparametrized neural networks. The main contribution seems to be a theoretical analysis distinguishing between smooth and non-smooth activations, particularly focusing on the eigenvalues of the Gram matrix (G-matrix) which affects training convergence. \n\nReviewer 1 points out strengths like the theoretical analysis and experiments but criticizes the appendix length and some clarity issues. The authors responded by reorganizing the appendix, adding a table of contents, and addressing minor points. Reviewer 2 recommends acceptance, praising the thorough theoretical study and clear writing, with suggestions to improve theorem references and appendix organization. The authors addressed these by adjusting theorem statements and appendix structure.\n\nThe key points to consider are the paper's contribution to the field. The theoretical distinction between activation types and their impact on training dynamics is significant, especially for understanding why certain activations like ReLU perform well. The empirical validation on synthetic data supports the theory, though the CIFAR10 results have gaps, which the authors acknowledge. \n\nThe weaknesses mentioned, like the lengthy appendix, seem mitigated by the authors' revisions. The theoretical depth and the extension to various activation functions add value. The paper addresses a gap in understanding activation functions' roles, which is important for both theory and practice. \n\nPotential concerns might be the practical implications given the synthetic experiments and the gap in real data (CIFAR10). However, the theoretical contributions are solid and the paper structure has been improved based on reviews. Both reviewers lean towards acceptance after revisions, and the authors have addressed the raised issues adequately.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper provides a significant theoretical contribution by rigorously analyzing how activation functions influence training dynamics in overparametrized neural networks. It distinguishes between smooth and non-smooth activations, linking their properties to the eigenvalues of the Gram matrix and convergence rates. This addresses a critical gap in understanding why certain activations (e.g., ReLU) perform well empirically, while others (e.g., tanh) may struggle under specific data conditions.  \n\n**Strengths:**  \n1. **Theoretical Rigor:** The analysis is thorough, extending prior work by providing quantitative bounds for eigenvalues of the Gram matrix and clarifying the role of activation smoothness.  \n2. **Broad Applicability:** The framework accommodates diverse activation functions (ReLU, swish, polynomials) and connects low-dimensional data structure to training limitations for smooth activations.  \n3. **Empirical Validation:** Synthetic experiments align with theoretical predictions, and the authors transparently discuss gaps in real-data experiments (CIFAR10), highlighting areas for future work.  \n4. **Revisions:** The authors addressed reviewer concerns by reorganizing the appendix, clarifying theorem statements, and improving readability, mitigating initial weaknesses.  \n\n**Weaknesses Addressed:**  \n- The lengthy appendix, initially a concern, was streamlined with a table of contents and better organization.  \n- Minor technical inaccuracies (e.g., theorem references, notation) were corrected.  \n\n**Impact:**  \nThe paper advances the theoretical foundation of deep learning by elucidating activation functions' role in training dynamics. Its insights are valuable for both theorists and practitioners, guiding activation selection and architecture design. While real-data experiments are limited, the theoretical contributions justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper's contribution. The authors are tackling an important question in deep learning: how the depth and width of ReLU networks affect their ability to represent certain functions. They connect this to Sharkovsky's Theorem from dynamical systems, which is a novel approach. That's interesting because Sharkovsky's Theorem deals with periodic points and chaos, which might offer new insights into why deeper networks can learn certain functions that shallow ones can't.\n\nThe reviewers seem mostly positive. Reviewer 2 is very enthusiastic, calling the paper a \"delight to read\" and recommends acceptance. Reviewer 3 also finds the paper serious and well-written but has some structural concerns and questions about practical applicability. Reviewer 1's comment is brief but positive, already at the maximum score. The authors addressed the reviewers' comments by restructuring the paper, adding examples, and even including experiments on a synthetic dataset to show empirical results. That's a good sign because they took the feedback seriously.\n\nLooking at the author response, they restructured the paper to present main results earlier, which addresses Reviewer 3's concern about the structure. They also added intuition for definitions and discussed the practical implications, pointing to real-world examples like the 3-body problem. The experiments on synthetic data with classification error vs. depth plots help ground the theoretical results, which Reviewer 2 requested. The code availability adds transparency.\n\nPotential weaknesses: The practical application part is still a bit hand-wavy. Reviewer 3's point about how to assess the period for a given task isn't fully resolved, but the authors acknowledge it's a hard problem. However, they provide examples where such analysis might be relevant, which is a reasonable compromise for a theoretical paper. The brittleness of periodicity to bias terms is discussed, showing they considered model variations.\n\nThe theoretical contribution seems solid. Using Sharkovsky's Theorem to derive depth-width trade-offs is novel and answers an open question from Telgarsky's work. The lower bounds on depth based on periodic points add a new perspective to understanding network expressivity. The proofs are deemed correct by Reviewer 2, which is a good sign.\n\nConsidering all this, the paper makes a meaningful theoretical contribution with a novel approach, addresses reviewer concerns adequately, and provides both theoretical and some empirical validation. The concerns raised don't seem to outweigh the strengths, especially given the positive reviewer feedback and the authors' thorough responses.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThis paper makes a significant theoretical contribution by connecting Sharkovsky's Theorem from dynamical systems to depth-width trade-offs in ReLU networks, addressing an open question posed by Telgarsky. The work is novel, rigorous, and well-executed, with clear technical depth (e.g., eigenvalue analysis of dynamical systems) and a well-structured response to reviewer feedback. Key strengths include:  \n\n1. **Theoretical Insight:** The application of Sharkovsky’s Theorem provides a natural characterization of functions that require deep networks, advancing the understanding of depth separation beyond worst-case analyses. This directly addresses Telgarsky’s open question and offers a principled framework for analyzing network expressivity.  \n\n2. **Reviewer Engagement:** The authors thoughtfully addressed reviewer concerns, restructuring the paper to foreground contributions, adding synthetic experiments to illustrate depth-error trade-offs, and clarifying definitions. The inclusion of empirical results (classification error vs. depth) strengthens the practical intuition behind their bounds.  \n\n3. **Broader Relevance:** While the practical assessment of periodic points in real-world tasks remains challenging, the authors contextualize their work within applications like physical simulations (e.g., turbulent flows, the 3-body problem), suggesting plausible domains where their theory could guide architecture design.  \n\n4. **Rigor and Clarity:** Reviewers praised the clean proofs, correct technical execution, and improved exposition. The paper’s theoretical contributions are deemed \"clever\" and \"well in place,\" with the restructuring enhancing readability.  \n\nMinor limitations (e.g., brittleness of periodic points to bias terms, practical period estimation) are acknowledged but do not diminish the core contribution. The paper meets the standards of a top-tier conference by providing a novel, rigorous, and impactful analysis of neural network expressivity."
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper presents a weakly-supervised method for segmenting trajectories into skills using multiple instance learning (MIL). The core idea is to label each time-step with a skill based only on trajectory-level labels indicating which skills are present, without needing their order. The authors claim their approach works across various environments and allows zero-shot transfer to new skill combinations.\n\nReviewer 1's main concerns are about the framing vs. results. They argue that the paper claims to learn reusable skills but doesn't show how the segmented data aids in policy learning. The experiments are seen as limited, lacking comparisons to relevant baselines like behavior cloning with sub-skills. Also, the experimental setup is unclear, and there's a worry about overfitting due to few skills and possible inconsistent predictions.\n\nReviewer 2 questions the novelty, noting that log-sum-exp pooling is not new. They find the results underwhelming (35-60% accuracy) and want comparisons to fully supervised oracles. The behavioral cloning success rate (50%) is low, and baselines like Policy Sketches are missing. They suggest the contribution is minor.\n\nReviewer 3 is concerned about feasibility with many skills and dataset requirements. They point out the low classification accuracy and lack of confusion matrices or qualitative results. They also mention the need for comparisons with unsupervised clustering and theoretical bounds on dataset requirements.\n\nThe authors responded by clarifying that their method requires less supervision than Shiarlis et al. (no permutation info), addressed smoothing's minor impact, and explained that downstream tasks use segmented data to train skills. They plan to add fully supervised comparisons and clustering baselines but note that Policy Sketches uses more supervision. They argue that low accuracy is due to ambiguous timesteps and that MIL's application here is novel despite log-sum-exp's prior use.\n\nNow, considering all this: The problem is valid and challenging. Weak supervision for skill segmentation is important. However, the paper's contributions are incremental. The method applies existing MIL techniques without significant algorithmic novelty. The experiments show some promise but lack thorough comparisons. The accuracy metrics are low, and without confusion matrices or detailed qualitative analysis, it's hard to assess if the segmentation is meaningful. The zero-shot transfer is a plus but only at 50% success, which is not strong. The rebuttal addresses some points but doesn't fully resolve concerns about novelty and empirical rigor. For a top-tier conference, the paper needs more compelling evidence of effectiveness and broader impact. The reviewers' concerns about novelty, baselines, and experimental depth suggest the paper isn't ready for acceptance yet.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper addresses an important problem in robot learning by proposing a weakly-supervised method for trajectory segmentation. While the problem is well-motivated and the application of MIL to skill segmentation is sensible, the paper falls short of the standards for a top-tier conference due to several critical shortcomings:  \n\n1. **Limited Novelty:** The core technical contribution—log-sum-exp pooling for MIL—is not novel, as acknowledged by Reviewer 2. The adaptation to trajectory segmentation, while practical, lacks significant algorithmic innovation. The rebuttal does not sufficiently justify the novelty beyond prior MIL applications.  \n\n2. **Insufficient Empirical Validation:**  \n   - **Weak Baselines:** Comparisons to unsupervised clustering methods (e.g., feature-based clustering) and fully supervised oracles are missing, leaving the reader uncertain about the method’s relative performance.  \n   - **Ambiguous Metrics:** Classification accuracy (35–60%) is low and inadequately explained. The lack of confusion matrices, precision/recall metrics, or qualitative visualizations (e.g., segmentation consistency over time) makes it difficult to assess whether the model learns meaningful skill boundaries.  \n   - **Zero-Shot Transfer:** The 50% success rate in zero-shot tasks is underwhelming and lacks comparison to alternative approaches (e.g., Policy Sketches).  \n\n3. **Theoretical Gaps:** The paper does not address fundamental limitations of the weakly supervised setup, such as dataset requirements for skill disentanglement (e.g., co-occurring skills as noted by Reviewer 3). Theoretical analysis of identifiability or data diversity assumptions is absent.  \n\n4. **Rebuttal Limitations:** While the authors clarified aspects of their method (e.g., smoothing’s role, comparisons to Shiarlis et al.), they did not fully resolve concerns about novelty or provide new experiments (e.g., fully supervised baselines, clustering results) in the rebuttal.  \n\n**Conclusion:**  \nWhile the problem is compelling, the paper’s incremental technical contribution, limited empirical scope, and lack of rigorous validation hinder its suitability for a top-tier venue. Significant revisions—including deeper theoretical analysis, expanded baselines, and clearer metrics—are needed to strengthen the claims."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes a hierarchical reinforcement learning framework where multiple agents (or end-effectors) learn sub-skills independently with behavior diversification and then coordinate them. The reviewers have raised several points, and the authors have responded to them. \n\nReviewer 1 had concerns about notation clarity, the use of T_low=1, high variance in results, and the need for more discussion on multi-agent methods. The authors addressed these by revising notations, adding experiments with different T_low values, running more seeds to reduce variance, and expanding the related work section. They also added success rate plots.\n\nReviewer 2 questioned the differentiation between single and multi-agent temporal abstraction and the choice of DIAYN. The authors clarified that the focus is on coordination, not temporal abstraction per se, and explained that DIAYN was chosen for diversity but other methods could be used. They also added baselines using DIAYN without subtasks, which failed, showing the importance of predefined subtasks.\n\nReviewer 3 was concerned about the reliance on predefined subtasks and suggested a DIAYN-only baseline. The authors added these baselines, which performed poorly, reinforcing the need for prior subtask knowledge. They also clarified the latent vector size and discussed the design choice of fixed T_low.\n\nThe authors made significant revisions: adding experiments, clarifying methodology, expanding discussions, and addressing notation issues. The key strengths are the novel approach to skill coordination, empirical validation across tasks, and thorough responses to reviews. Weaknesses include the reliance on predefined subtasks, which limits generality, but the authors argue this is necessary for complex tasks. The experiments show the method's effectiveness, and the baselines support their claims.\n\nThe paper seems suitable for a top-tier conference as it presents a novel method with solid empirical results and addresses reviewer concerns adequately. The work contributes to both hierarchical RL and multi-agent coordination, which are active research areas.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel hierarchical reinforcement learning framework for coordinating diverse skills across multiple agents or end-effectors. The authors address a critical gap in multi-agent coordination by leveraging pre-trained, adaptable skills with behavior diversification, enabling efficient collaboration in complex tasks. The method is empirically validated across challenging manipulation and locomotion tasks, demonstrating superior performance over baselines. Key strengths include:\n\n1. **Technical Contribution:** The integration of skill behavior diversification with hierarchical coordination addresses a significant challenge in multi-agent systems. The modular framework is well-motivated and aligns with human-like task decomposition, offering a practical solution for complex robotic control.\n\n2. **Responsive Revisions:** The authors thoroughly addressed reviewer concerns:\n   - Added baselines (centralized/decentralized SBD) to evaluate the necessity of predefined subtasks, showing that prior knowledge is crucial for complex tasks.\n   - Clarified design choices (e.g., fixed \\( T_{low} \\)) with additional experiments, demonstrating that smaller \\( T_{low} \\) values improve flexibility.\n   - Expanded discussions on multi-agent methods, temporal abstraction, and skill discovery, strengthening the paper’s positioning in related work.\n   - Improved empirical rigor by reducing variance through more seeds, adding success rate metrics, and reporting latent embedding size.\n\n3. **Empirical Validation:** The experiments are comprehensive, spanning bimanual manipulation and multi-agent locomotion. The ablation studies (e.g., \\( T_{low} \\), diversity weight sensitivity) and failure analysis of DIAYN-only baselines provide strong evidence for the method’s efficacy.\n\n**Weaknesses Mitigated:**  \nWhile the reliance on predefined subtasks limits generality, the authors convincingly argue that this is necessary for complex tasks and show that unsupervised skill discovery (e.g., DIAYN alone) fails in their settings. The paper’s focus on coordination—rather than skill discovery—is justified, and the method’s modularity allows integration with other skill-learning approaches.\n\n**Impact and Relevance:**  \nThe work advances both hierarchical RL and multi-agent coordination, with practical implications for robotics. The code and videos enhance reproducibility. The revisions and clarifications have elevated the paper to meet top-tier conference standards, making it a valuable contribution to the field."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper is about using machine learning, specifically bidirectional LSTM RNNs, to predict DNA folding patterns (TADs) in Drosophila using epigenetic data. The authors compared their model against linear models and gradient boosting, claiming superior performance with their approach. The reviewers had several concerns, and the authors responded to them.\n\nFirst, Reviewer 1 was worried about whether the improvement was due to the LSTM or the custom weighted MSE loss. The authors clarified that all models used the same loss function, which addresses that concern. But Reviewer 1 also mentioned generalizability to other datasets like ATAC-seq. The authors responded that they focused on ChIP-seq for biological reasons and didn't test on other data, which might be a limitation but they justified it. However, this could be a point against them if the conference values broad applicability.\n\nReviewer 2 had several technical points, like clarifying equations and figure labels, which the authors addressed. They also wanted a discussion on why regression vs. neural networks. The authors expanded the discussion, explaining that NNs capture local processes better. That seems reasonable, but maybe not deeply explored. The figure corrections and equation clarifications are good, but some issues like the choice of the center hidden state in the LSTM (Reviewer 3's point 8) were answered by saying all hidden states contribute, which might not fully resolve the concern. Maybe the model could have been designed differently, but the authors defended their choice.\n\nReviewer 3 was harsher, pointing out lack of novelty, unclear methods, and insufficient evaluation. The authors responded by expanding literature review, clarifying data processing, and adding more evaluation metrics. However, the core criticism about novelty remains. They argue their work is different because they predict TAD characteristics from ChIP-seq, not Hi-C data. But if prior work exists in similar areas, even if not identical, the novelty might still be in question. The authors mention some papers were workshop abstracts, so excluding them might be okay, but if there's a lack of comparison to recent relevant work, that's a problem.\n\nAnother major point from Reviewer 3 was the comparison to non-recurrent models like CNNs. The authors didn't test CNNs, which is a gap. They focused on sequential models due to DNA's linearity, but CNNs also capture local patterns and might be a strong baseline. Not including them could weaken the evaluation.\n\nThe authors added more evaluation metrics (MSE, MAE, R²), which strengthens their results. They also addressed hyperparameter tuning details, which is good. However, the feature importance analysis using linear models but not for the LSTM (point 5) is a weakness. They mentioned using feature dropout, but more sophisticated methods like SHAP or attention weights could provide better insight. This might leave some questions about interpretability.\n\nThe authors' response to the loss function (point 7) was that they considered other metrics but found wMSE suitable. They added comparisons, which is positive. However, the choice of K=11 in the loss function was explained based on the target distribution, which seems ad-hoc but biologically motivated. It's a trade-off between domain specificity and generalizability.\n\nOverall, the paper applies a known model (biLSTM) to a specific biological problem with some methodological adjustments. The improvements over baselines are demonstrated, but the novelty is somewhat incremental. The lack of comparison to CNNs or other architectures and potential prior work in similar areas (even if not identical) might limit the perceived contribution. However, the application to TAD prediction from ChIP-seq data could still be novel enough for a top-tier conference if the execution is solid. The authors addressed many technical concerns, but the methodological novelty and thoroughness of evaluation (missing baselines) might still be sticking points. If the conference values application-focused work with solid results, it might be acceptable. But if they prioritize methodological innovation, it might fall short.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nWhile the paper presents a biologically relevant application of bidirectional LSTMs for predicting chromatin folding patterns, several critical issues remain unresolved despite the authors' revisions:  \n\n1. **Lack of Methodological Novelty:** The core approach (biLSTM) is not novel, and the authors fail to sufficiently differentiate their work from prior literature. While they argue their focus on TAD prediction from ChIP-seq (not Hi-C) is unique, the rebuttal does not convincingly address similarities to existing works (e.g., arXiv:1811.09619). The omission of non-recurrent baselines (e.g., CNNs) further weakens the technical contribution.  \n\n2. **Incomplete Evaluation:**  \n   - **Baseline Models:** The absence of comparisons to convolutional neural networks (CNNs) or attention-based models is a significant gap, as these architectures are natural candidates for capturing local/global genomic dependencies.  \n   - **Feature Importance Analysis:** The reliance on linear models for feature importance (without analogous analysis for the biLSTM) limits interpretability. Techniques like gradient-based attribution or attention visualization would strengthen claims about biological insights.  \n\n3. **Generalizability Concerns:** The authors dismiss testing on ATAC-seq or other organisms/cell types, citing biological specificity. While reasonable, this limits the broader impact of the work and raises questions about the universality of the proposed approach.  \n\n4. **Technical Clarifications:**  \n   - The justification for using the \"center hidden state\" in the biLSTM (Reviewer 3’s point 8) remains unconvincing. A more rigorous ablation study (e.g., comparing concatenated vs. center states) is needed.  \n   - The ad-hoc choice of K=11 in the loss function, while explained, lacks systematic validation (e.g., sensitivity analysis).  \n\n5. **Reproducibility:** Details about hyperparameter tuning (e.g., grid search ranges, computational resources) and data preprocessing (e.g., handling chromosome ends) remain insufficient for replication.  \n\n**Conclusion:**  \nThe paper addresses an important problem in computational biology but falls short of the methodological rigor and novelty expected at a top-tier conference. While the authors have made efforts to address reviewer concerns, unresolved issues in evaluation, baseline comparisons, and technical justification undermine the work's contribution. A more comprehensive analysis of model choices, feature importance, and generalizability would be required for acceptance."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes a hierarchical reinforcement learning framework where multiple agents (like robot arms) learn individual skills with diversity and then coordinate them. The reviewers had some concerns but overall seemed positive after the authors' responses.\n\nReviewer 1 initially had some technical concerns about notation, the meta-controller's skill selection interval (T_low=1), high variance in results, and the need for more multi-agent baselines. The authors addressed these by clarifying notations, adding experiments with different T_low values, running more seeds to reduce variance, and including more baselines like centralized and decentralized SBD. They also expanded the discussion on multi-agent methods, which probably satisfied Reviewer 1's request for more related work.\n\nReviewer 2 questioned the differentiation between multi-agent and single-agent temporal abstraction and the choice of DIAYN. The authors explained that while temporal abstraction is similar, coordination is the key challenge here. They also clarified that DIAYN was a choice for diversity but other methods could work. They added baselines showing that without predefined subtasks (using DIAYN alone), performance drops, which addresses the concern about the necessity of expert knowledge.\n\nReviewer 3 liked the approach but was concerned about predefined subtasks and the lack of a DIAYN-only baseline. The authors added those baselines, showing that their method outperforms when using predefined skills. They also clarified the role of the latent embedding size and how q(z|s) is learned, which probably resolved those technical doubts.\n\nThe authors did a thorough job addressing the reviews. They added experiments, clarified methodologies, and expanded discussions. The key strengths are the novel approach to skill coordination, the empirical validation across tasks, and the effective responses to critiques. The weaknesses include reliance on predefined subtasks, which limits generality, but the authors argue this is necessary for complex tasks. The experiments now include more baselines and analysis, which strengthens their claims.\n\nThe paper seems suitable for a top-tier conference as it presents a clear contribution with solid empirical backing and addresses reviewer concerns adequately. The approach is novel in combining skill diversity with coordination in multi-agent settings, and the results demonstrate effectiveness. The added baselines and analysis help position the work within existing literature.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel hierarchical reinforcement learning framework for coordinating diverse skills across multiple agents, addressing a significant challenge in complex manipulation tasks. The authors effectively respond to reviewer concerns through extensive revisions, including:  \n1. **Additional Baselines:** Introducing centralized/decentralized SBD baselines demonstrates the necessity of predefined subtasks for complex coordination, addressing critiques about expert knowledge reliance.  \n2. **Technical Clarifications:** Resolving ambiguities in notation, latent embedding size, and skill duration (T_low) strengthens methodological rigor. Experiments with varying T_low values (Figure 7) validate design choices.  \n3. **Empirical Robustness:** Increased seeds and success rate metrics (Figure 6) mitigate concerns about variance and evaluation consistency.  \n4. **Theoretical Context:** Expanded discussion of multi-agent vs. single-agent temporal abstraction and comparisons to prior work (e.g., DIAYN, communication-based MARL) clarify the paper’s niche.  \n\nWhile the reliance on predefined subtasks limits generality, the authors justify this as essential for complex tasks and show that unsupervised skill discovery (DIAYN-only) fails in their settings. The experiments on bimanual manipulation and multi-agent locomotion demonstrate clear improvements over baselines, with ablation studies validating key components.  \n\nThe work’s strengths—novelty in skill coordination, rigorous empirical validation, and thorough revisions—outweigh its limitations, making it suitable for a top-tier conference."
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes a multi-task learning framework that models multi-level task dependencies using attention mechanisms. The main idea is to capture both general task dependencies and data-specific dependencies, especially in hierarchical data like text and graphs. The authors claim improvements over existing methods on public datasets.\n\nLooking at the reviews, there's a mix of criticism and some positive points. Reviewer 1 points out grammar issues and the need for clearer distinction between the two dependency levels. Reviewer 2 acknowledges the novelty but wants more motivation and experimental validation, like comparing with an attention-less version. Reviewer 3 questions the claim about existing methods not handling task dependencies and raises concerns about parameter size. Reviewer 4 is critical, citing missing prior work (like Taskonomy), unclear methodology, and presentation issues.\n\nThe authors responded by clarifying their contributions over prior works, especially in handling hierarchical data and reducing parameter complexity. They addressed some technical questions and added experiments, like comparing without attention. However, some issues remain unresolved, like the thorough comparison with Taskonomy and the clarity of certain methodological details.\n\nThe key strengths are the novel approach to multi-level dependencies and the parameter reduction. The experiments show improvements, but the reviewers still have concerns about the presentation, clarity, and comparison with existing works. The rebuttal helped but didn't fully resolve all issues, especially regarding the differentiation from Taskonomy and the necessity of both dependency levels. The grammar and clarity issues were addressed, but the methodological explanations might still be lacking.\n\nConsidering the standards of a top-tier conference, the paper introduces a potentially valuable contribution but has significant presentation and comparison shortcomings. The experimental validation, while improved, might not be sufficient to conclusively demonstrate superiority over all relevant baselines. The unresolved issues with prior work comparison and methodology clarity could hinder acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel approach to multi-task learning by modeling multi-level task dependencies with attention mechanisms, addressing hierarchical data structures in text/graphs. While the concept is promising and the authors have made efforts to address reviewer concerns, several critical issues remain unresolved:\n\n1. **Prior Work Integration:**  \n   The rebuttal inadequately addresses comparisons with foundational works like Taskonomy. The distinction between \"multi-level\" dependencies and Taskonomy's task affinity remains unclear, especially in demonstrating why both general and data-specific dependencies are necessary. Without rigorous experimental comparison, the claimed superiority is unsubstantiated.\n\n2. **Methodological Clarity:**  \n   Key components (e.g., the \"Transfer Block,\" decomposition of parameters) remain under-explained. While the authors clarified some equations, the presentation still lacks precision, leaving doubts about technical soundness. For instance, the mapping functions in Eq. 2 and the role of hierarchical dependencies in reducing parameters require deeper exposition.\n\n3. **Experimental Rigor:**  \n   Although the authors added an ablation study (removing attention), the experiments primarily focus on multi-label classification. Broader validation across diverse task types (e.g., regression, structured prediction) and datasets beyond text/graphs is missing. Additionally, claims about improved data efficiency under low-resource settings are not convincingly supported by the results.\n\n4. **Presentation Issues:**  \n   While grammar improvements were made, the manuscript still suffers from organizational flaws (e.g., undefined terms like \"multi-view\" vs. \"multi-level\") and insufficient motivation for hierarchical dependencies in non-image domains. Figure 1’s purpose remains ambiguous, and notation inconsistencies (e.g., dual use of index *i*) persist.\n\n5. **Scalability Concerns:**  \n   Reviewer 3’s criticism about parameter growth with task count was partially addressed, but the rebuttal’s promise of future experiments with shared encoders does not resolve the current limitation. This raises doubts about practicality for large-scale applications.\n\n**Conclusion:**  \nWhile the work introduces a potentially valuable contribution, the unresolved issues in clarity, comparison to prior art, and experimental breadth render it unsuitable for acceptance at a top-tier venue in its current form. Significant revisions are needed to solidify the methodological foundation and validate claims against state-of-the-art baselines."
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a Differentiable Hebbian Consolidation model for continual learning, aiming to mitigate catastrophic forgetting. The key idea is using a softmax layer with both fixed (slow) and plastic (fast) weights, inspired by Hebbian learning and complementary learning systems. They tested it on several benchmarks like Permuted MNIST and introduced an imbalanced variant. The reviewers had mixed reactions.\n\nReviewer 1 points out that while the method is interesting, the empirical evaluation isn't strong enough, especially compared to existing methods like SI on Split CIFAR. The authors responded by adding experiments on Split CIFAR-10/100, showing DHP Softmax outperforms finetuning but still lags behind SI. They argue that combining DHP with SI could help, but the results here aren't conclusive. Reviewer 2 was confused about some technical details but found the approach novel. The authors clarified the methodology and added a figure in the appendix. Reviewer 3 wanted more comparisons with other CLS-based methods and more experiments. The authors addressed some of this by testing on Split CIFAR and discussing why other CLS methods weren't suitable. Reviewer 4 was more critical, noting weak results on simpler tasks and suggesting more challenging benchmarks. The authors added Split CIFAR but didn't test on larger datasets like ILSVRC or CORe50 due to time constraints.\n\nThe strengths here are the novel approach combining Hebbian learning with synaptic consolidation, the introduction of an imbalanced dataset, and the thorough responses to reviewers including additional experiments. However, the weaknesses include the incremental improvement over baselines, especially since DHP alone isn't outperforming state-of-the-art methods like SI. The empirical validation, even with the added Split CIFAR, still feels limited. The authors argue that combining DHP with other methods works best, but the gains aren't substantial enough. Also, the lack of experiments on more complex datasets is a drawback, though they explained computational limitations.\n\nConsidering this is a top-tier conference, the paper's contributions might be seen as incremental. The method is novel and well-motivated, but the empirical results don't strongly demonstrate superiority over existing approaches. While the rebuttal addressed some concerns, the core issue of marginal performance gains remains. The paper introduces a promising direction but might need more extensive validation to warrant acceptance at a top venue.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a novel approach integrating Hebbian plasticity with synaptic consolidation for continual learning, addressing catastrophic forgetting. While the idea is theoretically motivated and aligns with neurobiological principles, the empirical validation falls short of the standards expected for a top-tier conference. Key concerns include:  \n\n1. **Marginal Empirical Improvements**:  \n   - The method shows only modest gains over baselines (e.g., finetuning) and underperforms compared to state-of-the-art methods like Synaptic Intelligence (SI) on critical benchmarks (e.g., Split CIFAR). While combining DHP Softmax with SI improves results, the incremental benefit is not compelling enough to justify a significant contribution.  \n\n2. **Limited Scope of Experiments**:  \n   - Despite rebuttal efforts, evaluations remain restricted to MNIST variants and a 5-dataset vision benchmark. The authors acknowledge but do not address the lack of results on large-scale datasets (e.g., CIFAR-100, CORe50, or ILSVRC), which are standard in recent continual learning literature. The Split CIFAR-10/100 experiments added during rebuttal are a step forward but insufficient to demonstrate scalability or robustness.  \n\n3. **Weak Baseline Comparisons**:  \n   - The paper does not adequately compare against other CLS-inspired methods (e.g., Hebbian Softmax or meta-learning approaches), leaving open questions about its uniqueness. The rebuttal clarifies why some methods were excluded but does not provide empirical evidence to validate claims of superiority.  \n\n4. **Theoretical Clarity**:  \n   - While the rebuttal addresses some confusion (e.g., notation, motivation for the softmax focus), the core mechanism of how Hebbian consolidation complements synaptic methods remains underdeveloped. The intuition provided is plausible but lacks rigorous theoretical or empirical grounding.  \n\n**Conclusion**:  \nThough the paper introduces a promising direction for continual learning, the incremental empirical results, limited benchmarking, and unresolved questions about comparative performance weaken its impact. For a top-tier venue, stronger evidence of scalability, broader applicability, and clear advantages over existing methods is required."
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a defense mechanism against adversarial attacks using an encoder-assisted GAN framework. The main idea is to use an auto-encoder trained with adversarial loss to create a latent space where they can search for a natural reconstruction of the input image, thereby purifying adversarial examples. The authors claim their method is more computationally efficient than existing approaches like Defense-GAN while maintaining or improving performance.\n\nReviewers had mixed reactions. Reviewer 1 was positive, noting the method's efficiency and improved performance over Defense-GAN with fewer iterations. However, Reviewers 2-4 raised several concerns. Reviewer 2 felt the contribution was incremental, lacking comparisons with state-of-the-art methods like PGD adversarial training and TRADES, and noted issues with clarity and typos. Reviewer 3 questioned the reliance on the auto-encoder's detection capability, especially under white-box attacks, and the latent space alignment. Reviewer 4 criticized the writing quality and the limited evaluation on simple datasets (MNIST/Fashion-MNIST), suggesting the need for more complex datasets like CIFAR-10.\n\nThe authors responded by adding experiments with PGD adversarial training and CelebA, showing improved results. They addressed some concerns about detection mechanisms and provided additional analyses. However, they didn't include CIFAR-10 results yet, only CelebA, which is still not as standard as CIFAR-10 for adversarial defense benchmarks. They also tried to clarify the methodology and fixed some typos, but Reviewer 4 still found the writing unclear.\n\nStrengths: The method offers a computationally efficient alternative to Defense-GAN, validated on MNIST and Fashion-MNIST with some extension to CelebA. The encoder initialization reduces search steps, which is a practical improvement. The responses to reviewers included additional experiments, which is a good sign of addressing concerns.\n\nWeaknesses: The incremental contribution is a big issue. Defense-GAN is from 2018, and the field has advanced with methods like adversarial training. The lack of comparison with TRADES and limited dataset evaluation (missing CIFAR-10) weakens the claims. The writing and clarity issues, even after revisions, might hinder the paper's impact. The reliance on the auto-encoder's detection under white-box attacks remains a concern, despite the authors' additional experiments. The CelebA results are a step forward but not sufficient for a top-tier conference without CIFAR-10.\n\nConsidering the standards of a top-tier conference, the paper needs to demonstrate significant novelty and robust evaluations. While the efficiency gain is notable, the incremental nature over Defense-GAN and missing comparisons with the latest methods make the contribution less impactful. The writing issues and dataset limitations further reduce the paper's readiness for publication in a top venue. The authors' efforts are commendable, but the paper might need more thorough experimentation and clearer presentation to meet the high bar.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes an encoder-assisted GAN framework (AE-GAN+sr) for adversarial defense, aiming to improve computational efficiency over Defense-GAN. While the idea of leveraging an encoder for latent-space initialization is sensible and the results on MNIST/Fashion-MNIST show promise, the work falls short of the standards for a top-tier conference due to several critical issues:  \n\n1. **Incremental Contribution**: The core idea builds heavily on Defense-GAN (2018), with the primary novelty being encoder-based initialization for faster optimization. However, the field of adversarial defense has advanced significantly since 2018, and the authors fail to adequately compare with state-of-the-art methods like TRADES or recent adversarial training variants. The added comparison to PGD adversarial training is insufficient, as results on MNIST/Fashion-MNIST (low-dimensional datasets) do not generalize to modern benchmarks like CIFAR-10/ImageNet.  \n\n2. **Limited Evaluation**: Experiments on CelebA are a step forward but lack depth (e.g., no results on standard attacks like PGD/CW for CelebA). The absence of CIFAR-10 results—a standard benchmark for adversarial defense—severely undermines the claim of broad applicability.  \n\n3. **Technical Concerns**:  \n   - The reliance on the auto-encoder’s detection mechanism remains vulnerable to white-box attacks, despite the authors’ rebuttal. The defense’s robustness against adaptive attacks (e.g., BPDA) is questionable, as evidenced by the drop in accuracy under BPDA attacks.  \n   - The latent space alignment between the encoder and prior distribution is unresolved, potentially limiting generalization.  \n\n4. **Presentation Issues**: Multiple reviewers noted poor writing quality, typos, and unclear methodology descriptions. While revisions were attempted, the rebuttal did not fully resolve these issues, impacting readability and reproducibility.  \n\n5. **Obfuscated Gradients**: The method’s reliance on iterative optimization in the latent space risks falling into the \"obfuscated gradients\" trap, as highlighted by the BPDA results. This raises doubts about the defense’s true robustness.  \n\n**Conclusion**: While the efficiency gains over Defense-GAN are notable, the paper’s incremental contribution, limited evaluation scope, and unresolved technical weaknesses make it unsuitable for a top-tier venue. The authors should address the above issues, particularly by expanding evaluations to CIFAR-10/ImageNet and rigorously comparing with modern defenses, before resubmission."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The title suggests that the authors are using Monte Carlo Tree Search (MCTS) to refine MCTS agents, which sounds a bit meta. The abstract mentions improving the coordination of episode generation by treating the entire system as a game tree search. They claim their method handles the exploration-exploitation trade-off better than Alpha Zero in a small problem.\n\nNow, the reviewers' comments are pretty harsh. All three reviewers point out major issues. Reviewer 1 says the paper is \"less than half-baked,\" lacking references and poorly written. They didn't cite any previous work, which is a big red flag. Reviewer 2 and 3 both mention the writing is so bad that they couldn't understand the paper. The author response is apologetic, explaining they didn't know how to use a .bib file and removed references, which is a basic academic practice. They also mention finding a similar idea in their country but not internationally, which might indicate they didn't do a thorough literature review.\n\nThe author's response doesn't address the core issues. They apologize for the poor writing and lack of references but don't provide a plan to fix these in a revision. The fact that there are no references is a critical flaw because it shows the authors aren't situating their work within existing research. Without that context, it's impossible to evaluate the novelty or contribution.\n\nThe experiments are only on a small problem (probably tic-tac-toe), which might not demonstrate the method's effectiveness in more complex environments. The reviewers couldn't even understand the method due to the poor writing, so the technical merits are unclear. The lack of mathematical definitions and unclear algorithm descriptions add to the confusion.\n\nTop-tier conferences require clarity, rigor, and a solid foundation in existing literature. This paper fails on all fronts. Even if the idea has potential, the presentation is so lacking that it's impossible to assess its validity. The author's response doesn't suggest they can adequately revise the paper to meet these standards in the current submission cycle. Therefore, rejection seems necessary to maintain the conference's standards.\n</think>\n\n**Reasoning:**  \nThe paper presents a novel concept of using MCTS to refine MCTS-based RL agents, potentially addressing exploration-exploitation trade-offs in AlphaZero-like systems. However, critical flaws make it unsuitable for acceptance at a top-tier conference:  \n\n1. **Severe Writing and Clarity Issues:**  \n   - All reviewers highlight the paper’s incoherent writing, grammatical errors, and lack of clarity. The abstract and technical sections are described as incomprehensible, with undefined notation and poorly explained algorithms.  \n   - The author’s response acknowledges these issues but does not propose concrete fixes, suggesting the current version is irredeemably flawed.  \n\n2. **Lack of Scholarly Rigor:**  \n   - The absence of references (attributed to formatting issues) invalidates the paper’s academic foundation. No prior work is cited, making it impossible to contextualize the contribution or assess novelty.  \n   - The author’s admission of discovering a similar “meta-tree” idea post-submission further undermines claims of originality.  \n\n3. **Insufficient Technical Validation:**  \n   - Experiments are limited to a “small problem” (likely tic-tac-toe), which is insufficient to demonstrate robustness or scalability.  \n   - Reviewers could not verify the method’s technical merits due to unclear explanations, raising doubts about its validity.  \n\n4. **Author Response Deficiencies:**  \n   - The response focuses on apologies and future intent rather than addressing specific critiques (e.g., adding references, clarifying methodology). This suggests the paper is not yet ready for rigorous peer review.  \n\nWhile the core idea of meta-optimizing MCTS is intriguing, the paper fails to meet basic standards of academic writing, technical clarity, and scholarly context required for a top-tier venue.  \n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about proving that stochastic gradient descent-ascent (SGDA) can converge to a global solution for Wasserstein GANs (WGANs) when using a one-layer generator and quadratic discriminator. The reviewers have raised several concerns, and the authors have responded to them. My task is to evaluate whether the paper should be accepted or rejected based on the provided information.\n\nFirst, looking at the contributions: The authors claim they are the first to show global convergence of SGDA for non-linear generators in WGANs. That's a significant claim. However, the reviewers point out that the discriminator used is too simple (quadratic or linear) compared to the neural networks typically used in WGANs. The core of WGANs is using a Lipschitz-constrained neural network as the discriminator, but here they use a quadratic function with regularization. The authors argue that this choice simplifies the dynamics and maintains sample efficiency, referencing prior work that suggests using a too complex discriminator can lead to higher sample complexity. \n\nReviewer 1 and 3 are concerned that the quadratic discriminator doesn't align with the WGAN framework. The authors respond that the regularization on the quadratic discriminator enforces a Lipschitz constraint for bounded data. But the data here is generated from a teacher network with Gaussian input, which is unbounded. This seems contradictory. If the data isn't bounded, the Lipschitz constraint via regularization might not hold, which undermines the connection to WGANs. The authors mention they'll add an explanation in the revision, but it's unclear if they addressed this adequately.\n\nAnother major point is the choice of a one-layer generator. The reviewers question whether this is a true one-layer network or a linear model. The authors clarify it's a single hidden layer with element-wise activation, which is non-convex. However, Reviewer 2 and 3 argue that even with this, the analysis might not capture the complexity of real WGANs. The authors counter that their focus is on showing SGDA can work with minimal discriminator complexity, but the practical relevance is questionable.\n\nThe reviewers also mention that the quadratic discriminator only captures pairwise interactions, which might not suffice for joint distributions. The authors claim their Theorem 3 shows recovery of the true generator, but the reviewers are skeptical if this extends beyond the simplified setup. The authors reference sample complexity matching prior work, which is a plus, but the practical applicability is still in doubt.\n\nThe authors emphasize their contribution as the first convergence result for non-linear generators. However, the non-linearity here is limited, and the discriminator's simplicity might not generalize. The reviewers acknowledge the theoretical contribution but question its impact given the simplifications. Reviewer 2 even states that while the optimization result is important, the simple discriminator is a weakness.\n\nIn the author response, they stress that using a more complex discriminator would increase sample complexity, citing Feizi et al. and Bai et al. This is a valid point, but the trade-off is between theoretical tractability and practical relevance. The authors' choice prioritizes the former, which is acceptable for a theoretical paper, but the reviewers are concerned about the disconnect from real WGAN usage.\n\nThe paper's title mentions \"WGANs,\" but the setup deviates from standard WGANs by using quadratic discriminators. This could be seen as misleading. The authors argue their discriminator has sufficient power for their generator class, but the term WGAN might imply a more standard setup. This is a point of contention regarding the paper's framing.\n\nReviewer 3 also points out that even with the quadratic discriminator, the analysis doesn't handle error propagation in more complex discriminators. The authors respond that it's unnecessary here, but this limits the broader implications of their work.\n\nOverall, the paper makes a theoretical contribution by showing convergence for a specific case, which is non-trivial. However, the simplifications in the model (quadratic discriminator, one-layer generator) significantly limit the practical implications and the connection to real-world WGANs. The reviewers' concerns about overclaiming contributions and the applicability of the results are valid. The authors' defense focuses on theoretical novelty and sample complexity, but the gap between their setup and standard WGANs remains a critical issue.\n\nConsidering the top-tier conference standards, the paper's theoretical contribution is notable but may be too narrow. The choice of a quadratic discriminator, while justified for analysis, might not align with the conference's expectations for relevance to practical GAN training. The reviewers' mixed opinions (some acknowledging the contribution, others pointing out significant limitations) suggest that the paper is on the borderline. However, given that Reviewer 2 improved their score after the response but still noted weaknesses, and the other reviewers maintained their concerns, the paper might not meet the high bar for acceptance. The key issue is whether the theoretical advance justifies the simplifications. In this case, the disconnect from practical WGANs and the potential overclaim in the title might tip the balance towards rejection.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a theoretical analysis of SGD convergence for WGANs with a one-layer generator and quadratic discriminator. While the authors make a notable contribution by proving global convergence for non-linear generators—a first in this context—the work faces significant criticisms that undermine its suitability for a top-tier conference.  \n\n1. **Misalignment with WGAN Framework:** The use of quadratic discriminators diverges substantially from standard WGANs, which rely on Lipschitz-constrained neural networks. The authors' justification for this choice (simplifying dynamics and sample complexity) is valid theoretically but weakens the connection to practical WGAN applications. The regularization argument for enforcing Lipschitz constraints under unbounded data (Gaussian inputs) remains inadequately addressed, casting doubt on the relevance to true WGAN settings.  \n\n2. **Overstated Contributions:** The title and claims emphasize \"WGANs\" and \"one-layer networks,\" but the analysis applies only to highly simplified architectures. Reviewers correctly note that the quadratic discriminator lacks the expressive power of neural networks used in practice, making the results less impactful for understanding real-world GAN training dynamics.  \n\n3. **Limited Generalizability:** While the authors demonstrate convergence for their specific setup, the analysis does not extend to deeper generators or standard discriminators. The focus on pairwise interactions in the quadratic discriminator limits the recovery of full joint distributions, and the reliance on a teacher-student framework with known activations reduces the practical significance.  \n\n4. **Reviewer Consensus:** Reviewers acknowledged the technical effort but unanimously criticized the narrow scope and lack of alignment with WGAN fundamentals. Even after author responses, concerns about overclaiming and insufficient justification for simplifications persisted.  \n\nWhile the paper offers a meaningful theoretical advance, its limited applicability to real-world GANs and unresolved methodological gaps make it more suitable for a specialized theoretical venue rather than a top-tier conference seeking broad impact or practical relevance."
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper and the reviews. The paper proposes a domain adaptation method using a multi-branch network where each domain can take different computational paths. The main idea is that different domains might need different processing, which is a novel approach compared to existing methods that use the same processing for all domains.\n\nReviewer 1 had minor issues, mostly about terminology, references, and code availability. The authors addressed these by renaming the method to avoid confusion with existing terms, removing arXiv references where possible, and committing to release code. These seem resolved.\n\nReviewer 2 initially had concerns about experiments not showing different computation amounts and comparisons with SOTA methods. The authors added experiments with varying flow capacities and integrating MCD (a SOTA method), which improved results. They also clarified the parameter sharing in Figure 4. The reviewer's issues were resolved, and they maintained their score post-rebuttal.\n\nReviewer 3 was critical about insufficient experiments, especially missing comparisons with SOTA like CDAN and lack of ablation studies. The authors responded by adding MCD comparisons, some ablation studies on the number of flows, and multi-source experiments. However, they didn't compare with CDAN directly but used MCD instead. They also mentioned that more extensive ablation studies were time-consuming but provided some initial results.\n\nThe strengths of the paper include a novel approach to domain adaptation with adaptive computation paths, which is technically sound and well-motivated. The method's flexibility for multiple domains is a plus. The experiments, while initially lacking, were improved post-rebuttal, though not all SOTA comparisons were addressed. The authors made efforts to respond to most concerns, but some issues like full ablation studies and direct CDAN comparisons remain.\n\nPotential weaknesses are the remaining gaps in experiments. While they added MCD, not comparing with CDAN might leave a gap in SOTA comparison. The ablation studies are somewhat limited, which is a common concern in reviews. However, the core idea's novelty and the improved results with MCD might compensate for this.\n\nConsidering the reviewers' feedback post-rebuttal and the authors' responses, the paper seems to have addressed enough concerns to warrant acceptance, especially given the method's novelty and the positive experimental results. The remaining issues might be acceptable for a conference paper, with future work suggested.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel approach to unsupervised domain adaptation (UDA) by introducing a multibranch network that allows domains to undergo distinct computational paths. This addresses a key limitation of existing methods that enforce uniform processing across domains. The authors’ responses to reviewer concerns and the paper’s strengths/weaknesses are analyzed as follows:\n\n### **Strengths:**\n1. **Novelty and Technical Soundness:**  \n   The core idea of adaptive computation paths for domain-specific processing is novel in UDA. The method is technically sound, with a clear motivation and architecture design. The application of adaptive computation graphs to UDA is underexplored and aligns with the intuition that domains may require different feature transformations.\n\n2. **Improved Experiments Post-Rebuttal:**  \n   The authors addressed critical experimental gaps raised by reviewers:\n   - Added comparisons with **Maximum Classifier Discrepancy (MCD)**, a SOTA method, showing improved performance over the original MCD.\n   - Conducted ablation studies on the number of parallel flows (\\(K\\)) and computational unit configurations, demonstrating that \\(K > D\\) (domains) is unnecessary and that flows naturally adapt to domain complexity.\n   - Clarified parameter-sharing behavior in Figure 4, showing that domains share flows in later network stages while using domain-specific paths in earlier layers.\n\n3. **Responsiveness to Minor Issues:**  \n   - Renamed the method to \"Domain-Adaptive Multibranch Networks\" to avoid terminology clashes.\n   - Removed arXiv references where possible and committed to code release.\n   - Expanded related work to include non-deep-learning transfer learning methods (e.g., subspace alignment, CORAL).\n\n### **Weaknesses:**\n1. **Incomplete SOTA Comparisons:**  \n   While the authors compared with MCD, they did not evaluate against **CDAN**, a widely recognized SOTA method. This omission weakens the claim of generality, as CDAN’s adversarial alignment mechanism is orthogonal to their approach and could complement it. However, the rebuttal clarifies that their method is compatible with alignment strategies like MCD, which partially mitigates this concern.\n\n2. **Limited Ablation Studies:**  \n   The authors acknowledged that exhaustive ablation studies on computational unit groupings were infeasible due to time constraints. While they provided some analysis (e.g., flow capacity experiments), deeper exploration of hyperparameters (e.g., plasticity decay schedules) would strengthen the paper.\n\n3. **Multi-Domain Adaptation Scope:**  \n   The multi-source experiments added post-rebuttal are preliminary, and multi-target adaptation remains unexplored. However, the authors reasonably defer this to future work, given the complexity.\n\n### **Reviewer Consensus:**  \n- **Reviewer 1**’s concerns were fully resolved.  \n- **Reviewer 2** maintained their score post-rebuttal, indicating satisfaction with the revisions.  \n- **Reviewer 3**’s major issues (e.g., SOTA comparisons, ablation studies) were partially addressed, but residual gaps remain.  \n\n### **Conclusion:**  \nThe paper’s **novelty**, **technical rigor**, and **improved experimentation** outweigh its limitations. While not all concerns are fully resolved, the core contribution—adaptive computation for UDA—is significant and well-supported. The remaining issues (e.g., CDAN comparisons) do not invalidate the method’s merits and can be addressed in future work.\n\n**Final Decision:** Accept"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about learning Mahalanobis metric spaces using geometric approximation algorithms. The reviewers have raised several points, and the authors have responded. Let me try to unpack everything.\n\nFirst, the paper's main contribution seems to be an FPTAS for minimizing violated constraints in Mahalanobis metric learning. They use LP-type problems and computational geometry techniques, which they claim haven't been applied here before. Reviewer 2 points out that porting results from LP-type problems into this context is interesting and could encourage further work. That's a plus.\n\nReviewer 1 mentions that the state-of-the-art isn't clearly discussed. The authors responded by comparing their work to ITML and LMNN, which are established methods. They argue their approach is novel because it's the first with a provable guarantee on adversarial inputs. However, Reviewer 1 also notes missing recent papers like Verma & Branson and Ye et al. The authors say those works are different in setting (sample complexity vs. adversarial inputs), which might be valid, but maybe they should still discuss them more to position their work better.\n\nReviewer 2 brings up some technical points. The authors addressed most of them, like clarifying that the FPTAS assumes constant dimension, fixing typos in the algorithm, and defining terms. The hardness of the problem is mentioned as suspected NP-hard when dimension is unbounded, which is a reasonable assumption but not proven. That's a minor issue since the paper focuses on fixed dimensions.\n\nReviewer 3 is more critical, questioning the novelty and practical benefits. The authors argue that their method directly minimizes violations, which is different from prior error minimization approaches. They also show in Figure 3 that their method handles adversarial noise better. But Reviewer 3 points out that the experiments don't show clear accuracy improvements over existing methods, which is a concern. The authors need to better highlight the advantages, maybe in terms of robustness rather than just accuracy.\n\nThe experiments: They compare against ITML and LMNN on synthetic and real data. The adversarial example in Figure 3 is a strong point, showing their method's robustness. However, if other experiments don't show significant gains, that might weaken the case. The authors should clarify the practical scenarios where their method is superior.\n\nTheoretical contributions: The FPTAS with near-linear runtime for fixed dimensions is solid. The use of LP-type problems and low-dimensional linear programming techniques is a good theoretical foundation. However, the assumption of fixed d might limit practical applicability, but the authors mention it's common in computational geometry, which is fair.\n\nReviewer 1's question about the non-monotonic running time in Figure 4 is addressed by the authors, explaining PCA's impact. That seems plausible but might need more elaboration in the paper.\n\nMissing conclusion: Reviewer 1 notes the conclusion is missing. The authors didn't mention adding one in their response. That's a red flag. A conclusion is crucial for summarizing contributions and future work. If they didn't add it, that's a weakness.\n\nOverall, the paper has a strong theoretical contribution with an FPTAS, which is novel in this context. The practical benefits are somewhat demonstrated but could be more compelling. The responses address most technical concerns, but the lack of a conclusion and insufficient comparison with recent works (as per Reviewer 1) are issues. However, the theoretical novelty and the first provable guarantees for adversarial inputs might outweigh these, especially for a top-tier conference that values theoretical advancements. The reviewers' concerns are mostly addressed, though not perfectly. The paper seems to make a meaningful contribution, so leaning towards acceptance, but it's a close call.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel application of LP-type problems and computational geometry techniques to Mahalanobis metric learning, offering the first FPTAS with provable guarantees on adversarial inputs. While reviewers raised valid concerns about clarity, comparisons to recent works, and experimental motivation, the authors addressed most technical issues (e.g., clarifying assumptions, fixing typos, defining terms) and highlighted the theoretical significance of their approach. Key strengths include:\n\n1. **Theoretical Contribution:** The FPTAS for minimizing constraint violations under adversarial settings is a rigorous advancement, particularly for fixed dimensions. This bridges computational geometry and metric learning, opening avenues for future work.  \n2. **Robustness to Adversarial Noise:** The adversarial example in Figure 3 demonstrates practical relevance, distinguishing the method from prior convex optimization-based approaches.  \n3. **Responses to Critiques:** The authors clarified the novelty (directly minimizing violations vs. error penalties), addressed hardness assumptions, and explained non-monotonic runtime behavior.  \n\nWeaknesses include insufficient discussion of the conclusion section (not explicitly addressed) and limited experimental comparison to state-of-the-art methods. However, the theoretical novelty and robustness analysis justify acceptance in a top-tier venue, provided the authors incorporate minor revisions (e.g., adding a conclusion, clarifying comparisons). The work advances the field by introducing a new algorithmic perspective with provable guarantees."
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is titled \"Complex Query Answering with Neural Link Predictors\" and proposes a framework called Continuous Query Decomposition (CQD) for answering complex queries on incomplete knowledge graphs. The key idea is to use pre-trained neural link predictors to handle complex queries involving logical operations without needing extensive training on complex queries themselves. The authors claim improvements over existing methods in terms of accuracy and data efficiency, along with explainability.\n\nFirst, I'll look at the reviewers' comments. Reviewer 1 seems satisfied after the authors' response. Reviewer 2 highlights the elegance and efficiency of the solution, SOTA results, but raises questions about the query types and KG incompleteness handling. Reviewer 3 has technical concerns about the optimization methods, inference time, and comparisons with prior work. Reviewer 4 appreciates the practical approach but wants more on data efficiency and ablation studies. Reviewer 5 finds the paper excellent despite some presentation issues.\n\nThe authors responded to most concerns: they added experiments with DistMult, clarified data usage, included timing results, and addressed notation issues. They also provided more examples and explanations about query types and evaluation protocols.\n\nStrengths of the paper:\n1. **Novelty**: The approach of decomposing complex queries using pre-trained link predictors without needing to train on complex queries is novel. The use of t-norms and t-conorms for combining scores is a solid theoretical foundation.\n2. **Performance**: The paper shows significant improvements in Hits@3 over existing methods, even with less training data. The results are validated across multiple datasets.\n3. **Explainability**: The model provides intermediate solutions, which is a plus for interpretability.\n4. **Responses to Reviews**: The authors addressed most reviewer concerns adequately, adding experiments (e.g., DistMult, timing), clarifying methodology, and improving presentation.\n\nPotential Weaknesses:\n1. **Inference Time**: While the authors included timing results, the combinatorial method's exponential complexity for longer chains could be a limitation, though mitigated by current dataset sizes.\n2. **Ablation Studies**: Although they added some ablation with DistMult and embedding sizes, more exploration of different t-norms/t-conorms could strengthen the claims.\n3. **Comparison Fairness**: The use of different link predictors (ComplEx vs. others in baselines) might still leave questions about whether gains are due to the method or the link predictor. However, the authors showed that even with DistMult, their method outperforms baselines, which helps mitigate this.\n\nReviewers' concerns were largely addressed. The remaining issues (like inference time for longer chains) are noted but don't outweigh the contributions. The paper's strengths in novelty, performance, and practical applicability make it suitable for a top-tier conference. The authors' thorough responses and additional experiments demonstrate rigor and responsiveness to feedback.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel and efficient framework (CQD) for answering complex queries on incomplete knowledge graphs, achieving state-of-the-art results with significantly less training data. Key strengths include:  \n1. **Technical Innovation**: By decomposing complex queries into atomic components and leveraging pre-trained neural link predictors with t-norms/t-conorms, the method avoids costly training on complex queries while enabling explainability.  \n2. **Empirical Validation**: The approach outperforms baselines (e.g., GQE, Q2B) by 8–40% in Hits@3 across datasets, even with reduced embedding sizes. Ablation studies with DistMult and timing analyses address concerns about model dependency and scalability.  \n3. **Responsive Revisions**: The authors thoroughly addressed reviewer critiques, adding experiments (e.g., DistMult, timing results), clarifying methodology, and improving presentation. Remaining limitations (e.g., combinatorial search complexity) are acknowledged but do not detract from the core contributions.  \n4. **Impact**: The work bridges a critical gap in handling EPFO queries with incomplete KGs, offering practical advantages in data efficiency and interpretability.  \n\nWhile minor issues (e.g., further ablation on t-norms) remain, the paper’s novelty, rigor, and empirical results justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about using machine learning, specifically bidirectional LSTM RNNs, to predict DNA folding patterns (TADs) in Drosophila using epigenetic data. The authors compared their model against linear models and gradient boosting, claiming superior performance with their RNN approach. The reviewers had several concerns, and the authors responded to them. \n\nStarting with Reviewer 1's concerns: The main issue was whether the improvement from the LSTM was due to the model itself or the custom weighted MSE loss. The authors responded that all models used the same loss function, which addresses the fairness of comparison. However, Reviewer 1 also questioned generalizability to other datasets like ATAC-seq. The authors explained they focused on ChIP-seq for biological reasons but didn't test on other data, which might limit the broader applicability claim. The value '11' in the loss function was clarified, but it's still a bit arbitrary. The authors added more evaluation metrics, which is good.\n\nReviewer 2 had several technical points, mostly about clarity and methodology. The authors addressed most of these, like fixing citations and labels. However, the explanation for using biLSTM based on DNA's bidirectional nature is a bit hand-wavy. DNA's structure is double-stranded, but the input features here are epigenetic marks, not the DNA sequence itself. The connection between biLSTM and the biological justification seems a stretch. Also, the feature importance analysis using linear models but not for the LSTM (they mention using dropout but not more sophisticated methods like activation analysis) is a weakness. They added more metrics, which helps, but the core issue of model interpretability remains.\n\nReviewer 3 was harsher, pointing out lack of novelty, unclear methodology, and insufficient evaluation. The authors claim their work is novel because they predict TAD characteristics from ChIP-seq, not Hi-C data. But if prior work exists (even in workshops), not citing them is problematic. The authors argue those works were in workshops or not directly comparable, but in academia, even workshop papers should be acknowledged if relevant. The data preprocessing details (like how features were represented) were clarified, but the hyperparameter tuning section is still vague. They added more baselines and metrics, which strengthens the evaluation, but the methodological novelty is still in question. Comparing to CNNs or other architectures would have been better, as Reviewer 3 suggested.\n\nThe authors' responses improved the paper by adding evaluations, clarifying methods, and expanding literature reviews. However, some core issues remain. The biological justification for biLSTM isn't solid—using DNA's bidirectional nature as a reason when the model isn't processing the DNA sequence but epigenetic features. Also, the novelty claim is shaky if similar approaches exist, even in workshops. The technical improvements are there, but the lack of comparison with non-RNN models like CNNs or attention-based models is a gap. \n\nIn top-tier conferences, novelty, technical soundness, and thorough evaluation are key. The authors addressed some points but left others unresolved. The methodological contribution is incremental, applying existing models (biLSTM) to a new problem, but without a strong theoretical or empirical justification for why biLSTM is better suited here than other architectures. The evaluation is better now but still lacks comparisons with state-of-the-art models in related areas. \n\nGiven the mixed reviews and the authors' partial addressing of concerns, especially regarding novelty and model justification, I'm leaning towards rejection. However, the application area is important, and the work could be valuable if the remaining issues were addressed. But for a top-tier conference, the bar is high, and the unresolved methodological concerns and novelty questions might not meet that standard.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nWhile the authors addressed several reviewer concerns (e.g., clarifying the loss function, expanding evaluations, and improving methodology descriptions), critical issues remain unresolved:  \n1. **Lack of Methodological Novelty:** The paper applies established techniques (biLSTM) to a biological problem without sufficient justification for why this architecture is uniquely suited to model chromatin folding. The biological rationale (DNA’s bidirectional nature) is weakly connected to the model design, as inputs are epigenetic features, not DNA sequence. Comparisons to non-recurrent architectures (e.g., CNNs, transformers) are absent, leaving the superiority of biLSTM unconvincing.  \n2. **Incomplete Generalizability and Evaluation:** The authors did not test their approach on other datasets (e.g., ATAC-seq, other organisms), limiting claims of broader applicability. While they added evaluation metrics, the absence of comparisons to state-of-the-art baselines in chromatin structure prediction undermines the significance of their results.  \n3. **Unresolved Citations and Prior Work:** The dismissal of relevant workshop papers (even if not in main conferences) weakens the literature review. Top-tier conferences often expect thorough engagement with related work, including preliminary studies.  \n4. **Interpretability Gaps:** Feature importance analysis for the biLSTM remains superficial (e.g., using dropout instead of activation analysis), reducing biological insight.  \n\nFor a top-tier conference, the incremental application of existing methods, unresolved methodological justification, and incomplete evaluation do not meet the threshold for acceptance. While the work has potential, it requires deeper biological validation, architectural comparisons, and engagement with prior literature to justify publication."
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper's main contribution is reducing the number of supervised synaptic updates needed for models to match the primate ventral stream, using three strategies: reducing training data, improving initial connectivity, and training only a subset of synapses. The authors claim this makes the models more biologically plausible in terms of development.\n\nReviewer 1 and 2 are concerned about the use of BrainScore as the main metric. They argue that a high BrainScore doesn't necessarily mean the model's representations are similar to the brain's. The authors responded by clarifying that BrainScore is a proxy and that they made the limitations clearer in the text. They also added some validation, like generalization to new stimuli. But the core issue remains: how much does BrainScore actually reflect biological similarity? The authors argue that while it's not perfect, it's the best available metric and they've taken steps to address its limitations.\n\nReviewer 3 questions the biological plausibility of the weight compression method and the overall assumption that DNNs are good models of visual system development. The authors counter by stating they don't claim their methods are exact models but are steps toward more plausible hypotheses. They also added comparisons showing their initialization (WC) outperforms standard methods like Kaiming Normal, which strengthens their point.\n\nReviewer 4 is positive, highlighting the importance of the problem and thorough experiments. Reviewer 5 has mixed feelings, pointing out some methodological issues but acknowledging the potential. The authors addressed many of these, like clarifying terminology and fixing numerical inaccuracies.\n\nThe main points of contention are the validity of BrainScore and the biological relevance of the methods. The authors have made efforts to clarify these points but haven't entirely resolved the fundamental concerns. However, their work introduces novel strategies that reduce the training burden while maintaining performance, which is a significant contribution. The paper's strengths in addressing an understudied problem, thorough experimentation, and potential to inspire future research seem to outweigh the remaining methodological concerns. The fact that one reviewer strongly recommends acceptance and others are borderline but the authors addressed some issues tips the balance towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel approach to reducing the number of supervised synaptic updates required for deep neural networks (DNNs) to model the primate ventral stream, addressing a critical gap in biological plausibility. While reviewers raised valid concerns about the interpretation of BrainScore metrics and the biological relevance of methods like weight compression (WC), the authors provided substantive revisions and clarifications. Key strengths include:  \n1. **Methodological Innovation**: The combination of reduced training, improved initialization (WC), and selective parameter updates (CT) achieves significant reductions in supervised updates while retaining ~80% of BrainScore performance.  \n2. **Biological Relevance**: The authors clarified that their goal is not to fully model biological development but to propose hypotheses for improving alignment with biological constraints (e.g., genomic bottleneck, developmental timelines).  \n3. **Empirical Rigor**: Additional analyses (e.g., permutation tests showing WC outperforms Kaiming Normal initialization) and error corrections (e.g., fixing numerical inaccuracies) strengthened the results.  \n4. **Impact**: The work opens new avenues for studying developmental plausibility in DNNs, a timely and underexplored area in computational neuroscience.  \n\nWhile concerns about BrainScore’s interpretability and the speculative nature of WC remain, the authors acknowledged these limitations and reframed claims to avoid overstatement. The paper’s contributions—demonstrating feasible strategies to reduce training demands while maintaining brain-like representations—justify acceptance in a top-tier conference."
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about modeling time-evolving graphs using a deep neural approach based on temporal point processes. The authors propose a model called DIP, which incorporates LSTM with time gates, a selection mechanism for important nodes, and a fusion of multiple temporal resolutions via attention. The reviewers have raised several concerns, and the authors have responded to them. \n\nStarting with the strengths: The problem of modeling dynamic graphs is definitely important and relevant to top-tier conferences. The authors have shown empirical improvements over existing methods on multiple datasets, including a real-world financial application, which is a strong point. The introduction of a selection mechanism with co-attention seems novel compared to prior work, and the model's ability to handle growing/shrinking graphs is a practical advantage. The authors also added comparisons with the JODIE baseline in their response, addressing one of the major concerns from Reviewer 1.\n\nHowever, there are significant concerns about novelty. Reviewers 1 and 2 pointed out that components like LSTMs with time gates and temporal attention have been used before in similar contexts. The authors argue that their combination and application to graph-structured data with a focus on k-hop subgraphs and mutual information-based attention is novel. But the reviewers might still find the incremental nature of these contributions a drawback. The authors' responses clarify some distinctions from prior work, like handling multi-dimensional point processes and the specific mechanisms for node selection, but whether this is sufficiently novel for a top-tier conference is debatable.\n\nAnother major issue is the technical clarity and justification. Reviewer 3 found the paper hard to follow, and while the authors added figures to improve clarity, some questions about the log-likelihood formulation and model details remain. The authors corrected a mistake in the log-likelihood equation, which is good, but Reviewer 2's concern about the LL formulation being different from standard TPPs might still linger. The experimental section's depth was also questioned—Reviewer 1 wanted more ablation studies. The authors provided additional experiments on parameters like k and L, but it's unclear if these are in the main paper or appendix, which might affect the paper's impact.\n\nThe computational efficiency and scalability aspects were addressed by the authors, explaining that their method can handle growing graphs with k-depth subgraphs and parallel training. However, without concrete scalability experiments (e.g., on very large graphs), this remains a theoretical advantage.\n\nThe comparison with JODIE was a critical point. The authors included it in their revised version, which strengthens their empirical validation. However, the reviewers might still question if the improvements are due to the model's architecture or just increased parameters. The authors mention they tested with smaller embedding sizes and still performed well, which helps, but more detailed parameter comparisons would solidify this.\n\nIn terms of the response quality, the authors addressed most points, but some answers were still vague. For example, regarding the LL formulation, the authors mention it's for multi-dimensional TPPs but didn't provide a detailed derivation, which might leave doubts. The justification for the algorithm's coloring mechanism versus BFS could be more thorough, even though they explained it's for constructing the TDG incrementally.\n\nOverall, the paper presents a solid approach with empirical validations, but the novelty is somewhat incremental. The application to dynamic graphs and the specific mechanisms might be enough for acceptance if the technical contributions are framed as a novel integration of existing components tailored for dynamic graphs. However, the concerns about incremental contributions and clarity might tip the balance towards rejection in a highly competitive conference setting. The authors' responses mitigate some issues but not all, especially regarding the depth of analysis and novelty.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper addresses an important problem in modeling time-evolving graphs and demonstrates empirical improvements over existing methods. However, significant concerns remain unresolved:  \n1. **Novelty**: The core components (LSTM with time gates, stacked layers, temporal attention) are largely incremental. While the authors argue for novelty in their combination and application to graph-structured data, the differentiation from prior work (e.g., JODIE, Trivedi 2019) is insufficiently justified. The selection mechanism and fusion approach, though novel in detail, are not rigorously analyzed to show why they uniquely address the problem.  \n2. **Technical Clarity**: Despite revisions, key aspects of the model (e.g., log-likelihood formulation, subgraph construction, and training dynamics) remain unclear. Reviewer 2’s concern about the LL objective and Reviewer 3’s confusion about scalability and graph dynamics are not fully resolved.  \n3. **Empirical Rigor**: While the authors added comparisons to JODIE, the ablation studies (e.g., parameter sensitivity to \\(k\\), \\(L\\), and embedding size) are relegated to the appendix, weakening the main paper’s analysis. The improvements over baselines could stem from increased model capacity rather than architectural innovations, a point not conclusively addressed.  \n4. **Presentation**: The paper’s organization and clarity issues (noted by all reviewers) hinder reproducibility and impact. Critical details (e.g., the role of the fusion mechanism, temporal attention’s dependency) are under-explained.  \n\nWhile the work has merit, the incremental contributions, unresolved technical ambiguities, and insufficient empirical depth make it unsuitable for a top-tier conference in its current form."
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is \"Deep Coherent Exploration For Continuous Control,\" which proposes a new exploration framework for deep reinforcement learning. The authors build on prior work (van Hoof et al., 2017) that combined step-based and trajectory-based exploration for linear policies, extending it to deep neural networks. The main contributions are simplifying the approach by focusing on the last layer parameters, a recursive inference method for scalable updates, and integration with both on-policy (A2C, PPO) and off-policy (SAC) algorithms. The reviewers have a mix of positive and negative points, so I need to weigh them carefully.\n\nStarting with the positives: The paper is well-written and addresses an important problem in RL exploration. The empirical results for on-policy methods (A2C, PPO) are strong, showing clear improvements over baselines. The ablation studies are thorough, and the appendix provides enough detail for reproducibility. The theoretical derivations for the on-policy case are solid, and the scalability to deep networks is a notable contribution. Reviewers 1, 3, and 4 acknowledge the method's potential impact, especially for on-policy methods.\n\nNow the concerns: The off-policy (SAC) results are underwhelming. The integration here seems heuristic and less rigorous compared to the on-policy case. The performance gains with SAC are minimal or even negative in some tasks (like HalfCheetah). Reviewers 2 and 4 point out that SAC is a state-of-the-art method, so the lack of significant improvement here might limit the practical impact. Additionally, the experiments are limited to MuJoCo tasks, which are standard but not the most challenging for exploration. Some reviewers also mention that the font in figures is too small and that the explanation of step-based vs. trajectory-based exploration could be clearer.\n\nThe authors responded by revising the SAC integration, claiming better results, and addressing some presentation issues. However, the updated SAC results weren't fully included yet, which leaves some uncertainty. They also clarified that their method targets undirected exploration, which is valid, but the lack of tests on harder exploration tasks (like sparse rewards) is a limitation.\n\nReviewer 1 suggests reframing the paper to focus on on-policy methods, which might make the contribution clearer. The authors didn't take this approach but instead tried to improve the SAC part. The core issue is whether the contributions are sufficient for a top-tier conference. The on-policy improvements are solid, but the off-policy part is weaker. However, the scalability to deep networks and the theoretical framework for on-policy methods are significant. The paper's reproducibility and thoroughness in the on-policy case add to its strengths.\n\nConsidering the balance, the paper presents a meaningful advancement in exploration for deep RL, especially for on-policy methods. The weaknesses in the SAC integration and limited experiments are noted, but the strengths in theory and on-policy results likely outweigh these. The authors addressed some concerns in their response, though not all. For a top-tier conference, where incremental but solid contributions are acceptable if well-executed, this paper seems acceptable. However, if the SAC results remain weak and the experiments too narrow, it might be borderline. But given the positive aspects and the potential for impact in on-policy RL, I lean towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper \"Deep Coherent Exploration For Continuous Control\" presents a scalable exploration framework for deep RL, addressing a critical challenge in policy search methods. While the work builds on prior research (van Hoof et al., 2017), it makes notable contributions:  \n1. **Scalability to Deep Networks:** By focusing on perturbing only the last layer parameters and deriving a recursive inference method, the authors successfully extend generalized exploration to deep policies, a significant technical advancement.  \n2. **Strong Empirical Results for On-Policy Methods:** The experiments with A2C and PPO demonstrate clear improvements in learning speed and stability across MuJoCo tasks, supported by rigorous ablations. These results validate the method’s practical utility.  \n3. **Theoretical Rigor:** The analytical derivation of low-variance gradients for non-Markov policies is a key strength, particularly for on-policy algorithms.  \n\n**Concerns and Mitigations:**  \n- **Weak Off-Policy (SAC) Results:** While the SAC integration initially underperformed, the authors proposed a revised formulation in their response, aligning it more closely with the on-policy theoretical framework. Though not fully validated in the current submission, this adjustment addresses reviewer concerns and suggests potential for improvement.  \n- **Limited Experiment Scope:** The focus on MuJoCo tasks and undirected exploration is justified given the method’s scope, though testing on harder exploration benchmarks (e.g., sparse rewards) would strengthen the work.  \n- **Presentation Issues:** The authors acknowledged and committed to addressing minor issues like figure readability and terminology clarity.  \n\n**Conclusion:**  \nThe paper’s strengths—novel scalability, strong on-policy results, and theoretical contributions—outweigh its limitations. The revisions to the SAC integration and the method’s general applicability to widely used algorithms like PPO make it a valuable addition to the RL literature, suitable for a top-tier conference."
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a method to disentangle representations in pre-trained transformers like BERT by learning binary masks over weights or hidden units. The idea is to identify subnetworks that encode specific features, such as syntax vs. semantics or sentiment vs. genre. The authors claim their method performs comparably or better than existing approaches like VAEs and adversarial training, without needing to fine-tune the base model.\n\nReviewer 1 is concerned about the generality of the method, questioning if it's only applicable to the specific cases tested. The authors responded by emphasizing the method's potential for robustness in any scenario where spurious correlations are a concern, which is a common issue. However, they only tested on two datasets (sentiment/genre and syntax/semantics), which might not be sufficient to prove general applicability. They mention considering more datasets for the camera-ready version, but as of now, the evidence is limited.\n\nReviewer 2 points out that the paper doesn't clearly show superiority over previous methods like VAEs and lacks benchmark improvements. The authors added VAE comparisons in one experiment, showing better performance, but didn't include them in the sentiment/genre task. They argue that their focus is on disentanglement for robustness rather than SOTA benchmarks, which is valid but might not fully address the reviewer's concern about impact.\n\nReviewer 3 appreciates the approach's simplicity and the problem's importance but raises issues about the triplet loss's scalability beyond two factors and potential attribute leakage. The authors clarify that the method can be extended to more factors but acknowledge scalability challenges. They also address the leakage concern by noting the use of MLP probes but admit stronger probes might reveal more leakage, which is a limitation.\n\nReviewer 4 praises the writing and results but wants more domains tested and a discussion on weight vs. hidden unit masking. The authors mention testing two distinct tasks as evidence of generalizability but agree to consider more datasets. They explain the difference between weight and hidden unit masking, which adds clarity. However, the lack of experiments on more domains remains a weakness.\n\nReviewer 5 questions the experimental setup's limited genres and the utility of disentangled representations in downstream tasks. The authors justify their genre choice based on existing correlations and mention using STS for semantic similarity, but more downstream applications would strengthen the case.\n\nThe authors addressed several concerns, like adding VAE comparisons and clarifying implementation details. However, some issues remain unresolved. The generality of the method is still in question due to limited datasets. While the approach is novel and the results are promising, the lack of extensive validation across diverse tasks and domains might limit its perceived impact. Additionally, the comparison to existing methods isn't comprehensive enough, especially in the sentiment/genre experiment. The paper's strengths include a novel approach to disentanglement without fine-tuning and showing sparsity benefits. However, the weaknesses in experimental breadth and some unresolved reviewer concerns might hold it back from a top-tier conference acceptance unless the authors can provide more compelling evidence in the final version.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a novel method for disentangling representations in pre-trained transformers via masking, offering potential benefits in robustness and interpretability. While the approach is innovative and addresses an important problem, several critical issues remain unresolved, leading to a recommendation for rejection at a top-tier conference:  \n\n1. **Limited Generality and Scope of Experiments**:  \n   - The experiments are confined to two narrowly defined tasks (sentiment/genre and syntax/semantics) with limited datasets (e.g., only two movie genres). While the authors argue that these tasks demonstrate robustness to spurious correlations, the lack of validation across diverse domains (e.g., books, music) or more complex attribute combinations undermines claims of general applicability.  \n\n2. **Insufficient Comparison to Baselines**:  \n   - Although the authors added VAE comparisons in one experiment, key baselines (e.g., adversarial methods, other disentanglement frameworks) are not comprehensively evaluated across all tasks. The sentiment/genre experiment lacks comparisons to VAEs, leaving gaps in demonstrating superiority over existing approaches.  \n\n3. **Unresolved Methodological Concerns**:  \n   - Scalability of the triplet loss to more than two factors is acknowledged as a challenge but not empirically addressed.  \n   - Attribute leakage remains a concern, as probes for leakage are limited to simple MLPs, and the impact of stronger probes is unexplored.  \n\n4. **Weaknesses in Downstream Task Validation**:  \n   - While the paper includes semantic similarity (STS) and robustness experiments, broader downstream applications (e.g., text classification, NLI) are not sufficiently explored to demonstrate practical utility.  \n\n5. **Presentation and Clarity Issues**:  \n   - Key implementation details (e.g., masking only the last 9 layers) were initially omitted, and some figures (e.g., Figure 3) require clearer interpretation to support claims of superiority over baselines.  \n\n**Strengths**:  \n- The core idea of disentangling representations via masking in pre-trained models is novel and aligns with trends in model interpretability and efficiency.  \n- The method’s ability to achieve sparsity via masking and pruning is compelling.  \n- The paper is well-structured, and the authors thoughtfully engaged with reviewer feedback.  \n\n**Conclusion**:  \nWhile the work has merit, the current evaluation does not sufficiently validate the method’s generality, scalability, or superiority over existing approaches. Additional experiments on diverse tasks, broader comparisons, and deeper analysis of limitations (e.g., leakage, computational overhead) are needed to meet the high bar of a top-tier conference."
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about a generalized probability kernel (GPK) for discrete distributions and its application in two-sample testing. The reviewers have a lot of concerns. Let me start by summarizing the key points from the reviews and the authors' response.\n\nReviewer 1 points out several issues: the definition of the kernel is unclear, confusion between K and k, missing proofs (especially Theorem 5), and the lack of a simulation section. They also mention that some sections are trivial or known. The authors responded by removing the KSD part, focusing on polynomial GPK, and revising notations. But they didn't address the missing simulations, which is a big red flag for a methods paper.\n\nReviewer 2 criticizes the lack of theoretical innovation and experimental evaluation. They note that the theory is elementary and there's no analysis of the estimators' usefulness. The authors added some theoretical results on polynomial GPK but still no experiments. Without experiments, it's hard to validate the claims, especially for a conference expecting practical impact.\n\nReviewer 3 questions the motivation for using kernels in discrete settings where labels don't have a natural distance. The authors mention word2vec as an example, but that's a stretch. Also, there are issues with definitions and typos. The authors revised notations and added a discussion on NLP, but the core issue of applicability in typical discrete settings (like categorical data without embeddings) remains unclear.\n\nReviewer 4 is confused about the kernel definitions and their application in two-sample tests. They also mention missing literature and unclear presentation. The authors tried to clarify definitions but didn't fully resolve how the kernel works when p and q are unknown. The removal of KSD might have simplified things, but the core application in testing is still underdeveloped.\n\nThe authors' response indicates they removed problematic sections (like KSD), fixed notations, and focused on polynomial GPK. However, they didn't add any experiments, which is a major omission. Top-tier conferences usually require empirical validation, especially for new methods. The theoretical contributions are incremental, building on existing MMD, but without showing significant improvement or novel applications. The presentation issues were addressed, but the lack of experiments and unresolved theoretical concerns (like the practical scenarios where GPK is beneficial) make the paper incomplete. Even after revisions, the paper doesn't meet the bar for a top-tier conference due to missing critical components like experiments and thorough theoretical validation.\n</think>\n\nThe paper proposes a generalized probability kernel (GPK) for discrete distributions, aiming to unify existing discrepancy measures like MMD and extend their applicability. While the idea has potential, significant flaws remain unresolved, as highlighted by reviewers and the authors’ response:\n\n1. **Theoretical Gaps**:  \n   - The core definition of GPK remains ambiguous. Reviewers noted confusion between kernel types (e.g., scalar vs. matrix outputs) and insufficient justification for calling GPK a \"kernel\" rather than a distance. The authors’ revisions improved notation but did not fully resolve foundational ambiguities, such as the role of \\(\\phi\\) in RKHS and the conditions under which GPK is a valid kernel.  \n   - Key proofs (e.g., Theorem 5) were initially missing, and the revised focus on polynomial GPK lacks depth. The theoretical contributions are incremental, relying heavily on existing MMD frameworks without novel insights.  \n\n2. **Motivation and Applicability**:  \n   - The discrete setting’s relevance to kernel methods is weakly justified. While the authors cite NLP (e.g., word2vec) as motivation, this example is niche and does not address the broader challenge of applying GPK to categorical data without pre-existing embeddings.  \n   - The removal of the KSD discussion due to incorrect proofs leaves the paper narrower in scope, reducing its perceived impact.  \n\n3. **Experimental Deficiency**:  \n   - **No experiments were conducted** to validate GPK’s utility. For a method targeting two-sample testing, empirical validation of power, convergence, and comparison to baselines is critical. The authors’ response does not address this, rendering the claims about GPK’s advantages unsupported.  \n\n4. **Presentation Issues**:  \n   - Despite revisions, lingering typos and unclear explanations (e.g., undefined terms like \"pmfs,\" inconsistent notation) hinder readability. The lack of a clear narrative structure further weakens the paper’s persuasiveness.  \n\n**Conclusion**: While the authors addressed some concerns (e.g., notation, focus on polynomial GPK), the absence of experiments, unresolved theoretical ambiguities, and limited motivation for the discrete setting make the paper unsuitable for a top-tier conference. The work remains preliminary and does not meet the expected standards of novelty, rigor, or practical relevance.  \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this paper review. The paper proposes αVIL, a method for dynamic task weighting in multitask learning. The reviewers have a mix of positive and negative points. \n\nFirst, the strengths: The paper is well-written and the method is intuitive. They tested it on both vision and NLP tasks, which is good for generality. The idea of using model updates for task weighting is novel, as the authors claim it's the first to do so.\n\nBut the concerns are significant. Reviewer 1 and 3 both point out that the experimental results aren't convincing. The improvements over baselines are marginal, especially in NLP tasks, and often within the margin of error. The baselines might not be comprehensive enough. For example, they compared against standard MTL and DIW, but maybe other dynamic weighting methods are missing. Also, the theoretical foundation is lacking. Without a formal analysis, it's hard to justify why αVIL works beyond empirical results.\n\nReviewer 2 mentions that the method feels ad-hoc, like the step where they subtract 1 from alphas. The authors tried to explain it, but it's still a bit unclear. Also, the connection to meta-learning isn't fully explored. If it's similar to MAML or other meta-approaches, the novelty might be in question.\n\nThe authors responded to some concerns. They addressed the task weight evolution in MultiMNIST, showing weights trend towards 1 and 0, which is a sanity check. They also mentioned computational constraints for some experiments, like limited runs for NLP tasks. However, they didn't add theoretical justification yet, which is a big point. They plan to include it in the camera-ready version, but it's not there now.\n\nThe results on MultiMNIST are better, but the NLP results are mixed. Since MTL often has small gains in NLP, maybe the method's effectiveness is domain-specific. But for a top-tier conference, the contributions need to be more solid. The authors argue that DIW is a strong baseline, but if αVIL only slightly outperforms it, the practical impact is limited.\n\nAnother issue is the lack of statistical significance in results. The authors mention standard deviations for MNIST but not consistently for NLP. Without proper statistical tests, it's hard to claim the improvements are real.\n\nThe reviewers also wanted more ablation studies and analysis of task weights. The authors mentioned space constraints but plan to add them. However, in the current submission, these are missing, weakening the empirical validation.\n\nIn terms of novelty, while the method uses model updates dynamically, the reviewers question if it's sufficiently different from existing meta-learning approaches. The authors argue it's different because they focus on task weights instead of model initialization, but this distinction might not be enough without a clear theoretical framework.\n\nConsidering all this, the paper has some merits but falls short in key areas expected for a top-tier conference. The lack of theoretical grounding and marginal empirical improvements make it hard to justify acceptance. The authors need more thorough experiments and a solid theoretical basis to strengthen their claims. As it stands, the paper is incremental and doesn't sufficiently advance the field beyond existing methods.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces αVIL, a novel method for dynamic task weighting in multitask learning (MTL) by leveraging task-specific parameter updates. While the idea of using model updates for task weighting is innovative and the paper is well-structured, several critical shortcomings preclude acceptance at a top-tier conference:  \n\n1. **Limited Theoretical Justification:** The method lacks formal analysis or theoretical grounding. While the authors acknowledge this and propose to address it in future work, the absence of such justification weakens the paper's contribution, especially given the heuristic nature of existing MTL weighting approaches.  \n\n2. **Marginal Empirical Improvements:**  \n   - On NLP tasks (Table 2), αVIL shows inconsistent gains over baselines (e.g., DIW), with improvements often small and statistically insignificant. The authors note that MTL gains in NLU are typically modest, but this does not sufficiently justify the method's added complexity.  \n   - On MultiMNIST (Table 1), αVIL outperforms baselines in mean accuracy, but results overlap within standard deviations, raising questions about significance.  \n\n3. **Incomplete Empirical Validation:**  \n   - The NLP experiments lack standard deviations, making it difficult to assess significance.  \n   - The choice of MultiMNIST (with only one auxiliary task) limits the demonstration of scalability to more complex MTL settings.  \n   - Limited ablation studies (e.g., task weight trajectories, sensitivity analysis) and sanity checks (e.g., degenerate auxiliary tasks) reduce confidence in the method's robustness.  \n\n4. **Novelty Concerns:** Reviewers noted similarities to meta-learning approaches (e.g., MAML) and existing dynamic weighting methods (e.g., DIW). While the authors argue that αVIL differs by focusing on task weights rather than model initialization, the distinction is not sufficiently clarified or validated.  \n\n5. **Author Responses:** The authors addressed some concerns (e.g., clarifying weight updates, adding standard deviations for NLP in future versions) but did not resolve core issues like theoretical analysis or statistically significant results. Promised improvements (e.g., theoretical justification) remain speculative.  \n\n**Conclusion:** While αVIL presents a promising direction, the current submission lacks the theoretical depth, empirical rigor, and clear novelty required for a top-tier venue. Significant revisions—including theoretical analysis, expanded experiments, and stronger baselines—are needed to establish its contribution."
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper tackles distributed mean estimation (DME) with a focus on scenarios where input vectors have large norms but are close to each other. The main contribution is a lattice-based quantization method that reduces communication costs while maintaining accuracy dependent on input variance rather than input norm. The authors claim their approach is asymptotically optimal and provide both theoretical and experimental results.\n\nReviewer feedback highlights several strengths: the problem's importance, the novel use of lattices, matching upper/lower bounds, and practical experiments. However, there are concerns about the clarity of certain explanations, the practicality of optimal lattices, experimental scope (n=2 mainly), and some technical details in proofs and definitions.\n\nThe authors addressed most concerns in their response. They clarified theoretical vs. practical algorithm distinctions, agreed to add more experiments with larger n, corrected definitions (packing radius), and plan to elaborate on the computationally simpler algorithm using ℓ∞ reduction. They also acknowledged the trade-offs in communication rounds and setup costs, which they'll detail more.\n\nReviewers 1-3 had minor issues but overall positive. Reviewer 4 noted the practical algorithm doesn't match optimal bounds but still found the work solid. The authors' responses were thorough, addressing technical points and promising revisions. The paper's theoretical contributions (novel lattice approach, optimal bounds) seem significant for a top-tier conference. While experiments are limited, the theoretical depth and problem relevance likely outweigh this, especially given planned revisions. The paper seems to meet the bar for acceptance despite some flaws, as the core contributions are strong and the authors addressed concerns adequately.\n</think>\n\nFinal Decision: **Accept**\n\n**Reasoning:**  \nThis paper addresses a fundamental problem in distributed machine learning—distributed mean estimation (DME)—with a novel lattice-based quantization approach. The work is theoretically rigorous, providing asymptotically optimal communication-error trade-offs and extending prior results that depend on input norms to the more practical setting where inputs are concentrated around the mean. The authors also offer a computationally feasible variant using cubic lattices, albeit with a logarithmic factor loss, and validate their method experimentally. \n\n**Strengths:**  \n1. **Problem Significance:** DME is central to distributed learning, and the focus on input variance (vs. norm) addresses a critical gap in real-world applications.  \n2. **Theoretical Contributions:** The lattice-based framework and matching upper/lower bounds are novel and impactful. The reduction to ℓ∞ via cubic lattices, while not new, is cleverly adapted here.  \n3. **Practical Relevance:** Experiments, though limited to small \\( n \\), demonstrate tangible improvements over baselines. The authors also commit to expanding experiments in revisions.  \n4. **Responsive Revisions:** The authors thoroughly addressed reviewer concerns (e.g., clarifying definitions, adding algorithmic descriptions, expanding experiments) and acknowledged limitations (e.g., theoretical vs. practical algorithm trade-offs).  \n\n**Weaknesses:**  \n1. **Experimental Scope:** The primary experiments use \\( n=2 \\), which is narrow, though supplementary results for \\( n=8, 16 \\) are noted.  \n2. **Algorithmic Practicality:** The optimal lattice-based method is computationally intensive, and the practical variant sacrifices tight bounds. However, the authors justify this trade-off clearly.  \n3. **Minor Technical Gaps:** Some proofs and definitions required corrections (e.g., packing radius), but these were resolved in the response.  \n\n**Conclusion:**  \nDespite minor flaws, the paper’s theoretical advancements, problem relevance, and thorough revision plan justify acceptance. The work advances the state-of-the-art in DME and provides a foundation for future research in distributed optimization."
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviews. The paper introduces Shoot Tree Search (STS), a planning algorithm that combines elements of MCTS and random shooting, aiming to balance depth and breadth in search. The authors claim STS offers better performance in challenging domains like Sokoban and Google Research Football.\n\nReviewer 1 questions the technical novelty, pointing out that multi-step expansion isn't entirely new (citing prior work like Soemers et al. 2016). They also raise concerns about the experiments not isolating the effect of multi-step expansion from the value function approximator. The reviewer suggests more ablation studies and mentions issues with presentation quality, like unclear algorithms and hyperparameter choices.\n\nReviewer 2 feels the paper lacks intuitive explanation of why STS is better and is confused about its advantages in sparse reward scenarios. They also note issues with figures and missing references for methods like random shooting.\n\nReviewer 3 appreciates the simplicity and empirical results but is concerned about comparisons with MCTS under high simulation counts and missing related work. They ask about benefits over AlphaGo-style rollouts and performance in different leaf evaluation settings.\n\nReviewer 4 doubts the novelty, comparing STS to AlphaGo's use of MCTS with rollouts, and questions if hyperparameter tuning of MCTS could achieve similar results. They also ask for bias-variance analysis.\n\nReviewer 5 acknowledges the idea's simplicity and thorough evaluation but points out missing discussion of prior work (Coulom 2006/2007) and confusion about experiment setup. They also note minor issues in writing and algorithms.\n\nThe authors responded by adding experiments comparing STS to AlphaGo-style evaluations, showing STS's superiority. They addressed concerns about computational budgets, sparse rewards, and backpropagation schemes. They clarified experiment setups, related work differences, and hyperparameter choices. They also plan to revise the paper for clarity and include more discussions.\n\nNow, considering all reviews and the response: The main concerns were about novelty, experiment design, and presentation. The authors addressed some points with new experiments and clarifications. However, some issues remain. The novelty is somewhat contested by prior work, though the authors argue the context with neural value functions is new. The experiments now include more comparisons, but some reviewers might still find them insufficient. Presentation issues like unclear algorithms and hyperparameters are partially addressed but need thorough revision.\n\nThe paper's strengths are the empirical results showing STS's effectiveness and the thorough response. However, the weaknesses in novelty and clarity might still hold it back. Given the mixed reviews and the authors' efforts in addressing concerns, but with remaining doubts, the decision is borderline. However, since the authors provided substantial new evidence and plan revisions, acceptance might be justified if they can adequately address the remaining issues in the final version.\n</think>\n\nFinal Decision: **Accept**\n\n**Reasoning:**  \nThe paper introduces Shoot Tree Search (STS), a novel planning algorithm that interpolates between MCTS and random shooting, offering explicit control over the depth-breadth trade-off in tree search. While reviewers raised valid concerns about technical novelty, experimental rigor, and clarity, the authors provided a robust response addressing these points:  \n1. **Novelty**: The authors clarified distinctions from prior work (e.g., Soemers et al. 2016) by emphasizing STS’s integration with learned value functions and its role in bias-variance trade-offs, which differentiates it in modern RL contexts.  \n2. **Experiments**: New ablation studies (e.g., AlphaGo-style rollouts, sparse reward settings, and large-scale Sokoban experiments) strengthened claims about STS’s advantages. Results demonstrated consistent improvements over baselines, even in high-compute regimes.  \n3. **Presentation**: The authors committed to revising unclear sections (e.g., algorithm descriptions, hyperparameter details) and expanding discussions on intuition and related work.  \n\nWhile minor issues remain (e.g., clarifying N_p vs. C in experiments), the core contributions—a simple yet effective method with empirical validation—are sufficient for acceptance at a top-tier conference. The authors’ thorough rebuttal and planned revisions mitigate initial concerns, making the paper a candidate for acceptance with minor revisions."
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes that the cerebellum acts as a decoupled neural interface (DNI) to solve the locking problem in neural networks, similar to how DNIs work in deep learning. They test this with models on tasks like target reaching and caption generation, showing that their CC-DNI model performs better than truncated BPTT and mimics deficits seen in cerebellar patients.\n\nReviewer 1 found the hypothesis intriguing but had concerns about clarity, especially around the focus on feedforward vs. recurrent settings and the comparison to existing DNI work. They wanted more details on bootstrapping and architectural differences. The authors responded by adding new figures and clarifying their focus on temporal feedback, along with explaining bootstrapping in the model.\n\nReviewer 2 was critical, arguing that the paper doesn't offer new insights but just combines existing ideas. They felt the connection between cerebellum and DNI was superficial. The authors added new experiments and predictions, like ablation studies and correlation analyses, to strengthen their claims and link to biological data.\n\nReviewer 3 appreciated the clarity but wanted more biological validation and concrete predictions. They also pointed out citation inaccuracies. The authors addressed this by adding new figures comparing model predictions with experimental findings, like cerebellar activity during learning phases and correlation patterns, and corrected the citations.\n\nThe authors' response included three new figures and a new section with predictions, addressing the main concerns about biological relevance and novel insights. They clarified the model's focus on temporal tasks and bootstrapping, and made efforts to link their findings to existing neuroscience research.\n\nNow, considering the revisions, does the paper meet the standards of a top-tier conference? The topic is interdisciplinary, bridging neuroscience and deep learning, which is a plus. The hypothesis is novel and could stimulate further research. The addition of concrete predictions and comparisons with experimental data strengthens the paper's contribution to neuroscience. However, the technical contribution to deep learning might be incremental compared to existing DNI work, as the main innovation is applying DNI concepts to model the cerebellum. \n\nThe paper's strengths are its novel hypothesis, interdisciplinary approach, and the addition of biological validations. Weaknesses include potential incremental technical contributions in ML and some remaining questions about biological plausibility (e.g., gradient computation in the brain). However, the authors addressed most reviewer concerns adequately. Given the positive aspects and the revisions made, the paper seems suitable for acceptance, especially as it opens new research avenues.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel and interdisciplinary hypothesis linking cerebellar function to decoupled neural interfaces (DNIs), offering a biologically inspired solution to the locking problem in neural computation. While initial reviews raised valid concerns about clarity, biological validation, and novelty, the authors addressed these effectively in their revision:  \n\n1. **Strengthened Biological Relevance:**  \n   - Added experiments (Figs. 5–7) directly tying the model to cerebellar biology, including ablation studies, correlation analyses, and predictions about cerebellar activity patterns. These align with experimental findings (e.g., Wagner et al. 2019) and generate testable hypotheses (e.g., cerebellar silence in well-learned tasks).  \n   - Expanded discussion of predictions, such as asynchronous updates and distributed neocortical representations, enhancing the paper’s neuroscientific impact.  \n\n2. **Clarified Technical Contributions:**  \n   - Focused the narrative on temporal feedback (BPTT) and clarified the role of bootstrapping, addressing Reviewer 1’s concerns.  \n   - Differentiated CC-DNI from prior DNI work by emphasizing cerebellar-inspired sparsity and modularity, which improve parameter efficiency and decorrelation.  \n\n3. **Interdisciplinary Significance:**  \n   - The framework bridges deep learning and neuroscience, proposing a mechanistic role for the cerebellum in credit assignment. This could catalyze collaboration across fields, a key criterion for top-tier venues.  \n\n**Remaining Limitations:**  \n- The biological plausibility of gradient computation (e.g., Purkinje cell-to-cortical neuron ratios) remains partially unresolved, though the authors propose testable mechanisms (e.g., thalamic gating).  \n- While the DNI implementation is not revolutionary, the cerebellar-inspired modifications (sparsity, modularity) offer practical insights for ML.  \n\nOverall, the revisions substantiate the paper’s novelty, rigor, and potential to influence both neuroscience and machine learning, warranting acceptance at a top-tier conference."
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviews. The paper proposes integrating language into both top-down and bottom-up visual processing for referring expression segmentation. The main claim is that using language throughout the visual pathway improves performance over just top-down approaches. The experiments show some improvements on standard datasets, but there are mixed reviews from the reviewers.\n\nReviewer 1 thinks the novelty is thin and suggests adding more explanations and citations. They also have questions about model depth and fusion methods. The authors addressed some of the suggestions, like adding figures and citations, but Reviewer 1 lowered their rating post-rebuttal because a key baseline wasn't properly cited.\n\nReviewer 2 is concerned about empirical results. They note that the model doesn't match SOTA on some test sets and questions over-tuning. The authors responded by explaining their tuning process, but Reviewer 2 remains unconvinced, especially after discrepancies in numbers between tables.\n\nReviewer 3 is positive, praising the problem's importance and the model's design. They recommend acceptance based on the results and clarity.\n\nReviewer 4 rejects the paper, citing poor conceptual motivation and insufficient analysis. They want more on why language affects low-level processing and better error analysis. The authors mention some planned analyses but haven't included them yet. Reviewer 4 isn't swayed by the rebuttal and still doubts the significance.\n\nThe authors' response addresses some issues but not all. They added some requested figures and citations but didn't perform significance tests or in-depth error analyses. The cognitive motivation is still weak, relying on existing studies without a clear link to their model. The empirical improvements are there but not huge, and test set variability is a concern.\n\nConsidering top-tier conference standards, the paper needs strong novelty, solid empirical results, and thorough analysis. While the idea of integrating language in both pathways is interesting, the execution has flaws. The mixed reviews, especially the concerns about significance and analysis from Reviewers 2 and 4, are major red flags. The authors' responses didn't fully alleviate these concerns. The lack of statistical significance tests and detailed error analysis makes it hard to validate their claims. The conceptual motivation is underdeveloped, which is critical for a top-tier venue. Even though Reviewer 3 is positive, the other reviewers' criticisms are substantial. Therefore, the paper might not meet the high bar for acceptance without more convincing evidence and analysis.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents an intriguing approach to integrating language into both top-down and bottom-up visual processing for referring expression segmentation. While the core idea—modulating low-level visual features with language—is promising and aligns with cognitive science insights, the execution and presentation fall short of the standards expected at a top-tier conference. Key issues include:  \n\n1. **Conceptual Motivation and Novelty**:  \n   - The cognitive science literature is cited but not effectively leveraged to justify the architectural choices. The connection between cognitive findings (e.g., language influencing early visual processing) and the proposed model remains superficial, weakening the paper’s theoretical contribution.  \n   - The innovation over existing methods (e.g., Step-ConvRNN) is unclear. While the authors argue their model enables ablation studies, the rebuttal does not sufficiently differentiate their approach from prior work.  \n\n2. **Empirical Rigor**:  \n   - Results are inconsistent across datasets (e.g., underperformance on UNC test sets and ReferIt compared to SOTA), raising doubts about generalizability. The rebuttal attributes this to validation-based tuning but does not address test-set variability convincingly.  \n   - Statistical significance tests are absent, and the rebuttal dismisses their necessity despite Reviewer 4’s valid critique. The claimed standard deviation (0.06 IoU) lacks context and does not resolve concerns about robustness.  \n\n3. **Analysis and Interpretability**:  \n   - The paper lacks critical error analysis and interpretability studies (e.g., visualizing how language modulates low-level features). While the authors mention ongoing work (e.g., clustering language filters), these analyses are not included, leaving the mechanism behind the improvements opaque.  \n   - Reviewer 4’s request for contrastive failure cases or alignment with linguistic structure remains unaddressed, weakening the empirical narrative.  \n\n4. **Rebuttal Limitations**:  \n   - Key critiques (e.g., missing baseline citations, over-tuning concerns, and conceptual motivation) were inadequately resolved. The rebuttal deflects questions about novelty and significance rather than providing concrete evidence.  \n\nWhile the paper explores a timely and relevant problem, the combination of unresolved methodological concerns, insufficient empirical validation, and underdeveloped conceptual framing makes it unsuitable for acceptance at a top-tier venue. The work would benefit from deeper analysis, stronger theoretical grounding, and more rigorous evaluation to substantiate its claims."
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes GG-GAN, a geometric graph generative adversarial network. The main idea is to generate graphs by placing nodes in a geometric space and connecting them based on similarity. The authors claim improvements in modeling complex relations, handling isomorphism, and exploiting latent distributions. They use a WGAN framework, ensuring permutation equivariance and scalability.\n\nReviewer 1's main concerns are about the datasets being too small (9-20 nodes) and possible isomorphism issues. They also mention missing baselines like NetGAN and simpler models. Reviewer 2 questions edge dependency modeling, mode collapse, and theoretical contributions. Reviewer 4 is concerned about novelty, literature review, and scalability claims. Reviewer 5 points out unclear aspects of the method and experiments.\n\nThe authors didn't submit a response, so I can't see their counterarguments. Let's break down the issues:\n\n1. **Datasets and Scalability**: The datasets are small (9-20 nodes). Generating larger graphs (like thousands of nodes) is a claim, but experiments don't validate this. Without empirical evidence on larger graphs, scalability is unproven. Also, small graphs might have many isomorphic duplicates, affecting training and evaluation.\n\n2. **Baseline Comparisons**: Missing key baselines like NetGAN and others. The current results show GG-GAN isn't clearly superior. For a top-tier conference, thorough comparison is essential. The authors need to show they outperform or match SOTA in more metrics.\n\n3. **Theoretical Contributions**: The theorems (like Corollary 1) might not add significant value. If the existence of a distribution is trivial, as Reviewer 2 notes, this doesn't strengthen the paper. The connection between theory and the model's design needs clarification.\n\n4. **Methodology Concerns**: Edge dependencies are handled via Bernoulli sampling, which is independent. The claim of modeling complex edge dependencies might be overstated. Also, the use of hand-crafted features could limit the model's adaptability, as mentioned by Reviewer 2.\n\n5. **Novelty and Literature Review**: The paper's contributions are questioned. While combining geometric approaches with GANs isn't entirely new, the authors need to clearly differentiate their work. The literature review is insufficient, missing discussions on recent works like Transformer-based models.\n\n6. **Experimental Rigor**: The experiments lack diversity in datasets and metrics. Mode collapse isn't addressed, and scalability isn't empirically validated. Without these, the claims remain unsubstantiated.\n\nGiven the absence of author responses, the concerns aren't mitigated. For a top-tier conference, the paper needs stronger empirical validation, clearer theoretical contributions, and comprehensive comparisons. The current version has potential but doesn't fully meet the high standards required.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces GG-GAN, a geometric graph generative model with promising theoretical properties, such as permutation equivariance and scalability. However, significant concerns raised by reviewers remain unaddressed due to the lack of author responses, undermining confidence in the claims. Key issues include:  \n1. **Weak Empirical Validation**: Experiments are limited to small graphs (9–20 nodes), raising doubts about scalability claims. The datasets may contain redundant isomorphic graphs, potentially inflating performance metrics. Comparisons with critical baselines (e.g., NetGAN, Transformer-based models) are missing, and results show marginal improvements over existing methods.  \n2. **Overstated Theoretical Contributions**: Theorems (e.g., Corollary 1) are criticized as trivial or weakly connected to the model’s design. Claims about modeling edge dependencies are contradicted by the use of independent Bernoulli sampling.  \n3. **Novelty and Literature Gaps**: The paper inadequately positions itself against prior work (e.g., isomorphism handling in graphRNN, scalability of autoregressive models). The literature review is incomplete, and the distinction from existing methods is unclear.  \n4. **Methodological Concerns**: The collision-avoidance mechanism, edge dependency modeling, and hand-crafted features lack rigorous justification. Mode collapse and diversity in generated graphs are not empirically addressed.  \n\nWhile the idea of geometric graph generation via GANs is interesting, the paper falls short of the rigor and innovation expected at a top-tier conference. Addressing these issues would require substantial revisions and additional experiments."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about a new method for multitask learning called αVIL. The main idea is dynamically adjusting task weights during training by using model parameter updates. The reviewers have a mix of positive and negative comments. \n\nReviewer 1 says the paper is well-written and the method is intuitive but lacks theoretical justification. The experiments aren't convincing because the baselines are limited and the results aren't consistently better. They suggest improvements in theory and experiments.\n\nReviewer 2 points out that the method seems ad-hoc without clear justification. They compare it to existing meta-learning approaches and aren't convinced of its novelty. The experimental results are close to baselines, especially on NLU tasks, and there are questions about the setup and statistical significance. They also mention some style issues.\n\nReviewer 3 is concerned about the algorithm's clarity, the experimental setup (like using only one auxiliary task in MultiMNIST), and the lack of significant improvements. They suggest the results are not strong enough and recommend rejection unless more experiments and analysis are added.\n\nThe authors responded by addressing some concerns: explaining the algorithm's steps, adding more context about dataset choices, and promising to include theoretical analysis and more experiments in the final version. They also mention that improvements in NLU are typically small, which might explain the marginal gains.\n\nNow, considering this is a top-tier conference, the standards are high. The method's novelty is questioned, especially in relation to existing meta-learning approaches. The experiments show some improvements but not consistently across all tasks, and the statistical significance is in doubt. The lack of theoretical backing is a weakness, though the authors plan to add it. However, the paper is well-written and the idea of using model updates for task weighting is interesting. \n\nThe key issues are whether the incremental improvement is enough for a top-tier venue and if the method's novelty is sufficient. The reviewers point out that the results are not significantly better than existing methods, and the theoretical foundation is missing. The authors' response addresses some points but doesn't fully resolve the concerns about the empirical evidence and novelty. \n\nIn top-tier conferences, even a well-executed paper with marginal contributions might struggle. The mixed experimental results and lack of strong theoretical justification might lead to a rejection. However, the idea itself could be seen as a novel approach in multitask learning, and the authors did respond to some concerns. But given the reviewers' criticisms about the methodology's justification and the experimental results not being compelling enough, the paper might not meet the acceptance threshold.\n</think>\n\nThe paper proposes αVIL, a method for dynamic task weighting in multitask learning (MTL) by leveraging task-specific parameter updates. While the idea of using model updates for task weight optimization is novel, the paper faces significant criticisms that impact its suitability for a top-tier conference.\n\n### Key Strengths:\n1. **Novelty**: The core idea of directly using model updates to adjust task weights dynamically is a fresh approach in MTL, differentiating it from heuristic or search-based methods.\n2. **Clarity**: The paper is well-structured and accessible, with clear explanations of the algorithm and experiments.\n3. **Empirical Scope**: Evaluations span both vision (MultiMNIST) and NLU tasks, demonstrating cross-domain applicability.\n\n### Critical Weaknesses:\n1. **Theoretical Justification**: The method lacks formal analysis or theoretical grounding. While the authors acknowledge this and promise to address it in a revised version, its absence weakens the paper’s rigor.\n2. **Marginal Empirical Gains**: \n   - On MultiMNIST, αVIL shows modest improvements over baselines (e.g., DIW), but differences are small and within standard deviation ranges.\n   - On NLU tasks, results are mixed, with no clear superiority over existing methods. The authors argue that MTL gains in NLU are inherently limited, but this does not fully mitigate concerns about the method’s impact.\n3. **Baseline and Experimental Limitations**:\n   - The MultiMNIST setup uses only one auxiliary task, limiting the demonstration of scalability to more complex MTL scenarios.\n   - Statistical significance is not comprehensively reported for NLU tasks, raising doubts about the robustness of claims.\n4. **Novelty Concerns**: Reviewers note similarities to meta-learning approaches (e.g., MAML) and question whether αVIL’s contributions are sufficiently distinct from prior work.\n\n### Authors’ Response:\nThe authors addressed several concerns, clarifying algorithmic steps, dataset choices, and promising to add theoretical analysis and experiments. However, their responses do not fully resolve the core issues:\n- The lack of statistical significance for NLU results remains unaddressed.\n- Theoretical justification is deferred to a future version, leaving the current submission incomplete.\n- The novelty argument hinges on differentiating αVIL from meta-learning, but this distinction is not convincingly established.\n\n### Conclusion:\nWhile αVIL introduces an interesting idea, the paper falls short of the high bar for top-tier conferences. The marginal empirical gains, unresolved questions about statistical significance, and absence of theoretical analysis undermine its contribution. The authors’ responses improve clarity but do not substantively alter the experimental or theoretical limitations highlighted by reviewers. For a top-tier venue, the work requires stronger theoretical grounding, more compelling empirical results, and clearer differentiation from related methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes αVIL, a method for dynamic task weighting in multitask learning. The main idea is to adjust task weights based on parameter updates between epochs. The reviewers have a mix of positive and negative points.\n\nReviewer 1 thinks the paper is well-written but lacks theoretical justification and the experiments aren't convincing enough. They mention that the baselines are limited and the results don't consistently outperform others. Reviewer 2 finds the method ad-hoc and not novel enough, comparing it to meta-learning approaches. They also question the experimental results' significance. Reviewer 3 is concerned about the algorithm's clarity and the marginal improvements over baselines, suggesting rejection.\n\nThe authors responded by addressing some concerns: explaining the intuition behind the algorithm, adding more experiments (like standard deviations for NLU tasks), and planning to include theoretical analysis. They also clarified the choice of MultiMNIST and ensemble reporting.\n\nNow, considering the strengths: the paper introduces a novel approach using parameter updates for task weighting, which hasn't been done before. The method is evaluated on both vision and NLP tasks, showing some improvements, especially in MultiMNIST. The authors addressed some concerns in their response, like clarifying the algorithm's steps and planning additional experiments.\n\nHowever, the weaknesses are significant. The experimental results are not strong enough. In NLP tasks, the improvements are minimal and might not be statistically significant. The theoretical foundation is missing, which is a big issue for a top-tier conference. The novelty is questioned, as dynamic task weighting exists in prior work. The authors' response mentions future plans for theory and more experiments, but these aren't in the current submission.\n\nTop-tier conferences require both solid theory and strong empirical results. While the idea is interesting, the lack of theoretical analysis and marginal empirical gains make it hard to justify acceptance. The authors need more thorough experimentation and theoretical grounding to strengthen their claims. Given the current state, the paper isn't ready for acceptance yet.\n</think>\n\nThe paper proposes αVIL, a method for dynamically adjusting task weights in multitask learning by leveraging task-specific parameter updates. While the idea of using model updates for task weighting is novel and the paper is well-structured, several critical issues prevent it from meeting the standards of a top-tier conference.\n\n### Strengths:\n1. **Novelty**: The core idea of directly using task-specific parameter updates to dynamically adjust task weights is a fresh perspective in multitask learning, distinguishing it from heuristic or gradient-based weighting methods.\n2. **Empirical Evaluation**: Experiments span both computer vision (MultiMNIST) and NLP tasks, demonstrating generality. On MultiMNIST, αVIL shows consistent (though modest) improvements over baselines.\n3. **Clarity**: The paper is well-written, with a clear algorithm description and motivation.\n\n### Weaknesses:\n1. **Theoretical Justification**: The method lacks formal analysis or theoretical grounding. While the authors acknowledge this and promise future work, the absence of even an intuitive connection to optimization theory (e.g., meta-learning, bilevel optimization) weakens the contribution.\n2. **Marginal Empirical Gains**: \n   - On NLP tasks, improvements over baselines (e.g., DIW) are minimal and inconsistent. For example, αVIL underperforms DIW on COPA and MRPC (Table 2). \n   - Results on MultiMNIST, while better, have overlapping standard deviations with baselines (Table 1), raising questions about statistical significance.\n3. **Experimental Scope**: \n   - Only one auxiliary task is used in MultiMNIST, limiting the claim’s generality for multitask settings with many tasks.\n   - Lack of ablation studies (e.g., analyzing learned weights, training dynamics) and sanity checks (e.g., degenerate auxiliary tasks) reduces interpretability.\n4. **Novelty Concerns**: The method’s similarity to meta-learning (e.g., MAML-like parameter updates) and existing dynamic weighting approaches (e.g., DIW) is not sufficiently differentiated. The authors’ response comparing αVIL to DIW and meta-learning is insightful but does not fully resolve these concerns in the current manuscript.\n\n### Author Response Assessment:\nThe authors addressed several points (e.g., clarifying algorithm steps, standard deviations for NLU tasks, MultiMNIST rationale) but left critical gaps:\n- No theoretical analysis was added, only promised for future work.\n- Statistical significance for NLU tasks remains unclear, and ensemble vs. single-model discrepancies are unresolved.\n- The response to novelty concerns (e.g., comparison to meta-learning) is compelling but not reflected adequately in the paper.\n\n### Conclusion:\nWhile αVIL introduces an interesting idea, the lack of theoretical grounding, marginal empirical improvements, and insufficient differentiation from prior work make it premature for acceptance at a top-tier venue. The authors need to (1) provide theoretical insights, (2) expand experiments to more tasks/domains with rigorous statistical analysis, and (3) better situate the method within the meta-learning and multitask literature. As it stands, the contribution is incremental rather than transformative.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper introduces an efficient approximation for the loss function in ecological inference, applying it to models like deep neural networks and Bayesian networks. They use real election data to show improvements in accuracy and covariance structure recovery.\n\nReviewer 1 points out strengths like tackling an important problem and providing interesting insights but criticizes the paper's structure, clarity, and lack of implementation details. They also mention typos. Reviewer 2 appreciates the connection between fields but finds experiments incomplete, missing key baselines, and lacking reproducibility. They also note that the linear model performed better on some metrics, which isn't addressed. Reviewer 3 thinks the work is important and likely useful but suggests more comparisons and synthetic data tests. Reviewer 4 claims the approach isn't new, cites insufficient related work, and finds the experiment section unreadable.\n\nThe authors didn't submit a response, so there's no rebuttal to these points. \n\nLooking at the key issues: \n\n1. **Clarity and Structure**: Multiple reviewers found the paper hard to follow, with unclear sections and missing details. This is a major issue because if the method isn't clearly explained, it's hard to assess its validity and for others to replicate.\n\n2. **Experimental Rigor**: The experiments are criticized for missing baselines (like standard MLM), incomplete details (train/test splits, hyperparameters), and not addressing why the linear model outperformed on some tasks. Without proper baselines and reproducibility, the claims of improvement are questionable.\n\n3. **Novelty and Related Work**: Reviewer 4 and others mention that the approach isn't novel and related work (like LLP, distribution regression) isn't adequately cited. This is a problem because the paper needs to position itself within existing literature to show contribution.\n\n4. **Typos and Presentation**: While minor compared to other issues, multiple typos and formatting errors suggest a lack of polish, which can affect the perception of the work's quality.\n\nOn the positive side, the problem is important, the application to real election data is valuable, and integrating deep learning with ecological inference is a promising direction. However, the weaknesses in methodology explanation, experimental setup, and comparison to existing work are significant. Top-tier conferences require both technical soundness and clear contributions. The paper's current state, with unresolved issues in clarity, missing baselines, and insufficient discussion of related work, makes it fall short of acceptance. Even though Reviewer 3 is more positive, the other reviewers' concerns are substantial and not addressed. Without author responses to clarify or rebut these points, the decision leans towards rejection.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper addresses an important problem in ecological inference and demonstrates potential by integrating deep learning techniques with real-world election data. However, critical flaws hinder its suitability for a top-tier conference:  \n1. **Clarity and Structure:** Multiple reviewers noted the paper's poor organization, unclear explanations (e.g., loss function derivation, experimental tasks), and unresolved typos, which impede understanding and reproducibility.  \n2. **Incomplete Experiments:** Key baselines (e.g., standard multi-level models) are missing, and experimental details (train/test splits, hyperparameters) are insufficiently documented. The lack of comparison to prior methods (e.g., distribution regression, collective graphical models) weakens claims of novelty.  \n3. **Unaddressed Results:** The paper does not reconcile why a linear model outperforms proposed deep models on held-out R², raising doubts about the practical benefits of complexity.  \n4. **Related Work Gaps:** Critical connections to existing literature (e.g., learning from label proportions, distribution regression) are underdeveloped, failing to position the work within the broader field.  \n\nWhile the application to election data is compelling, the unresolved methodological and presentation issues outweigh the strengths. The absence of author responses exacerbates these concerns, leaving reviewers’ questions unanswered. For a top-tier venue, the paper requires substantial revisions to address these shortcomings."
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. So, the paper is about applying theoretical insights from meta-learning theory to improve existing algorithms through regularization. The authors claim that by incorporating these theoretical assumptions as regularization terms, they can enhance the generalization of meta-learning models in few-shot classification tasks.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 and Reviewer 4 are more critical, pointing out that the proposed regularizations might not be novel enough, comparing them to existing techniques like spectral normalization and weight decay. They also question the significance of the improvements and the lack of comparison with recent methods. Reviewer 2 and 3 have concerns about the theoretical assumptions and their practical applicability, as well as the experimental setup, like the use of a validation set and the statistical significance of results. \n\nThe authors responded by clarifying that their goal isn't to propose a new state-of-the-art method but to validate whether theoretical insights hold in practice. They adjusted the objective function with hyperparameters, added experiments, and provided a synthetic example to show how their regularization can enforce theoretical assumptions. They argue that their regularizations are different from existing methods because they target specific theoretical conditions (like the ratio of singular values) and that combining both regularizations is crucial, which isn't the case with standalone techniques like weight decay.\n\nHowever, the reviewers' concerns about novelty and the marginal improvements remain. The authors did address some points, like adding more baselines and discussing the differentiation between their approach and existing methods. But the core issue seems to be whether the contribution is sufficient for a top-tier conference. The theoretical validation is a good angle, but if the practical implementation doesn't show significant improvements or the methods are too similar to existing ones, it might not be enough. The rebuttal does add some experiments and clarifications, but the improvements are still within the margin of standard deviations in some cases, which weakens the impact.\n\nAnother point is the technical aspect of computing gradients for singular values. The authors mention that auto-differentiation handles it, but Reviewer 4 was concerned about the details. The authors' reference to prior work using similar methods might alleviate that concern, but it's still a potential weakness.\n\nIn the end, the paper's strength is in bridging theory and practice, which is valuable. However, the criticisms about the incremental nature of the regularizations and the modest experimental results might hold it back. For a top-tier conference, where novelty and significant contributions are key, this might fall short. The authors' response does mitigate some issues, but not all. The decision is borderline, but considering the mixed reviews and the emphasis on novelty in top conferences, I'm leaning towards reject.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a valuable effort to bridge theoretical insights from meta-learning with practical algorithms, aiming to validate whether theoretical assumptions improve few-shot learning performance. However, several critical issues remain unresolved, making it unsuitable for acceptance at a top-tier conference:  \n\n1. **Novelty of Contributions**:  \n   - The proposed regularization methods (singular value ratio control and Frobenius norm regularization) are argued to differ from spectral normalization and weight decay. However, the rebuttal fails to convincingly demonstrate that these techniques are fundamentally novel rather than incremental adaptations of existing methods. While the authors emphasize the necessity of combining both regularizations, the theoretical justification for their unique synergy remains underdeveloped.  \n\n2. **Marginal Empirical Improvements**:  \n   - The performance gains over baselines are often within the margin of standard deviation (as noted by Reviewer 3) or comparable to improvements from simple implementation tweaks (e.g., MAML++). This undermines the claim that the theoretical insights lead to significant practical advancements. The addition of Meta-Curvature as a baseline does not sufficiently address concerns about benchmarking against state-of-the-art methods.  \n\n3. **Theoretical-Practical Alignment**:  \n   - While the synthetic experiment in Figure 1 illustrates how regularization can enforce theoretical assumptions, it does not fully resolve concerns about whether these assumptions hold in real-world settings. Reviewer 3’s critique—that assumptions about optimal predictors depend on the problem itself, not the algorithm—remains pertinent. The paper does not conclusively show that enforcing these assumptions universally benefits performance, especially when tasks violate them.  \n\n4. **Technical Clarity**:  \n   - Reviewer 4’s concerns about the feasibility of computing subgradients for singular values and the applicability of regularization to complex models (e.g., LEO) are only partially addressed. The reliance on auto-differentiation frameworks without explicit implementation details weakens reproducibility claims.  \n\n5. **Narrative Focus**:  \n   - The authors’ revised narrative emphasizes theoretical validation over algorithmic innovation, but the experiments remain tied to performance metrics. This creates a mismatch between the stated goal (testing theory) and the evaluation (accuracy improvements), leaving the paper’s core contribution ambiguous.  \n\nWhile the work is a commendable step toward connecting theory and practice in meta-learning, the unresolved issues in novelty, empirical significance, and theoretical rigor outweigh its strengths for a top-tier venue."
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the reviewers' comments and the paper details. The paper introduces PERIL, a method combining meta-RL and imitation learning. The main idea is to use demonstrations to precondition exploration policies, improving adaptation rates in unseen tasks. The reviewers have a mix of positive and negative points, but there are some recurring issues that need careful consideration.\n\nReviewer 1 appreciates the idea but finds the method overly complex with too many loss terms. They also point out inconsistencies in notation and mathematical formulation, making it hard to follow. The experiments lack sufficient baselines and ablations to justify each component. The problem formulation's POMDP aspect seems mismatched with the algorithm's assumption of a fixed task.\n\nReviewer 2 questions the originality, noting similarities to prior works like PEARL and Yu et al. They also mention missing comparisons to relevant methods and unclear explanations of equations and methodology. The experiments are criticized for not including key baselines and for unclear reward setups.\n\nReviewer 3 highlights the method's potential but criticizes the lack of clarity, proper citations, and statistical significance in experiments. They also mention that the claims about zero-shot learning and applicability to robotics aren't well-supported. The method's complexity and insufficient ablation studies are significant drawbacks.\n\nReviewer 4 acknowledges the problem's importance but points out strong assumptions, like the expert policy conditioning on the learned latent z. Clarity issues in problem formulation and notation are major concerns. The experiments are seen as not sufficiently demonstrating exploration beyond demonstrations, and there's a lack of proper citations.\n\nCommon themes across reviewers:\n1. **Clarity and Presentation**: Multiple reviewers note issues with notation inconsistencies, unclear equations, and terminology. The problem setup (POMDP vs. fixed task) is confusing.\n2. **Methodological Concerns**: The complexity of PERIL with multiple loss terms isn't justified through sufficient ablations. Comparisons to relevant baselines (e.g., PEARL with demonstrations) are missing.\n3. **Experiments and Evaluation**: Lack of statistical significance (few seeds), insufficient task diversity, and unclear reward setups. Claims about zero-shot learning and generalization aren't fully substantiated.\n4. **Originality and Citations**: The work isn't adequately differentiated from prior works like PEARL and Yu et al. Missing citations for key related works.\n\nThe paper's strengths include addressing an important problem and showing promising results in combining meta-RL with imitation learning. However, the weaknesses in clarity, methodological justification, and experimental rigor are significant. Top-tier conferences require not only novel ideas but also thorough validation and clear presentation. The reviewers' concerns about the lack of ablations, insufficient comparisons, and statistical validation are critical. Additionally, the mismatch between the problem formulation and algorithm assumptions needs addressing.\n\nGiven these points, the paper in its current state doesn't meet the high standards for acceptance. The authors need to address the clarity issues, provide more comprehensive experiments with proper statistical analysis, include necessary ablations, and better situate their work within existing literature. Without these improvements, the paper's contributions aren't sufficiently validated.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel approach, PERIL, combining meta-RL and imitation learning, with promising empirical results. However, critical flaws hinder its readiness for a top-tier conference:  \n1. **Clarity and Presentation:** Inconsistent notation, ambiguous problem formulation (e.g., POMDP vs. fixed-task assumptions), and poorly explained equations reduce reproducibility and understanding.  \n2. **Methodological Justification:** The complexity of PERIL’s six loss terms lacks sufficient ablation studies to validate their necessity. Key comparisons (e.g., PEARL with demonstrations) are missing, leaving the method’s novelty and efficacy unclear.  \n3. **Experimental Rigor:** Experiments lack statistical significance (e.g., few seeds, no confidence intervals), fail to substantiate claims of zero-shot adaptation, and omit comparisons to relevant prior works (e.g., Mendonca et al.).  \n4. **Originality and Citations:** The work overlaps significantly with PEARL and Yu et al. but inadequately differentiates itself. Citations for foundational meta-RL/IL literature are missing or misrepresented.  \n\nWhile the problem is important and the results are promising, the unresolved issues in clarity, validation, and positioning within the literature prevent acceptance. Major revisions addressing these concerns are required for resubmission."
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a method called MIMO, which uses a multi-input multi-output configuration to train subnetworks within a single model, allowing for efficient ensembling with minimal parameter increase and computational cost. The main claim is that this approach achieves robustness and uncertainty benefits comparable to traditional ensembles but with a single forward pass, making it computationally efficient.\n\nLooking at the reviews, there's a mix of positive and critical feedback. Reviewer 1 appreciates the minimal parameter increase and the use of MIMO but is concerned about the \"for free\" claim and the lack of detailed parameter analysis. Reviewer 2 praises the thorough experiments and clarity but wants more discussion on related work and theoretical understanding. Reviewer 3 finds the idea clever but is worried about theoretical grounding and practical advantages. Reviewer 4 questions originality and computational cost reporting, even calling some aspects deceptive.\n\nThe authors responded by clarifying the parameter increase (1% comes from input and output layers), adding FLOPs data, expanding on related work, and addressing notation issues. They also provided reasoning on subnetworks' independence through empirical analysis rather than theory.\n\nStrengths of the paper include a novel approach to efficient ensembling, strong empirical results across multiple datasets, and thorough ablation studies showing subnetworks' independence. The method's practical impact is significant for scenarios where computational efficiency is crucial.\n\nWeaknesses pointed out by reviewers include limited theoretical analysis, potential issues with originality due to existing multi-branch architectures, and concerns about how computational costs are presented. However, the authors addressed some of these by adding FLOPs data and expanding discussions on related work. The lack of theoretical grounding is a common issue in ML papers and might be acceptable given the empirical strength.\n\nThe key concerns are originality and computational cost transparency. The authors argue that their use of multiple inputs for independence is novel compared to prior multi-head approaches. They also clarified that FLOPs are nearly the same, supporting their efficiency claim. While Reviewer 4's concerns about originality are valid, the authors' response and the empirical results might mitigate this.\n\nConsidering the revisions and the paper's contributions—efficient ensembling with strong empirical backing—the strengths likely outweigh the weaknesses. The paper offers a practical solution with solid experimentation, which is valuable for top-tier conferences. However, the decision hinges on whether the reviewers' remaining concerns are adequately addressed. Since the authors provided clarifications and additional data, acceptance seems justified.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel approach (MIMO) for training independent subnetworks within a single model, enabling efficient ensembling with minimal parameter overhead and no added inference latency. The method addresses a critical challenge in deploying robust ensembles by eliminating the computational cost of multiple forward passes. Key strengths include:  \n1. **Empirical Rigor:** Extensive experiments across CIFAR10/100, ImageNet, and OOD variants demonstrate significant improvements in accuracy, calibration, and robustness. The ablation studies (loss landscapes, activation analysis) convincingly validate the independence of subnetworks.  \n2. **Practical Impact:** The method’s computational efficiency (1% parameter increase, negligible FLOPs difference) makes it highly applicable to latency-sensitive domains (e.g., autonomous systems).  \n3. **Responsive Revisions:** Authors addressed reviewer concerns by clarifying parameter/FLOPS calculations, expanding discussions on related work (e.g., BatchEnsemble), and improving notation consistency.  \n\nWhile weaknesses remain (e.g., limited theoretical analysis, questions about originality compared to multi-branch architectures), the authors’ empirical evidence and revisions mitigate these issues. The core idea—leveraging multiple inputs to induce independence—is novel and impactful, and the paper meets the standards of a top-tier conference in terms of technical contribution and experimental validation."
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper introduces Block Minifloat (BM), a new floating-point format for training DNNs with 4-8 bits. The main idea is using a shared exponent bias across blocks to reduce the exponent bits needed, which allows for more efficient hardware, specifically using Kulisch accumulators. The results show significant hardware efficiency gains compared to FP8 and FP32, with minimal accuracy loss.\n\nReviewer 1 had some technical questions about the novelty of shared exponents, the impact of the accumulator on convergence, handling denormals, and the lack of larger network experiments. The authors addressed these by clarifying the novelty in hardware implementation, explaining that FP32 accumulation approximates the Kulisch well, detailing denormal handling, and noting computational constraints for ResNet50.\n\nReviewer 2 initially had a major concern about using multiple formats (BM8, BM6) leading to hardware overhead but later raised the score after the authors clarified that the synthesis already accounted for this. They also mentioned minor issues like power modeling details and clarity on BM configurations, which the authors addressed in revisions.\n\nReviewer 3 wanted more mathematical explanation of the alignment process and block size determination. The authors added equations and explanations, though some parts might still be a bit unclear. They also pointed out confusing result presentations, which the authors tried to fix by redefining columns and adding BM7 to tables.\n\nThe authors' responses seem thorough. They addressed major concerns about hardware overhead by explaining that their synthesis included the necessary configurations. They added equations and explanations where needed, though some parts like the derivation of Equation 7 might still be a bit hand-wavy. The hardware results are compelling, showing significant efficiency gains, which is a strong point for a top-tier conference. The experiments cover multiple models and datasets, though ResNet50 is missing, which is a minor drawback but understandable due to computational limits.\n\nPotential weaknesses include the complexity of handling multiple formats and whether the hardware overhead is truly minimal. However, the synthesis results support their claims. The novelty is in the combination of shared exponent bias with Kulisch accumulators, which seems novel enough for publication despite prior work on block formats.\n\nOverall, the paper presents a solid contribution with rigorous hardware evaluation and reasonable software experiments. The revisions addressed most concerns, and the results are promising. It meets the standards for a top-tier conference.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper introduces Block Minifloat (BM), a novel numerical representation for efficient DNN training, combining shared exponent bias with narrow floating-point formats to enable hardware-friendly Kulisch accumulators. The work addresses a critical challenge in low-precision training and demonstrates significant hardware efficiency gains (e.g., 4.1× smaller area and 2.3× lower energy vs. FP8) while maintaining accuracy. Key strengths include:  \n1. **Technical Novelty**: While shared exponents and block formats exist in prior work, BM uniquely optimizes exponent bias to reduce exponent bits, enabling dense integer-like hardware for FMA operations. The integration of Kulisch accumulators for minifloats is a notable advancement.  \n2. **Rigorous Hardware Evaluation**: The synthesis of BM units (place-and-route) provides concrete evidence of efficiency gains, surpassing typical quantization papers in methodological rigor.  \n3. **Comprehensive Software Validation**: Experiments span multiple models (ResNet, VGG, Transformer) and datasets (CIFAR, ImageNet), demonstrating broad applicability.  \n\n**Addressing Concerns**:  \n- **Reviewer 1**: The authors clarified how smaller exponents enable Kulisch accumulators (via alignment with integer-like hardware) and justified denormal handling through synthesized results. While ResNet50 experiments are absent, the focus on smaller networks aligns with embedded applications.  \n- **Reviewer 2**: The revised draft clarifies that hardware overhead for hybrid formats (e.g., BM8’s (2e,5)/(4e,3)) is already factored into synthesis results. Power modeling limitations are acknowledged but do not undermine core claims.  \n- **Reviewer 3**: Added equations and explanations (e.g., Equation 3 for alignment, Equation 7 for block size) improve clarity, though some derivations remain high-level. Table revisions resolve ambiguities.  \n\n**Remaining Weaknesses**:  \n- The need for hybrid formats (different precisions in forward/backward passes) introduces complexity, but the authors show this is manageable in hardware.  \n- Limited discussion of training-time overhead for exponent bias updates, though the impact appears minimal.  \n\nOverall, the paper’s combination of algorithmic innovation, hardware validation, and practical utility meets the bar for a top-tier conference. The revisions adequately address reviewer concerns, and the results advance the field of efficient DNN training."
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper is about using CycleGAN for automatic music accompaniment by treating Mel-spectrograms as images. The reviewers have a mix of positive and negative points. \n\nReviewer 1 points out several issues: the title is misleading, claims of being the first to use spectrograms as images are incorrect, lack of details in methodology, and insufficient justification for evaluation metrics. The authors responded by clarifying some points but kept the title, arguing that music production is a long-term goal. They added more details about Demucs and CycleGAN, but some issues like the title and evaluation justification might still be problematic.\n\nReviewer 2 appreciates the novelty but notes missing comparisons with other models, limited experimental settings, and dataset details. The authors added some experiments and dataset IDs but didn't compare with other GAN variants, which is a weakness. They also addressed some minor points but maybe not enough on inter-annotator agreement and task variety.\n\nReviewer 3 finds the paper interesting but raises concerns about source separation's impact, phase reconstruction, and technical details. The authors added explanations on phase retrieval and Demucs' time-equivariance, but the phase distortion in audio examples is still a concern. They also didn't fully address the impact of using FMA vs. MusDB data.\n\nThe authors made several revisions: added technical details, citations, dataset info, and addressed some methodology concerns. However, key issues remain. The title is still potentially misleading. The lack of comparison with other models (like other GAN variants) is a significant gap. The evaluation metrics, while improved, still have subjective elements and limited validation. The phase reconstruction method's impact on audio quality isn't fully resolved, which affects the results' credibility.\n\nFor a top-tier conference, the paper needs to have strong technical rigor, clear novelty, and thorough evaluation. While the approach is novel in application, the methodological gaps and unresolved issues from reviews might not meet the high standards. The authors' responses improved the paper, but some critical flaws remain, especially in evaluation and comparative analysis. Therefore, the paper might not be ready for acceptance without further revisions.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel application of CycleGAN for automatic music accompaniment using Mel-spectrograms, addressing an underexplored area in audio-domain music generation. While the approach is innovative and the problem is relevant, several critical issues highlighted by the reviewers remain unresolved, even after the authors’ revisions:\n\n1. **Title and Scope Misalignment**:  \n   The title claims \"Automatic Music Production,\" which encompasses tasks like mixing and mastering, but the work focuses narrowly on accompaniment generation. The authors defend this by arguing that producers often separate production stages, but the title remains overly broad and risks misleading readers. This undermines clarity and precision, which are essential for top-tier publications.\n\n2. **Methodological Gaps**:  \n   - **Source Separation Impact**: While the authors manually verified minimal bleed-through in Demucs-separated tracks, they did not quantify the impact of separation errors on model performance. This leaves uncertainty about whether the results are artificially inflated by residual artifacts.  \n   - **Phase Reconstruction**: The authors added details on phase retrieval (e.g., Griffin-Lim algorithm), but the perceptual quality of reconstructed audio remains problematic, as noted in the examples. Phase distortion is a critical limitation for audio applications, yet its effect on evaluation metrics (e.g., human judgments) is not rigorously analyzed.  \n   - **CycleGAN Implementation**: Architectural details (e.g., layer configurations, training protocols) remain sparse, hindering reproducibility. While the authors added a section on CycleGAN, key hyperparameters (e.g., learning rates, loss weights) are still omitted.\n\n3. **Evaluation Weaknesses**:  \n   - **Subjective Metrics**: The proposed evaluation metric relies on expert judgments, but the small sample size (4 annotators) and lack of inter-annotator agreement statistics (beyond a correlation matrix) weaken its validity. The binarization of scores further reduces granularity.  \n   - **Objective Metrics**: STOI and FID are used without strong justification for their relevance to music quality. The authors admit these features were selected post hoc, raising concerns about metric hacking.  \n   - **Task-Specific Analysis**: The model performs poorly on the vocal→full arrangement task, but the authors do not investigate why (e.g., harmonic complexity, dataset bias). This limits actionable insights.\n\n4. **Comparative Analysis**:  \n   The authors did not compare their approach to other image-to-image translation models (e.g., Pix2Pix) or symbolic-domain baselines, despite reviewer requests. This omission makes it difficult to assess the novelty and effectiveness of their method relative to existing work.\n\n5. **Reproducibility**:  \n   While the authors promised to release code, critical dataset details (e.g., FMA subset IDs, MusDB splits) and preprocessing steps (e.g., windowing functions, Mel-scale parameters) are still incomplete. Without these, replication remains challenging.\n\n**Strengths**:  \n- The application of CycleGAN to Mel-spectrograms for accompaniment generation is novel and addresses a meaningful gap in music AI.  \n- The hybrid evaluation (human + automatic metrics) is a step toward addressing the subjectivity of music generation tasks.  \n- The authors made commendable efforts to address reviewer feedback, clarifying phase reconstruction, dataset selection, and technical details.\n\n**Conclusion**:  \nWhile the paper introduces a promising direction, the unresolved methodological and evaluative limitations prevent it from meeting the rigor expected at a top-tier conference. The title’s overreach, insufficient comparative analysis, and unvalidated metrics are particularly critical. The work would benefit from deeper ablation studies, expanded evaluations, and direct comparisons to existing methods.\n\n**Final Decision**: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let me start by going through all the information provided. The paper in question addresses the problem of model calibration on out-of-distribution (OOD) data, proposing a post-hoc calibration method using outlier exposure. The authors claim their method improves calibration on corrupted data compared to existing benchmarks like Ovadia et al. 2019. \n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had strong concerns, pointing out that the method assumes knowledge of possible distributions, which might simplify the problem unrealistically. However, after the authors' response, they still had reservations about incomplete baselines. Reviewer 2 found the method simple and effective but wanted more experiments. Reviewer 3 had clarity issues and questions about generalization but raised their score after the rebuttal. Reviewer 4 increased their rating after additional experiments, especially on OOD data like SVHN.\n\nThe authors addressed several concerns in their response. They clarified that their method uses a single corruption type (contrast) for calibration and tests on entirely different corruptions, which they demonstrated with new experiments. They added results on completely OOD datasets (SVHN, Fashion-MNIST) showing effectiveness. They also provided more intuitive explanations and corrected notation issues. \n\nHowever, some issues remain. The assumption of using a specific corruption (contrast) for calibration might limit the method's applicability if the test corruption is vastly different. While they showed some OOD results, the core experiments are still on known corruption types from benchmarks. Reviewer 1's point about the problem setup differing from true OOD scenarios (like unsupervised domain adaptation) is valid but the authors argue they're addressing a different problem. \n\nThe method's simplicity is a strength but also a weakness if it doesn't generalize beyond their tested scenarios. The authors added theoretical intuition and more experiments, which strengthens their case. The results showing improved ECE across various corruptions and OOD datasets are compelling. \n\nConsidering the revisions and responses, the paper makes a meaningful contribution by improving calibration on OOD data with a simple, post-hoc method. The concerns about baseline comparisons and problem setup are partially mitigated but not entirely resolved. However, the empirical results and the practical utility of the method likely outweigh these concerns for a top-tier conference.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel post-hoc calibration method for improving uncertainty estimates on OOD data, demonstrating consistent improvements over existing benchmarks. While initial reviews raised valid concerns about assumptions, clarity, and baselines, the authors addressed these effectively in their rebuttal. Key strengths include:  \n1. **Empirical Robustness:** The method achieves strong results across diverse corruptions (e.g., noise, blur) and even on entirely OOD datasets (SVHN, Fashion-MNIST), showing generalization beyond the calibration corruption (contrast).  \n2. **Simplicity and Practicality:** The approach is computationally lightweight, requires no retraining, and integrates seamlessly with existing models.  \n3. **Clarity Improvements:** The authors clarified notation, added theoretical intuition, and expanded experiments to address reviewer concerns about generalization and OOD performance.  \n\nWhile Reviewer 1’s concern about the problem setup (assuming a known set of corruptions) remains partially valid, the rebuttal clarifies that the method does not require prior knowledge of test-time corruptions and is evaluated on unseen OOD shifts. The additional experiments on SVHN and Fashion-MNIST strengthen the case for broader applicability.  \n\nThe paper’s contributions—a simple yet effective calibration method with strong empirical results—align with the standards of a top-tier conference. Minor weaknesses in baseline comparisons and theoretical depth are outweighed by the practical impact and methodological novelty."
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes MQES, a new exploration method for continuous RL that combines information-theoretic principles with distributional Q-functions to handle both epistemic and aleatoric uncertainty. The reviewers have a mix of positive and negative points. \n\nStarting with the strengths: The idea seems novel, leveraging mutual information to guide exploration by considering uncertainties. The authors claim improvements over SAC and DSAC in Mujoco tasks, especially in sparse environments. They also provided responses to reviewers, addressing some clarity issues and adding ablation studies.\n\nBut the weaknesses are significant. Reviewers pointed out unclear explanations, especially in the theoretical sections. The notation was confusing, and key equations like Eq. 8 and 9 were hard to follow. The experiments were criticized for being preliminary, lacking comparisons to relevant baselines like OAC and VIME, and insufficient ablation studies. The choice of horizon (100 steps) was questioned, and the empirical results didn't show a significant improvement over DSAC in some tasks. \n\nThe authors' response tried to address these by revising sections, adding experiments, and clarifying theory. However, some issues remain. For example, the explanation of mutual information and posterior distributions still seems vague. The ablation studies, while added, might not fully validate the method's components. Also, the comparison to OAC was only briefly mentioned, and the significance of results in sparse tasks wasn't thoroughly demonstrated with statistical measures like p-values.\n\nConsidering the standards of a top-tier conference, the paper's current state has notable gaps. The theoretical clarity is lacking, which is crucial for acceptance. The empirical evaluation, even with revisions, doesn't convincingly surpass existing methods. While the idea is promising, the execution and presentation need more polish. Therefore, despite the authors' efforts, the paper isn't ready for acceptance yet.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces MQES, a novel information-theoretic exploration strategy for continuous RL, addressing epistemic and aleatoric uncertainty via distributional Q-function approximations. While the core idea is promising, several critical issues prevent acceptance at a top-tier conference:  \n\n1. **Clarity and Theoretical Rigor**:  \n   - The theoretical foundation (Section 4.1) remains unclear despite revisions. Key equations (e.g., mutual information derivations) lack intuitive explanations, and the connection between uncertainty types and exploration is not convincingly justified.  \n   - Notation inconsistencies (e.g., misuse of terms like $Z^*$, $\\pi^*$) and vague definitions (e.g., posterior distributions) persist, hindering reproducibility.  \n\n2. **Empirical Evaluation**:  \n   - **Baseline Comparisons**: The omission of OAC and VIME—key exploration baselines—weakens the empirical claims. Comparisons to SAC/DSAC alone are insufficient to demonstrate broad superiority.  \n   - **Statistical Significance**: Results rely on 5 seeds without p-values or confidence intervals, raising doubts about robustness.  \n   - **Sparse-Reward Justification**: While sparse environments are highlighted, results on standard Mujoco tasks (e.g., Hopper, Walker) show minimal gains over DSAC, undermining the method’s novelty.  \n\n3. **Methodological Concerns**:  \n   - **Horizon Choice**: The use of 100-step episodes (vs. standard 1000-step) is inadequately justified, casting doubt on scalability.  \n   - **Ablation Limitations**: Newly added ablations (e.g., uncertainty separation) are superficial and fail to isolate the impact of core components like mutual information maximization.  \n\n4. **Writing and Presentation**:  \n   - The paper remains dense and jargon-heavy, with poorly motivated terminology (e.g., \"heteroscedastic aleatoric uncertainty\"). Critical sections (e.g., Algorithm 2) are relegated to appendices, limiting accessibility.  \n\nWhile the authors addressed some reviewer concerns (e.g., hyperparameter analysis), unresolved issues in theoretical clarity, empirical rigor, and presentation preclude acceptance. The work shows potential but requires substantial revisions to meet top-tier standards."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question introduces a new algorithm for reducing the number of neurons in deep ReLU networks based on regularization theory. The authors claim their method significantly reduces neurons with minimal impact on accuracy. However, the reviewers have raised several concerns.\n\nFirst, looking at the reviewers' comments, there's a common theme about the paper's clarity and presentation. Reviewers 2, 3, 4, 5, and 6 all mention issues with the writing, organization, and insufficient explanation of key concepts. The authors reference their unpublished work, which makes it hard to verify the theoretical foundations. This is a major red flag because the core theory isn't accessible for peer review. Without that, it's tough to assess the validity of their claims.\n\nNext, the experiments are a big point of contention. The paper uses toy datasets and MNIST, which are not sufficient for a top-tier conference. Reviewers 1, 3, 4, and 5 all point out the lack of comparison with state-of-the-art pruning methods and the need for more complex datasets like CIFAR. The authors responded that they plan to add comparisons with TensorFlow's pruning, but these results aren't included yet. In academic publishing, especially for conferences, the submitted version needs to stand on its own; promised future work doesn't count.\n\nThe methodology's novelty is also questioned. Reviewers note that parts of the algorithm (like pruning always-active neurons) might not be novel. The authors argue their clustering step (step 3) is novel and theory-driven, but without the unpublished theory, it's hard to validate. The connection between their algorithm and the theoretical framework isn't clearly explained in the paper, leading to confusion among reviewers.\n\nAnother issue is the technical presentation. There are typos, unclear definitions (like the P-functional), and insufficient mathematical rigor in presenting key results. For example, Section 4.1's main observation isn't framed as a theorem, which weakens the theoretical contribution. The authors acknowledge some of these issues in their response but haven't addressed them adequately in the current submission.\n\nThe authors' response does address some concerns, like clarifying the architecture differences and promising additional experiments. However, their explanations still rely heavily on the unpublished work, which isn't available for review. They also mention improving the presentation, but the current version hasn't incorporated these changes sufficiently.\n\nConsidering all these factors: the paper's theoretical underpinnings are not verifiable, the experiments are insufficient for a top-tier venue, the novelty is in question without proper comparison to existing methods, and the presentation needs significant improvement. While the idea of a theory-driven pruning algorithm is interesting, the current submission doesn't meet the standards expected at a top conference. The authors need to publish their foundational theory first, conduct more thorough experiments, and improve the clarity of their writing before the work is ready for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel neuron reduction algorithm for ReLU networks with theoretical grounding in regularization. However, significant issues prevent acceptance at a top-tier conference:  \n1. **Theoretical Validity and Clarity**: The core theory relies heavily on unpublished work, making verification impossible. Key sections (e.g., Section 3) lack rigor, with critical claims presented informally rather than as theorems. The connection between the theoretical framework and the proposed algorithm is inadequately explained, leaving reviewers unable to assess correctness.  \n2. **Experimental Insufficiency**: Experiments are limited to toy datasets and MNIST, with no comparison to state-of-the-art pruning methods (e.g., magnitude/gradient-based pruning). While the authors mention plans to add TensorFlow pruning comparisons, these results are absent in the current submission. The lack of statistical significance analysis (e.g., error bars) further weakens empirical validation.  \n3. **Novelty and Positioning**: Reviewers question the novelty of steps 1 and 2 of the algorithm (pruning inactive/always-active neurons), noting similarities to existing work (e.g., highway networks). The authors’ defense of step 3’s uniqueness is undermined by insufficient derivation and reliance on inaccessible theory.  \n4. **Presentation Issues**: The paper suffers from poor organization, undefined terms (e.g., P-functional), and numerous typos. Critical details (e.g., computational complexity, hyperparameters) are omitted, and figures lack clarity.  \n\nWhile the authors’ responses acknowledge weaknesses and propose improvements (e.g., adding experiments, clarifying theory), the current submission does not meet the standards of a top-tier venue. The work requires substantial revisions: publishing foundational theory first, rigorous empirical validation on complex datasets, and clearer exposition."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. The paper presents a new algorithm for pruning ReLU neural networks based on regularization theory. The authors claim their method can reduce neurons significantly with minimal loss in accuracy. But the reviewers have several concerns.\n\nFirst, looking at the reviews, there's a mix of criticisms. Reviewer 1 initially had concerns but retracted some after author responses. However, they still mention the need for more experiments and comparisons with state-of-the-art methods. Reviewer 2 and 3 are more negative, pointing out poor writing, lack of clarity, and insufficient experiments. Reviewer 4 and 5 also highlight issues with presentation, insufficient comparisons, and reliance on unpublished work. Reviewer 6 echoes similar points about unclear contributions and experiments only on simple datasets.\n\nThe authors responded to these critiques by acknowledging the need for clearer explanations, more experiments, and comparisons. They mention plans to add experiments showing their method outperforms TensorFlow's pruning. They also explain their theoretical contributions, emphasizing that their algorithm is theory-driven, unlike heuristic methods. However, they admit that the presentation was lacking and that referencing unpublished work made it hard to validate their claims.\n\nKey issues here are:\n\n1. **Theoretical Foundation**: The paper heavily relies on an unpublished work, making it hard for reviewers to verify the theory. While the authors explain some aspects in their response, the core theory isn't accessible, which is a major problem for validation.\n\n2. **Experiments**: The experiments are limited to toy datasets and MNIST. Top-tier conferences usually expect more robust evaluations, possibly on datasets like CIFAR or ImageNet. The lack of comparison with existing pruning methods (until promised future work) is a significant weakness.\n\n3. **Clarity and Presentation**: Multiple reviewers note the paper is hard to follow, with unclear explanations and typos. The authors agree but haven't fixed these issues in the current submission.\n\n4. **Novelty**: Some reviewers question the novelty, comparing the architecture to Highway Networks. The authors argue differences exist, but without clear distinction in the paper, this remains a concern.\n\nOn the positive side, the method's theoretical grounding could be a strength if properly presented and validated. The idea of clustering neurons based on specific vector representations is interesting and might offer a new angle on pruning. However, the current presentation doesn't effectively communicate these points.\n\nConsidering the standards of a top-tier conference, the paper falls short in several areas. The reliance on unpublished work makes the theory unverifiable, and the experimental validation is insufficient. While the authors address some concerns in their response, the necessary improvements (like more experiments and clearer writing) aren't yet implemented. Without these, the paper's contributions aren't convincingly demonstrated.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a theoretically motivated pruning algorithm for ReLU networks but faces critical shortcomings that preclude acceptance at a top-tier conference:  \n\n1. **Unverifiable Theoretical Claims**: The core theory relies heavily on an unpublished work, making it impossible for reviewers to validate the foundational assumptions or derivations. This undermines the paper's credibility and reproducibility, a critical requirement for impactful research.  \n\n2. **Insufficient Experimental Validation**: Experiments are limited to toy datasets and MNIST, with no comparisons to state-of-the-art pruning methods (e.g., magnitude-based, gradient/Hessian-aware pruning). While the authors promise future comparisons, the current results lack the rigor expected for a top-tier venue.  \n\n3. **Presentation and Clarity Issues**: The paper is criticized for poor organization, unclear explanations, and grammatical errors. Key sections (e.g., the algorithm’s derivation, architecture details) are ambiguously described, hindering comprehension.  \n\n4. **Marginal Novelty**: Reviewers note similarities to existing architectures (e.g., Highway Networks) and question the uniqueness of steps 1 and 2 of the algorithm. The theoretical insights, while potentially valuable, are not effectively communicated or grounded in accessible literature.  \n\n5. **Lack of Statistical Significance**: Results on MNIST show minimal accuracy differences, but no error margins or statistical tests are provided, raising doubts about the method’s robustness.  \n\nWhile the authors’ responses address some concerns (e.g., clarifying vector representations, promising additional experiments), the revisions are not yet reflected in the manuscript. The combination of unverified theory, limited experiments, and unclear presentation renders the paper unsuitable for acceptance in its current form."
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about learning layout representations for graphic design using a transformer-based model called CANVASEMB, pre-trained on a large dataset of over a million slides. The authors claim state-of-the-art results on tasks like element role labeling and image captioning. The reviewers have raised several concerns, and the authors have responded to them. \n\nStarting with the strengths: The construction of a large-scale dataset is a significant contribution, as data scarcity is a common issue in layout learning. Using transformers for layout representation is novel in this context, and the idea of pre-training aligns with current trends in NLP and vision. The authors also propose new tasks for evaluation, which could be useful for the community.\n\nHowever, the reviewers pointed out several weaknesses. The evaluation is considered weak. The tasks like element role labeling and image captioning are compared mainly against a decision-tree baseline, which isn't sufficient. There's a lack of comparison with existing works like Cao et al. (SIGGRAPH 2019), Li et al. (2020b), and READ (CVPR 2020). These omissions make it hard to assess the true advancement of the method. Reviewer 3 also mentions that the layout auto-completion task is simplified compared to prior work, which might overstate the model's capabilities.\n\nAnother major issue is the motivation for using transformers over other architectures like GNNs. The authors didn't adequately justify why transformers are better suited here, especially since GNNs are known for handling structural data efficiently. The dataset details are also lacking—statistics like the number of elements per slide and parsing methods are crucial for reproducibility and understanding the model's input.\n\nThe authors' response addresses some concerns by promising to add more comparisons with existing works, include more dataset details, and refine evaluations. However, these are future promises rather than present improvements. The current version still lacks these critical analyses, which are essential for a top-tier conference. \n\nMoreover, the clarity of the method section is questioned. Reviewers wanted more details on the transformer architecture and how it's adapted for layout data. The authors mention adding these in the next version, but as it stands, the technical depth is insufficient.\n\nThe reviewers also noted that some tasks, like image captioning, are framed as binary classification, which might not capture the complexity implied by the term \"captioning.\" This could be misleading and needs clarification.\n\nIn terms of significance, while layout representation is important, the paper's impact is limited by the evaluation's narrow scope. Without thorough comparisons and more challenging tasks, the claimed SOTA results are not convincingly supported.\n\nConsidering all this, the paper has potential but falls short in its current form. The lack of comprehensive evaluations, insufficient comparisons with relevant baselines, and unresolved questions about architectural choices are major drawbacks. For a top-tier conference, these issues are critical and would require substantial revisions. Therefore, despite the promising approach and dataset contribution, the paper isn't ready for acceptance yet.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces a novel transformer-based approach (CANVASEMB) for layout representation learning and contributes a large-scale dataset of presentation slides. While the dataset and pre-training framework are promising, the paper falls short of the standards for a top-tier conference due to critical weaknesses in evaluation, motivation, and technical clarity:  \n\n1. **Insufficient Evaluation:**  \n   - The proposed tasks (element role labeling, image captioning) are compared primarily against a simple decision-tree baseline, failing to benchmark against relevant state-of-the-art methods (e.g., Cao et al. 2019, READ CVPR 2020).  \n   - The \"image captioning\" task is framed as binary classification, which misrepresents its complexity and limits its significance.  \n   - Layout auto-completion and retrieval evaluations lack rigor (e.g., simplified masking protocols, ad-hoc human scoring), weakening claims of generalizability.  \n\n2. **Motivation and Technical Justification:**  \n   - The choice of transformers over graph neural networks (GNNs) or other architectures is not adequately justified. GNNs, which naturally model structural relationships, are dismissed without empirical comparison, undermining the rationale for the transformer-based approach.  \n   - Key dataset statistics (e.g., element count distribution, parsing methodology) are omitted, raising concerns about reproducibility and dataset quality.  \n\n3. **Clarity and Comparison to Prior Work:**  \n   - The technical description of the transformer architecture is sparse, and the \"Related Work\" section inadequately differentiates the contribution from prior layout generation/representation methods.  \n   - While the authors promise to address these gaps (e.g., adding baseline comparisons, refining evaluations), the current version lacks these critical analyses, which are essential for assessing novelty and impact.  \n\n4. **Significance and Impact:**  \n   - Although the large-scale dataset is valuable, the paper does not demonstrate that CANVASEMB enables breakthroughs in downstream applications beyond incremental improvements on narrowly defined tasks.  \n\n**Conclusion:**  \nWhile the work has potential, the unresolved methodological and evaluative shortcomings prevent it from meeting the high bar of a top-tier conference. The authors should address the reviewers’ concerns (e.g., comprehensive baselines, rigorous task design, architectural justification) in a substantial revision before resubmission."
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about a new GNN model called GNAN that uses global self-attention with learnable spectral filters to handle both assortative and disassortative graphs. The main claim is that it outperforms existing methods on disassortative graphs while being competitive on assortative ones. \n\nLooking at the reviews, Reviewer 1 and 4 have significant concerns about novelty and complexity. They argue that existing methods like ChevNet can already capture global information with large K, and the computational complexity of GNAN's approach might be too high. Reviewer 1 also questions whether the non-local attention is truly novel compared to prior work like Non-Local GNNs. The authors responded by explaining that ChevNet's polynomial filters are still localized and rely on homophily assumptions, while their method uses adaptive spectral filters via MLP, allowing more flexibility. They also addressed complexity by explaining Chebyshev polynomial approximations and sparsification techniques, providing runtime comparisons showing GNAN is competitive.\n\nReviewer 3 and 2 are more positive. Reviewer 3 appreciates the thorough experiments and motivation but raises scalability and dataset size issues. The authors added experiments on larger datasets like Chameleon and discussed sparsification's impact on runtime. Reviewer 2 found the paper clear and valuable despite lacking theoretical results.\n\nThe key points of contention are novelty and scalability. The authors argue their method's novelty lies in learnable spectral filters, differentiating from ChevNet and others. They also tackle complexity through approximations and sparsification, showing empirical runtime improvements. However, the reviewers remain skeptical about whether these contributions are sufficient for a top-tier conference. The lack of theoretical analysis is a weakness, but the empirical results are strong, especially on disassortative graphs.\n\nThe authors addressed most concerns in their response, but some doubts linger, especially regarding whether the complexity is manageable for larger graphs and if the novelty is enough. However, the practical improvements on challenging datasets and the thorough experimentation might outweigh these concerns. Given the positive aspects and the authors' responses, the paper might be borderline but could be accepted if the committee values the empirical results and the approach's potential impact on handling heterophily in GNNs.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel GNN architecture (GNAN) that addresses the critical challenge of generalizing to disassortative graphs by leveraging adaptive spectral filters and global attention. While concerns about novelty and computational complexity were raised, the authors provided compelling rebuttals:  \n1. **Novelty**: The distinction from ChevNet and prior work lies in the *learnable* spectral filters, which adaptively combine low- and high-frequency components without homophily assumptions. This addresses a key limitation of existing methods and is a meaningful contribution.  \n2. **Complexity**: The authors clarified that Chebyshev polynomial approximations and sparsification techniques reduce complexity to \\(O(m \\times |E|)\\), with empirical runtime comparisons showing competitive performance against baselines (e.g., GNAN-K runs faster than GAT on some datasets).  \n3. **Experiments**: Expanded evaluations on larger disassortative graphs (e.g., Chameleon) and runtime analysis strengthen the empirical validation. While theoretical guarantees are absent, the results demonstrate clear practical advantages on heterophilic graphs, a significant pain point in GNN research.  \n\nReviewer criticisms were largely mitigated, and the paper’s strengths—methodological innovation, thorough experimentation, and relevance to an open problem—outweigh its limitations. The work advances the field by offering a flexible, scalable solution for graphs beyond local homophily, aligning with the standards of a top-tier conference."
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. So, the paper is about applying boosting techniques from supervised learning to reinforcement learning (RL), specifically in Markov Decision Processes (MDPs) with large state spaces. The main claim is that their algorithm's sample complexity and running time don't depend on the number of states, which is a big deal because RL typically struggles with scalability.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns but upgraded to a 5 after author responses. Reviewer 2 and 5 point out issues with novelty, clarity, and lack of experiments. Reviewer 3 is more positive, highlighting the technical contribution despite some weaknesses. Reviewer 4 is critical about the paper's clarity and undefined notations. Reviewer 6 acknowledges the interesting approach but notes the paper's density and lack of motivation for definitions.\n\nThe authors responded to these concerns by agreeing to clarify notations, add a notation list, and provide more intuition in the final version. They also defended their technical novelty by differentiating their work from prior research, like Hazan and Singh 2021, by handling non-convex cases. However, they didn't address the lack of experiments, which multiple reviewers mentioned as a weakness.\n\nThe key strengths are the theoretical contribution of applying boosting to RL with state-independent complexity bounds and overcoming non-convexity using Frank-Wolfe. The weaknesses are mainly around clarity, presentation, and the absence of empirical validation. The technical novelty is somewhat debated, with some reviewers seeing it as incremental, but others recognizing the adaptation to non-convex settings as a significant step.\n\nFor a top-tier conference, clarity and presentation are crucial. The paper's current state has significant issues with notation and readability, which could hinder its acceptance. However, the theoretical contribution seems solid, and the authors have committed to improving clarity. The lack of experiments is a drawback, but since the focus is theoretical, maybe that's acceptable. The split in reviewer opinions (some weak accepts, some rejects) makes it a borderline case. Considering the potential impact of the theoretical advancement and the authors' willingness to revise, leaning towards a weak accept. But given the clarity issues and some concerns about incremental contribution, maybe rejection is warranted. However, top conferences sometimes accept papers with strong theoretical contributions even if they're dense, provided the core idea is novel and impactful. The authors' response addressed some concerns, but not all. Given the mixed reviews and the importance of the theoretical result, I think it's a borderline case but might lean towards acceptance with the expectation that the authors will make the necessary revisions.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a theoretically grounded approach to applying boosting in RL with a state-independent sample complexity, which is a notable contribution. However, significant issues hinder its readiness for a top-tier conference:  \n1. **Clarity and Readability:** Multiple reviewers highlighted severe notation overload, undefined terms, and a lack of intuitive explanations. While the authors promised revisions, the current manuscript is too dense and inaccessible, even for expert readers.  \n2. **Novelty Concerns:** The technical core was critiqued as incremental compared to prior work (e.g., Hazan & Singh 2021), particularly regarding the weak learner assumptions and non-convex optimization. The rebuttal emphasizes differences in handling non-convexity but does not fully dispel concerns about foundational novelty.  \n3. **Lack of Empirical Validation:** Despite being theory-focused, the absence of even simple experiments (as in related works) weakens the case for practical relevance and leaves the algorithm’s efficacy unverified.  \n4. **Assumption Justification:** Key assumptions (e.g., weak learner guarantees, distribution mismatch bounds) were questioned, with insufficient discussion of their practicality or limitations.  \n\nWhile the theoretical direction is promising, the unresolved clarity issues, marginal novelty, and lack of empirical grounding make the paper unsuitable for acceptance in its current form. Top-tier conferences demand both technical rigor and accessibility, which this submission has not yet achieved."
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about using Fourier convolutional networks for 3D snapshot microscopy. The main idea is that traditional UNet-based decoders can't handle the non-local optical encoding required for this task, so they propose a Fourier-based approach that's more efficient and effective. They show results in simulation and on a lensless imaging dataset.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns but upgraded to a 6 after the authors addressed some points. Reviewer 2 is positive. Reviewer 3 is on the fence, noting that the machine learning ideas aren't novel but the application is solid. They suggest it might be better suited for an application-specific conference. Reviewers 4 and 5 point out issues with technical novelty, especially citing Rippel et al. (2015) which already explored Fourier domain convolutions. They also question the evaluation's comprehensiveness, like the lack of real-world data and limited testing on a single dataset. Reviewer 6 and 7 are more positive about the application's potential but still note the technical contributions might not be groundbreaking.\n\nThe authors responded by clarifying that their main contribution is the application of FourierNets to enable end-to-end optimization of non-local optical encoders, not the Fourier method itself. They added citations to prior work like Rippel and addressed some technical concerns, like parameter counts and training times. They also defended their evaluation scope, explaining the challenges in obtaining real data and the significance of their simulated results.\n\nNow, considering this is a top-tier conference, the key factors are technical novelty, empirical validation, and impact. The technical aspect seems incremental over existing Fourier-based methods, as multiple reviewers noted. The application to 3D microscopy is novel, but the core ML technique isn't. The empirical results are strong in simulation but lack real-world validation, which is a significant limitation. However, the authors argue that simulation is necessary for PSF optimization, which is a fair point. The lensless imaging results add some external validity but aren't the paper's main focus.\n\nThe reviewers' main concerns are about novelty and evaluation. While the authors addressed some points, the technical contribution is still seen as building on prior work. For a top-tier venue like ICLR, which emphasizes ML innovations, this might not be enough unless the application is groundbreaking. The application here is important but perhaps more suited to a specialized venue. However, the thoroughness in addressing the problem and the potential impact in computational imaging could justify acceptance if the application's novelty is deemed sufficient.\n\nBalancing all this, the paper presents a solid application with thorough methodology and addresses a real problem in computational microscopy. However, the technical novelty is limited, and the lack of real-world experiments might be a drawback. Given that some reviewers upgraded their scores post-rebuttal and the authors addressed key concerns, but others still question the novelty, it's a borderline case. If the conference values application-driven research with solid execution, it might lean towards acceptance. But if the emphasis is on ML innovation, it might not make the cut.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a well-executed application of Fourier convolutional networks to 3D snapshot microscopy, addressing critical bottlenecks in end-to-end optimization of non-local optical encoders. While technical novelty in machine learning methodology is limited (as prior work like Rippel et al. 2015 explored Fourier-domain convolutions), the authors make significant contributions in the following areas:  \n1. **Application-Specific Innovation**: The integration of FourierNets with differentiable wave-optics simulations enables optimization of highly non-local point spread functions (PSFs) for programmable microscopy, a novel and impactful advancement in computational imaging.  \n2. **Empirical Rigor**: The authors provide extensive simulations on a unique zebrafish dataset and demonstrate superior performance over UNet-based methods in both PSF optimization and reconstruction tasks. While real-world validation is absent, the simulation-based approach is justified for PSF design, and the lensless imaging experiments (on real data) further validate the method’s generalizability.  \n3. **Clarity and Scope**: The paper is well-written, addresses limitations transparently, and responds thoroughly to reviewer concerns (e.g., parameter counts, training times, and citations to prior work).  \n\nThough the technical core builds on existing ideas, the work’s value lies in its interdisciplinary impact, bridging ML, optics, and neuroscience. The rebuttal effectively addressed concerns about novelty and evaluation, and the majority of reviewers (including those initially skeptical) upgraded their ratings post-rebuttal. For a top-tier conference like ICLR, which encourages impactful applications of ML, this paper’s contributions justify acceptance despite incremental methodological advances."
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper presents a new variant of Target Propagation (TP) using regularized inversion, aiming to make TP more practical by leveraging differentiable programming frameworks. The abstract mentions comparing computational complexity with backpropagation (BP) and applying TP to train RNNs on sequence problems. The reviewers have mixed opinions, but there's a trend towards concerns about novelty, empirical evidence, and theoretical grounding.\n\nReviewer 1 is critical, pointing out that the proposed TP variant loses some key advantages of traditional TP, like avoiding weight transport, which is important for neuromorphic hardware and distributed systems. They also question the computational cost compared to BP and the lack of theoretical guarantees for the descent direction. The experiments aren't convincing enough for them.\n\nReviewer 2 is more positive, highlighting the algorithm's novelty and the use of autograd but notes the empirical results are a bit confusing and there's a lack of convergence analysis. They still think the approach is novel and tractable.\n\nReviewer 3 and 4 both point out weaknesses in the experimental section. Reviewer 3 mentions that the experiments on image datasets (like CIFAR) have low accuracy, making it hard to assess the method's potential. They also suggest testing on more established models. Reviewer 4 wants more extensive experiments and comparisons with other optimization methods for RNNs, like Hessian-Free Optimization. They also feel the theory isn't fully fleshed out, especially the connection to Gauss-Newton methods.\n\nThe authors didn't provide a response, so there's no rebuttal to address these concerns. \n\nLooking at the common themes: the technical novelty is questioned by multiple reviewers (scores 2-3), empirical results are seen as insufficient or not convincing (scores 2-3), and there's a lack of thorough theoretical analysis. The method's practicality is doubted because it requires weight transport and is computationally more expensive than BP. The experiments don't show clear superiority over BP, especially on realistic tasks. \n\nFor a top-tier conference, the paper needs strong novelty, solid theoretical contributions, and convincing empirical results. The reviewers agree that while the idea is interesting, the execution falls short. The lack of author response to address these points makes it harder to justify acceptance. The paper might be a good workshop candidate but doesn't meet the high bar of a top-tier conference without addressing the raised issues.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes a novel Target Propagation (TP) variant using regularized inversion, aiming to improve computational tractability and integration with autograd frameworks. While the idea is conceptually interesting, the reviews highlight critical shortcomings that undermine its suitability for a top-tier conference:  \n\n1. **Limited Novelty and Theoretical Grounding:**  \n   - The technical contributions are perceived as incremental, with reviewers noting similarities to prior TP work (e.g., Lee et al., 2015; Meulemans et al., 2020). The theoretical analysis (e.g., Lemma 3.1) is criticized for being underdeveloped, with insufficient discussion of how TP differs from or improves upon backpropagation (BP). The connection to Gauss-Newton methods is also under-explored, weakening the theoretical rationale.  \n\n2. **Weak Empirical Validation:**  \n   - Experiments lack depth and fail to convincingly demonstrate superiority over BP. Performance on image benchmarks (e.g., CIFAR-10) is notably poor (accuracy <20%), raising doubts about practical utility. Comparisons with alternative RNN optimization methods (e.g., Hessian-Free Optimization) are absent, limiting the paper’s relevance to the broader literature.  \n\n3. **Practical Limitations:**  \n   - The proposed TP variant requires weight transport, negating a key advantage of classical TP for neuromorphic hardware and distributed systems. Its higher computational cost compared to BP is acknowledged but not justified by significant empirical gains.  \n\n4. **Unaddressed Concerns:**  \n   - Critical questions from reviewers (e.g., handling non-unique minimizers in ReLU inverses, vanishing/exploding gradients in TP, convergence guarantees) remain unanswered due to the absence of an author response.  \n\nWhile the paper introduces a potentially useful implementation of TP, it does not meet the high bar for technical novelty, theoretical rigor, or empirical impact expected at a top-tier venue. Addressing these issues would require substantial revisions and additional experimentation."
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. The paper is about understanding the implicit biases in neural networks, specifically looking at how architecture and gradient descent contribute to generalization. The authors present theoretical results on the average test error of NNGP and argue that gradient descent improves generalization by selecting functions with larger margins. \n\nLooking at the reviews, there's a mix of opinions. Some reviewers acknowledge the theoretical contributions, especially the new PAC-Bayes bounds and the analysis of NNGP. However, several concerns are raised. The main issues seem to be around the experimental validation and the claims about gradient descent's implicit bias towards large margins. \n\nReviewer 1 and others point out that the experiments in Section 5 are not convincing. The bounds derived are loose, and the comparison with prior work like Valle-Pérez et al. is lacking. There's also criticism about the use of the Nero optimizer instead of standard GD, which might affect the validity of the results. Reviewer 7 mentions that the experiments are limited to specific settings and don't adequately compare different architectures or optimization methods. \n\nThe authors' response clarifies some misunderstandings, like the difference between their approach and existing NNGP literature. They emphasize that their focus is on how large margins improve generalization for both NNGP and GD-trained networks. However, some reviewers remain unconvinced, pointing out that the empirical evidence is still insufficient, especially regarding the role of GD and the applicability to different architectures.\n\nThe technical novelty of the bounds is recognized, but their practical significance is questioned because they don't apply to CNNs and the experiments are on simple setups. The margin analysis is seen as underdeveloped, with some reviewers noting that the connection to GD's implicit bias isn't rigorously established. \n\nConsidering the strengths and weaknesses, the paper does present some novel theoretical tools and insights, which are valuable. However, the experimental section's limitations and the unresolved concerns about the margin analysis and optimization methods make the contribution borderline. Top-tier conferences usually require both theoretical and empirical rigor. While the theoretical part is solid, the empirical validation doesn't fully support the claims, and the rebuttal didn't fully address all concerns. \n\nGiven the mixed reviews and the fact that even some reviewers who increased their scores still have reservations, the paper might not meet the high bar for acceptance. The consensus leans towards the paper being premature, needing more thorough experiments and clearer connections to existing literature.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a nuanced investigation into the implicit biases of neural network architectures and gradient descent (GD), offering theoretical contributions such as analytical bounds on the average test error of NNGP posteriors and insights into margin-based generalization. However, significant concerns raised by reviewers undermine its readiness for publication in a top-tier venue:\n\n1. **Weak Empirical Validation**:  \n   - The experimental results, particularly in Section 5, are criticized for being limited to specific settings (e.g., MLPs, the Nero optimizer) and lacking comparisons to prior work (e.g., Valle-Pérez et al.). The use of Nero, a non-standard optimizer with norm constraints, complicates claims about GD’s implicit bias, as its behavior may not generalize to vanilla GD or SGD.  \n   - The bounds derived for NNGP are acknowledged as non-vacuous but are deemed loose and of limited practical significance. Experiments fail to convincingly demonstrate that the gap between bounds and empirical performance is due to GD’s margin-seeking behavior rather than slack in the bounds themselves.  \n\n2. **Overstated Claims About Margin and GD**:  \n   - The analysis of GD’s implicit bias toward large margins is criticized as underdeveloped. While the authors argue that GD selects functions with larger margins, reviewers note that this connection is not rigorously established (e.g., no theoretical guarantees for GD with squared loss, limited empirical evidence across architectures/datasets). The analogy between NNGP posterior concentration and GD’s behavior is seen as speculative, especially given conflicting results in prior work (e.g., NTK underperformance in CNNs).  \n\n3. **Limited Scope and Incremental Contributions**:  \n   - The theoretical tools, while novel, are restricted to MLPs and do not extend to architectures like CNNs, reducing their broader impact. Comparisons to alternative methods (e.g., Monte Carlo sampling for NNGP error estimation) are missing, leaving the practical utility of the proposed bounds unclear.  \n   - The work builds heavily on Valle-Pérez et al. (2019), with reviewers arguing that improvements are incremental rather than transformative.  \n\n4. **Reviewer Consensus**:  \n   - While some reviewers raised their scores post-rebuttal, lingering doubts about experimental rigor, clarity of claims, and relevance to real-world architectures (e.g., CNNs) persist. The paper’s central theme—disentangling architectural and optimization biases—is compelling but insufficiently supported by evidence.  \n\n**Conclusion**:  \nThough the paper introduces valuable theoretical tools and raises important questions, the unresolved methodological and empirical shortcomings prevent it from meeting the high bar of a top-tier conference. A more focused analysis of architectural bias, coupled with comprehensive experiments across architectures and optimization methods, would strengthen its contribution."
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about the theoretical analysis of the Feedback Alignment (FA) algorithm in deep linear networks. The authors provide convergence guarantees and study implicit regularization and anti-regularization phenomena under specific initializations. The reviewers have mixed opinions, with some praising the theoretical contributions and others concerned about the practical significance and assumptions made.\n\nReviewer 1 is concerned that the one-dimensional dynamics might oversimplify the problem and questions the novelty. They also mention that the convergence for deeper networks is only proven under special initialization. Reviewer 2 points out that the results are under strong conditions, like co-diagonalizable matrices, and the focus on linear networks leaves a gap with real-world applications. Reviewer 3 is more positive, highlighting the interesting findings on implicit bias and the clear structure of the paper. Reviewer 4 acknowledges the technical correctness but worries about the significance due to the restrictive spectral initialization and suggests more experiments with non-spectral cases.\n\nThe authors responded by clarifying their assumptions, adding more discussions on spectral initialization, including new figures to illustrate dynamics, and providing additional proofs in the appendix. They argue that even with non-spectral initializations, some phenomena like incremental learning can be observed, and they address concerns about alignment and initialization schemes.\n\nNow, considering the strengths and weaknesses: The paper provides rigorous theoretical analysis in a simplified setting (linear networks), which is a common approach in the field to build foundational understanding. The identification of implicit anti-regularization is novel and could have broader implications. However, the reliance on specific initializations (spectral) and the gap to practical scenarios (non-linear networks, random initializations) are valid concerns. The authors did add some experiments with non-spectral cases, but the main theoretical results still hinge on those assumptions.\n\nThe technical correctness is confirmed by all reviewers, which is a strong point. The novelty is debated, but Reviewer 3 and the authors' responses suggest that the findings on implicit bias are significant. The paper's contribution to understanding FA's dynamics, even in linear cases, adds value to the literature, especially since FA is an alternative to backpropagation that's not fully understood.\n\nBalancing these aspects, the paper makes a solid theoretical contribution within its scope. While the assumptions limit immediate practical application, they are necessary for tractable analysis. The community interested in optimization dynamics and alternative training algorithms would benefit from these insights. The authors addressed the reviewers' concerns adequately, though not all issues (like full practical relevance) can be resolved in a single study. Given the theoretical rigor and novel findings, the paper seems suitable for acceptance, especially in a top-tier conference that values theoretical advancements.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper provides a rigorous theoretical analysis of Feedback Alignment (FA) in deep linear networks, offering convergence guarantees and uncovering novel phenomena like implicit anti-regularization. While reviewers raised valid concerns about the restrictive assumptions (e.g., spectral initialization) and the gap to practical non-linear networks, the authors addressed these points effectively in their response:  \n1. **Technical Correctness:** All reviewers confirmed the proofs and claims are correct.  \n2. **Novelty:** The identification of implicit anti-regularization and its dependence on initialization is a significant theoretical contribution, with potential broader implications for understanding optimization dynamics.  \n3. **Relevance:** FA is an active area of research, and this work fills a gap in theoretical understanding, even if limited to linear networks. The analysis aligns with prior work on linear networks as a foundational step.  \n4. **Revisions:** The authors clarified assumptions, added experiments with non-spectral initializations, and provided additional proofs (e.g., for deep networks with independent layer initializations), strengthening the paper’s robustness.  \n\nWhile the practical impact of the results is limited by the linear setting, the paper advances theoretical understanding of FA and offers insights into implicit regularization/anti-regularization. The work meets the standards of a top-tier conference, balancing technical depth with clear exposition."
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question addresses the issue of greedy learning in multi-modal deep neural networks, where models tend to rely too much on one modality and under-utilize others. The authors propose a metric called conditional utilization rate and an efficient proxy, conditional learning speed, to measure this imbalance. They then introduce a training algorithm to balance the learning across modalities and validate it on three datasets.\n\nLooking at the reviews, there's a mix of opinions. Reviewers 1, 2, 3, 4, and 6 have significant concerns. The main issues raised include insufficient experimental validation, especially on datasets with natural modalities (like audio-visual), lack of comparison with state-of-the-art methods, unclear explanations of the re-balancing step, and weak theoretical support. Reviewer 4 points out that the main claims about generalization aren't strongly supported by the experiments, especially since some results aren't statistically significant. Reviewer 6 mentions missing comparisons with existing methods and the need for more parameter analysis. Reviewer 5 and 7 are more positive, noting the structured approach and empirical results but still highlighting areas needing clarification.\n\nThe authors responded by adding experiments with RUBi, hyperparameter studies, and expanding related work. They addressed some clarity issues and provided additional explanations. However, key concerns remain. For instance, the datasets used are still primarily visual, and comparisons with other methods like those mentioned by the reviewers (e.g., RUBi) show mixed results. The theoretical foundation is still mostly empirical, and some explanations in the appendix remain unclear despite revisions.\n\nThe core issue is whether the paper's contributions are significant and well-supported enough for a top-tier conference. While the problem is interesting and the approach has merit, the validation seems limited. The experiments on Colored MNIST show clear improvements, but the other datasets don't demonstrate strong statistical significance. The lack of natural multi-modal datasets (e.g., audio-visual) is a drawback, as it limits the generalizability of the findings. Additionally, the comparisons with existing methods are either missing or show minimal improvement, which weakens the case for the proposed method's novelty and effectiveness.\n\nThe authors' responses do address some concerns, like adding hyperparameter studies and RUBi comparisons, but these additions don't fully resolve the issues. The theoretical explanation remains weak, relying heavily on empirical observations without deeper analysis. For a top-tier conference, the paper needs stronger theoretical grounding, more comprehensive experiments across diverse modalities, and clearer superiority over existing methods. Given the mixed reviews and remaining gaps, especially in empirical validation and theoretical support, the paper doesn't meet the high bar required for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem in multi-modal learning—greedy modality utilization—and proposes a novel training algorithm to mitigate it. While the topic is timely and the core idea (conditional learning speed as a proxy for modality utilization) is promising, the paper faces critical shortcomings that undermine its suitability for a top-tier conference:  \n\n1. **Weak Empirical Validation**:  \n   - The experiments primarily rely on synthetic or vision-only datasets (e.g., Colored MNIST), which lack the complexity of natural multi-modal tasks (e.g., audio-visual). Results on real-world datasets (ModelNet40, NVGesture) show marginal or statistically insignificant improvements, raising doubts about the method’s generalizability.  \n   - Comparisons with state-of-the-art methods (e.g., RUBi, [1,2,3] cited by reviewers) are either incomplete or show minimal gains, failing to demonstrate clear superiority.  \n\n2. **Theoretical and Technical Gaps**:  \n   - The connection between conditional learning speed and utilization rate lacks rigorous theoretical justification. Reviewers noted that the proxy metric’s validity is not fully established, and the re-balancing step’s mechanism remains under-explained.  \n   - The method’s sensitivity to hyperparameters (e.g., α, Q) is only briefly addressed in the appendix, leaving practical deployment concerns unresolved.  \n\n3. **Insufficient Clarity and Scope**:  \n   - Key sections (e.g., deriving uni-modal outputs, re-balancing logic) are ambiguously described, even after revisions.  \n   - The focus on vision-only modalities limits the work’s applicability to broader multi-modal scenarios, a point emphasized by multiple reviewers.  \n\nWhile the authors made efforts to address reviewer concerns (e.g., adding RUBi comparisons, hyperparameter studies), the revisions do not sufficiently strengthen the paper’s core claims or empirical rigor. For a top-tier venue, the paper requires stronger theoretical grounding, broader experimentation (including natural modalities), and clearer benchmarks against existing methods. As it stands, the contributions are incremental and inadequately validated."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper presents AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The main contributions seem to be the software platform itself, a new strategy called Believer-Penalizer (BP) for asynchronous batch optimization, and a GUI for ease of use. \n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 and 4 are more critical, pointing out that the technical novelty is limited. They mention that BP isn't sufficiently justified theoretically and that the empirical results aren't clearly superior. Reviewer 2 and 3 also question the methodological contribution, seeing it more as a software tool than a scientific paper. Reviewer 5 is more positive, appreciating the problem motivation and the platform's usability but notes some minor issues.\n\nThe authors responded by clarifying their contributions, emphasizing the engineering aspects and the GUI's importance. They addressed concerns about the number of objectives by extending support to single and many-objective cases. They also adjusted their claims about BP's performance, acknowledging it's more about robustness than state-of-the-art. They added more comparisons with other platforms and revised the paper based on feedback.\n\nHowever, the core issue seems to be whether the paper's contributions are sufficient for a top-tier conference. The main technical contribution, BP, is seen as a minor innovation with limited empirical advantage. The platform's engineering and usability aspects are strengths, but conferences like ICLR might prioritize algorithmic novelty over system building unless the system itself is groundbreaking. \n\nThe reviewers' scores are mostly around 3 for technical and empirical novelty, which suggests the paper is borderline. The authors did address some concerns, but the lack of strong theoretical backing for BP and mixed empirical results might still be a problem. Also, the fact that another arXiv paper with the same title exists (Tian et al., 2021) but wasn't cited initially is a red flag, though the authors might have addressed this in revisions.\n\nIn the end, while the platform is useful and well-engineered, the scientific contribution might not meet the high bar for acceptance at a top-tier conference. The paper's strengths are in its practical tool and modular design, but the novelty and empirical evidence for the main technical contribution (BP) are not strong enough. Therefore, despite the positive aspects, the paper might be better suited for a venue that values software tools more than theoretical advances.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents AutoOED, a platform for multi-objective Bayesian optimization (MOBO) with a focus on usability, modularity, and asynchronous batch optimization. While the platform demonstrates engineering rigor and practical utility, its suitability for a top-tier conference hinges on balancing technical novelty, empirical validation, and alignment with conference standards.\n\n1. **Technical Novelty**:  \n   - The primary methodological contribution, the Believer-Penalizer (BP) strategy, is critiqued for lacking theoretical grounding and clear empirical superiority. While BP combines existing ideas (Kriging Believer and Local Penalization), reviewers note that its formulation is intuitive but not rigorously justified. The authors revised claims to position BP as a \"robust\" rather than \"state-of-the-art\" method, but this adjustment does not fully address concerns about its novelty.  \n   - The platform’s modular design and GUI are strengths, but these are engineering contributions. While valuable, top-tier conferences like ICLR prioritize algorithmic or theoretical advances unless the system itself introduces transformative capabilities. AutoOED’s differentiation from existing tools (e.g., BoTorch, Ax) lies in usability and multi-objective support, but reviewers argue these features are incremental rather than groundbreaking.\n\n2. **Empirical Validation**:  \n   - Results for BP show mixed performance (e.g., underperforming on DTLZ3 in Figure 4), and comparisons with other platforms (Figure 5) reveal AutoOED is not consistently superior. The authors’ emphasis on \"robustness over state-of-the-art\" weakens the case for empirical significance.  \n   - The real-world hardware experiment, while a proof of concept, lacks comparison to baselines (e.g., random search is the only benchmark), limiting its impact.  \n\n3. **Alignment with Conference Standards**:  \n   - ICLR accepts system papers, but they must demonstrate significant methodological or practical impact. AutoOED’s usability and modularity are commendable, but reviewers question whether these contributions are sufficient for a research-focused venue. The authors’ focus on \"bridging the gap between research and application\" is compelling but may align better with applied or interdisciplinary conferences.  \n   - The failure to cite a closely related arXiv paper (Tian et al., 2021) initially and the limited discussion of prior work (e.g., many BO tools omitted in comparisons) weaken scholarly rigor.\n\n4. **Author Responses**:  \n   - The authors addressed several concerns: extending support to single/many objectives, revising BP’s claims, and adding comparisons. However, core issues—BP’s theoretical limitations, overstated empirical claims, and marginal novelty—remain unresolved.  \n\n**Final Decision**: Reject  \n\nWhile AutoOED is a well-engineered platform with practical value, the paper’s technical and empirical contributions fall short of the high bar for acceptance at a top-tier conference. The lack of theoretical grounding for BP, mixed empirical results, and incremental system improvements make it more suitable for a venue emphasizing software tools or applied engineering."
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's contributions. The main claim is that they can characterize all globally optimal solutions for two-layer ReLU networks via a convex optimization program. This builds on prior work by Pilanci and Ergen but extends it by fully characterizing all optimal solutions rather than just finding one. That seems like a solid theoretical advancement.\n\nReviewer 1 points out some minor issues with the paper's technical details, like typos and notation inconsistencies, but overall acknowledges the significance of the results. They gave a 3 for correctness and technical novelty, which suggests some reservations but not major flaws. The authors addressed these typos in their response, which probably resolves those concerns.\n\nReviewer 2 is very positive, calling it the best submission they've reviewed and recommending acceptance. They mention the novelty of connecting non-convex landscapes to convex optimization, which could have broader implications. Their main questions are about extensions to other activations and deeper networks, which the authors addressed by pointing to related work and future directions. The reviewer's high correctness score (4) indicates trust in the results.\n\nReviewer 3 also acknowledges the contributions but questions the technical novelty compared to prior work. They note missing references and the lack of empirical validation. The authors responded by adding references and discussing extensions, but the absence of experiments might be a weakness. However, since this is a theoretical paper, empirical results might not be as critical. The reviewer's scores are middling but not dismissive.\n\nReviewer 4 is strongly in favor, highlighting the complete characterization of global minima and the algorithm for testing optimality. They mention the limitation to two-layer networks but still recommend acceptance. Their high scores (4 in technical novelty) suggest they see the work as a significant step forward.\n\nThe common thread among reviewers is that the paper makes important theoretical contributions, especially in extending previous convex formulations to fully characterize solutions. The weaknesses are mainly about scope (only two-layer, ReLU) and some technical presentation issues, which the authors addressed. The lack of experiments is noted, but given the theoretical focus, this might be acceptable. The authors' responses seem to adequately address the reviewers' concerns, clarifying definitions and fixing typos. \n\nConsidering the paper's impact on understanding neural network optimization landscapes through convex analysis, and the positive reception from most reviewers despite minor issues, the strengths outweigh the weaknesses. The work provides a foundation for future research, even if it's limited to two-layer networks. Therefore, acceptance seems justified.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents significant theoretical advancements in understanding the optimization landscape of two-layer ReLU networks through a convex optimization lens. Key strengths include:  \n1. **Novel Characterization of Global Optima:** The work extends prior results (e.g., Pilanci & Ergen 2020) by fully characterizing *all* global optima via a convex program, resolving a critical gap in the literature.  \n2. **Algorithmic Contributions:** A polynomial-time algorithm for verifying global optimality and the construction of a continuous path to the global minimum are impactful tools for both theory and practice.  \n3. **Clarity and Scope:** The analysis is rigorous and provides new insights into invariant transformations, spurious valleys, and the role of network width.  \n\n**Addressing Weaknesses:**  \n- While the work is limited to two-layer ReLU networks, the authors acknowledge this and discuss potential extensions (e.g., deeper networks, other activations) in their response, aligning with Reviewer 2’s suggestions.  \n- Minor technical issues (e.g., typos, notation inconsistencies) were resolved in the rebuttal, and missing references were incorporated.  \n- The lack of empirical validation is noted but is less critical given the paper’s theoretical focus.  \n\n**Reviewer Consensus:**  \nReviewers unanimously recognized the paper’s significance, with three explicitly recommending acceptance. Criticisms were largely addressed, and the core contributions—novel theoretical tools and a unifying convex framework—are deemed substantial for a top-tier conference.  \n\nThis work advances the theoretical foundation of neural network optimization and aligns with the standards of a top-tier conference."
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about improving the sampling efficiency of diffusion models. The main contributions are a new family of samplers called Generalized Gaussian Diffusion Models (GGDM) and an optimization method called Differentiable Diffusion Sampler Search (DDSS) that uses gradient-based optimization to improve sample quality with fewer steps. The authors claim significant improvements in FID scores compared to existing methods like DDPM and DDIM when using fewer inference steps.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns about theoretical grounding but raised their score after the authors addressed some points, especially Theorem 1. Reviewer 2 was critical about the motivation and clarity but also increased their score post-rebuttal. Reviewer 3 was positive about the results but mentioned clarity issues. Reviewer 4 had concerns about theoretical guarantees and dataset scale, while Reviewer 5 was generally positive but noted the need for clarity improvements.\n\nThe authors' response addressed several key points. They renamed their method to GGDM, connected their perceptual loss to Kernel Inception Distance (KID), which provides a better theoretical basis, and included additional experiments. They also provided more details on the ELBO and parameterizations, which likely helped clarify the method.\n\nHowever, some concerns remain. The theoretical justification, while improved, still might not fully satisfy all reviewers. The experiments are on smaller datasets (CIFAR-10, ImageNet 64x64), and there's a mention of adding LSUN results in the camera-ready version, but those aren't shown here. Also, the computational cost of DDSS with gradient rematerialization could be a practical limitation, though the authors argue it's manageable.\n\nThe strengths are the novel approach to optimizing samplers, significant empirical results in FID improvements, and addressing reviewer concerns in the rebuttal. The weaknesses include remaining questions about the theoretical depth and the scope of experiments. However, the results are compelling, and the method's compatibility with pre-trained models without retraining is a strong practical advantage.\n\nConsidering the reviews post-rebuttal, most reviewers increased their scores, indicating that the authors addressed many concerns. The empirical results are strong, and the method tackles an important problem in diffusion models. While there are some remaining issues, they don't seem severe enough to warrant rejection, especially given the positive empirical outcomes and the improvements made during the rebuttal.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel method (DDSS) and sampler family (GGDM) that significantly improve the efficiency of diffusion models by optimizing sample quality with fewer steps. Key strengths include:\n\n1. **Empirical Performance:** Demonstrates strong FID improvements (e.g., 4.82 vs. 14.9 for DDIM with 20 steps on LSUN church), validating the method's effectiveness in the low-step regime.\n2. **Theoretical Improvements:** The authors addressed concerns by linking their perceptual loss to Kernel Inception Distance (KID), grounding the objective in distribution matching via MMD. Theorem 1 and expanded ELBO analysis further strengthened theoretical justification.\n3. **Practical Relevance:** Compatibility with pre-trained models without fine-tuning makes the approach widely applicable, addressing a critical need for efficient inference in diffusion models.\n4. **Rebuttal Impact:** Authors convincingly addressed reviewer concerns—clarifying motivations, expanding experiments, and improving theoretical connections—leading to score increases from reviewers initially leaning toward rejection.\n\nWhile minor issues remain (e.g., limited high-resolution dataset results, computational cost of optimization), the paper’s empirical contributions and post-rebuttal revisions meet the bar for a top-tier conference. The work advances the field by enabling faster sampling without sacrificing quality, a pressing challenge in diffusion models."
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about analyzing adversarial robustness from a frequency perspective. The main claim is that adversarial examples aren't just high or low frequency but depend on the dataset. They also propose some methods related to adversarial training using frequency constraints. \n\nLooking at the reviews, there's a mix of opinions. Reviewers 1, 2, 3, and 5 have concerns, while 4 and 6 are more positive. The main issues raised are about the novelty of the contributions, the sufficiency of experiments, and clarity in differentiating from prior work. \n\nReviewer 1 points out that the motivation might be overclaimed because previous works have already questioned the high-frequency assumption. They also mention that the dataset dependency isn't a new conclusion. Reviewer 2 is concerned about the contributions not being clear enough and whether the proposed methods actually improve performance. Reviewer 3 lists several weaknesses, including that the dataset dependency is already known and that experiments are limited. Reviewer 5 is confused about the contributions and thinks they aren't well-supported. \n\nOn the other hand, Reviewers 4 and 6 find the experimental insights valuable and recommend acceptance despite some presentation issues. \n\nThe authors' response addresses these points by clarifying their contributions, adding more experiments (like different datasets and architectures), and explaining how their work differs from prior research. They emphasize that their focus is on the frequency of adversarial perturbations rather than the images themselves, which is a distinction from some cited works. They also provided additional experiments in the appendix, like using Auto-Attack and class-wise analysis.\n\nHowever, some concerns might still linger. For instance, the technical novelty is questioned because similar ideas exist, even if the specific angle (perturbations vs. images) is different. The authors argue that their analysis is novel, but if the community already accepts dataset dependency, then the contribution might be incremental. Also, the practical impact of their methods (like controlling the accuracy-robustness trade-off) isn't shown to outperform existing methods, which could limit the perceived significance.\n\nThe rebuttal does a decent job addressing the technical concerns, but some reviewers might not be fully convinced. The paper's strength lies in the comprehensive analysis and the potential new perspective it offers, even if the core idea isn't entirely new. Given that top-tier conferences often value novel insights and thorough experimentation, this might tip the balance towards acceptance, especially with the positive reviews from 4 and 6. However, the split in reviews and remaining doubts about novelty could lead to a borderline decision. \n\nIn the end, considering the authors' efforts to address concerns, the additional experiments, and the potential for providing a new framework for understanding adversarial examples, I think the paper might be accepted with the expectation that the final version incorporates all the promised revisions and clarifications.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a frequency-based analysis of adversarial examples, challenging the common assumption of their high-frequency nature and emphasizing dataset dependency. While reviewers raised valid concerns about novelty, experiment breadth, and contribution clarity, the authors provided substantive rebuttals addressing these issues:\n\n1. **Novelty and Differentiation:**  \n   The authors clarified that their focus on adversarial *perturbation* frequencies (rather than image frequencies) distinguishes their work from prior studies like Bernhard et al. (2021). They also highlighted novel contributions, such as frequency-restricted adversarial training and insights into the accuracy-robustness trade-off, which reviewers acknowledged as valuable.\n\n2. **Empirical Rigor:**  \n   Additional experiments on MNIST, CIFAR-100, SVHN, AutoAttack, and ViT/DenseNet architectures (included in the appendix) strengthened the dataset-dependency claim. While some reviewers requested further validation (e.g., C&W attacks), the rebuttal demonstrated sufficient effort to address methodological gaps.\n\n3. **Clarity and Impact:**  \n   The authors revised the introduction to better articulate contributions and acknowledged limitations (e.g., focus on natural images). Their analysis of frequency-controlled trade-offs, though not benchmark-breaking, offers a fresh perspective for the community, aligning with Reviewer 4’s and 6’s positive assessments.\n\n**Weaknesses Mitigated:**  \n- Concerns about overclaiming were addressed by refining the motivation and differentiating from concurrent works.  \n- Presentation issues (figure placement, grammar) were acknowledged and will be fixed.  \n\nWhile the paper’s technical novelty is incremental, its empirical depth, rebuttal engagement, and potential to shift community perspectives justify acceptance at a top-tier conference. The work provides actionable insights for future research, satisfying the bar for publication despite lingering reservations from some reviewers."
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question investigates the behavior of SGD with momentum (SGDm) under covariate shift, particularly focusing on how non-iid data can lead to divergence due to resonance effects. The authors present both theoretical analysis and empirical results, extending their findings to non-periodic shifts and nonlinear models like neural networks.\n\nStarting with the strengths, the paper addresses an important and underexplored area: optimization dynamics in non-iid settings, which is crucial for applications like reinforcement learning and continual learning. The identification of resonance as a mechanism for divergence is novel in the ML context, and the connection to parametric oscillators is a creative interdisciplinary link. The empirical section is thorough, testing the theory under various relaxed assumptions, which adds robustness to their claims. The authors also engaged constructively with reviewer feedback, addressing concerns about novelty, intuition, and the impact of stochasticity.\n\nHowever, there are significant concerns. The theoretical contributions are questioned by multiple reviewers for relying heavily on existing mathematical frameworks (e.g., Floquet theory, ODE analysis) without sufficient novel proof techniques. While the application of these theories to ML is new, the core mathematical innovations seem limited. Reviewer 2 and others pointed out that the technical novelty might be overstated, as the proofs lean on prior work. Additionally, the analysis under expected gradients (removing the 'S' in SGDm) raises doubts about the practical relevance to true stochastic settings, though the authors provided additional experiments to mitigate this.\n\nThe presentation and clarity of the theoretical results were also critiqued. Reviewers found the explanation of resonance and its connection to specific parameters (like T=45) lacking intuitive depth. While the authors offered analogies (e.g., the ball and bowl), integrating these into the main text could improve understanding. The mismatch between the problem setting in Section 2 and the experiments (e.g., non-converging distributions) was another point of confusion, suggesting a need for clearer alignment.\n\nOn the empirical side, while the experiments are extensive, some reviewers desired more evidence linking the observed phenomena in neural networks directly to resonance, beyond correlation. The practical implications were also questioned—resonance might be avoidable with parameter tuning, and its occurrence in real-world data (with broader frequency spectra) remains unproven. However, the authors' response about testing on synthetic data with controlled spectra and plans for future work on real datasets is a step in the right direction.\n\nThe reviewers' scores are mixed. Reviewers 4 and 6 lean towards acceptance, praising the structure and empirical validation. Reviewers 2, 3, and 5 have reservations about technical novelty and practical impact. The authors addressed many concerns in their response, softening claims about novelty and adding experiments, which strengthens the paper. However, the core issue of whether the theoretical contribution is sufficiently novel for a top-tier conference remains. The application of existing mathematical theories to a new problem in ML is valuable, but the incremental nature of the technical work might not meet the high bar expected.\n\nIn conclusion, while the paper presents an interesting phenomenon and thorough experiments, the reliance on established mathematical methods without groundbreaking technical innovation, combined with remaining questions about practical relevance, might not justify acceptance at a top-tier venue. However, the authors' effective engagement with reviews and the potential for future impactful work could sway the decision. Balancing these factors, the paper's empirical contributions and the novelty of the resonance concept in ML might tip the scale towards acceptance despite the theoretical concerns.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper identifies a novel phenomenon—resonance-driven divergence in SGDm under covariate shift—and rigorously connects it to parametric oscillators, offering a fresh perspective on optimization dynamics in non-iid settings. While the theoretical analysis relies on established mathematical tools (e.g., Floquet theory), the synthesis of these methods to explain resonance in ML optimization is original and valuable. The empirical validation across linear and nonlinear models, including neural networks, demonstrates robustness beyond the theoretical assumptions. Reviewer concerns about technical novelty and stochasticity were addressed through revised language, additional experiments, and clarifications, strengthening the paper's credibility. Though the practical implications of resonance in real-world data remain open, the foundational insights and methodological rigor justify acceptance at a top-tier conference. The paper’s clarity, interdisciplinary linkage, and empirical thoroughness outweigh lingering concerns about incremental technical contributions."
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question proposes a method called Differentiable Diffusion Sampler Search (DDSS) to optimize fast samplers for diffusion models by differentiating through sample quality scores. They also introduce Generalized Gaussian Diffusion Models (GGDM), a non-Markovian sampler family. The main goal is to reduce the number of inference steps needed for high-quality samples while maintaining or improving metrics like FID.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns about theoretical grounding but raised their score after the authors addressed some points, especially Theorem 1. Reviewer 2 was critical about the motivation and clarity but also moved from 3 to 6 post-rebuttal. Reviewer 3 was positive, highlighting the empirical results and practical techniques, though noted some clarity issues. Reviewer 4 raised concerns about theoretical guarantees and dataset scope, while Reviewer 5 found the idea solid but had clarity concerns.\n\nThe authors responded by clarifying the connection of their perceptual loss to KID and MMD, providing more empirical evidence, and improving the theoretical exposition. They also addressed parameterization details and included more experiments, though some reviewers still wanted more high-resolution dataset results.\n\nStrengths of the paper include tackling an important problem (reducing inference steps), introducing a novel optimization approach (DDSS), and showing strong empirical results (e.g., FID improvements on CIFAR and ImageNet). The method's compatibility with pre-trained models without retraining is a practical advantage.\n\nWeaknesses revolve around theoretical motivation. While the authors linked their loss to KID/MMD, some reviewers still felt the theoretical grounding was insufficient. The clarity of the method's exposition and the novelty compared to prior work (like DDIM) were also points of contention. Additionally, the lack of results on higher-resolution datasets (e.g., LSUN 256x256) might limit the perceived generality.\n\nHowever, the empirical results are compelling. The FID scores reported (e.g., 4.82 with 20 steps vs. 14.9 for DDIM) are significant improvements. The authors addressed major concerns through rebuttals, improving the paper's theoretical motivation and clarity. While some issues remain, the practical impact and strong results likely outweigh these concerns for a top-tier conference.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel method (DDSS) for optimizing diffusion model samplers, achieving significant empirical improvements in sample quality with fewer inference steps. While initial reviews raised concerns about theoretical grounding and clarity, the authors addressed these through rebuttals by:  \n1. **Theoretical Motivation:** Linking the perceptual loss to Kernel Inception Distance (KID) and Maximum Mean Discrepancy (MMD), clarifying the distribution-matching objective.  \n2. **Empirical Robustness:** Demonstrating consistent results across kernels (linear/cubic) and datasets (CIFAR-10, ImageNet 64x64), with plans to include additional datasets.  \n3. **Technical Clarifications:** Expanding proofs (Theorem 1), parameterization details, and reverse process derivations.  \n\nThough minor issues remain (e.g., limited high-resolution dataset results), the work’s practical impact—enabling faster, higher-quality sampling without retraining—and strong empirical performance (e.g., FID of 4.82 vs. 14.9 for DDIM at 20 steps) justify acceptance. The revisions adequately address reviewer concerns, aligning the paper with top-tier conference standards."
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about a \"weighted mean trick\" for optimizing higher-order moments of the loss distribution to improve robustness. The reviewers have a mix of opinions, but there are some recurring concerns. \n\nFirst, looking at the technical contributions. The main idea is that by using weighted means, they can control higher-order moments like variance, skewness, etc. The theoretical results show equivalence between weighted ERM and regularized objectives. However, multiple reviewers pointed out that the theoretical derivations might be straightforward or lack depth. For example, Reviewer 5 mentions that the theorems are \"basic computations\" and the technical novelty is low. Reviewer 2 and 3 question the algorithm's correctness, noting that the gradient descent step omits a term from the chain rule, which could mean the algorithm isn't actually minimizing the intended objective. This is a significant issue because if the algorithm isn't correctly optimizing the proposed loss function, the empirical results might be misleading.\n\nThen there's the issue of convexity. The authors claim convexity under certain conditions, but reviewers are confused about how this holds when weights depend on θ. Lemma 2's applicability is questioned because if the weights are a function of θ, the overall objective might not remain convex. This is a critical point because preserving convexity is essential for convergence guarantees.\n\nEmpirical evaluation is another area of concern. While the experiments show comparable performance to existing methods, reviewers note that the improvements might be due to hyperparameter tuning rather than the method's inherent advantages. Reviewer 1 points out that without theoretical guidance on choosing λ, the empirical results aren't convincing. Also, the comparison with other robust methods (like trimmed mean or MoM) is lacking, which makes it hard to assess the method's true effectiveness.\n\nThe authors' response addresses some concerns, like clarifying notation and adding experiments, but some issues remain unresolved. For example, the gradient descent algorithm's correctness isn't fully justified. They mention detaching weights from the computation graph, which might be a practical workaround but doesn't resolve the theoretical inconsistency. Also, the connection between negative λ and robustness isn't sufficiently explained, leaving doubts about the method's motivation.\n\nReviewer 7 is more positive, seeing potential in the idea, but even they note the theorems are straightforward. The mix of scores (some 4s in correctness but lower in novelty) suggests the paper's foundation is solid but not groundbreaking.\n\nConsidering the conference's top-tier standards, the paper needs both theoretical depth and empirical rigor. The unresolved algorithm correctness, lack of convergence guarantees, and insufficient comparison with relevant methods are red flags. While the idea of using weighted means for moment control is interesting, the execution and presentation have significant gaps that lower the paper's contribution below the threshold for a top-tier conference.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces a weighted mean approach to optimize higher-order loss moments for robustness. While the concept is intriguing, several critical issues hinder its suitability for a top-tier conference:  \n\n1. **Theoretical Concerns:**  \n   - The gradient descent algorithm (Algorithm 1) omits a term in the gradient computation, raising doubts about whether it truly minimizes the proposed objective. Reviewers highlighted this as a fundamental flaw, and the authors’ response did not fully resolve this issue.  \n   - Convexity claims (Lemma 2) are questioned, especially when weights depend on model parameters θ. The theoretical justification for preserving convexity under these conditions remains unclear.  \n\n2. **Empirical Limitations:**  \n   - Improvements over baselines may stem from hyperparameter tuning rather than the method’s inherent properties. The lack of theoretical guidance for selecting λ weakens the empirical validation.  \n   - Comparisons to robust baselines (e.g., trimmed mean, MoM) are insufficient, limiting the assessment of practical utility.  \n\n3. **Novelty and Significance:**  \n   - The theoretical results, while correct, are deemed incremental. The connection between negative λ and robustness is underdeveloped, and the framework’s flexibility (allowing both positive/negative weights) introduces ambiguity rather than insight.  \n\n4. **Presentation Issues:**  \n   - Key sections (e.g., notation, algorithm description) were criticized for clarity, and minor errors (e.g., undefined variables) persist despite revisions.  \n\nWhile the idea has potential, unresolved theoretical gaps, limited empirical scope, and presentation shortcomings render the paper insufficient for a top-tier venue in its current form."
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper is about TimeVAE, a VAE-based model for generating multivariate time series. The authors claim it's better than GANs in terms of interpretability, domain knowledge integration, and training efficiency. The reviewers have mixed opinions, but there's a trend towards rejection. Let me break down the key points.\n\nFirst, the strengths. The paper is well-written, which is a plus. They introduce interpretable components in the decoder, like trend and seasonality, which could be useful. The empirical results show that TimeVAE performs comparably or better than GANs on some datasets, especially in similarity and prediction tasks. Also, the idea of incorporating domain knowledge is a good angle, as interpretability is a hot topic.\n\nNow the weaknesses. The main issue seems to be the lack of technical novelty. Reviewers 2 and 3 point out that the architecture isn't significantly different from a standard VAE applied to time series. Using convolutional layers instead of recurrent ones might not be enough. The reviewers wanted more discussion on how this approach is novel compared to existing VAEs for time series. Also, the hyperparameters for the new decoder blocks aren't explored, and there's no ablation study to show the impact of these components. Without that, it's hard to assess their contribution.\n\nAnother big problem is the lack of evaluation on interpretability. The authors claim it's a key contribution, but there's no experiment showing how the interpretable components work. Reviewers 1 and 4 specifically mention this. If they added those blocks but didn't test their effect, the claim isn't supported.\n\nEmpirical evaluation has issues too. The datasets used are synthetic or not well-known, making comparisons difficult. Reviewer 3 notes that using standard benchmarks would strengthen the results. Also, the discriminator score results in Table 1 are confusing—better performance with less data? The authors didn't address this in their response (though there's no author response here, which might be a problem). \n\nReviewer 2 questions the originality, saying TimeVAE is just a standard VAE applied to time series without the extra innovations seen in TimeGAN. The loss function isn't discussed in depth, and there's no comparison to other VAEs like GP-VAE. Technical details are missing, which undermines the paper's credibility.\n\nReviewer 3 brings up the architecture's limitation of using a single latent vector for the entire sequence, which might not capture temporal dependencies as well as per-time-step latent variables. They also mention the lack of comparison to autoregressive models, which are common in time series. The evaluation metrics are not well-justified either, like using an external LSTM for prediction instead of integrating it into the model.\n\nReviewer 4 points out that the results don't show a clear advantage over existing methods except on one dataset. The claimed interpretability isn't demonstrated, which is a major weakness since it's a key contribution.\n\nThe author response isn't available, so we can't see if they addressed these points. But based on the reviews, the paper has significant flaws in novelty, evaluation, and supporting claims. The technical contribution is minimal, the experiments are insufficient, and the interpretability aspect isn't validated. For a top-tier conference, these issues are critical. Even though some reviewers gave higher scores, the consensus leans towards rejection due to these unresolved problems.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces TimeVAE, a VAE-based architecture for multivariate time-series generation, with claims of interpretability, domain knowledge integration, and reduced training time. While the idea of combining VAEs with interpretable components (e.g., trend/seasonality) is promising, the reviews highlight critical shortcomings that undermine its readiness for publication in a top-tier conference:  \n\n1. **Lack of Technical Novelty**:  \n   - The core architecture is perceived as a straightforward application of VAEs to time-series data, with insufficient differentiation from prior work (e.g., standard VAEs or GP-VAE). Reviewers note that the proposed \"base\" model lacks innovations (e.g., novel loss functions or latent variable structures) to justify its novelty compared to methods like TimeGAN, which introduced domain-specific adaptations.  \n\n2. **Insufficient Empirical Validation**:  \n   - **Interpretability Claims Unsupported**: Despite being a key contribution, no experiments or ablation studies demonstrate the interpretability of the decoder components (e.g., how domain knowledge is encoded or visualized).  \n   - **Limited Benchmarking**: Experiments use non-standard or synthetic datasets, making comparisons to prior work difficult. Key baselines (e.g., autoregressive models, state-space approaches) are omitted.  \n   - **Ambiguous Results**: Discriminator scores in Table 1 show counterintuitive trends (e.g., better performance with smaller training sets), which are not adequately explained.  \n\n3. **Methodological Weaknesses**:  \n   - The use of a single global latent variable for entire sequences (vs. per-time-step latents) is flagged as a limitation.  \n   - Hyperparameter selection for the decoder blocks is not addressed, raising concerns about reproducibility and practical utility.  \n\n4. **Incomplete Literature Context**:  \n   - The paper overlooks critical related work in state-space models, particle filters, and probabilistic autoregressive models, weakening its positioning in the broader field.  \n\nWhile the paper is well-written and the idea of interpretable VAEs has potential, the unresolved issues in novelty, evaluation, and methodological rigor make it unsuitable for acceptance in its current form. Addressing these concerns would require substantial revisions, including expanded experiments, comparisons to broader baselines, and validation of interpretability claims."
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about improving face detection efficiency and accuracy, specifically targeting small faces in low-resolution images. The authors propose two main methods: Computation Redistribution (CR) and Sample Redistribution (SR). They claim state-of-the-art results on the WIDER FACE dataset, with significant improvements in both accuracy and speed compared to existing methods like TinaFace.\n\nLooking at the reviews, there's a mix of opinions. Reviewers 1-3 seem positive but have some reservations. Reviewer 4 and 7 point out weaknesses in the technical novelty and the lack of comparisons with other NAS methods. Reviewer 6 is more enthusiastic, highlighting the strong results but questioning the search strategy's novelty. The authors responded by adding comparisons with evolutionary methods and other NAS approaches, which addresses some concerns.\n\nThe strengths are clear: the paper presents effective methods (CR and SR) that improve performance on a challenging dataset. The ablation studies show the components work, and the results are impressive, especially the 3x speedup with higher accuracy. The authors also provided additional experiments and clarifications in their response, which strengthens their case.\n\nHowever, the weaknesses revolve around the perceived lack of technical novelty. Some reviewers found the search strategies straightforward and not sufficiently novel. The initial submission might have lacked comparisons with other NAS methods, but the authors addressed this by adding evolutionary and BFBox comparisons in their revisions. The technical contribution is more about effective application and optimization rather than a groundbreaking new method, which might be a point of contention for a top-tier conference.\n\nThe authors' responses are thorough. They added figures, algorithms, and more detailed comparisons, which likely mitigate some of the reviewers' concerns. For example, including an evolutionary method comparison shows their approach's superiority, and expanding the cross-dataset evaluations addresses claims about SOTA performance.\n\nConsidering the significance of the problem (efficient face detection is crucial for real-world applications) and the empirical results, the paper's practical contributions are strong. Even if the technical novelty is incremental, the substantial performance improvements and thorough evaluation might justify acceptance. However, top-tier conferences often prioritize both novelty and impact. The reviewers' scores on technical novelty are mixed (some 2s and 3s), which could be a red flag. But the empirical significance is higher, with some reviewers giving 3s and 4s.\n\nIn balance, the paper makes a solid contribution with practical impact, even if the methods aren't entirely novel. The authors addressed most criticisms adequately, and the results are compelling. For a top-tier conference, this might be on the borderline, but the strong empirical results and application relevance could tip the scales towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents SCRFD, a method combining Computation Redistribution (CR) and Sample Redistribution (SR) to improve efficiency and accuracy in face detection, particularly for small faces. While reviewers raised concerns about technical novelty (e.g., straightforward search strategies and insufficient comparisons with NAS methods), the authors addressed these effectively in their response. Key strengths include:  \n1. **Strong Empirical Results:** SCRFD achieves state-of-the-art performance on WIDER FACE, with a 4.78% AP improvement over TinaFace and 3× faster inference. Cross-dataset evaluations (AFW, Pascal, FDDB) further validate its superiority.  \n2. **Practical Impact:** The work addresses a critical challenge (efficient small-face detection) with clear real-world applications. The code release enhances reproducibility and utility.  \n3. **Responsive Revisions:** The authors added comparisons with evolutionary and NAS baselines (e.g., BFBox), demonstrating SCRFD’s advantages. They also clarified methodology (added figures, algorithms, and ablation studies) to address reviewer concerns.  \n\nWhile the technical novelty is incremental (CR/SR build on prior NAS/redistribution ideas), the paper’s **empirical significance**—demonstrating how systematic computation/sample redistribution improves efficiency-accuracy trade-offs—justifies acceptance at a top-tier venue. The revisions strengthened the paper’s rigor, and the results are compelling enough to warrant publication despite moderate novelty concerns."
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about understanding overparameterization in recursive value estimation, specifically looking at TD learning, FVI, and RM in the overparameterized linear regime. The authors argue that even in this regime, different algorithms converge to different fixed points, and they propose a unified interpretation and new regularizers for deep models.\n\nFirst, the strengths. The topic is timely and relevant, as overparameterization is a hot area in ML, especially with the success of deep learning. The theoretical analysis seems thorough, showing differences in fixed points and providing generalization bounds. The connection to deep RL through regularizers is a plus, even if the experiments are on simpler environments. The authors addressed many reviewer concerns in their response, which improved some scores.\n\nNow the weaknesses. Several reviewers pointed out that some theoretical results, like Theorem 4, are not novel and are standard in convex optimization. There's a concern about insufficient credit to prior work, which could affect the paper's originality. The experiments are seen as limited—using toyish domains and not clearly connecting to deep RL's practical scenarios. Also, some technical issues were noted, like proof errors and matrix decomposition assumptions that need better justification.\n\nReviewers 7 and 8 were positive post-rebuttal, leaning towards acceptance, while others had reservations. The technical correctness was mostly affirmed, but novelty varied. The empirical section was criticized for not being compelling enough, though the theory is solid.\n\nThe authors' responses addressed many points, clarifying assumptions and adding references where needed. However, some issues remain, like the practical relevance of linear overparameterization to deep RL and the depth of experiments. The connection to NTK is a good point but might need more elaboration in the paper.\n\nConsidering all this, the paper makes a meaningful theoretical contribution by analyzing overparameterized RL algorithms, which is understudied. The regularizers and insights could influence future research. Despite some flaws in novelty claims and experiments, the theoretical rigor and the addressed concerns tip the balance towards acceptance.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a timely theoretical analysis of overparameterized linear models in recursive value estimation, addressing a gap in understanding modern deep RL systems. While concerns about technical novelty (e.g., Theorem 4 being a standard convex optimization result) and insufficient empirical validation in complex environments are valid, the core contributions—demonstrating distinct fixed points for TD, FVI, and RM in overparameterized regimes, unifying their interpretations, and deriving generalization bounds—are novel and impactful for RL theory. The proposed regularizers, though tested in simple domains, align with theoretical insights and show promise. Reviewer concerns about prior work attribution and technical errors were partially addressed in the rebuttal, and the connection to neural tangent kernels (NTK) adds relevance to deep learning. While the empirical section is limited, the theoretical rigor and the potential to influence future work on implicit bias in RL justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the author response. The paper is about compositional learning behaviors using meta-referential games. The authors propose a new benchmark and a Symbolic Continuous Stimulus (SCS) representation. The reviewers have several concerns.\n\nReviewer 1 found the idea interesting but had issues with clarity and insufficient experiments. They wanted more details on why SCS helps and more thorough experiments, including comparisons with other representations like one-hot encoding. The authors only used two seeds, which might not be enough for reliable results. Reviewer 2 also had clarity issues, pointing out that the introduction was too dense and the benchmark wasn't emphasized enough. They questioned the choice of SCS over OHE, especially since OHE performed better in some experiments. They also wanted more details on the experimental setup and possible ambiguities in SCS over time. Reviewer 3 wasn't convinced of the contributions' utility outside the paper's setting and noted repetition and unclear sections. They recommended rejection. Reviewer 4 thought the paper was ambitious but needed more development, especially in comparing SCS with continuous representations, clarity in methodology, and evaluation on existing benchmarks. They also mentioned the low performance (29% accuracy) and possible hyperparameter issues.\n\nThe authors' response is brief, thanking the reviewers and promising improvements but without specific details on how they'll address each concern. \n\nThe main issues are clarity, insufficient experiments, and doubts about the significance and novelty of contributions. The technical novelty of SCS is questioned, as reviewers think it's not clear how it's better than existing methods or applicable beyond the paper's setup. The empirical results are weak (low performance) and lack comparisons with relevant baselines. The benchmark's novelty is also under scrutiny, with requests for more details and differentiation from existing work. \n\nFor a top-tier conference, the paper needs strong technical and empirical contributions. The reviewers agree that the current experiments are insufficient, the clarity is lacking, and the significance isn't well established. The authors' response doesn't provide concrete plans to address these, which is a red flag. Even with revisions, the paper might not meet the high bar required. Therefore, rejection seems appropriate.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a novel benchmark for compositional learning via meta-referential games and proposes a Symbolic Continuous Stimulus (SCS) representation. While the problem of compositional generalization is important, the paper faces significant criticisms across all reviews that undermine its suitability for a top-tier conference:  \n\n1. **Clarity and Presentation Issues:**  \n   - Multiple reviewers found the paper difficult to follow due to unclear problem framing, undefined terms (e.g., \"positional disentanglement metric\"), repetitive text, and insufficiently explained methodology. The introduction and related work sections overlap excessively, and key concepts like the benchmark’s structure and SCS’s advantages are underdeveloped.  \n   - Figures and examples (e.g., stimulus representations, agent interactions) are inadequately described, limiting reproducibility and reader engagement.  \n\n2. **Technical and Empirical Weaknesses:**  \n   - **SCS Representation:** Reviewers questioned the novelty and utility of SCS. While it avoids fixed-shape limitations of one-hot encodings (OHE), its benefits over continuous representations (common in modern ML) are unclear. The paper fails to justify why SCS is preferable in real-world scenarios or how it scales to more complex tasks (e.g., images). Experiments show OHE outperforms SCS in some cases, raising doubts about its value.  \n   - **Benchmark and Baselines:** The benchmark lacks differentiation from existing work (e.g., SCAN/gSCAN) and suffers from insufficient empirical validation. Results are weak (e.g., 29% accuracy, barely above chance), and the authors provide no ablation studies, hyperparameter analysis, or comparisons to continuous representations. The choice of architectures (e.g., DNC vs. LSTM) is poorly justified, and training details (e.g., model size, optimization) are omitted.  \n\n3. **Significance and Novelty Concerns:**  \n   - Reviewers unanimously rated technical and empirical contributions as \"marginally significant or novel.\" The SCS representation is seen as narrowly applicable, and the benchmark’s impact is limited by unclear evaluation protocols and lack of connection to broader literature (e.g., compositional generalization in cognitive science or modern NLP).  \n   - The paper does not engage deeply with foundational work on compositionality (e.g., Smolensky, Chalmers) or demonstrate how its approach advances beyond prior art.  \n\n4. **Author Response Limitations:**  \n   The authors’ response is generic, promising to \"improve the paper\" without addressing specific concerns (e.g., clarifying SCS’s advantages, adding experiments with continuous representations, or expanding the benchmark analysis). This lack of concrete rebuttals suggests unresolved issues.  \n\nWhile the problem of compositional learning is compelling, the paper’s current form lacks the clarity, technical rigor, and empirical depth required for a top-tier venue. Significant revisions are needed to validate SCS, strengthen the benchmark, and situate contributions within the broader literature."
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a new adversarial training framework called BiLAW, which uses a bi-level optimization approach inspired by MAML to reweight training samples based on their multi-class margins. The goal is to improve both clean and robust accuracy compared to existing methods. The reviewers have a mix of opinions, so I need to carefully weigh their concerns and the authors' responses.\n\nStarting with the strengths: The idea of learning sample weights through a meta-learning approach is novel in the context of adversarial robustness. The authors show improvements over baselines like GAIRAT and TRADES, especially when combined with TRADES. The use of AutoAttack for evaluation is a strong point, as it's a reliable benchmark. The paper is also well-written, which is a plus.\n\nNow, the major concerns. Reviewer 1 and others question the novelty, pointing out that BiLAW is a direct adaptation of MAML. While the authors argue their contribution is the formulation and multi-class margin, some reviewers feel this is incremental. Technical novelty is crucial for top-tier conferences, so this is a valid point.\n\nEvaluation issues are significant. Reviewers are concerned about the lack of adaptive attacks. The authors responded with experiments showing adaptive attacks don't significantly impact performance, but these are preliminary. Without thorough evaluation against adaptive attacks, the robustness claims might be overstated. Also, the improvements over SOTA are marginal, and some datasets (MNIST/F-MNIST) are considered outdated. The absence of CIFAR-100 results in the main text is another gap.\n\nReviewer 3 and others highlight the lack of justification for using multi-class margins over other input encodings. The authors mention ablation studies in the appendix, but these don't fully address why their specific input encoding is superior. This leaves questions about the core design choices.\n\nThe computational cost and training time are mentioned but not thoroughly addressed. Reviewer 6 points out that BiLAW's performance might be due to combining with TRADES rather than its own merits. The authors provide some comparisons, but the gains when combined with TRADES are still a point of contention.\n\nOn the positive side, some reviewers (4,7,8,10) find the empirical results sufficient for acceptance, noting the clear methodology and thorough experiments. The authors have addressed some concerns, like adding CIFAR-100 results and discussing adaptive attacks, albeit not exhaustively.\n\nBalancing all this: The paper introduces a novel approach with solid empirical results, but the technical novelty is somewhat incremental. The evaluation gaps, especially regarding adaptive attacks and justification of input encoding, are critical. Top-tier conferences demand both novelty and rigorous validation. While the authors made efforts in their response, key experiments are still pending or preliminary. The mixed reviews, with several recommending rejection due to these issues, suggest the paper isn't yet ready for acceptance. It needs more thorough evaluation and deeper analysis of design choices to meet the high bar.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes a novel adversarial training framework (BiLAW) that learns sample reweighting via bi-level optimization, demonstrating improvements in clean and robust accuracy. While the approach is methodologically sound and well-motivated, several critical issues prevent it from meeting the bar of a top-tier conference:  \n\n1. **Novelty Concerns**: The core technical contribution—applying MAML-style bi-level optimization to adversarial training—is perceived as incremental by multiple reviewers. While the multi-class margin formulation adds nuance, its superiority over alternative input encodings (e.g., raw data or activations) is not rigorously justified. The authors’ ablation studies focus on margin variants but fail to address why their specific design choices (e.g., logit-based margins) are optimal, leaving the novelty underdeveloped.  \n\n2. **Incomplete Evaluation**:  \n   - **Adaptive Attacks**: The lack of thorough evaluation against adaptive attacks targeting the reweighting module is a major flaw. While preliminary experiments suggest minimal impact, these are insufficient for a top-tier venue. Reviewers correctly argue that adaptive attacks are within the scope of the work, as the reweighting mechanism is central to the method’s claims.  \n   - **Dataset and Baseline Limitations**: Results on CIFAR-100 are omitted in the main text, and comparisons to SOTA methods like AWP are incomplete. The reliance on older datasets (MNIST/F-MNIST) and small architectures further weakens the empirical contribution.  \n\n3. **Marginal Improvements**: The gains over baselines (e.g., TRADES) are modest and partly attributed to combining BiLAW with existing techniques rather than its standalone efficacy. Reviewer 6 notes that BiLAW alone underperforms under AutoAttack, raising doubts about its fundamental robustness benefits.  \n\n4. **Theoretical and Design Justification**: The connection between multi-class margins and robust generalization is not rigorously established. Reviewers highlight the need for deeper analysis of how reweighting dynamics affect model boundaries, which the authors only partially address.  \n\nWhile the paper’s clarity and empirical framework are strengths, the unresolved concerns about novelty, evaluation rigor, and mechanistic justification outweigh these positives. The authors’ responses, though thoughtful, do not fully bridge these gaps (e.g., pending experiments on adaptive attacks and input encodings). For a top-tier conference, the work requires more compelling evidence of technical innovation and robustness to adversarial scrutiny."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The reviewers have a mix of opinions, but there's a common thread of concern about the novelty and the empirical results. \n\nStarting with the positive aspects: The platform seems well-engineered, with a modular design and a GUI, which makes it accessible to non-experts. That's a practical contribution. The Believer-Penalizer (BP) strategy is introduced as a novel method for asynchronous batch optimization. However, reviewers point out that BP's theoretical foundation is weak and the empirical results don't consistently show it outperforming existing methods. \n\nReviewer 1 initially leaned towards rejection, noting mixed performance in Figure 5 and limited novelty. Reviewer 2 and 4 highlight that the work is more of a software tool than a scientific contribution, with Reviewer 4 mentioning that BP's claims are overstated. Reviewer 3 also questions the scientific contribution, pointing out the lack of theoretical proof for BP. Reviewer 5 is more positive but still notes minor issues.\n\nThe authors responded by clarifying the engineering contributions, expanding the platform to handle more objectives, and adjusting their claims about BP's performance. They also added comparisons with more existing tools. However, the core issue remains: the technical novelty is limited. BP is a combination of existing methods (KB and LP) without a strong theoretical justification. The empirical evaluations show mixed results, which the authors adjusted to say BP is \"robust\" rather than state-of-the-art, but even then, the evidence isn't compelling enough for a top-tier conference.\n\nAnother point is the target venue. The authors argue that ICLR considers software platforms, but the reviews suggest that the novelty bar is high. While the engineering work is solid, the scientific contribution—especially the BP method—doesn't meet the threshold for a top conference. The lack of theoretical analysis and the incremental nature of BP might be the main sticking points. \n\nMoreover, the comparison with existing platforms (like BoTorch, Ax, etc.) shows that AutoOED is competitive but not clearly superior. The real-world example is a proof of concept but not a rigorous benchmark. \n\nIn conclusion, while the platform is useful and well-designed, the paper's contributions in terms of novel algorithms or significant empirical advancements are insufficient for a top-tier conference like ICLR. The reviewers' concerns about novelty and overstated results, even after revisions, lead me towards a rejection.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents AutoOED, a platform for multi-objective Bayesian optimization (MOBO) with a focus on usability, modularity, and asynchronous batch optimization. While the platform demonstrates engineering rigor and practical utility, its suitability for acceptance at a top-tier conference hinges on three key factors: **novelty**, **empirical validation**, and **alignment with conference standards**.\n\n### 1. **Novelty**  \nThe primary technical contribution—the Believer-Penalizer (BP) strategy—is critiqued for its incremental nature. BP combines existing methods (Kriging Believer and Local Penalization) in a straightforward way, and the authors acknowledge that theoretical justification for its effectiveness is lacking. While the modular framework and GUI are valuable engineering contributions, they are not groundbreaking in the context of existing BO platforms (e.g., BoTorch, Ax, Optuna). The extension to asynchronous MOBO is noted as novel, but its empirical validation is insufficient to establish it as a significant advancement.\n\n### 2. **Empirical Validation**  \nThe experimental results are mixed. While BP is claimed to be \"robust,\" it does not consistently outperform baselines (e.g., Figure 4 shows BP underperforming on DTLZ3). Comparisons with other platforms (Figure 5) show AutoOED is competitive but not clearly superior. The real-world hardware experiment is a proof of concept but lacks rigorous benchmarking against alternatives. The authors’ revisions (e.g., adding many-objective support) address some concerns but do not resolve the core issue of overstated claims.\n\n### 3. **Alignment with Conference Standards**  \nTop-tier conferences like ICLR prioritize methodological innovation or transformative empirical results. While AutoOED’s usability and modularity are practical strengths, the paper leans more toward system documentation than scientific advancement. The BP strategy, though useful, is not sufficiently novel or theoretically grounded to meet the high bar for algorithmic contributions. The authors’ argument that engineering contributions are equally valid for ICLR is partially valid, but the platform’s technical novelty remains marginal compared to prior work.\n\n### 4. **Author Responses**  \nThe authors addressed several concerns (e.g., expanding objective support, revising claims about BP, adding comparisons). However, they did not resolve fundamental issues:  \n- BP’s theoretical limitations remain unaddressed.  \n- Empirical results still lack clear differentiation from existing methods.  \n- The platform’s novelty is overshadowed by its similarity to other tools (e.g., BoTorch’s modularity, Optuna’s ease of use).\n\n### Conclusion  \nAutoOED is a well-engineered platform with practical value for experimental design, particularly for non-experts. However, its incremental technical contributions and insufficient empirical validation of its core innovations (BP, asynchronous MOBO) make it better suited for a software/application-focused venue rather than a top-tier machine learning conference.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about improving RNNs for hierarchical motor control using insights from neuroscience, specifically the thalamocortical architecture. The main contributions are a biologically-inspired preparatory module that helps in chaining motor motifs without retraining transitions, leading to better accuracy and robustness.\n\nThe reviewers have mixed opinions. Reviewer 2 and 3 are critical, pointing out weaknesses in the biological plausibility and the scale of experiments (only 10 motifs, 300 units). They argue that the problem setup is too toy-like and that alternative approaches (like training on transitions) weren't adequately tested. Reviewer 3 also mentions the lack of comparison with obvious baselines and the small scale making the results less impactful for ML. Reviewer 4 is more positive, noting the simplicity and biological grounding of the approach. Reviewer 5 raises issues with the presentation and technical details, especially around continual learning metrics and the existence of trivial solutions.\n\nThe authors responded by clarifying their approach, adding experiments to show the advantages over reset strategies, and explaining how their method avoids interference in continual learning. They also addressed concerns about noise robustness and provided more evidence on the benefits of their preparatory module.\n\nNow, considering the strengths and weaknesses: The paper introduces a novel architecture inspired by neuroscience, which is a plus. The idea of using thalamocortical insights to improve RNNs is interesting and could have broader implications. However, the experiments are on a small scale, which might limit the perceived impact. The reviewers' concerns about not testing enough alternatives (like training on transitions) are valid, but the authors argue that their approach avoids the need for such training, which is a key advantage. The presentation issues mentioned by Reviewer 5 could hinder clarity, but the authors have made efforts to improve it.\n\nIn top-tier conferences, novelty and potential impact are crucial. The biological inspiration and the zero-shot transfer capability are novel. However, the empirical evaluation's scale and the lack of comparison with more baselines might hold it back. The authors' responses do address some concerns, but whether they sufficiently mitigate the issues is debatable. The split in reviewer opinions (some positive, some negative) suggests that the paper is on the borderline. Given the emphasis on novelty and the potential for future work, but balanced against methodological concerns, it's a tough call. However, the significant concerns about empirical validation and the toy problem setup might tip the decision towards rejection.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a biologically inspired RNN architecture for continual learning of motor motifs, leveraging thalamocortical insights to enable zero-shot chaining of learned motifs. While the premise is novel and the connection to neuroscience is a strength, several critical issues remain unresolved:  \n\n1. **Scale and Relevance**: The experiments (10 motifs, 300-unit RNNs) are deemed too small-scale and synthetic to convincingly demonstrate broader applicability in machine learning. Reviewer 3 and others question whether insights from such a toy task generalize to real-world scenarios.  \n\n2. **Insufficient Comparison to Alternatives**: Reviewers highlight the lack of rigorous testing against alternative approaches (e.g., training on transitions, noise-augmented resets). While the authors argue that their method avoids transition training, they do not empirically rule out simpler solutions, weakening their claim of necessity for the proposed architecture.  \n\n3. **Biological Plausibility Concerns**: The thalamocortical reset mechanism’s biological and computational justification is contested. Reviewers argue that the reset mechanism’s specificity is not sufficiently validated, and alternative inhibitory mechanisms could achieve similar results.  \n\n4. **Presentation and Clarity**: Reviewer 5 notes significant issues with writing clarity and task formalization, which hinder reproducibility and understanding. Though the authors addressed some concerns, the core presentation remains subpar for a top-tier venue.  \n\n5. **Mixed Empirical Support**: While the preparatory module shows benefits in noise robustness and training efficiency, reviewers question the statistical rigor of claims (e.g., marginal vs. joint distributions, initialization choices).  \n\nThough the authors made efforts to address critiques (e.g., adding experiments on reset strategies), the work’s incremental technical novelty, limited empirical scope, and unresolved methodological concerns outweigh its strengths. For a top-tier conference, the paper does not sufficiently demonstrate transformative impact or generality."
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about epoch-wise double descent in neural networks, using a linear teacher-student model to derive analytical results. The reviewers have given their comments, and the authors have responded to them. My task is to determine whether the paper should be accepted or rejected for a top-tier conference.\n\nFirst, looking at Reviewer 1's comments: They found the work insightful and well-presented but had questions about the interpretation of results, such as the possibility of triple descent and the differences between epoch-wise and model-wise double descent. They also noted a possible figure error. The authors addressed these by adding discussions on triple descent, clarifying the differences, and fixing the figure. Reviewer 1's summary was positive, giving high correctness and significance scores.\n\nReviewer 2 raised concerns about connections to prior work on implicit regularization (like ridge regression) and high-dimensional linear regression. The authors responded by adding discussions and experiments linking their work to these areas, which seems to address the concerns. Reviewer 2's main issue was ensuring connections to existing literature, which the authors attempted to cover.\n\nReviewer 3 had more critical feedback, pointing out typos, technical novelty, and the need for clearer connections to realistic settings. The authors fixed typos, added derivations in the appendix, and provided new experiments with ResNet on CIFAR-10 to show relevance. However, Reviewer 3 was still concerned about technical advancements and clarity in the theory.\n\nThe authors' responses were thorough. They addressed each point, fixed errors, added experiments, expanded discussions, and clarified connections to prior work. They also provided additional materials like Colab notebooks and new figures. The key strengths of the paper are the analytical treatment of epoch-wise double descent, which is a less studied phenomenon, and the connection to real networks via NTK analysis. The derivation of closed-form expressions using replica theory is technically solid, and the empirical validation supports their claims.\n\nPotential weaknesses include the reliance on linear models, which might limit direct applicability to deep networks, but the authors acknowledge this and provide some empirical validation. The technical novelty is somewhat debated, but the application of replica methods to finite-time dynamics seems novel enough for a top-tier venue. The paper's contribution is significant as it provides a theoretical framework for understanding a complex phenomenon in deep learning.\n\nConsidering all this, the paper makes a meaningful contribution by explaining epoch-wise double descent through feature learning dynamics. The authors addressed most reviewer concerns adequately, and the remaining issues (like the depth of technical novelty) are balanced by the clarity and potential impact of the findings. Therefore, the paper should be accepted.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper provides a theoretical analysis of epoch-wise double descent, a phenomenon observed in deep learning, using a linear teacher-student model. The work derives closed-form expressions for generalization error dynamics, attributing double descent to multi-scale feature learning. While reviewers raised concerns about technical novelty, connections to prior work, and clarity, the authors addressed these effectively in their response:  \n1. **Technical Novelty:** The adaptation of replica theory to study finite-time dynamics (rather than asymptotic behavior) and the explicit link between early-stopping and ridge regularization are novel contributions. The analytical framework enables insights into transient phenomena like epoch-wise double descent, advancing beyond prior asymptotic analyses.  \n2. **Empirical Validation:** The authors strengthened connections to deep networks by analyzing NTK singular values in ResNet-18 on CIFAR-10, showing how feature learning scales correlate with error dynamics. This bridges theory and practice.  \n3. **Clarity and Corrections:** Typos and notation inconsistencies were resolved, derivations were expanded in the appendix, and discussions were added to clarify differences between epoch-wise and model-wise double descent, as well as connections to related works (e.g., Heckel & Yilmaz, Stephenson & Lee).  \n4. **Reviewer Concerns Mitigated:** The authors provided additional experiments (e.g., triple descent), addressed implicit regularization links, and justified assumptions about SGD noise.  \n\nWhile the linear model simplification limits direct applicability to nonlinear networks, the paper’s analytical rigor and empirical alignment with deep networks justify its relevance. The work offers a foundational understanding of epoch-wise double descent, meeting the standards of a top-tier conference."
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper presents MILAN, a method for generating natural language descriptions of neurons in deep visual models. The reviewers generally found the work well-motivated and technically sound, with promising applications. Let's break down the key points.\n\nFirst, the strengths. The paper introduces a novel approach (MILAN) that goes beyond existing methods by providing richer, compositional descriptions of neuron activations. The applications in model analysis, auditing, and editing are compelling and well-demonstrated. The authors have shown generalizability across architectures and tasks, which is a big plus. The dataset (MILANNOTATIONS) is a significant contribution, and the code/data release adds reproducibility value.\n\nReviewers highlighted some concerns. Reviewer 2 questioned the inter-annotator agreement in the dataset and how inconsistencies were handled. The authors responded by explaining that BERTScore is multi-reference, so MILAN matching one human annotation suffices. They also mentioned potential biases in the dataset but didn't implement specific controls, pointing to future work. This seems acceptable but could be a minor weakness.\n\nReviewer 4 raised scope issues, suggesting the title be narrowed to visual features, which the authors addressed. They also asked about transfer to non-vision tasks. The authors clarified that MILAN is focused on vision but could be extended, which is reasonable. The experiments on using off-the-shelf captioning models as baselines were added, showing MILAN's superiority, which strengthens their claims.\n\nTechnical novelty was rated as \"somewhat new\" by most reviewers. While the core idea of using mutual information isn't entirely new, its application to neuron description in this manner seems innovative. The empirical results are solid, with thorough testing across models and tasks.\n\nThe authors addressed many concerns in their response: clarifying the title, adding failure mode discussions, dataset stats, and exploring non-robust neurons. They also acknowledged limitations and future directions, like handling bias, which is responsible.\n\nPotential weaknesses include the dataset's inter-annotator agreement and the focus on vision without NLP examples. However, the paper's scope is clear post-title change. The experiments are comprehensive within the visual domain, and the applications are well-validated.\n\nOverall, the paper makes a meaningful contribution to interpretability in deep learning. The methodology is sound, the results are significant, and the authors have adequately addressed reviewer concerns. The minor issues don't outweigh the strengths, making this suitable for a top-tier conference.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper introduces MILAN, a novel method for generating natural language descriptions of neurons in deep visual models, addressing a critical gap in model interpretability. The work is well-executed, with rigorous experiments demonstrating MILAN’s generalizability across architectures, datasets, and tasks. Key strengths include:  \n1. **Technical Soundness:** The method is well-motivated, combining mutual information optimization with a human-annotated dataset (MILANNOTATIONS). The authors validated MILAN’s descriptions against human annotations and demonstrated its utility in analysis, auditing, and model editing.  \n2. **Significance:** The applications (e.g., detecting spurious correlations, improving robustness) are impactful and align with growing interest in transparent AI. The dataset and code release enhance reproducibility.  \n3. **Responsive Revisions:** The authors addressed reviewer concerns effectively: narrowing the title’s scope, expanding discussions of failure modes, adding dataset statistics, and exploring non-robust feature detection. They also provided preliminary results for suggested baselines (e.g., zero-shot captioning models), strengthening their claims.  \n4. **Limitations Acknowledged:** The paper acknowledges dataset biases, inter-annotator variability, and scope constraints (vision-only focus), while outlining future directions (e.g., bias mitigation, NLP extensions).  \n\nWhile minor issues remain (e.g., inter-annotator agreement in MILANNOTATIONS, limited discussion of non-vision tasks), these do not detract from the paper’s contributions. The work advances interpretability research and meets the standards of a top-tier conference."
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is titled \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning,\" focusing on differentially private data synthesis using deep generative models. The abstract highlights a one-shot sanitization approach to avoid privacy degradation during training, which is a common issue in gradient-based methods like DP-SGD. The reviewers have given a mix of positive and constructive feedback, so I need to weigh their comments and the authors' responses to determine if the paper meets the standards for a top-tier conference.\n\nReviewer 1 found the approach novel and well-motivated, with solid empirical results but noted the need for more thorough benchmarking, especially against methods like PATE-GAN and on more complex datasets like CIFAR-10. Reviewer 2 had some technical questions about privacy budget allocation and sensitivity calculations, which the authors addressed in their response, but also pointed out a lack of performance variation with different epsilon values. Reviewer 3 emphasized missing related work, the need for more baselines, datasets, and complexity analysis. The authors responded by adding references, datasets (Credit), baselines (DPCGAN), and a complexity analysis in the appendix.\n\nThe authors addressed most concerns: they added experiments on new datasets, included more baselines, provided complexity analysis, and clarified technical aspects. However, some issues remain. For example, they didn't test on CIFAR-10, citing the lack of SOTA DP methods for such data, which might be a valid point but leaves a gap in empirical validation. They also didn't include PATE-GAN due to replication issues, which could be a drawback if that's a standard baseline. The reviewers' scores were mostly 3s in technical and empirical novelty, indicating the work is solid but not groundbreaking.\n\nThe paper's strengths include a novel approach with theoretical guarantees, improved privacy handling compared to iterative methods, and reasonable empirical results. The weaknesses are the limited dataset scope and some missing baselines, though the authors have addressed some of these in their rebuttal. The key question is whether the remaining limitations are significant enough to warrant rejection at a top-tier conference. Given that the authors have made substantial revisions and the core contributions are sound, the paper might be acceptable despite some shortcomings. However, the lack of evaluation on more complex datasets like CIFAR-10 and certain baselines could be a concern for a top-tier venue expecting comprehensive evaluations.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel framework (PEARL) for differentially private data synthesis, addressing critical limitations of gradient-based methods (e.g., privacy degradation during iterative training). The authors convincingly argue that their one-shot sanitization approach avoids privacy budget inflation and demonstrate competitive empirical performance against baselines like DP-MERF and DP-GAN. While reviewers raised valid concerns about benchmarking breadth and technical clarity, the authors addressed most issues in their response:  \n- **Expanded Experiments:** Added the Credit dataset, DPCGAN baselines, and evaluation tasks (marginal release, range query, MMD).  \n- **Theoretical Clarifications:** Provided detailed explanations of sensitivity calculations, privacy budget allocation, and computational complexity.  \n- **Missing References:** Incorporated suggested related work (e.g., PrivSyn, PrivBayes).  \n\n**Remaining Concerns:**  \n1. **CIFAR-10 Evaluation:** The authors’ rationale for omitting CIFAR-10 (no SOTA DP methods for complex images) is reasonable but highlights a limitation in scope.  \n2. **PATE-GAN Baseline:** While replication challenges are noted, excluding a widely recognized baseline weakens empirical comparison.  \n\n**Strengths Outweigh Weaknesses:**  \n- **Technical Novelty:** The integration of characteristic functions and adversarial re-weighting offers a fresh perspective on private data synthesis.  \n- **Theoretical Rigor:** Privacy guarantees are rigorously derived, and the one-shot mechanism is a clear advancement over iterative methods.  \n- **Empirical Validity:** Results on tabular and image datasets (MNIST, Adult, Credit) show consistent improvements over baselines at practical privacy budgets.  \n\nFor a top-tier conference, the paper’s theoretical contributions, methodological innovation, and responsive revisions justify acceptance despite minor empirical gaps. The work advances the field of private generative modeling and provides a foundation for future research."
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about using tropical geometry for neural network compression. The reviewers have a mix of opinions, but there's a common thread in their feedback. Let me break down the key points.\n\nFirst, the strengths. The theoretical contribution seems solid. Reviewer 1 mentions the mathematical analysis is strong, and the novel tropical geometry perspective could inspire more research. The authors have a theorem linking approximation error to Hausdorff distance of tropical zonotopes, which is a new angle. That's a plus. Also, they proposed a compression algorithm using K-means, which outperforms some existing tropical methods.\n\nBut the weaknesses are significant too. The experiments are on small datasets (MNIST, Fashion-MNIST) and older architectures like LeNet and VGG. Reviewers 1, 2, and 3 all point out that testing on modern architectures (ResNet, Transformers) would be better. The authors argue that their work is a proof-of-concept, but for a top-tier conference, the empirical validation might be seen as insufficient. Also, the comparison to non-tropical methods is limited. ThiNet is from 2017, and there's a question about whether it's still state-of-the-art. Reviewer 1 notes that the claims in the abstract about advancing baseline methods and being competitive might be overreaching.\n\nAnother issue is the explanation of the bounds. Reviewer 3 questions why the error bounds decrease as more weights are removed, which seems counterintuitive. The authors responded that the bound is dominated by a parameter K, which might be loose. That's a valid point, but it could indicate that the theoretical results aren't as tight as needed for practical use.\n\nReviewer 4 is more positive, leaning towards acceptance because of the novelty, but they also mention the need for pseudo-code and better comparisons. The authors addressed some of these by providing code and clarifying their focus on structured pruning.\n\nThe authors' response to the reviews seems thorough. They added a new theorem, clarified the scope, and adjusted the abstract. They also provided code and addressed some methodological concerns. However, the core issue remains: the empirical evaluation is limited. They argue that their work is theoretical and the experiments are proof-of-concept, but top-tier conferences often expect both strong theory and solid empirical validation, especially in applied fields like ML.\n\nThe technical novelty is there, with Reviewer 1 giving a 4 for technical significance, but the empirical part is rated lower. The authors' focus on structured pruning and theoretical bounds is a contribution, but the practical impact is unclear. If the conference values theoretical advances even with limited experiments, it might be acceptable. However, given the emphasis on empirical results in ML conferences, the lack of modern architectures and datasets could be a deal-breaker.\n\nBalancing all this, the paper introduces a novel theoretical framework with some initial empirical support. The reviewers acknowledge the theoretical contribution but are concerned about the experiments. Since the authors have addressed some concerns and the theory is strong, but the empirical part is still lacking, I'm on the fence. However, given that the authors have clarified the scope as a theoretical contribution with proof-of-concept experiments, and considering the potential for inspiring future work, it might be acceptable. But the mixed reviews, especially the concerns about overclaiming results and outdated comparisons, could tip it towards rejection. In top-tier conferences, even theoretical papers need to have convincing empirical validation where applicable. The authors' response helps, but the empirical weaknesses remain significant.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel theoretical contribution by linking neural network approximation to tropical zonotopes via Hausdorff distance, offering a fresh geometric perspective. This theoretical foundation is its strongest asset, with potential to inspire future work. However, the empirical evaluation falls short of the standards expected at a top-tier conference. Key issues include:\n\n1. **Limited Empirical Scope:** Experiments focus on small datasets (MNIST, Fashion-MNIST) and outdated architectures (LeNet, VGG). While the authors frame this as \"proof-of-concept,\" modern benchmarks (e.g., ResNets, Transformers) are necessary to demonstrate broader applicability, especially given the emphasis on structured pruning for practical deployment.\n\n2. **Overstated Claims:** The abstract initially overstated comparisons to non-tropical methods (e.g., ThiNet from 2017 is not state-of-the-art). Though the authors revised the text, the empirical results still lack convincing evidence of competitiveness with contemporary pruning techniques, weakening the practical significance.\n\n3. **Theoretical Bounds and Reproducibility:** While the theoretical bounds are novel, their practical utility is questioned (e.g., bounds decreasing with fewer weights). The authors acknowledge looseness, but this limits immediate applicability. Additionally, reproducibility concerns (e.g., code availability, runtime/memory complexity) were only partially addressed.\n\n4. **Mixed Reviewer Sentiment:** Reviewers highlighted the theoretical novelty but consistently raised concerns about empirical rigor. Reviewer 1 and 3 emphasized the need for stronger experiments, while Reviewer 4’s acceptance lean was conditional on theoretical clarity, which the authors partially addressed.\n\n**Conclusion:**  \nWhile the theoretical contribution is noteworthy, the empirical evaluation does not sufficiently validate the method’s relevance to modern neural network compression. For a top-tier conference, even theoretically oriented work must demonstrate robust empirical validation where applicable. The authors’ revisions improved clarity but did not resolve fundamental limitations in experimental scope and claims. Thus, the paper is better suited for a venue more focused on theoretical machine learning or after further empirical validation."
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about a Variational Neural Cellular Automata (VNCA) model for generative tasks. The authors claim it's inspired by biological cellular processes and that it's a proper probabilistic model. The reviews are mixed, with some reviewers leaning towards acceptance and others towards rejection. \n\nFirst, looking at the strengths: The paper is well-written and clear, which is a big plus. The idea of using NCAs in a generative model is novel, even if some aspects relate to prior work like weight-tied ResNets or neural ODEs. The authors addressed some concerns in their rebuttal, adding experiments on damage recovery and latent space analysis. These new sections might strengthen the paper's contribution by showing the model's robustness and unique properties compared to traditional VAEs.\n\nHowever, the main criticism revolves around the underwhelming experimental results. The VNCA doesn't match state-of-the-art models in terms of likelihoods or sample quality, especially on datasets beyond MNIST. Reviewers 1, 4, and 6 point out that the practical impact is limited because the results aren't competitive. While the authors argue that showing NCAs can model data distributions is novel, some reviewers feel this isn't enough if the performance isn't there. \n\nAnother concern is the technical novelty. Reviewers 2 and 6 mention that the connection to weight-tied ResNets isn't sufficiently explored, and the NCA interpretation might not add much beyond existing architectures. The rebuttal clarifies that the key contribution is showing NCAs can handle complex distributions through morphogenesis, but whether this is a significant enough contribution is debatable.\n\nThe authors did address some issues, like adding references and clarifying the model's relation to prior work. They also provided more analysis on damage recovery, which showcases the model's unique resilience. However, the core issue of performance gaps remains. For a top-tier conference, even if the idea is interesting, the empirical results need to be strong. The fact that the VNCA lags behind SOTA on CelebA and other datasets is a red flag.\n\nOn the other hand, Reviewer 3 and 5 were positive, especially after the rebuttal, highlighting the paper's value in bridging NCA with generative models. The damage recovery experiments and the self-organizing aspect could be seen as a meaningful contribution, even if the raw performance isn't top-tier. The paper might open new research directions, which is a plus.\n\nBalancing all this, the paper introduces a novel approach with some interesting properties but doesn't achieve SOTA results. Top conferences often look for both novelty and strong empirical validation. The authors made efforts to improve the paper, but the performance gap and questions about technical novelty might still hold it back. However, the unique aspects of self-organization and robustness could justify acceptance as a contribution to an emerging area, even if the application isn't yet competitive with existing methods.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper introduces the Variational Neural Cellular Automata (VNCA), a novel generative model inspired by biological cellular processes. While the experimental results (e.g., likelihoods, sample quality) lag behind state-of-the-art methods, the work makes meaningful contributions to the growing field of neural cellular automata (NCA) by demonstrating their applicability to probabilistic generative modeling. Key strengths include:  \n1. **Novelty and Conceptual Value**: The VNCA is the first proper probabilistic generative model based on NCAs, bridging gaps between self-organizing systems and deep generative models. The self-organizing dynamics and inherent robustness to perturbations (demonstrated in new damage-recovery experiments) offer unique insights distinct from traditional VAEs.  \n2. **Methodological Rigor**: The authors rigorously evaluate the VNCA across datasets (MNIST, CelebA, Noto Emoji), address reproducibility concerns, and provide a thorough analysis of latent space structure and model behavior.  \n3. **Rebuttal Improvements**: The authors strengthened the paper by adding experiments on damage recovery, latent space analysis, and clarifying connections to prior work (e.g., weight-tied ResNets). These additions highlight the VNCA’s unique properties, such as stability and iterative refinement.  \n\nWhile the empirical performance gap to SOTA is a limitation, the paper’s focus on foundational insights into NCA-based generative processes and its potential to inspire future research in self-organizing systems justify acceptance. The work aligns with the exploratory nature of top-tier conferences, where novel paradigms with demonstrated promise are valued even if not yet fully optimized."
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes an online knowledge distillation framework using a specialist ensemble with label prior shift and post-compensation. The main idea is to diversify teachers in the ensemble by skewing their training data distributions, which they argue improves student performance and calibration. The authors conducted extensive experiments showing improvements over existing methods on various datasets.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 and 3 are somewhat positive but have concerns about novelty and the need for more ablations. Reviewer 2 and 5 are more critical, pointing out issues with baseline comparisons and novelty. Reviewer 4 also mentions missing comparisons with recent works. The authors responded by adding experiments comparing with PCL, MVCL, CGL, and L-MCL, showing their method's superiority in error rates and calibration.\n\nThe strengths here are the thorough empirical evaluation and addressing some of the reviewers' concerns with additional experiments. The method seems effective, especially in calibration error, which is highlighted as a notable result. The authors also provided theoretical grounding and reproducibility details.\n\nHowever, the main weaknesses revolve around technical novelty. Multiple reviewers noted that the components (label shift, importance sampling, PC-Softmax) are not entirely new, and the combination might be incremental. While the framework's application to online KD is novel, the core ideas are adaptations of existing techniques. The authors argue their contribution is the novel application context, but some reviewers remain unconvinced about the significance.\n\nThe rebuttal adds important comparisons, which strengthen the empirical case. However, the novelty concern isn't fully mitigated. In top-tier conferences, even solid empirical results might not suffice if the technical novelty is deemed insufficient. The authors' responses clarify their contributions but may not have convinced all reviewers that the work is groundbreaking.\n\nConsidering the balance between empirical strength and novelty, and the fact that some reviewers still find the contributions marginal, the paper might fall short of the high bar for a top-tier conference. However, the thorough experiments and improvements in calibration could tip the scale if the committee values application novelty and practical impact.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a well-structured framework for online knowledge distillation (KD) using label prior shift and specialist ensembles, demonstrating empirical improvements in accuracy and calibration. However, key concerns remain unresolved:  \n1. **Novelty:** Multiple reviewers highlighted that core components (label shift, importance sampling, PC-Softmax) are adaptations of existing techniques. While the application to online KD is novel, the technical contributions are perceived as incremental rather than groundbreaking.  \n2. **Baseline Comparisons:** Despite the authors’ rebuttal with additional experiments, some reviewers questioned whether the improvements over methods like PCL and MVCL are sufficiently significant (e.g., marginal gains on ImageNet). Comparisons with recent works (e.g., L-MCL) remain incomplete.  \n3. **Methodological Justification:** Questions about the necessity of specific design choices (e.g., uniform teacher averaging vs. weighted aggregation) and the specialization of the setup (online vs. offline KD) were not fully addressed.  \n4. **Reproducibility Concerns:** Reviewer 2 noted inconsistencies in reproducing baseline results, raising doubts about the robustness of the empirical claims.  \n\nWhile the paper’s empirical results and calibration improvements are commendable, the perceived lack of technical novelty and unresolved methodological critiques make it fall short of the high bar for acceptance at a top-tier conference. The work would benefit from deeper theoretical insights and a clearer articulation of its unique contributions relative to prior art."
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is \"Behavior Proximal Policy Optimization\" which proposes using an on-policy algorithm, specifically a modified PPO, for offline RL. The main claim is that the inherent conservatism of on-policy methods addresses the overestimation issue in offline RL. The authors present theoretical analysis and empirical results on the D4RL benchmark.\n\nFirst, looking at the reviews. Reviewer 1 raises concerns about the clarity of some sections, lack of experiments showing overestimation mitigation, and questions about the algorithm's soundness. They also note that the method might not be as novel as claimed, suggesting it's an off-policy version of PPO rather than a true online method. Reviewer 2 points out that the theoretical contributions overlap with prior work like GePPO and question the advantage approximation's soundness. Reviewer 3 is more positive but has technical questions. Reviewer 4 initially had concerns about empirical significance but upgraded to weak accept after rebuttal, though still unsure about practical benefits. Reviewer 5 questions assumptions and the replacement of advantage terms, and the convergence guarantees.\n\nThe authors' response addresses several points. They modified Assumption 1 to be data-dependent, added experiments on Antmaze and other tasks, included ablation studies, and compared with more baselines. They argue that replacing the advantage with the behavior policy's helps avoid overestimation, even if it loosens the bound. They also highlight significant performance gains on Adroit, Antmaze, and Kitchen tasks.\n\nNow, considering the strengths: The paper presents a simple modification to PPO that works in offline settings, which is a practical contribution. The theoretical analysis, while contested, attempts to ground the method in policy improvement theory. The empirical results, especially on harder tasks like Adroit and Antmaze, show notable improvements over existing methods, which is a strong point.\n\nWeaknesses include the theoretical overlap with prior work (GePPO), which might reduce novelty. The advantage replacement, while practical, lacks strong theoretical justification and might not hold in all cases. The experiments, though improved, still face skepticism about the significance of gains, especially in simpler environments. The assumption modifications and convergence guarantees remain points of contention.\n\nThe rebuttal adds necessary experiments and clarifies theoretical points, but some concerns linger. The core issue is whether the novelty and empirical results are sufficient for a top-tier conference. The method's simplicity and performance on challenging tasks are strengths, but the theoretical contributions might be incremental given prior work. However, the practical effectiveness and the thorough empirical evaluation, especially in harder domains, could justify acceptance despite some theoretical shortcomings.\n\nFinal Decision: Accept\n</think>\n\nThe paper \"Behavior Proximal Policy Optimization\" proposes a novel approach to offline reinforcement learning (RL) by leveraging the inherent conservatism of on-policy algorithms like PPO. The authors argue that online on-policy methods can naturally address the overestimation problem in offline RL and introduce BPPO, a modified version of PPO that avoids explicit constraints or regularization. The theoretical analysis derives an offline monotonic policy improvement bound, and empirical results on the D4RL benchmark demonstrate competitive or superior performance compared to state-of-the-art methods, particularly on challenging tasks like Adroit, Antmaze, and Kitchen.\n\n### Key Strengths:\n1. **Empirical Performance**: BPPO achieves significant improvements over existing methods on complex tasks (e.g., +61.7% on Adroit, +24.6% on Antmaze), demonstrating its effectiveness in diverse and sparse-reward settings. These results are compelling and highlight practical relevance.\n2. **Simplicity**: The algorithm is a straightforward modification of PPO, making it easy to implement and reproduce. This aligns with recent trends favoring simple yet effective methods in RL.\n3. **Theoretical Contribution**: The offline monotonic policy improvement framework, while debated for overlap with prior work (e.g., GePPO), provides a principled foundation for the method. The authors addressed concerns about assumptions by revising them to be data-dependent, improving rigor.\n\n### Key Weaknesses:\n1. **Theoretical Novelty**: The policy improvement analysis overlaps with generalized off-policy monotonic improvement frameworks (e.g., GePPO). While the advantage replacement is novel, its theoretical justification (e.g., looser bounds) remains underdeveloped.\n2. **Assumption Validity**: Even after revisions, assumptions about policy closeness and advantage estimation lack full empirical validation. For example, the impact of replacing \\(A_{\\pi_k}\\) with \\(A_{\\pi_\\beta}\\) on bound tightness is not rigorously analyzed.\n3. **Marginal Gains on Standard Benchmarks**: While BPPO excels on harder tasks, its improvements on simpler Gym environments are modest, raising questions about its broad applicability.\n\n### Rebuttal Impact:\nThe authors addressed several concerns:\n- **Assumption Revision**: Replaced Assumption 1 with a data-dependent divergence metric, improving theoretical grounding.\n- **Additional Experiments**: Added results on Antmaze and ablations (e.g., clip decay analysis), strengthening empirical claims.\n- **Clarified Novelty**: Differentiated BPPO from Onestep RL and explained the necessity of advantage replacement to avoid overestimation.\n\nHowever, lingering issues remain:\n- The theoretical overlap with GePPO and other off-policy TRPO/PPO methods is not fully resolved.\n- The practical significance of performance gains (e.g., small improvements on Gym tasks) is still debated.\n\n### Conclusion:\nDespite theoretical concerns, the paper’s empirical contributions are substantial. The strong performance on challenging tasks like Adroit and Antmaze, combined with the method’s simplicity, make it a valuable addition to the offline RL literature. While the theoretical analysis is incremental, the practical insights and results justify acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about detecting out-of-distribution (OOD) data using graph structures based on features from images. The main idea is to represent images as graphs of visual concepts and use graph embeddings for OOD detection. The authors claim high AUROC scores on LSUN dataset, comparable to state-of-the-art methods.\n\nLooking at the reviewers' comments, there's a mix of opinions but mostly leaning towards rejection. Reviewer 1 points out strengths like clear intuition and extensive comparisons but criticizes vague terms, lack of detailed methodology, and insufficient validation against SOTA. They also mention grammatical issues. Reviewer 2 is harsher, saying the paper is not well-prepared, lacks proper comparisons, and has confusing notations. They recommend strong reject. Reviewer 3 thinks the idea is interesting but lacks novelty and proper experiments, especially comparisons with existing methods. Reviewer 4 finds the idea novel but criticizes reliance on a pre-trained object detector and poor writing, along with missing experiments and details.\n\nThe common themes across reviews are: \n1. Lack of comparison with existing SOTA methods. The authors only compare with some graph embedding techniques but not the latest OOD detection approaches. This makes their claims about performance questionable.\n2. Methodological clarity and reproducibility issues. The paper doesn't provide enough details on the feature extraction process, graph construction, and hyperparameters. Without this, others can't reproduce the results.\n3. Writing quality and presentation. Grammatical errors, unclear explanations, and missing references to related work (like the mentioned survey) weaken the paper's credibility.\n4. Over-reliance on a pre-trained object detector. If the detector isn't accurate or doesn't cover the in-distribution data's semantics, the method might fail, but this isn't analyzed.\n5. Lack of ablation studies and broader experiments. Only testing on LSUN isn't sufficient; other datasets like CIFAR or ImageNet would strengthen the results.\n\nThe authors didn't provide a response, so there's no rebuttal to these points. For a top-tier conference, the paper needs to have strong technical novelty, rigorous evaluation, and clear presentation. While the idea of using graph structures is interesting, the execution seems lacking. The reviewers, especially 2 and 4, point out critical flaws in the experimental setup and comparisons. Without addressing these, the paper's contributions aren't validated properly. The technical novelty is also questioned, as it's seen as combining existing methods without significant innovation.\n\nGiven the high standards of top-tier conferences, the paper's current state with major issues in methodology validation, comparison to SOTA, and clarity would likely lead to rejection. Even though some reviewers acknowledge potential, the consensus leans towards the paper not being ready for publication without substantial revisions.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents an intriguing approach leveraging graph structures for OOD detection, with potential for interpretability. However, critical flaws highlighted by reviewers make it unsuitable for acceptance at a top-tier conference:  \n1. **Lack of SOTA Comparisons:** The authors claim performance \"comparable to state-of-the-art\" but fail to directly compare with established OOD detection methods (e.g., Mahalanobis distance-based or self-supervised approaches). This undermines their contribution’s significance.  \n2. **Methodological Ambiguity:** Key details (e.g., graph construction, object detector architecture, hyperparameters) are insufficiently described, hindering reproducibility. The reliance on a pre-trained object detector’s accuracy is also unexamined.  \n3. **Weak Experimental Scope:** Experiments are limited to LSUN without ablation studies, broader dataset validation (e.g., CIFAR, ImageNet), or analysis of interpretability claims.  \n4. **Writing and Presentation Issues:** Poorly defined terminology, grammatical errors, and missing references (e.g., OOD detection surveys) reduce clarity and scholarly rigor.  \n5. **Marginal Novelty:** While the graph-based representation is novel in context, reviewers note the approach largely combines existing techniques without transformative innovation.  \n\nThough the idea has merit, the paper’s unresolved methodological and empirical shortcomings, coupled with inadequate validation against benchmarks, render it premature for publication in its current form."
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about the convergence analysis of gradient descent (GD) for deep linear neural networks. The authors claim to provide sharp convergence rates that are independent of initialization type and network depth, given sufficient width. The reviewers have mixed opinions, with some praising the clarity and technical contributions, while others question the novelty and depth of the analysis compared to prior work.\n\nFirst, looking at the strengths: The paper addresses an important theoretical question in deep learning—understanding why GD works well despite non-convexity. The results suggest that overparameterization allows GD to achieve rates comparable to convex problems, which is a significant insight. The addition of Section 4 in response to reviews adds valuable discussion on why overparameterization avoids bad saddles and the necessity of width, which addresses some concerns about the lack of intuitive explanations.\n\nHowever, the main criticisms revolve around novelty. Reviewers 1, 3, and 5 point out that parts of the analysis, especially the convergence rates for Gaussian and orthogonal initializations, build heavily on prior works like Du & Hu (2019) and Hu et al. (2020). The authors argue that their extension to general loss functions and varying layer widths presents technical challenges, such as handling low-rank data directly and using seminorm analysis. While these points are valid, some reviewers remain unconvinced that these extensions are sufficiently novel for a top-tier conference.\n\nThe trajectory result (Theorem 3.3) is highlighted as a novel contribution, showing that GD on deep linear networks follows the convex trajectory. This seems to be a new insight not present in prior works, which could be a strong point in favor of acceptance. However, the reliance on existing techniques for parts of the analysis might dilute the perceived novelty.\n\nThe authors made several revisions, including modifying the title to emphasize overparameterization, adding discussions on constants and the role of width, and addressing font size issues in figures. These changes improve the paper's clarity and mitigate some overclaiming issues. Reviewer 6 initially recommended rejection due to lack of insight but noted that the issues could be fixed, and the authors did address these points in their response.\n\nBalancing the reviews, the paper presents a solid theoretical contribution with some novel elements, particularly in the trajectory analysis and the discussion of overparameterization's role. While parts of the technical analysis build on prior work, the extensions and new insights justify publication in a top-tier venue. The reviewers' concerns were addressed adequately in the rebuttal, enhancing the paper's contributions and clarity.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper provides a rigorous analysis of gradient descent (GD) convergence for overparameterized deep linear neural networks, offering insights into why GD avoids bad saddles and achieves depth-independent rates akin to convex optimization. While some technical components extend prior work (e.g., Du & Hu 2019), the novel contributions—particularly Theorem 3.3, which establishes trajectory equivalence between GD on linear networks and convex models—are significant. The authors addressed reviewer concerns by revising the title/abstract to avoid overclaims, adding a detailed discussion section (Section 4) on overparameterization’s role, and clarifying technical extensions (e.g., handling low-rank data and general loss functions). Though some reviewers questioned novelty, the trajectory analysis and theoretical insights into overparameterization’s impact on optimization dynamics are sufficiently novel and impactful for a top-tier conference. The rebuttal effectively resolved most concerns, justifying acceptance."
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. So, the paper is about combining Transformers and Monte Carlo Tree Search (MCTS) for offline reinforcement learning, specifically tested on SameGame. The reviewers have a mix of opinions, but mostly leaning towards rejection. Let me go through their points step by step.\n\nFirst, Reviewer 1 thinks the idea is promising but has issues with clarity and empirical evaluation. They mention the paper is unclear in exposition, uses a single toy domain (SameGame), and lacks comparisons to relevant baselines like Decision Transformers or AlphaZero. The correctness score is 2, which is not great. The technical novelty is rated 3, so some novelty but not groundbreaking. The empirical results are considered weak because of the limited experiments.\n\nReviewer 2 is harsher, saying the novelty is very poor because it's similar to AlphaGo/AlphaZero but replacing ResNet with a Transformer. They also point out that the experiments are unfair because they compare different simulation counts and don't use the same time limits. Also, comparing to a random player isn't meaningful. The technical and empirical novelty scores are both 1, which is bad. They also mention that the method is easy to reproduce but lacks code.\n\nReviewer 3 also criticizes the limited novelty, being just a combination of existing methods. They want more comparisons with prior work on SameGame and other methods like behavior cloning. The experiments are not convincing because they don't use standardized test sets or compare with enough baselines. The clarity is okay but has typos. The technical and empirical significance are rated 2, so low.\n\nThe authors didn't provide a response, which is a problem because they can't address the reviewers' concerns. \n\nPutting this all together: The main issues are lack of novelty (since combining MCTS and neural networks is similar to AlphaZero, just with Transformers), limited and non-standard experiments (only SameGame, no comparisons to key baselines), and clarity issues in writing. The reviewers agree that the idea has potential but the execution is lacking. For a top-tier conference, the paper needs strong empirical validation and clear novelty. The authors didn't address the major concerns about the experimental setup and comparisons. Without these, the paper's claims aren't sufficiently supported. Even though the idea of combining Transformers with MCTS is interesting, the current evaluation doesn't demonstrate its superiority or general applicability beyond a single game. The lack of comparison to AlphaZero-like methods is a big red flag. Also, the writing issues make the method hard to understand, which is another negative point. \n\nGiven all this, despite the potential of the idea, the paper in its current form doesn't meet the standards for acceptance. The reviewers' criticisms about empirical validation and novelty are too significant to overlook. Therefore, the decision should be to reject.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes an intriguing idea of integrating Transformers with MCTS for offline RL, but critical flaws highlighted by reviewers make it unsuitable for acceptance at a top-tier conference. Key issues include:  \n\n1. **Limited Novelty**: The core methodology resembles AlphaZero’s framework (MCTS + neural networks), with Transformers replacing ResNet. Reviewers unanimously criticized the lack of technical novelty, as the combination of MCTS and neural networks is well-established. The adaptation to Transformers, while potentially useful, is not sufficiently differentiated from prior work.  \n\n2. **Weak Empirical Validation**:  \n   - **Narrow Evaluation**: Experiments are confined to SameGame, a \"toy-ish\" domain, without comparisons to standardized benchmarks (e.g., Atari, MiniGo) or widely accepted baselines (e.g., Decision Transformers, AlphaZero).  \n   - **Unfair Comparisons**: MCTS baselines lack learned priors/value functions, and results are compared using simulation counts rather than time limits, skewing fairness. Comparisons to random policies are uninformative.  \n   - **Missing Baselines**: No comparison to offline RL methods (e.g., behavior cloning) or prior SameGame-specific algorithms (e.g., Schadd et al., Cazenave) weakens claims of superiority.  \n\n3. **Clarity and Presentation**: The paper suffers from unclear exposition, underspecified figures, and typographical errors. Key components (e.g., MCTS baseline details, rollout policy) are poorly explained, hindering reproducibility and understanding.  \n\n4. **Lack of Author Response**: The absence of rebuttals leaves critical concerns unaddressed, such as experimental fairness, domain generality, and novelty.  \n\nWhile the idea of combining Transformers with MCTS has potential, the paper fails to demonstrate significant technical or empirical advancements over existing approaches. For a top-tier venue, rigorous validation, broader applicability, and clearer articulation of contributions are essential. The current execution does not meet these standards."
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper and the reviews. The paper proposes a new contrastive learning objective to address text degeneration in language models. The main idea is to adjust the loss function to penalize repetitive tokens more effectively than existing methods like unlikelihood training. \n\nReviewer 1 points out that the novelty is thin, arguing that the core idea is too similar to unlikelihood training and that the authors misrepresented the UL approach by not acknowledging that UL also considers positive tokens. This is a significant concern because if the method isn't sufficiently novel, it might not meet the bar for a top-tier conference. Additionally, Reviewer 1 criticizes the experiments for being insufficient, suggesting that testing on tasks like summarization or machine translation would be more convincing than just perplexity and repetition metrics.\n\nReviewer 2 also questions the experimental setup, noting that using GPT-2 small is outdated and that the metrics (like perplexity) might not be reliable indicators of quality. They mention that other metrics like dist-2 or dist-3 are more standard, and the choice of beam search is unclear. The lack of comparison with newer sampling methods like typical sampling is another weakness. However, they acknowledge the simplicity and potential of the method.\n\nReviewer 3 and 4 both agree that the paper is clear and the idea is intuitive but lack novelty. Reviewer 4 also brings up the point that larger models might not suffer as much from repetition, so the experiments on GPT-2 small might not show the method's effectiveness on state-of-the-art models. The human evaluation results not being statistically significant is another issue, weakening the claims about improved fluency.\n\nThe authors didn't provide a response, which is a problem because some of the reviewers' concerns (like the novelty and experimental setup) could have been addressed. Without a rebuttal, it's hard to know if the issues raised are valid or if the authors have counterarguments.\n\nConsidering all this, the main issues are:\n1. **Novelty**: The method is seen as incremental over unlikelihood training, with reviewers pointing out that the differences might not be substantial enough. The misrepresentation of UL's approach in the paper is a red flag.\n2. **Experiments**: The experiments are limited in scope (small models, few tasks, questionable metrics). Without testing on larger models or more tasks, the claims of state-of-the-art performance are not fully supported.\n3. **Significance**: The improvements shown might not be significant enough, especially since human evaluations didn't reach statistical significance. The practical utility for larger models is unclear.\n\nFor a top-tier conference, these factors are critical. Even though the paper is well-written and the idea is simple, the lack of strong novelty and comprehensive experiments makes it fall short. The reviewers' consensus leans towards rejection due to these shortcomings. Without a rebuttal to address these points, the decision would likely be to reject.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes a contrastive token learning objective to mitigate text degeneration in language models. While the idea is intuitive and the method is simple to implement, the reviews highlight critical shortcomings that undermine its suitability for a top-tier conference:  \n\n1. **Novelty Concerns**: Multiple reviewers (1, 3, 4) argue that the work is incremental over unlikelihood training (UL). Reviewer 1 specifically criticizes the misrepresentation of UL’s formulation, noting that UL already incorporates both positive and negative token probabilities, which the authors downplay. This weakens the claim of technical novelty.  \n\n2. **Insufficient Experiments**:  \n   - The experiments focus on GPT-2 small, which is outdated compared to modern large-scale LMs. Reviewer 2 and 4 question whether the method’s benefits hold for larger models.  \n   - Metrics like perplexity and dist-1 are criticized as unreliable or incomplete (Reviewer 2, 4). The lack of evaluation on downstream tasks (e.g., summarization, translation) further limits the scope.  \n   - Human evaluations lack statistical significance (Reviewer 4), weakening claims about fluency improvements.  \n\n3. **Methodological Gaps**:  \n   - The choice of beam search decoding and the exclusion of recent sampling techniques (e.g., typical sampling) are flagged as limitations (Reviewer 2).  \n   - The impact of using only *M* previous tokens (vs. all tokens in UL) is not rigorously analyzed (Reviewer 4).  \n\n4. **Author Response Absence**: The lack of rebuttal leaves critical questions unanswered, such as the novelty distinction from UL and experimental scalability.  \n\nWhile the paper’s clarity and reproducibility are strengths, the incremental technical contribution, limited empirical validation, and unresolved methodological concerns make it fall short of the high bar for acceptance at a top-tier conference."
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question investigates the order of Batch Normalization (BN) and activation functions, specifically focusing on bounded activations like Tanh. The main finding is that placing BN after the activation (Swap model) leads to better performance compared to the conventional order (BN before activation). The authors attribute this improvement to asymmetric saturation and increased sparsity in the activation outputs.\n\nFirst, looking at the strengths: The topic is relevant as BN and activation functions are fundamental components in neural networks. The discovery that swapping the order improves performance for bounded activations is interesting and counterintuitive, especially since saturation is typically seen as problematic. The paper provides empirical evidence across various architectures and datasets, which adds credibility. The authors also address some reviewer concerns in their response, such as adding experiments with different weight decay values and clarifying the model used for deeper layers.\n\nHowever, there are significant weaknesses. Reviewers pointed out that the arguments linking asymmetric saturation and sparsity to improved generalization are not fully convincing. The analysis is limited to certain architectures without residual connections, which reduces the practical applicability since most modern networks use ResNet-like structures. Additionally, the Swap model with Tanh doesn't outperform ReLU-based models, which are standard, making the practical contribution less impactful. Some metrics and claims, like the saturation and sparsity measures, were questioned for their validity and completeness. The authors' responses, while addressing some points, still leave gaps in explaining why certain layers show lower saturation or how exactly asymmetric saturation leads to better generalization beyond correlation.\n\nThe reviewers' scores are mixed. Technical and empirical novelty are rated as moderate, with some reviewers noting that the contributions are incremental. Correctness issues were raised, especially regarding the robustness of claims and potential overstatements. The authors' rebuttal improved some aspects, like model adjustments for CIFAR and additional experiments, but core issues about the mechanism behind the performance improvement remain partially unresolved.\n\nConsidering the standards of a top-tier conference, the paper presents an intriguing empirical observation but lacks a solid theoretical foundation and broader applicability. The practical significance is limited since ReLU-based models still perform better, and the analysis doesn't extend to widely used architectures with residual connections. While the findings are interesting, the paper doesn't sufficiently bridge the gap between observation and underlying principles to warrant acceptance in a top-tier venue.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents an intriguing empirical observation that swapping Batch Normalization (BN) after bounded activation functions (e.g., Tanh) improves performance, attributed to asymmetric saturation and sparsity. While the core finding is novel and merits attention, the work falls short of the rigor and significance expected for a top-tier conference:  \n1. **Weak Causal Links:** The connection between asymmetric saturation, sparsity, and generalization remains correlational rather than causal. Reviewers highlighted inconsistencies in saturation/sparsity metrics across layers and insufficient justification for why these properties improve performance.  \n2. **Limited Practical Impact:** The Swap model with Tanh underperforms ReLU-based models (the standard in practice), reducing the work’s applicability. Experiments exclude residual architectures (e.g., ResNet), which dominate modern networks, limiting generalizability.  \n3. **Methodological Gaps:** Key claims (e.g., saturation metrics, weight decay effects) lack rigorous theoretical or empirical validation. Reviewer critiques about dataset-model mismatches (e.g., VGG16 on CIFAR) were partially addressed but not fully resolved.  \n4. **Overstated Contributions:** While the swapped BN-activation order is novel for bounded functions, prior work has explored BN-activation ordering (e.g., with ReLU), diminishing technical novelty.  \n\nThe authors’ rebuttal improved clarity (e.g., model adjustments, code details) but failed to address core issues like the mechanism linking saturation to generalization or broader architectural relevance. The work is a valuable empirical study but lacks the theoretical depth and practical significance required for acceptance at a top-tier venue."
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. So, the paper is about improving adversarial robustness by adding more regularization to less robust samples. The authors propose a new adversarial training algorithm called ARoW, which they claim is theoretically motivated and outperforms existing methods. \n\nLooking at the reviews, there's a mix of opinions. Some reviewers think the theoretical contributions are weak, especially regarding the comparison to TRADES. They argue that the theorem presented is just an extension of TRADES to multi-class problems and that the bounds are loose. Others point out that the experimental improvements are marginal, though consistent. However, some reviewers appreciate the practical value of the method, the thorough experiments, and the clear writing.\n\nThe authors responded to the reviews by providing additional experiments, clarifying the theoretical contributions, and addressing concerns about the comparison with HAT when using extra data. They showed slight improvements over HAT even with gradient accumulation and highlighted other advantages like easier implementation and fairness improvements.\n\nThe main issues seem to revolve around the theoretical novelty and the significance of empirical results. The theorem's extension to multi-class might not be groundbreaking, but the practical algorithm seems effective. The experiments, while showing small improvements, do consistently outperform baselines. However, the theoretical justification is questioned for being a loose bound and not as novel as claimed.\n\nIn top-tier conferences, theoretical rigor and significant empirical advancements are crucial. The paper's theory might not be strong enough, but the practical contributions and consistent results could justify acceptance if the community values application over theory. However, given the mixed reviews and the concerns about overclaiming theoretical contributions, the paper might be on the borderline. The authors addressed some concerns, but the core issues about the theorem's significance remain. \n\nConsidering all this, the paper might be a borderline case. However, since the experiments show consistent improvements and the method is practical, and given that some reviewers increased their scores after the rebuttal, I lean towards acceptance. The weaknesses are noted, but the practical contributions might be sufficient for a top-tier conference.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper proposes ARoW, a novel adversarial training method that applies stronger regularization to vulnerable samples. While concerns about theoretical novelty and tightness of bounds were raised, the authors addressed key points in their response:  \n1. **Theoretical Contribution:** The extension of TRADES to multi-class problems, though not groundbreaking, provides a foundation for the method. The rebuttal clarified the theorem's scope and adjusted claims to focus on multi-class settings.  \n2. **Empirical Results:** ARoW consistently outperforms baselines (e.g., TRADES, HAT) in standard and robust accuracy across datasets, even if improvements are marginal. Additional experiments with gradient accumulation and fairness analysis strengthen the case for practical utility.  \n3. **Relevance and Impact:** Adversarial robustness remains critical for real-world deployment. The method’s simplicity, reproducibility, and fairness improvements add value despite theoretical limitations.  \n\nWhile the theoretical contribution is modest, the empirical rigor, clear presentation, and addressing reviewer concerns during rebuttal justify acceptance at a top-tier conference. The work advances the field incrementally but meaningfully."
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper and the reviews. The paper proposes Collaborative Adversarial Training (CAT), which trains two models using different adversarial methods and uses their adversarial examples to guide each other. The main idea is leveraging the diversity in errors between different adversarial training approaches to improve robustness.\n\nReviewer 1 points out strengths like the framework and promising results but notes weaknesses in computational cost, lack of theory, and missing comparisons with ensemble methods. They rate correctness high but novelty as moderate. Reviewer 2 and 3 are more critical, citing low novelty, missing baselines, insufficient analysis, and lack of experiments on larger datasets. Reviewer 4 is somewhat positive but also raises concerns about computational overhead and the need for more ablations.\n\nThe key issues here are novelty, empirical validation, and comparison to prior work. The idea of combining adversarial training methods isn't entirely new; ensemble methods and collaborative training have been explored. The reviewers mention prior works like ensemble adversarial training and others that the authors didn't cite. This is a problem because it makes the paper's contribution seem less novel. \n\nThe computational cost is another concern. Training two models doubles the resources, which is a significant drawback. The authors didn't compare against training a single model with multiple attacks or objectives, which is a strong baseline. Without this comparison, it's unclear if the benefits come from collaboration or just using more data.\n\nThe experiments are limited to CIFAR datasets and smaller networks. Top-tier conferences often expect results on larger datasets like ImageNet to show scalability. The lack of such experiments weakens the paper's impact. Also, missing ablations (e.g., using the same method for both models) make it hard to isolate the effect of collaboration versus method diversity.\n\nTheoretical analysis is another gap. The paper lacks a solid explanation of why collaboration helps. Without this, the method feels heuristic. Reviewers wanted more analysis on why combining methods leads to better robustness, possibly through metrics like prediction diversity or theoretical guarantees.\n\nThe authors didn't respond to the reviews, which is a red flag. Addressing reviewer concerns is crucial, especially when there are valid criticisms. The absence of a response might mean the issues weren't fixed, leaving the paper's weaknesses unaddressed.\n\nOn the positive side, the method does achieve SOTA results on CIFAR-10 under AutoAttack, which is a strong empirical result. However, given the concerns about novelty, missing comparisons, and lack of scalability, these results might not be sufficient for a top-tier conference. The paper's core idea is interesting but overlaps with existing ensemble methods, and the execution has notable gaps in analysis and experiments.\n\nIn conclusion, while the paper presents a useful approach, the lack of thorough comparison with relevant baselines, insufficient theoretical grounding, and limited experimental scope make it fall short of the standards expected in a top-tier conference. The reviewers' criticisms, especially regarding novelty and empirical validation, are significant enough to warrant rejection.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes an intuitive collaborative adversarial training (CAT) framework that leverages complementary robustness between different adversarial training methods. While the empirical results on CIFAR-10/100 are promising, the work falls short of top-tier conference standards due to critical weaknesses:  \n\n1. **Marginal Novelty:** The core idea of combining adversarial training methods overlaps with prior ensemble-based defenses (e.g., ensemble adversarial training, diversity-promoting methods). The authors fail to adequately differentiate CAT from these approaches or cite relevant literature, weakening the technical novelty.  \n\n2. **Insufficient Empirical Validation:**  \n   - Experiments are limited to small-scale datasets (CIFAR-10/100) and lack validation on larger benchmarks (e.g., ImageNet), raising concerns about scalability.  \n   - Key baselines are missing, such as training a single model with multiple attacks/objectives or comparing against recent state-of-the-art methods (e.g., AWP).  \n   - Ablation studies are incomplete (e.g., training two identical models, varying the number of collaborators, or analyzing computational trade-offs).  \n\n3. **Lack of Theoretical Analysis:** The paper does not provide a theoretical justification for why collaborative training improves robustness. The absence of metrics (e.g., prediction diversity between models) or analysis of error correlations limits interpretability.  \n\n4. **Computational Overhead:** Training two models doubles the computational cost, a significant drawback given the already high expense of adversarial training. The authors do not justify this trade-off or explore efficiency optimizations.  \n\n5. **Unaddressed Reviewer Concerns:** The absence of an author response exacerbates these issues, leaving critical questions (e.g., comparison to data augmentation, catastrophic overfitting, theoretical motivation) unanswered.  \n\nWhile the idea of collaborative adversarial training is interesting, the paper’s technical contributions are incremental, and the empirical evaluation lacks the depth and breadth required for a top-tier venue. Addressing these limitations would require substantial revisions and additional experiments."
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question investigates the order of batch normalization (BN) and activation functions, specifically focusing on bounded activations like Tanh. The main claim is that placing BN after the activation (Swap model) leads to better performance due to asymmetric saturation and increased sparsity. The reviewers have raised several concerns, and the authors have responded to them. \n\nStarting with the strengths: The paper tackles an interesting and somewhat understudied aspect of neural network architecture—how the order of BN and activation functions affects performance, especially with bounded activations. The discovery that the Swap model with Tanh outperforms the conventional order is intriguing and counterintuitive, given that saturation is typically seen as problematic. The authors provide empirical evidence across various architectures and datasets, which adds credibility. The introduction of metrics for saturation, skewness, and sparsity is a plus, offering a quantitative approach to analyzing activation distributions.\n\nHowever, the weaknesses pointed out by the reviewers are significant. A major issue is the lack of strong evidence linking asymmetric saturation and sparsity to improved generalization. While the authors present correlations, the causal relationship isn't convincingly established. For instance, Reviewer 2 questions whether the observed sparsity is a direct result of asymmetric saturation or if other factors are at play. The authors' response about normalization shifting the distribution to increase sparsity is somewhat hand-wavy and lacks rigorous mathematical proof. \n\nAnother concern is the practical relevance. The Swap model with Tanh doesn't outperform ReLU-based models, which are standard in practice. This limits the paper's impact since the proposed method doesn't offer a clear advantage over existing approaches. Reviewer 3 and 5 both highlight this, noting that the findings might not be compelling enough for practitioners to switch from ReLU. \n\nThe reviewers also pointed out issues with the experimental setup. The use of VGG-like architectures without residual connections and the focus on smaller datasets like CIFAR-100 raise questions about the generalizability of the results. The authors mention testing ResNet20 with some success, but these results are not central to the paper, leaving doubts about applicability to more complex models. Additionally, the saturation metrics and their interpretation are critiqued for not being comprehensive, especially in deeper layers where saturation decreases. The authors adjusted the model (VGG16_11) to address this, but it's unclear if this adjustment is standard or ad-hoc, potentially affecting reproducibility.\n\nTechnical novelty is another sticking point. While the empirical observation is new, the underlying mechanisms (BN after activation) have been explored before. The theoretical contributions are limited, with the analysis relying heavily on empirical correlations rather than deriving principles from first principles. Reviewer 4's request for analytical derivations at initialization to confirm asymmetric saturation's independence from the dataset wasn't fully addressed, leaving a gap in the theoretical foundation.\n\nThe authors' responses do mitigate some concerns, like providing additional experiments with weight decay and clarifying definitions. However, key issues remain unresolved. For example, the link between asymmetric saturation and generalization isn't conclusively proven, and the practical benefits over ReLU are still lacking. The rebuttal adds some experiments (e.g., ResNet20 results), but these are supplementary and don't form the core analysis.\n\nConsidering the criteria for a top-tier conference, the paper presents an interesting empirical finding but falls short in providing a robust theoretical explanation and practical significance. The mixed reviewer scores (some 3s and 2s in correctness and novelty) reflect this. While the work is valuable, it may not meet the high bar for acceptance without stronger evidence of the mechanisms at play and broader applicability.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents an intriguing empirical observation that swapping the order of batch normalization and bounded activation functions (e.g., Tanh) improves performance via asymmetric saturation and sparsity. However, significant weaknesses undermine its suitability for a top-tier conference:  \n1. **Weak Causal Link:** The connection between asymmetric saturation, sparsity, and generalization remains correlational rather than causal. While the authors provide metrics and experiments, they fail to rigorously prove that these factors directly cause improved performance.  \n2. **Limited Practical Impact:** The Swap model with bounded activations does not outperform ReLU-based models, which are standard in practice. This reduces the work’s applicability and novelty.  \n3. **Theoretical Gaps:** The analysis lacks mathematical rigor (e.g., no derivation of how BN relocation induces sparsity) and relies heavily on empirical correlations. Key claims about saturation and gradient dynamics are under-supported.  \n4. **Experimental Scope:** The focus on non-residual architectures (e.g., VGG-like models) and smaller datasets (CIFAR) limits generalizability. While ResNet20 results are mentioned, they are peripheral.  \n5. **Reviewer Consensus:** Reviewers highlighted unresolved issues (e.g., saturation metrics, layer-wise inconsistencies, unclear definitions) and rated technical/empirical novelty as marginal.  \n\nWhile the core observation is interesting, the paper does not meet the high bar for theoretical or practical significance required by top-tier venues. Further work is needed to establish causality, generalize findings, and demonstrate clear advantages over existing methods."
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper proposes a bi-objective optimization method to maintain diverse playing styles in self-play reinforcement learning. The main idea is to balance skill level and diversity, which they claim helps in generalization. They tested it on Pong and Justice Online, showing better performance compared to some baselines.\n\nReviewer 1 has some major concerns. They mention missing related work, specifically Lanctot et al. 2017, which covers multi-agent RL frameworks like Fictitious Play. Not comparing to these could be a problem. Also, the experimental setup in Table 3 might be biased because EMOGI and BiO agents make up most of the opponent pool, giving them an unfair advantage. The reviewer questions the technical and empirical novelty, giving low scores.\n\nReviewer 2 is more positive but points out issues. They note that reducing playing style to a single scalar is a big simplification and needs discussion. Some parts of the methodology are unclear, like the variance calculation. They also ask for more details on experiments, like confidence intervals. However, they rate correctness high and see some novelty.\n\nReviewer 3 highlights strengths like improving over baselines and straightforward implementation. But weaknesses include few domains tested (only two games), missing comparisons to quality-diversity algorithms like MAP-Elites, and possible overfitting in experiments. They also mention the need for more baseline comparisons, like domain randomization. The reviewer thinks the technical novelty is marginal.\n\nThe common threads here are: \n1. Missing key related works (Lanctot, quality-diversity methods).\n2. Experimental design flaws (opponent sampling bias, lack of statistical significance details).\n3. Over-simplification of playing style as a scalar without sufficient justification.\n4. Limited empirical evaluation (only two games, no comparison to important baselines).\n\nThe authors didn't provide a response, so these issues aren't addressed. For a top-tier conference, these are significant problems. The technical contribution seems incremental, combining existing concepts (NSGA-II with self-play) without clear differentiation from prior work. The experiments don't convincingly show superiority due to possible biases and lack of breadth. The missing baselines like Fictitious Play or MAP-Elites make it hard to assess the true novelty and effectiveness.\n\nWhile the idea is interesting, the paper as it stands has critical flaws in its literature review, experimental setup, and validation. These issues likely make it unsuitable for a top-tier conference where novelty, rigor, and thorough comparisons are essential.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning**:  \nThe paper presents an interesting approach to maintaining diverse playing styles in self-play RL via a bi-objective optimization framework. However, critical flaws identified by reviewers undermine its suitability for a top-tier conference:  \n1. **Missing Baselines and Related Work**: The paper omits key comparisons to foundational methods like Fictitious Play (Lanctot et al., 2017) and quality-diversity algorithms (e.g., MAP-Elites), which are directly relevant to maintaining diverse populations. This omission raises doubts about the claimed novelty and significance.  \n2. **Experimental Design Issues**: The opponent sampling strategy in Table 3 disproportionately favors the proposed method (BiO) and EMOGI, as they dominate the opponent pool. This introduces bias, making it unclear whether BiO’s performance stems from genuine superiority or experimental setup.  \n3. **Simplification of Playing Style**: Reducing \"playing style\" to a single scalar is a significant limitation that lacks justification. While the approach works in the tested domains, the paper does not address how this simplification affects generalizability or scalability to higher-dimensional style representations.  \n4. **Limited Empirical Validation**: The evaluation spans only two games (one simple, one complex), and lacks comparisons to domain randomization or other competitive RL benchmarks. This narrow scope limits the ability to assess the method’s broader applicability.  \n5. **Technical Novelty Concerns**: The core idea—combining NSGA-II with self-play—is seen as incremental by reviewers, with insufficient differentiation from prior work in multi-objective optimization and population-based RL.  \n\nWhile the paper’s premise is promising, these issues collectively weaken its contribution. For a top-tier conference, the lack of rigorous comparisons, methodological clarity, and broader validation renders the work insufficiently impactful."
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about regression with label differential privacy. The reviewers have a mix of opinions, so I need to carefully weigh their comments and the authors' responses.\n\nFirst, the strengths. The paper proposes a novel mechanism called \"randomized response on bins\" for label DP in regression tasks. They prove its optimality under certain conditions and provide an efficient algorithm. The experiments show it outperforms existing methods like Laplace and Exponential mechanisms. Reviewers 3, 4, and 5 highlight the clarity of writing and the theoretical contributions. Reviewer 4 especially praises the optimality proof and the practical experiments. The authors addressed some concerns, like adding comparisons to Ghazi et al. in the appendix, which seems to have satisfied Reviewer 3.\n\nNow, the weaknesses. Some reviewers question the distinction between regression and classification. Reviewer 1 initially confused the two, but the authors clarified that regression losses account for label proximity, unlike classification. That makes sense, especially with their example of MSE on classes vs. integers. However, Reviewer 5 and 6 mention that the technical novelty is incremental compared to prior work like Ghazi et al., though the authors argue their LP approach and dynamic programming solution are novel. Reviewer 4's point about the second term in Equation (1) influencing the Bayes error is valid, but the authors acknowledge this as a future direction. \n\nReviewer 6 and others note that mapping continuous labels to bins might be a limitation. The authors responded by extending their theorem to continuous intervals in the appendix, which helps. The empirical results are strong, but some reviewers wanted more comparisons. The authors added a table comparing with Ghazi et al., showing their method's superiority, which addresses Reviewer 3's concern.\n\nThe technical correctness seems solid. Most reviewers rated correctness as 3 or 4, with some minor issues but no major flaws. The authors provided proofs in the appendix, which adds credibility. The main criticism is about novelty. While some reviewers see it as a significant step, others feel it's an extension of existing work. However, the application to regression with a generalized loss function and the optimality proof under those conditions do seem novel enough for a top-tier conference.\n\nConsidering the balance between strengths and weaknesses, the paper presents a clear contribution with both theoretical and empirical validation. The authors addressed most reviewer concerns adequately, and the remaining issues (like the second term in Equation 1) are noted as future work. The comparison to Ghazi et al. in the experiments, even if not perfect, shows practical benefits. Given the positive trajectory in reviews (some reviewers increased scores after responses) and the overall quality, acceptance seems warranted.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel mechanism for label differential privacy in regression tasks, demonstrating both theoretical and empirical contributions. Key strengths include:  \n1. **Theoretical Novelty:** The \"randomized response on bins\" mechanism is rigorously proven to be optimal under a given regression loss, extending prior work focused on classification (e.g., Ghazi et al.). The dynamic programming solution and characterization of the mechanism’s structure are non-trivial advancements.  \n2. **Practical Relevance:** Experiments on real-world datasets validate the method’s efficacy over baseline DP mechanisms (e.g., Laplace), and the authors addressed concerns by adding comparisons to Ghazi et al. in the appendix.  \n3. **Responsive Revisions:** Authors adequately addressed reviewer critiques, clarifying distinctions between regression/classification losses, extending results to continuous label spaces, and discussing limitations (e.g., the impact of the second term in Equation 1).  \n\nWhile some reviewers noted incremental technical novelty compared to prior work, the generalization to regression losses and the optimality proof under this setting constitute a meaningful contribution. Minor weaknesses (e.g., limited discussion of the Bayes optimal error’s influence) are acknowledged as future work but do not undermine the core contribution. The paper meets the bar for a top-tier conference due to its clarity, theoretical rigor, and practical validation."
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The title suggests they're using pretrained structure embeddings for protein interaction prediction. The abstract mentions that previous works focused on sequence and network info but ignored structural info. Their method leverages pretrained embeddings and shows improvement.\n\nNow, the reviewers have several concerns. Reviewer 1 points out limited contribution because they're using OmegaFold's embeddings directly and just applying GNNs. The mean pooling might lose local structural info. Also, inconsistent results in tables. Reviewer 2 and 3 also mention limited novelty, just combining existing methods without significant changes. The empirical improvements are marginal, around 5%, and they didn't compare to state-of-the-art methods like dMaSIF or AlphaFold-Multimer. Reviewer 4 is concerned about weak baselines and unclear model design. They didn't compare to pairwise models without GNNs or use better baselines.\n\nThe authors didn't provide a response, which is a problem because they can't address these issues. The main issues are novelty and empirical validation. The technical contribution is minimal since they're using existing embeddings and standard GNNs. The experiments don't convincingly show superiority over existing methods. The AUC improvements are small, and without confidence intervals, it's hard to tell if they're significant. Also, the baselines might not be the strongest, so the comparison isn't fair.\n\nTop-tier conferences require both novelty and strong empirical results. Here, the approach isn't novel enough, and the experiments have flaws. The reviewers consistently rate technical and empirical novelty as low. Even though the problem is important, the execution doesn't meet the bar for a top conference. The lack of author response makes it harder to resolve these issues. Therefore, the paper should be rejected.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes using pretrained structural embeddings (from OmegaFold) with graph neural networks (GNNs) for protein-protein interaction (PPI) prediction. While the motivation to incorporate structural information is well-founded, the reviewers raise critical concerns that undermine the paper’s suitability for a top-tier conference:  \n\n1. **Limited Novelty and Contribution**:  \n   - The core methodology relies on off-the-shelf structural embeddings (OmegaFold) and standard GNN architectures (e.g., GraphSAGE, GAT) without significant technical innovation. Reviewers unanimously criticize the lack of novelty, noting that the work primarily combines existing tools rather than introducing novel pre-training strategies or architectural advancements.  \n\n2. **Weak Empirical Validation**:  \n   - **Marginal Improvements**: The reported performance gains (e.g., ~5% over baselines) are modest and lack statistical significance (e.g., no confidence intervals).  \n   - **Incomplete Baselines**: Key state-of-the-art methods (e.g., AlphaFold-Multimer, dMaSIF) are omitted, raising doubts about the validity of claims.  \n   - **Unclear Model Design**: The choice of GNNs over pairwise interaction models is not justified, and critical ablation studies (e.g., comparing structural embeddings to sequence-based embeddings like OmegaPLM) are missing.  \n\n3. **Methodological Concerns**:  \n   - The use of mean pooling for structural embeddings risks losing local interaction-relevant information.  \n   - Negative sampling and dataset construction (e.g., ATI dataset with only 2 unique nodes) are inadequately explained, casting doubt on reproducibility.  \n\n4. **Presentation Issues**:  \n   - Tables lack clarity (e.g., inconsistent results across datasets, missing highlights for best results) and the writing contains grammatical errors.  \n\nWhile the problem (PPI prediction) is important and the integration of structural data is promising, the paper fails to demonstrate a meaningful advance over existing approaches. The absence of author responses further prevents addressing these concerns. For a top-tier venue, the work lacks the technical depth, empirical rigor, and novelty required for acceptance."
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a new validation protocol called Proximal Validation Protocol (PVP) to address the trade-off between having enough data for training and a reliable validation set. The core idea is to create a validation set through data augmentation and distribution-consistent sampling, allowing the entire original dataset to be used for training. \n\nReviewers have mixed opinions. Reviewer 1 points out that the experiments are unclear and lack thoroughness, especially regarding model selection and hyperparameter tuning. They also mention issues with clarity in the presentation of results. Reviewer 2 is concerned about the small datasets used and the lack of theoretical guarantees. Reviewer 3 likes the problem's practicality but wants more experiments on real-world scenarios, ablation studies, and statistical significance. Reviewer 4 rejects the idea, arguing there's no theoretical basis and the metrics don't convincingly show the validation set's representativeness.\n\nThe strengths of the paper include addressing an under-researched, practical problem and proposing a novel method. However, the weaknesses are significant. The experiments are not comprehensive enough—using small datasets and not testing on real-world industrial setups. The lack of theoretical justification and unclear presentation of results (like undefined metrics) weaken the claims. Additionally, the absence of author responses to address these concerns makes it hard to resolve the issues raised.\n\nFor a top-tier conference, the paper needs strong empirical validation, especially when introducing a new methodology. The current empirical support is insufficient, and the theoretical foundation is lacking. While the problem is important, the execution falls short in providing convincing evidence. Therefore, despite the novelty, the paper doesn't meet the high standards required for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper addresses a significant and under-explored problem in machine learning—the trade-off between training data usage and reliable validation. The proposed Proximal Validation Protocol (PVP) introduces a novel approach using data augmentation and distribution-consistent sampling to construct validation sets without sacrificing training data. While the problem is timely and the idea is innovative, the paper falls short in several critical areas required for acceptance at a top-tier conference:  \n\n1. **Empirical Weaknesses:**  \n   - The experiments lack thoroughness and clarity. Key metrics (e.g., \"F1 deterioration\") are ambiguously defined, and results do not convincingly demonstrate that PVP improves model selection (e.g., hyperparameter tuning) rather than just evaluating fixed models.  \n   - Small-scale datasets (e.g., CIFAR-LT) and limited modalities (e.g., only one text dataset) undermine claims of broad applicability. Reviewer 2 and 3 note the absence of large-scale benchmarks (e.g., ImageNet) and real-world industrial scenarios, which are crucial given the paper’s focus on ML production.  \n\n2. **Theoretical Gaps:**  \n   - The method lacks theoretical justification for why the augmented/sampled validation set approximates the true data distribution. Reviewer 4 highlights this as a critical flaw, arguing that the proposed metrics (e.g., test-validation gap) do not guarantee representativeness.  \n\n3. **Clarity and Reproducibility:**  \n   - The writing suffers from unclear explanations (e.g., undefined terms like \"F1 deterioration\") and informal language, reducing readability. While code is provided, insufficient methodological details (e.g., ablation studies on augmentation choices or sampling hyperparameters) hinder reproducibility.  \n\n4. **Reviewer Consensus:**  \n   - All reviewers note significant weaknesses, with Reviewer 1 and 4 advocating rejection. Reviewer 3’s borderline support is contingent on addressing major experimental gaps, which the authors did not (due to missing responses).  \n\nWhile the problem is important and the idea has potential, the paper’s unresolved methodological and empirical shortcomings make it unsuitable for a top-tier venue in its current form."
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about protein sequence and structure co-design using an equivariant translation model. The authors claim their method, ProtSeed, outperforms existing approaches in terms of both performance and inference speed. The reviewers have a mix of positive and critical comments, and the authors have provided detailed responses to address these concerns.\n\nFirst, looking at the strengths mentioned by the reviewers: several note that the paper tackles an important problem in protein design, presents a novel approach with strong empirical results, and the method is well-motivated. The experiments show significant improvements over existing baselines, which is a big plus. Reviewers 1, 2, 3, and 8 seem satisfied after the authors' responses, especially regarding clarifications and additional experiments like ablation studies. Reviewer 8 even recommends acceptance, highlighting the practical impact of the method beyond antibodies.\n\nHowever, there are notable weaknesses and concerns. Technical novelty is a recurring issue. Reviewers 4, 6, and 7 question whether the approach is sufficiently novel compared to prior work like AlphaFold2 or Jin et al.'s methods. The authors responded by clarifying their contributions, emphasizing cross-conditioning and the efficiency of their one-shot approach. They also adjusted claims about being the \"first\" in co-design to be more accurate, which addresses some overclaiming concerns.\n\nAnother major concern is the evaluation metrics and methodology. Reviewer 5 points out that metrics like RMSD and AAR might not reflect functional protein design. The authors added ablation studies and discussed context features, but functional evaluations (like protein interactions) are still missing. However, they argue that such evaluations are beyond the current scope and refer to biological literature, which might be a limitation but not a deal-breaker if the method's technical merits are solid.\n\nReproducibility is another point. The authors committed to releasing code and models upon acceptance, which is crucial. They also provided training details and hyperparameters in the response, addressing Reviewer 5's concerns. The lack of these in the initial submission was a weakness, but their inclusion in the response helps.\n\nReviewer 6 had detailed technical questions about the model's components, like SeqIPA calculations and feature updates. The authors addressed these by adding appendix sections and clarifying methodologies, which should improve the paper's clarity. They also conducted ablation studies to validate the importance of cross-conditioning and iterative steps, which strengthens their claims.\n\nThe discussion around the term \"co-design\" being misleading (Reviewer 8) was addressed by clarifying that their method requires context features but can be adapted for general use. They added a section in the appendix about generating proteins without context, which shows the framework's flexibility.\n\nConsidering the reviewers' initial scores, there's a mix. Some reviewers upgraded their scores after the rebuttal, while others remained concerned about novelty. However, the authors made significant efforts to address the critiques, especially in clarifying claims, adding experiments, and ensuring reproducibility.\n\nIn top-tier conferences, technical soundness, novelty, and impact are key. The method's empirical results are strong, and the efficiency gains are significant. While the technical components build on existing work (like AlphaFold's IPA), the application to co-design with cross-conditioning and one-shot inference presents a meaningful advancement. The authors' responses mitigate many concerns, though some issues like functional evaluation remain. However, given the scope of the paper focusing on the model's design and computational efficiency, this might be acceptable for publication with the understanding that future work could address functional aspects.\n\nFinal Decision: Accept\n</think>\n\nThe paper presents a novel approach for protein sequence and structure co-design using an equivariant translation framework, addressing computational inefficiencies in existing methods. The authors demonstrate strong empirical results across multiple tasks, showing significant improvements in inference speed and performance metrics (e.g., RMSD, AAR) compared to autoregressive and diffusion-based baselines. While reviewers raised concerns about technical novelty, evaluation methodology, and reproducibility, the authors provided thorough responses, including clarifications on claims, ablation studies, and commitments to release code. Key strengths include the iterative cross-conditioning mechanism, one-shot inference, and applicability to general protein design. Weaknesses include limited functional validation and residual questions about novelty relative to prior work. However, the revisions and rebuttals sufficiently address most concerns, making the paper suitable for publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question addresses online adaptation of machine learning models to label-shift when there's also conditional shift. The reviewers have a mix of opinions, but there's a trend towards rejection. Let me unpack the key points.\n\nFirst, the strengths. The problem is well-motivated. Real-world deployments often face combined distribution shifts, so studying how existing label-shift methods fare under such conditions is relevant. The authors also extend previous work to regression and propose heuristics like OOD validation and handling non-invertible confusion matrices. They present empirical results across synthetic and real datasets, which adds practical value.\n\nHowever, the weaknesses are significant. Reviewers point out that the paper lacks a systematic approach to the types of shifts considered. Label-shift with conditional shift is essentially a general shift, but the paper doesn't frame it that way, leading to confusion. The experiments are deemed modest and not comprehensive enough. For instance, key baselines like optimal fixed classifiers or domain adaptation methods aren't compared, making it hard to gauge the proposed methods' effectiveness. The heuristics, especially for non-invertible matrices, are seen as ad-hoc without theoretical grounding. Reviewers also note that the improvements over existing methods are marginal or inconsistent across datasets, which undermines the claims of efficacy.\n\nThe technical novelty is questioned. The heuristics, like using OOD validation, are considered incremental. The Bayesian discussion's relevance isn't clear, and some sections are confusing due to undefined terms and notation. Reproducibility is a concern because of missing details in methodology and experiments. The authors' response addresses some issues, like clarifying the Bayesian connection and adding baselines, but doesn't fully resolve the core criticisms about the empirical scope and theoretical justification.\n\nReviewer 1 acknowledges the problem's importance but finds the empirical study insufficient. Reviewers 2 and 3 highlight the lack of significant improvements and unclear methodology. Reviewer 4 deems the work preliminary. The authors' response attempts to clarify but doesn't overcome the major weaknesses pointed out, such as the limited empirical comparisons and the ad-hoc nature of heuristics.\n\nIn conclusion, while the problem is relevant, the paper's contributions are marginal. The heuristics lack strong empirical support and theoretical foundation. The experiments don't convincingly demonstrate superiority over existing methods, and the presentation issues hinder clarity. Given the top-tier conference standards, the paper doesn't meet the threshold for acceptance.\n</think>\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this. The paper presents a new framework called Functional Risk Minimization (FRM), challenging the traditional Empirical Risk Minimization (ERM) by modeling each data point as coming from its own function. The reviewers have mixed opinions, but there's a consensus on the novelty and potential impact. However, there are significant concerns about clarity, scalability, and theoretical grounding.\n\nStarting with strengths: The idea is definitely novel. Modeling each data point with its own function could better capture real-world variations, which is a big deal. The authors also provided empirical results showing FRM outperforming ERM in certain scenarios, like regression with non-uniform noise and reinforcement learning. The new experiments with CNNs on MNIST variants address scalability concerns to some extent, showing FRM's applicability beyond small-scale tasks. The connection to over-parameterized models and generalization is another plus, as it taps into current ML research trends.\n\nNow the weaknesses. Reviewer 1 pointed out major issues with the algorithm's derivation and scalability. The connection between the theoretical framework and the implemented algorithm isn't clear, and the use of Hessian approximations might not scale well for large models. However, the authors responded by explaining that they use efficient Hessian-vector products, which are feasible even for large networks. They also added experiments with CNNs, which helps, but it's still not clear how this would work on very large datasets like ImageNet without significant computational resources.\n\nReviewer 2 was confused about the relation to hierarchical Bayesian models and the test-time parameter usage. The authors clarified that FRM uses a single parameter at test time despite training with individual functions, but this still feels a bit hand-wavy. The new generative model experiments partially address this, but there's lingering doubt about the consistency between training and test assumptions.\n\nReviewer 3 was more positive, highlighting the novelty and the well-written nature, but still noted computational costs and the need for more discussion on why FRM is better than modified ERM. The authors' response about structured variations and the potential for FRM to leverage over-parameterization is compelling but still theoretical.\n\nThe authors' responses are thorough. They addressed scalability by showing CNN experiments and explaining approximation methods. They clarified the Bayesian comparison and test-time usage, though some conceptual clarity might still be needed. The new experiments on unsupervised learning with VAEs are a strong addition, showing practical benefits in representation learning.\n\nHowever, the core issues remain: the paper's clarity is still a problem. Even after revisions, reviewers noted confusion around ERM definitions and the FRM objective's exact formulation. Theoretical guarantees are weak; the authors argue that under FGM assumptions, FRM minimizes test loss, but without formal proofs or bounds, this is speculative. The empirical results, while improved, are still on relatively small-scale tasks. The MNIST variants are a step up, but not enough to conclusively prove scalability for state-of-the-art models.\n\nIn top-tier conferences, a balance between novelty and rigor is crucial. FRM's novelty is high, but the technical execution and presentation have gaps. The lack of formal theory and limited empirical validation on large-scale problems might not meet the bar. However, the potential impact is significant, and the authors have made reasonable efforts to address concerns. The discussion around over-parameterization and generalization is timely and could inspire further research.\n\nConsidering all this, the paper's strengths in novelty and potential impact might outweigh its weaknesses, especially given the authors' efforts in addressing reviewer concerns. However, the remaining issues in clarity and theoretical grounding are substantial. On the fence, but leaning towards acceptance as a provocative initial step in a promising direction.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper introduces Functional Risk Minimization (FRM), a novel framework that challenges the foundational assumptions of ERM by modeling data points as originating from individual functions. While the paper has weaknesses in clarity and theoretical rigor, its strengths in novelty, potential impact, and responsiveness to reviewer critiques justify acceptance at a top-tier conference.  \n\n**Key Strengths:**  \n1. **High Novelty:** FRM offers a paradigm shift by rethinking noise modeling and connecting to over-parameterized regimes, aligning with modern ML challenges (e.g., double descent, implicit regularization).  \n2. **Empirical Breadth:** The authors expanded experiments to include CNNs on MNIST variants (supervised/unsupervised) and reinforcement learning, demonstrating scalability beyond initial small-scale settings. These experiments show FRM’s advantage in structured noise scenarios.  \n3. **Responsive Revisions:** The authors addressed critical concerns:  \n   - Clarified FRM’s relationship to Bayesian methods and test-time consistency.  \n   - Provided scalable approximations (e.g., Hessian-vector products) and justified computational feasibility.  \n   - Added generative model experiments, directly addressing reviewer requests.  \n\n**Persistent Weaknesses:**  \n1. **Clarity Issues:** While improved, the presentation remains dense, with lingering ambiguities in differentiating FRM from ERM/Bayesian frameworks and defining the FRM objective.  \n2. **Theoretical Gaps:** Formal guarantees (e.g., generalization bounds, finite-sample convergence) are still lacking, though the authors link FRM to empirical success under FGM assumptions.  \n3. **Scalability Limits:** While CNN experiments show progress, large-scale applications (e.g., ImageNet) remain unexplored, leaving practical scalability questions open.  \n\n**Conclusion:**  \nDespite its flaws, the paper’s core idea—reimagining risk minimization for modern over-parameterized models—is timely and impactful. The revisions and new experiments sufficiently address scalability and applicability concerns for a preliminary contribution. Acceptance is warranted to stimulate discussion and follow-up work on this promising direction."
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. Let me start by summarizing the key points from the paper and the reviews.\n\nThe paper introduces TabLeak, a method for data leakage attacks in federated learning (FL) on tabular data. The authors claim that tabular data poses unique challenges compared to images or text, such as mixed discrete-continuous features, higher variance in reconstructions, and difficulty in assessing reconstruction quality. They propose three components: a softmax structural prior, pooled ensembling, and entropy-based quality assessment. The experiments show improvements over baselines, especially on datasets like Adult.\n\nNow, looking at the reviews. Reviewer 1 and 2 have major concerns about novelty and presentation. Reviewer 1 thinks the methods are standard and the writing is poor. Reviewer 2 also questions the technical novelty, suggesting that softmax and ensembling are not new. However, Reviewer 3 is more positive, acknowledging the empirical contributions and the identification of a new problem area, though they have some concerns about claims and experimental justification.\n\nThe authors' response addresses these points. They added experiments to justify claims about variance due to mixed data types, tested alternative methods like Gumbel-Softmax, and provided more analysis on ensemble size and pooling methods. They also clarified the novelty compared to text-based attacks and improved the paper's clarity.\n\nReviewer 3's concerns were mostly addressed with additional experiments and clarifications. However, Reviewers 1 and 2's concerns about technical novelty might still linger. The authors argue that their work is novel in applying these techniques to the tabular FL context, which hasn't been explored before. They also showed that their method outperforms baselines derived from image/text attacks, which weren't effective for tabular data.\n\nThe key points for acceptance would be: (1) the paper addresses a significant gap (tabular data in FL attacks), (2) the proposed method, while using known components, is effectively adapted to a new problem with thorough experimentation, and (3) the empirical results are strong and reproducible. The weaknesses in writing were addressed, and the technical contributions, though incremental, are meaningful in a new context.\n\nHowever, the concerns about the technical novelty being low (Reviewers 1 and 2) could be a sticking point. Top-tier conferences often look for both novelty and impact. The authors' rebuttal does show that their approach is tailored to the unique challenges of tabular data, which might be enough to argue for significance despite using existing techniques.\n\nIn the end, the paper presents a novel application with practical implications for high-stakes FL applications. The empirical results are solid, and the authors addressed most reviewer concerns. The technical contributions, while not groundbreaking, are sufficient for a top-tier conference if the problem's importance is recognized.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper addresses a critical gap in federated learning (FL) security by focusing on tabular data, a high-stakes domain often overlooked in prior work. While reviewers raised valid concerns about technical novelty (e.g., softmax relaxations and ensembling being established techniques), the authors convincingly demonstrated that their adaptation to tabular data’s unique challenges (mixed discrete-continuous features, variance reduction, and reconstruction quality assessment) is novel and impactful. The rebuttal strengthened the paper by:  \n1. **Empirical Validation:** Added experiments (e.g., synthetic datasets, ablation studies) to justify claims about variance in mixed-type data and the effectiveness of pooling.  \n2. **Novelty Clarification:** Contrasted their approach with text/image domains, showing why prior methods (e.g., gradient sparsity in NLP) fail for tabular data.  \n3. **Technical Rigor:** Tested alternatives (Gumbel-Softmax, mean pooling) and demonstrated TabLeak’s superiority over baselines derived from image/text attacks.  \n\nReviewer 3’s positive assessment of empirical significance and reproducibility, coupled with the authors’ thorough rebuttal addressing clarity and methodological concerns, outweigh the weaker novelty scores from Reviewers 1–2. The work advances FL security by exposing vulnerabilities in a critical domain, meeting the bar for a top-tier conference."
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about privacy-preserving machine learning, specifically using a method called Scrunch. The main idea is splitting the neural network into two parts, an encoder on the client side and a classifier on the server side. They use center loss to minimize sensitive information in the encoded data. The reviewers have several concerns.\n\nReviewer 1 says the idea is incremental, not novel enough. They mention that splitting the network isn't new and the experiments are limited. Only two datasets, MNIST and CelebA, which are pretty standard. Also, they didn't compare with enough baselines, like homomorphic encryption. The technical novelty is rated low.\n\nReviewer 2 points out major technical flaws. The threat model is problematic because the server could potentially bypass the center loss, making the privacy measures ineffective. Also, during training, the server has access to labels and class centers, which might allow reconstruction attacks. The experiments are on shallow models like LeNet and VGG, not more modern architectures. The privacy claims might not hold under stronger attacks.\n\nReviewer 3 is confused about the security model and thinks the method doesn't adequately prevent information leakage. They mention that encoded features might still leak info, like in time-series data or NLP. The lack of a clear security model is a big issue.\n\nThe authors didn't provide a response, so there's no rebuttal to these points. \n\nPutting this together: The main issues are technical flaws in the threat model and privacy guarantees, limited experimental validation, and incremental novelty. For a top-tier conference like ICLR, these are significant red flags. The reviewers agree that the technical contribution is marginal and the experiments aren't thorough enough. Without addressing these concerns, especially the threat model and comparison with other methods, the paper doesn't meet the high bar required. The lack of author response makes it harder to overlook these issues. So, the decision is likely to reject.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper proposes a privacy-preserving method (Scrunch) for MLaaS by splitting neural networks into client-side encoders and server-side classifiers, using center loss to minimize sensitive information leakage. However, the reviewers raise critical concerns that undermine its suitability for a top-tier conference:\n\n1. **Technical Flaws in Threat Model and Privacy Guarantees:**\n   - **Reviewer 2** highlights a fundamental flaw: The server controls the training process (including center loss computation), which creates a contradiction in the threat model. A malicious server could disable privacy mechanisms (e.g., ignore center loss) or reconstruct inputs via model inversion attacks. The authors do not address how privacy is enforced if the server is adversarial during training.\n   - **Reviewer 3** questions the security model’s clarity, noting that intermediate features may still leak sensitive information (e.g., trends in time-series data). The lack of formal privacy guarantees (e.g., differential privacy) weakens the claims.\n\n2. **Limited Empirical Validation:**\n   - **Reviewer 1** criticizes the simplistic experiments: Only two datasets (MNIST, CelebA) and shallow models (LeNet-5, VGG-16) are tested. No comparisons with cryptographic baselines (e.g., homomorphic encryption) or modern architectures (e.g., ResNets) are provided.\n   - **Reviewer 2** points out that the private task for MNIST is undefined, and the CelebA evaluation (gender vs. smile) is not sufficiently challenging. Real-world scenarios (e.g., identity vs. sensitive attributes) are not explored.\n\n3. **Incremental Novelty:**\n   - All reviewers agree the core idea (split learning with mutual information control) is not novel. **Reviewer 1** notes prior work like Shredder, while **Reviewer 2** observes that split learning is a well-known paradigm unmentioned in the paper. The use of center loss adds limited technical novelty.\n\n4. **Clarity and Reproducibility:**\n   - Ambiguous statements (e.g., confusing “inter-class” and “intra-class” distances) and missing implementation details (e.g., optimizer settings) reduce reproducibility and clarity.\n\n5. **Lack of Author Response:**\n   - The absence of a rebuttal leaves critical issues unresolved, making it impossible to assess whether the authors could address the concerns.\n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. The paper is about identifying phase transition thresholds in permuted linear regression using message passing. The reviewers have a mix of opinions, but there's a trend towards rejection. Let me go through each aspect.\n\nFirst, the strengths. The paper tackles an interesting problem with phase transitions, which is a significant topic. The identification of precise thresholds for both oracle and non-oracle cases could be a novel contribution. Reviewer 4 mentions that the precise characterization of the SNR threshold is a novel contribution, which is a plus. The use of message-passing algorithms and connecting it to prior work like Semerjian et al. 2020 shows some technical depth.\n\nHowever, the weaknesses are substantial. The main issues are clarity and presentation. Multiple reviewers (1, 2, 5) point out that the paper is poorly written, notation-heavy, and not self-contained. Reviewer 1 suggests the paper needs serious reorganization and expansion for clarity. The heavy reliance on prior works (Semerjian et al. and Zhang et al.) without sufficient explanation makes it hard for readers unfamiliar with those papers. This is a big problem for a conference paper that should be more self-contained.\n\nTechnical novelty is questioned. Reviewers 1 and 2 mention that the results are incremental, building on existing methods. The non-oracle case's phase transition threshold is an approximation, and the justification for the approach (like Assumption 1) is weak. Reviewer 5 criticizes the contradiction between claiming precise thresholds and using approximations in the non-oracle case. The partial permutation estimator in Section 6 is underdeveloped and not well motivated, according to Reviewer 2.\n\nEmpirical results are mixed. While some experiments align with theoretical predictions, there's a gap between theory (asymptotic) and practice (finite samples). Reviewer 5 points out that claiming alignment between empirical and theoretical results is problematic because of this. Reproducibility is also an issue since the code isn't provided.\n\nCorrectness is another concern. Reviewers 2 and 5 note that some claims are not well-supported. For example, Proposition 1's reliance on a rescaled identity matrix is unrealistic. The technical derivations, especially in Section 5, are unclear and lack proper explanation, leading to doubts about correctness.\n\nThe reviewers' scores reflect these issues. Reviewers 1, 2, and 5 lean towards rejection due to clarity, novelty, and correctness issues. Reviewer 3 is more positive but notes standard techniques. Reviewer 4 is somewhat positive but concerned about clarity and reproducibility.\n\nIn conclusion, while the problem is interesting and there's some technical contribution, the paper's current state has significant flaws in presentation, clarity, and justification of claims. These issues make it unsuitable for a top-tier conference where clarity and self-containment are crucial. The authors need to address these concerns through major revisions, possibly targeting a journal where space allows more detailed exposition.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper addresses an important problem in permuted linear regression, aiming to identify precise phase transition thresholds using message-passing algorithms. While the technical contributions—particularly the identification of SNR thresholds for the non-oracle case—have potential significance, the paper suffers from critical flaws that outweigh its merits in its current form:  \n\n1. **Clarity and Organization:**  \n   - The paper is heavily criticized for poor writing, excessive notation, and lack of self-containment. Reviewers note that key sections (e.g., message-passing derivations) are unintelligible without prior familiarity with Semerjian et al. (2020) and Mezard-Montanari’s work. This undermines accessibility for a conference audience.  \n   - Multiple sections (e.g., Section 3.2, 4.2) are described as confusing, with undefined terms (e.g., \"Ener\"), inconsistent notation, and insufficient motivation for assumptions (e.g., Assumption 1).  \n\n2. **Technical Novelty and Correctness:**  \n   - The results are seen as incremental, building directly on prior work (Zhang et al., Semerjian et al.) without sufficient differentiation. The non-oracle case relies on approximations (e.g., Taylor expansions) that are weakly justified, and the claimed \"precise\" thresholds conflict with the use of approximations in practice.  \n   - Critical claims (e.g., partial permutation recovery in Section 6) lack theoretical or empirical support, and the connection between theoretical (asymptotic) predictions and finite-sample experiments is inadequately addressed.  \n\n3. **Reproducibility and Empirical Rigor:**  \n   - The absence of code and unclear experimental setup (e.g., unexplained gaps between theory and simulations in Table 3) hinder reproducibility. Reviewer 2 highlights issues with numerical presentation (e.g., non-standard error reporting).  \n\n4. **Reviewer Consensus:**  \n   - Four reviewers explicitly recommend rejection due to the above issues, with only Reviewer 4 offering a marginally positive assessment contingent on code release. The paper’s flaws in clarity, novelty, and rigor render it unsuitable for a top-tier conference in its current state.  \n\n**Recommendation:** The authors should significantly revise the paper to improve clarity, rigorously justify assumptions, and better contextualize contributions relative to prior work. A journal submission may be more appropriate given the need for extended exposition."
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper introduces DeepSeaProbLog, an extension of DeepProbLog that supports both discrete and continuous variables. The main contributions are the new language, its semantics, an implementation, and experimental evaluation. \n\nLooking at the reviews, there's a mix of opinions. Reviewers 1, 3, and 4 are more positive, acknowledging the technical novelty and significance, though they have concerns about the empirical evaluation's breadth and some technical details. Reviewers 2, 5, 6, and 7 are more critical, pointing out issues like incremental contributions, limited experiments, and clarity in explaining the handling of discrete variables. \n\nThe authors responded to these concerns by clarifying their contributions, addressing the handling of discrete variables, adding more experiments and explanations, and discussing scalability. They also emphasized the theoretical contributions, like differentiable WMI and extending previous results on relaxed functions.\n\nNow, considering the strengths: The paper tackles a clear gap in existing NPLP systems by supporting continuous variables, which is a significant step. The integration of WMI for inference and providing a differentiable approach is novel. The theoretical proofs add depth. The implementation and experiments, while limited, do show practical applications and improvements over baselines.\n\nOn the weaknesses: The empirical evaluation is somewhat narrow, relying on a few tasks. Some reviewers found the examples contrived or not sufficiently challenging. The scalability and efficiency compared to existing methods like DeepProbLog in discrete settings weren't fully addressed. There's also a concern about the paper's clarity, with some technical details relegated to appendices.\n\nThe authors' responses addressed many points, adding more experiments, clarifying the handling of discrete variables, and explaining the theoretical underpinnings. However, the core issue of whether the empirical evaluation is sufficient for a top-tier conference remains. While the experiments are not extensive, they do demonstrate the system's capabilities in hybrid domains, which is the paper's main contribution.\n\nGiven that the technical contribution is solid and addresses a clear gap, and the authors have made efforts to respond to reviewers' concerns, the paper might still be acceptable. However, the mixed reviews, especially the concerns about empirical evaluation and incremental nature, make this a borderline case. \n\nIn top-tier conferences, technical novelty and potential impact are crucial. The extension to continuous variables is a meaningful contribution, and the theoretical aspects are strong. The empirical part, while not exhaustive, shows the approach works in practical scenarios. The authors' commitment to releasing code aids reproducibility.\n\nFinal Decision: Accept\n</think>\n\nThe paper introduces DeepSeaProbLog, a neural probabilistic logic programming framework that extends existing systems to handle both discrete and continuous variables. The work addresses a significant limitation in neural-symbolic AI by integrating weighted model integration (WMI) for hybrid domains and enabling gradient-based learning. While the technical contributions—such as linking probabilistic logic programming semantics to WMI and proposing differentiable inference—are novel and theoretically sound, the empirical evaluation is limited in scope, relying on a few synthetic tasks. Reviewers raised valid concerns about incrementalism, scalability, and clarity, but the authors addressed many issues in their response, clarifying the handling of discrete variables, expanding experiments, and emphasizing theoretical extensions (e.g., differentiable WMI). Though the empirical results are narrow, they demonstrate the framework’s potential in hybrid settings. The work’s technical novelty, theoretical rigor, and practical implementation justify acceptance despite the modest evaluation. \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes using binary labels as an auxiliary task in multi-task learning for image classification. The main idea is that representing class labels as binary vectors (randomly assigned) and training the model to predict both the standard one-hot labels and these binary labels can improve performance. The reviewers have a mix of opinions, but there are some common concerns.\n\nReviewer 1 points out strengths like the interesting idea and some performance improvements but criticizes the experimental section for being weak, lack of analysis on when the method works, and clarity issues. They also mention missing comparisons with other baselines and insufficient discussion of related work. The technical novelty is considered somewhat new but not thoroughly explored.\n\nReviewer 2 is harsher, questioning the novelty and correctness. They mention that the authors didn't properly situate their work in existing literature (like knowledge distillation, ordinal regression) and criticize the technical presentation and experimental setup. The empirical results are deemed insufficient compared to SOTA methods.\n\nReviewer 3 finds the idea simple and effective but notes that the improvements are small and the contribution might not be significant enough. They also point out missing related work on error-correcting output codes, which seems very relevant. The clarity is good, but the empirical significance is questioned.\n\nReviewer 4 highlights the ease of implementation and some positive results but raises concerns about the limited technical contribution, lack of motivation for why binary labels help, missing implementation details, and the absence of experiments on larger datasets like ImageNet. They also question the choice of metrics and the robustness of the method.\n\nThe common themes across reviews are:\n1. **Novelty and Related Work**: The idea has some novelty but isn't sufficiently differentiated from prior work. Key related works like error-correcting output codes and unsupervised meta-learning aren't adequately discussed.\n2. **Technical Contribution**: The method is seen as simple, perhaps too much so, with a basic multi-task setup. The auxiliary task's intuition and why it works aren't well-explained.\n3. **Experiments**: The experiments are limited in scope (no large datasets like ImageNet), lack comparisons with SOTA, and don't explore variations (like different binary label generation methods). There's also a lack of analysis on when and why the method works.\n4. **Clarity and Reproducibility**: Some parts of the paper are unclear, equations have issues, and implementation details are missing, making reproducibility difficult.\n\nThe authors didn't provide a response, so there's no rebuttal to address these points. For a top-tier conference, the paper needs to have strong technical novelty, thorough experimentation, and clear contributions. While the idea is interesting, the reviews suggest that the current version doesn't meet these standards. The lack of comparison with relevant baselines (like error-correcting codes), insufficient analysis, and unclear motivation weaken the case. The empirical improvements, while present, are not consistently strong enough across different scenarios, and the absence of ImageNet-level validation is a red flag for scalability and generalizability.\n\nGiven the mixed reviews and the major concerns about novelty, technical depth, and experimental rigor, the paper likely doesn't meet the high bar of a top-tier conference in its current form. The reviewers, especially Reviewer 2 and 4, point out critical flaws that would need substantial work to address. Without a rebuttal or author response, it's hard to see how these issues could be resolved post-review.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces an intriguing concept of using binary label representations as an auxiliary task in multi-task learning for image classification. While the idea has potential, several critical shortcomings prevent acceptance at a top-tier conference:  \n\n1. **Limited Technical Novelty and Related Work Gaps**:  \n   - The core idea of label transformations (e.g., binary partitions) overlaps with prior work on error-correcting output codes (ECOC) and unsupervised meta-learning, which are insufficiently addressed. Key references like Dietterich & Bakiri (1994) on ECOC are missing, weakening the novelty claim.  \n   - The auxiliary task design lacks a clear theoretical or intuitive justification. While random binary grouping is simple, the paper does not explain why this specific approach captures meaningful class relationships better than structured alternatives (e.g., hierarchical groupings).  \n\n2. **Weak Empirical Evaluation**:  \n   - Experiments are limited to small/medium datasets (e.g., CIFAR-100, Stanford Cars) and lack validation on large-scale benchmarks like ImageNet, raising concerns about scalability.  \n   - Improvements over baselines are inconsistent and often marginal (as noted by Reviewers 1 and 3). Comparisons to SOTA methods (e.g., knowledge distillation, ECOC) are absent, making it difficult to assess the method’s practical value.  \n   - Critical analyses—such as sensitivity to binary label randomization, task weighting, and performance under varying dataset complexities—are missing.  \n\n3. **Clarity and Reproducibility Issues**:  \n   - Key details (e.g., Metabalance implementation, binary label generation process) are unclear, hindering reproducibility.  \n   - Mathematical notation and equations contain errors (e.g., loss function scaling), and the experimental setup lacks sufficient detail (e.g., hyperparameters, training protocols).  \n\n4. **Underdeveloped Motivation**:  \n   - The paper does not rigorously establish why binary labels improve learning. While Reviewer 4 questions the “problem” that binary labels solve, the authors provide no theoretical or empirical analysis (e.g., feature space visualizations) to justify their design.  \n\n**Conclusion**:  \nWhile the idea of exploring label representations is promising, the paper’s technical contributions, empirical validation, and clarity fall short of the standards for a top-tier conference. Significant revisions—addressing related work gaps, expanding experiments, and providing deeper analysis—are needed to strengthen the case for publication."
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about analyzing the average-case performance of no-regret learning dynamics in games, specifically looking at q-replicator dynamics. The main contributions seem to be proving convergence to Nash equilibria and comparing different dynamics in terms of the quality of equilibria they reach, especially in 2x2 coordination games.\n\nThe reviewers have mixed opinions. Reviewer 1 initially had some concerns about the novelty compared to prior work (PP16) but seemed somewhat satisfied after the authors' response. Reviewer 2 was critical, pointing out that the results are incremental and don't sufficiently acknowledge PP16. Reviewer 3 thought the problem was interesting but felt the results were preliminary, especially since they're limited to 2x2 games. Reviewer 4 was more positive, highlighting the importance of the problem despite the limited scope.\n\nThe authors responded by clarifying their contributions, adjusting the abstract, fixing citations, and adding more discussion about PP16. They argue that their work expands on PP16 by considering more general settings and providing comparisons between different dynamics. However, the technical results are still mainly in 2x2 games, and the theoretical analysis there is partial (e.g., only comparing q=0 and q=1 for certain parameters, relying on empirical evidence for some claims).\n\nThe key issues here are novelty and significance. While the problem is important, the reviewers note that the technical contributions are incremental over PP16 and limited in scope. The authors do address some of the concerns in their response, but the core results still seem narrow. For a top-tier conference, the paper needs to have substantial technical novelty and broader applicability. The fact that the main theorems are for 2x2 games and the extension to more general cases is only empirical or conjectural might be a problem. \n\nOn the other hand, initiating a new line of research (average-case analysis in learning dynamics) could be seen as a contribution, even if the initial results are limited. The paper also combines tools from different fields, which is a strength. However, the reviewers' concerns about the incremental nature and the lack of a comprehensive theoretical treatment in more general cases might outweigh these positives.\n\nIn the end, the paper's contributions might not be significant enough for a top-tier conference. The authors made efforts to address the reviewers' points, but the core limitations remain. The mixed reviews, with some leaning towards rejection due to incremental results and limited scope, suggest that the paper might not meet the high bar required.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem—evaluating the average-case performance of no-regret learning dynamics—and combines tools from game theory, dynamical systems, and machine learning. However, the reviewers raised critical concerns that outweigh its strengths:  \n1. **Incremental Technical Contributions:** The results are largely confined to 2x2 symmetric coordination games, with partial theoretical guarantees (e.g., Theorem 4.4 only compares q=0 and q=1 for specific parameter ranges). The extension to general q-replicator dynamics relies on empirical evidence and conjectures rather than formal proofs.  \n2. **Limited Novelty Relative to Prior Work:** While the authors expanded their discussion of PP16 in revisions, reviewers noted that the core technical results (e.g., convergence to equilibria, regions of attraction, and average price of anarchy analysis) closely follow PP16’s framework, with insufficient differentiation in theoretical depth or generality.  \n3. **Scope and Significance:** The analysis of 2x2 games, though non-trivial, is preliminary and lacks clear pathways to generalization. Reviewers questioned the broader impact, given the restricted setting and the absence of formal results for higher-dimensional games.  \n4. **Mixed Reviewer Consensus:** Despite addressing some concerns post-rebuttal, reviewers highlighted unresolved issues (e.g., overclaiming in the abstract, technical limitations in proofs) and downgraded scores for novelty and significance.  \n\nWhile the paper’s motivation and problem setup are compelling, the technical contributions are deemed insufficiently novel and narrowly scoped for a top-tier conference. The work would benefit from a more comprehensive theoretical treatment of general games or a stronger empirical demonstration of scalability beyond 2x2 cases."
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper introduces the RuDar dataset, which adds 3D radar echo observations for precipitation nowcasting. The reviewers have a mix of opinions but generally agree that the dataset is a solid contribution. However, they point out issues like the baseline models being too simple, lack of methodological novelty, and limited experiments beyond the main task.\n\nReviewer 1 appreciates the dataset but questions the model comparisons and dataset usability. They gave low scores on technical and empirical novelty. Reviewer 2 also likes the dataset but wants more experiments, especially on other ML tasks mentioned, and better metrics. Reviewer 3 finds the dataset similar to existing ones and notes the lack of methodological innovation. All reviewers agree the dataset is valuable but the technical contribution is limited.\n\nThe authors responded by adding Earthformer results, which addresses some concerns about model variety. However, they couldn't fully integrate all data inputs due to time constraints. They also defended their focus on the dataset rather than methodological novelty and explained limitations in cross-dataset evaluation.\n\nThe key points are: the dataset's uniqueness (3D, multiple variables, geographical coverage) vs. its perceived similarity to others; the baselines being standard but now including a recent model; and the lack of experiments on other tasks. The paper's main contribution is the dataset, which might be significant, but top-tier conferences often expect more technical innovation. However, datasets can be impactful if they fill a clear gap. The authors released the data and code, aiding reproducibility. The rebuttal adds some experiments but not all requested ones. Considering the dataset's potential impact despite the methodological limitations, but the reviewers' concerns about novelty for a top-tier venue might lean towards rejection. However, if the dataset is sufficiently novel and useful, acceptance could be justified. The paper's focus on dataset creation and benchmarking is valid, but the conference's standards for technical novelty might not be fully met here. The reviewers' scores lean towards lower novelty, so rejection might be the outcome.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces the RuDar dataset, which provides 3D radar echo observations for precipitation nowcasting, addressing a gap in vertical dimension data and incorporating geographical variability. While the dataset is comprehensive (covering multiple climate zones, altitude levels, and spanning two years), reviewers raised critical concerns about its novelty, methodological contributions, and empirical depth:  \n1. **Limited Technical Novelty:** The work focuses on dataset creation and benchmarking standard baselines (e.g., ConvLSTM, Earthformer). While datasets are valuable, top-tier conferences typically require significant methodological innovation, which this paper lacks.  \n2. **Weak Empirical Scope:** Reviewers noted insufficient experiments (e.g., no cross-dataset evaluation, minimal exploration of other ML tasks like anomaly detection) and reliance on basic metrics (MSE/F1). The addition of Earthformer results partially addresses this but remains incomplete.  \n3. **Dataset Impact vs. Redundancy:** Though RuDar adds vertical resolution and multi-variable data, reviewers questioned its distinctiveness compared to existing datasets (e.g., SEVIR, RADOLAN). The 2 km spatial resolution was also flagged as coarse for precipitation tasks.  \n4. **Reproducibility and Usability:** While the dataset and code are publicly available, the rebuttal did not fully resolve concerns about cross-region applicability or practical utility compared to operational systems.  \n\nThe paper’s strengths—a well-documented dataset and thorough baselines—are commendable but align more with a data paper venue than a top-tier conference emphasizing algorithmic or theoretical advances. Given the reviewers’ consensus on marginal novelty and the absence of groundbreaking insights, rejection is justified despite the dataset’s potential utility."
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this paper review. The paper proposes FedPA, a parameterized aggregator using meta-learning to address client and period drift in federated learning. The reviewers have a mix of opinions, but there are some common concerns.\n\nStarting with strengths: The idea of addressing both client and period drift is interesting. Period drift, in particular, is a new concept that the authors introduce, which could be a valuable contribution. The use of meta-learning for aggregation is novel, and the experiments show some improvements over baselines on certain datasets. Reviewer 2 and 4 acknowledge the identification of period drift as a strength, which is a good sign.\n\nNow the weaknesses. Reviewer 1 points out that the distinction between client drift and period drift isn't clearly made, both theoretically and experimentally. Without this, the motivation for FedPA is weakened. Also, the proxy dataset requirement is a limitation, as it might not be available in real FL scenarios. Comparisons to similar methods like FedET and DS-FL are missing, which is a big issue. The experiments are on simple datasets (EMNIST, MovieLens), and reviewers want more complex ones like CIFAR-100 or Stack Overflow. Reviewer 3 mentions that the accuracy on FEMNIST is lower than existing work, which raises questions about effectiveness.\n\nTechnical novelty is questioned. Reviewers 1 and 2 say the contributions are only marginally novel. The meta-learning approach isn't sufficiently justified compared to other methods. The dynamic systems analogy in the method section is seen as unnecessary and confusing. Also, the architecture choice of two NNs isn't well-explained, which Reviewer 2 highlights.\n\nReproducibility is a concern since there's no code provided. Clarity and writing quality are issues, with grammatical errors and small font figures. Reviewer 4 notes typos and formatting inconsistencies. The lack of ablation studies on proxy data distribution and hyperparameters is another weakness. Reviewer 3 suggests simpler alternatives like scalar weights, which the authors didn't explore.\n\nThe author response isn't available, so we can't see if they addressed these points. Given the major concerns about novelty, experimental rigor, and clarity, the paper doesn't meet the bar for a top-tier conference. Even though the idea has potential, the current presentation and validation are insufficient. The reviewers lean towards rejection, especially with Reviewer 1 and 2 being critical. The lack of comparison to relevant works and insufficient experiments are critical flaws. Therefore, the paper should be rejected.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces FedPA, a meta-learning-based aggregation strategy to address client and period drift in federated learning (FL). While the concept of \"period drift\" is novel and the idea of a parameterized aggregator is promising, the paper faces significant criticisms that undermine its suitability for a top-tier conference:  \n\n1. **Weak Theoretical and Empirical Justification**:  \n   - The distinction between \"client drift\" and \"period drift\" is not rigorously established. Reviewers note a lack of theoretical analysis or ablation studies to isolate the impact of period drift, leaving the core motivation unvalidated.  \n   - Experiments are limited to small-scale datasets (e.g., FEMNIST, MovieLens) and lack comparisons to critical baselines (e.g., FedET, DS-FL, and methods addressing distributed concept drift). Results on FEMNIST also underperform compared to prior work, raising doubts about efficacy.  \n\n2. **Technical and Practical Limitations**:  \n   - The reliance on a proxy dataset restricts applicability in real-world FL settings, where such data may not exist due to privacy constraints. The method’s compatibility with privacy-preserving techniques (e.g., SecAgg) is unaddressed.  \n   - The meta-learning framework’s design choices (e.g., dual neural networks for aggregation) are insufficiently motivated. Reviewers question whether simpler alternatives (e.g., scalar weighting) could achieve similar results.  \n\n3. **Reproducibility and Clarity Issues**:  \n   - No code is provided, and experimental details (e.g., hyperparameter tuning, proxy dataset distribution) are inadequately described.  \n   - The writing suffers from grammatical errors, inconsistent citations, and unclear explanations (e.g., the dynamic systems analogy).  \n\n4. **Marginal Novelty**:  \n   - While the period drift concept is new, the technical approach (meta-learning for aggregation) overlaps with prior work on knowledge distillation and adaptive aggregation. Reviewers argue the novelty is incremental rather than transformative.  \n\n**Conclusion**:  \nDespite identifying an interesting problem (period drift) and proposing a novel aggregator, the paper fails to convincingly demonstrate its significance, theoretical grounding, or empirical superiority over existing methods. The unresolved weaknesses in experimentation, reproducibility, and clarity make it unsuitable for acceptance at a top-tier venue."
    }
  ]
}