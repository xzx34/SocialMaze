{
  "summary": {
    "model": "llama-3.3-70B",
    "total_papers": 100,
    "correct_predictions": 72,
    "accuracy": 72.0,
    "confusion_matrix": [
      [
        48,
        28
      ],
      [
        0,
        24
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 63.1578947368421,
        "precision": 100.0
      },
      "Accept": {
        "total": 24,
        "recall": 100.0,
        "precision": 46.15384615384615
      }
    }
  },
  "results": [
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Score and Lyrics-Free Singing Voice Generation\" presents a novel approach to generating singing voices without the need for pre-assigned scores and lyrics. The authors propose three different schemes for singing voice generation: free singer, accompanied singer, and solo singer. They also outline the associated challenges and propose a pipeline to tackle these new tasks.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. The strengths include the novelty of the approach, the clarity of the writing, and the provision of code and audio samples. The weaknesses include the lack of motivation for the three different schemes, the lack of comparison with other methods, and the weakness of the evaluation metrics.\n\nThe authors have responded to the reviewer comments by providing additional motivation for the three schemes, adding comparisons with other methods, and strengthening the evaluation metrics. They have also clarified some of the technical details, such as the use of source separation and the computation of vocalness.\n\nHowever, some of the reviewer comments still raise concerns. For example, Reviewer 1 notes that the evaluation metrics may not be suitable for assessing the quality of the generated singing voices, and Reviewer 3 notes that the literature review could be improved. Reviewer 4 also raises concerns about the lack of justification for the choice of neural network architecture and the lack of experimental results to support some of the claims.\n\nDespite these concerns, the authors have made a significant effort to address the reviewer comments and improve the paper. The addition of comparisons with other methods and the strengthening of the evaluation metrics are particularly notable. The paper also presents a novel and interesting approach to singing voice generation, which could be of interest to the research community.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Extreme Classification via Adversarial Softmax Approximation\" presents a novel approach to extreme classification, a problem that arises when dealing with a large number of classes. The authors propose an adversarial softmax approximation method that generates negative samples from an adversarial model, which mimics the data distribution. This approach aims to reduce the gradient variance and improve the convergence rate of the training process.\n\nThe paper has several strengths. Firstly, the authors provide a clear and detailed explanation of their approach, making it easy to understand for non-experts. The paper is well-structured, and the authors provide a thorough analysis of the related work. The experimental results demonstrate the effectiveness of the proposed approach, showing a significant reduction in training time compared to several competitive baselines.\n\nHowever, the paper also has some weaknesses. One of the main concerns raised by the reviewers is the lack of comparison with other state-of-the-art methods, such as Slice and DiSMEC. The authors respond to this concern by arguing that their approach is focused on negative sampling, and they provide a detailed discussion of the differences between their approach and other methods, such as sampled softmax. While the authors' response is reasonable, it would be beneficial to include more comparisons with other state-of-the-art methods to demonstrate the superiority of their approach.\n\nAnother concern raised by the reviewers is the lack of evaluation on smaller datasets, where the full softmax loss can be optimized. The authors respond to this concern by agreeing to run experiments on the smaller EURLex dataset, which will provide insight into the approximation gap due to negative sampling.\n\nOverall, the paper presents a novel and effective approach to extreme classification. While there are some weaknesses, the authors have responded to the concerns raised by the reviewers, and the paper has the potential to make a significant contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a novel out-of-distribution detection method, MALCOM, which utilizes the normalized compression distance to identify out-of-distribution samples without requiring any out-of-distribution samples for validation or retraining the model. The method is based on extracting informative sequential patterns from the feature maps of convolutional neural networks.\n\nThe reviewers have raised several concerns and suggestions, including the need for a more thorough comparison with existing methods, the importance of justifying the strict constraints, and the potential for improving the performance of the proposed method. The authors have responded to these comments, providing additional experiments and justifications for their approach.\n\nOne of the main concerns is the comparison with existing methods, particularly the Mahalanobis detector, which requires validation to determine weights for feature ensembling. The authors have addressed this concern by conducting additional experiments, including a comparison with the Mahalanobis detector validated by adversarial samples. The results show that the proposed method, MALCOM, performs comparably to the Mahalanobis detector in some cases and outperforms it in others.\n\nAnother concern is the justification for the strict constraints, particularly the assumption that no out-of-distribution samples are available for validation. The authors have argued that this assumption is necessary for out-of-distribution detection, as the main motivation is to detect samples that are not known ahead of time. They have also provided additional experiments to demonstrate the effectiveness of their method in cases where out-of-distribution samples are available for validation.\n\nThe reviewers have also suggested that the authors provide more theoretical guarantees for their method and explore its application in more complex settings, such as safety-critical applications. The authors have acknowledged these suggestions and proposed potential future work in these areas.\n\nOverall, the paper has addressed the concerns and suggestions raised by the reviewers, and the additional experiments and justifications have strengthened the paper. The proposed method, MALCOM, has shown promising results, and the authors have demonstrated its effectiveness in various settings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Improved Training of Certifiably Robust Models\" proposes two regularizers to train neural networks that yield convex relaxations with tighter bounds, which can improve the certified robustness of the models. The paper has undergone significant revisions based on the reviewer comments, and the authors have addressed many of the concerns raised by the reviewers.\n\nThe main strengths of the paper are:\n\n1. The paper proposes a novel approach to improve the certified robustness of neural networks by using regularizers to train models that yield tighter convex relaxations.\n2. The authors provide extensive experimental results to demonstrate the effectiveness of their approach, including comparisons with state-of-the-art methods.\n3. The paper has undergone significant revisions to address the concerns raised by the reviewers, and the authors have provided detailed responses to the reviewer comments.\n\nHowever, there are also some weaknesses and concerns:\n\n1. Some reviewers found the analysis section to be confusing and unclear, and the authors' responses to these comments suggest that there may be some subtleties and nuances that need to be carefully considered.\n2. One reviewer noted that the proposed regularizers may not be effective in all cases, and that the approach may not work well for certain types of networks or datasets.\n3. Another reviewer suggested that the paper could benefit from more experimental results on additional datasets and networks to further demonstrate the effectiveness of the approach.\n\nAfter carefully considering the reviewer comments and the authors' responses, I believe that the paper has made significant contributions to the field of certified robustness and has the potential to be a valuable addition to the conference program.\n\nThe authors have addressed many of the concerns raised by the reviewers, and the revised paper provides a clearer and more detailed explanation of the approach and its limitations. While there may still be some subtleties and nuances that need to be carefully considered, I believe that the paper provides a significant contribution to the field and has the potential to be of interest to a wide range of researchers.\n\nTherefore, based on the information provided, I recommend accepting the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning disentangled representations in a hierarchical manner using a variational autoencoder (VAE). The authors propose a progressive learning strategy that incrementally grows the capacity of the network by adding new latent variables and parameters, which helps to improve disentanglement.\n\nThe paper has several strengths. Firstly, the idea of progressive learning is fresh and well-explained, and the experiments are thorough. The results suggest that this is a promising direction for disentangling representations. The authors also provide a clear and well-structured paper, making it easy to follow their arguments.\n\nHowever, the paper also has some weaknesses. One of the main concerns is the lack of clarity in the purpose of the paper. The authors claim that the goal is to learn disentangled representations, but the experiments focus more on hierarchical representation learning. The relationship between hierarchical representation and disentangled representation is not clearly explained. Additionally, the qualitative experiments are not convincing, and the proposed metric requires ground truth for the generative factors, which may limit its practicality.\n\nThe reviewers' comments provide valuable insights into the paper's strengths and weaknesses. Reviewer 1 suggests that the authors should add an experiment to compare the mutual information of each layer with VLAE. Reviewer 2 raises several concerns, including the lack of clarity in the purpose of the paper, the relationship between hierarchical representation and disentangled representation, and the limitations of the proposed metric. Reviewer 3 suggests that the authors should provide more quantitative experiments on information flow and clarify the definition of \"z from different abstraction\". Reviewer 4 suggests that the authors should provide more ablation studies to investigate the effect of the implementation strategies.\n\nThe authors' response to the reviewers' comments is thorough and addresses most of the concerns. They provide additional experiments, clarify the definition of \"z from different abstraction\", and discuss the relationship between hierarchical representation and disentangled representation. They also modify the paper to better formalize the statements and provide more quantitative results.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field of disentangled representation learning. The authors have addressed most of the concerns raised by the reviewers, and the additional experiments and clarifications have strengthened the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" proposes a novel approach to reduce the training time of convolutional neural networks (CNNs) by utilizing a multi-precision optimization strategy. The authors introduce a precision-switching mechanism that decides at runtime when to switch between different precision levels, allowing for a balance between training time and accuracy.\n\nThe reviewers have raised several concerns and suggestions, which the authors have addressed in their response. The main concerns include:\n\n1. Lack of clear motivation and explanation for the switching mechanism: The authors have added more details to Section 3.3 to explain the reasoning behind the switching mechanism and the choice of hyperparameters.\n2. Confusing presentation of the precision switching policy: The authors have updated Section 3.3 to clarify the notation and provide more explanation for the ratio \"p\" and the threshold parameters.\n3. Limited advantages over state-of-the-art methods: The authors have added more experiments and analysis to Section 4.4 to demonstrate the benefits of MuPPET, including an accuracy-time trade-off plot.\n4. Minor issues with typos, grammar, and figure readability: The authors have corrected the typos and grammar mistakes and added the figures to Appendix B at a larger scale.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the authors have addressed the major concerns and improved the clarity and presentation of the paper. The proposed approach is novel and has the potential to reduce training time while maintaining accuracy. The experiments and analysis demonstrate the effectiveness of MuPPET, and the authors have provided a clear explanation of the switching mechanism and hyperparameter choices.\n\nHowever, I also note that the reviewers have raised some concerns about the significance of the results and the comparison to state-of-the-art methods. While the authors have addressed these concerns to some extent, I believe that the paper could benefit from further clarification and analysis to demonstrate the superiority of MuPPET over existing methods.\n\nIn conclusion, based on the analysis of the paper and the reviewer comments, I believe that the paper has the potential to be accepted for publication, but it requires some revisions to address the remaining concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper in question, \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH,\" proposes an improvement to Monte Carlo Tree Search (MCTS) agents, a method that has been successful in games like chess, shogi, and Go, particularly through the lens of reinforcement learning as seen in Alpha Zero. The abstract suggests a focus on the coordination of episode generation, treating the entire system as a game tree search to balance exploitation and exploration. However, the execution and presentation of this idea are severely lacking, as highlighted by all three reviewers.\n\nThe primary issues with the paper are its poor writing quality, lack of clarity, absence of references, and overall unprofessional presentation. Reviewer 1 notes the paper's incompleteness, lack of citations, and poor writing, suggesting that it is \"less than half-baked.\" Reviewer 2 had significant difficulty understanding the paper due to grammatical issues and unclear sentences, stating that despite their effort, they could not comprehend the abstract or the paper's content. Reviewer 3 echoes these sentiments, describing the paper as \"very poorly written\" with \"many incomprehensible passages\" and no references, concluding that it does not appear to be a serious academic endeavor.\n\nThe author's response acknowledges these criticisms, expressing a desire to improve their writing skills and thanking the reviewers for their feedback. They also mention discovering similar ideas after submitting their paper and express interest in continuing research on the topic. However, the response does not address the fundamental issues raised by the reviewers in a manner that would suggest significant improvements to the paper's quality, clarity, or academic rigor.\n\nGiven the standards of a top-tier conference, which typically require papers to be well-written, clearly presented, and contribute meaningfully to the field with appropriate references to prior work, this paper falls short in several critical areas. The lack of clarity and poor writing make it difficult to assess the novelty or validity of the proposed method. The absence of references not only neglects the contribution of previous research but also fails to situate the paper's work within the broader context of the field.\n\nIn conclusion, while the idea of refining MCTS agents might be of interest, the paper's current state does not meet the expectations for publication in a top-tier conference due to its significant shortcomings in presentation, clarity, and academic rigor.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"NORML: Nodal Optimization for Recurrent Meta-Learning\" proposes a novel meta-learning framework that utilizes an LSTM-based meta-learner to perform neuron-wise optimization on a learner for efficient task learning. The paper's main contribution is the introduction of NORML, which allows the number of meta-learner parameters to increase linearly relative to the number of learner parameters, potentially enabling it to scale to learner networks with a large number of parameters.\n\nHowever, upon careful analysis of the reviewer comments and the paper itself, several concerns and issues arise that cast doubt on the paper's suitability for publication at a top-tier conference.\n\nFirstly, Reviewer 1 points out that the paper's comparison to the work of Ravi & Larochelle is misleading. The reviewer argues that Ravi & Larochelle's approach also employs parameter sharing, which means the number of parameters in the LSTM meta-learner is actually a constant relative to the size of the learner network. This undermines the paper's claim of novelty and improvement over existing methods.\n\nSecondly, Reviewer 1 and Reviewer 3 both express concerns about the lack of a direct comparison with the Meta-Learner LSTM (Ravi & Larochelle) and other relevant baselines, such as MAML++ (Antoniou et al.). The reviewers argue that the paper's results are not convincing and that a more comprehensive comparison with existing methods is necessary to validate the effectiveness of NORML.\n\nThirdly, Reviewer 2 and Reviewer 3 criticize the paper's writing style, citing numerous grammatical mistakes, inconsistent citation formatting, and poor flow of ideas. While these issues do not directly impact the paper's technical content, they do reflect poorly on the authors' attention to detail and ability to communicate their ideas effectively.\n\nLastly, Reviewer 3 raises concerns about the limited applicability of NORML, which is currently designed for fully connected networks. The reviewer argues that this limitation restricts the method's usefulness, as most few-shot learners typically employ convolutional layers.\n\nIn light of these concerns and issues, it is clear that the paper requires significant revisions and improvements before it can be considered suitable for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" presents a novel approach for training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. The authors propose three main contributions: (1) minimizing an upper bound to the GMM log likelihood, (2) introducing a new regularizer to prevent SGD from converging to pathological local minima, and (3) enforcing constraints inherent to GMM training when using SGD.\n\nThe reviewers raise several concerns and criticisms, including:\n\n1. **Lack of understanding of the history of online EM**: Reviewer 1 feels that the authors do not adequately acknowledge the existing work on online EM and develop an ad-hoc approach.\n2. **Incremental contribution**: Reviewer 1 believes that the work is largely incremental and does not provide significant new insights.\n3. **Numerical stability**: Reviewers 1 and 2 point out that the authors' approach to numerical stability is not well-justified and that standard tricks like log-sum-exp could be used instead.\n4. **Regularization**: Reviewer 2 is unclear about the motivation for the regularization term and its effect on the optimization problem.\n5. **Experimental evaluation**: Reviewers 1 and 2 express concerns about the experimental evaluation, including the choice of datasets, the lack of comparison to other baselines, and the absence of guarantees for convergence.\n6. **Clarity and presentation**: Reviewers 1 and 2 point out several issues with the presentation, including unclear notation, missing definitions, and repetitive paragraphs.\n\nThe authors respond to these concerns, providing additional context and justification for their approach. They acknowledge the limitations of their work and propose to address some of the concerns in future revisions.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has some merits, but it requires significant revisions to address the concerns raised by the reviewers. The authors' approach to training GMMs using SGD is novel, and their experiments demonstrate some promising results. However, the paper lacks clarity, and the presentation needs improvement. Additionally, the authors need to provide more convincing evidence of the effectiveness of their approach, including more comprehensive experiments and comparisons to other baselines.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" proposes a generative model called ROOTS for unsupervised object-wise 3D scene decomposition and rendering. The model is based on the Generative Query Networks (GQN) framework but provides object-oriented representation decomposition. The authors claim that their model can learn 3D representations of objects in a scene without supervision and can generalize to new scenes and objects.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. **Readability and clarity**: The reviewers have pointed out that the paper is poorly written, with many typos, grammatical errors, and inconsistencies in notation. The authors have acknowledged these issues and have rewritten the paper to improve its clarity and readability.\n2. **Lack of comparison to existing works**: The reviewers have noted that the paper does not adequately compare ROOTS to existing works in the field, particularly in the area of 3D scene understanding and object recognition. The authors have added more references to related works and have clarified the position of their work in comparison to existing works.\n3. **Experimental evaluation**: The reviewers have raised concerns about the experimental evaluation of ROOTS, including the lack of ablation studies, the use of a simple baseline (GQN), and the limited number of experiments. The authors have added more experiments, including ablation studies and comparisons to other models, to address these concerns.\n4. **Claims and novelty**: The reviewers have questioned the novelty of the paper's claims, particularly the claim that ROOTS is the first unsupervised model to identify objects in a 3D scene. The authors have clarified their claims and have acknowledged that their work builds upon existing research in the field.\n\nDespite these concerns, the reviewers have also praised the paper for its interesting ideas, impressive qualitative results, and potential impact on the field. The authors have addressed many of the reviewers' concerns in their rebuttal and have provided additional experiments and clarifications to support their claims.\n\nBased on the analysis of the paper and the reviewers' comments, I believe that the paper has the potential to make a significant contribution to the field of 3D scene understanding and object recognition. While the paper has some flaws, the authors have made a good faith effort to address the reviewers' concerns and have provided additional evidence to support their claims.\n\nTherefore, I recommend accepting the paper for publication, subject to the authors' revisions and clarifications.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of the impact of activation functions on the training of overparametrized neural networks. The authors provide a comprehensive study of the effect of different activation functions on the minimum eigenvalue of the Gram matrix, which is crucial for the convergence rate of training. The paper separates the analysis into two cases: non-smooth activation functions (e.g., ReLU, SELU, ELU) and smooth activation functions (e.g., tanh, swish, polynomial).\n\nThe strengths of the paper include:\n\n1. **Thorough theoretical analysis**: The authors provide a detailed and rigorous analysis of the effect of activation functions on the minimum eigenvalue of the Gram matrix.\n2. **Clear separation of cases**: The paper clearly separates the analysis into two cases, non-smooth and smooth activation functions, and provides distinct results for each case.\n3. **Empirical validation**: The authors provide empirical experiments on synthetic data and CIFAR10, which validate the theoretical results.\n4. **Well-written and self-contained**: The paper is well-written, and the authors provide a clear and concise explanation of the results and techniques used.\n\nThe weaknesses of the paper include:\n\n1. **Lengthy appendix**: The appendix is too long and may be difficult for a general audience to parse.\n2. **Some minor errors and clarifications**: The authors have addressed some minor errors and clarifications pointed out by the reviewers, such as the notation \"DZXP\" and the explanation of Assumption 1.\n\nThe reviewers' comments and the authors' responses have addressed the weaknesses of the paper. The authors have revised the paper to address the concerns, including adding a table of contents to the appendix, improving section names, and providing more explanations in some proofs.\n\nBased on the analysis, the paper provides a significant contribution to the understanding of the effect of activation functions on the training of overparametrized neural networks. The theoretical results are rigorous and well-supported by empirical experiments. The authors have addressed the weaknesses of the paper, and the revised version is improved.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" presents a novel approach to segmenting trajectories into sub-skills using weak supervision. The authors propose a method inspired by multiple instance learning (MIL) to learn reusable skills from a collection of human demonstrations. The paper is well-motivated, and the problem statement is interesting and of clear value.\n\nHowever, the reviewers have raised several concerns that need to be addressed. Reviewer 1 points out that the framing and motivation of the paper do not correspond to the results and experiments reported, which seems misleading. The reviewer also notes that the paper is limited in scope and that further experiments and comparisons to relevant baselines are needed to support the claims made in the paper. Reviewer 2 finds the novelty and results of the paper to be minor and suggests that the authors should provide more thorough baselines and comparisons to fully supervised methods. Reviewer 3 raises concerns about the feasibility of the approach when a large number of skills are present and suggests that the authors should provide more analysis and theoretical bounds on the chances of meeting the requirements needed for learning.\n\nThe authors have responded to the reviewers' comments, addressing some of the concerns and providing additional explanations and justifications for their approach. However, the reviewers' comments and the authors' responses suggest that the paper still has some limitations and areas for improvement.\n\nOne of the main concerns is that the paper's results are not impressive, with classification accuracy being relatively low. The authors attribute this to the ambiguity of some timesteps in trajectories, which makes it difficult to determine the correct skill. However, this raises questions about the effectiveness of the proposed method in practice.\n\nAnother concern is the lack of comparison to fully supervised methods and other baselines. The authors have promised to add these comparisons in the final version, which will help to better evaluate the performance of their approach.\n\nThe authors have also been asked to provide more analysis and theoretical bounds on the chances of meeting the requirements needed for learning. This is an important aspect, as it will help to understand the limitations and potential applications of the proposed method.\n\nIn conclusion, while the paper presents an interesting and novel approach to trajectory segmentation, it still has some limitations and areas for improvement. The authors have addressed some of the concerns, but further work is needed to strengthen the paper and provide more convincing results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns from epigenetic marks. The authors propose a bidirectional LSTM RNN model that outperforms traditional machine learning models, demonstrating the importance of sequential DNA states for chromatin folding.\n\nThe reviewer comments highlight several concerns, including the lack of methodological novelty, unclear description of data and methods, and limited evaluation. Reviewer 3 votes for rejection, citing the similarity to previously published works and the need for additional baseline models and evaluation metrics.\n\nThe author responses address some of these concerns, providing additional explanations and clarifications. The authors expand the description of the data and methods, add more evaluation metrics, and compare their model to other approaches. However, some concerns remain, such as the choice of the weighted MSE metric and the lack of generalizability to other Hi-C datasets or epigenetic features.\n\nDespite these limitations, the paper presents a well-motivated application of machine learning to a biological problem, and the authors demonstrate a good understanding of the underlying biology. The use of a bidirectional LSTM RNN model is a reasonable choice, given the sequential nature of DNA, and the authors provide some insights into the importance of sequential DNA states for chromatin folding.\n\nThe paper's contributions, although not highly novel, are still relevant and interesting. The authors provide a thorough comparison of their model to other approaches, and the results are well-presented and easy to follow. The paper's limitations, such as the lack of generalizability, are acknowledged by the authors, and they provide some discussion of potential future directions.\n\nOverall, while the paper has some limitations, it presents a well-motivated and interesting application of machine learning to a biological problem. The authors demonstrate a good understanding of the underlying biology, and the results are well-presented and easy to follow.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to coordinating multiple agents with pre-learned skills using a hierarchical reinforcement learning framework. The authors propose a modular framework that first trains individual skills with skill behavior diversification and then learns to coordinate these skills using a meta-policy.\n\nThe reviewers have provided constructive feedback, highlighting both the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they also raise concerns about the notation, the design choice of using a fixed horizon for the low-level skills, and the high variance in performance.\n\nReviewer 2 questions the need for treating temporal abstraction under the multi-agent setting differently from the single-agent setting and asks for clarification on the design choice of using a fixed horizon for the low-level skills. They also request more discussion on alternative multi-agent methods.\n\nReviewer 3 finds the approach to be simple and scalable but criticizes the reliance on pre-defined subtasks. They suggest using the DIAYN objective on its own to encourage diverse behaviors and question the importance of expert knowledge in the final performance.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional experiments to support their claims. They have also added more discussion on alternative multi-agent methods and clarified the design choices.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field of multi-agent reinforcement learning. The authors have presented a novel approach to coordinating multiple agents with pre-learned skills, and the empirical results demonstrate the effectiveness of their method.\n\nWhile the reviewers have raised some concerns, the authors have adequately addressed these concerns in their response. The additional experiments and discussions provided by the authors have strengthened the paper and alleviated the concerns raised by the reviewers.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel connection between the expressivity of deep neural networks (DNNs) and Sharkovsky's Theorem from dynamical systems. The authors leverage this connection to characterize the depth-width trade-offs of ReLU networks for representing functions with periodic points. The paper provides a thorough analysis of the problem, including a lower bound on the depth necessary to represent periodic functions.\n\nThe reviewer comments are overwhelmingly positive, with Reviewer 1 and Reviewer 2 praising the paper's clarity, technical soundness, and contribution to the field. Reviewer 3 provides some constructive criticism, suggesting that the paper's structure could be improved and that the theoretical background, although interesting, could be presented in a more accessible way.\n\nThe author response addresses the reviewers' comments and suggestions, providing additional examples, discussions, and experimental results to support the paper's claims. The authors also clarify the potential impact of introducing a bias term and provide intuition for some of the definitions in the paper.\n\nThe paper's strengths include:\n\n1. **Novel connection**: The paper establishes a new connection between DNNs and Sharkovsky's Theorem, which is a significant contribution to the field.\n2. **Technical soundness**: The paper's technical analysis is thorough and well-founded, with a clear and well-structured presentation.\n3. **Clear writing**: The paper is well-written, with clear explanations and concise notation.\n4. **Strong reviewer feedback**: The reviewer comments are overwhelmingly positive, indicating that the paper is of high quality and has a significant impact on the field.\n\nThe paper's weaknesses include:\n\n1. **Limited practical applicability**: Reviewer 3 raises a valid point that the paper's results, although theoretically interesting, may not be directly applicable to practical problems.\n2. **Structural suggestions**: Reviewer 3 suggests that the paper's structure could be improved, with a clearer presentation of the main results and contributions.\n\nHowever, the paper's strengths outweigh its weaknesses, and the author response addresses the reviewers' comments and suggestions. The paper's novel connection, technical soundness, and clear writing make it a significant contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to coordinating multiple agents with pre-learned adaptable skills. The authors propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification and then learns to coordinate end-effectors using diverse behaviors of the skills.\n\nThe reviewers have provided constructive feedback, highlighting both the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they also raise concerns about the notations, the design choice of the fixed horizon of the low-level skills, and the high variance in performance.\n\nReviewer 2 questions the novelty of the work, asking whether the authors are simply applying existing techniques to a new problem. They also ask about the importance of expert knowledge in specifying subtasks and whether the DIAYN method is the best choice for skill discovery.\n\nReviewer 3 provides a weak acceptance, stating that the approach is simple and seems scalable but relies on specifying subtasks in advance. They suggest that the authors consider using the DIAYN objective on its own to encourage sufficiently diverse behaviors.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional experiments to support their claims. They have also added new baselines, including a centralized policy and a decentralized policy with Skill Behavior Diversification, to evaluate the importance of expert knowledge.\n\nAfter carefully analyzing the paper, the reviewer comments, and the author responses, I conclude that the paper has made significant contributions to the field of multi-agent reinforcement learning. The authors have presented a novel approach to coordinating multiple agents with pre-learned adaptable skills, and the empirical results demonstrate the effectiveness of their method.\n\nWhile the reviewers have raised some concerns, the authors have adequately addressed them in their responses. The additional experiments and baselines provided by the authors have strengthened the paper and demonstrated the importance of expert knowledge in specifying subtasks.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"SGD Learns One-Layer Networks in WGANs\" presents a theoretical analysis of the convergence of stochastic gradient descent-ascent (SGDA) in the context of Wasserstein Generative Adversarial Networks (WGANs) with a one-layer generator network and a quadratic discriminator. The authors claim to provide the first result showing convergence to global equilibrium points for non-linear generators in the GAN setting.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. **Simplistic discriminator**: Reviewers 1, 2, and 3 argue that the quadratic discriminator is too simple and does not capture the true nature of WGANs, which typically use more complex neural network discriminators.\n2. **Limited generality**: Reviewers 1 and 3 point out that the results are limited to a specific setting (one-layer generator and quadratic discriminator) and may not be extendable to more complex scenarios.\n3. **Lack of clarity**: Reviewer 1 expresses difficulty in understanding how the quadratic discriminator can be treated as a WGAN, and Reviewer 3 requests more explanations to bound the error propagation.\n\nThe authors respond to these concerns by:\n\n1. **Justifying the choice of discriminator**: The authors argue that the quadratic discriminator has sufficient distinguishing power to learn the family of distributions parametrized by the generator class and that using a larger discriminator family would increase the sample complexity.\n2. **Highlighting the contribution**: The authors emphasize that their primary contribution is the analysis of gradient descent-ascent in the GAN setting, which has not been extensively studied before, and that their result provides a first step towards understanding the convergence of SGDA in WGANs.\n3. **Providing additional explanations**: The authors clarify the meaning of \"one-layer generator\" and provide additional explanations for the choice of discriminator and the error propagation.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has some merits, but the concerns raised by the reviewers are significant. The paper's contribution is limited to a specific setting, and the use of a quadratic discriminator may not be representative of the typical WGAN setup. While the authors provide some justifications for their choices, the paper's results may not be generalizable to more complex scenarios.\n\nHowever, the paper does provide a first step towards understanding the convergence of SGDA in WGANs, and the authors' analysis of the gradient descent-ascent dynamics is non-trivial. With some revisions to address the reviewers' concerns, the paper could be improved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning to Transfer via Modelling Multi-level Task Dependency\" proposes a novel multi-task learning framework that models both general task dependency and data-specific task dependency using an attention mechanism. The authors claim that their approach can capture the hierarchical structure in text and graph data, which is not fully utilized by previous works.\n\nThe reviewers have raised several concerns and suggestions, including:\n\n1. Grammar and clarity: The paper needs thorough proof-reading to improve grammar and clarity.\n2. Distinction between general task dependency and data-specific task dependency: The reviewers suggest that the authors need to provide more clarity on the difference and contribution of each level.\n3. Comparison with previous works: The reviewers point out that the authors need to compare their approach with previous works, such as Taskonomy, and provide a clear discussion on how their approach is different and better.\n4. Experimental evaluation: The reviewers suggest that the authors need to add more baselines, such as a method that does not involve attention, and provide a more thorough evaluation of their approach.\n\nThe authors have responded to the reviewers' comments and suggestions, providing more details on their approach, clarifying the distinction between general task dependency and data-specific task dependency, and adding more experimental results.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has some merits, but it still needs significant improvements. The authors have addressed some of the reviewers' concerns, but there are still some issues that need to be resolved.\n\nThe main strengths of the paper are:\n\n1. Novel approach: The paper proposes a novel approach to multi-task learning that models both general task dependency and data-specific task dependency.\n2. Hierarchical structure: The authors provide a clear motivation for modeling the hierarchical structure in text and graph data.\n\nThe main weaknesses of the paper are:\n\n1. Lack of clarity: The paper still needs significant improvements in terms of clarity and grammar.\n2. Limited comparison with previous works: The authors need to provide a more thorough comparison with previous works, such as Taskonomy, and discuss how their approach is different and better.\n3. Limited experimental evaluation: The authors need to provide more baselines and a more thorough evaluation of their approach.\n\nBased on these strengths and weaknesses, I believe that the paper needs significant revisions before it can be accepted for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" proposes a novel method for defending against adversarial attacks on deep neural networks. The method, called AE-GAN+sr, uses an autoencoder-based approach to purify input images by searching for the closest natural reconstruction in the latent space.\n\nThe paper has several strengths. Firstly, the proposed method shows promising results in defending against various types of adversarial attacks, including white-box, black-box, and gray-box attacks. The authors also provide a thorough analysis of the method's performance, including ablation studies and comparisons with other state-of-the-art defense methods.\n\nHowever, the paper also has some weaknesses. One of the main concerns is the lack of clarity in the writing, which makes it difficult to follow the paper's arguments and understand the method's details. Additionally, the paper has several typos and grammatical errors, which further detract from its overall quality.\n\nAnother concern is the incremental nature of the proposed method. The authors build upon existing work, such as Defense-GAN, but the improvements are not substantial enough to warrant a separate paper. The method's performance is also not significantly better than other state-of-the-art defense methods, which raises questions about its novelty and impact.\n\nThe reviewers' comments also highlight several issues with the paper. Reviewer 2 points out that the comparison with other defense methods is incomplete, and the authors should include more state-of-the-art methods in the comparison. Reviewer 3 raises concerns about the method's reliance on the autoencoder's ability to detect adversarial samples, which may not be robust against white-box attacks. Reviewer 4 finds the writing to be hard to follow and suggests that the authors should improve the paper's clarity and provide more details about the method's implementation.\n\nIn response to the reviewers' comments, the authors provide additional results and analysis, including comparisons with other defense methods and more detailed explanations of the method's implementation. However, the authors' responses do not fully address the reviewers' concerns, and some of the issues raised by the reviewers remain unresolved.\n\nOverall, while the paper has some promising results and ideas, its weaknesses and limitations outweigh its strengths. The lack of clarity in the writing, the incremental nature of the proposed method, and the unresolved issues raised by the reviewers all contribute to a negative assessment of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Differentiable Hebbian Consolidation for Continual Learning\" proposes a novel approach to address the problem of catastrophic forgetting in neural networks. The authors introduce a Differentiable Hebbian Consolidation model, which combines a Differentiable Hebbian Plasticity (DHP) Softmax layer with a fixed (slow changing) parameter to enable learned representations to be retained for a longer timescale.\n\nThe paper is well-written, and the authors provide a clear motivation for their approach, drawing inspiration from complementary learning system (CLS) theory. The method is simple and doesn't seem to entail a significant computational burden. The authors also provide a thorough evaluation of their approach on several benchmarks, including Permuted MNIST, Split MNIST, and Vision Datasets Mixture, and introduce an imbalanced variant of Permuted MNIST.\n\nThe reviewer comments highlight some strengths and weaknesses of the paper. Reviewer 1 suggests that the paper lacks a stronger empirical evaluation, particularly on more challenging datasets. Reviewer 2 finds the idea of fast/slow weights to be refreshing and different from previous work but has trouble following the details of the DHP Softmax. Reviewer 3 is generally positive about the paper but raises concerns about the lack of comparison to other CLS-based approaches and the limited scope of the experimental evaluation. Reviewer 4 considers the paper to be a weak reject, citing the limited performance of the DHP Softmax on simpler continual learning settings.\n\nThe authors' response addresses some of the reviewer comments, providing additional experiments on Split CIFAR-10/100 and clarifying some of the notation and terminology. However, the authors acknowledge that they did not have time to perform all the experiments suggested by the reviewers, particularly on more challenging datasets such as CIFAR-100 and ILSVRC.\n\nAfter carefully considering the paper and the reviewer comments, I believe that the paper has some merits, particularly in its novel approach to addressing catastrophic forgetting. However, the limited scope of the experimental evaluation and the lack of comparison to other CLS-based approaches are significant weaknesses.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The research paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" has been submitted for publication at a top-tier conference. After carefully analyzing the paper's title, abstract, reviewer comments, and author responses, several critical issues have been identified that significantly impact the paper's suitability for publication.\n\nFirstly, the paper lacks clarity and coherence in its writing. All three reviewers have commented on the poor writing quality, with grammatical issues, incomprehensible passages, and an overall lack of professionalism. This makes it extremely difficult for readers to understand the paper's content, methodology, and contributions. For a paper to be considered for publication at a top-tier conference, it must be well-written and clearly convey its ideas and research findings.\n\nSecondly, the paper does not cite any previous research or provide references. This is a significant omission, as it fails to demonstrate the authors' understanding of the existing body of work in the field and does not allow readers to contextualize the research within the broader literature. The author's response that they did not know how to use a.bib file and therefore removed all references is not a valid excuse, as proper citation and referencing are fundamental aspects of academic writing.\n\nThirdly, the reviewers have expressed concerns about the paper's lack of scientific rigor and academic seriousness. Reviewer 3 has noted that the paper appears to be merely a set of notes rather than a serious academic endeavor. This perception is reinforced by the absence of a clear methodology, the lack of defined mathematical symbols, and the poor presentation of algorithmic details.\n\nLastly, while the author has responded to the reviewers' comments and acknowledged the need to improve their writing skills, the response does not address the substantive issues raised by the reviewers. The author's hope to improve their skills and earn more money to afford proofreading before the next submission is not relevant to the current paper's suitability for publication.\n\nGiven these significant concerns, it is clear that the paper in its current form does not meet the standards expected of a top-tier conference. The issues with writing quality, lack of references, and perceived lack of scientific rigor are too substantial to overlook.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Domain Adaptive Multibranch Networks\" proposes a novel approach to unsupervised domain adaptation, which is a crucial problem in computer vision. The authors introduce a deep learning framework that allows each domain to undergo a different sequence of operations, enabling more complex domains to go through more computations than others. This approach is in contrast to state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations.\n\nThe reviewers have provided thorough comments on the paper, highlighting both its strengths and weaknesses. Reviewer 1 finds the paper well-motivated, with sufficient related work and convincing experimental results. However, they suggest linking the paper to more transfer learning techniques outside the deep learning domain and avoiding the use of arxiv references. Reviewer 2 appreciates the novelty of the approach but points out that the idea of adaptive computation is not new and has been explored before. They also suggest experimenting with different amounts of computation per domain and comparing the results to state-of-the-art domain adaptation methods. Reviewer 3 finds the paper interesting and novel but highlights the need for more extensive experiments, including comparisons to other state-of-the-art methods and ablation studies.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional experiments and explanations. They have updated the manuscript to reflect the changes and have provided more comparisons to state-of-the-art methods.\n\nAfter carefully analyzing the paper, the reviewer comments, and the author responses, I believe that the paper has made significant contributions to the field of domain adaptation. The proposed approach is novel and has shown promising results. The authors have addressed the reviewers' concerns and have provided additional experiments and explanations to support their claims.\n\nHowever, I also believe that the paper could benefit from more extensive experiments and comparisons to other state-of-the-art methods. The authors have provided some additional results, but more work is needed to fully demonstrate the effectiveness of their approach.\n\nDespite these limitations, I think that the paper has the potential to make a significant impact in the field of computer vision and domain adaptation. The authors have shown a good understanding of the problem and have proposed a novel solution that addresses the limitations of existing methods.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Interaction Processes for Time-Evolving Graphs\" proposes a novel approach to modeling continuous time-evolving graphs using a temporal point process framework. The authors introduce a principled deep neural approach that models continuous time-evolving graphs at multiple time resolutions based on a temporal point process framework. The approach includes a mixture of temporal cascades, a generalized LSTM with time gates, and a selection mechanism to give important nodes large influence in both k-hop subgraphs of nodes in an interaction.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. Reviewer 1 praises the paper's empirical performance on multiple datasets but raises concerns about the novelty and justification of the contributions. Reviewer 2 questions the novelty of the approach, citing prior work that introduces similar techniques. Reviewer 3 finds the paper difficult to understand and requests more details on the computational cost and scalability of the proposed network.\n\nThe author response addresses many of the reviewer comments, providing additional explanations, justifications, and experimental results. The authors clarify the novelty of their approach, highlighting the differences between their work and prior research. They also provide more details on the computational cost and scalability of their method.\n\nAfter carefully analyzing the paper, reviewer comments, and author response, I conclude that the paper has several strengths:\n\n1. **Novel approach**: The paper proposes a novel approach to modeling continuous time-evolving graphs using a temporal point process framework.\n2. **Empirical performance**: The paper demonstrates improved empirical performance on multiple datasets compared to baseline methods.\n3. **Addressing reviewer comments**: The author response addresses many of the reviewer comments, providing additional explanations and justifications.\n\nHowever, the paper also has some weaknesses:\n\n1. **Lack of clarity**: Reviewer 3 finds the paper difficult to understand, highlighting the need for more detailed explanations and illustrations.\n2. **Novelty concerns**: Reviewers 1 and 2 raise concerns about the novelty of the approach, citing prior work that introduces similar techniques.\n3. **Limited analysis**: The paper could benefit from more detailed analysis of the model, including ablation studies and sensitivity analysis.\n\nDespite these weaknesses, I believe that the paper's strengths outweigh its limitations. The author response addresses many of the reviewer comments, and the paper demonstrates improved empirical performance on multiple datasets.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents a study on reducing the number of supervised synaptic updates required to achieve a brain-like ventral visual stream in artificial neural networks (ANNs). The authors propose three complementary strategies: reducing the number of supervised updates, improving the random distribution of synaptic connectivity, and training only a subset of model synapses. They demonstrate that these strategies can substantially reduce the number of supervised updates while maintaining a high match to the brain.\n\nThe reviewers have raised several concerns and questions about the paper, including the interpretation of the Brain-Score metric, the biological plausibility of the proposed methods, and the significance of the results. The authors have responded to these concerns, clarifying the limitations of the Brain-Score metric, the hypothetical nature of their proposed methods, and the potential implications of their findings.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper makes a significant contribution to the field of computational neuroscience and deep learning. The authors' proposed strategies for reducing supervised synaptic updates are innovative and well-motivated, and their results demonstrate the potential for significant reductions in the number of updates required to achieve brain-like performance.\n\nHowever, I also acknowledge the concerns raised by the reviewers regarding the interpretation of the Brain-Score metric and the biological plausibility of the proposed methods. The authors have addressed these concerns to some extent, but further clarification and discussion may be necessary to fully resolve these issues.\n\nOverall, I believe that the paper has the potential to make a significant impact in the field and that the authors have made a good faith effort to address the concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns from epigenetic marks. The authors propose a bidirectional LSTM RNN model that outperforms traditional machine learning models, demonstrating the importance of sequential DNA states for chromatin folding.\n\nThe reviewer comments highlight several concerns, including:\n\n1. **Lack of methodological novelty**: Reviewer 3 points out that the paper is similar to previously published works, which may undermine its originality.\n2. **Unclear data and methods description**: Reviewers 2 and 3 mention that the description of the data and methods is not clear enough, making it difficult to understand the paper.\n3. **Insufficient evaluation**: Reviewers 1, 2, and 3 suggest that the evaluation of the models is not comprehensive enough, with some metrics and baselines missing.\n4. **Lack of generalizability**: Reviewer 1 questions the generalizability of the approach to other Hi-C datasets or epigenetic features beyond ChIP-seq.\n\nThe author responses address some of these concerns, providing additional explanations and clarifications. However, some issues remain:\n\n1. **Methodological novelty**: The authors argue that their work is focused on predicting Topologically Associated Domain characteristics from ChIP-seq data, which is different from previous works. However, this distinction may not be sufficient to establish methodological novelty.\n2. **Data and methods description**: While the authors have expanded the description of the data and methods, some reviewers may still find it unclear or incomplete.\n3. **Evaluation**: The authors have added more evaluation metrics and baselines, but the reviewers may still consider the evaluation insufficient.\n\nConsidering these factors, I weigh the strengths and weaknesses of the paper:\n\nStrengths:\n\n* The paper presents an interesting application of machine learning to a biological problem.\n* The authors have made an effort to address the reviewer comments and improve the paper.\n\nWeaknesses:\n\n* The paper may lack methodological novelty, which is a crucial aspect of a top-tier conference.\n* The description of the data and methods may still be unclear or incomplete.\n* The evaluation may not be comprehensive enough to fully support the authors' claims.\n\nAfter careful consideration, I conclude that the paper requires further improvements to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Complex Query Answering with Neural Link Predictors\" proposes a novel framework for efficiently answering complex queries on incomplete Knowledge Graphs (KGs). The authors introduce two optimization methods, continuous and combinatorial, to solve the query answering problem. The continuous method uses gradient-based optimization, while the combinatorial method employs a beam search approach.\n\nThe paper has several strengths:\n\n1. **Novel approach**: The authors propose a new approach to complex query answering, which is different from existing methods that embed queries into a low-dimensional space. Instead, they use a neural link predictor to score atomic queries and then combine the scores using t-norms and t-conorms.\n2. **State-of-the-art results**: The authors report state-of-the-art results on several benchmark datasets, outperforming existing methods such as GQE and Q2B.\n3. **Explainability**: The authors provide a way to explain the results of the model by showing the intermediate answers for the atomic queries.\n4. **Less training data required**: The authors claim that their method requires less training data than existing methods, as it only needs to train on 1-hop queries.\n\nHowever, the paper also has some weaknesses:\n\n1. **Dependence on neural link predictor**: The authors' method relies heavily on the choice of neural link predictor. While they report results with ComplEx, it is unclear how the method would perform with other neural link predictors.\n2. **Lack of ablation studies**: The authors do not provide ablation studies to analyze the contribution of each component of their method.\n3. **Timing results**: The authors do not provide detailed timing results for their method, which makes it difficult to evaluate its efficiency.\n4. **Mathematical formalism**: Some reviewers found the mathematical formalism in the paper to be excessive and unnecessary.\n\nThe authors have addressed some of the reviewers' concerns in their response, such as providing additional experiments with different neural link predictors and including timing results. However, some concerns remain, such as the lack of ablation studies and the dependence on the neural link predictor.\n\nOverall, the paper presents a novel and effective approach to complex query answering, with state-of-the-art results and explainability. While it has some weaknesses, the authors have addressed some of the concerns raised by the reviewers. Therefore, I believe that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Coherent Exploration For Continuous Control\" presents a novel exploration framework for deep reinforcement learning (RL) algorithms on continuous control tasks. The method, called Deep Coherent Exploration, generalizes step-based and trajectory-based exploration by modeling the last layer parameters of the policy network as latent variables and using a recursive inference step within the policy update.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The main strengths of the paper are:\n\n1. **Scalability**: The method is scalable to deep RL algorithms, which is a significant contribution to the field.\n2. **Improved performance**: The experimental results show that Deep Coherent Exploration improves the speed and stability of learning of A2C, PPO, and SAC on several continuous control tasks.\n3. **Mathematical solidity**: The paper provides a clear and detailed mathematical derivation of the method, which is a significant strength.\n\nHowever, there are also some weaknesses and concerns raised by the reviewers:\n\n1. **Relationship to prior work**: The method is closely related to prior work (van Hoof et al., 2017), and some reviewers question the novelty of the approach.\n2. **Off-policy methods**: The integration of Deep Coherent Exploration with off-policy methods, such as SAC, is more heuristic than the integration with on-policy methods, and the experimental results are not as strong.\n3. **Limited experiments**: The experiments are limited to MuJoCo tasks, and some reviewers suggest that more experiments on other domains and tasks are needed to demonstrate the method's effectiveness.\n\nThe authors have addressed some of the concerns in their response, including:\n\n1. **New formulation for Coherent-SAC**: The authors propose a new mathematical formulation for Coherent-SAC, which is more consistent with the approach for on-policy methods.\n2. **Updated experimental results**: The authors are working on updating the experimental results for Coherent-SAC and expect to show improved performance.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' responses, I conclude that the paper has made significant contributions to the field of reinforcement learning, particularly in terms of scalability and improved performance. While there are some weaknesses and concerns, the authors have addressed some of these concerns in their response, and the paper has the potential to be a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes a generalized probability kernel (GPK) on discrete distributions with finite support, which generalizes existing discrepancy statistics such as maximum mean discrepancy (MMD) and kernel Stein discrepancy (KSD). The authors aim to connect the fields of discrete distribution-property estimation and kernel-based hypothesis testing.\n\nHowever, the paper has several major issues that need to be addressed. The reviewers have pointed out numerous typos, grammatical errors, and inconsistencies in notation, which make it difficult to follow the paper. The authors have acknowledged these issues and claim to have revised the paper to address them.\n\nThe reviewers have also raised several technical concerns. Reviewer 1 points out that the definition of the GPK is unclear and lacks rigor, and that the proof of Theorem 5 is missing. Reviewer 2 argues that the paper lacks innovation and that the presented theory is mostly a rehashing of existing results. Reviewer 3 questions the relevance of the methodology in the context of discrete random variables, where the labels do not necessarily relate to a notion of distance or ordering. Reviewer 4 is confused about the notation and the definition of the GPK, and points out that the paper lacks a clear explanation of how the GPK can be applied to two-sample testing.\n\nThe authors have responded to these concerns, but their responses are not entirely satisfactory. They acknowledge that their initial proof of Theorem 7 was incorrect and have removed the discussion of KSD from the revised paper. They also claim to have revised the notation and presentation to address the reviewers' concerns. However, the authors' responses do not fully address the technical concerns raised by the reviewers, and it is unclear whether the revised paper has adequately addressed these issues.\n\nFurthermore, the paper lacks experimental evaluation, which is a critical component of any paper proposing a new methodology. The authors claim to have proposed a new statistic for two-sample testing, called power-MMD, but they do not provide any empirical evidence to support its effectiveness.\n\nIn light of these concerns, I believe that the paper requires significant revisions before it can be considered for publication. The authors need to address the technical concerns raised by the reviewers, provide a clear and rigorous definition of the GPK, and include experimental evaluations to demonstrate the effectiveness of their proposed methodology.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" presents a novel approach to the problem of Mahalanobis metric learning, formulating it as an optimization problem with the objective of minimizing the number of violated similarity/dissimilarity constraints. The authors propose a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time, leveraging tools from the theory of linear programming in low dimensions. They also provide experimental results on synthetic and real-world data sets, demonstrating the algorithm's performance and its ability to handle adversarial noise.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. Reviewer 1 appreciates the sound theory and the algorithm's performance on standard data sets but questions the state-of-the-art and novelty of the approach. Reviewer 2 finds the paper interesting, noting that it brings an LP-type problem into the context of distance learning, but raises several technical questions and suggests improvements. Reviewer 3 is more critical, suggesting that the transformation of the problem into a linear programming formulation is not particularly novel and that the computational or theoretical gains of the proposed approach are unclear.\n\nThe authors' responses address many of the reviewers' concerns, providing clarifications on the novelty of their contribution, the differences between their approach and prior works, and the theoretical guarantees of their algorithm. They acknowledge the need for clearer explanations and definitions in the paper and commit to revising the manuscript to address these issues.\n\nUpon careful analysis, several key points emerge:\n\n1. **Novelty and Contribution**: The authors argue that their algorithm is the first with a provable guarantee on the number of violated constraints for arbitrary inputs, which is a significant contribution to the field of metric learning. This novelty, combined with the application of LP-type problems to distance learning, suggests a substantial contribution.\n\n2. **Technical Soundness**: The reviewers raise several technical questions, but the authors' responses provide satisfactory clarifications. The use of LP-type problems and the derivation of an FPTAS are technically sound and represent a valid approach to the problem.\n\n3. **Experimental Evaluation**: The experimental results demonstrate the algorithm's performance, especially in handling adversarial noise, which is a significant advantage. However, the comparison with state-of-the-art methods could be more comprehensive.\n\n4. **Clarity and Presentation**: The reviewers criticize the paper for lacking clarity in some explanations and for omitting important definitions. The authors acknowledge these issues and plan to revise the paper to improve clarity and completeness.\n\n5. **State-of-the-Art and Comparisons**: The discussion on the state-of-the-art could be more detailed, including a clearer comparison with recent works like Verma and Branson, and Ye et al. The authors provide some comparisons but could further elaborate on how their approach differs and improves upon existing methods.\n\nConsidering these points, the paper demonstrates significant technical contributions, a novel approach to Mahalanobis metric learning, and promising experimental results. While there are areas for improvement, particularly in terms of clarity and the presentation of comparisons with prior works, these can be addressed through revisions.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Disentangling Representations of Text by Masking Transformers\" presents a novel approach to learning disentangled representations from pre-trained language models. The method involves learning binary masks over transformer weights or hidden units to identify subnetworks that encode distinct, complementary aspects of the representation. The authors evaluate their approach on two tasks: disentangling syntax and semantics, and sentiment from genre in movie reviews.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novel approach**: The paper presents a new and interesting approach to learning disentangled representations, which is a key challenge in natural language processing.\n2. **Well-written paper**: The paper is well-organized, and the authors provide clear explanations of their method and experiments.\n3. **Convincing results**: The experimental results demonstrate the effectiveness of the proposed approach in disentangling representations, and the authors provide a thorough analysis of the results.\n\nHowever, the reviewers also raise several concerns and questions, including:\n\n1. **Lack of generality**: Some reviewers question the generality of the proposed method, as it is evaluated on only two tasks and two datasets.\n2. **Comparison to baselines**: The reviewers suggest that the paper could benefit from more comprehensive comparisons to existing baselines, such as variational auto-encoders.\n3. **Experimental setup**: Some reviewers question the experimental setup, particularly the choice of datasets and tasks.\n4. **Interpretability**: The reviewers ask for more insight into the learned masks and their relationship to the underlying representations.\n\nThe author response addresses some of these concerns, providing additional experiments and analysis to support the claims made in the paper. However, some concerns remain, particularly regarding the generality of the proposed method and the lack of comparison to existing baselines.\n\nAfter carefully considering the reviewer comments and author response, I conclude that the paper has several strengths, including a novel approach, well-written paper, and convincing results. However, the paper also has some weaknesses, including limited generality, incomplete comparison to baselines, and lack of interpretability.\n\nTo address these concerns, I would recommend that the authors:\n\n1. **Conduct additional experiments**: Evaluate the proposed method on more tasks and datasets to demonstrate its generality.\n2. **Compare to existing baselines**: Provide more comprehensive comparisons to existing baselines, such as variational auto-encoders.\n3. **Improve interpretability**: Provide more insight into the learned masks and their relationship to the underlying representations.\n\nGiven the current state of the paper, I believe that it has the potential to make a significant contribution to the field of natural language processing. However, I also believe that the authors need to address the concerns raised by the reviewers to strengthen the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"New Bounds For Distributed Mean Estimation and Variance Reduction\" presents a novel approach to distributed mean estimation and variance reduction, leveraging lattice theory to achieve improved bounds on communication complexity. The authors provide a thorough analysis of their method, including theoretical guarantees and experimental evaluations.\n\nThe reviewers' comments highlight several strengths of the paper, including its clear presentation, natural motivation, and thorough analysis. The authors' response to the reviewers' comments demonstrates a willingness to address concerns and improve the paper.\n\nHowever, the reviewers also raise several concerns, including the lack of comparison to existing methods, the need for more discussion on the choice of convex hull, and the potential for improvement in the experimental evaluation. The authors' response acknowledges these concerns and provides plans for addressing them in the revision.\n\nUpon careful analysis of the paper and the reviewers' comments, it is clear that the paper makes a significant contribution to the field of distributed machine learning. The authors' approach is novel and well-motivated, and their analysis is thorough and well-supported by experimental evidence.\n\nWhile the reviewers' comments highlight several areas for improvement, the authors' response demonstrates a willingness to address these concerns and improve the paper. Therefore, I believe that the paper has the potential to be a strong contribution to the field, and that the authors are capable of addressing the reviewers' concerns in the revision.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" proposes a novel method for multitask learning that dynamically adjusts task weights during training. The method, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), uses task-specific updates of the model's parameters to estimate the positive or negative influence of auxiliary tasks on the target task.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical justification for the proposed algorithm, the limited experimental results, and the similarity to existing methods. Reviewer 1 suggests that the paper needs more theoretical analysis and justification for the proposed methodology, while Reviewer 2 questions the novelty of the method and its difference from existing approaches. Reviewer 3 is not convinced by the proposed algorithm and suggests that the experimental results are not significant enough to justify the method.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and justifications for their method. They acknowledge the lack of theoretical analysis and promise to address this in a future version of the paper. They also provide more details about their experimental setup and results, and clarify the differences between their method and existing approaches.\n\nAfter carefully analyzing the paper and the reviewers' comments, I have several concerns that need to be addressed:\n\n1. **Lack of theoretical justification**: The paper lacks a clear theoretical justification for the proposed algorithm. The authors acknowledge this and promise to address it in a future version, but this is a significant concern that needs to be addressed before publication.\n2. **Limited experimental results**: The experimental results are limited to two domains (computer vision and natural language understanding) and a small number of tasks. More extensive experiments are needed to demonstrate the effectiveness of the method.\n3. **Similarity to existing methods**: The method bears some similarity to existing approaches, such as dynamic task weighting and meta-learning. The authors need to clearly demonstrate the novelty and advantages of their method compared to existing approaches.\n4. **Statistical significance**: The experimental results are not statistically significant in some cases, which raises concerns about the reliability of the method.\n\nDespite these concerns, the paper has some strengths, including:\n\n1. **Clear writing**: The paper is well-written and easy to follow.\n2. **Novel approach**: The proposed method is novel and has the potential to contribute to the field of multitask learning.\n3. **Good response to reviewers**: The authors have provided a detailed and thoughtful response to the reviewers' comments, addressing many of the concerns raised.\n\nIn conclusion, while the paper has some strengths, the concerns raised by the reviewers and the lack of theoretical justification, limited experimental results, and similarity to existing methods are significant enough to warrant rejection. The authors need to address these concerns and provide more extensive experiments and theoretical analysis before the paper can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method for multitask learning, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), which adjusts task weights dynamically during model training. The method is based on the idea of using task-specific updates of the model's parameters between training epochs to estimate the positive or negative influence of auxiliary tasks on the target task.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical justification for the proposed algorithm, the limited experimental results, and the similarity to existing methods. Reviewer 1 suggests that the paper needs to provide more theoretical analysis and justification for the proposed method, and that the experimental results are not convincing enough to justify the proposed method. Reviewer 2 points out that the proposed method is not particularly new, and that dynamic task weighting is not a novel concept. Reviewer 3 suggests that the paper can be made stronger with more thorough discussion on the algorithm and its properties, and that the experimental results suggest that the proposed algorithm performs more or less similar to previous methods.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and justifications for their method. They acknowledge the lack of theoretical justification and suggest that they will remedy this in the camera-ready version. They also provide additional experimental results and comparisons to existing methods, which suggest that their method performs well on certain tasks.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has some merits, but it also has some significant weaknesses. The proposed method is novel and has some potential, but it lacks theoretical justification and the experimental results are limited. The authors have provided some additional explanations and justifications, but they are not sufficient to fully address the reviewers' concerns.\n\nTherefore, I believe that the paper needs significant revisions and improvements before it can be considered for publication. The authors need to provide more theoretical analysis and justification for their method, and they need to conduct more extensive experimental evaluations to demonstrate the effectiveness of their method.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents a novel approach to integrating linguistic and visual information for image segmentation tasks based on referring expressions. The authors argue that using language to modulate both top-down and bottom-up visual processing can lead to significant improvements over traditional methods that only consider top-down attention. The paper is well-written, and the proposed model is sound and well-described.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 suggests adding figures to explain the concepts of top-down and bottom-up processing, citing relevant papers for referring expression comprehension, and improving the clarity of Figure 1. Reviewer 2 questions the effectiveness of the model, citing concerns about over-tuning and the lack of statistical significance tests. Reviewer 3 is generally positive about the paper, finding it well-written and addressing an important problem. Reviewer 4 votes for rejection, citing concerns about the significance and quality of the paper, including the lack of clear conceptual motivation and error analysis.\n\nThe authors have responded to the reviewers' comments, addressing concerns about over-tuning, statistical significance, and the novelty of the model. They have also implemented some of the suggested changes, such as adding citations and improving figure clarity.\n\nAfter carefully considering the paper, the reviewer comments, and the author responses, I believe that the paper has some merits, including a novel approach to integrating linguistic and visual information and promising experimental results. However, the concerns raised by the reviewers about the significance and quality of the paper, including the lack of clear conceptual motivation and error analysis, are significant.\n\nThe authors' responses to the reviewers' comments have partially addressed these concerns, but some issues remain. For example, the authors acknowledge the lack of statistical significance tests but argue that performing such tests would be time-consuming. While this is a valid concern, it is an important aspect of ensuring the reliability of the results.\n\nFurthermore, the authors' claims about the novelty of the model and its improvement over existing work are not entirely convincing. The comparison to Step-ConvRNN, in particular, is not clear, and the authors' argument that their model is more suitable for ablation studies is not fully persuasive.\n\nIn light of these concerns, I believe that the paper requires significant revisions to address the reviewers' comments and improve its overall quality and significance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method for multitask learning, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), which adjusts task weights dynamically during model training. The method is based on the idea of using task-specific updates of the model's parameters to estimate the positive or negative influence of auxiliary tasks on the target task.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical justification, the limited experimental results, and the similarity to existing methods. Reviewer 1 suggests that the paper needs more theoretical analysis and justification, and that the experimental results are not convincing enough to justify the proposed method. Reviewer 2 questions the novelty of the method and suggests that it is similar to existing meta-learning algorithms. Reviewer 3 is not convinced by the proposed algorithm and suggests that it is not superior to the baselines.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and justifications for their method. They acknowledge the lack of theoretical justification and suggest that they will remedy this in the camera-ready version. They also provide additional experimental results and comparisons to existing methods.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has some merits, but it also has some significant weaknesses. The proposed method is novel and has some potential, but it lacks theoretical justification and the experimental results are limited. The authors have not convincingly demonstrated the superiority of their method over existing approaches.\n\nFurthermore, the paper has some presentation issues, such as the lack of clarity in the explanation of the algorithm and the use of unclear notation. The authors have not provided sufficient details about the experimental setup and the hyperparameter tuning.\n\nIn conclusion, while the paper has some potential, it needs significant improvements in terms of theoretical justification, experimental results, and presentation. Therefore, I recommend rejecting the paper in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Cortico-cerebellar networks as decoupled neural interfaces\" presents a novel hypothesis that the cerebellum acts as a decoupled neural interface (DNI) to help the cerebral cortex solve the credit assignment problem. The authors propose a framework, cortico-cerebellar-DNI (CC-DNI), which combines the concepts of DNIs and cerebellar forward and inverse models to facilitate learning in neural networks.\n\nThe paper has received mixed reviews from the three reviewers. Reviewer 1 provides positive feedback, suggesting that the paper presents a compelling and novel explanation of cerebellar deficits, but also raises several concerns regarding clarity, details, and comparisons with existing work. Reviewer 2 is more critical, stating that the paper does not offer new insights into either the cerebellum or DNIs, and that the authors simply juxtapose existing knowledge without adding significant value. Reviewer 3 provides a balanced review, acknowledging the paper's potential but also highlighting the need for more comparisons with experimental findings and concrete testable predictions.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and suggestions raised. They have added new figures, a new section, and extended the discussion to provide more predictions and comparisons with experimental findings. The authors have also clarified the use of bootstrapping in their model and highlighted the potential benefits of their framework.\n\nAfter carefully analyzing the paper, reviews, and author responses, I believe that the paper has potential but requires further refinement. The authors have made a significant effort to address the reviewers' concerns, but some issues remain. For example, Reviewer 2's concern about the lack of new insights into the cerebellum or DNIs is still partially valid, as the paper's contributions are largely based on combining existing concepts. Additionally, the paper could benefit from more rigorous comparisons with experimental findings and a clearer explanation of the implications of the CC-DNI model.\n\nHowever, the paper's novelty, potential impact, and the authors' willingness to revise and improve their work are notable. The added predictions, comparisons, and clarifications have strengthened the paper, and the authors have demonstrated a good understanding of the reviewers' concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Structure and randomness in planning and reinforcement learning\" presents a novel method called Shoot Tree Search (STS) that combines elements of Monte Carlo Tree Search (MCTS) and random shooting to balance the trade-off between depth and breadth in planning. The authors claim that STS can achieve better performance than standard MCTS and other baseline methods in challenging domains such as Sokoban and Google Research Football.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. The main strengths include:\n\n1. The paper introduces a simple yet interesting idea that combines MCTS and random shooting.\n2. The authors provide thorough empirical evaluations and interesting results that demonstrate the effectiveness of STS.\n3. The paper is well-written and easy to understand.\n\nHowever, the reviewers also raise several concerns and weaknesses, including:\n\n1. The technical novelty of the paper is questioned, as similar ideas have been explored in previous works.\n2. The comparison with related work is not thorough, and some relevant references are missing.\n3. The experimental setup and results are not fully clear, and some questions are raised about the fairness of the comparison.\n4. The paper lacks intuition and discussion about the advantages of STS over other methods.\n5. Some minor points, such as unclear notation and missing references, are also mentioned.\n\nThe author response addresses some of the concerns and provides additional experimental results to support the claims made in the paper. The authors clarify the differences between their method and previous works, provide more details about the experimental setup, and offer some insights into the advantages of STS.\n\nAfter carefully analyzing the paper, reviewer comments, and author response, I conclude that the paper has some merits, but the concerns and weaknesses raised by the reviewers are significant. While the authors have made an effort to address some of the concerns, the paper still lacks clarity and thoroughness in some areas.\n\nThe main concerns that remain are:\n\n1. The technical novelty of the paper is still questionable, and the authors could have done a better job of discussing the relationships with previous works.\n2. The comparison with related work is still not thorough, and some relevant references are missing.\n3. The experimental setup and results could be improved to provide a clearer and more fair comparison.\n\nGiven these concerns, I believe that the paper requires significant revisions to address the weaknesses and concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"GG-GAN: A Geometric Graph Generative Adversarial Network\" proposes a novel approach to graph generation using a Wasserstein GAN. The authors claim that their method addresses several key challenges in graph generation, including modeling complex relations, modeling isomorphic graphs consistently, and fully exploiting the latent distribution.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. **Limited experimental evaluation**: Reviewer 1 and Reviewer 5 point out that the experimental evaluation is limited to small graphs (with 9 or 20 vertices) and that the results may not generalize to larger graphs. Reviewer 1 also notes that the datasets used in the experiments may contain a large portion of isomorphic graphs, which could affect the performance of the model.\n2. **Lack of comparison to state-of-the-art methods**: Reviewer 1 and Reviewer 4 note that the paper does not provide a comprehensive comparison to state-of-the-art methods, such as NetGAN and TagGen. Reviewer 4 also points out that the paper claims to be faster than autoregressive models, but does not provide a thorough comparison.\n3. **Mode collapse and generation diversity**: Reviewer 2 raises concerns about the potential for mode collapse and limited generation diversity in the proposed method.\n4. **Theoretical analysis**: Reviewer 2 and Reviewer 5 point out that the theoretical analysis in the paper is not well-connected to the proposed method and that some of the theorems may not be informative.\n5. **Literature review**: Reviewer 4 notes that the paper does not provide a comprehensive literature review, particularly with regards to the connection to previous work on graph generation.\n\nDespite these concerns, the reviewers also praise the paper for its novelty, clarity, and potential impact. Reviewer 3 notes that the proposed method is inspiring and has sufficient theoretical support. Reviewer 5 praises the paper for being well-written and didactic.\n\nAfter carefully considering the reviews and the paper, I believe that the concerns raised by the reviewers are significant and need to be addressed. The limited experimental evaluation, lack of comparison to state-of-the-art methods, and potential for mode collapse and limited generation diversity are all major concerns that need to be addressed. Additionally, the theoretical analysis and literature review need to be improved to provide a more comprehensive and well-connected presentation of the proposed method.\n\nTherefore, I conclude that the paper should be rejected for publication in its current form. However, I believe that the authors have a promising idea and that with significant revisions to address the concerns raised by the reviewers, the paper could be resubmitted and potentially accepted for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Ecological Inference\" proposes a novel approach to ecological inference, a problem of significant importance in fields such as political science, by leveraging deep learning techniques. The authors introduce an efficient approximation to the loss function for ecological inference, enabling the construction of ecological versions of linear models, deep neural networks, and Bayesian neural networks. They demonstrate the effectiveness of their approach using real-world data from the Maryland 2018 midterm elections, showcasing improvements in accuracy and the recovery of covariance structures compared to benchmark data.\n\nHowever, upon careful analysis of the reviewer comments, several concerns arise that impact the paper's suitability for publication in a top-tier conference:\n\n1. **Clarity and Readability**: Multiple reviewers (Reviewer 1, Reviewer 2, and Reviewer 4) mention that the paper is not easy to follow, with suggestions for improving the structure, particularly in the introduction of key concepts and the description of input data and evaluation tasks. This lack of clarity hampers the understanding and assessment of the paper's contributions.\n\n2. **Experimental Analysis and Baselines**: Reviewer 2 criticizes the experimental analysis for being incomplete and unclear, lacking a comparison to a standard multi-level model (MLM) as a baseline. This omission makes it difficult to assess the significance of the improvements claimed by the proposed deep MLM. Furthermore, the reviewer points out the absence of detailed analysis that could interpret the results, such as why the deep multi-level model leads to more reasonable estimates of partisan crossover voting.\n\n3. **Reproducibility**: Reviewer 2 also highlights the lack of experimental details necessary for reproducibility, including how train/test splits are created, the specifics of neural network architectures (e.g., number of hidden units, nonlinearities), and the latent features used in few-shot learning experiments.\n\n4. **Typos and Minor Errors**: Several reviewers note the presence of typos and minor writing issues throughout the paper, which, while not critical, contribute to the overall impression of a manuscript that requires further refinement.\n\n5. **Novelty and Related Work**: Reviewer 4 questions the novelty of the proposed approach, suggesting that it may not significantly advance the state-of-the-art. Additionally, there is a concern about the adequacy of related work citation, which is crucial for situating the paper's contributions within the broader context of ecological inference and related machine learning problems.\n\n6. **Experimental Settings and Hyperparameters**: Reviewer 4 emphasizes the need for clearer specification of experimental settings, including few-shot settings and hyperparameters like optimizer selection and learning rate, to ensure reproducibility and facilitate a thorough evaluation of the method's performance.\n\nDespite the interesting application of deep learning to ecological inference and the potential impact of this work, the cumulative effect of these concerns suggests that the paper, in its current form, may not meet the standards expected of a top-tier conference publication. The issues related to clarity, completeness of experimental analysis, reproducibility, and novelty are significant enough to warrant further revision and refinement before the paper could be considered ready for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical insights from few-shot learning and the practical performance of meta-learning algorithms. The authors propose two regularization terms based on recent theoretical advances in meta-learning and demonstrate their effectiveness in improving the generalization capacity of popular meta-learning algorithms on few-shot classification benchmarks.\n\nThe reviewer comments highlight several concerns, including the novelty of the proposed regularization terms, the significance of the improvement over vanilla meta-learning, and the consistency of the theoretical setting with the few-shot learning setting. Reviewer 1 argues that the regularization terms are similar to existing methods, such as spectral normalization and weight decay, and that the improvement is not significant. Reviewer 2 questions the consistency of the theoretical setting with the few-shot learning setting and suggests that the authors should provide more discussions on the differences between the two settings. Reviewer 3 raises concerns about the efficacy of the proposed methods and suggests that the authors should conduct synthetic experiments to demonstrate the effectiveness of the regularization terms. Reviewer 4 is skeptical about the novelty of the second regularization term and suggests that the authors should provide more comparisons with recent methods.\n\nThe author response addresses these concerns by providing additional explanations, experiments, and justifications for the proposed regularization terms. The authors argue that their regularization terms are fundamentally different from existing methods, such as spectral normalization and weight decay, and that the improvement is statistically significant. They also provide additional experiments and comparisons with recent methods to demonstrate the effectiveness of their approach.\n\nAfter carefully analyzing the paper, reviewer comments, and author response, I conclude that the paper has made a significant contribution to the field of meta-learning by providing a theoretical justification for the use of regularization terms in meta-learning algorithms. The authors have demonstrated the effectiveness of their approach on several few-shot classification benchmarks and have provided additional experiments and comparisons to address the concerns raised by the reviewers.\n\nHowever, I also note that the paper could be improved by providing more discussions on the consistency of the theoretical setting with the few-shot learning setting and by conducting synthetic experiments to demonstrate the effectiveness of the regularization terms. Additionally, the authors could provide more comparisons with recent methods to further demonstrate the significance of their contribution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to training deep neural networks using a block minifloat representation, which combines the benefits of minifloats and block floating-point representations. The authors demonstrate the effectiveness of their approach through extensive experiments on various deep learning models and datasets, including ImageNet.\n\nThe paper has several strengths:\n\n1. **Novel contribution**: The paper introduces a new representation, Block Minifloat (BM), which is a significant contribution to the field of deep learning.\n2. **Extensive experiments**: The authors conduct thorough experiments to evaluate the performance of BM on various models and datasets, including ImageNet.\n3. **Hardware evaluation**: The paper provides a detailed hardware evaluation of BM, including area and power consumption, which is essential for understanding the practical implications of the proposed approach.\n\nHowever, the paper also has some weaknesses:\n\n1. **Lack of clarity**: Some reviewers found the paper to be unclear or confusing, particularly with regards to the notation and explanation of certain concepts, such as Equation 6.\n2. **Limited comparison**: The paper could benefit from a more comprehensive comparison with other existing methods, such as Flexpoint.\n3. **Minor issues**: There are some minor issues, such as the lack of definition for certain columns in Table 2 and the unclear notation in Equation 1.\n\nThe authors have addressed many of the reviewers' concerns in their response, providing additional explanations and clarifications. They have also revised the paper to improve clarity and address minor issues.\n\nAfter carefully considering the strengths and weaknesses of the paper, I believe that the paper makes a significant contribution to the field of deep learning and has the potential to be a valuable resource for researchers and practitioners.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning\" presents a novel approach to combining meta-reinforcement learning (meta-RL) and imitation learning (IL) to enable efficient adaptation to new tasks. The method, called PERIL, leverages demonstrations to precondition exploration policies and improve adaptation rates in unseen tasks.\n\nThe reviewers have provided detailed comments and critiques of the paper, highlighting both strengths and weaknesses. The main strengths of the paper include:\n\n1. **Addressing an important problem**: The paper tackles the challenge of quickly learning new tasks, which is a crucial aspect of real-world RL applications.\n2. **Encouraging experimental results**: The authors demonstrate the effectiveness of PERIL in outperforming baselines and generalizing to a wide distribution of 2D tasks.\n3. **Combining meta-RL and IL**: The paper explores a promising direction of combining meta-RL and IL to improve sample efficiency and adaptation rates.\n\nHowever, the reviewers have also raised several concerns and weaknesses, including:\n\n1. **Lack of clarity**: The paper's notation, terminology, and problem statement are often unclear, making it challenging to understand the approach and its contributions.\n2. **Strong assumptions**: The paper assumes the availability of an expert distribution over trajectories conditioned on the learned latent z, which may be a restrictive assumption.\n3. **Insufficient related work**: The paper lacks appropriate citations and discussion of related work, particularly in the areas of meta-IL, posterior sampling, and exploration in meta-RL.\n4. **Methodological concerns**: The reviewers have raised questions about the optimization of the variational lower bound, the definition of the task-dependent objective, and the use of undefined loss functions.\n5. **Experimental limitations**: The paper's experimental evaluation is limited, and the reviewers have suggested additional experiments and comparisons to strengthen the results.\n\nAfter carefully considering the reviewers' comments and the paper's strengths and weaknesses, I conclude that the paper requires significant revisions to address the concerns and improve its clarity, completeness, and overall quality.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel framework for automatic music accompaniment using generative adversarial networks (GANs) and Mel-spectrograms. The authors claim that their approach can generate high-quality accompaniments for bass and voice lines. However, the reviewers have raised several concerns and criticisms about the paper.\n\nOne of the main concerns is that the title and abstract are misleading, as the paper only focuses on automatic accompaniment, not music production in general. The reviewers also point out that the authors claim to be the first to treat music audio as images, but this is not entirely accurate, as there are many existing papers that use spectrograms as images for music processing tasks.\n\nThe reviewers also criticize the lack of details about the Demucs algorithm used for source separation, the CycleGAN architecture, and the training procedure. They also question the choice of evaluation metrics and the subjective nature of the evaluation process.\n\nAdditionally, the reviewers point out that the paper lacks depth in its analysis and discussion of the results, and that the authors do not provide enough context about the relevance of the selected experimental settings to music production.\n\nThe authors have responded to these criticisms and provided additional details and clarifications. However, it is unclear whether these responses fully address the concerns raised by the reviewers.\n\nDespite these criticisms, the paper does propose a novel approach to automatic music accompaniment, and the results seem promising. The use of Mel-spectrograms and GANs is an interesting combination, and the authors' attempt to evaluate their system using human listeners and objective metrics is commendable.\n\nHowever, after careful consideration, I believe that the paper requires significant revisions to address the concerns raised by the reviewers. The authors need to provide more details about their methodology, clarify their claims, and provide a more in-depth analysis and discussion of their results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel algorithm for pruning deep ReLU neural networks based on the theory of implicit and explicit regularization. The algorithm aims to reduce the number of neurons in a trained network while preserving the learned function. The paper presents two experiments that demonstrate the efficiency of the algorithm in reducing the number of neurons with minimal loss in accuracy.\n\nHowever, the paper has several major issues that need to be addressed. Firstly, the presentation is unclear and lacks detail, making it difficult for reviewers to understand the methodology and theory behind the algorithm. The paper relies heavily on an unpublished work, which is not available for review, and this makes it challenging to verify the claims made in the paper.\n\nSecondly, the experimental evaluation is insufficient. The paper only presents results on simple toy datasets and MNIST, which is not sufficient to demonstrate the effectiveness of the algorithm on more complex datasets. The paper also lacks comparison with state-of-the-art pruning techniques, which makes it difficult to assess the novelty and significance of the proposed algorithm.\n\nThirdly, the paper has several technical issues, including typos, grammatical errors, and inconsistent notation. The paper also lacks a clear explanation of the theory behind the algorithm, and the connection between the theory and the algorithm is not well-established.\n\nThe reviewers have raised several concerns and questions, including the lack of motivation for the proposed technique, insufficient experimental evaluation, and unclear presentation. The authors have responded to these concerns, acknowledging the issues and promising to improve the presentation, provide more experiments, and clarify the theory behind the algorithm.\n\nHowever, despite the authors' responses, the paper still has significant flaws that need to be addressed. The paper lacks a clear and concise presentation of the methodology and theory, and the experimental evaluation is still insufficient. The paper also lacks a clear comparison with state-of-the-art pruning techniques, which makes it difficult to assess the novelty and significance of the proposed algorithm.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel algorithm for reducing the number of neurons in a deep ReLU network based on the theory of implicit and explicit regularization. The algorithm consists of three steps: removing dead ReLU neurons, combining neurons that are always active, and using weighted k-means to combine other neurons. The authors claim that their method can reduce the number of neurons by 90% to 99% without introducing sparsity and without significant loss in accuracy.\n\nHowever, the paper has several major issues that need to be addressed. Firstly, the presentation is unclear and lacks detail, making it difficult for reviewers to understand the methodology and theoretical foundations of the algorithm. The authors rely heavily on an unpublished work, which is not available for review, and this makes it challenging to verify the correctness of the proposed technique.\n\nSecondly, the experimental evaluation is insufficient. The authors only conduct experiments on simple toy datasets and MNIST, which is not enough to demonstrate the effectiveness of the algorithm on more complex datasets. The comparison to state-of-the-art pruning techniques is also lacking, and the authors do not provide a clear motivation for their approach.\n\nThirdly, the paper contains several typos, grammatical errors, and inconsistencies in notation, which further complicates the understanding of the paper. The authors' response to the reviewers' comments acknowledges these issues and provides some clarifications, but it is still unclear whether the proposed algorithm is novel, effective, and theoretically sound.\n\nConsidering these issues, the majority of the reviewers recommend rejecting the paper. While the authors' response provides some additional insights into their approach, it is still unclear whether the paper meets the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a novel exploration strategy for continuous reinforcement learning, called Max-Q Entropy Search (MQES). The method aims to efficiently explore the environment by maximizing the information about the globally optimal Q-function distribution. The authors provide a theoretical framework for MQES and demonstrate its effectiveness in various Mujoco tasks.\n\nThe reviewers have raised several concerns and suggestions for improvement, which the authors have addressed in their response. The main issues raised by the reviewers include:\n\n1. Lack of clarity and notation inconsistencies: The authors have rewritten Section 4.1 to improve clarity and consistency.\n2. Limited baseline comparisons: The authors have added comparisons with OAC and conducted ablation studies to demonstrate the effectiveness of MQES.\n3. Unclear impact of hyperparameters: The authors have conducted ablation studies to investigate the impact of hyperparameters on MQES performance.\n4. Limited discussion of empirical results: The authors have provided more detailed discussions of the empirical results, including the performance of MQES in different tasks and environments.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has addressed the major concerns and has the potential to be a valuable contribution to the field of reinforcement learning.\n\nThe strengths of the paper include:\n\n1. Novelty of the approach: MQES is a new and interesting exploration strategy that combines the principles of optimism and information-theoretic exploration.\n2. Theoretical framework: The authors provide a clear and well-motivated theoretical framework for MQES, which is a significant contribution to the field.\n3. Empirical results: The authors demonstrate the effectiveness of MQES in various Mujoco tasks, including sparse-reward environments.\n\nThe weaknesses of the paper include:\n\n1. Clarity and notation: Although the authors have improved the clarity and consistency of the notation, some reviewers still found the paper difficult to follow.\n2. Limited comparisons: Although the authors have added comparisons with OAC, some reviewers felt that more comprehensive comparisons with other state-of-the-art methods would be beneficial.\n\nOverall, I believe that the paper has addressed the major concerns and has the potential to be a valuable contribution to the field of reinforcement learning.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a simple post-hoc calibration method for deep image classifiers on out-of-distribution (OOD) data. The method uses outlier exposure to properly calibrate the model probabilities. The authors claim that their method significantly improves on benchmark results on a wide range of corrupted data.\n\nThe reviewers have raised several concerns about the paper. Reviewer 1 argues that the approach is fundamentally flawed because it assumes that the novel image comes from one of a small set of possible distributions, which is not a realistic assumption. Reviewer 1 also points out that the method is simplistic and relies on extra information that is not available in practice. Reviewer 2 finds the method to be simple and effective but raises questions about the motivation for the multiple image method and the choice of contrast corruption. Reviewer 3 finds the method to be fast to compute and effective but raises concerns about the clarity of the presentation and the lack of evaluation on more realistic OOD cases. Reviewer 4 finds the method to be interesting and effective but raises questions about the choice of corruption used in the calibration set and the evaluation on completely OOD data.\n\nThe authors have responded to the reviewers' comments and have provided additional explanations and experiments to address the concerns. They have clarified that their method is designed for calibration under distribution shift, not unsupervised domain adaptation, and that they do not assume knowledge of the novel distribution. They have also provided additional results on completely OOD data and have evaluated their method on more realistic OOD cases.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field of uncertainty estimation for deep image classifiers. The authors have proposed a simple and effective method for calibrating model probabilities on OOD data, and they have provided extensive experiments to demonstrate the effectiveness of their method. While the reviewers have raised some concerns about the paper, the authors have addressed these concerns in their response and have provided additional explanations and experiments to clarify their method.\n\nHowever, I also believe that the paper could be improved with some revisions to address the reviewers' comments. The authors could provide more clarity on the presentation of their method, and they could evaluate their method on more realistic OOD cases. They could also provide more discussion on the limitations of their method and the potential applications of their method in practice.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field, and I recommend that it be accepted for publication after some revisions to address the reviewers' comments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Training independent subnetworks for robust prediction\" presents a novel approach to improving the robustness and uncertainty estimation of neural networks by training multiple independent subnetworks within a single network using a multi-input multi-output (MIMO) configuration. The authors demonstrate that this approach can achieve state-of-the-art results on several benchmarks, including CIFAR10, CIFAR100, and ImageNet, with a negligible increase in parameters and computational cost.\n\nThe reviewers have provided detailed and thoughtful feedback, highlighting both the strengths and weaknesses of the paper. The main strengths of the paper include:\n\n1. **Novelty and originality**: The paper presents a new and innovative approach to training independent subnetworks within a single network, which has the potential to improve the robustness and uncertainty estimation of neural networks.\n2. **Empirical results**: The authors provide extensive empirical results demonstrating the effectiveness of their approach on several benchmarks, including CIFAR10, CIFAR100, and ImageNet.\n3. **Efficiency**: The MIMO approach allows for efficient ensemble methods, reducing the computational cost and inference delay compared to traditional ensemble methods.\n\nHowever, the reviewers have also raised several concerns and weaknesses, including:\n\n1. **Lack of theoretical analysis**: The paper lacks a detailed theoretical analysis of the MIMO approach, which makes it difficult to understand why the subnetworks behave independently.\n2. **Limited evaluation**: The paper only evaluates the MIMO approach on a limited set of benchmarks and datasets, which may not be representative of all possible scenarios.\n3. **Comparison to other methods**: The paper could benefit from a more detailed comparison to other ensemble methods and approaches, including BatchEnsemble and other multi-branch architectures.\n4. **Clarity and notation**: Some reviewers have raised concerns about the clarity and notation used in the paper, which could be improved to make the paper more accessible to readers.\n\nIn response to the reviewer comments, the authors have provided a detailed and thoughtful response, addressing many of the concerns and weaknesses raised by the reviewers. The authors have:\n\n1. **Clarified the source of independence**: The authors have provided a clear explanation of why the subnetworks behave independently, which is due to the fact that the subnetworks do not share features.\n2. **Provided additional empirical results**: The authors have provided additional empirical results, including results on the regression case, which demonstrate the effectiveness of the MIMO approach.\n3. **Addressed concerns about computational cost**: The authors have addressed concerns about the computational cost of the MIMO approach, demonstrating that it is equivalent to a standard deep neural network at test time.\n\nOverall, based on the detailed analysis of the paper and the reviewer comments, I believe that the paper has the potential to make a significant contribution to the field of neural networks and ensemble methods. While there are some weaknesses and concerns, the authors have provided a thoughtful and detailed response to the reviewer comments, addressing many of the concerns and weaknesses raised.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" proposes a novel approach to learning layout representations for graphic design using a Transformer-based model and a large-scale dataset of PowerPoint slides. The paper has several strengths, including the construction of a large-scale dataset, the proposal of two novel layout understanding tasks, and the achievement of state-of-the-art performances on these tasks.\n\nHowever, the paper also has several weaknesses, including a lack of detailed evaluation, a lack of comparison to previous works, and a lack of clarity in the writing. The reviewers have pointed out several issues, including the need for more formal evaluation methods, the need for comparison to other lower complexity methods and existing works, and the need for more details about the dataset and the model.\n\nThe authors have responded to the reviewers' comments and have agreed to improve the evaluation, add more baselines, and refine the writing. They have also provided more details about the dataset and the model.\n\nOverall, the paper has potential, but it needs significant improvements to meet the standards of a top-tier conference. The authors have shown a willingness to address the reviewers' concerns, and with revisions, the paper could be stronger.\n\nThe main concerns that need to be addressed are:\n\n1. Evaluation: The paper needs more formal evaluation methods, including comparison to previous works and other lower complexity methods.\n2. Comparison to previous works: The paper needs to compare the proposed approach to other existing works, including those related to layout generation.\n3. Clarity: The writing needs to be refined to make it clearer and more concise.\n4. Dataset: More details about the dataset need to be provided, including the average number of elements in each slide and the parsing tool used.\n\nIf the authors can address these concerns and provide a revised version that meets the standards of a top-tier conference, the paper could be accepted. However, based on the current version, the paper needs significant revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Global Node Attentions via Adaptive Spectral Filters\" presents a novel approach to graph neural networks (GNNs) by introducing a global self-attention mechanism defined using learnable spectral filters. This allows the model to attend to any nodes, regardless of distance, which is particularly useful for disassortative graphs where local homophily does not hold.\n\nThe paper is well-motivated, and the proposed method is intuitive and well-explained. The authors provide a thorough empirical evaluation on six benchmark datasets, demonstrating that their model generalizes well to both assortative and disassortative graphs and outperforms state-of-the-art baselines on disassortative graphs.\n\nHowever, the reviewers raise several concerns that need to be addressed. Reviewer 1 questions the novelty of the paper, arguing that ChevNet can also capture far-away nodes and that the failure of ChevNet is due to over-overparameterization and overfitting. The authors respond by explaining that their method uses adaptive filters via multi-head attention, which allows it to learn appropriate filters for each node, whereas ChevNet uses the same polynomial filter over the entire network.\n\nReviewer 1 also raises concerns about the complexity of the model, arguing that the authors' claim of linear complexity is incorrect. The authors respond by explaining that they use Chebyshev polynomials to approximate the graph wavelet transform, which reduces the complexity to O(p x |E|), where p is the order of the Chebyshev polynomial and |E| is the number of edges.\n\nReviewer 3 suggests that the authors should provide more experiments on disassortative graphs and synthetic graphs with controllable β. The authors respond by adding experiments on the Chameleon dataset and promising to conduct further experiments on synthetic graphs.\n\nReviewer 4 raises concerns about the novelty and complexity of the paper, arguing that the idea of using attention to capture features from far-away nodes is not new and that the model's complexity is high. The authors respond by explaining that their method is novel in its use of adaptive spectral filters and that the complexity is reduced by using Chebyshev polynomials.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has addressed the major concerns raised by the reviewers. The authors have provided additional experiments and explanations to clarify the novelty and complexity of their method.\n\nThe paper's strengths include its well-motivated approach, thorough empirical evaluation, and clear explanations. The authors have also demonstrated a willingness to engage with the reviewers' comments and address their concerns.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents a novel approach to 3D snapshot microscopy by utilizing Fourier convolutional neural networks for end-to-end optimization of optical encoders and deep learning decoders. The authors demonstrate the effectiveness of their approach through simulations and comparisons with existing state-of-the-art methods, including UNet architectures.\n\nThe strengths of the paper include:\n\n1. **Novel Application of Fourier Convolutional Networks**: The authors apply Fourier convolutional networks to the problem of 3D snapshot microscopy, demonstrating their potential in handling highly non-local optical encoders.\n2. **End-to-End Optimization**: The paper shows that Fourier convolutional networks can be used for end-to-end optimization of optical encoders and deep learning decoders, which is a significant contribution to the field of computational microscopy.\n3. **Comparisons with State-of-the-Art Methods**: The authors provide thorough comparisons with existing state-of-the-art methods, including UNet architectures, demonstrating the superiority of their approach in terms of reconstruction quality and efficiency.\n\nHowever, the paper also has some weaknesses and concerns raised by the reviewers, including:\n\n1. **Lack of Technical Novelty**: Some reviewers argue that the technical ideas presented in the paper are not entirely new, as Fourier convolutional networks have been explored in previous works (e.g., Rippel et al., 2015).\n2. **Limited Evaluation**: The paper's evaluation is primarily based on simulated data, which may not fully capture the complexities of real-world experiments. Some reviewers express concerns about the lack of experimental validation and the potential gap between simulation and reality.\n3. **Comparison with UNet**: The comparison with UNet architectures raises some questions, as the UNet results seem to be significantly worse than expected, even with a global receptive field.\n\nThe authors' response to the reviewers' comments addresses some of these concerns, providing additional context and clarifications. They acknowledge the existence of previous works on Fourier convolutional networks and emphasize the novelty of their application to 3D snapshot microscopy. They also provide more details on the simulation-based optimization and the potential for experimental validation.\n\nAfter carefully considering the paper's strengths and weaknesses, as well as the authors' response to the reviewers' comments, I conclude that the paper makes a significant contribution to the field of computational microscopy and demonstrates the potential of Fourier convolutional networks in handling highly non-local optical encoders.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical analysis of the Feedback Alignment (FA) algorithm, an alternative to backpropagation for training neural networks. The authors provide convergence guarantees with rates for deep linear networks for both continuous and discrete dynamics, and study incremental learning phenomena for shallow linear networks. They also identify an implicit anti-regularization phenomenon for certain initializations and propose initialization schemes that provide a form of implicit regularization.\n\nThe reviewer comments are mixed, with some reviewers praising the paper's clarity, technical novelty, and significance, while others raise concerns about the paper's limitations, such as the assumption of spectral initialization, the focus on linear networks, and the lack of comparison with Gradient Descent (GD).\n\nReviewer 1 and Reviewer 2 express concerns about the paper's significance and novelty, citing the restrictive assumption of spectral initialization and the focus on linear networks. Reviewer 3, on the other hand, finds the paper to be a nice theoretical development, extending the theory of linear neural networks to the case of FA, and recommends acceptance. Reviewer 4 raises concerns about the significance of the results, the lack of connections to initialization schemes used in practice, and the need for more experiments under non-spectral initialization.\n\nThe authors respond to the reviewer comments, addressing the concerns and providing additional explanations and justifications for their work. They clarify the assumption of spectral initialization, provide additional experiments under non-spectral initialization, and discuss the implications of their results for non-linear networks.\n\nAfter carefully considering the reviewer comments and the author responses, I believe that the paper makes a significant contribution to the understanding of the FA algorithm and its convergence properties. While the paper has some limitations, such as the focus on linear networks and the assumption of spectral initialization, the authors provide a rigorous theoretical analysis and address some of the concerns raised by the reviewers.\n\nThe paper's technical novelty and significance are evident in its contribution to the understanding of the FA algorithm and its convergence properties. The authors provide a clear and well-structured presentation of their results, and their responses to the reviewer comments demonstrate a thorough understanding of the subject matter.\n\nIn conclusion, while the paper has some limitations, its technical novelty, significance, and clarity make it a valuable contribution to the field. Therefore, I recommend acceptance of the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Boosting Approach to Reinforcement Learning\" presents a novel approach to reinforcement learning by applying boosting techniques to convert weak learners into effective policies. The authors provide a theoretical analysis of their algorithm, including sample complexity and running time bounds that are polynomial in the natural parameters of the problem, but do not explicitly depend on the number of states.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novel Approach**: The application of boosting to reinforcement learning is considered interesting and novel by several reviewers. This approach has the potential to contribute to both the reinforcement learning and optimization communities.\n2. **Theoretical Analysis**: The paper provides a solid theoretical analysis, including bounds on sample complexity and running time. These bounds are significant because they do not depend on the number of states, making the algorithm potentially competitive in large state-space MDPs.\n3. **Technical Significance**: Some reviewers find the technical results and the adaptation of the Frank-Wolfe method to non-convex settings to be of independent interest and significant.\n\nHowever, the paper also has several weaknesses:\n\n1. **Clarity and Readability**: Multiple reviewers criticize the paper for being extremely dense, hard to read, and lacking in clarity. The notation is heavy, and many terms are not well-explained or are introduced without sufficient motivation.\n2. **Lack of Intuition and Examples**: Reviewers suggest that the paper would benefit from more intuitive explanations of the algorithm's steps and the inclusion of numerical examples to help understand the practical implications of the method.\n3. **Comparison with Prior Work**: Some reviewers question the novelty of the approach, comparing it to existing work like Hazan and Singh 2021, and suggest that the paper could better highlight its unique contributions and differences.\n4. **Practicality and Empirical Evaluation**: The lack of empirical results or discussions on the practical implementation of the algorithm is noted by several reviewers, which could limit the paper's impact and readability for some audiences.\n\nThe authors' response addresses some of these concerns, promising to clarify notation, add more intuitive explanations, and provide missing citations. However, the response does not fully address the need for empirical evaluations or detailed comparisons with prior work, which are crucial for a comprehensive understanding of the algorithm's significance and applicability.\n\nGiven the information provided, the decision to accept or reject the paper hinges on the balance between its theoretical contributions and its shortcomings in terms of clarity, practicality, and empirical validation. While the paper presents a novel and theoretically significant approach to reinforcement learning, its lack of clarity and the absence of empirical results are significant drawbacks.\n\nConsidering the standards of a top-tier conference, which typically require not only technical soundness but also clarity, impact, and completeness, the paper falls short in several areas. However, the novelty and potential impact of the approach are undeniable, suggesting that with revisions to address the clarity, practicality, and empirical evaluation concerns, the paper could be significantly strengthened.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"A Frequency Perspective of Adversarial Robustness\" presents a frequency-based understanding of adversarial examples in deep neural networks, challenging the common misconception that adversarial examples are high-frequency noise. The authors provide theoretical and empirical findings to support their claims, including an analysis of the frequency properties of adversarial examples and the impact of frequency constraints on adversarial training.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers 1, 3, and 5 raise concerns about the novelty and significance of the contributions, citing existing works that have explored similar ideas. Reviewer 2 questions the authors' claims about the frequency properties of adversarial examples and suggests that the conclusions may not hold under different criteria, such as Fourier transforms. Reviewer 4 finds the insights beneficial for the community but suggests improvements to the presentation and experimentation.\n\nThe author responses address the reviewer comments, providing clarifications and additional explanations to support their claims. The authors acknowledge the similarities with existing works but highlight the differences and nuances of their approach. They also provide additional results and analyses to address the concerns raised by the reviewers.\n\nAfter carefully analyzing the paper, reviewer comments, and author responses, I believe that the paper has made significant contributions to the understanding of adversarial robustness from a frequency perspective. The authors have provided extensive empirical evidence to support their claims, and their analysis has shed new light on the properties of adversarial examples.\n\nWhile the paper may not be entirely novel, as some reviewers have pointed out, it provides a fresh perspective on the topic and has the potential to inspire new research directions. The authors have also demonstrated a willingness to engage with the reviewers' comments and provide additional evidence to support their claims.\n\nHowever, I also acknowledge that the paper has some limitations, such as the focus on natural images and the lack of experimentation with other types of images or network architectures. Additionally, some reviewers have raised concerns about the presentation and clarity of the paper, which could be improved.\n\nOverall, I believe that the paper has made significant contributions to the field and has the potential to be a valuable addition to the conference program.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" presents an interesting and relevant problem in the field of multi-modal learning. The authors propose a hypothesis that multi-modal deep neural networks tend to rely on one modality while under-utilizing the other modalities, and they validate this hypothesis through empirical experiments. They also propose a training algorithm, balanced multi-modal learning, to address this issue and demonstrate its effectiveness on three datasets.\n\nThe reviewer comments provide a mixed assessment of the paper. Some reviewers (e.g., Reviewer 5 and Reviewer 7) find the paper to be well-written, easy to follow, and providing a clear explanation of the problem and the proposed solution. They also appreciate the empirical validation of the hypothesis and the effectiveness of the proposed algorithm. However, other reviewers (e.g., Reviewer 2, Reviewer 4, and Reviewer 6) raise several concerns, including:\n\n1. Lack of theoretical analysis: Several reviewers point out that the paper lacks a solid theoretical analysis of the greedy learning problem and the proposed method.\n2. Insufficient comparison with other methods: Reviewer 6 and Reviewer 7 mention that the paper does not provide a sufficient comparison with other methods that solve the imbalance problem in multi-modal learning.\n3. Limited experimental evaluation: Reviewer 4 and Reviewer 6 note that the experimental evaluation is limited to only three datasets, and the results are not statistically significant in some cases.\n4. Clarity and presentation: Some reviewers (e.g., Reviewer 2 and Reviewer 5) find the paper to be unclear or poorly presented in certain sections.\n\nThe author response addresses some of these concerns, providing additional explanations and justifications for the proposed method and the experimental evaluation. However, the response does not fully address the concerns regarding the lack of theoretical analysis and the limited comparison with other methods.\n\nAfter carefully considering the reviewer comments and the author response, I conclude that the paper has some merits, but it also has several limitations and areas for improvement. The paper presents an interesting problem and a proposed solution, and the empirical validation is a strength of the paper. However, the lack of theoretical analysis and the limited comparison with other methods are significant weaknesses.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"On the Implicit Biases of Architecture & Gradient Descent\" presents a nuanced exploration of how both the architecture of neural networks and the gradient descent optimization algorithm contribute to the generalization performance of these models. The authors provide theoretical insights, including a novel PAC-Bayes bound that accounts for the architecture's influence on generalization, and empirical evidence to support their claims about the role of margin in generalization.\n\nSeveral strengths of the paper are highlighted by the reviewers, including its well-written and clear presentation, the novelty and technical correctness of the theoretical results, and the interesting empirical findings. The discussion on margin and its impact on generalization is particularly insightful, suggesting that large margin functions generalize better, a phenomenon observed in both Gaussian Process (GP) classification and Neural Network (NN) classification.\n\nHowever, the reviewers also raise several concerns and criticisms. A primary issue is the perceived lack of novelty and significance in the contributions, with some reviewers feeling that the results, while interesting, do not substantially advance the state of the art or are not broadly applicable beyond the specific settings explored (e.g., MLPs). The experimental setup is criticized for being limited, with suggestions for more comprehensive evaluations across different datasets, architectures (such as CNNs and ResNets), and optimization methods. The choice of using the \"Nero\" optimizer, a relatively new algorithm, is questioned, with requests for comparisons with more standard optimizers like SGD or Adam.\n\nAdditionally, there are concerns about the clarity and accuracy of the presentation, particularly regarding the discussion of gradient descent's implicit bias and its relation to margin. Some reviewers point out that the concept of implicit bias in gradient descent is not novel and has been explored in other works, suggesting that the paper's contribution in this area may be less significant than claimed.\n\nThe authors' response to the reviews addresses some of these concerns, providing additional context and clarifications. They acknowledge the limitations of their current experimental setup and the potential for future work to expand on their findings. They also clarify the novelty of their approach to understanding margin and its effect on generalization, distinguishing their work from previous studies on implicit bias in gradient descent.\n\nGiven the detailed feedback from the reviewers and the authors' responses, the decision to accept or reject the paper hinges on whether the contributions are deemed significant and novel enough to warrant publication in a top-tier conference. While the paper presents interesting theoretical and empirical results, the concerns about the breadth of applicability, the novelty of the contributions, and the clarity of the presentation are substantial.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a novel analysis of the optimization landscape of two-layer ReLU neural networks. The authors prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. This analysis is significant, as it provides a rich framework for studying the landscape of neural network training loss through convexity.\n\nThe reviewer comments are overwhelmingly positive, with all reviewers praising the paper's technical novelty, significance, and clarity. Reviewer 2 states that this is the best paper they have reviewed for ICLR 2022, and Reviewer 4 recommends a clear accept. The reviewers do raise some minor issues, such as typos, grammar errors, and suggestions for future work, but these do not detract from the paper's overall quality.\n\nThe authors' response to the reviewer comments is thorough and addresses all the raised issues. They provide clear explanations for the typos and errors, and they discuss the potential for extending their results to deep neural networks and other activation functions.\n\nOne potential concern is that the paper's results are limited to two-layer ReLU neural networks, and it is unclear how they can be extended to more complex architectures. However, the authors acknowledge this limitation and discuss potential avenues for future work.\n\nAnother potential concern is that some of the reviewers mention that the paper's contributions are not entirely new, as they build upon previous work by Pilanci and Ergen. However, the authors clearly explain how their work differs from and improves upon previous results, and they provide a detailed analysis of the relationship between their work and existing literature.\n\nOverall, based on the reviewer comments and the authors' response, I believe that the paper makes a significant contribution to the field of neural network optimization and deserves to be published in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the reviewer comments, the paper's technical novelty and significance, empirical novelty and significance, correctness, and clarity.\n\nThe paper proposes a modified version of target propagation (TP) that is easily implementable via autograd systems. It compares the computational complexity of the algorithm with backpropagation (BP) and shows empirical results on recurrent neural networks (RNNs) with long sequences. The main strengths of the paper include:\n\n1. **Novelty of the Approach**: The paper presents a new way of approaching optimization in RNNs based on target propagation, which is an alternative to gradient back-propagation.\n2. **Ease of Implementation**: The proposed algorithm is straightforward to implement within a standard computational graph, making it accessible to a wide range of researchers and practitioners.\n3. **Theoretical Guarantees**: There is a theoretical guarantee available that bounds the difference between target propagation and backpropagation, providing a foundation for understanding the algorithm's behavior.\n\nHowever, the paper also has several weaknesses that were highlighted by the reviewers:\n\n1. **Lack of Theoretical Depth**: Reviewers felt that the theory needs to be more fully explored and discussed, particularly in terms of how target propagation differs from backpropagation and its relationship to Gauss-Newton methods.\n2. **Empirical Evidence**: The empirical evidence presented is limited, with few experiments and runs. Reviewers suggested that more detailed and extensive experimentation is needed to understand when and why target propagation outperforms gradient descent.\n3. **Comparison to Existing Methods**: The paper lacks a comprehensive comparison with other learning algorithms designed to improve learning in RNNs, which is necessary to establish the novelty and significance of the proposed method.\n4. **Minor Issues with Correctness**: Some reviewers noted minor issues with the paper's claims, suggesting that a few statements are not well-supported or require small changes to be made correct.\n\nGiven these points, the decision to accept or reject the paper hinges on whether the strengths outweigh the weaknesses and whether the paper meets the standards of a top-tier conference.\n\nTop-tier conferences typically look for papers that are not only novel and significant but also well-executed, thoroughly validated, and clearly presented. While the paper proposes an interesting and potentially useful algorithm, the lack of depth in theoretical analysis, the limited empirical evidence, and the need for a more comprehensive comparison with existing methods are significant concerns.\n\nWithout an author response to address these concerns, it is challenging to determine whether the authors can adequately address the reviewers' comments and improve the paper to meet the conference's standards.\n\nConsidering the information provided and the standards of a top-tier conference, the paper's weaknesses in terms of theoretical depth, empirical evidence, and comparison to existing methods outweigh its strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform for optimal experimental design, leveraging machine learning and Bayesian optimization to accelerate the discovery of solutions with optimal objective trade-offs. The platform, AutoOED, is designed to be user-friendly, modular, and capable of handling multi-objective optimization problems efficiently.\n\nThe reviewers' comments highlight several strengths and weaknesses of the paper. On the positive side, Reviewer 1 appreciates the benefit of AutoOED and its potential to advance the field of optimal experimental design. Reviewer 5 likes the problem formulation, the clarity of the paper, and the claims being well-supported by experiments. The reviewers also acknowledge the engineering contributions of AutoOED, including its ease of use, modular design, and the introduction of asynchronous multi-objective Bayesian optimization.\n\nHowever, several reviewers express concerns about the paper's scientific contributions, novelty, and empirical evaluations. Reviewer 2 and Reviewer 3 question the novelty of the method, stating that it is more like an assembly of existing software design documents and a manual for an open-source platform. Reviewer 4 points out that the paper's claims are overstated, and the empirical results are not as strong as claimed. Reviewer 4 also raises concerns about the lack of theoretical proof for the Believer-Penalizer (BP) strategy and the limited comparison with other Bayesian optimization packages.\n\nThe authors' response addresses some of these concerns, providing additional explanations and updates to the paper. They clarify the motivations and target audience for AutoOED, highlighting its ease of use and modular design. They also acknowledge the limitations of the platform, including its initial restriction to 2 or 3 objectives, and announce that they have updated the platform to support any number of objectives.\n\nAfter carefully considering the reviewers' comments and the authors' response, I conclude that the paper has some merits, including its potential to advance the field of optimal experimental design and its ease of use. However, the paper's scientific contributions and novelty are limited, and the empirical evaluations could be stronger.\n\nThe final decision is based on the standards of a top-tier conference, which typically require significant scientific contributions, novelty, and rigorous empirical evaluations. While AutoOED is a well-engineered platform, its scientific contributions and novelty are not sufficient to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"The weighted mean trick – optimization strategies for robustness\" presents a novel approach to optimizing the higher-order moments of a loss distribution, which can lead to improved robustness against outliers. The authors provide theoretical results and experimental evidence to support their claims.\n\nHowever, upon careful analysis of the reviewer comments and author responses, several concerns arise:\n\n1. **Lack of clarity and coherence**: Multiple reviewers pointed out that the paper is hard to understand, and the notation is inconsistent. The authors' responses acknowledge these issues and promise to address them in the camera-ready version.\n2. **Theoretical contributions**: Reviewers 4 and 5 question the significance and novelty of the theoretical contributions, stating that the results are straightforward or not well-supported. The authors' responses do not fully address these concerns.\n3. **Algorithmic issues**: Reviewers 2, 3, and 5 raise concerns about the algorithm's correctness, convergence, and relationship to existing methods. The authors' responses provide some clarification but do not fully resolve these issues.\n4. **Experimental results**: While the authors provide some experimental results, Reviewer 7 suggests that the results are not particularly impressive, and the comparison to other robust loss functions is limited.\n\nDespite these concerns, the paper does propose an interesting idea, and some reviewers (e.g., Reviewer 7) find the observation valuable and potentially applicable to other areas.\n\nAfter weighing the pros and cons, I conclude that the paper's limitations and concerns outweigh its potential contributions. While the authors have made an effort to address some of the issues, the paper still requires significant revisions to improve its clarity, coherence, and technical soundness.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach to improving the sampling efficiency of diffusion models, a class of generative models that have shown promising results in image generation tasks. The authors propose Differentiable Diffusion Sampler Search (DDSS), a method that optimizes fast samplers for pre-trained diffusion models by differentiating through sample quality scores. They also introduce Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models.\n\nThe reviewers have raised several concerns and suggestions, including the need for clearer motivation, improved clarity in the exposition, and more thorough theoretical analysis. However, the authors have addressed many of these concerns in their response, providing additional explanations, proofs, and experimental results.\n\nThe reviewers have also praised the paper for its interesting and important problem, novel approach, and impressive empirical results. Reviewer 3 notes that the paper's results are \"quite impressive\" and that the proposed method \"outperforms DDIM and DDSS + DDIM, showing the advantage of DDSS.\" Reviewer 5 states that the paper \"makes a novel contribution towards\" accelerating diffusion models and that the \"empirical results show that DDPMs can be adapted for much faster sampling and better quality retention with our procedure.\"\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has made significant contributions to the field of generative models and has the potential to improve the efficiency and quality of diffusion-based image generation.\n\nThe authors have demonstrated the effectiveness of their approach through extensive experiments, including comparisons with state-of-the-art methods such as DDPM and DDIM. They have also provided a clear and well-motivated explanation of their method, including a detailed analysis of the theoretical foundations and limitations.\n\nWhile some reviewers have raised concerns about the paper's clarity and theoretical analysis, the authors have addressed these concerns in their response and have provided additional explanations and proofs to support their claims.\n\nOverall, I believe that the paper has made a significant contribution to the field and has the potential to improve the efficiency and quality of diffusion-based image generation.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" presents a novel analysis of the behavior of Stochastic Gradient Descent with Momentum (SGDm) under non-iid sampling, specifically covariate shift. The authors identify a resonance phenomenon that can lead to divergence of SGDm and provide theoretical and empirical evidence to support their claims.\n\nThe paper has several strengths:\n\n1. **Novelty**: The paper introduces a new perspective on the behavior of SGDm under non-iid sampling, which is a common scenario in many machine learning applications.\n2. **Theoretical foundations**: The authors provide a rigorous theoretical analysis of the resonance phenomenon, leveraging tools from nonlinear resonance theory and parametric oscillators.\n3. **Empirical evidence**: The paper presents extensive empirical results, including experiments with synthetic and real-world data, to demonstrate the resonance phenomenon and its implications for SGDm.\n4. **Clear writing**: The authors have made a significant effort to present their work in a clear and well-structured manner, making it easier for readers to follow their arguments.\n\nHowever, the paper also has some weaknesses and areas for improvement:\n\n1. **Technical novelty**: Some reviewers have questioned the technical novelty of the paper, arguing that the mathematical techniques used are not particularly new or innovative.\n2. **Limited scope**: The paper focuses primarily on the linear setting with periodic covariate shift, which may not be representative of all real-world scenarios.\n3. **Lack of empirical examples**: Some reviewers have suggested that the paper could benefit from more empirical examples, particularly using real-world datasets, to demonstrate the practical relevance of the resonance phenomenon.\n4. **Connection to practice**: The paper could be improved by providing more guidance on how the resonance phenomenon can be mitigated or avoided in practice, and how the results can be used to improve the design of SGDm algorithms.\n\nAfter carefully considering the reviewer comments and the authors' responses, I believe that the paper has made a significant contribution to the understanding of SGDm under non-iid sampling. While there are some areas for improvement, the paper's strengths outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach to improving the sampling efficiency of diffusion models, a class of generative models that have shown promising results in image synthesis tasks. The authors propose Differentiable Diffusion Sampler Search (DDSS), a method that optimizes fast samplers for pre-trained diffusion models by differentiating through sample quality scores. They also introduce Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models.\n\nThe reviewers have raised several concerns and suggestions, including the need for clearer motivation, improved clarity in the exposition, and more thorough theoretical analysis. However, the authors have addressed many of these concerns in their rebuttal and have provided additional experiments and analysis to support their claims.\n\nThe reviewers have also praised the paper for its interesting and important problem, its novel approach, and its strong empirical results. Reviewer 3 notes that the paper's results are \"quite impressive\" and that the samples shown in Figure 2 are \"strikingly better\" than the baseline approach. Reviewer 5 states that the paper \"makes a novel contribution\" and that the idea is \"relatively simple and straightforward.\"\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has made significant contributions to the field of generative models and has the potential to improve the efficiency and quality of diffusion-based image synthesis.\n\nThe authors have demonstrated the effectiveness of their approach through extensive experiments on several datasets, including CIFAR-10 and ImageNet 64x64. They have also provided a clear and well-motivated explanation of their method, including a detailed analysis of the theoretical foundations of their approach.\n\nWhile there are some minor issues with clarity and notation, the authors have addressed these concerns in their rebuttal and have provided additional explanations and examples to support their claims.\n\nIn conclusion, based on the strengths of the paper, including its novel approach, strong empirical results, and clear explanation, I believe that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents a theoretical analysis of the convergence properties of three classical value estimation algorithms (TD, FVI, and RM) in the overparameterized linear case. The authors provide a unified interpretation of these algorithms as minimizing the Euclidean norm of the weights subject to alternative constraints. They also propose two regularizers to improve the stability of TD and FVI and demonstrate their effectiveness in experiments.\n\nThe reviewer comments are generally positive, with some reviewers praising the paper's novelty, technical soundness, and potential impact on the field. However, some reviewers also raise concerns about the paper's limitations, such as the restricted focus on linear representations, the lack of comparison with underparameterized cases, and the need for more empirical evaluations.\n\nThe authors' response to the reviewer comments is thorough and addresses most of the concerns raised. They provide additional explanations, clarifications, and references to support their claims and acknowledge the limitations of their work.\n\nAfter carefully analyzing the paper and the reviewer comments, I conclude that the paper makes a significant contribution to the understanding of overparameterization in recursive value estimation. The authors' analysis provides new insights into the behavior of TD, FVI, and RM in the overparameterized regime, and their proposed regularizers have the potential to improve the stability of these algorithms.\n\nWhile the paper has some limitations, such as the focus on linear representations and the need for more empirical evaluations, these limitations do not outweigh the paper's contributions. The authors' response to the reviewer comments demonstrates their willingness to engage with the concerns raised and to improve their work.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I recommend accepting the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" proposes a novel architecture for synthetically generating time-series data using Variational Auto-Encoders (VAEs). The authors claim that their approach has several distinct properties, including interpretability, the ability to encode domain knowledge, and reduced training times.\n\nThe reviewer comments provide a comprehensive evaluation of the paper, highlighting both strengths and weaknesses. Reviewer 1 praises the paper as well-written and interesting but criticizes the lack of details on hyper-parameter selection and model selection. They also question the interpretability of the newly introduced building blocks and suggest that the empirical evaluation could be improved. Reviewer 2 argues that the proposed model is not substantially original and does not seem to present any significant novelty. They also point out that the VAE loss function is not well-discussed and that the empirical evaluation could be expanded. Reviewer 3 suggests that the paper lacks general context of the literature and that the technical contribution is unclear. They also criticize the description of the methods and the background on generative models, as well as the experimental methodology. Reviewer 4 doubts the contribution of the proposed method to the machine learning community and points out that there is no discussion about interpretability in the results session.\n\nDespite the criticisms, the reviewers agree that the paper is well-written and that the proposed approach is interesting. However, the lack of novelty, the limited empirical evaluation, and the unclear technical contribution are significant concerns.\n\nConsidering the reviewer comments, the paper has some strengths, such as its well-written presentation and interesting approach. However, the weaknesses, including the lack of novelty, limited empirical evaluation, and unclear technical contribution, outweigh the strengths.\n\nThe author response is not available, which makes it difficult to assess whether the authors have addressed the concerns raised by the reviewers.\n\nIn conclusion, based on the reviewer comments and the paper's content, I believe that the paper requires significant revisions to address the concerns raised by the reviewers. The lack of novelty, limited empirical evaluation, and unclear technical contribution are significant issues that need to be addressed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Sample and Computation Redistribution for Efficient Face Detection\" presents a novel approach to face detection, focusing on optimizing computation distribution and sample augmentation to improve detection accuracy, particularly for small faces in low-resolution images. The authors propose two methods: Computation Redistribution (CR) and Sample Redistribution (SR), which are combined into the Sample and Computation Redistribution for Face Detection (SCRFD) framework. The paper includes extensive experiments on the WIDER FACE dataset, demonstrating state-of-the-art accuracy-efficiency trade-offs.\n\nSeveral strengths of the paper are highlighted by the reviewers, including:\n- The effectiveness of the proposed CR and SR methods in improving detection accuracy, especially for small faces.\n- The simplicity and practicality of the approach, making it suitable for real-world applications such as face detection on mobile devices.\n- The achievement of state-of-the-art performance on the WIDER FACE dataset, outperforming competitors like TinaFace and BFBox.\n\nHowever, reviewers also point out several weaknesses and areas for improvement:\n- The search strategies implemented are considered straightforward and not particularly interesting or novel by some reviewers.\n- There is a lack of comparison with other state-of-the-art network search methods, which is seen as necessary to fully evaluate the contribution of the paper.\n- Some reviewers question the generalizability of the method beyond face detection, suggesting it might be limited to object detection with a wide range of object scales.\n\nThe authors have provided detailed responses to the reviewers' comments, addressing the concerns and suggestions for improvement. They have added comparisons with evolutionary search methods and other state-of-the-art network search approaches, further demonstrating the effectiveness of their proposed method. They have also clarified the novelty and significance of their contribution, highlighting the practicality and insightfulness of their approach for efficient network design.\n\nConsidering the strengths, weaknesses, and the authors' responses to the reviewers' comments, the paper demonstrates a significant contribution to the field of face detection and efficient network design. The experiments are thorough, and the results are compelling, showing state-of-the-art performance. While some reviewers have raised points about novelty and the straightforwardness of the search strategies, the authors have adequately addressed these concerns and provided additional context and comparisons to strengthen their paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning Sample Reweighting for Adversarial Robustness\" proposes a novel approach to improve the robustness of neural network classifiers against adversarial attacks by learning to reweight the loss associated with individual training samples based on a notion of class-conditioned margin. The approach, termed BiLAW, formulates weighted adversarial training as a bilevel optimization problem, where the upper-level task corresponds to learning a robust classifier, and the lower-level task corresponds to learning a parametric function that maps from a sample's multi-class margin to an importance weight.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. On the positive side, the paper is well-written, and the proposed methodology is sound. The experimental results demonstrate improvements in both clean and robust accuracy compared to related techniques and state-of-the-art baselines. Some reviewers appreciate the novelty of applying meta-learning to adversarial training and the potential of the proposed approach to improve robustness.\n\nHowever, several reviewers express concerns regarding the novelty of the approach, given its similarity to existing methods like MAML. They suggest that the paper could benefit from a deeper investigation into the reweighting mechanisms and more extensive ablation studies to decouple the contributions of different components of the proposed approach. Additionally, some reviewers question the significance of the improvements reported in the paper, especially when considering the performance of the method in isolation (without combining it with other techniques like TRADES).\n\nAnother significant concern raised by multiple reviewers is the lack of evaluation against adaptive attacks, which could potentially exploit the learned reweighting. The authors' response addresses this concern to some extent by discussing the feasibility of weight-aware attacks and providing preliminary experimental results. However, the reviewers emphasize the importance of thoroughly evaluating the method against such attacks to ensure its robustness.\n\nConsidering the comments and the authors' responses, the paper has both merits and areas for improvement. The proposed approach is novel in its application to adversarial training, and the experimental results are promising. However, the concerns regarding novelty, the need for more extensive ablation studies, and the evaluation against adaptive attacks are valid and should be addressed.\n\nGiven the current state of the paper and the feedback from the reviewers, it seems that the authors have made a significant effort to address the concerns and provide additional experiments and justifications. While there are still some areas that could be improved, the paper contributes to the field by introducing a new perspective on sample reweighting for adversarial robustness and demonstrating its potential through comprehensive experiments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Meta-Referential Games to Learn Compositional Learning Behaviours\" proposes a novel benchmark to investigate state-of-the-art artificial agents' abilities to exhibit compositional learning behaviors. The authors introduce a Symbolic Continuous Stimulus (SCS) representation and cast the problem of learning compositional behaviors as a meta-reinforcement learning problem.\n\nAfter carefully analyzing the paper and the reviewer comments, several concerns and issues arise that impact the paper's overall quality and contribution to the field. \n\n1. **Clarity and Readability**: Multiple reviewers (Reviewer 1, Reviewer 2, Reviewer 3, and Reviewer 4) mention that the paper is difficult to follow, with long and repetitive sentences, and lacks clear explanations of key concepts such as the \"positional disentanglement metric\" and \"systematic generalization.\" This lack of clarity hinders the understanding and evaluation of the paper's contributions.\n\n2. **Technical Novelty and Significance**: The reviewers question the novelty and significance of the proposed SCS representation. Reviewer 4 suggests that continuous representations are widely used in modern applications for compositional generalization and that the SCS representation may not offer substantial advantages. Reviewer 2 asks for more discussion on why one-hot encoding (OHE) shouldn't be used and why SCS is preferred, especially since the initial experimental results suggest OHE gives higher performance.\n\n3. **Empirical Evaluation**: The empirical results presented are limited and not convincing. Reviewer 1 notes that the experiments are not sufficient to support the claimed contributions, and Reviewer 4 points out that the performance of the SCS representation is barely above chance. There's also a lack of ablation studies and comparisons with other representations, such as one-hot encodings, which are necessary to understand the value of the proposed approach.\n\n4. **Engagement with Existing Literature**: Reviewer 4 emphasizes the need for the paper to be better situated within the broader literature on compositional generalization, including discussions on discrete vs. continuous representations and the role of communication in instilling symbols. This would help clarify the paper's contributions and novelty.\n\n5. **Correctness and Technical Quality**: Reviewer 4 states that the main claims of the paper are not supported by theory or empirical results, which is a significant concern regarding the paper's correctness and technical quality.\n\nGiven these concerns and the fact that all reviewers rate the paper's technical novelty, empirical novelty, and significance as marginally significant or novel at best, it's clear that the paper requires substantial revisions and improvements before it can be considered for publication in a top-tier conference.\n\nThe final decision is based on the paper's current state, which does not meet the standards of clarity, technical novelty, empirical significance, and engagement with the literature expected of publications in top-tier conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review presents a novel approach to improving the accuracy and robustness of Recurrent Neural Networks (RNNs) in the context of hierarchical control of continually learned autonomous motor motifs. The authors draw inspiration from neuroscience, specifically the thalamocortical circuit, to design a preparatory module that enhances the performance of RNNs in generating sequences of motor motifs without requiring explicit training on motif transitions.\n\nThe reviewers' comments highlight several strengths and weaknesses of the paper. On the positive side, the paper tackles an important problem in the field of continual learning and motor control, and the proposed approach shows promising results in terms of accuracy and robustness. The use of insights from neuroscience to inform the design of the preparatory module is also seen as a strength.\n\nHowever, the reviewers also raise several concerns, including the lack of evaluation of alternative approaches, the small scale of the models and data, and the unclear relevance of the results to the broader field of machine learning. Some reviewers also question the novelty and significance of the contributions, citing existing work in the field.\n\nIn response to the reviewers' comments, the authors provide additional explanations and clarifications, and offer to revise the manuscript to address the concerns raised. They emphasize the importance of their approach in enabling zero-shot transfer from single motif training to motif sequencing, and highlight the advantages of their preparatory module over alternative strategies.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has potential but requires significant revisions to address the concerns raised. The authors need to provide more convincing evidence of the effectiveness of their approach, including a more thorough evaluation of alternative strategies and a clearer explanation of the relevance of their results to the broader field.\n\nFurthermore, the authors should consider expanding the scale of their models and data to demonstrate the scalability of their approach. They should also provide more detailed analyses of the results, including quantitative metrics and statistical tests, to support their claims.\n\nIn conclusion, while the paper shows promise, it requires significant revisions to address the concerns raised by the reviewers. Therefore, my decision is to reject the paper in its current form, but I encourage the authors to revise and resubmit their work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper presents AutoOED, an Automated Optimal Experimental Design platform that leverages machine learning to accelerate the discovery of solutions with optimal objective trade-offs. The platform implements popular multi-objective Bayesian optimization (MOBO) algorithms and introduces a novel strategy called Believer-Penalizer (BP) for asynchronous batch experiments. The authors also provide a graphical user interface (GUI) for users without extensive coding experience.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers praise the paper's clarity, organization, and the potential impact of AutoOED on the field of optimal experimental design. However, they also express concerns about the paper's technical novelty, empirical novelty, and the lack of theoretical contributions.\n\nSeveral reviewers question the paper's technical novelty, stating that the contributions are marginally significant or novel. They argue that the platform mostly implements existing techniques and that the BP strategy, although straightforward and simple, lacks a clear theoretical foundation. Reviewer 2 notes that the paper reads like a software manual rather than a scientifically methodological paper, which might not be suitable for a top-tier conference like ICLR.\n\nThe empirical evaluations of AutoOED also receive criticism. Reviewers point out that the results are not always consistent, and the comparisons to other platforms are sometimes misleading. Reviewer 4 argues that the claims about BP's performance are overstated and that a more accurate statement would be that BP is a robust approach that performs well on average.\n\nThe authors' response addresses some of the reviewers' concerns, providing additional context and clarifications about the platform's capabilities and limitations. They emphasize the importance of user-friendliness, modular design, and the introduction of asynchronous MOBO, which they believe are significant contributions to the field.\n\nHowever, despite the authors' efforts to address the reviewers' concerns, the paper's limitations and lack of technical novelty remain a significant issue. The paper's focus on engineering contributions, while valuable, might not be sufficient to warrant publication in a top-tier conference like ICLR, which typically prioritizes papers with significant scientific and technical contributions.\n\nIn conclusion, while AutoOED has the potential to be a useful tool for optimal experimental design, the paper's limitations and lack of technical novelty make it less suitable for publication in a top-tier conference like ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper under review presents a theoretical analysis of the epoch-wise double descent phenomenon in neural networks, using a linear teacher-student model. The authors derive closed-form expressions for the generalization error and demonstrate that double descent can occur due to the learning of features at different scales. The paper also provides numerical experiments to validate the theoretical findings.\n\nThe reviewer comments are overall positive, with Reviewer 1 praising the paper as \"valuable\" and \"insightful\", Reviewer 2 considering it \"solid\" with \"theoretical analysis and empirical evaluation\", and Reviewer 3 finding it \"illuminating\" and \"worthwhile\". However, the reviewers also raise several concerns and suggestions for improvement.\n\nReviewer 1 asks for more interpretation of the results, particularly regarding the difference between epoch-wise and model-wise double descent, and the connection to other works. Reviewer 2 requests a discussion of the connection to pioneering works, such as Ali and Ray Tibshrani's work on implicit regularization. Reviewer 3 points out numerous typos and errors, and suggests that the paper could be improved by providing a self-contained discussion of the exact results drawn from older literature.\n\nThe authors respond to the reviewer comments, addressing each of the concerns and suggestions. They provide additional discussions, clarifications, and references to related works. They also acknowledge the typos and errors, and state that they have been corrected.\n\nConsidering the reviewer comments and the authors' responses, the paper appears to be well-written, and the authors have made a significant effort to address the concerns and suggestions. The paper presents a valuable contribution to the understanding of the double descent phenomenon, and the theoretical analysis is sound.\n\nHowever, the paper is not without its limitations. The linear teacher-student model is a simplification of the more complex neural network architectures used in practice. Additionally, the paper could benefit from more empirical evaluations, particularly on larger datasets and more complex models.\n\nIn conclusion, based on the reviewer comments and the authors' responses, I believe that the paper has been significantly improved, and it presents a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Natural Language Descriptions of Deep Visual Features\" presents a novel approach to interpreting deep learning models for computer vision by generating natural language descriptions of neurons in the model. The method, called MILAN, uses mutual information to guide the linguistic annotation of neurons, resulting in fine-grained descriptions that capture categorical, relational, and logical structure in learned features.\n\nThe paper is well-written, and the authors provide sufficient technical details for readers to understand and reproduce their work. The experiments demonstrate the effectiveness of MILAN in generating descriptions that agree with human-generated feature descriptions across a diverse set of model architectures and tasks. The authors also highlight three applications of natural language neuron descriptions, including analysis, auditing, and editing.\n\nThe reviewer comments are generally positive, with reviewers praising the paper's clarity, technical soundness, and promising results. Reviewer 2 notes that the paper is well-motivated and picks up an intriguing topic, while Reviewer 3 appreciates the amount of effort put into testing the system. Reviewer 4 finds the paper to be strong, with a clear hypothesis and careful experimentation.\n\nHowever, the reviewers also raise some concerns and suggest improvements. Reviewer 2 questions the inter-annotator agreement among human annotations and suggests that the authors should consider scaling up their methods by leveraging existing large datasets. Reviewer 3 suggests that the limits of the approach should be discussed further, while Reviewer 4 asks about the transferability of the results to other tasks and datasets.\n\nThe authors respond to the reviewer comments, addressing the concerns and suggesting additional experiments to further validate their approach. They also provide additional information about the dataset and the generated descriptions, which helps to clarify some of the issues raised by the reviewers.\n\nOverall, the paper presents a significant contribution to the field of computer vision and deep learning, with a novel approach to interpreting deep learning models. The authors demonstrate the effectiveness of their method through thorough experimentation and provide a clear and well-written paper. While there are some concerns and suggested improvements, the authors address these in their response, and the paper is well-suited for publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" proposes a novel framework for synthesizing data using deep generative models in a differentially private manner. The reviewers have provided detailed comments and suggestions, which the authors have addressed in their response.\n\nThe main strengths of the paper are:\n\n1. The approach is well-motivated and the paper is well-written.\n2. The mathematical formulation is easy to follow and the approach is novel.\n3. The empirical evaluations show that the approach compares favorably against other methods at reasonable levels of privacy.\n\nHowever, the reviewers have also raised some concerns and suggestions for improvement:\n\n1. The lack of thorough benchmarking, particularly for tabular data and image data.\n2. The need for more justifications about the novelty claims and more related work.\n3. The need for more advanced baselines and more benchmark datasets.\n4. The lack of complexity analysis and the need for more details about the optimization procedure.\n\nThe authors have addressed these concerns in their response by:\n\n1. Adding more baselines and datasets, including DPCGAN and Credit.\n2. Providing more justifications about the novelty claims and adding more related work.\n3. Providing more details about the optimization procedure and the complexity analysis.\n4. Clarifying the privacy-preserving optimization method and the one-shot sampling strategy.\n\nBased on the reviewers' comments and the authors' response, it appears that the paper has made significant contributions to the field of differentially private data synthesis. The approach is novel, well-motivated, and has been empirically evaluated on multiple datasets. The authors have addressed the concerns raised by the reviewers and have provided more details and justifications about their approach.\n\nHowever, the reviewers have also noted that the paper is not perfect and that there are some areas that need further improvement. For example, the lack of thorough benchmarking and the need for more advanced baselines and more benchmark datasets.\n\nOverall, based on the information provided, it appears that the paper has made significant contributions to the field and has addressed the concerns raised by the reviewers. Therefore, the final decision is:\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a novel online knowledge distillation framework that utilizes a label prior shift to induce diversity among teachers and a post-compensation technique to aggregate teacher predictions. The authors demonstrate the effectiveness of their approach through extensive experiments on various datasets, including CIFAR-10/100 and ImageNet.\n\nThe reviewers have raised several concerns and suggestions, which the authors have addressed in their responses. The main concerns include:\n\n1. **Novelty**: Reviewers have questioned the novelty of the approach, suggesting that the combination of label prior shift and importance sampling is not significantly new. However, the authors argue that the application of these techniques in the context of online knowledge distillation is novel and provides a unique contribution to the field.\n2. **Comparison to existing methods**: Reviewers have requested comparisons to other state-of-the-art methods, such as PCL, MVCL, and CGL. The authors have provided additional experiments and results, demonstrating that their approach outperforms these methods in various settings.\n3. **Experimental results**: Reviewers have raised concerns about the experimental results, including the choice of baselines and the presentation of results. The authors have addressed these concerns by providing additional experiments and clarifying their methodology.\n4. **Writing and clarity**: Reviewers have pointed out typos and suggested improvements to the writing and clarity of the paper. The authors have acknowledged these suggestions and revised their manuscript accordingly.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has addressed the main concerns and provides a significant contribution to the field of online knowledge distillation. The authors have demonstrated the effectiveness of their approach through extensive experiments and have provided a clear and well-written manuscript.\n\nHowever, I also acknowledge that the novelty of the approach may not be entirely new, and the authors could have done a better job of highlighting the unique contributions of their work. Additionally, some reviewers have raised concerns about the comparison to existing methods, which the authors have addressed to some extent.\n\nOverall, I believe that the paper provides a significant contribution to the field and has addressed the main concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" proposes a novel framework that combines Monte-Carlo Tree Search (MCTS) with the Transformer architecture in offline reinforcement learning. The idea of combining these two approaches is intriguing, and the paper presents some promising results on the SameGame domain.\n\nHowever, after carefully analyzing the paper and the reviewer comments, several concerns arise that impact the paper's overall quality, novelty, and significance.\n\n1. **Lack of Clarity and Presentation**: Reviewer 1 and Reviewer 3 mention that the paper lacks clarity in its exposition, with Reviewer 1 suggesting that the writing and presentation are \"extremely unclear\" and require multiple passes to understand. This lack of clarity can hinder the reader's ability to fully grasp and appreciate the contributions of the paper.\n\n2. **Limited Empirical Evaluation**: The empirical evaluation of the MCTransformer is limited to the SameGame domain, which Reviewer 1 describes as \"relatively toy-ish.\" This limited evaluation makes it challenging to assess the generalizability and robustness of the proposed method across different domains and tasks. Reviewer 2 and Reviewer 3 also emphasize the need for more comprehensive comparisons with other state-of-the-art methods, including those that combine MCTS with neural networks like AlphaGo and AlphaZero.\n\n3. **Novelty and Originality**: Reviewer 2 and Reviewer 3 express concerns about the novelty of the paper, with Reviewer 2 stating that the structure is similar to AlphaGo/AlphaZero's setting except for replacing the neural network from Resnet to Transformer. This raises questions about the originality and significance of the contributions, as the combination of MCTS with Transformers, while novel in the context of offline reinforcement learning, may not be sufficiently groundbreaking.\n\n4. **Comparison and Baselines**: The comparisons provided in the paper are limited and, in some cases, may be considered unfair or not fully convincing, as pointed out by Reviewer 1 and Reviewer 2. For example, the comparison with MCTS-based solutions without learned prior policies or value functions may not accurately reflect the state-of-the-art in MCTS applications.\n\n5. **Reproducibility**: While Reviewer 3 mentions that the paper provides enough information for reproducibility, the lack of source code and detailed experimental settings could still pose challenges for exact replication.\n\nGiven these considerations, while the paper presents an interesting idea and some promising initial results, the concerns regarding clarity, empirical evaluation, novelty, comparison to baselines, and reproducibility are significant enough to impact the paper's suitability for publication in a top-tier conference at this time.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Behavior Proximal Policy Optimization\" presents a novel approach to offline reinforcement learning (RL) by leveraging the inherent conservatism of online on-policy algorithms. The authors propose a new algorithm, Behavior Proximal Policy Optimization (BPPO), which solves offline RL without introducing extra constraints or regularization compared to Proximal Policy Optimization (PPO). The paper provides theoretical analysis and empirical evaluations on the D4RL benchmark, demonstrating the effectiveness of BPPO.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novel theoretical analysis**: The paper provides a new perspective on offline RL by analyzing the conservatism of online on-policy algorithms.\n2. **Simple and effective algorithm**: BPPO is a straightforward extension of PPO, making it easy to implement and understand.\n3. **Competitive empirical performance**: BPPO achieves competitive or superior performance on various tasks in the D4RL benchmark.\n\nHowever, the reviewers also raise several concerns:\n\n1. **Assumption 1 is too strong**: The assumption that the estimated behavior policy is close to the true behavior policy might be difficult to satisfy in practice.\n2. **Replacement of $A_{\\pi_k}$ with $A_{\\pi_\\beta}$**: The authors' decision to replace the advantage function with the behavior policy's advantage function might loosen the bound and affect the soundness of the method.\n3. **Lack of ablation studies**: The paper could benefit from more comprehensive ablation studies to understand the impact of different components and hyperparameters on the performance of BPPO.\n4. **Comparison to prior works**: The reviewers question the novelty and significance of the contributions, as some prior works (e.g., GePPO) have similar theoretical results and algorithmic designs.\n\nThe authors have addressed some of these concerns in their response, providing additional explanations, modifications, and experimental results. They have:\n\n1. **Replaced Assumption 1**: The authors have replaced the strong assumption with a more reasonable distance metric between the estimated behavior policy and the offline dataset.\n2. **Provided additional ablation studies**: The authors have added more ablation studies to investigate the impact of different hyperparameters and components on BPPO's performance.\n3. **Clarified the relation to prior works**: The authors have discussed the differences and similarities between their work and prior approaches, such as GePPO and Onestep RL.\n\nAfter carefully considering the paper, the reviewers' comments, and the authors' response, I conclude that the paper has made significant contributions to the field of offline RL. While there are some concerns and areas for improvement, the authors have addressed many of these issues in their response.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Variational Neural Cellular Automata\" proposes a novel generative model that combines the concepts of neural cellular automata (NCA) and variational autoencoders (VAE). The authors demonstrate the model's ability to learn complex data distributions and show its resilience to damage. The paper is well-written, and the authors have addressed the reviewers' comments and concerns in their rebuttal.\n\nThe reviewers have raised several points, both positive and negative. Some reviewers have praised the paper's clarity, novelty, and the interesting results, while others have criticized the experimental results, the lack of comparison to other methods, and the limited novelty of the approach.\n\nAfter carefully analyzing the reviews and the authors' responses, it is clear that the paper has both strengths and weaknesses. The strengths include the paper's novelty, the interesting results, and the authors' efforts to address the reviewers' concerns. The weaknesses include the limited experimental results, the lack of comparison to other methods, and the potential lack of novelty.\n\nHowever, it is also clear that the authors have made a significant effort to improve the paper based on the reviewers' comments. They have added new experiments, clarified the methodology, and addressed the concerns raised by the reviewers.\n\nIn terms of the standards of a top-tier conference, the paper meets some of the expectations. The paper is well-written, and the authors have demonstrated a good understanding of the subject matter. The results are interesting, and the authors have made an effort to evaluate the model fairly.\n\nHowever, the paper falls short in terms of the experimental results and the comparison to other methods. The results are not state-of-the-art, and the authors have not demonstrated a significant improvement over existing methods.\n\nDespite these weaknesses, I believe that the paper has the potential to make a contribution to the field. The concept of combining NCA and VAE is novel, and the authors have demonstrated some promising results. The paper also raises interesting questions about the potential applications of NCA in generative modeling.\n\nTherefore, based on the analysis of the paper and the reviews, I conclude that the paper should be accepted for publication. The authors have demonstrated a good understanding of the subject matter, and the paper has the potential to make a contribution to the field. While the paper has some weaknesses, the authors have made an effort to address the reviewers' concerns, and the paper meets some of the expectations of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes\" presents a novel approach to neural network compression using tropical geometry. The authors provide a theoretical framework for understanding the approximation error of compressed neural networks and propose two algorithms, Zonotope K-means and Neural Path K-means, for compressing fully connected layers of ReLU-activated neural networks.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novelty and theoretical significance**: The paper introduces a new perspective on neural network compression using tropical geometry, which is a significant contribution to the field.\n2. **Mathematical rigor**: The authors provide a solid mathematical analysis of the approximation error of compressed neural networks, which is a valuable contribution to the theoretical understanding of neural network compression.\n3. **Experimental results**: The authors provide experimental results demonstrating the effectiveness of their algorithms on several datasets, including MNIST, Fashion-MNIST, and CIFAR-10/100.\n\nHowever, the reviewers also point out several weaknesses:\n\n1. **Limited empirical evaluation**: The experimental evaluation is limited to small-scale datasets and architectures, which may not be representative of real-world scenarios.\n2. **Comparison to state-of-the-art methods**: The authors do not provide a comprehensive comparison to state-of-the-art pruning methods, which makes it difficult to assess the effectiveness of their approach.\n3. **Lack of runtime and memory complexity analysis**: The authors do not provide a detailed analysis of the runtime and memory complexity of their algorithms, which is essential for understanding their practical applicability.\n4. **Clarity and presentation**: Some reviewers found the paper difficult to follow, and the authors could improve the clarity and presentation of their work.\n\nIn response to the reviewers' comments, the authors have provided a detailed rebuttal and have made several changes to the paper, including:\n\n1. **Clarifying the scope of the paper**: The authors have clarified that their paper is primarily a theoretical contribution, and the experimental results are proof-of-concept.\n2. **Adding more context**: The authors have added more context to the paper, including a discussion of the related work and the significance of their contribution.\n3. **Improving the presentation**: The authors have improved the presentation of the paper, including the addition of more figures and tables to illustrate the results.\n\nBased on the reviewers' comments and the authors' rebuttal, I believe that the paper has been improved significantly. The authors have addressed the main concerns raised by the reviewers, and the paper now provides a clearer and more comprehensive presentation of the work.\n\nHowever, I still have some concerns regarding the limited empirical evaluation and the lack of comparison to state-of-the-art methods. While the authors have provided some experimental results, they are still limited to small-scale datasets and architectures. Additionally, the authors do not provide a comprehensive comparison to state-of-the-art pruning methods, which makes it difficult to assess the effectiveness of their approach.\n\nDespite these concerns, I believe that the paper makes a significant contribution to the field of neural network compression and tropical geometry. The authors have introduced a new perspective on neural network compression, and their theoretical analysis provides a valuable understanding of the approximation error of compressed neural networks.\n\nTherefore, I recommend accepting the paper for publication, but with the condition that the authors provide a more comprehensive empirical evaluation and comparison to state-of-the-art methods in a future version of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the reviewer comments, the paper's content, and the author's response (if available). Given that there is no author response provided, we will rely solely on the information from the paper itself and the reviewer comments.\n\n1. **Novelty and Originality**: A significant concern raised by multiple reviewers is the lack of novelty in the proposed method. Reviewer 1 states that the core idea has been proposed before by unlikelihood training (UL), and the authors' claim about UL only considering negative tokens is disputed. Reviewer 3 also mentions that the idea is very similar to \"unlikelihood training.\" This suggests that the paper may not bring significant new insights or contributions to the field.\n\n2. **Experimental Setup and Results**: Several reviewers criticize the experimental setup for being insufficient or not convincing. Reviewer 1 suggests that the experiments are limited and that the paper should have included results from tasks like abstractive summarization, machine translation, and image captioning to make the proposed method more convincing. Reviewer 2 points out that the primary result is based on GPT2-small fine-tuned on wikitext, which is not a convincing setup by today's standards, and that the metrics used are not fully aligned with prior work. Reviewer 4 questions the importance of the repetition issue for better or larger models and suggests that scaling experiments could help resolve this question.\n\n3. **Clarity and Reproducibility**: On a positive note, all reviewers agree that the paper is well-written and easy to follow. Reviewers also appreciate the reproducibility of the results, with the code being made available.\n\n4. **Technical and Empirical Significance**: The technical and empirical novelty and significance of the paper are rated as low to marginal by most reviewers. While the proposal of a new training objective is seen as interesting, its impact and novelty are questioned due to similarities with existing methods and the lack of comprehensive experimental validation.\n\nGiven these considerations, the primary concerns with the paper are its novelty, the comprehensiveness of its experimental setup, and its ability to demonstrate significant improvements over existing methods. Without a strong author response to address these concerns, it's challenging to justify acceptance based on the current reviewer feedback.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" presents a convergence analysis of gradient descent (GD) for deep linear neural networks with different random initialization. The authors provide sharp rates of convergence of GD for deep linear networks and show that the convergence rate does not depend on the type of random initialization. They also demonstrate that the depth of the network does not affect the optimal rate of convergence, provided that the width of each hidden layer is sufficiently large.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The main strengths of the paper include:\n\n1. The paper provides a sharp convergence analysis of GD for deep linear neural networks, which is a significant contribution to the field.\n2. The authors consider various initialization schemes, including Gaussian initialization, orthogonal initialization, and special balanced initialization.\n3. The paper shows that the convergence rate of GD for deep linear networks is similar to that of GD for convex problems, which is a surprising and important result.\n\nHowever, the reviewers also raise some concerns about the paper's novelty and technical significance. Some reviewers argue that the paper's results are not entirely new and that similar results have been obtained in previous works. Others point out that the paper's analysis is local and that the minimum width requirement depends on constants that are not well-defined.\n\nThe authors respond to these concerns by providing additional insights and explanations. They argue that their results are novel and that they provide a more comprehensive analysis of GD for deep linear neural networks than previous works. They also provide additional discussions on the constants and the minimum width requirement.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper makes a significant contribution to the field of deep learning and optimization. While some reviewers raise concerns about the paper's novelty and technical significance, the authors provide convincing responses that address these concerns.\n\nThe paper's strengths, including its sharp convergence analysis and comprehensive consideration of various initialization schemes, outweigh its weaknesses. The authors' responses to the reviewers' comments demonstrate a clear understanding of the paper's contributions and limitations.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I recommend accepting the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper based on the provided abstract, reviewer comments, and the standards of a top-tier conference.\n\n1. **Novelty and Significance**: The paper proposes using graph structures and topological properties to detect out-of-distribution (OOD) data, which is an interesting approach. However, reviewers have raised concerns about the novelty of the method, suggesting that it combines existing techniques without significant innovation. The lack of comparison with state-of-the-art OOD detection methods and the reliance on a pre-trained object detection network for semantic information are notable weaknesses.\n\n2. **Technical Quality and Correctness**: Reviewers have pointed out several technical issues, including the lack of rigorous demonstration of claims, poorly defined terms, and insufficient detail in the methodology and experimental setup. The paper's failure to provide a thorough analysis of how the object detector's accuracy impacts the model and the absence of ablation studies are significant oversights.\n\n3. **Clarity, Quality, and Reproducibility**: The writing quality has been criticized for being poor, with grammatical mistakes, missing details, and a lack of clarity in explaining the methodology and results. The experimental section is deemed insufficient for validating the effectiveness of the introduced method, with missing comparisons to other feature-based OOD detection methods and limited datasets.\n\n4. **Empirical Evaluation**: The empirical results, although promising with high AUROC values on the LSUN dataset, are not convincing due to the lack of extensive comparisons with state-of-the-art techniques and the limitation to a single dataset. The absence of interpretability analysis of the OOD detection by semantic graphs further weakens the empirical contribution.\n\n5. **Response to Reviewer Comments**: Unfortunately, there is no author response provided to address the concerns and suggestions raised by the reviewers. This lack of response makes it challenging to assess whether the authors can adequately address the issues identified.\n\nConsidering these points, the paper falls short in several critical areas expected of a top-tier conference publication, including novelty, technical quality, clarity, and empirical significance. While the idea of using graph structures for OOD detection is intriguing, the execution, as presented, does not meet the standards required for publication in a prestigious academic conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting observation that swapping the order of batch normalization and bounded activation functions can lead to improved performance in certain deep learning architectures. The authors provide empirical evidence to support their claim and attempt to explain the underlying reasons for this phenomenon.\n\nHowever, upon careful analysis of the reviewer comments and author responses, several concerns and weaknesses in the paper become apparent. These include:\n\n1. **Lack of clarity and rigor**: Multiple reviewers pointed out that the paper's claims and explanations are not clearly clarified or rigorous. The authors' responses attempt to address these concerns, but some issues persist.\n2. **Limited analysis and experimentation**: The paper's analysis and experimentation are limited to specific architectures and datasets. Reviewers questioned the generality of the findings and the lack of exploration of other factors that might influence the results.\n3. **Unclear or unsupported claims**: Some of the paper's claims, such as the relationship between asymmetric saturation and generalization performance, are not well-supported or are based on incomplete analysis.\n4. **Comparison to existing work**: The paper's comparison to existing work, particularly ReLU-based models, is not comprehensive, and the authors' claims about the superiority of their approach are not fully substantiated.\n5. **Code and reproducibility**: Although the authors provided code, some reviewers had difficulty reproducing the results or finding specific implementations of certain metrics.\n\nDespite these concerns, the paper does present an interesting and potentially valuable contribution to the field. The authors' responses demonstrate a willingness to engage with the reviewers' comments and address some of the weaknesses.\n\nAfter weighing the strengths and weaknesses of the paper, I conclude that the paper requires significant revisions to address the concerns and weaknesses identified by the reviewers. While the paper shows promise, it is not yet ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting phenomenon where swapping the order of batch normalization and bounded activation functions (such as Tanh) can lead to improved performance on various benchmarks and architectures. The authors investigate the reasons behind this improvement and attribute it to the asymmetric saturation of activation values, which can induce higher sparsity and potentially improve generalization.\n\nThe reviewers have raised several concerns and questions about the paper, including:\n\n1. Clarification of claims and definitions: Some reviewers have pointed out that certain claims and definitions in the paper are not clear or rigorous, such as the concept of \"center of the function\" and the relationship between saturation and sparsity.\n2. Experimental setup and analysis: Reviewers have questioned the experimental setup, including the choice of models, datasets, and hyperparameters. Some have also asked for more analysis and comparison of the results, such as the effect of weight decay on sparsity and accuracy.\n3. Novelty and significance: Some reviewers have expressed concerns about the novelty and significance of the paper, given that the idea of swapping batch normalization and activation functions is not entirely new, and the results may not be universally applicable.\n4. Reproducibility: One reviewer has asked for more information about the code and calculations used to compute saturation, skewness, and sparsity.\n\nThe authors have responded to these concerns and questions, providing additional clarification, analysis, and experimental results. They have also acknowledged some of the limitations and potential issues with their work.\n\nAfter carefully considering the paper, reviewer comments, and author responses, I believe that the paper has some interesting and potentially valuable contributions to make. However, I also think that the paper could be improved with more rigorous definitions, clearer explanations, and more comprehensive analysis and experimentation.\n\nThe authors have shown that the swapped order of batch normalization and bounded activation functions can lead to improved performance, and they have provided some insights into the possible reasons behind this improvement. However, more work is needed to fully understand the mechanisms at play and to establish the generality and significance of the results.\n\nTherefore, I would recommend that the paper be revised and resubmitted, addressing the concerns and questions raised by the reviewers. With further refinement and clarification, the paper could make a more substantial contribution to the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" proposes a new adversarial training algorithm that applies more regularization to data vulnerable to adversarial attacks. The authors claim that their algorithm is theoretically well-motivated and empirically superior to other existing algorithms.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. Theoretical contribution: Some reviewers have questioned the novelty and significance of the theoretical contribution, arguing that the proposed theorem is not new and is similar to existing work, such as TRADES.\n2. Experimental results: Some reviewers have pointed out that the experimental results are not convincing, with marginal improvements over existing methods, and that the results are not consistent across different datasets and settings.\n3. Comparison to existing methods: Some reviewers have noted that the comparison to existing methods, such as HAT, is not fair, and that the results are not consistent with other published results.\n4. Clarity and presentation: Some reviewers have commented on the clarity and presentation of the paper, suggesting that the writing is not clear, and that the notation and definitions are not well-explained.\n\nThe authors have responded to these concerns, providing additional explanations and results to address the reviewers' comments. However, despite these efforts, the concerns about the paper's theoretical contribution, experimental results, and comparison to existing methods remain.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper has some strengths, such as its well-written presentation and its attempt to provide a theoretical justification for the proposed algorithm. However, the paper's weaknesses, including the lack of novelty and significance in the theoretical contribution, the marginal experimental results, and the inconsistent comparison to existing methods, outweigh its strengths.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper should be rejected for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" proposes a novel approach to improve the generalization of policies in complex games by maintaining a population of agents with diverse playing styles and high skill levels. The authors introduce a bi-objective optimization model and an evolutionary algorithm to optimize the agents' skill level and playing style simultaneously.\n\nThe reviewers' comments highlight both strengths and weaknesses of the paper. Reviewer 1 praises the paper's clarity and the interesting topic but criticizes the lack of discussion on related work, particularly the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al. (2017). Reviewer 1 also questions the experimental design, suggesting that it may give an unfair advantage to the proposed method. Reviewer 2 finds the paper well-written and the approach promising but requires more clarification on certain aspects, such as the definition of \"playing style\" and the experimental results. Reviewer 3 appreciates the algorithm's simplicity and the promising results but notes the lack of comparison to quality-diversity algorithms and the limited number of evaluation domains.\n\nThe reviewers' comments on the paper's novelty and significance are mixed. Reviewer 1 rates the technical novelty and significance as 2, indicating that the contributions are only marginally significant or novel. Reviewer 2 rates the technical novelty and significance as 3, suggesting that the contributions are significant and somewhat new. Reviewer 3 rates the technical novelty and significance as 2, stating that the contributions are only marginally significant or novel.\n\nConsidering the reviewers' comments, the paper's strengths and weaknesses, and the standards of a top-tier conference, the decision to accept or reject the paper depends on the following factors:\n\n1. **Novelty and Significance**: While the paper proposes a novel approach, the reviewers question its significance and novelty compared to existing work.\n2. **Technical Quality**: The paper's technical quality is good, but the reviewers suggest improvements in experimental design, clarification of certain aspects, and comparison to related work.\n3. **Reproducibility**: The paper provides sufficient details for reproducibility, but the lack of source code and comparison to quality-diversity algorithms limits the reproducibility.\n\nBased on these factors, the paper requires revisions to address the reviewers' concerns, particularly regarding the discussion of related work, experimental design, and comparison to quality-diversity algorithms. However, the paper's promising results and simplicity of the algorithm suggest that it has potential.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Regression with Label Differential Privacy\" presents a novel approach to training regression models with label differential privacy (DP) guarantees. The authors propose a label DP randomization mechanism, called \"randomized response on bins,\" which is optimal under a given regression loss function. The paper provides a thorough theoretical analysis, including proofs of the mechanism's optimality, and demonstrates the efficacy of the algorithm through extensive experiments on several datasets.\n\nThe reviewer comments are generally positive, with most reviewers praising the paper's clarity, technical novelty, and empirical significance. Reviewer 3 notes that the paper is easy to follow and has the potential for a net positive impact, while Reviewer 4 states that the writing is clear and the proposed algorithm is novel and optimal. Reviewer 5 finds the paper well-written and enjoyable to read, although they question the technical novelty and significance of the contributions.\n\nSome reviewers raise minor concerns and suggestions for improvement. Reviewer 1 points out that the distinction between regression and classification may not be necessary, while Reviewer 2 suggests including a comparison with other label-DP methods in the appendix. Reviewer 3 asks for a discussion on why the paper does not compare with other label-DP methods, such as Ghazi et al. (2021). Reviewer 4 suggests considering the second term in Equation (1) and notes that the number of optimal bins could be smaller than the number of different labels. Reviewer 5 questions the practicality of the approach and suggests considering the overall generalization error.\n\nThe authors respond to the reviewer comments, addressing the concerns and suggestions. They clarify the difference between regression and classification, provide additional experiments, and discuss the potential for future work on considering the second term in Equation (1).\n\nBased on the reviewer comments and author responses, the paper appears to be well-written, technically sound, and novel. The authors have addressed the minor concerns and suggestions, and the paper has the potential for a net positive impact on the field of differential privacy.\n\nHowever, some reviewers have raised questions about the technical novelty and significance of the contributions, particularly in comparison to prior work such as Ghazi et al. (2021). The authors have responded to these concerns, highlighting the differences between their approach and prior work, and demonstrating the efficacy of their algorithm through experiments.\n\nAfter careful consideration of the reviewer comments and author responses, I conclude that the paper is suitable for publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" proposes a novel method for predicting protein-protein interactions (PPIs) by leveraging pretrained structure embeddings. The authors claim that their approach leads to significant improvements in PPI prediction compared to sequence and network-based methods.\n\nHowever, upon analyzing the reviewer comments, several concerns and limitations are raised. Reviewer 1 notes that the novelty of the paper is limited, as the pre-trained structural embedding is directly adopted from OmegaFold, and the main contribution is using graph neural networks with structural embeddings as inputs for PPI prediction. Reviewer 2 also comments that the approach is not particularly novel, as it combines existing methods without significant modification. Reviewer 3 finds the paper to be unclear, with marginal improvements in empirical results, and lacking confidence intervals. Reviewer 4 questions the baseline choices, model design, and the advantage of using both structural embeddings and GNN aggregation.\n\nThe reviewers also raise several technical concerns, such as the use of mean pooling to obtain fixed-length representations, the inconsistency of results in Table 2 and 3, and the lack of comparison to state-of-the-art methods like AlphaFold-Multimer. Additionally, the reviewers suggest that the paper could be improved with more detailed explanations, better presentation, and additional experiments to support the claims.\n\nGiven the concerns and limitations raised by the reviewers, it appears that the paper has several significant flaws. The lack of novelty, unclear presentation, and marginal improvements in empirical results are major concerns. Additionally, the technical concerns and the lack of comparison to state-of-the-art methods raise questions about the validity and significance of the results.\n\nIn conclusion, based on the analysis of the reviewer comments and the paper's content, it is clear that the paper requires significant revisions to address the concerns and limitations raised. However, given the current state of the paper, it is not suitable for publication in a top-tier conference like ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"CAT: Collaborative Adversarial Training\" should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper based on the provided information, including the paper's abstract, reviewer comments, and the absence of an author response.\n\n1. **Novelty and Originality**: The reviewers have raised concerns about the novelty of the proposed method. Reviewer 1 mentions that the idea of combining multiple defenses is not novel and cites previous works on ensemble methods for adversarial training. Reviewer 2 also states that similar ideas have been proposed and studied before. This suggests that the paper may not significantly advance the state-of-the-art in terms of novelty.\n\n2. **Technical Quality and Significance**: The technical quality of the paper is a crucial aspect. While the paper proposes a collaborative adversarial training framework and demonstrates its effectiveness on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art robustness without using additional data on CIFAR-10 under the Auto-Attack benchmark, several reviewers have questioned the lack of theoretical analysis and the increased computational cost. Reviewers 1, 2, and 3 have pointed out the need for more detailed experimental analysis, including comparisons with more recent and stronger baselines, and the exploration of the method's applicability to larger datasets and different network architectures.\n\n3. **Empirical Evidence and Reproducibility**: The empirical evidence provided by the authors, such as the experiments on CIFAR-10 and CIFAR-100, shows promise but is limited. Several reviewers have suggested that the experiments are not comprehensive enough, lacking comparisons with important baselines, ablation studies, and evaluations on larger datasets like ImageNet. The reproducibility of the results is also a concern, as Reviewer 3 mentions that the reproducibility is unknown, and Reviewer 4 encourages the authors to make the code available.\n\n4. **Clarity, Quality, and Overall Presentation**: The clarity of the paper is generally praised by the reviewers, with most finding it easy to follow. However, there are mentions of typos and the need for proofreading. The quality and novelty of the paper are more controversial, with most reviewers rating these aspects as low or marginally significant.\n\n5. **Reviewer Consensus**: While there is no complete consensus among the reviewers, the majority express concerns about the paper's novelty, the lack of comprehensive experimental analysis, and the need for further clarification and justification of the proposed method. Reviewer 4 suggests that the work is interesting and novel but requires more motivation on why the approach works and how the trained models differ.\n\nGiven these considerations, the paper faces significant challenges in terms of demonstrating high novelty, comprehensive technical quality, and robust empirical evidence. The absence of an author response to address the reviewers' concerns further complicates the evaluation, as it leaves many questions and criticisms unaddressed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Functional Risk Minimization\" presents a novel framework for supervised learning problems, challenging the classic assumption of data coming from a single function followed by some noise in the output space. Instead, it models each data point as coming from its own function, leading to the derivation of Functional Risk Minimization (FRM) as a general framework for scalable training objectives. The paper claims that FRM can provide better performance in small experiments in regression and reinforcement learning and offers an avenue towards understanding generalization in the over-parameterized regime.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewer 1 praises the novelty and potential impact of the idea but criticizes the lack of clarity, the unclear relation between the implementable algorithm and the theoretical presentation, and the absence of formal arguments for why FRM would outperform Empirical Risk Minimization (ERM). Reviewer 2 finds the idea interesting and well-written but questions the difference between FRM and Bayesian learning, the scalability of the proposed algorithm, and the lack of experimental evaluation on the proposed functional generative model. Reviewer 3 believes the paper is marginally above the acceptance line, suggesting acceptance, and highlights the computational cost of FRM and the need for discussion on why FRM is crucial and not just an incremental change on ERM.\n\nThe authors' response addresses many of the concerns raised by the reviewers, including scalability, the relationship between FRM and Bayesian learning, and the experimental evaluation. They provide new experiments comparing ERM and FRM on a CNN Variational AutoEncoder (VAE) on variants of MNIST, showing that FRM results in better representations, especially when data variations are structured. They also clarify that while the pure version of FRM is not scalable, the approximation they use is both accurate and scalable, and they discuss potential ways to further increase the scalability of FRM.\n\nConsidering the detailed comments and the authors' responses, the paper presents a novel and interesting idea with potential significance for the machine learning community. The authors have made an effort to address the concerns raised by the reviewers, providing additional experiments and clarifications. However, some reviewers still express concerns about the scalability and the lack of formal arguments for the superiority of FRM over ERM.\n\nDespite these concerns, the overall novelty, the positive experimental results, and the potential impact of the paper on the field of machine learning are significant. The authors' willingness to address the reviewers' comments and their ability to provide additional experiments and clarifications strengthen the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel scheme, dubbed Proximal Validation Protocol (PVP), which aims to resolve the problem of validation set construction by assembling a proximal set as a substitution for the traditional validation set. The construction of the proximal validation set is established with dense data augmentation followed by a novel distributional-consistent sampling algorithm. The authors claim that PVP works better than existing validation protocols on three data modalities, demonstrating its feasibility towards ML production.\n\nHowever, upon analyzing the reviewer comments, several concerns and weaknesses are raised. Reviewer 1 points out that the paper only compares validation sets in terms of how well they evaluate the performance of a fixed model, whereas the purpose of validation sets is to select between different models/hyperparameters. The reviewer also criticizes the lack of clarity in the experiments section, particularly in defining the 'F1' score and the values in tables 3 and 4. Additionally, the reviewer suggests that more thorough empirical evaluation is needed, such as investigating the dependence of PVP on the choice of data augmentation.\n\nReviewer 2 raises concerns about the empirical evaluation and theoretical guarantees. The reviewer notes that most of the datasets used are very small, which cannot fully support the claims, and that the authors should use larger datasets such as ImageNet. The reviewer also requests theoretical results to support the claims.\n\nReviewer 3 agrees that the problem addressed is significant and practical, but points out some limitations in the experiments. The reviewer suggests that more experiments on industrial or real-world classification setups are required, and that the authors should discuss or clarify the applicability of PVP to other tasks such as object detection, semantic segmentation, and QA. The reviewer also requests more intensive ablation studies to support the hypothesis.\n\nReviewer 4 recommends rejecting the paper, stating that there is no reason to believe that the proximal set constructed is actually representative of the data generating distribution. The reviewer criticizes the lack of theoretical basis and the metrics used for evaluation.\n\nConsidering the concerns and weaknesses raised by the reviewers, it appears that the paper has some significant flaws. The lack of clarity in the experiments section, the limited empirical evaluation, and the absence of theoretical guarantees are major concerns. While the problem addressed is significant and practical, the paper's contributions are not sufficiently supported by the results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach for protein sequence and structure co-design, which is a significant challenge in the field of bioengineering. The proposed method, ProtSeed, uses a trigonometry-aware encoder and a roto-translation equivariant decoder to iteratively update the sequence and structure of proteins. The experimental results show that ProtSeed outperforms previous state-of-the-art baselines by a large margin and is able to design proteins with high fidelity in terms of both sequence and structure.\n\nThe reviewer comments are mostly positive, with some reviewers praising the paper's novelty, technical soundness, and empirical results. However, some reviewers also raise concerns about the paper's clarity, novelty, and reproducibility. For example, Reviewer 5 points out that the paper lacks specific hyperparameter and training details, and Reviewer 6 questions the novelty of the method, suggesting that it is similar to previous work, such as AlphaFold2.\n\nThe authors' responses to the reviewer comments are thorough and address most of the concerns raised. They provide additional details about the method, clarify the novelty of their approach, and discuss the limitations of their work. They also conduct additional ablation studies to demonstrate the effectiveness of their method and provide more information about the training cost and reproducibility.\n\nOverall, the paper presents a significant contribution to the field of protein design, and the authors have made a good effort to address the concerns raised by the reviewers. While there are some minor issues with the paper's clarity and novelty, these are not sufficient to reject the paper.\n\nThe final decision is based on the overall quality of the paper, the significance of the contribution, and the authors' responses to the reviewer comments. Considering these factors, the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Data Leakage in Tabular Federated Learning\" presents a novel approach to reconstructing tabular data from gradients in federated learning, addressing the unique challenges posed by mixed discrete-continuous data. The authors propose TabLeak, a method that utilizes a softmax structural prior, pooled ensembling, and an entropy measure to assess reconstruction quality.\n\nThe reviewer comments highlight both strengths and weaknesses of the paper. Reviewer 1 criticizes the paper for lacking novelty and having poor writing quality, with some sentences being obscure and containing grammatical errors. Reviewer 2 acknowledges the paper's well-written nature but finds the technical contributions to be weak, suggesting that the ideas are not innovative and that the benchmark models are fairly weak. Reviewer 3 provides a more balanced assessment, appreciating the paper's simplicity, empirical novelty, and convincing experimental demonstration while also pointing out some minor concerns regarding claims about the specificity of the problems tackled for tabular data.\n\nThe author responses address the reviewers' comments, providing clarifications, additional experiments, and justifications for their methodological choices. They acknowledge the simplicity of their approach but argue that it is a strength, allowing for easy integration with existing methods. They also provide evidence to support their claims about the challenges of tabular data leakage and demonstrate the effectiveness of their approach through extensive experiments.\n\nAfter carefully considering the paper, reviewer comments, and author responses, I conclude that the paper makes a significant contribution to the field of federated learning and data privacy. The authors' approach to tackling the unique challenges of tabular data leakage is novel and well-motivated, and their experimental evaluation is thorough and convincing.\n\nWhile the reviewers raise some valid concerns about the paper's writing quality, novelty, and benchmark models, the authors' responses adequately address these issues. The paper's strengths, including its simplicity, empirical novelty, and convincing experimental demonstration, outweigh its weaknesses.\n\nTherefore, based on the provided information, I recommend accepting the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review explores the problem of online black-box adaptation to label-shift in the presence of conditional-shift. The authors propose several heuristics to improve the performance of existing label-shift adaptation methods in this setting. The paper's main strengths include its well-motivated problem statement, clear empirical results, and honesty about the limitations of the proposed methods.\n\nHowever, the paper also has several weaknesses. The reviewers point out that the paper lacks a systematic approach to understanding the types of distribution shifts considered, and the empirical study is limited in scope. The paper's contributions are also considered marginally significant and novel by the reviewers. Additionally, the paper has some clarity issues, with some sections being difficult to understand, and the notation is sometimes abused.\n\nThe author response addresses some of the reviewers' comments, providing additional background information, clarifying the estimation of p, and rewriting parts of the Bayesian connection section. However, the authors acknowledge that they do not provide theoretical justifications for their proposed heuristics, which is a significant limitation.\n\nConsidering the strengths and weaknesses of the paper, as well as the author response, I believe that the paper is not ready for publication in its current form. While the paper explores an interesting problem and provides some useful empirical results, its limitations and lack of theoretical justifications are significant concerns. The paper would benefit from a more systematic approach to understanding the types of distribution shifts considered, a more comprehensive empirical study, and a clearer and more self-contained presentation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi Task Learning of Different Class Label Representations for Stronger Models\" proposes a novel approach to improve image classification performance by using a multi-task learning framework with binary labels as an auxiliary task. The paper has received mixed reviews from the four reviewers, with some praising the simplicity and effectiveness of the approach, while others criticizing the lack of technical novelty, limited experimental evaluation, and unclear motivation behind the binary labels.\n\nThe main strengths of the paper are:\n\n1. The proposed approach is simple and easy to implement, making it a potentially useful technique for practitioners.\n2. The experimental results show small but consistent improvements over baseline methods on several datasets.\n3. The paper highlights the importance of label representation, which is an understudied aspect of machine learning.\n\nHowever, the paper also has several weaknesses:\n\n1. The technical contribution is limited, and the multi-task learning architecture is basic.\n2. The paper lacks a clear motivation and intuition behind the binary labels, making it difficult to understand why they improve performance.\n3. The experimental evaluation is limited, and the paper does not provide a thorough analysis of the results, including standard deviations and sensitivity to hyperparameters.\n4. The paper does not clearly explain how the binary labels are generated, and the implementation details are missing, making it difficult to reproduce the results.\n5. The paper is not thoroughly compared to existing methods, including state-of-the-art approaches, and the related work section is incomplete.\n\nOverall, while the paper has some interesting ideas and promising results, it falls short in terms of technical novelty, experimental evaluation, and clarity. The reviewers' comments and criticisms are significant, and the paper requires substantial revisions to address these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" presents a novel approach to neural-symbolic AI, introducing DeepSeaProbLog, a probabilistic logic programming language that supports both discrete and continuous random variables. The authors provide a solid theoretical foundation, proving the correctness of their approach and its connection to weighted model integration (WMI) inference. They also demonstrate the practicality of their method through several experiments, showcasing its potential in various applications.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers praise the paper's novelty, technical soundness, and the potential impact of the proposed approach. However, some reviewers express concerns about the incrementality of the work, the lack of extensive empirical evaluations, and the need for more clarity in certain aspects of the presentation.\n\nThe authors' response addresses many of the reviewers' concerns, providing additional explanations, clarifications, and references to support their claims. They also acknowledge the limitations of their current implementation and propose potential directions for future research.\n\nUpon careful analysis of the paper, reviewer comments, and author responses, it is clear that the paper makes significant contributions to the field of neural-symbolic AI. The introduction of DeepSeaProbLog and its connection to WMI inference is a novel and important step forward. The authors' ability to render WMI inference differentiable is a key innovation, and their implementation, which merges DeepProbLog with TensorFlow Probability, bridges an important gap between discrete and continuous methods in a declarative framework.\n\nWhile the paper could benefit from more extensive empirical evaluations and clearer explanations in some areas, the authors' responses and plans for future work, including the public release of their implementation and experimental setup, mitigate these concerns.\n\nConsidering the overall quality, novelty, and significance of the paper, as well as the authors' thorough responses to the reviewers' comments, the decision is to accept the paper for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review presents a study on the phase transition thresholds of permuted linear regression via message passing. The research aims to identify the precise locations of phase transitions in terms of the signal-to-noise ratio (SNR) for both the oracle case, where the signal of interest is known, and the non-oracle case, where it is not. The authors employ a message-passing algorithm to reconstruct the permutation matrix and analyze the phase transition phenomenon.\n\nHowever, upon careful analysis of the reviewer comments, several significant concerns arise that impact the paper's suitability for publication in its current form:\n\n1. **Clarity and Quality of Writing**: Multiple reviewers (Reviewer 1, Reviewer 2, Reviewer 4, and Reviewer 5) have pointed out that the paper is poorly written, with confusing explanations, typos, and a lack of clarity in presenting the mathematical concepts and techniques. This makes it difficult for readers to follow and understand the paper's contributions.\n\n2. **Technical Novelty and Significance**: While the paper attempts to contribute to the understanding of phase transitions in permuted linear regression, reviewers have noted that the techniques used are not entirely new (Reviewer 2, Reviewer 3, Reviewer 4) and that the paper's novelty is somewhat limited by its reliance on existing methods and theories.\n\n3. **Correctness and Support for Claims**: Reviewers have raised questions about the correctness of some claims (Reviewer 1, Reviewer 2, Reviewer 5) and the lack of rigorous support for certain statements and assumptions made in the paper. This includes concerns about the practicality of assumptions, the clarity of definitions, and the validity of comparisons between theoretical and empirical results.\n\n4. **Reproducibility**: The reproducibility of the results is a concern, as noted by Reviewer 4, who mentions the lack of publicly available code for the numerical experiments. This is an important aspect of research, especially in fields relying heavily on computational methods.\n\n5. **Empirical Novelty and Significance**: The empirical contributions of the paper are also questioned, with reviewers noting that the empirical results, while interesting, do not fully align with theoretical predictions (Reviewer 1, Reviewer 5) and that there are gaps in explaining the practical implications and motivations of the research, particularly concerning the partial permutation recovery (Reviewer 2).\n\nGiven these concerns, it is clear that the paper, in its current state, does not meet the standards expected for publication in a top-tier conference. The issues with clarity, technical novelty, correctness, reproducibility, and empirical significance are significant enough to warrant a rejection. However, it is also evident that the paper addresses an interesting problem and has the potential for contributing valuable insights into the field of permuted linear regression. Therefore, with substantial revisions addressing the mentioned concerns, the paper could potentially be resubmitted for consideration.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper's strengths and weaknesses based on the provided information, including the abstract, reviewer comments, and the absence of an author response.\n\n1. **Novelty and Significance**: The paper proposes a novel aggregation strategy, FedPA, which uses a parameterized aggregator to debias aggregation in cross-device federated learning. This addresses the issue of \"period drift,\" a concept introduced by the authors to describe the heterogeneity among different communication rounds. Reviewers have mixed opinions on the novelty and significance of the contributions, with some finding the idea interesting but needing improvement and others seeing it as marginally significant or novel.\n\n2. **Technical Quality**: The technical quality of the paper has been questioned by several reviewers. They point out issues such as the lack of theoretical justification for the effectiveness of the algorithm, the need for more ablation studies, and concerns about the assumption of having a proxy dataset, which might limit the applicability of the method. Additionally, there are suggestions for improving the experimental settings, such as including more complex datasets and comparing with other relevant methods.\n\n3. **Clarity and Reproducibility**: Reviewers have noted issues with the clarity and writing quality of the paper, including grammatical mistakes, inconsistencies in citation formats, and the need for better explanations of certain concepts and equations. The reproducibility of the results is also a concern, as no code was provided, and there are questions about hyperparameter tuning and the choice of experimental settings.\n\n4. **Empirical Evidence**: The empirical evidence provided in the paper shows competitive performance of FedPA compared to conventional baselines on certain datasets. However, reviewers have suggested that the experimental settings are not strong enough and that the results could be more convincing with additional experiments on more diverse datasets and tasks.\n\nGiven these considerations, the main concerns with the paper are its technical quality, clarity, reproducibility, and the marginal novelty and significance of its contributions. While the idea of addressing \"period drift\" in federated learning is interesting and the paper presents some promising empirical results, the issues raised by the reviewers are substantial and would need to be addressed for the paper to meet the standards of a top-tier conference.\n\nWithout an author response to address these concerns, it is challenging to assess whether the authors can adequately revise the paper to meet the high standards required. However, based on the information provided and the significant concerns raised by the reviewers, the most appropriate decision would be to reject the paper in its current form, encouraging the authors to revise and resubmit after addressing the issues mentioned.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper presents a new weather radar dataset, RuDar, which includes 3D radar echo observations and covers multiple climate zones in the European part of Russia. The dataset is comprehensive, with over 50,000 timestamps and 100 GB of data, and is publicly available along with the code for data processing and model preparation.\n\nThe reviewers have provided mixed feedback, with some praising the dataset's quality and potential impact, while others have raised concerns about the paper's novelty, technical significance, and methodological contributions.\n\nReviewer 1 finds the dataset attractive and the results promising but notes that the methods used are relatively low-cost and wonders how more heavy-cost models would perform. Reviewer 2 suggests that the baseline methods seem too naive and lack ablation study experiments, and also questions the evaluation metric used. Reviewer 3 finds the paper clear and well-written but thinks that the dataset is too similar to others and lacks methodological novelties.\n\nThe authors have responded to the reviewers' comments, providing additional experimental results with the Earthformer model and addressing concerns about the dataset's usability and potential applications.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper's strengths outweigh its weaknesses. The RuDar dataset is a significant contribution to the field of precipitation nowcasting, and its public availability will likely benefit the research community. The paper's clarity and reproducibility are also notable strengths.\n\nWhile the paper may not present groundbreaking methodological novelties, its focus on providing a high-quality dataset and evaluating baseline models is a valuable contribution to the field. The authors' responses to the reviewers' comments have also addressed some of the concerns raised, and the additional experimental results provided will strengthen the paper.\n\nHowever, I also acknowledge that the paper's technical novelty and significance are not extremely high, and some reviewers have raised concerns about the paper's impact and relevance to the top-tier conference.\n\nConsidering all the factors, I believe that the paper is a good fit for a top-tier conference, but it may not be among the most competitive submissions. Nevertheless, the paper's contributions to the field of precipitation nowcasting and its potential impact on the research community justify its publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" proposes a deep learning system to create anonymized representations for data, aiming to preserve privacy in Machine Learning as a Service (MLaaS) settings. The reviewers have raised several concerns that significantly impact the paper's validity and contribution to the field.\n\nFirstly, Reviewer 1 points out that the idea of splitting a neural network into two parts is not new, especially in the context of privacy. This lack of novelty is a significant concern, as top-tier conferences typically expect submissions to present groundbreaking or substantially innovative work. Furthermore, the reviewer criticizes the experimental analysis for being too simplistic, with comparisons limited to only two datasets and a lack of comprehensive benchmarks against other state-of-the-art methods. This limited empirical evaluation fails to convincingly demonstrate the effectiveness and robustness of the proposed approach.\n\nReviewer 2's comments are even more critical, highlighting severe flaws in the proposed approach. The reviewer argues that the method is flawed because it requires fine-tuning the encoder component during the training phase, which can only be done with access to private labels and class centers. This contradicts the goal of preserving privacy, as the server could potentially recreate a surrogate encoder, thereby accessing all the information needed to compromise privacy. Additionally, the reviewer questions the threat model, suggesting that a curious server could easily bypass the privacy-preserving mechanisms by turning off the center loss. These criticisms fundamentally undermine the paper's claims about privacy preservation.\n\nReviewer 3 also expresses concerns about the security model and the risk of information leakage through the use of middle features of neural networks. The reviewer provides a practical example involving physiological time-series data from smart watches, which could still contain trends after encoding, thus potentially leaking sensitive information. This concern highlights the need for a more rigorous security model and a deeper analysis of potential privacy risks.\n\nGiven these criticisms, the paper's technical novelty and significance, as well as its empirical novelty and significance, are marginal at best. The lack of a convincing response from the authors to address these concerns further weakens the paper's position. In the context of a top-tier conference like ICLR, which expects high standards of novelty, technical soundness, and empirical rigor, this paper falls short.\n\nTherefore, considering the severe technical flaws, the lack of novelty, and the insufficient empirical evaluation, the paper does not meet the standards expected for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics\" presents a study on the average-case performance of no-regret learning dynamics, specifically q-replicator dynamics, in potential games. The authors aim to quantify the outcome of optimal learning dynamics via average performance metrics, which couple the regions of attraction with the quality of each attracting point.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Interesting problem**: The paper tackles an important and challenging problem in the field of learning in games, which is the average-case performance of no-regret learning dynamics.\n2. **Solid results**: The authors provide solid theoretical and experimental results, including the proof of pointwise convergence to Nash equilibria and bounds on the average price of anarchy.\n3. **Well-written paper**: The paper is well-organized, and the authors have made an effort to explain the motivation, proof ideas, and results clearly.\n\nHowever, the reviewers also point out several weaknesses:\n\n1. **Lack of comparison with prior work**: The paper does not provide sufficient comparison with prior work, particularly PP16, which is a precursor to this work.\n2. **Limited scope**: The results are limited to 2x2 symmetric coordination games, and the authors do not provide a clear explanation of why their techniques cannot be extended to more general games.\n3. **Minor errors and clarifications**: The reviewers point out minor errors, such as overloading notation, and request clarifications on certain points, such as the factorization of Ψq(x, y).\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional explanations and clarifications. They have also updated the paper to reflect the changes.\n\nAfter carefully considering the reviewers' comments and the authors' response, I believe that the paper has made significant contributions to the field of learning in games. The authors have initiated the analysis of the performance of no-regret algorithms, which is an important and challenging problem. While the results are limited to 2x2 symmetric coordination games, the authors have provided a solid foundation for future work in this area.\n\nHowever, I also agree with the reviewers that the paper could be improved with more comparison with prior work, particularly PP16, and a clearer explanation of why the techniques cannot be extended to more general games.\n\nGiven the strengths and weaknesses of the paper, I believe that it should be accepted for publication, but with the condition that the authors address the minor errors and clarifications pointed out by the reviewers.\n\nFinal Decision: Accept"
    }
  ]
}