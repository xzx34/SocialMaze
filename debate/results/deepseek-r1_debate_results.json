{
  "summary": {
    "model": "deepseek-r1",
    "total_papers": 100,
    "correct_predictions": 88,
    "accuracy": 88.0,
    "confusion_matrix": [
      [
        72,
        4
      ],
      [
        8,
        16
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 94.73684210526315,
        "precision": 90.0
      },
      "Accept": {
        "total": 24,
        "recall": 66.66666666666666,
        "precision": 80.0
      }
    }
  },
  "results": [
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The title suggests that they're using Monte Carlo Tree Search (MCTS) to refine MCTS agents, which sounds a bit meta. The abstract mentions improving the coordination of episode generation by treating the system as a game tree search, aiming to handle the exploration-exploitation trade-off better. They tested it on a small problem and claim better robustness compared to AlphaZero.\n\nNow, the reviewers' comments are pretty harsh. All three reviewers point out major issues. Reviewer 1 mentions the lack of references and poor writing, calling it \"less than half-baked.\" Reviewer 2 can't even understand the abstract and notes grammatical issues everywhere. Reviewer 3 says the paper is incomprehensible with undefined symbols and poor structure.\n\nThe key issues here are clarity and academic rigor. Even if the idea has potential, the paper isn't communicating it effectively. No references is a big red flag; it's essential to situate the work within existing research. The writing problems are severe enough that reviewers couldn't grasp the methodology or contributions. Without understanding, they can't assess the validity or significance of the results. The experiment on a small problem (like tic-tac-toe) might not be convincing for a top-tier conference, which expects more substantial empirical validation.\n\nTop-tier conferences require clear contributions, proper citations, and professional presentation. This paper fails on all fronts. The authors need to address the writing issues, add relevant literature, clarify their methods, and possibly test on more complex problems. As it stands, rejection seems necessary to maintain conference standards.\n</think>\n\n**Reasoning:**  \nThe paper presents an intriguing concept of using MCTS to refine MCTS agents, potentially addressing exploration-exploitation trade-offs in reinforcement learning. However, the reviewers unanimously highlight critical flaws that render the paper unsuitable for a top-tier conference:  \n\n1. **Severe Writing and Clarity Issues:**  \n   - The abstract and paper are described as \"badly written,\" \"incomprehensible,\" and riddled with grammatical errors. Reviewers could not decipher the core methodology or contributions, which is fatal for scientific communication.  \n   - Key terms (e.g., \"coordination of episode generation\") are undefined, and mathematical symbols lack explanations, undermining technical rigor.  \n\n2. **Lack of Academic Foundation:**  \n   - The absence of references or related work violates academic standards, making it impossible to contextualize the work within existing research (e.g., AlphaZero, prior MCTS advancements).  \n\n3. **Methodological and Empirical Weakness:**  \n   - The experiments are limited to a \"small problem\" (likely tic-tac-toe), which is insufficient to demonstrate robustness or scalability for a top-tier venue.  \n   - Algorithm descriptions are unclear, and the paper’s structure (e.g., unexplained blank spaces, undefined algorithms) suggests incompleteness.  \n\n4. **Reviewer Consensus:**  \n   All three reviewers strongly recommend rejection due to the paper’s lack of clarity, missing foundational elements, and unprofessional presentation. These issues prevent meaningful evaluation of the technical contribution.  \n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by evaluating this paper based on the provided information. The paper's title and abstract suggest it's a theoretical analysis of how activation functions affect the training of overparametrized neural networks. The main contribution seems to be differentiating between smooth and non-smooth activations in terms of their impact on the Gram matrix's eigenvalues, which relates to training dynamics. \n\nReviewer 1 points out strengths like the theoretical analysis and experiments but mentions weaknesses such as an overly long appendix and some unclear parts. They also note that the experiments on CIFAR10 had a gap between theory and practice, though ReLU results aligned. The reviewer's main concerns are about the appendix's organization and some technical clarifications.\n\nReviewer 2 is more positive, recommending acceptance. They highlight the thorough theoretical study, clear writing, and validation on synthetic data. Their suggestions are about improving clarity in referencing appendix theorems and discussing real data implications further.\n\nConsidering the strengths: The paper addresses an important gap in understanding activation functions' theoretical impacts, which is relevant for the deep learning community. The theoretical results seem novel, especially the distinction between smooth and non-smooth activations. The inclusion of both synthetic and real data experiments (even with some gaps) adds empirical support. The authors also address prior work, situating their contributions within existing literature.\n\nWeaknesses include the appendix length and some organizational issues. However, these seem manageable with revisions. The CIFAR10 experiment's discrepancy is noted, but the authors acknowledge it, which is a strength in transparency. The theoretical contributions appear solid, and the paper's focus on a key aspect of neural network training (activation functions) is significant for a top-tier conference.\n\nThe reviewers' concerns are mostly about presentation and clarity rather than fundamental flaws. The core theoretical contributions seem strong enough for acceptance, provided the authors address the reviewers' comments in revisions. Since Reviewer 2 recommends acceptance and Reviewer 1's concerns are addressable, the paper likely meets the standards for a top-tier conference.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a rigorous theoretical analysis of how activation functions influence training dynamics in overparametrized neural networks, addressing a significant gap in understanding the role of smoothness in activation functions. The separation of results into non-smooth (e.g., ReLU) and smooth (e.g., tanh) activations, with quantitative bounds on Gram matrix eigenvalues, offers novel insights. The theoretical contributions are well-supported by synthetic experiments and partially validated on CIFAR10, with transparent discussion of gaps. While Reviewer 1 raises concerns about appendix length and clarity, these are offset by the paper’s core strengths: its theoretical depth, alignment with empirical results in key cases, and relevance to the field. Reviewer 2’s endorsement further underscores the paper’s technical soundness and clarity. Minor revisions (e.g., organizing the appendix, clarifying theorem references) would enhance readability but do not undermine the work’s suitability for a top-tier venue. The paper advances the theoretical foundations of deep learning and merits acceptance."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. The title is \"Refining Monte Carlo Tree Search Agents by Monte Carlo Tree Search.\" The abstract mentions using MCTS to control episode generation in reinforcement learning, aiming to improve robustness over Alpha Zero in a small problem like tic-tac-toe.\n\nLooking at the reviewers' comments, all three are negative. Reviewer 1 points out the lack of references and poor writing, calling it \"less than half-baked.\" Reviewer 2 can't understand the paper due to grammatical issues and unclear sentences. Reviewer 3 also mentions poor writing, undefined symbols, and lack of clarity. All agree the paper isn't up to academic standards.\n\nThe main issues here are clarity and academic rigor. The paper doesn't cite previous work, which is a big red flag. Even if the idea has merit, without situating it within existing research, it's hard to assess its contribution. The writing problems make it nearly impossible to understand the methodology and results. The experiments are only on a small problem (tic-tac-toe), which might not demonstrate significant advancement over Alpha Zero. \n\nTop-tier conferences require clear contributions, proper citations, and professional presentation. This paper seems to fail on all fronts. The reviewers couldn't even grasp the core idea due to the writing issues. Without a coherent presentation and proper context, the paper's scientific validity is questionable. Even if there's a good idea buried here, the current form isn't suitable for publication. The authors need to address the writing, add references, clarify their method, and possibly test on more complex problems. Therefore, rejection seems necessary.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper in question proposes a method to refine Monte Carlo Tree Search (MCTS) agents using MCTS itself, aiming to address exploration-exploitation trade-offs in reinforcement learning. While the core idea of meta-optimizing MCTS via MCTS is intriguing, the reviewers’ unanimous criticisms highlight critical flaws that render the paper unsuitable for a top-tier conference:\n\n1. **Lack of Academic Rigor**:  \n   - The absence of references and failure to contextualize the work within existing literature (e.g., AlphaZero, prior MCTS improvements) undermines the paper’s credibility. This omission makes it impossible to assess novelty or contribution relative to the state of the art.  \n   - The experiments are limited to a \"small problem\" (likely tic-tac-toe), which is insufficient to demonstrate scalability or meaningful advancement over AlphaZero. No quantitative results or baselines are provided to validate claims of \"robust performance.\"\n\n2. **Severe Writing and Presentation Issues**:  \n   - All reviewers note pervasive grammatical errors, unclear phrasing, and incoherent passages. For example, the abstract’s statement about \"coordination of episode generation\" is vague and unexplained.  \n   - The paper’s structure is described as unprofessional, with undefined mathematical symbols, incomplete algorithms, and excessive blank space. These issues prevent readers from understanding the method or reproducing the work.\n\n3. **Insufficient Technical Clarity**:  \n   - The central contribution—using MCTS to control episode generation—is not clearly articulated. Reviewers could not discern how the proposed \"master game tree\" differs from standard MCTS or addresses exploration-exploitation trade-offs.  \n   - The lack of formalism (e.g., no problem definition, reward structure, or pseudocode) further obscures the technical approach.\n\n4. **Reviewer Consensus**:  \n   All three reviewers explicitly recommend rejection due to the paper’s incomplete, poorly written, and unsubstantiated nature. Their inability to parse the paper’s intent or value despite careful reading underscores its inadequacy for publication.\n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviewers' comments. The paper proposes a hierarchical reinforcement learning approach for coordinating multiple agents (like robot arms) using pre-learned skills with diversity. The main idea is to train each agent's skills separately with a diversity objective and then coordinate them with a meta-policy. The experiments show it works better than baselines in tasks like bimanual manipulation.\n\nReviewer 1 thinks the paper is interesting but has some concerns. They mention issues with notation clarity, the choice of T_low=1 in some environments (which might make skills too short), high variance in training curves, and the need for more discussion on multi-agent methods. They also want success rates instead of just episode rewards.\n\nReviewer 2 is confused about the novelty in multi-agent vs. single-agent settings. They question why DIAYN (a single-agent method) is used here and whether temporal abstraction in multi-agent is different. They also have technical questions about the skill learning and the fixed T_low parameter.\n\nReviewer 3 likes the approach but is concerned about the need for predefined subtasks. They suggest comparing with a DIAYN-only baseline and mention the latent vector size isn't specified. They give a weak accept.\n\nNow, considering all this, the paper's strengths are the novel application of skill diversity in multi-agent coordination and solid empirical results. However, there are valid concerns about clarity, technical details, and baselines. The authors addressed some points in their responses, but some issues like high variance and the need for more multi-agent discussion remain. The method's reliance on predefined subtasks might limit its applicability, which is a valid criticism. However, the approach seems effective in the tested scenarios. Given that the reviewers lean towards acceptance with revisions (especially Reviewer 1 and 3), and the paper's contributions are relevant to the field, I think it's borderline but acceptable for a top-tier conference if the authors can address the major concerns in the final version.\n</think>\n\nThe paper presents a modular hierarchical reinforcement learning framework for coordinating multi-agent skills through diversity-driven pre-training. While the approach is technically sound and addresses an important problem in robotics and multi-agent systems, several critical issues raised by reviewers warrant careful consideration:\n\n**Strengths**:  \n1. **Novelty**: The integration of skill diversification (via maximum entropy objectives) with hierarchical coordination introduces a fresh perspective on multi-agent skill composition. The idea of leveraging latent-conditioned skill diversity to enable adaptive coordination is compelling.  \n2. **Empirical Validation**: The method demonstrates clear performance improvements over baselines in challenging bimanual manipulation and multi-agent locomotion tasks, with ablation studies validating design choices.  \n3. **Practical Relevance**: The framework’s modularity aligns with real-world robotic training paradigms (e.g., practicing sub-skills before coordination), enhancing its applicability.  \n\n**Weaknesses and Concerns**:  \n1. **Technical Clarity**: Reviewers noted inconsistent notation and insufficient details in Figure 2, which hinder reproducibility. The fixed \\( T_{low} \\) parameter (especially \\( T_{low}=1 \\)) risks reducing skills to primitive actions rather than temporally extended behaviors, requiring deeper justification.  \n2. **Methodological Limitations**: The reliance on predefined subtasks (e.g., \"pick,\" \"push\") limits generalizability, as noted by Reviewer 3. A comparison to a DIAYN-only baseline would strengthen the case for the hybrid approach.  \n3. **Experimental Rigor**: High variance in training curves (Fig. 4) and missing success-rate metrics (vs. episode rewards) weaken confidence in the results. Additional seeds and metric standardization are needed.  \n4. **Multi-Agent Contextualization**: The discussion of multi-agent coordination vs. single-agent temporal abstraction is underdeveloped. Reviewer 2’s concerns about the applicability of single-agent skill discovery (DIAYN) to multi-agent settings remain partially unresolved.  \n\n**Impact vs. Limitations**:  \nWhile the paper’s core idea is promising, the reliance on predefined subtasks and insufficient exploration of multi-agent-specific challenges (e.g., emergent coordination dynamics) may limit its broader impact. However, the empirical results in complex environments and the modular design’s practicality justify consideration for acceptance, contingent on addressing key concerns in revisions.\n\n**Final Decision**: Accept  \nThe paper’s novel approach to skill coordination and strong empirical results outweigh its current limitations, provided the authors address clarity issues, expand multi-agent discussions, and improve experimental rigor in the final version. The work aligns with top-tier conference standards by advancing hierarchical RL for real-world robotics applications."
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The title mentions depth-width trade-offs for ReLU networks using Sharkovsky's Theorem. That's interesting because Sharkovsky's Theorem is from dynamical systems and chaos theory, which isn't something I see every day in neural network papers. The abstract says they're addressing an open question from Telgarsky's work by connecting periodic points in functions to the representational power of ReLU networks. They provide lower bounds on width based on depth using eigenvalue analysis. That sounds novel and potentially significant.\n\nReviewer 1 is already at the maximum score, so they must have been very positive. They mentioned the synthetic dataset, which the authors included. That's a good sign because it shows empirical support, even if it's synthetic. But Reviewer 1 didn't raise any issues, so maybe they were already convinced.\n\nReviewer 2 loved the paper, calling it a delight to read and praising the motivating examples and exposition. They suggest acceptance. Their questions are about the bias term's effect and the sharpness of the lower bounds. The authors might have addressed these in their responses, but since the reviewer still recommends acceptance, maybe the answers were satisfactory. The second question about a plot showing classification error vs. depth for a synthetic task is a good point. If the authors didn't include such a plot, maybe that's a minor weakness, but Reviewer 2 still thinks it's strong enough without it.\n\nReviewer 3 also finds the paper serious and well-written but has structural criticisms. They suggest moving the main theorem earlier to highlight the contribution before diving into the math. That's a valid point because clarity and structure are important for readability. They also mention that the lower bound, while nice, isn't very useful in practice because assessing the period in real tasks is hard. But theoretical contributions don't always need immediate practical applications; deepening understanding is valuable. The reviewer still acknowledges the clever application of Sharkovsky's Theorem.\n\nPutting this together: The paper presents a novel theoretical connection between dynamical systems and neural network expressivity, addressing an open question from prior work. The reviewers are positive, with minor suggestions but no major flaws. The main concerns are about structure and practical applicability, but these are common in theoretical papers. The use of Sharkovsky's Theorem is original and seems well-executed. The lower bounds contribute to the theoretical understanding of depth-width trade-offs, which is a fundamental question in deep learning. Even if the practical utility isn't immediate, the theoretical insight is significant for the field. The authors responded well to reviewers, including a synthetic dataset as requested. Considering all this, the paper should be accepted. The strengths in originality, technical correctness, and addressing an open problem outweigh the minor structural issues and theoretical nature.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel theoretical connection between Sharkovsky's Theorem (from dynamical systems) and depth-width trade-offs in ReLU networks, directly addressing an open question posed by Telgarsky. The work is rigorous, with clean proofs and a compelling application of non-trivial mathematical tools to machine learning. While Reviewer 3 raises valid concerns about structural organization and practical utility, these do not outweigh the paper’s strengths:  \n1. **Significance:** The paper provides a principled characterization of functions that are hard to approximate with shallow networks, advancing the theoretical understanding of neural network expressivity.  \n2. **Originality:** Leveraging Sharkovsky’s Theorem and periodic points to analyze ReLU networks is a creative and underexplored approach.  \n3. **Technical Soundness:** Reviewers 2 and 3 affirm the correctness of the proofs, and the authors addressed Reviewer 2’s empirical curiosity with a synthetic dataset.  \n4. **Responsiveness:** The authors engaged constructively with reviewer feedback (e.g., adding synthetic experiments).  \n\nMinor weaknesses (e.g., structural adjustments suggested by Reviewer 3) can be addressed in revisions. Theoretical contributions of this nature, even if not immediately practical, are valuable for foundational understanding in deep learning. The paper meets the standards of a top-tier conference."
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by looking at the paper's contributions and the reviewers' comments. The paper introduces a domain adaptation method using a multi-branch network where each domain can have different computational paths. The main idea is that different domains might need different processing, which is a novel approach compared to existing methods that use the same processing for all domains.\n\nReviewer 1 had minor issues like terminology and references but overall found the motivation clear and the experiments convincing. They suggested linking to more transfer learning techniques outside deep learning and avoiding arXiv references. The authors might have addressed some of these in revisions since Reviewer 1 is happy post-discussion.\n\nReviewer 2 points out that while the idea of adaptive computation isn't new, applying it to domain adaptation is novel. However, they note that the experiments don't show varying computation (same layers for all domains) and the baselines aren't state-of-the-art. They also mention the need to explain why all domains end up with different computations. The authors need to address these points, but Reviewer 2's issues were resolved after discussion, so maybe the authors provided some clarifications.\n\nReviewer 3 appreciates the idea but criticizes the lack of comparison with SOTA methods like CDAN and insufficient ablation studies. They also mention the need for multi-domain experiments. The baseline results being lower than other papers is a concern. These are significant issues because without comparing to current methods, it's hard to assess the contribution's impact.\n\nThe main strengths are the novel approach and the framework's flexibility. The weaknesses are the experimental validation. The paper's experiments are not comprehensive enough. They didn't compare with top methods, didn't explore hyperparameters (like number of branches), and didn't test multi-domain scenarios. Also, the baselines used (DANN, RPT) are outdated. For a top-tier conference, experiments need to be thorough to prove the method's effectiveness beyond existing approaches.\n\nThe authors might argue that their focus is on the architecture rather than beating SOTA, but Reviewer 3's point about combining with SOTA methods (like CDAN) is valid. Without that, the practical benefit is unclear. The rebuttal might have addressed some concerns, but the reviewers still note unresolved issues, especially regarding experiments.\n\nGiven that two reviewers (2 and 3) have substantial concerns about the experiments, even after discussion, and the lack of SOTA comparisons, the paper might not meet the high bar of a top-tier conference. However, Reviewer 2's final stance was that their issues were resolved, and Reviewer 1 was positive. But Reviewer 3's points about missing comparisons and ablation studies are critical. Since the paper's core idea is novel but the experimental validation is lacking, it's on the borderline. However, top conferences often require strong empirical results. Without those, even a good idea might not be enough. The decision could lean towards rejection unless the authors provided convincing responses, but based on the reviewers' comments, especially Reviewer 3's unresolved issues, the paper might not be accepted.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel approach to domain adaptation by introducing a multi-branch network that allows domain-specific computational paths, addressing a valid limitation of existing methods. However, critical flaws in experimental validation and comparison undermine its suitability for a top-tier conference:  \n1. **Lack of SOTA Comparisons:** The experiments compare only against outdated baselines (e.g., DANN, RPT) and omit state-of-the-art methods (e.g., CDAN). This makes it impossible to assess the method’s practical impact relative to current research.  \n2. **Insufficient Ablation Studies:** Key hyperparameters (e.g., number of branches, computational unit configurations) and failure modes (e.g., gate activation stability) are not rigorously analyzed, leaving the method’s robustness and scalability unclear.  \n3. **Unvalidated Claims:** The claim that \"more complex domains require more computation\" is not empirically tested (e.g., using variable-depth branches), weakening the core motivation.  \n4. **Incomplete Multi-Domain Analysis:** While the framework claims to handle multiple domains, experiments focus on single-source adaptation, missing a critical opportunity to validate scalability.  \n\nWhile the idea is innovative and the theoretical framework is sound, the experimental section does not meet the rigor expected at a top conference. Reviewer 3’s concerns about missing comparisons and ablation studies remain unresolved, and the rebuttal phase did not fully address these gaps. For these reasons, the paper does not yet meet the acceptance threshold."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's contributions. The authors are applying machine learning, specifically bidirectional LSTMs, to predict DNA folding patterns using epigenetic data. They claim their model outperforms traditional methods like linear models and gradient boosting. The reviewers have several concerns, so I need to weigh those.\n\nReviewer 1's main issue is whether the improvement is due to the LSTM or the custom loss function (wMSE). The authors didn't compare the LSTM with and without their proposed loss, or apply the same loss to other models. That's a valid point. If the loss function is the main driver, then the model's architecture might not be the key factor. The lack of generalizability tests (other datasets or features) is another concern.\n\nReviewer 2 points out clarity issues in the paper, like unclear equations and figure labels. They also mention the need to explain why regression vs. neural networks work better here. The discussion on errors in Figure 6 is important but not elaborated. These are mostly presentation issues but could affect the paper's readability and scientific rigor.\n\nReviewer 3 is harsher, citing lack of novelty, unclear methods, and insufficient evaluation. They mention that the approach is similar to prior work without proper citation. The data preprocessing and feature representation are unclear (e.g., how 20kb segments are handled). The choice of biLSTM over other architectures like CNNs isn't justified. Also, the evaluation metrics are limited, and hyperparameter tuning isn't discussed. Feature importance analysis is only done for linear models, not the LSTM, which is a big gap.\n\nPutting this together: The paper applies a known model (biLSTM) to a biological problem. The novelty seems limited, as similar ML applications in genomics exist. The methodological details are lacking, making it hard to reproduce. The comparison with other models isn't fair if the loss functions differ. The reviewers also point out missing baselines (like CNNs) and insufficient analysis of why biLSTM works here. The authors need to address these points to show their contribution is novel and robust.\n\nThe paper's strengths are the application to Drosophila data and identifying informative epigenetic features. However, the weaknesses in methodology clarity, novelty, and evaluation are significant. For a top-tier conference, the paper needs to demonstrate both technical soundness and novel contributions. The current issues, especially regarding novelty and thorough evaluation, might not meet the high standards required. The reviewers' concerns, especially Reviewer 3's points on prior work and methodology, are critical. Without addressing these, the paper might not be suitable for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nWhile the paper applies machine learning to an important biological problem (predicting DNA folding patterns) and demonstrates that bidirectional LSTMs outperform traditional models, several critical issues prevent it from meeting the standards of a top-tier conference:  \n\n1. **Lack of Methodological Novelty:** The core approach (using biLSTMs for genomic data) and the proposed weighted MSE loss lack sufficient novelty. Reviewer 3 notes similarities to prior work, and the authors fail to cite or differentiate their contributions from existing methods. The choice of biLSTM over non-recurrent architectures (e.g., CNNs) is not rigorously justified, weakening the biological and technical rationale.  \n\n2. **Unfair Comparisons and Evaluation Gaps:**  \n   - The performance improvement attributed to the biLSTM may stem from the custom loss function rather than the model architecture. The authors do not compare the biLSTM with other models trained on the same loss or evaluate the biLSTM with a standard loss.  \n   - Evaluation metrics are limited (only weighted MSE is emphasized), and critical baselines (e.g., CNNs, attention-based models) are missing.  \n\n3. **Technical Clarity and Reproducibility:**  \n   - Key details about data preprocessing (e.g., handling of genomic segments, feature representation) and hyperparameter tuning are unclear.  \n   - The feature importance analysis is limited to linear models, ignoring the biLSTM’s learned patterns, which undermines the biological insights claimed.  \n\n4. **Generalizability Concerns:** The paper does not validate the approach on other datasets (e.g., human Hi-C data) or epigenetic features (e.g., ATAC-seq), raising doubts about broader applicability.  \n\nWhile the application domain is compelling, the unresolved methodological and presentation issues, combined with insufficient novelty and evaluation rigor, justify rejection. Addressing these concerns would require substantial revisions beyond the current scope."
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. The paper is about generating singing voices without using scores or lyrics, which is a novel idea. The authors proposed three different schemes: free singer, accompanied singer, and solo singer. They used GANs and developed some custom metrics for evaluation.\n\nLooking at the reviewers' comments, there are several concerns. Reviewer 1 is worried about the evaluation metrics not properly assessing singing quality. They mention that vocalness and average pitch don't capture aspects of singing. Also, the audio samples were hard to evaluate. Reviewer 2 points out the lack of comparison with existing methods, like using extracted scores with state-of-the-art synthesis models. They also mention issues with dataset clarity and motivation for the three tasks. Reviewer 3 criticizes the evaluation methods, especially the use of CREPE for pitch tracking and the vocalness metric. They also question some technical choices. Reviewer 4 thinks the paper tries to tackle too many problems without sufficient justification for the architecture and lacks baselines. Reviewer 5 suggests comparing with MIDI-based generation and questions the choice of working with raw waveforms.\n\nThe common themes are: weak evaluation metrics, lack of baselines/comparisons, unclear motivation for task setups, dataset issues, and some technical concerns. The strengths are the novelty of the problem, the effort in creating a dataset, and providing code. However, the evaluation is a major issue. Without proper metrics or comparisons, it's hard to judge the contribution. The authors didn't address how their approach compares to existing methods, even if those methods use scores. They could have used existing datasets like MIR-1k for cleaner evaluation. The reviewers also noted that the motivation for the three different schemes isn't well-explained, and the metrics like vocalness and average pitch might not be sufficient. The lack of ablation studies and justification for architectural choices (like using BEGAN for sequences) adds to the concerns.\n\nIn top-tier conferences, even novel ideas need solid evaluation and clear motivation. The paper introduces a new task but doesn't convincingly show that their approach is effective. The reviewers' concerns about evaluation and baselines are significant. While the problem is interesting, the execution seems lacking in critical areas. Therefore, despite the novelty, the paper might not meet the acceptance threshold due to these unresolved issues.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper introduces a novel task of score/lyrics-free singing voice generation and proposes a GAN-based pipeline with three distinct generation schemes. While the problem is innovative and the technical effort is substantial, several critical flaws highlighted by reviewers undermine its suitability for a top-tier conference:  \n\n1. **Weak Evaluation:**  \n   - Custom metrics (vocalness, average pitch) are insufficient to assess singing quality (e.g., melodic coherence, harmony with accompaniment).  \n   - No comparison to baselines (e.g., MIDI-based generation or score-conditioned synthesis using extracted scores) limits validation of the method’s effectiveness.  \n   - Subjective audio samples lack context (e.g., no comparison to real data or prior work), making qualitative assessment difficult.  \n\n2. **Motivation and Task Design:**  \n   - The three proposed tasks (free, accompanied, solo singer) are not rigorously justified. The paper does not clarify why these specific settings are meaningful or how they advance the field beyond existing synthesis approaches.  \n\n3. **Dataset and Reproducibility:**  \n   - Training data details (genres, instrumentation, splits) are unclear, raising concerns about domain bias and generalizability.  \n   - Reliance on in-house datasets and lack of comparison to established datasets (e.g., MIR-1k) weaken reproducibility.  \n\n4. **Technical Concerns:**  \n   - Architectural choices (e.g., adapting BEGAN for sequences, GRU/dilated convolutions) lack ablation studies or citations to justify design decisions.  \n   - Reviewer critiques of pitch tracking (CREPE) and vocalness metrics highlight methodological weaknesses.  \n\n5. **Writing and Presentation:**  \n   - Key sections (e.g., limitations, task motivation) are underdeveloped, and minor errors (e.g., \"24 keys\") reduce clarity.  \n\nWhile the problem is promising and the authors have demonstrated technical effort, the absence of rigorous evaluation, baselines, and task motivation—coupled with unresolved methodological questions—renders the paper premature for acceptance. Addressing these issues would require significant revisions, including comparisons to existing methods, improved metrics, and clearer justification of the task design."
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by looking at the paper and the reviewers' comments. The paper proposes an adversarial sampling method for extreme classification, aiming to improve training efficiency by reducing gradient variance. The main contributions are the adversarial sampling mechanism, a proof on gradient variance minimization, and experimental results showing faster training.\n\nReviewer 1 is positive, finding the paper educative and the results convincing. They recommend acceptance. Reviewer 2 acknowledges the contributions but points out missing related works and some technical issues. They mention prior works that do similar things with logarithmic cost, which the authors didn't compare. Also, they note the experiments lack comparison with the full softmax and have some typos. Reviewer 3 is more critical, pointing out that the experiments miss state-of-the-art baselines like Slice and DiSMEC, don't evaluate on tail labels or multi-label settings, and suggests testing on more datasets.\n\nThe key issues here are the missed comparisons with recent SOTA methods and the limited experimental validation. The authors didn't address important works like Slice, which could be a strong baseline. Also, not comparing with DiSMEC, which might perform better even if slower, leaves a gap in understanding the trade-offs. The lack of evaluation on tail labels and multi-label scenarios is another concern, as extreme classification often deals with imbalanced data. \n\nThe authors' response would be crucial here. If they addressed these points in revisions, maybe acceptance is possible. But since this is the initial submission, the reviewers' concerns about missing comparisons and insufficient experiments are valid. For a top-tier conference, thorough experimentation and situating the work within existing literature are essential. The paper's technical contributions are there, but the experimental section's weaknesses might undermine its impact. Without addressing these, the paper might not meet the high standards required. So, leaning towards reject unless major revisions are made, but since this is a conference decision, it's likely a reject.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel adversarial sampling approach for efficient training in extreme classification, offering theoretical guarantees and empirical improvements. However, several critical issues raised by the reviewers hinder its readiness for acceptance at a top-tier conference:\n\n1. **Incomplete Related Work and Baselines:**\n   - Reviewer 2 and 3 highlight the omission of key prior works (e.g., [1], [3], [4], Slice, DiSMEC) that address similar challenges. The authors do not adequately contextualize their method against these approaches, particularly those achieving logarithmic sampling costs or state-of-the-art performance. This undermines the paper’s novelty and contribution claims.\n\n2. **Limited Experimental Validation:**\n   - The experiments focus on only two datasets (Wikipedia-500K and Amazon-670K) and exclude critical comparisons:\n     - **Full softmax performance:** Reviewer 2 notes the absence of comparisons with the exact softmax loss on smaller datasets, making it unclear how much accuracy is sacrificed for speed.\n     - **SOTA baselines:** Reviewer 3 emphasizes missing comparisons with Slice and DiSMEC, which are established benchmarks. For instance, DiSMEC reportedly outperforms the proposed method on Wiki-500K (19% vs. the paper’s results), raising questions about the method’s effectiveness.\n     - **Tail-label performance:** The paper does not evaluate metrics like Macro-F1 to assess performance on rare classes, a critical aspect of extreme classification.\n\n3. **Technical and Presentation Issues:**\n   - Typos and unclear phrasing (e.g., \"several two classifications\") weaken the presentation.\n   - The auxiliary adversarial model’s computational overhead is not rigorously analyzed. While the authors claim logarithmic cost, Reviewer 3 questions whether this holds in multi-label settings or with dynamic updates to the adversarial model.\n\n4. **Scope and Generalizability:**\n   - The method’s applicability to multi-label classification (a common use case in extreme classification) is unexplored. Reviewer 3 argues this limits the work’s impact compared to alternatives like Slice or AttentionXML.\n\n**Strengths:**\n- The adversarial sampling mechanism and theoretical analysis of gradient variance minimization are technically sound and novel.\n- The wall-time improvements over uniform sampling baselines (e.g., AandR, NCE) are significant and practically relevant.\n\n**Conclusion:**\nWhile the paper introduces a promising idea, the lack of thorough empirical validation against relevant baselines, incomplete discussion of prior work, and limited scope (e.g., single-label focus) make it insufficient for a top-tier conference. Addressing these issues would require substantial revisions, including expanded experiments and comparisons, which are beyond the scope of a conference review cycle.\n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes ROOTS, an unsupervised model for 3D scene decomposition and rendering, building on GQN. The main contributions seem to be object-oriented decomposition and hierarchical representations. The reviewers have a mix of positive and negative points.\n\nReviewer 1 initially rejected due to writing quality but upgraded to a 6 after revisions. They pointed out issues like incorrect claims of being the first, notation problems, and some technical questions. The authors addressed some, but writing still needs work.\n\nReviewer 2 mentions the paper's potential but criticizes unsupported claims, lack of clarity in figures, and missing comparisons. They also note the experiments are weak. The authors added some experiments but maybe not enough.\n\nReviewer 3 highlights lack of citations, missing ablation studies, and insufficient comparison with existing work. They feel the contribution is minor over GQN. The authors added some references but maybe not enough in 3D vision literature.\n\nReviewer 4 is critical of the method's complexity, weak experiments, and broad claims. They stress the need for better related work positioning and ablation studies. The authors tried to address clarity but experiments still lack depth, like proper ablations and fair comparisons.\n\nThe Area Chair (AC) notes that while revisions improved clarity, experiments are still weak. They're concerned about lack of ablation studies and whether the improvements over GQN are due to the method's components or other factors. The AC is inclined to reject based on experimental shortcomings.\n\nKey issues: \n\n1. **Clarity**: Though improved, some parts remain unclear. Complex methods need clear explanations, which might still be lacking.\n\n2. **Experiments**: The improvements over GQN are marginal (small MSE differences). No ablation studies to show which components matter. Object detection metrics lack baselines. Generalization results are mixed.\n\n3. **Citations and Related Work**: Not sufficiently engaging with existing 3D vision literature. Claims of being the first are problematic without proper context.\n\n4. **Methodological Soundness**: Questions about whether the model's benefits are due to architecture changes rather than the object-oriented approach. Lack of ablation makes it hard to validate.\n\nThe AC's main concern is the experimental validation. Without ablation studies and stronger evidence of the method's unique contributions, it's hard to justify acceptance. The qualitative results are nice but not enough for a top-tier conference where empirical rigor is crucial. The authors addressed some issues but key weaknesses in experiments and comparisons remain. Given the high standards, rejection seems appropriate.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents an innovative approach to unsupervised 3D scene decomposition, building on the GQN framework with object-oriented and hierarchical representations. While the core idea is promising, critical flaws in experimental validation, clarity, and positioning within related work undermine its suitability for a top-tier conference. Key issues include:  \n\n1. **Weak Experimental Evidence**:  \n   - Marginal quantitative improvements over GQN (e.g., small MSE gains in Table 1) lack statistical significance and are insufficient to demonstrate clear superiority.  \n   - No ablation studies to isolate the impact of key components (e.g., hierarchical representations, 3D inductive biases), leaving the necessity of the proposed architecture unproven.  \n   - Object detection metrics (Table 2) lack baseline comparisons (e.g., GQN or simple heuristics), making results uninterpretable.  \n   - Generalization experiments show inconsistent performance (ROOTS underperforms GQN in 3/9 cases), contradicting claims of improved compositional reasoning.  \n\n2. **Insufficient Engagement with Related Work**:  \n   - The paper overlooks foundational and recent work in 3D scene understanding (e.g., 3D-R2N2, DeepVoxels, multi-view stereo methods), weakening its novelty claims.  \n   - Broad assertions like \"first unsupervised model for 3D object identification\" are inaccurate without proper qualification, given prior work in voxel segmentation and point-cloud processing.  \n\n3. **Clarity and Reproducibility**:  \n   - Despite revisions, the method remains ambiguously described (e.g., undefined terms like STN-INV, inconsistent notation, missing details for \\( f_{3D\\rightarrow2D} \\)).  \n   - The appendix fails to resolve critical ambiguities, hindering reproducibility.  \n\n4. **Methodological Concerns**:  \n   - Training protocols (e.g., capacity matching between ROOTS and GQN, convergence checks) are inadequately addressed, raising doubts about fairness in comparisons.  \n   - The reliance on synthetic Mujoco data limits real-world applicability, yet no discussion of scalability or robustness is provided.  \n\nWhile the qualitative results (e.g., object swapping, viewpoint invariance) are compelling, they do not compensate for the lack of rigorous empirical validation. Top-tier conferences demand robust evidence of both technical novelty and practical efficacy, which this work currently lacks. A rejection is warranted, with encouragement to address these issues in a future submission."
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper's main contribution is introducing two regularizers to improve the tightness of convex relaxations for certifiable robustness in neural networks. The abstract claims that their regularizers lead to tighter certification bounds compared to non-regularized baselines. \n\nReviewer 1 raises several technical concerns. They point out that the analysis in Sections 4.1 and 4.3 might be confusing or flawed. For example, they mention that the authors might be conflating the original problem with its convex relaxation, leading to incorrect conclusions. Also, the reviewer questions the practicality of the regularizers, suggesting that the conditions for tight bounds (like few ambiguous ReLUs) might be too restrictive. The experiments are criticized for missing key baselines like Gowal et al., which could show if the improvements are significant or just incremental.\n\nReviewer 2 is confused about the theoretical justification. They're unclear on how the proposed regularizers differ from existing methods like Fast-Lin and why using a looser relaxation would help. There's a concern about whether the optimal perturbation found via the relaxed model is valid within the input bounds. The experiments also need clarification on which relaxations were used in comparisons, and whether the regularizers could be applied to tighter relaxations.\n\nReviewer 3 thinks the problem is interesting and the results promising but criticizes the use of only MNIST, which is seen as insufficient for a top-tier conference. They also note presentation issues but lean towards weak accept.\n\nPutting this together, the paper's strengths are addressing an important problem (closing the gap between empirical and certifiable robustness) and showing empirical improvements. However, the weaknesses are significant. The theoretical analysis has potential flaws and unclear explanations, as pointed out by Reviewers 1 and 2. The experiments are limited in scope (only MNIST) and missing key baselines, which makes it hard to assess the true contribution. The confusion in the methodology sections suggests that the paper's claims might not be as strong as presented.\n\nFor a top-tier conference, theoretical rigor and clear empirical validation are crucial. The technical issues raised by the reviewers, especially the confusion around the convex relaxations and the missing comparisons, are major red flags. While the idea of using regularizers to tighten bounds is interesting, the current presentation and evaluation don't sufficiently support the claims. The lack of experiments on more complex datasets (like CIFAR-10) also limits the impact, as MNIST is often considered too simple for robust ML research nowadays.\n\nTherefore, despite the potential, the paper in its current form has significant shortcomings that would need addressing. Given the high standards of a top-tier conference, these issues warrant rejection.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper proposes novel regularizers to tighten convex relaxation bounds for certified robustness, addressing an important gap in adversarial training. However, significant issues raised by reviewers undermine its suitability for a top-tier venue:  \n\n1. **Theoretical Clarity and Validity:**  \n   - Multiple reviewers highlight confusion in the analysis (e.g., conflating the original non-convex problem with its convex relaxation in Sections 4.1–4.3). Key claims about the feasibility of achieving tight bounds rely on restrictive assumptions (e.g., few ambiguous ReLUs), which are not sufficiently justified.  \n   - The connection between the proposed regularizers and existing methods (e.g., Fast-Lin, CROWN-IBP) is unclear, raising doubts about novelty and theoretical grounding.  \n\n2. **Empirical Evaluation:**  \n   - Experiments are limited to MNIST, a dataset considered insufficiently challenging for modern robustness research. Lack of results on standard benchmarks like CIFAR-10 weakens the claim of broad applicability.  \n   - Critical baselines (e.g., Gowal et al.) are omitted, making it difficult to assess whether improvements are meaningful or incremental.  \n\n3. **Presentation and Technical Accuracy:**  \n   - Reviewer 1 identifies citation errors, inconsistent terminology, and ambiguities in problem formulation (e.g., confusion between “optimal convex relaxation” and the original problem).  \n   - Reviewer 2 questions the validity of the regularization approach, noting potential inconsistencies in how perturbations are derived from relaxed models.  \n\nWhile the problem is relevant and the idea of regularizing for tighter bounds is promising, the paper’s theoretical gaps, limited experimentation, and unclear comparisons to state-of-the-art methods render it premature for acceptance at a top-tier conference. Addressing these issues would require a substantial revision, including expanded experiments, clearer theoretical framing, and direct comparisons with key baselines."
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper and the reviewers' comments. The paper presents an FPTAS for learning Mahalanobis metrics by minimizing violated constraints using geometric algorithms. The reviewers have a mix of concerns and some positive points.\n\nReviewer 1 is not an expert but notes that the paper lacks a clear comparison with recent works and has some technical questions about the proofs and figures. They also mention the conclusion is missing. The missing conclusion is a red flag because it's a standard section, and its absence might indicate the paper is incomplete. The questions about Lemma 2.1 and Figure 4 need addressing. If the authors didn't explain why adding constraints makes the problem infeasible, that's a gap in the logic. Also, non-monotonic running time in Figure 4 could be due to varying problem complexities, but the authors should clarify.\n\nReviewer 2 finds the paper interesting but points out missing elements like hardness discussion, definition of terms, and clarity on whether the FPTAS assumes constant dimension. The reviewer also notes that the algorithm's relation to prior work isn't clearly stated. These are valid points. If the FPTAS's runtime depends on d being fixed, that's a limitation that should be explicit. Not discussing computational hardness might leave readers wondering about the problem's inherent difficulty. The technical comments about definitions and algorithm details (like Exact-LPTML) suggest the paper might be lacking in clarity, which is crucial for a top-tier conference.\n\nReviewer 3 is more critical, questioning the novelty and practical benefits. They argue that transforming the problem into LP isn't novel and that experiments don't show improvement. The experiments are key here—if the method doesn't outperform existing ones, the contribution is mainly theoretical. However, the authors claim favorable performance in adversarial noise, which might be a plus. But Reviewer 3's point about unclear theoretical guarantees for the approximation needs addressing. If the approximation method isn't novel, the paper's contribution is weakened.\n\nSynthesizing the reviews: The paper introduces a theoretical algorithm with an FPTAS, which is a strong point. However, the presentation has issues—missing sections (conclusion), unclear comparisons with SOTA, lack of hardness discussion, and possible lack of experimental superiority. The reviewers' technical questions indicate that the proofs and explanations might be insufficient. For a top-tier conference, theoretical contributions need to be both novel and well-presented. The authors' response to these critiques isn't provided, so we have to assume they haven't addressed them yet.\n\nThe main strengths are the FPTAS result and practical implementation. Weaknesses include unclear novelty, missing discussions, and experimental results that don't clearly demonstrate advantages. The missing conclusion is a minor issue but points to possible rushed writing. The key question is whether the theoretical contribution (FPTAS for this problem) is significant enough despite the presentation flaws. Since the problem is well-motivated and FPTAS in low dimensions is a strong result, but the reviewers point out that the paper doesn't situate itself well within existing literature and lacks clarity in some arguments. For a top-tier conference, these issues might lead to rejection unless the authors can adequately address them. Since the reviews are mixed and the concerns are substantial, especially regarding novelty and experimental validation, the paper might not meet the high bar required.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a theoretically grounded approach for Mahalanobis metric learning with an FPTAS, which is a notable contribution. However, significant concerns raised by reviewers undermine its suitability for a top-tier conference:  \n1. **Novelty and Positioning**: The paper inadequately clarifies its novelty compared to existing methods (e.g., ITML, LMNN) and recent work (e.g., Verma & Branson 2015). Reviewer 3 questions whether the LP formulation is sufficiently innovative, and the authors fail to articulate computational/theoretical advantages over prior convex optimization approaches.  \n2. **Technical Gaps**:  \n   - Missing hardness analysis (Reviewer 2) and incomplete proofs (e.g., Lemma 2.1’s infeasibility claim, non-monotonic runtime in Figure 4).  \n   - Ambiguity in definitions (e.g., \"accuracy\" in Theorem 1.1, \"combinatorial dimension\") and algorithmic details (e.g., Exact-LPTML).  \n3. **Experiments**: Results do not convincingly demonstrate superiority over baselines, especially in accuracy. While adversarial noise performance is noted, the lack of clear practical benefits weakens the case for impact.  \n4. **Presentation Issues**: Absence of a conclusion (Reviewer 1) and insufficient discussion of limitations (e.g., dependence on fixed dimension for FPTAS) reduce clarity and completeness.  \n\nWhile the FPTAS result is theoretically valuable, the unresolved issues in methodology, evaluation, and exposition prevent the paper from meeting the rigorous standards of a top-tier venue."
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about improving disentanglement in hierarchical representations using a progressive learning approach with VAEs. The title and abstract suggest they combine progressive learning (like progressively growing networks) with disentanglement in a hierarchical VAE setup. The reviewers had some concerns but after the rebuttal, some of them were addressed.\n\nReviewer 1 initially had issues with the training of the first layer compared to VLAE and wanted more experiments. The authors added experiments showing pro-VLAE performs better, which made Reviewer 1 upgrade to a weak accept. Reviewer 2 was more critical, pointing out unclear purpose, unconvincing experiments, and similarity to prior work. They also mentioned issues with the new metric and lack of ablation studies. However, the rebuttal addressed some concerns, but Reviewer 2 still thought the paper wasn't ready. Reviewer 3 liked the idea, found it fresh, and had some minor comments. Reviewer 4 thought the paper was well-motivated and results were convincing but wanted more comparisons with VLAE.\n\nNow, considering all this, the key points are: novelty, addressing the concerns, and the strength of the results. The main contributions seem to be the progressive approach for hierarchical disentanglement and a new metric. The rebuttal addressed some experiments, like showing better performance than VLAE and information flow in MNIST. However, there are remaining issues like the unclear relationship between hierarchical and disentangled representations (Reviewer 2's point), the metric's practicality (since it needs ground truth), and possible overlap with prior work like [3]. Also, the ablation studies and robustness to hyperparameters (beta) weren't fully addressed. \n\nBut Reviewer 3 and 4 are more positive, and after rebuttal, Reviewer 1 is weakly accepting. The paper introduces a novel approach that combines two concepts (progressive learning and hierarchical VAEs) which haven't been combined before. The experiments, while having some flaws, show improvements in disentanglement metrics. However, the weaknesses pointed out by Reviewer 2, especially regarding the purpose and the metric's limitations, might still be significant. For a top-tier conference, the paper needs to have clear contributions and solid experimentation. The authors addressed some points but maybe not all. The lack of ablation studies and thorough comparison with VLAE (as per Reviewer 4) could be a problem. Also, the metric MIG-sup's novelty is questioned if it's similar to prior work. \n\nBalancing all this, the paper has some merits but might not be strong enough for a top-tier conference. The rebuttal improved the score for Reviewer 1, but Reviewer 2 still has valid concerns. If the majority of reviewers are leaning towards weak accept or accept, maybe it's borderline. However, given the remaining issues, especially in clarity of purpose and thoroughness of experiments, the decision might lean towards rejection. Alternatively, if the novelty and the positive aspects from Reviewers 3 and 4 are strong enough, maybe accept. But considering top-tier standards, the remaining methodological concerns and lack of ablation studies might tip it towards reject.\n</think>\n\nFinal Decision: **Reject**\n\n**Reasoning:**  \nThe paper presents a novel approach combining progressive learning with hierarchical VAEs to improve disentanglement, which is a promising direction. However, several critical issues remain unresolved, making it unsuitable for a top-tier conference:\n\n1. **Clarity of Purpose and Contribution:**  \n   Reviewer 2 highlights ambiguity in whether the focus is on hierarchical or disentangled representations. The authors fail to rigorously justify why hierarchical learning aids disentanglement, weakening the theoretical foundation.\n\n2. **Methodological Concerns:**  \n   - The proposed metric, MIG-sup, is critiqued for lacking novelty (overlap with [2]) and practicality (dependency on ground-truth factors).  \n   - Ablation studies to isolate the effects of progressive learning and architectural choices (e.g., fade-out) are absent, leaving the core innovation’s impact unclear.  \n   - Hyperparameter sensitivity (e.g., beta in the KL term) is not adequately addressed, raising concerns about robustness.\n\n3. **Experimental Limitations:**  \n   - While qualitative results on MNIST are improved post-rebuttal, Reviewer 2 notes that earlier comparisons with VLAE were unconvincing.  \n   - Quantitative analysis of information flow across layers (Reviewer 3’s request) remains incomplete.  \n\n4. **Overlap with Prior Work:**  \n   The similarity to [3] (progressive disentanglement) is not sufficiently discussed, leaving the incremental contribution ambiguous.  \n\n**Conclusion:**  \nDespite addressing some concerns in the rebuttal, the paper lacks the methodological rigor and clarity expected at a top-tier venue. The unresolved issues in contribution definition, metric novelty, and experimental thoroughness outweigh the promising aspects of the approach."
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes AE-GAN+sr, a defense against adversarial examples using an encoder-assisted search in the latent space. The main idea seems to be improving upon Defense-GAN by using an autoencoder to initialize the search process, which reduces computational costs. \n\nReviewer 1 points out that the method is an alternative to Defense-GAN with lower computation but notes a trade-off in accuracy. They mention that the results are reasonable on MNIST and F-MNIST. Reviewer 2 thinks the contribution is incremental and criticizes the lack of comparison with state-of-the-art methods like PGD adversarial training and TRADES. They also mention issues with the evaluation baseline (Szegedy) being outdated and some typos. Reviewer 3 raises concerns about the autoencoder's detection capability under white-box attacks and the latent space alignment. They also question the performance discrepancy between attack types. Reviewer 4 rejects due to unclear writing, weak experiments, and lack of evaluation on complex datasets.\n\nThe key points to consider are: \n1. **Novelty and Contribution**: The main contribution is reducing computation compared to Defense-GAN. However, if this is the only improvement and the defense performance isn't significantly better, it might be incremental. The reviewers note that Defense-GAN is from 2018, and the field has advanced since then. Not comparing with newer methods like adversarial training makes the contribution less compelling.\n\n2. **Experiments and Evaluation**: The experiments are limited to MNIST and F-MNIST. For a top-tier conference, more complex datasets like CIFAR-10 or ImageNet are often expected. The lack of comparison with state-of-the-art defenses (e.g., TRADES) weakens the paper's claims. Reviewer 2 and 4 both mention this as a flaw.\n\n3. **Technical Soundness**: The reliance on the autoencoder's detection ability might be a vulnerability, especially against white-box attacks where the attacker knows the defense mechanism. The alignment between the latent space and the prior distribution is questioned, which could affect the method's effectiveness.\n\n4. **Writing and Clarity**: Multiple reviewers mention typos, missing words, and unclear explanations. The paper was noted to be over the page limit, which suggests a lack of thorough editing. Clarity in methodology and implementation details is crucial for reproducibility and validation.\n\n5. **Reproducibility**: Reviewer 4 mentions that the implementation details are unclear. Without sufficient information on training procedures, hyperparameters, or the search process steps, it's hard to assess the method's reproducibility.\n\nConsidering all these factors, while the idea of reducing computation is useful, the paper seems to have significant shortcomings. The incremental contribution, limited evaluation, and presentation issues make it less suitable for a top-tier conference. The reviewers' criticisms, especially the lack of comparison with current methods and dataset limitations, are major red flags. Additionally, the writing issues and technical concerns about the autoencoder's robustness further weaken the case for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes an encoder-assisted defense mechanism against adversarial attacks, building upon Defense-GAN to reduce computational costs. While the idea of leveraging an autoencoder for latent space initialization is sensible, the work suffers from critical shortcomings that render it unsuitable for a top-tier conference:  \n\n1. **Incremental Contribution**: The core improvement—reducing computation time—is framed as an incremental advance over Defense-GAN (2018). The authors fail to compare against modern adversarial defense baselines (e.g., PGD adversarial training, TRADES), making the claimed benefits unclear in the context of current state-of-the-art methods.  \n\n2. **Limited Evaluation**: Experiments are restricted to MNIST and Fashion-MNIST, which are insufficient to demonstrate robustness for real-world applications. The absence of results on complex datasets like CIFAR-10 or ImageNet undermines the method’s scalability and generalizability.  \n\n3. **Technical and Conceptual Weaknesses**:  \n   - The reliance on the autoencoder’s adversarial detection capability is vulnerable to white-box attacks, which are not thoroughly addressed.  \n   - The latent space alignment between the encoder’s output and the GAN’s prior is not rigorously justified, raising concerns about reconstruction quality.  \n   - The inconsistent performance across attack types (e.g., better results on white-box than black-box attacks) lacks a clear explanation.  \n\n4. **Presentation Issues**: Multiple reviewers noted poor writing quality, including grammatical errors, unclear phrasing, and missing implementation details (e.g., gradient search steps, training protocols). The paper exceeds page limits, suggesting insufficient revision.  \n\n5. **Reproducibility**: Incomplete methodological descriptions (e.g., hyperparameters, search process) hinder reproducibility, a critical requirement for publication in top venues.  \n\nWhile the computational efficiency gain is a valid strength, the paper’s lack of novelty, limited evaluation, and unresolved technical concerns outweigh this merit. Addressing these issues would require substantial revisions and expanded experiments, beyond the current submission’s scope."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper and the reviewers' comments. The paper proposes using SGD to train GMMs in high-dimensional spaces, which is an alternative to the traditional EM algorithm. The main contributions are threefold: using an upper bound of the log-likelihood for numerical stability, a new regularizer to avoid bad local minima, and a method to enforce GMM constraints during SGD. They also suggest a simplified covariance structure to save memory.\n\nReviewer 1 thinks the work is incremental and criticizes the authors for not engaging deeply with existing online EM literature. They also question the numerical stability approach, the regularization method, and the experimental evaluation on not-so-high-dimensional datasets. The experiments are a big concern here.\n\nReviewer 2 points out issues with the theoretical grounding, like lack of convergence guarantees, incorrect statements about EM's local minima, and confusion in the covariance matrix constraints. The regularization is seen as a heuristic without clear justification. The experiments are deemed unconvincing due to missing details and comparisons, especially regarding runtime.\n\nReviewer 3 acknowledges the paper's clarity and practical contributions but notes that the novelty is limited. The max-component trick might not be better than existing methods like log-sum-exp. The experiments are criticized for not showing clear advantages over EM with K-means initialization and for focusing only on diagonal covariances. The regularization's effectiveness is acknowledged but its motivation is unclear.\n\nNow, putting this together. The paper's main strengths are the practical SGD approach for GMMs and the proposed regularizer. However, the weaknesses are significant. The reviewers highlight that the technical contributions might not be as novel as claimed, with existing methods (like online EM) not properly addressed. The experiments are a major issue: datasets aren't truly high-dimensional, comparisons with EM lack runtime data, and the covariance simplification isn't thoroughly tested. The theoretical flaws, like incorrect statements about EM and local minima, also hurt credibility.\n\nFor a top-tier conference, the paper needs strong theoretical contributions or empirical results that clearly advance the state-of-the-art. Here, the authors haven't sufficiently justified their methods against existing approaches, and the experiments don't convincingly demonstrate superiority. The reviewers' concerns about novelty, technical soundness, and experimental rigor are substantial. Even though there's some practical value, the paper doesn't meet the high bar required for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper proposes SGD-based training for GMMs in high-dimensional spaces, addressing numerical stability, regularization, and covariance constraints. While the topic is relevant, the reviewers identified critical flaws that undermine its suitability for a top-tier conference:  \n\n1. **Lack of Novelty and Technical Rigor:**  \n   - The proposed \"max-component\" approximation (Sec. 3.1) is perceived as a limited contribution compared to established numerical stability techniques (e.g., log-sum-exp). Reviewer 3 notes it does not clearly improve upon existing methods.  \n   - The regularization strategy (Sec. 3.3) is criticized as heuristic and inadequately justified. Reviewer 2 questions its theoretical basis, and Reviewer 1 highlights the ad-hoc nature of the grid-based smoothing.  \n   - Claims about EM’s local minima (Sec. 3.3) are factually incorrect, as EM is also prone to local minima, weakening the paper’s motivation.  \n\n2. **Experimental Weaknesses:**  \n   - **Datasets:** Experiments on 28x28-pixel images (e.g., MNIST) do not convincingly demonstrate scalability to \"high-dimensional\" settings (Reviewer 1).  \n   - **Comparisons:** Missing runtime comparisons with EM (despite claiming SGD’s efficiency) and insufficient ablation studies (e.g., regularization’s impact with K-means initialization) limit practical insights (Reviewers 2, 3).  \n   - **Covariance Simplification:** The low-rank covariance model is only validated on diagonal cases, failing to show utility for more complex structures (Reviewer 3).  \n\n3. **Theoretical Gaps:**  \n   - No convergence guarantees or formal analysis are provided, unlike recent work in moment matching (Reviewer 2).  \n   - Misstatements about GMM constraints (e.g., covariance matrix positivity in Sec. 3.2) reflect conceptual errors.  \n\n4. **Presentation Issues:**  \n   - Repetitive experimental descriptions and unclear definitions (e.g., \\(\\mu^i\\) in Sec. 3.4) reduce clarity.  \n   - The regularization schedule (Sec. 3.3) and grid-based smoothing lack rigorous justification.  \n\nWhile the paper offers practical insights (e.g., TensorFlow implementation) and addresses an important problem, the combination of incremental technical contributions, unconvincing experiments, and theoretical shortcomings makes it unsuitable for a top-tier venue. Significant revisions—including deeper engagement with prior work, stronger empirical validation, and theoretical grounding—are needed to meet publication standards."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes a hierarchical RL approach where they train diverse skills for each agent (like robot arms) and then learn a meta-policy to coordinate them. The reviewers have a mix of positive and negative points. \n\nReviewer 1 thinks the problem is interesting and the method could be impactful but has some concerns about notation clarity, the meta-policy's timing (T_low=1), high variance in results, and the need for more multi-agent baselines. They also mention the empirical results are good but want more consistency in reporting.\n\nReviewer 2 questions the novelty of applying temporal abstraction in multi-agent vs. single-agent settings and why DIAYN is used here. They also have technical questions about the skill learning and the fixed T_low steps. The reviewer seems a bit confused about the contribution's uniqueness.\n\nReviewer 3 likes the approach but is concerned about the need for predefined subtasks, suggesting a baseline with DIAYN alone. They also note missing details like the latent vector size and have a weak accept.\n\nNow, considering the strengths: the method shows empirical success in complex tasks, uses diversity effectively, and the modular approach is practical. The weaknesses include the reliance on predefined subtasks (limiting generality), possible lack of comparison to recent multi-agent methods, and some technical clarifications needed.\n\nThe key issues are whether the novelty is sufficient and if the experiments are thorough. The authors addressed some concerns in their responses, but reviewers still have points. For a top-tier conference, the paper needs strong novelty and rigorous evaluation. The reliance on predefined subtasks might be a limitation, but the results are promising. The method's ability to coordinate diverse skills is a plus. However, the high variance in results and lack of certain baselines could be red flags. \n\nReviewer 3's weak accept suggests it's borderline. The paper's practical approach and positive results might outweigh the concerns if the authors can address the issues in the final version. But if the predefined subtasks are a major limitation, that could be a problem. Given that the reviewers' concerns are addressable (like adding more analysis, clarifying notation, discussing related work), and the empirical results are solid, I lean towards acceptance with the expectation that the authors will revise based on the feedback.\n</think>\n\nThe paper presents a modular hierarchical RL framework for coordinating diverse skills in multi-agent manipulation tasks. While the approach is technically sound and demonstrates empirical success, several concerns must be weighed against its strengths to determine its suitability for a top-tier conference.\n\n**Strengths**:  \n1. **Problem Relevance**: The work addresses a critical challenge in robotics and multi-agent systems: coordinating temporally extended skills across agents. The tasks (e.g., bimanual manipulation) are complex and practically relevant.  \n2. **Methodological Novelty**: The integration of skill diversification (via maximum entropy objectives) with hierarchical coordination introduces a novel twist to prior skill-based RL frameworks. The use of latent vectors to control skill diversity adds interpretability and adaptability.  \n3. **Empirical Validation**: The experiments show clear improvements over baselines, and the ablation studies (e.g., diversity weight sensitivity) provide insights into the method’s robustness.  \n\n**Weaknesses and Concerns**:  \n1. **Predefined Subtasks**: The reliance on predefined subtasks (e.g., \"pick,\" \"push\") limits the framework’s generality. Reviewer 3 rightly questions whether the approach could work with purely unsupervised skill discovery (e.g., DIAYN alone). The lack of a baseline without subtask specifications weakens the claim of broad applicability.  \n2. **Technical Ambiguities**:  \n   - Reviewer 1 notes inconsistencies in notation and the meta-policy’s temporal granularity (T_low=1), which risks conflating skills with primitive actions.  \n   - Reviewer 2 highlights unresolved questions about the distinction between single-agent and multi-agent temporal abstraction, which the paper does not rigorously address.  \n3. **Experimental Rigor**:  \n   - High variance in training curves (Reviewer 1) and missing details (e.g., latent vector size) raise reproducibility concerns.  \n   - The multi-agent baselines are limited to older methods (e.g., Lowe et al., 2017), omitting comparisons to recent advances in MARL.  \n\n**Impact and Rebuttal Potential**:  \nThe core idea—leveraging diversified skills for coordination—is promising and could influence both robotics and multi-agent RL. However, the paper’s impact is partially hampered by its reliance on predefined subtasks and insufficient discussion of how multi-agent coordination fundamentally differs from single-agent skill composition. The authors’ responses to reviewers (if available) could mitigate some concerns, such as clarifying notation, adding baselines, or expanding the theoretical discussion.  \n\n**Conclusion**:  \nWhile the paper presents a valuable contribution with strong empirical results, the reliance on task-specific subtask definitions and unresolved technical questions about multi-agent temporal abstraction leave room for improvement. For a top-tier conference, the work meets the bar for acceptance if the authors address these concerns in revisions, particularly by adding a DIAYN-only baseline and clarifying the multi-agent novelty.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes a deep learning approach for modeling time-evolving graphs using temporal point processes. The key components are a modified LSTM with time gates, a selection mechanism for important nodes, and stacked layers with attention for multiple time resolutions. The application is in anti-fraud detection and interaction prediction.\n\nReviewer 1 raises concerns about novelty, insufficient analysis of contributions, missing comparisons with JODIE, and lack of detailed ablation studies. They also mention issues with the algorithm's justification and some technical details. Reviewer 2 points out that the components like recurrent networks for TPPs and temporal attention aren't novel on their own, and questions the log-likelihood formulation. They also mention parameter size discrepancies and lack of common benchmarks. Reviewer 3 finds the paper hard to follow, questions the handling of dynamic graphs, and asks about computational costs.\n\nThe strengths of the paper include improved empirical results on real-world tasks, especially interaction classification, which isn't common in existing works. The model's inductive nature for handling new nodes is a plus. However, the weaknesses are significant. The novelty is incremental, combining existing techniques without sufficient justification or analysis. Missing JODIE comparison is a big oversight, as it's a key baseline. The ablation studies are superficial, not exploring parameter impacts. The log-likelihood issue raised by Reviewer 2 could be a technical flaw. Clarity is another problem, with the model's workings not well-explained, and no diagram provided.\n\nFor a top-tier conference, even if the empirical results are strong, the lack of thorough analysis, novelty concerns, and missing comparisons might outweigh the positives. The authors addressed some points in their response, but the reviews suggest the paper doesn't sufficiently advance beyond existing methods. The technical issues (like the LL formulation) need verification. Without addressing these adequately, the paper might not meet the high bar for acceptance.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a method for modeling continuous time-evolving graphs using a temporal point process framework with novel components like time-gated LSTMs, attention mechanisms, and stacked layers for multi-resolution learning. While the approach demonstrates strong empirical results on interaction prediction and classification tasks, several critical issues prevent it from meeting the standards of a top-tier conference:\n\n1. **Novelty & Contribution Justification:**  \n   - The core components (time gates, stacked LSTMs, attention) are largely incremental. Prior works (e.g., Neural Hawkes, DyRep, JODIE) have already combined recurrent networks with temporal point processes or attention. The authors fail to rigorously justify why their specific combination of techniques constitutes a significant theoretical or practical advancement. For instance, the \"time gate\" is not sufficiently differentiated from existing temporal mechanisms in TPP literature.\n   - The claimed novelty of handling multiple time resolutions via stacked LSTMs lacks empirical validation. The ablation study does not demonstrate how different layers capture distinct temporal scales or how parameter choices (e.g., \\(K\\)-hop subgraphs) affect performance.\n\n2. **Technical Concerns:**  \n   - Reviewer 2 identifies a potential error in the log-likelihood formulation (Section 3.6.1), which deviates from the standard TPP likelihood. If this is a mistake, it undermines the technical soundness of the method.\n   - The selection mechanism (Algorithm 2) and subgraph construction are poorly motivated. The authors do not clarify why their coloring/hashing approach is preferable to simpler alternatives like BFS, nor do they analyze computational complexity or scalability.\n\n3. **Empirical Evaluation:**  \n   - The omission of JODIE, a state-of-the-art baseline for interaction prediction, is a major oversight. Comparisons with shallow models (e.g., DeepCoevolve) are insufficient to claim superiority in a rapidly evolving field.\n   - Parameter fairness is questioned: the proposed model’s parameter count (due to stacked layers) may be significantly larger than baselines, potentially inflating results. The authors do not control for this in experiments.\n   - Interaction classification experiments lack comparisons with temporal baselines. Using embeddings from other methods with a classifier would have strengthened the claims.\n\n4. **Clarity & Reproducibility:**  \n   - The paper is dense and lacks intuitive explanations. Key components (e.g., how the model handles dynamic graphs inductively) are not clearly illustrated. A model diagram or example would improve accessibility.\n   - Critical implementation details (e.g., subgraph depth, training order, handling of new nodes in prediction tasks) are ambiguously described, hindering reproducibility.\n\n5. **Analysis & Insights:**  \n   - The ablation studies are superficial. For example, disabling components (e.g., time gates) does not reveal how they interact with other parts of the architecture. There is no analysis of how attention weights correlate with node importance or temporal patterns in the data.\n   - The real-world financial application is underexplored. Insights into why the model succeeds in fraud detection (e.g., interpretable attention patterns) would strengthen practical relevance.\n\n**Final Decision:** Reject  \n\nWhile the paper addresses an important problem and shows promising results, the incremental technical contributions, unresolved technical concerns, insufficient empirical validation against relevant baselines, and lack of clarity/analysis make it unsuitable for acceptance at a top-tier conference. The authors should address the raised issues (e.g., JODIE comparison, likelihood formulation, deeper ablation studies) and improve the exposition to strengthen the work for resubmission."
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about multi-task learning using attention mechanisms to model task dependencies at multiple levels. The reviewers have a mix of positive and negative comments. Let me start by summarizing the key points from each section.\n\nThe abstract claims that existing methods assume predefined task relationships, which limits real-world applications. The proposed method uses attention to model dependencies and shows improvements on public datasets. \n\nReviewer 1 points out grammatical issues and that the distinction between general and data dependency isn't clear, leading to only slight improvements over SOTA. They also mention the need for more clarity on the contributions of each level.\n\nReviewer 2 thinks the idea of sample-dependent task relatedness is new but the motivation isn't clear. They also question the experiments, like possible data leakage and the need for ablation studies (like removing the attention component). The reviewer suggests the paper could be accepted with improved presentation and experiments.\n\nReviewer 3 challenges the claim that existing methods assume tasks are highly correlated, mentions scalability issues with separate encoders per task, and notes missing related work on attention in multi-task learning. They also found a typo.\n\nReviewer 4 is critical, comparing the work to Taskonomy (2018), which already models task relationships. They point out missing specifics in the method, unclear notation, and confusion between terms like \"multi-view\" and \"multi-level.\" Post-rebuttal, they still feel major issues aren't resolved, especially regarding differentiation from prior work and experimental comparisons.\n\nThe authors responded to some points, improving clarity and adding comparisons, but reviewers still have concerns. The main issues seem to be:\n\n1. **Novelty and Prior Work**: The paper doesn't sufficiently differentiate from existing methods like Taskonomy and other attention-based MTL approaches. Reviewers 3 and 4 highlight this as a major flaw. If the core idea is similar to prior work, the novelty is questionable.\n\n2. **Clarity and Presentation**: Multiple reviewers mention grammatical errors, unclear explanations, and confusing notation. Even after revisions, some parts remain hard to follow, which is a red flag for a top-tier conference.\n\n3. **Experimental Rigor**: The experiments might not fully validate the claims. Reviewer 2 wants ablation studies (e.g., removing attention), and Reviewer 4 wants comparisons with Taskonomy. The datasets used are multi-label, which might not cover the scenarios where multi-level dependencies are crucial. The improvements over SOTA are described as \"significant\" but the actual gains might be marginal, as hinted by Reviewer 1.\n\n4. **Methodological Details**: The technical description has gaps, like the definition of the general task dependency matrix and how exactly the attention mechanism operates. Confusing notation (e.g., index i used in two ways) makes it hard to assess the method's correctness.\n\nConsidering top-tier conferences require both technical novelty and solid empirical validation, the paper seems to fall short. The rebuttal didn't fully address the core issues, especially regarding prior work and experimental comparisons. While the idea of multi-level dependencies has potential, the current presentation and validation aren't sufficient. The reviewers' concerns about novelty, clarity, and experiments are significant enough to warrant rejection. However, if the method's core idea is indeed novel and the experiments show meaningful improvements, maybe acceptance is possible. But given the reviewers' consensus post-rebuttal, rejection seems more likely.\n</think>\n\nFinal Decision: **Reject**\n\n**Reasoning:**  \nThe paper introduces a multi-task learning framework using attention to model multi-level task dependencies. While the concept is potentially valuable, critical flaws highlighted by reviewers undermine its suitability for a top-tier venue:\n\n1. **Novelty Concerns:**  \n   - The core idea of modeling task relationships overlaps significantly with prior work (e.g., Taskonomy), which is not adequately addressed. Reviewers note that the proposed method lacks clear differentiation from existing approaches, especially in modeling task dependencies and using attention mechanisms in multi-task settings.  \n   - Claims about the uniqueness of addressing \"multi-level\" dependencies are weakened by insufficient justification and unclear definitions (e.g., \"multi-view\" vs. \"multi-level\").\n\n2. **Technical and Presentation Issues:**  \n   - The manuscript suffers from grammatical errors, inconsistent notation, and ambiguous explanations (e.g., undefined variables like *k* in equations, conflicting indexing). These issues hinder reproducibility and clarity.  \n   - Key components (e.g., the \"general task dependency\" matrix) are under-specified, raising doubts about the method’s technical soundness.\n\n3. **Insufficient Empirical Validation:**  \n   - Experiments lack critical baselines (e.g., Taskonomy, attention-free variants) and fail to isolate the contribution of the proposed multi-level dependency modeling. Marginal improvements over SOTA, as noted by reviewers, further question the method’s impact.  \n   - The rebuttal did not resolve concerns about dataset validity (e.g., potential data leakage in multi-label setups) or provide convincing hierarchical dependency analysis.\n\n4. **Scalability and Practicality:**  \n   - The use of task-specific encoders raises scalability concerns for tasks with limited labeled data, a common real-world scenario. This limitation is not adequately discussed or tested.\n\nWhile the idea of sample-dependent task transfer is promising, the paper’s unresolved issues in novelty, clarity, and experimental rigor make it unsuitable for acceptance at a top-tier conference in its current form."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me start by going through the paper details and the reviewers' comments. The paper is about using machine learning, specifically RNNs like bidirectional LSTMs, to predict DNA folding patterns based on epigenetic data. The main claim is that their LSTM model outperforms other methods, and they use a weighted MSE loss to improve performance.\n\nReviewer 1's main concern is whether the improvement is due to the LSTM itself or the new loss function. They also question the generalizability of the approach, especially regarding the specific parameters like the 11 in the loss function and other datasets. Reviewer 2 has several technical points, like clarifying equations, figure labels, and the discussion on why regression vs. neural networks works. They also mention the need to explain the error discrepancies in Figure 6. Reviewer 3 is harsher, pointing out lack of novelty, unclear method descriptions, and insufficient evaluation. They mention that the paper is similar to prior work and that data representation isn't clear. Also, they question the choice of biLSTM and the evaluation metrics.\n\nSo, first, the paper's strengths: applying RNNs to predict DNA folding is a relevant application in computational biology. They compared multiple models and found that biLSTM works best. They also identified important epigenetic features, which adds biological insight.\n\nBut the weaknesses are significant. The reviewers point out methodological issues. The loss function's design (weighted MSE with a window of 11) isn't well-justified, and they didn't test if other models would perform better with the same loss. Without this, it's unclear if the LSTM's success is due to architecture or the loss. Also, the data preprocessing and feature representation are unclear. For example, how exactly were the Hi-C signals processed? What's the 20kb referring to? Without clear method details, reproducibility is a problem.\n\nReviewer 3's point about novelty is critical. If similar work exists and isn't cited, the paper lacks the necessary context to show its contribution. The authors need to compare with existing methods more thoroughly. Additionally, the choice of biLSTM over other architectures like CNNs or transformers isn't well-motivated. The DNA's double-stranded nature might not directly relate to using biLSTM since the input features are epigenetic marks, not the DNA sequence itself. This makes the model choice seem arbitrary without proper justification.\n\nThe evaluation is another weak point. Using the same loss function across all models is essential for fair comparison. If only the LSTM uses the weighted MSE, then the comparison isn't fair. They should also include standard metrics like MSE or R². The feature importance analysis is based on linear models, but for the LSTM, they didn't perform similar analyses, which limits the interpretability of their findings.\n\nThe reviewers also mention presentation issues: unclear equations, figure labels, and references. These are important for clarity but might be fixable with revisions. However, the methodological flaws are more serious. The lack of comparison with existing works and insufficient evaluation metrics make it hard to assess the paper's contribution.\n\nIn top-tier conferences, novelty, methodological rigor, and thorough evaluation are crucial. This paper seems to apply existing techniques (LSTM) to a biological problem without sufficient justification for the model choice or thorough comparison with prior work. The technical issues raised by the reviewers, especially regarding the loss function and evaluation, are significant. Without addressing these, the paper's claims aren't convincingly supported. Even though the application is interesting, the current version has too many flaws to be accepted as is. The authors need to address the methodological concerns, clarify their approach, and better situate their work within existing literature. Therefore, I lean towards rejection.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nWhile the paper applies machine learning to an important problem in computational biology (predicting DNA folding patterns), it faces significant methodological and presentation issues that preclude acceptance at a top-tier conference:  \n\n1. **Lack of Methodological Clarity and Fair Comparisons:**  \n   - The improvement attributed to the biLSTM model is confounded by the use of a custom weighted MSE loss function. The authors do not demonstrate whether competing methods (e.g., linear models, gradient boosting) would perform similarly if trained with the same loss. This undermines claims about architectural superiority.  \n   - Critical details about data preprocessing (e.g., feature representation, handling of genomic segments across chromosomes) and hyperparameter tuning are omitted, limiting reproducibility.  \n\n2. **Unconvincing Novelty and Biological Insight:**  \n   - The choice of biLSTM is weakly motivated. The connection between DNA’s double-stranded nature and the bidirectional architecture is tenuous, as inputs are epigenetic marks, not raw DNA sequences. Comparisons to non-recurrent models (e.g., CNNs, transformers) are absent.  \n   - Feature importance analysis relies on linear model coefficients, ignoring the biLSTM’s learned representations. This limits biological interpretation and fails to validate if the biLSTM captures novel patterns.  \n\n3. **Incomplete Evaluation:**  \n   - The weighted MSE metric is narrowly defined (e.g., the window size of 11 is unexplained) and not benchmarked against alternatives (e.g., log-transformed MSE). Standard metrics like R² are missing.  \n   - Generalizability to other datasets (e.g., ATAC-seq, other species) and robustness checks (e.g., cross-chromosome validation) are not addressed.  \n\n4. **Presentation Issues:**  \n   - Technical ambiguities (e.g., undefined variables in equations, unclear references, mislabeled figures) hinder readability. Reviewer 3 notes similarities to prior work, suggesting insufficient citation and contextualization.  \n\nWhile the application is promising, the paper does not meet the rigor expected of a top-tier venue. Addressing these concerns would require substantial revisions, including fairer comparisons, deeper biological analysis, and clearer methodological exposition."
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what this paper is about. The title mentions \"Multi-Precision Policy Enforced Training (MuPPET)\" which aims to speed up CNN training by switching between different precisions during training. The abstract says they achieve similar accuracy to full-precision training with a 1.28x speedup on average. \n\nLooking at the reviewers' comments, there are some common themes. Reviewer 1 is confused about the motivation behind the switching mechanism, especially why gradient co-alignment indicates it's time to switch. They also mention the explanation feels heuristic and there are some typos. Reviewer 2 points out that the approach doesn't actually choose the quantization level dynamically but just increases it at fixed steps. They also question the choice of precision levels and note that the results aren't significantly better than existing methods. Reviewer 3 is concerned about the lack of clarity in the algorithm's necessity and reproducibility issues, like how scaling factors are determined.\n\nThe authors' response would be crucial here, but since it's not provided, I have to go off the reviewers' critiques. The main issues seem to be:\n\n1. **Clarity and Motivation**: The switching mechanism's rationale isn't well-explained. The gradient diversity concept from another paper is used, but the adaptation to epochs instead of mini-batches isn't justified. If gradients are co-aligned, why does that mean the model isn't learning? The reviewers are confused here, which suggests the paper didn't explain this well.\n\n2. **Algorithm Details**: The precision levels (8-, 12-, 14-, 16-bit) are chosen without clear reasoning. How were these specific bits selected? Also, the policy uses a ratio 'p' and a threshold, but the explanation is lacking. The steps in the algorithm (like what 'y' refers to) are unclear, making it hard to reproduce.\n\n3. **Empirical Results**: The speedup is 1.28x, which isn't a huge improvement. Some networks like AlexNet and ResNet have lower accuracy than the FP32 baseline. The training/validation curves aren't shown, which makes it hard to assess convergence behavior. Comparisons to state-of-the-art methods (like mixed-precision training) don't show clear superiority.\n\n4. **Reproducibility**: Details on quantization (scaling factors, exact implementation) are missing. Without this, others can't replicate the results. The figures are too small, and some parts of the algorithm are not well-defined.\n\n5. **Contribution**: The core claim is that dynamic switching based on gradient diversity is novel, but reviewers question if this is necessary. Pre-defined schedules might work as well, and the dynamic aspect isn't proven to be critical. The introduction doesn't clearly state the novel contribution beyond existing mixed-precision methods.\n\nConsidering top-tier conferences expect significant, well-validated contributions, the paper has several red flags. The lack of clarity in the algorithm's motivation and details, combined with marginal improvements and reproducibility issues, are major concerns. While the idea of multi-precision switching is interesting, the execution as presented doesn't meet the high standards required. The reviewers' critiques, especially the major points about unclear contributions and methodology, suggest that the paper isn't ready for acceptance without substantial revisions.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper proposes MuPPET, a multi-precision training strategy that dynamically switches between fixed-point precisions to reduce training time while maintaining accuracy. While the concept is promising, several critical issues prevent it from meeting the standards of a top-tier conference:\n\n1. **Unclear Algorithmic Motivation and Design:**\n   - The precision-switching mechanism, central to MuPPET, lacks rigorous justification. Reviewer 1 questions the rationale for using gradient diversity across epochs (rather than mini-batches) as a switching criterion. The authors fail to explain why gradient co-alignment implies diminished learning progress, leaving the policy heuristic and unconvincing.\n   - The choice of precision levels (8-, 12-, 14-, 16-bit) is arbitrary and unexplained (Reviewer 2). The policy’s reliance on a ratio \"p\" and threshold parameters is inadequately motivated, and key variables (e.g., \"y\" in Step 5 of the algorithm) are undefined, undermining reproducibility.\n\n2. **Marginal Empirical Improvements:**\n   - The reported 1.28× average speedup over FP32 training is modest compared to existing mixed-precision methods (e.g., [2]). For AlexNet and ResNet, MuPPET achieves lower accuracy than the FP32 baseline, raising doubts about its reliability.\n   - The absence of training/validation curves (requested by Reviewer 2) obscures critical dynamics like convergence behavior and stability. Comparisons to state-of-the-art methods (e.g., [2,3]) are insufficient to demonstrate superiority.\n\n3. **Reproducibility Concerns:**\n   - Critical implementation details (e.g., scaling factor computation, quantization mechanics) are omitted. Reviewer 3 highlights ambiguities in equations and parameter definitions, making replication difficult.\n   - Figures and tables (e.g., precision-switching thresholds in Figure 2) are poorly annotated, with text too small to interpret.\n\n4. **Weak Contribution Differentiation:**\n   - The dynamic switching mechanism is not proven essential. Reviewer 3 notes that pre-defined schedules might achieve similar results, weakening the claim that MuPPET’s runtime policy is a novel advancement.\n   - The introduction fails to crisply articulate how MuPPET advances beyond existing mixed-precision training frameworks, beyond incremental adjustments to fixed-point quantization.\n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper is about a new meta-learning method called NORML, which uses an LSTM to optimize neuron-wise updates for few-shot learning. The reviewers have raised several concerns, so I need to evaluate each point to decide if the paper should be accepted or rejected.\n\nStarting with Reviewer 1's comments. They point out that the authors might have misrepresented Ravi & Larochelle's work. The paper claims that Ravi's method requires a massive hidden state, but Reviewer 1 clarifies that Ravi actually uses parameter sharing, making the meta-learner's parameters constant relative to the learner network. If the authors didn't properly compare their method to Ravi's under the same conditions, that's a problem. Also, the experiments don't validate the claim about handling many inner-loop steps better than MAML. The results on Mini-ImageNet don't include Sun et al.'s better results, which is a red flag. Omniglot experiments using only FC layers instead of conv nets might limit the applicability.\n\nReviewer 2 mentions issues with writing quality, grammatical errors, and incorrect citations. These are important because clarity is crucial for scientific papers. The technical content might be solid, but poor presentation can hinder understanding. Also, some technical points like the explanation of MAML's limitations and equation details need clarification.\n\nReviewer 3 is more critical, citing limited novelty as an incremental step over Ravi & Larochelle. They also note missing baselines like MAML++ and inaccurate descriptions of prior work. The assumption of FC layers limits applicability, and there's a possible error in how LSTMs are applied per layer versus a single LSTM. Without direct comparisons to key baselines under the same settings, the claims of improvement are weak.\n\nPutting this together: The core issues are around novelty, proper comparison with existing methods, technical accuracy in describing prior work, and experimental validation. The authors need to show that their method offers a significant improvement over existing approaches, especially Ravi & Larochelle, but the reviewers point out that this isn't adequately demonstrated. The experiments are either missing key comparisons (like Sun et al. and MAML++) or use different architectures (FC vs conv nets), making the results less convincing. Additionally, the technical misunderstandings of prior work (like how LSTMs are parameterized in Ravi's method) undermine the paper's credibility.\n\nThe writing issues are a problem but might be fixable with revisions. However, the methodological and experimental flaws are more critical. For a top-tier conference, the paper needs to clearly demonstrate a novel contribution with solid empirical evidence. The reviewers' concerns about novelty, missing comparisons, and technical inaccuracies suggest that the current version doesn't meet that bar. Even though the idea of neuron-wise optimization is interesting, the execution in experiments and analysis isn't sufficient. Therefore, the paper should be rejected in its current form.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper introduces NORML, a meta-learning framework using an LSTM for neuron-wise optimization. While the idea of scaling meta-learning via parameter-efficient gradient modulation is promising, the submission has critical flaws that undermine its suitability for a top-tier conference:\n\n1. **Misrepresentation of Prior Work and Limited Novelty:**  \n   - The authors inaccurately characterize Ravi & Larochelle’s Meta-Learner LSTM, which already employs parameter sharing to avoid scaling issues. NORML’s claimed novelty—linear parameter scaling—is not sufficiently differentiated, as Ravi & Larochelle’s approach also maintains a constant parameter count relative to the learner network.  \n   - Reviewer 3 notes that NORML’s technical contribution is incremental, resembling a variant of existing LSTM-based meta-learners without justifying its superiority through direct comparisons (e.g., under identical architectures or pretraining protocols).\n\n2. **Unsupported Claims:**  \n   - The assertion that NORML enables effective gradient flow through many inner-loop steps lacks empirical validation. Experiments use few inner-loop steps (e.g., 5-step updates on Omniglot), failing to demonstrate superiority over MAML in long-horizon optimization.  \n   - Claims about scalability to large networks are not tested, as experiments focus on small fully connected (FC) networks or frozen convolutional backbones, limiting practical relevance.\n\n3. **Incomplete and Unconvincing Experiments:**  \n   - **Missing Baselines:** Key comparisons with Ravi & Larochelle’s method and MAML++ are absent. On Mini-ImageNet, results from Sun et al. (which outperforms NORML) are omitted, weakening the validity of claims.  \n   - **Architectural Limitations:** Omniglot experiments use FC networks instead of standard convolutional architectures, making comparisons to MAML (tested on conv nets) unfair and reducing applicability to real-world few-shot tasks.  \n   - **Ablation Deficits:** The ablation study does not isolate the impact of neuron-wise optimization versus feature reuse, leaving the source of improvements unclear.\n\n4. **Technical and Presentation Issues:**  \n   - Inaccurate descriptions of prior work (e.g., misattributing per-parameter LSTMs to Andrychowicz et al.) and citation errors (e.g., Rajeswaran 2019) erode credibility.  \n   - Poor writing quality, grammatical errors, and unclear explanations (e.g., ambiguous references to MAML’s limitations) hinder readability and rigor.\n\n**Conclusion:**  \nWhile NORML addresses an important problem (scalable meta-learning), the submission does not meet the bar for a top-tier conference. Critical methodological gaps, unsupported claims, and inadequate empirical validation overshadow its potential contributions. Significant revisions—including fair comparisons, expanded experiments, and technical corrections—are needed to justify acceptance."
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So the paper in question is about proving that SGD can learn one-layer networks in WGANs with polynomial time and sample complexity. The reviewers have several concerns, mainly around the simplicity of the discriminator architectures used (linear and quadratic) and whether this setup truly captures the essence of WGANs. \n\nFirst, the title claims \"SGD Learns One-Layer Networks in WGANs,\" but the analysis uses discriminators that are much simpler than what's typically used in practice. WGANs usually rely on neural networks with Lipschitz constraints (like with weight clipping or gradient penalty) to approximate the Wasserstein distance. The reviewers argue that using linear or quadratic discriminators might not align with the original WGAN framework because they can't approximate the Wasserstein distance effectively. This is a big point because if the discriminator is too simple, the min-max problem isn't really solving the Wasserstein GAN problem as intended.\n\nReviewer 1 and 3 both mention that the quadratic discriminator's connection to WGAN's Lipschitz constraint isn't clear. The authors tried to justify it by saying the regularization acts as a Lipschitz constraint for bounded data, but since the data is generated from a Gaussian input, which is unbounded, that justification might not hold. This could be a flaw in their theoretical setup.\n\nAnother issue is the generator structure. The paper uses a one-layer network, but Reviewer 2 points out that if the generator is x = Bφ(Az), that's actually a two-layer network (with φ as activation), which complicates things. The authors might be oversimplifying the generator structure, which could make their results less applicable to real-world scenarios where deeper networks are used.\n\nThe reviewers also question the practical relevance. Even if the theory holds for these simple discriminators, how does that help us understand real WGAN training with neural network discriminators? The authors argue that their analysis is a step towards understanding more complex cases, but the reviewers aren't convinced. They feel the gap is too big, and the analysis doesn't provide insights that can be extended.\n\nOn the positive side, Reviewer 2 acknowledges that analyzing gradient descent-ascent is a hard problem and the paper's results are a contribution. However, the choice of simple discriminators is seen as a weakness rather than a strength. Reviewer 3 also mentions the paper is well-written with valid analysis, but the settings are too simplified.\n\nThe authors' response to the reviewers didn't fully address the concerns, especially regarding the discriminator's relation to WGAN and the bounded data assumption. The reviewers expected more justification or experiments with standard neural network discriminators, which the authors didn't provide.\n\nIn terms of top-tier conference standards, theoretical papers need to either provide significant new insights that can influence practical applications or advance the understanding of complex models. Here, the simplifications might make the analysis tractable, but they might also limit the applicability. The core issue is whether the theoretical results here are a meaningful step forward. The reviewers seem split, with some acknowledging the technical contribution but others dismissing it due to the setup's limitations.\n\nGiven that the paper's claims are about WGANs but use discriminators that don't align with WGAN's practical implementations, the title and contributions might be misleading. The analysis, while rigorous, might not explain why WGANs work in practice. For a top-tier conference, the paper's relevance and the significance of the contributions are crucial. If the community feels that the setup is too far from real-world scenarios, the paper might not be accepted despite technical merits.\n\nSo, weighing the strengths (theoretical analysis of a difficult problem) against the weaknesses (simplified models that don't reflect practice and unclear connection to WGAN's actual setup), I think the paper might be rejected. The concerns about overclaiming contributions and the practical relevance are significant. The reviewers, especially Reviewer 1 and 3, have valid points that the authors didn't adequately address. For a top-tier conference, the paper's impact might be deemed insufficient due to these limitations.\n</think>\n\nThe paper presents a theoretical analysis of stochastic gradient descent-ascent (SGDA) for training Wasserstein GANs (WGANs) with a one-layer generator and simplified discriminators (linear or quadratic). While the technical analysis is rigorous and addresses a challenging problem in non-convex optimization, several critical issues raised by the reviewers undermine its suitability for a top-tier conference:\n\n1. **Misalignment with WGAN Framework**:  \n   The choice of linear/quadratic discriminators deviates significantly from the original WGAN formulation, which relies on neural networks with Lipschitz constraints to approximate the Wasserstein distance. The authors’ justification for treating quadratic discriminators as proxies for Lipschitz-constrained networks is insufficient, especially given the unbounded data distribution (Gaussian inputs) in their setup. This raises doubts about whether the analysis truly captures the essence of WGANs.\n\n2. **Overstated Contributions**:  \n   The title and claims about \"learning one-layer networks in WGANs\" are misleading. The generator studied (e.g., \\(x = B\\phi(Az)\\)) is effectively a two-layer network, and the discriminators are far simpler than those used in practice. The analysis does not extend to standard neural network discriminators, limiting its relevance to real-world WGAN training. Reviewer 2 notes that similar results for neural network discriminators would be far more impactful, but the current work does not address this.\n\n3. **Limited Practical and Theoretical Insight**:  \n   While the polynomial-time convergence result for SGDA is a technical contribution, the simplified setup (e.g., teacher-student framework with Gaussian inputs) restricts its applicability. The analysis does not resolve key challenges in WGAN training, such as error propagation in multi-layer discriminators or learning high-dimensional joint distributions. Reviewer 3 highlights that even the quadratic discriminator only recovers pairwise interactions, which is insufficient for full distribution recovery.\n\n4. **Inadequate Responses to Reviewer Concerns**:  \n   The authors’ revisions did not fully address critical questions, such as the connection between quadratic discriminators and Lipschitz constraints in WGANs or the rationale for avoiding standard neural network discriminators. This weakens confidence in the paper’s claims.\n\n**Conclusion**:  \nWhile the paper advances the theoretical understanding of SGDA in simplified WGAN settings, its narrow scope and misalignment with practical WGAN frameworks reduce its significance for a top-tier conference. The unresolved concerns about overclaims and relevance outweigh its technical merits.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me start by going through the paper details and the reviewers' comments. The paper proposes a method called Continuous Query Decomposition (CQD) for answering complex queries on incomplete knowledge graphs. The main idea is to use pre-trained neural link predictors and combine them using fuzzy logic operators, avoiding the need to train on complex queries. The reviewers have a mix of positive and constructive feedback, but overall, the reception seems positive.\n\nReviewer 1 didn't have any specific comments, just that their doubts were cleared. Reviewer 2 praises the elegance and efficiency of the solution, SOTA results, and mentions explainability but suggests more discussion on certain aspects. Reviewer 3 raises technical questions about the optimization methods, inference time, and comparisons with existing models. They also point out some notation issues. Reviewer 4 appreciates the simplicity and practicality but wants more ablation studies and timing results. Reviewer 5 finds the approach original and effective, with strong experiments, but has minor presentation issues.\n\nThe authors addressed some concerns in their responses, like clarifying the query types, providing more examples, and discussing KG incompleteness. They also mentioned that inference time for combinatorial optimization is manageable and compared it to baselines. However, some points remain, like the need for more ablation studies (e.g., using different t-norms or link predictors like TransE) and detailed timing comparisons. \n\nThe strengths are clear: the method achieves SOTA results with less training data, offers explainability, and is efficient. The weaknesses include potential issues with inference time for gradient-based methods and the need for more thorough ablation studies. However, the reviewers, especially Reviewer 5, still recommend acceptance despite these points. The paper's contribution to handling complex queries without extensive training on them is significant for the field. The minor issues can be addressed in revisions, and the core idea is solid and novel enough for a top-tier conference.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a novel and efficient framework (CQD) for answering complex queries on incomplete Knowledge Graphs, achieving state-of-the-art results with significantly less training data. The approach elegantly combines neural link predictors with fuzzy logic operators, avoiding the need for training on large sets of complex queries. Key strengths include:  \n1. **Technical Soundness:** The method is well-motivated, with rigorous experiments across multiple datasets demonstrating clear improvements (8–40% Hits@3 gains) over existing methods.  \n2. **Practical Impact:** By eliminating the need for training on complex queries, the approach reduces data requirements and computational overhead, making it highly scalable.  \n3. **Explainability:** The framework provides interpretable intermediate results, a valuable feature for real-world applications.  \n4. **Addressing Reviewer Concerns:** While reviewers raised valid questions (e.g., inference time, ablation studies, notation clarity), the authors addressed most concerns in their responses, and remaining issues (e.g., timing comparisons) can be resolved in revisions.  \n\nMinor weaknesses (e.g., limited ablation on alternative t-norms or link predictors) do not outweigh the paper’s contributions. The work advances the field of KG reasoning and aligns with the standards of a top-tier conference."
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes a Differentiable Hebbian Consolidation model for continual learning, aiming to mitigate catastrophic forgetting. The abstract mentions integrating Hebbian learning with synaptic consolidation methods and testing on MNIST variants and a new imbalanced dataset. The results claim reduced forgetting without extra hyperparameters.\n\nReviewer 1 points out that on Split CIFAR-10/100, the proposed method (DHP Softmax) underperforms compared to SI until the final split. They feel the empirical evaluation isn't strong enough and want more comparisons, especially with other methods combined. This is a concern because if the method doesn't outperform existing ones in standard benchmarks, its contribution might be limited.\n\nReviewer 2 is positive about the approach but confused about the technical details. They mention the need for clearer explanations, equations, and figures. The lack of clarity could hinder the paper's impact, especially for those less familiar with Hebbian learning. Also, they question why the Hebbian strategy is applied only to the softmax layer, which the authors need to justify.\n\nReviewer 3 appreciates the cognitive motivation and simplicity but criticizes the experiments. They want comparisons with other CLS-based approaches and more diverse datasets beyond MNIST variants. The suggestion to test on CIFAR or other complex datasets is valid since MNIST is considered a simpler benchmark. The authors' response to these points would be crucial.\n\nReviewer 4 is more critical, leaning towards weak reject. They note that the method alone doesn't perform well on challenging datasets and that combining it with other methods doesn't show significant improvement. They recommend testing on larger datasets like CIFAR-100 or CORe50 to demonstrate scalability and effectiveness in more realistic scenarios.\n\nPutting this together, the main issues are:\n1. **Empirical Evaluation**: The method doesn't clearly outperform existing baselines in key benchmarks (e.g., Split CIFAR). The experiments are limited to MNIST variants, which are less convincing for a top-tier conference. The new imbalanced dataset is a plus but not enough without broader validation.\n2. **Clarity and Technical Details**: The explanation of the model's architecture and equations is lacking, making it hard to follow. Figures and clearer equations could help, as Reviewer 2 mentioned.\n3. **Comparison with Existing Methods**: Lack of comparison with other CLS-based approaches and insufficient analysis of how the method complements existing techniques like EWC. Reviewer 3 and 4 highlight this.\n4. **Scalability and Real-world Applicability**: The method's performance on larger, more complex datasets is uncertain. Testing on CIFAR or similar would strengthen the claims.\n\nThe strengths include a novel approach combining Hebbian learning with synaptic consolidation, a well-motivated neurobiological basis, and the introduction of a new imbalanced dataset. However, the weaknesses in empirical validation and clarity are significant. For a top-tier conference, the paper needs to demonstrate clear superiority or novelty in results. The reviewers' concerns about the experiments not being comprehensive enough and the method's standalone performance being weak are major red flags. While the idea is promising, the current evaluation might not meet the high standards required. Therefore, the paper might need more extensive experiments and clearer explanations before acceptance.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel approach to continual learning by integrating Hebbian plasticity with synaptic consolidation, addressing catastrophic forgetting through a neurobiologically inspired framework. While the idea is innovative and well-motivated, several critical issues raised by reviewers limit its readiness for acceptance at a top-tier conference:\n\n1. **Weak Empirical Performance**:  \n   - The method underperforms compared to established baselines (e.g., SI on Split CIFAR-10/100) in key benchmarks. While it shows promise in combination with other methods (e.g., EWC), the standalone performance is not compelling enough to justify its novelty. Reviewer 4 notes that the gains when combined with synaptic consolidation are marginal compared to the consolidation methods alone, raising doubts about its unique contribution.\n\n2. **Limited Experimental Scope**:  \n   - The evaluation relies heavily on MNIST variants (Permuted/Split MNIST) and a custom imbalanced dataset. These are considered \"toy\" benchmarks in continual learning literature. The absence of results on more complex datasets (e.g., CIFAR-100, CORe50, or real-world scenarios like reinforcement learning environments) weakens the claim of generalizability. Reviewer 3 and 4 explicitly criticize this limitation.\n\n3. **Technical Clarity and Methodological Details**:  \n   - Reviewer 2 highlights confusion about the model’s architecture (e.g., indexing in equations, lack of figures, unclear motivation for applying Hebbian updates only to the softmax layer). These issues hinder reproducibility and understanding, which are critical for acceptance.\n\n4. **Insufficient Comparison to CLS-Based Approaches**:  \n   - While the method is motivated by complementary learning systems (CLS), Reviewer 3 notes that comparisons to other CLS-inspired methods are missing. This omission makes it difficult to assess whether the proposed approach advances the state-of-the-art in this specific paradigm.\n\n5. **Scalability Concerns**:  \n   - The method’s effectiveness on large-scale class-incremental settings (e.g., CIFAR-100 with 100 classes) remains unverified. Reviewer 4 emphasizes this as a key gap, given the focus on class-specific Hebbian updates.\n\n**Strengths**:  \n- The integration of Hebbian plasticity with synaptic consolidation is a fresh perspective in continual learning.  \n- The introduction of an imbalanced Permuted MNIST variant addresses an underexplored challenge (class imbalance + concept drift).  \n- The method’s simplicity and code availability (PyTorch) are practical advantages.\n\n**Conclusion**:  \nWhile the paper introduces a conceptually appealing approach, the empirical evaluation does not convincingly demonstrate superiority over existing methods, and the technical presentation lacks clarity. For a top-tier conference, these shortcomings are significant. The authors need to address the experimental gaps (e.g., testing on harder datasets, improving standalone performance) and clarify methodological details to strengthen their contribution.\n\n**Final Decision**: Reject"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper's core contribution. The authors propose a weakly-supervised method for trajectory segmentation to learn reusable skills from demonstrations. The key idea is using trajectory-level labels (which skills are present) without needing the order or exact segmentation. They apply multiple instance learning (MIL) techniques, specifically log-sum-exp pooling, to predict per-timestep skill labels. The experiments cover various environments, including real robots, and claim zero-shot transfer to new skill combinations.\n\nNow, looking at the reviewers' comments. Reviewer 1 raises major concerns. The main issue is that the paper's framing about learning reusable skills isn't supported by the experiments. They only show segmentation but don't demonstrate how these skills are reused in downstream tasks. The reviewer suggests testing by training policies with the segmented skills, which the authors didn't do. Also, the experiments are limited, with small numbers of skills and possible overfitting. The setup details are unclear, and there's no analysis of temporal consistency in predictions.\n\nReviewer 2 points out that the method's novelty is low since log-sum-exp pooling is already used in other domains. The results are underwhelming (35-60% accuracy), and the behavioral cloning success rate is 50%, which isn't impressive. They also mention missing comparisons to fully supervised baselines and other relevant works like Policy Sketches. The contribution seems incremental.\n\nReviewer 3 is concerned about the feasibility with many skills and the lack of theoretical bounds on dataset requirements. The classification metrics are poor, and there's a lack of confusion matrices or qualitative analysis. The baseline comparisons are limited, and unsupervised methods aren't considered. The real-world example's smooth predictions might be due to filtering, which isn't detailed.\n\nPutting this together, the paper's main issues are:\n\n1. **Lack of Evidence for Reusable Skills**: The core claim is about learning reusable skills, but the experiments don't validate this. Without showing that the segmented skills can be used effectively in new tasks (e.g., via BC or hierarchical control), the claim isn't substantiated.\n\n2. **Novelty and Technical Contribution**: The method applies existing MIL techniques (log-sum-exp) to a new domain, but reviewers note this isn't novel. The improvements over baselines are modest, and key comparisons (like fully supervised) are missing.\n\n3. **Experimental Rigor and Clarity**: The setup details are insufficient, making reproducibility hard. Metrics like accuracy might be misleading without confusion matrices. The temporal consistency of segments isn't analyzed, and hyperparameters (like Gaussian smoothing) aren't detailed.\n\n4. **Theoretical Limitations**: The paper doesn't address when the approach can/cannot work, such as the minimum data requirements or scenarios where skill identification is impossible (as Reviewer 3's example).\n\nThe authors' response would need to address these points, but based on the current submission, the reviewers' concerns are valid and significant. For a top-tier conference, the paper needs stronger empirical validation of its main claims, more thorough comparisons, and better theoretical grounding. The current results are preliminary and don't sufficiently support the proposed method's impact or effectiveness. Therefore, despite the interesting problem, the paper falls short of acceptance standards.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a weakly-supervised method for trajectory segmentation to learn reusable skills, leveraging multiple instance learning (MIL) with trajectory-level supervision. While the problem is relevant and the application of MIL to skill segmentation is promising, the paper falls short of the standards for a top-tier conference due to several critical shortcomings:  \n\n1. **Unsubstantiated Claims About Reusable Skills**:  \n   The framing emphasizes learning \"reusable skills,\" but the experiments focus solely on segmentation accuracy without demonstrating downstream utility. Reviewer 1 highlights the lack of experiments showing that the segmented skills improve policy learning (e.g., via behavior cloning or hierarchical control). Without evidence of reusability, the core contribution remains unvalidated.  \n\n2. **Limited Novelty and Incremental Technical Contribution**:  \n   The method builds on established MIL techniques (e.g., log-sum-exp pooling), which have been applied in other domains. Reviewer 2 notes that the novelty is minimal, and the results (e.g., 35–60% accuracy, 50% success in zero-shot tasks) are underwhelming. Comparisons to fully supervised baselines or state-of-the-art skill-learning frameworks (e.g., Policy Sketches) are absent, weakening the case for significance.  \n\n3. **Insufficient Experimental Rigor and Analysis**:  \n   - **Metrics**: Classification accuracy alone is insufficient to assess segmentation quality. Confusion matrices, precision/recall, or qualitative visualizations (as suggested by Reviewer 3) are missing, raising doubts about whether the model learns meaningful skill boundaries.  \n   - **Temporal Consistency**: The method lacks mechanisms to enforce temporal coherence in skill predictions, and the impact of post-hoc Gaussian smoothing is not analyzed.  \n   - **Real-World Generalization**: Reviewer 1 questions potential overfitting in the real-robot experiments, noting implausible predictions in early frames (e.g., predicting \"stirring\" before contact).  \n\n4. **Theoretical and Practical Limitations**:  \n   Reviewer 3 raises critical concerns about dataset requirements for skill disambiguation (e.g., overlapping skill combinations). The paper does not address these limitations theoretically or empirically, leaving open questions about the method’s applicability to complex scenarios.  \n\n5. **Missing Baselines and Comparisons**:  \n   The experiments compare only to weak baselines (e.g., max-pooling MIL variants). Unsupervised skill-discovery methods and hierarchical RL approaches (e.g., Andreas et al., 2017) are not evaluated, making it difficult to contextualize the contributions.  \n\n**Conclusion**:  \nWhile the problem of weakly supervised skill segmentation is important, the paper does not sufficiently validate its claims, lacks technical novelty, and suffers from methodological and experimental gaps. For a top-tier conference, stronger evidence of reusable skill utility, theoretical grounding, and rigorous comparisons are required."
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a new method called MALCOM for out-of-distribution (OOD) detection using the Normalized Compression Distance. The key selling points are that it doesn't require OOD samples for validation and doesn't retrain the model, which is practical for real-world applications. The reviewers have a mix of concerns, though.\n\nReviewer 1 points out some major issues. First, the comparison with prior work (Mahalanobis method) might be unfair because they didn't use adversarial samples for validation as the original paper suggested. The authors' replication of the prior method underperforms, which could mean their implementation was flawed. This is a big red flag because if their baseline isn't correctly implemented, the claimed improvements might be invalid. Also, the ablation study isn't thorough enough. They need to show whether each component of their method contributes meaningfully, not just that all components together work.\n\nReviewer 2 questions the necessity of the strict constraints (no retraining, no OOD validation). They suggest comparing with methods that don't have these constraints to show where MALCOM stands in the broader field. The justification for these constraints isn't strong enough. Plus, there are some grammatical issues in the abstract, which might indicate rushed writing or lack of attention to detail.\n\nReviewer 3 is concerned about the experimental settings. They mention that testing on more complex, safety-critical scenarios (like autonomous driving) would be more convincing. Also, the lack of theoretical guarantees on the method's exhaustiveness in detecting OOD samples is a weakness. The significance of the approach isn't fully established.\n\nPutting this together, the main issues are:\n1. **Unfair Comparisons**: The baseline implementations might be incorrect, leading to misleading results. The authors need to validate their replication of prior work and compare against the proper adversarial-validated version of Mahalanobis.\n2. **Ablation Study**: The effectiveness of each component in MALCOM isn't clearly demonstrated. Without this, it's hard to tell if the method's success is due to the novel approach or minor tweaks.\n3. **Justification of Constraints**: The paper doesn't adequately explain why the strict constraints (no retraining, no OOD data) are necessary or common in real-world scenarios. Comparing with less constrained methods would help contextualize the contribution.\n4. **Experiments and Significance**: The experiments are on standard benchmarks but lack more challenging, real-world scenarios. Theoretical underpinnings are missing, making the method's reliability unclear.\n\nThe authors addressed some points in their response, but reviewers still found the revisions insufficient. The core issue is whether the method's improvements are valid and significant. Given the concerns about baseline implementation fairness and the lack of thorough ablation studies, the paper's claims aren't fully supported. Additionally, without stronger justification for the constraints and more comprehensive experiments, the contribution to a top-tier conference is questionable.\n\nFinal Decision: Reject\n</think>\n\nThe paper presents a novel approach for out-of-distribution (OOD) detection using the Normalized Compression Distance (NCD), addressing practical constraints of avoiding OOD validation data and model retraining. While the problem setting is relevant and the method is conceptually interesting, significant concerns raised by reviewers undermine its readiness for publication in a top-tier conference. Below is the detailed analysis:\n\n### Key Strengths:\n1. **Novelty**: The use of NCD to capture spatial patterns in feature maps for OOD detection is a creative departure from Mahalanobis-based methods.\n2. **Practical Constraints**: The focus on avoiding OOD validation data and model retraining aligns with real-world deployment challenges.\n3. **Broad Evaluation**: Experiments span multiple benchmarks and architectures, demonstrating consistency.\n\n### Critical Weaknesses:\n1. **Unfair Comparisons**: \n   - Reviewer 1 highlights that the authors compared their method to a suboptimal version of the Mahalanobis baseline (vanilla or concatenated features) rather than the adversarial-validated variant proposed in Lee et al. (2018b). This undermines the validity of performance claims, as the baseline’s underperformance might stem from incorrect implementation rather than methodological superiority.\n   - The replication gap (lower performance of prior methods in the authors’ experiments vs. original papers) is not adequately addressed. Without clarifying whether this is due to implementation errors or experimental settings, the results are questionable.\n\n2. **Insufficient Ablation Study**:\n   - The proposed method’s effectiveness is attributed to combining global average pooling and spatial patterns, but the ablation study (Table 2) only shows improvements when all components are used. Reviewer 1 rightly questions whether minor tweaks (e.g., concatenation) drive the gains rather than the core NCD-based approach. A deeper analysis of component contributions is missing.\n\n3. **Weak Justification for Constraints**:\n   - Reviewer 2 questions the necessity of the strict constraints (no retraining, no OOD validation). While avoiding OOD data is pragmatic, the paper does not convincingly argue why both constraints must coexist in real-world scenarios or how common such cases are. Comparisons to methods with relaxed constraints (e.g., using adversarial validation) would better contextualize the trade-offs.\n\n4. **Theoretical and Practical Gaps**:\n   - Reviewer 3 notes the lack of theoretical guarantees for exhaustiveness in detecting OOD samples, which is critical for safety-critical applications. Experiments on more complex, high-stakes domains (e.g., autonomous driving) are absent, limiting the perceived significance.\n\n### Additional Issues:\n- **Writing Quality**: Grammatical issues in the abstract and elsewhere (noted by Reviewer 2) suggest insufficient polish.\n- **Citation Gaps**: Relevant prior work on compression-based methods in classification tasks (e.g., cited by Reviewer 2) is omitted.\n\n### Conclusion:\nWhile the paper introduces a promising idea, the unresolved issues in baseline comparisons, ablation studies, and justification for constraints prevent it from meeting the rigor expected at a top-tier conference. The reviewers’ concerns about implementation validity and methodological contributions remain unaddressed, and the revisions in the rebuttal were deemed insufficient. \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper introduces Deep Coherent Exploration, aiming to scale up a previous method (van Hoof et al., 2017) for deep RL in continuous control. The main contributions are modifying the last layer parameters as latent variables, a recursive inference method, and applying this to on-policy and off-policy algorithms.\n\nReviewer 1 thinks it's a solid contribution, especially for on-policy methods, but is concerned about the off-policy part. They suggest focusing on on-policy and noting the off-policy as a limitation. Reviewer 2 questions the significance since SAC (a state-of-the-art method) doesn't show much improvement. They also mention issues with baselines underperforming and limited experiments. Reviewer 3 finds the paper well-written but notes the theoretical part is dense and the figures have small fonts. Reviewer 4 points out that the idea is too similar to prior work and lacks exploration-focused experiments.\n\nThe strengths are clear: the method works well for on-policy algorithms like A2C and PPO, with strong empirical results and detailed derivations. The weaknesses include weaker performance with SAC, limited experimental domains (only MuJoCo), and questions about the novelty compared to prior work. The off-policy application seems underdeveloped, which might not meet top-tier conference standards. Also, the lack of exploration in more challenging or sparse-reward environments is a drawback. The reviewers are split but lean towards accept with reservations. However, the issues with off-policy methods and the incremental nature of the contribution might make it a borderline case. Considering top-tier conferences often prioritize significant advances, the mixed results and concerns about novelty could lead to a reject. But the on-policy improvements are notable. Hmm, maybe a weak accept? Or maybe the issues are too significant. The reviewers' scores might average around borderline. Given that Reviewer 1 and 4 are more positive but others have valid concerns, perhaps the paper needs more work, leading to a reject. But I'm not sure. Alternatively, if the on-policy part is sufficiently novel and effective, maybe accept. The authors addressed scalability, which is important. But the SAC results are underwhelming. The rebuttal could address some concerns, but based on current info, maybe leaning towards accept with the expectation that the authors clarify the limitations and focus on on-policy contributions.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a scalable extension of prior work (van Hoof et al., 2017) to deep RL, addressing an important problem in exploration for continuous control. While the core idea builds on existing theory, the technical contributions—particularly the recursive inference method for marginalizing last-layer parameters and its integration into modern on-policy algorithms—are novel and practically impactful. The empirical results for A2C and PPO are compelling, demonstrating clear improvements in learning speed and stability. The detailed derivations and ablation studies strengthen the paper's rigor.  \n\nHowever, the off-policy (SAC) results are less convincing, and the experiments are limited to MuJoCo tasks without exploration-centric benchmarks. Reviewers also raised concerns about baseline tuning and font readability. Despite these issues, the on-policy advancements and methodological clarity justify acceptance at a top-tier conference, provided the authors address the limitations (e.g., reframing the off-policy case as a limitation, expanding experiments in future work) in the final version. The work aligns with the community's interest in scalable exploration strategies and offers actionable insights for practitioners."
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper's main contributions. The authors are addressing a valid concern: current models of the primate ventral stream, like CORnet-S, require a lot of supervised updates, which isn't biologically plausible for how the brain develops. They propose three strategies to reduce the number of supervised updates needed while maintaining a high BrainScore, which measures how well the model matches brain activity.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 4 and 5 are positive, highlighting the importance of the problem and thorough experiments. However, Reviewers 1, 2, 3, and parts of 5 raise significant concerns. The main issue seems to be the interpretation of BrainScore. Reviewers 1 and 2 question whether achieving a certain percentage of BrainScore actually translates to biological relevance. They argue that the paper conflates \"80% of the score\" with \"80% match to the brain,\" which might be misleading. Reviewer 3 adds that the assumptions made, like the weight initialization method, lack a solid theoretical foundation and biological plausibility. \n\nThe authors' response to these concerns in the revised manuscript was deemed insufficient by Reviewer 1 and 2. They wanted clearer disclaimers about what BrainScore actually measures. Without this, the claims might overstate the biological significance. Reviewer 3 also points out that the methodology, especially the weight initialization, isn't well-justified from a neuroscientific perspective. \n\nOn the technical side, some reviewers question the statistical significance of results and the choice of layers for critical training. There's also a mention of missing references and methodological details in the appendix rather than the main text. Reviewer 5 notes that the analogy between model training and biological development might be stretched too far, as supervised learning isn't the only mechanism in brain development.\n\nDespite these issues, the paper does present novel approaches to reducing supervised updates, which is a step forward. The transfer experiments to ResNet and MobileNet add value. However, the core criticism about the validity of BrainScore as a metric and the biological plausibility of methods remains a major hurdle. For a top-tier conference, the paper needs to have both technical soundness and clear implications. If the central metric's validity is in question, the contributions might not hold up under scrutiny.\n\nThe split in reviewer opinions (some accept, some reject) suggests it's borderline. However, the unresolved concerns about the interpretation of results and biological relevance are critical. The authors didn't adequately address how BrainScore's limitations affect their claims. Without this, the paper's conclusions might not be as impactful as presented. Therefore, despite the interesting ideas, the paper might not meet the high standards required for acceptance in a top-tier venue.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper presents an innovative approach to reducing supervised updates in models of the primate ventral stream, addressing a biologically significant problem. However, critical concerns raised by multiple reviewers undermine its suitability for a top-tier conference:  \n\n1. **Interpretation of BrainScore as a Biological Metric**:  \n   - Reviewers 1, 2, and 5 emphasize that the paper conflates \"80% of BrainScore\" with \"80% match to the brain,\" a misleading framing. BrainScore’s validity as a proxy for biological similarity is questioned, and the authors’ revisions failed to sufficiently clarify this distinction. Without explicit caveats or validation of BrainScore’s biological meaning, the claims about developmental plausibility remain speculative.  \n\n2. **Biological Plausibility of Methods**:  \n   - The weight compression (WC) initialization and critical training (CT) strategies lack strong theoretical or empirical grounding in neuroscience. Reviewer 3 notes that WC’s connection to infant brain wiring is arbitrary, and Reviewer 5 critiques the assumption that supervised learning alone drives visual development. These methods, while technically interesting, are not convincingly tied to biological mechanisms.  \n\n3. **Methodological and Statistical Gaps**:  \n   - Key details (e.g., cluster initialization parameters) are relegated to appendices, and statistical significance testing is absent. Reviewer 3 highlights the risk of coincidental results due to single-experiment dependency.  \n\n4. **Overstated Claims**:  \n   - The paper’s framing implies that reducing supervised updates directly advances neuroscientific understanding of development. However, as Reviewer 3 argues, DNNs are already acknowledged as incomplete models of biological learning, and the incremental improvements here do not bridge this gap convincingly.  \n\n**Strengths Acknowledged**:  \n- The problem of reducing supervised updates is novel and impactful.  \n- The complementary strategies (WC, CT, and transfer to ResNet/MobileNet) are creative and rigorously tested.  \n- The work builds meaningfully on CORnet-S, the state-of-the-art model in BrainScore.  \n\n**Conclusion**:  \nWhile the paper explores a compelling direction, unresolved issues regarding metric validity, biological relevance, and methodological rigor prevent it from meeting the high bar of a top-tier conference. The authors’ revisions did not adequately address core criticisms about overinterpretation of BrainScore, leaving the central claims vulnerable to skepticism."
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviewers' comments. The paper proposes using binary masks on transformer weights or hidden units to disentangle representations in pre-trained models like BERT. The main idea is to find subnetworks that encode specific features without retraining the model. The experiments focus on syntax vs. semantics and sentiment vs. genre in movie reviews.\n\nReviewer 1 is concerned about the generality of the method. They question if the approach is only applicable to the specific cases tested. The authors need to show that their method works beyond the selected datasets. Reviewer 2 points out that the paper claims to outperform VAE and adversarial methods but doesn't provide enough evidence, especially on benchmarks. They also have questions about the efficiency and the specifics of mask training. Reviewer 3 likes the approach but mentions weaknesses like the triplet loss limiting to two factors, possible attribute leakage, and scalability issues with more attributes. They also ask about the impact of fine-tuning and which layers are masked. Reviewer 4 praises the writing and results but wants more domains tested and a discussion on weight vs. hidden unit masking. They also note missing related work citations. Reviewer 5 is concerned about the limited experimental setup (only two genres) and the lack of downstream task applications.\n\nThe strengths of the paper include a novel approach to disentanglement without retraining, promising results on specific tasks, and clear writing. However, the weaknesses are significant. The experiments are limited in scope (only two genres, specific tasks), which affects the claim of generality. The comparison to previous methods isn't thorough enough, especially since the abstract mentions outperforming VAEs but the main text doesn't elaborate. There's also a lack of analysis on mask sparsity, layer-specific effects, and scalability. The reviewers also pointed out missing citations and some technical issues like the triplet loss design.\n\nFor a top-tier conference, the paper needs to demonstrate broader applicability, more rigorous comparisons with existing methods, and deeper analysis of the method's components. The current experiments, while showing some success, are too narrow. The authors addressed some concerns in their responses, but key issues like generality and thorough benchmarking remain unresolved. Without these, the paper's impact and contribution aren't sufficiently proven. Therefore, despite the interesting idea, the paper isn't ready for acceptance in its current form.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning**:  \nThe paper presents a novel approach to disentangling representations in pre-trained transformers via masking, which is conceptually interesting and addresses an important problem in interpretability and robustness. However, significant concerns raised by reviewers undermine its readiness for acceptance at a top-tier conference:  \n\n1. **Generality and Scope**:  \n   - The experiments are narrowly focused on two genres (Drama/Horror) and specific tasks (sentiment/genre, syntax/semantics). This limits confidence in the method’s applicability to broader domains or more complex attribute combinations. Reviewer 1, 4, and 5 explicitly question the generality, and the authors’ responses do not adequately address this.  \n\n2. **Benchmark Comparisons**:  \n   - While the abstract claims superiority over variational autoencoders (VAEs) and adversarial methods, the paper lacks direct comparisons on established disentanglement benchmarks (e.g., DSprites, 3D Shapes) or NLP tasks beyond the authors’ curated datasets. Reviewer 2 highlights this omission, weakening the claim of outperforming prior work.  \n\n3. **Technical Depth and Analysis**:  \n   - Key technical questions remain unanswered: the impact of masking specific layers (Reviewer 2, 3), scalability to more attributes (Reviewer 3), and the relationship between weight masking and hidden-unit masking (Reviewer 4). The analysis of mask sparsity and overlap is also superficial.  \n\n4. **Downstream Utility**:  \n   - The paper does not demonstrate how disentangled representations improve performance on downstream tasks (e.g., classification, inference), which is critical for justifying practical relevance (Reviewer 5).  \n\n5. **Methodological Concerns**:  \n   - The triplet loss formulation restricts disentanglement to two factors (Reviewer 3), and attribute leakage remains a risk. Reviewer 3 also notes that the evaluation uses weak probes, potentially underestimating leakage.  \n\n6. **Missing Citations and Reproducibility**:  \n   - Reviewer 4 identifies gaps in related work (e.g., HUBERT, dropout-based methods), and the experimental setup lacks details (e.g., layer-specific masking strategies).  \n\nWhile the idea of masking pre-trained models for disentanglement is promising, the paper does not meet the rigor expected of a top-tier venue. The authors must expand experiments, address scalability/generality, and strengthen comparisons to SOTA to solidify their contributions."
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviews. The paper's main contribution is integrating language into both top-down and bottom-up visual processing for referring expression segmentation. The abstract claims that using language throughout the visual pathway improves performance over just top-down attention. The experiments show competitive results, but the reviews are mixed.\n\nReviewer 1 thinks the novelty is thin and points out missing citations and unclear figures. They also have questions about model depth and fusion methods. The authors responded by citing a baseline paper (Mei et al. 2018), but Reviewer 1 wasn't satisfied, lowering their score post-rebuttal. \n\nReviewer 2 is concerned about empirical results not matching existing works on some test sets and possible over-tuning. They also note discrepancies in tables and question the rationale for splitting textual representations. The authors didn't fully address these points, leading to lingering doubts.\n\nReviewer 3 is positive, praising the problem's importance, model design, and results. They recommend acceptance. \n\nReviewer 4 rejects the paper, citing poor conceptual motivation and insufficient analysis. They want more cognitive grounding or task-based reasoning for using language in early vision. Also, the quantitative improvements aren't significant enough, and there's a lack of error analysis. The authors' rebuttal didn't resolve these issues, especially regarding statistical significance and model interpretation.\n\nThe post-rebuttal updates show that Reviewers 1 and 4 remain unconvinced. The authors tried to address some points but missed key aspects like statistical tests and deeper analysis. The main issues are novelty, empirical strength, and conceptual motivation. \n\nTop-tier conferences require clear contributions, solid empirical results, and thorough analysis. While the idea of integrating language in bottom-up processing is interesting, the mixed reviews and unresolved concerns about significance and novelty are red flags. The paper's improvements are moderate, and without stronger evidence or deeper analysis, it's hard to justify acceptance. The lack of error analysis and statistical significance weakens the claims. \n\nGiven the split in reviews and the unresolved major concerns from Reviewers 1 and 4, along with Reviewer 2's empirical issues, the paper might not meet the high bar for a top-tier conference. The authors need more thorough validation and analysis to support their claims, which they haven't provided yet.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents an interesting approach to integrating language into both top-down and bottom-up visual processing for referring expression segmentation. While the core idea—modulating low-level visual features with language—is conceptually appealing, the reviews highlight critical weaknesses that undermine its suitability for a top-tier conference:  \n\n1. **Novelty and Conceptual Motivation**:  \n   - Reviewer 1 and 4 question the novelty, noting that the proposed method appears incremental compared to existing work (e.g., Step-ConvRNN). The authors’ rebuttal fails to adequately address concerns about missing baselines (e.g., Mei et al. 2018) and lacks a clear cognitive or task-driven rationale for why language should modulate low-level features.  \n\n2. **Empirical Results**:  \n   - Reviewer 2 and 4 highlight inconsistencies in test-set performance (e.g., underperformance on UNC testA/testB) and table discrepancies. The authors do not convincingly address concerns about over-tuning or statistical significance. The lack of bootstrap tests or error bars weakens claims of superiority.  \n\n3. **Analysis and Interpretation**:  \n   - The paper lacks critical error analysis (e.g., failure cases, linguistic-visual interaction patterns) to validate the mechanism behind its success. Reviewer 4’s request for heatmaps or alignment studies remains unmet, leaving the contribution’s interpretability in doubt.  \n\n4. **Rebuttal Limitations**:  \n   - Post-rebuttal, Reviewer 1 downgrades their score due to unresolved citation issues, and Reviewer 4 remains unconvinced by the empirical and conceptual arguments. The authors’ responses focus on incremental fixes (e.g., adding citations) but fail to address core critiques about significance and analysis.  \n\nWhile Reviewer 3 praises the work’s clarity and results, the unresolved concerns from other reviewers—particularly regarding novelty, empirical rigor, and interpretability—outweigh these strengths. For a top-tier venue, the paper requires deeper theoretical grounding, stronger empirical validation, and more thorough error analysis to justify its claims."
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes GG-GAN, a geometric graph generative adversarial network. The main contributions are addressing challenges like modeling complex relations, isomorphism consistency, and exploiting the latent distribution. They use a WGAN framework and claim permutation equivariance and scalability.\n\nReviewers have mixed opinions. Reviewer 1 and 2 have significant concerns. Reviewer 1 points out the datasets are small (9-20 nodes), which might include many isomorphic graphs, potentially skewing results. They also mention the baselines aren't comprehensive enough, missing NetGAN and others. Reviewer 2 questions the edge dependency modeling, mode collapse, and theoretical contributions. Reviewer 4 and 5 also raise issues about novelty, literature review, and scalability claims. Reviewer 3 is positive but brief.\n\nThe main issues seem to be around the experimental setup and validation. Small datasets might not sufficiently test scalability or generalization. The lack of comparison with key baselines like NetGAN and TagGen is a problem. The theoretical claims might not be as strong as presented, with some propositions being trivial or not well-connected to the method. The mode collapse and edge dependency issues are valid concerns that weren't adequately addressed.\n\nThe authors need to show more on larger graphs, compare with more baselines, and better justify their theoretical contributions. Without addressing these, the paper's claims of being state-of-the-art or scalable aren't fully supported. Given that top-tier conferences require strong empirical validation and clear advancements over existing work, the current presentation might not meet the bar. The reviewers' concerns, especially from 1, 2, 4, and 5, are substantial and would need thorough revisions. Since the authors haven't yet addressed these (as per the info given), the paper in its current form might not be acceptable.\n</think>\n\nThe paper proposes GG-GAN, a geometric graph generation framework leveraging Wasserstein GANs and permutation equivariance. While the idea of combining geometric embeddings with GANs for graph generation is promising, the paper faces significant criticisms that undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Theoretical Framework**: The paper provides theoretical guarantees (e.g., permutation equivariance, existence of distributions) to address isomorphism consistency and scalability.\n2. **Novelty**: The geometric interpretation and WGAN-based approach differentiate it from autoregressive or random-walk-based methods.\n\n### Critical Weaknesses:\n1. **Experimental Limitations**:\n   - **Small-Scale Datasets**: Experiments on graphs with ≤20 nodes are insufficient to validate scalability claims. Larger real-world graphs (e.g., social networks, molecular graphs) are needed to demonstrate practical utility.\n   - **Incomplete Baselines**: Missing comparisons with NetGAN, TagGen, and classical models (e.g., Chung-Lu) weaken claims of superiority. Reviewer 4 notes that GG-GAN achieves only marginal improvements over existing methods.\n   - **Isomorphism Bias**: Small datasets likely contain redundant isomorphic graphs, raising concerns about evaluation validity (Reviewer 1, 5).\n\n2. **Theoretical Concerns**:\n   - **Edge Dependency**: The independent Bernoulli edge sampling contradicts claims of modeling edge dependencies (Reviewer 2).\n   - **Weak Propositions**: Corollary 1 and Proposition 1 are criticized as trivial or underdeveloped (Reviewers 2, 5).\n   - **Collision Avoidance**: The mechanism for avoiding node collisions in geometric space is inadequately justified (Reviewers 2, 5).\n\n3. **Methodological Gaps**:\n   - **Mode Collapse**: No evidence is provided to show GG-GAN avoids this common GAN issue (Reviewer 2).\n   - **Ad-Hoc Features**: Handcrafted node features may limit generalizability (Reviewer 2).\n\n4. **Literature Review**: The paper insufficiently contextualizes its contributions relative to prior work (e.g., isomorphism handling in GraphRNN, scalability in TagGen), weakening novelty claims (Reviewer 4).\n\n### Conclusion:\nWhile the paper introduces a theoretically grounded approach, the experimental validation is insufficient for a top-tier venue. The lack of scalability tests, incomplete baselines, and unresolved theoretical concerns outweigh its strengths. Addressing these issues would require significant revisions, including larger-scale experiments, comparisons with SOTA methods, and deeper theoretical analysis.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's title and abstract. The authors propose a generalized probability kernel (GPK) for discrete distributions, aiming to unify existing discrepancy measures like MMD and KSD. They mention applications in two-sample testing and provide some theoretical analysis. The keywords suggest they're working within kernel methods and hypothesis testing, which are relevant areas for top-tier conferences.\n\nNow, the reviewers have raised several concerns. Reviewer 1 points out issues with definitions, especially around the kernel vs. distance terminology, confusion in notation (K vs. k), and the lack of clarity in the function spaces. They also mention missing proofs, particularly Theorem 5, and the absence of a simulation section. Reviewer 2 criticizes the lack of theoretical novelty, calling the theory elementary and noting the absence of experimental validation. They also mention presentation issues like grammar errors. Reviewer 3 questions the practical relevance of the setting where discrete labels have a distance structure, points out undefined terms and typos, and raises concerns about the applicability of Stein operators in discrete contexts. Reviewer 4 is confused about the kernel definitions, the feasibility of applying the method to two-sample tests without known distributions, and the lack of comparison to existing methods.\n\nThe common themes here are: theoretical gaps (missing proofs, unclear definitions), lack of experiments, presentation issues (notation, grammar), and questions about the practical relevance and novelty. For a top-tier conference, theoretical rigor and empirical validation are crucial. The paper seems to have foundational issues in its definitions and theoretical claims, which the reviewers found problematic. The absence of experiments is a major red flag, especially in a field like machine learning where empirical results are often expected to validate theoretical contributions. Additionally, the presentation issues make the paper hard to follow, which undermines its clarity and impact.\n\nThe authors' response would be critical here, but since there's no mention of their rebuttal, we have to go by the submitted version. Given the severity of the issues—especially the theoretical gaps and lack of experiments—it's unlikely the paper meets the standards for acceptance. Even if the core idea is interesting, the execution falls short. Top conferences require both solid theory and convincing experiments, which this paper lacks. The reviewers' unanimous concerns about missing elements and clarity further support rejection.\n</think>\n\nThe paper proposes a generalized probability kernel (GPK) for discrete distributions, aiming to unify and extend existing discrepancy measures like MMD and KSD. While the idea of unifying kernel-based distribution comparisons is theoretically interesting, the paper suffers from critical flaws that render it unsuitable for publication in a top-tier conference. Below is the detailed analysis:\n\n### **Key Strengths**\n1. **Conceptual Goal**: The unification of MMD, KSD, and other discrepancies under a generalized framework is a worthwhile objective that could advance kernel methods in hypothesis testing.\n2. **Theoretical Scope**: The paper attempts to bridge discrete distribution estimation and kernel-based testing, which could have broader implications if rigorously executed.\n\n### **Critical Weaknesses**\n1. **Theoretical Incompleteness**:\n   - **Ambiguous Definitions**: Reviewers highlight confusion in Definitions 1–6, including unclear distinctions between kernels and distances (e.g., GPK vs. MMD²), undefined terms (e.g., $\\phi$, $\\mathcal{H}$, $\\Delta^*$), and inconsistent notation (e.g., $K$ vs. $k$). These issues undermine the paper’s mathematical rigor.\n   - **Missing Proofs**: Key results, such as Theorem 5 and the symmetric KDSD’s validity as a kernel, lack proofs. The removal of the supremum over $f$ in Theorem 5 is unexplained, raising doubts about the claims.\n   - **Trivial or Known Results**: Sections 4.2 and 4.3.1 are criticized for rehashing elementary derivations without novel insights.\n\n2. **Lack of Empirical Validation**:\n   - **No Experiments**: The absence of simulations or comparisons with baselines leaves the utility of GPK unverified. For a method targeting two-sample testing, empirical validation is essential to demonstrate advantages over existing approaches.\n   - **Unanswered Practical Questions**: Reviewers question how GPK-based tests would work when $p$ and $q$ are unknown (a core requirement for two-sample testing), but the paper provides no clarity.\n\n3. **Presentation Issues**:\n   - **Poor Writing**: Grammatical errors, undefined terms (e.g., \"pmfs,\" \"RBH kernel\"), and inconsistent notation make the paper difficult to follow.\n   - **Motivation Gaps**: The relevance of embedding discrete labels in $\\mathbb{R}^k$ (which rarely have inherent distance structures) is not justified with examples, weakening the practical motivation.\n\n4. **Novelty Concerns**:\n   - The framework is critiqued as a straightforward extension of existing ideas (e.g., MMD, KSD) without significant theoretical or algorithmic innovation. The connection to Bernstein polynomials, mentioned in the abstract, is not explored.\n\n### **Conclusion**\nWhile the paper’s goal is commendable, its execution is deeply flawed. The theoretical claims lack rigor due to incomplete proofs and ambiguous definitions, the absence of experiments undermines its practical relevance, and the presentation obscures rather than clarifies its contributions. For a top-tier conference, these shortcomings are disqualifying. The work may benefit from a thorough revision addressing the theoretical gaps, empirical validation, and improved clarity.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviewers' comments. The paper proposes αVIL, a method for dynamic task weighting in multitask learning. The main idea is using model parameter updates between epochs to adjust task weights, aiming to improve the target task's performance by leveraging auxiliary tasks. \n\nReviewer 1 thinks the paper is well-written but has concerns about the lack of theoretical justification and the experimental results not being convincing enough. They mention that the baselines are limited and the improvements aren't consistent. Reviewer 2 points out that the method seems ad hoc without clear intuition, especially regarding certain algorithm steps. They also compare it to existing meta-learning approaches and find the results not significantly better than baselines. Reviewer 3 is critical about the algorithm's novelty and the experimental setup, noting that the results are minor and statistically insignificant. They also have questions about the distinction between alphas and task weights.\n\nThe common themes in the reviews are:\n1. **Lack of Theoretical Foundation**: All reviewers mention the need for more theoretical analysis to justify why the method works. Without this, the empirical results alone aren't sufficient.\n2. **Experimental Weaknesses**: The experiments don't consistently show superiority over baselines. The Multi-MNIST setup uses only one auxiliary task, which might not fully test MTL scenarios. The NLU results are mixed, and statistical significance isn't clear.\n3. **Clarity and Novelty Concerns**: Questions about how αVIL differs from existing methods like meta-learning. The algorithm steps, like subtracting 1 from weights, aren't well-justified, making the approach seem ad hoc.\n\nThe authors' response isn't provided here, but based on the reviews, the rebuttal would need to address these points. However, given the current information, the paper's contributions are in question. For a top-tier conference, novelty, theoretical grounding, and clear empirical improvements are crucial. \n\nThe method's novelty is claimed as the first to use model updates for task weighting. But reviewers point out similarities to meta-learning approaches. If the authors can't clearly differentiate αVIL from these, the novelty is weakened. \n\nThe experiments show some improvements but aren't decisive. In Multi-MNIST, the gains are within standard deviation, and NLU tasks have mixed results. Without statistical significance tests, especially for NLU, it's hard to claim the method's effectiveness. Also, using only one auxiliary task in the vision experiments limits the demonstration of MTL benefits.\n\nThe lack of theoretical analysis is a significant drawback. Top conferences often expect some form of theoretical insight or at least a solid motivation for why the method works, beyond empirical results. Without this, the method's foundation seems shaky.\n\nReviewer 3 suggests rejection, and the others have major concerns. The paper might need more work on theory, experiments with more tasks, clearer differentiation from existing methods, and better statistical analysis. Given the current state, it's likely not ready for acceptance at a top-tier conference.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a novel method for dynamic task weighting in multitask learning (MTL) but faces significant criticisms that undermine its suitability for a top-tier conference:  \n1. **Theoretical Justification**: The method lacks formal analysis or theoretical grounding. Reviewers highlighted the absence of a connection to existing optimization frameworks (e.g., meta-learning) or mathematical guarantees, which weakens the credibility of the approach.  \n2. **Empirical Weaknesses**:  \n   - Results are inconsistent and lack statistical significance. On MultiMNIST, improvements over baselines fall within standard deviation ranges, and NLU experiments show mixed outcomes.  \n   - Limited experimental scope: The vision experiments use only one auxiliary task, failing to demonstrate scalability to true MTL scenarios.  \n   - Insufficient ablation studies and analysis of learned task weights (e.g., no visualization of weight dynamics or sanity checks).  \n3. **Novelty and Clarity**:  \n   - The method’s distinction from prior work (e.g., meta-learning for task weights) is unclear. Reviewers noted similarities to existing techniques like MAML and DIW.  \n   - Key algorithmic steps (e.g., subtracting 1 from weights in Algorithm 1) lack intuitive or theoretical justification, making the approach appear ad hoc.  \n4. **Presentation Issues**: Incomplete baselines, inconsistent reporting of standard deviations, and formatting errors further reduce confidence in the results.  \n\nWhile the idea of leveraging model updates for task weighting is promising, the paper does not meet the rigor expected of a top-tier venue. Addressing these issues would require substantial revisions, including theoretical analysis, expanded experiments, and clearer differentiation from prior work."
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper introduces Shoot Tree Search (STS), which is a planning algorithm that combines elements of MCTS and random shooting. The main idea is to perform multi-step expansions during the tree search, aiming to balance depth and breadth more explicitly. The experiments show that STS outperforms baselines like MCTS and random shooting in Sokoban and Google Research Football.\n\nNow, looking at the reviewers' comments. Reviewer 1 questions the technical novelty, pointing out that multi-step expansion isn't entirely new, citing prior work like Soemers et al. 2016. They also mention issues with the experiments, like the lack of ablation studies without value functions and potential biases. The presentation quality is criticized, with unclear algorithms and missing details in the experiments. Reviewer 2 is concerned about the lack of intuitive explanation for why STS works better, especially in sparse reward scenarios. They also note issues with the empirical results and missing references. Reviewer 3 likes the simplicity but is unsure about comparisons with MCTS at higher simulation counts and missing related work. Reviewer 4 thinks the novelty is low, comparing it to AlphaGo's approach, and questions the benefit over hyperparameter tuning. Reviewer 5 acknowledges the idea's potential but points out missing discussions on prior work and experiment setup confusion, though they lean towards acceptance after discussion.\n\nThe main themes across reviews are: novelty concerns, experiment thoroughness, presentation clarity, and comparison with existing methods. The authors need to address whether STS is truly novel compared to prior multi-step expansions, especially with the use of value functions. The experiments need more ablation studies, like testing without value approximators, and clearer explanations of setup. Presentation issues like algorithm clarity and proper referencing are also critical.\n\nThe reviewers had a mix of positive and negative points. Some found the idea simple yet effective with good empirical results, while others felt the novelty wasn't sufficiently established and the experiments incomplete. The discussion after reviews shows some movement towards acceptance if issues are addressed, but the concerns about prior work and technical depth remain significant.\n\nConsidering the standards of a top-tier conference, the paper needs to demonstrate clear novelty and thorough experimentation. The current version has potential but has several unresolved issues. The lack of ablation studies to isolate the effect of multi-step expansion and the insufficient discussion of how STS differs from prior work (especially with value functions) are major drawbacks. Additionally, presentation issues like unclear algorithms and missing details in experiments lower the paper's clarity and reproducibility.\n\nGiven these points, even though the idea is interesting, the paper in its current form may not meet the high bar for acceptance. The authors need to address the reviewers' concerns more comprehensively, particularly regarding novelty and experimental validation. Without these, the paper might not provide enough new insights or rigorous validation to warrant publication at a top-tier venue.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper introduces Shoot Tree Search (STS), a method blending MCTS and random shooting to balance depth/breadth trade-offs in planning. While the idea is interesting and the empirical results show promise, critical flaws prevent acceptance at a top-tier conference:  \n\n1. **Novelty Concerns**:  \n   - Multiple reviewers note that multi-step tree expansion (the core contribution) has precedents (e.g., Soemers et al. 2016). The authors fail to clearly differentiate STS from prior work, particularly in how the use of learned value functions (vs. rollouts) justifies novelty. Without explicit discussion of how STS’s design leverages modern function approximation to overcome limitations of earlier multi-step approaches, the technical contribution appears incremental.  \n\n2. **Insufficient Empirical Validation**:  \n   - Key ablation studies are missing. Reviewer 1 highlights the need to isolate the impact of multi-step expansion by testing STS and MCTS with rollouts (not value networks). The performance gains may stem from reduced bias in value estimates rather than the search mechanism itself, which undermines claims about STS’s superiority.  \n   - Comparisons with MCTS at higher simulation budgets (e.g., AlphaGo-scale) are absent, raising doubts about scalability (Reviewer 3).  \n\n3. **Presentation and Clarity Issues**:  \n   - Algorithms are poorly explained (e.g., missing definitions like `CALCULATE_TARGET`), and critical implementation details (e.g., value function training, hyperparameter choices) are omitted. Reviewer 5 notes confusion about experiment metrics (e.g., \\(N_p\\) exceeding \\(C\\)), which undermines reproducibility.  \n   - The intuition behind STS’s advantages (e.g., in sparse-reward settings) is underdeveloped (Reviewer 2).  \n\n4. **Weak Baseline Comparisons**:  \n   - Reviewer 4 questions whether the marginal gains over MCTS could be replicated via hyperparameter tuning or standard MCTS with rollouts (as in AlphaGo). The authors do not address this convincingly.  \n\nWhile the paper’s simplicity and empirical results are strengths, the unresolved issues in novelty, validation, and clarity make it premature for acceptance. Addressing these would require substantial revisions, including deeper theoretical analysis, expanded experiments, and clearer ties to prior work."
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what this paper is about. The title mentions \"New Bounds For Distributed Mean Estimation and Variance Reduction\" using lattices. The abstract explains that they're tackling the problem where previous methods depend on the input vectors' norms, which can be problematic when the mean itself has a large norm. Their approach uses lattice theory to remove this dependency, focusing instead on the distance between inputs. They claim both theoretical improvements and practical results.\n\nLooking at the reviewers' comments, there's a mix of positive and constructive feedback. Reviewer 1 appreciates the problem's importance and the lattice idea but suggests more discussion on existing approaches and some technical clarifications. They also point out minor typos and possible errors in the math. Reviewer 2 finds the result interesting but notes that the techniques aren't entirely novel, suggesting a simpler algorithm presentation. Reviewer 3 praises the clarity and motivation but raises concerns about experiments limited to n=2 and some technical details. Reviewer 4 highlights the problem's natural definition and thoroughness but mentions the algorithm doesn't match optimal bounds and constants in O-notation might be an issue.\n\nThe key strengths here are the theoretical contributions: new bounds that improve upon previous work by removing dependency on input norms, supported by both upper and lower bounds showing optimality. The use of lattices is a novel angle, even if some components (like quantization) are known. The practical experiments, though limited, show improvements over prior methods, which adds value.\n\nHowever, there are valid concerns. The experiments are limited in scale (only n=2), which might not fully demonstrate the algorithm's effectiveness in realistic distributed settings. The presentation could be improved, especially regarding clarity in certain proofs and algorithm descriptions. Some technical details, like the handling of lattices and communication structures, need more explanation. Also, the reviewers pointed out possible typos and mathematical inaccuracies that need correction.\n\nDespite these issues, the paper's core contribution seems significant. Addressing distributed mean estimation without relying on input norms is a meaningful advancement, especially for applications where data is centered around a non-zero mean. The theoretical results are strong, with optimality proofs, which is a big plus. The practical implementation, even if not matching the optimal bounds exactly, still shows benefits, which is important for real-world applications.\n\nThe reviewers' concerns are mostly about presentation, experiments, and some technical details, but these can be addressed in a revision. The major theoretical contributions and the problem's relevance to distributed machine learning make this paper a candidate for acceptance, provided the authors address the reviewers' feedback adequately.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel approach to distributed mean estimation (DME) by leveraging lattice theory to decouple error bounds from input vector norms, addressing a critical limitation in prior work. The theoretical contributions are significant: the authors establish asymptotically optimal communication-error trade-offs, provide matching lower bounds, and extend their method to practical cubic lattices with minimal loss in guarantees. These results advance the state-of-the-art in distributed optimization, particularly for scenarios where input vectors cluster around a non-zero mean (e.g., federated learning with non-centered data).\n\n**Strengths:**\n1. **Problem Relevance:** DME is foundational in distributed ML, and the paper addresses a key gap (norm-dependent error bounds) with practical implications for variance reduction in SGD.\n2. **Theoretical Rigor:** The lattice-based quantization framework achieves error bounds dependent only on input variance, supported by optimality proofs. The reduction to cubic lattices (with logarithmic overhead) balances theory and practicality.\n3. **Empirical Validation:** Experiments demonstrate improvements over baseline methods, though limited to small-scale settings (e.g., \\(n=2\\)).\n\n**Weaknesses:**\n1. **Technical Clarity:** Reviewers noted ambiguities in algorithm descriptions (e.g., convex hull selection, decoding steps) and minor mathematical inconsistencies (e.g., Lemma 20’s coloring argument, Theorem 27’s sublinearity claim).\n2. **Experiments:** The evaluation is narrow (small \\(n\\), inconsistent plot scales), reducing its persuasiveness for real-world distributed systems.\n3. **Comparison to Baselines:** Reviewer 1’s suggested baseline (iterative norm reduction) and connections to median-of-means estimators are underexplored.\n\n**Addressability:**\n- The paper’s core ideas are sound, and most criticisms (e.g., typos, clarity, experiment scale) can be addressed via revisions. The authors should:\n  - Expand experiments to larger \\(n\\) and clarify practical trade-offs.\n  - Elaborate on lattice construction/decoding and reconcile Theorem 27’s sublinearity claim.\n  - Compare with iterative norm-reduction baselines and tighten connections to related work (e.g., median-of-means).\n\n**Conclusion:**\nWhile the paper has minor flaws, its theoretical advancements and practical relevance justify acceptance at a top-tier conference. The lattice-based approach represents a non-trivial leap over prior norm-dependent methods, and the authors’ response to reviewer feedback can further strengthen the submission.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes a GNN model with a global self-attention mechanism using learnable spectral filters, aiming to handle both assortative and disassortative graphs. The abstract claims improved performance on disassortative graphs and comparable results on assortative ones.\n\nReviewer 1 has concerns about novelty, arguing that existing methods like ChevNet can already capture global information with a large K, and the issue might be overfitting rather than locality. They also question the computational complexity, pointing out that the method requires full eigen decomposition, which is O(V^2), making it unscalable. Reviewer 2 is positive but defers to others. Reviewer 3 appreciates the problem focus and experiments but raises scalability and dataset size issues. Reviewer 4 criticizes the novelty, comparing it to prior work like Non-Local GNNs, and scalability again.\n\nThe authors' responses in the rebuttal need to address these points. However, from the reviewers' post-discussion updates, it seems they weren't fully convinced, especially regarding complexity. The key issues are:\n\n1. **Novelty**: The approach uses spectral filters for global attention. But if existing methods (ChevNet, Non-Local GNNs) already address similar ideas, the incremental contribution might be insufficient for a top-tier conference. The rebuttal's argument against ChevNet's limitations might not have been persuasive enough.\n\n2. **Complexity**: The method's scalability is a major concern. Even with sparsification, the initial computation of the full attention matrix and eigen decomposition is O(V^2), which isn't feasible for large graphs. The rebuttal's claims about linear complexity might not hold if eigen decomposition is still required.\n\n3. **Experiments**: The datasets used for disassortative graphs are small. Larger benchmarks would better validate scalability and effectiveness. The lack of runtime comparisons weakens the argument for practical applicability.\n\nConsidering the top-tier conference standards, the paper's technical contribution needs to be both novel and impactful. While addressing heterophily is important, the reviewers point out that the approach may not sufficiently advance beyond existing methods. The complexity issues are significant, especially since scalability is crucial for real-world applications. Without clear evidence of efficient computation or substantial performance improvements on larger datasets, the paper might not meet the acceptance threshold. The split in reviews (some positive, some critical) leans towards rejection due to unresolved major concerns.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes a GNN with global attention via adaptive spectral filters to address disassortative graphs, a valid and impactful problem. However, critical flaws highlighted by reviewers undermine its suitability for a top-tier conference:  \n1. **Novelty Concerns**: The core idea of global attention and spectral filters overlaps with prior work (e.g., Non-Local GNNs, ChevNet). Reviewers argue the technical distinction from existing methods is insufficiently justified, and the rebuttal fails to convincingly differentiate the approach.  \n2. **Scalability Issues**: The computational complexity of eigen decomposition (O(V²)) and global attention remains unresolved. Claims of linear complexity are disputed, as preprocessing steps like full eigen decomposition are inherently quadratic, limiting applicability to large graphs.  \n3. **Experimental Limitations**: Evaluations on small disassortative graphs (hundreds of nodes) and lack of runtime comparisons weaken claims of practicality. Larger benchmarks (e.g., actor, Chameleon) and scalability analysis are absent.  \n\nWhile the problem is relevant and the empirical results on small datasets are promising, unresolved novelty and scalability concerns—critical for top-tier acceptance—outweigh the strengths. The paper would benefit from deeper theoretical analysis, scalability improvements, and broader experiments."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper proposes αVIL, a method for dynamic task weighting in multitask learning. The main idea is to adjust task weights based on model parameter updates between epochs. The reviewers have a mix of positive and negative points.\n\nReviewer 1 thinks the paper is well-written and the method is intuitive but lacks theoretical justification. They also question the experimental results, noting that the improvements aren't consistent and the baselines are limited. Reviewer 2 finds the method ad hoc and not novel enough, comparing it to existing meta-learning approaches. They also point out that the results are not clearly superior, especially on NLU tasks. Reviewer 3 is concerned about the algorithm's clarity, the experimental setup (only one auxiliary task in MultiMNIST), and the lack of significant improvements over baselines.\n\nThe authors' response would be important here, but since it's not provided, I have to rely on the reviews. The key issues are:\n\n1. **Theoretical Foundation**: The method lacks theoretical analysis. Without this, it's hard to assess why it works or under what conditions. The reviewers wanted some formal justification, which isn't present.\n\n2. **Experimental Rigor**: The experiments show mixed results. On MultiMNIST, the method is better but within standard deviation. For NLU tasks, it's inconsistent. The baselines might not be comprehensive enough. Also, some experiments use only one auxiliary task, which doesn't fully test MTL scenarios.\n\n3. **Novelty and Clarity**: Reviewers question how different αVIL is from existing methods like meta-learning. The algorithm's steps, like subtracting 1 from weights, aren't well-justified. The distinction between alphas and task weights is unclear.\n\n4. **Presentation and Analysis**: Minor issues like formatting, but also deeper concerns about statistical significance, weight evolution analysis, and dataset choices.\n\nFor a top-tier conference, the paper needs strong novelty, solid theory, and convincing experiments. The method's intuition is there, but the lack of theoretical backing and marginal empirical gains are red flags. The reviews suggest the results aren't statistically significant in many cases, and the setup might not fully exploit MTL scenarios. Without addressing these, the paper's contribution seems incremental. The reviewers lean towards rejection, especially Reviewer 3. The authors need more thorough analysis and experiments, which they might not have provided. Therefore, despite some strengths, the paper might not meet the high bar for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper introduces αVIL, a method for dynamically adjusting task weights in multitask learning (MTL) by leveraging task-specific parameter updates. While the idea is intuitive and addresses an important problem, the reviews highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n1. **Lack of Theoretical Justification:**  \n   The method is presented without formal analysis or theoretical grounding. Reviewers explicitly note the absence of guarantees or insights into why parameter update differences should correlate with task importance. This omission weakens the paper's contribution, as it leaves the mechanism’s validity unclear.\n\n2. **Marginal and Inconclusive Empirical Results:**  \n   - On MultiMNIST, αVIL’s improvements over baselines (e.g., single-task and standard MTL) are small and often within standard deviation ranges, suggesting statistical insignificance.  \n   - On NLU tasks, results are mixed, with no clear dominance over existing methods (e.g., DIW). Notably, all MTL methods underperform single-task training on WNLI, raising questions about the setup’s robustness.  \n   - Experiments use only one auxiliary task in the vision domain, failing to demonstrate scalability to more complex MTL scenarios with multiple auxiliary tasks.\n\n3. **Novelty and Algorithmic Clarity Concerns:**  \n   - The method’s connection to meta-learning (e.g., MAML) is underexplored, and reviewers argue that dynamic task weighting is not novel. Steps like subtracting 1 from task weights (Algorithm 1) lack intuitive or mathematical justification.  \n   - The distinction between task weights (α) and parameter updates (Δ) is unclear, leading to confusion about the algorithm’s mechanics (Reviewer 3’s Question 1).\n\n4. **Methodological and Presentation Issues:**  \n   - Key experimental details (e.g., baseline implementations, dataset balancing) are insufficiently described.  \n   - Critical analyses—such as statistical significance tests for NLU tasks, ablation studies on learned weights, and weight evolution trajectories—are missing.  \n   - Minor but persistent formatting issues (e.g., inconsistent terminology, missing headers in tables) detract from readability.\n\n**Conclusion:**  \nWhile the paper tackles a relevant problem and presents a plausible approach, the lack of theoretical analysis, inconclusive experiments, and unresolved questions about novelty and methodology prevent it from meeting the high standards of a top-tier conference. The authors should address these gaps through rigorous theoretical grounding, expanded experiments (e.g., more auxiliary tasks, statistical significance tests), and clearer algorithmic justification."
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the task is to decide whether to accept or reject this paper based on the reviews. Let me start by summarizing the key points from each reviewer.\n\nReviewer 1 thinks the paper is interesting and tackles an important problem but has issues with clarity, structure, and missing implementation details. They mention typos and sections that are hard to follow, like the loss functions not being introduced early enough. The strengths are the problem's importance and the evaluation on real data, but the weaknesses could make the paper hard to understand.\n\nReviewer 2 also finds the idea interesting but criticizes the experiments for being incomplete and unconvincing. They point out missing baselines (like standard MLM), lack of reproducibility details, and some results not being discussed (like linear model performing better on R²). The experiments need more depth and explanation.\n\nReviewer 3 is more positive, noting the problem's importance in political science and the potential impact. They suggest some improvements like synthetic data tests and comparisons to state-of-the-art methods but overall think the paper is publishable with its current contributions.\n\nReviewer 4 is critical, stating the approach isn't novel, related work is lacking, and the experiment section is poorly written. They mention missing references to related machine learning areas like LLP and collective models. The experiments lack clarity in settings and hyperparameters.\n\nNow, considering all these points. The paper's strengths are the application of deep learning to ecological inference, real-world data evaluation, and potential impact. However, the weaknesses are significant: missing baselines, unclear experiments, lack of reproducibility details, and insufficient discussion of related work. The reviewers also note structural issues and typos.\n\nFor a top-tier conference, the paper needs to have solid technical contributions, thorough experiments, and clear writing. The fact that key baselines are missing (like standard MLM) and the experiments aren't fully explained makes the results less convincing. The lack of comparison to existing methods in the field (as pointed out by Reviewer 4) is a big issue because it's unclear how this approach advances the state of the art. Additionally, the writing issues and missing details could hinder reproducibility, which is crucial.\n\nReviewer 3's positive take is balanced by their suggestions for improvement, but the other reviewers highlight major flaws. The paper's current state seems preliminary. Even though the problem is important, the execution might not meet the high standards of a top-tier conference without addressing these concerns. The authors need to better situate their work within existing literature, provide thorough experiments with all necessary baselines, and improve clarity and reproducibility. Without these, the paper's contributions aren't sufficiently validated.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper addresses an important problem in ecological inference with potential real-world impact, particularly in political science. However, significant shortcomings prevent it from meeting the standards of a top-tier conference:  \n\n1. **Technical Rigor and Reproducibility:**  \n   - Key baselines (e.g., standard multi-level models) are omitted, making it unclear whether the proposed deep learning components genuinely improve upon existing methods.  \n   - Experimental details (e.g., train/test splits, hyperparameters, architecture specifics) are insufficiently documented, undermining reproducibility.  \n   - Contradictory results (e.g., linear models outperforming deep models on R²) are not adequately addressed, raising questions about the method’s consistency.  \n\n2. **Clarity and Presentation:**  \n   - The paper’s structure and writing hinder readability. Critical concepts (e.g., Poisson/multinomial losses) are introduced late, and sections like the experimental setup lack coherence.  \n   - Typos and grammatical errors further detract from the presentation.  \n\n3. **Methodological Novelty and Related Work:**  \n   - The proposed approach is critiqued as incremental, with insufficient discussion of connections to established fields like learning from label proportions (LLP) or collective graphical models.  \n   - Key references (e.g., Flaxman et al. 2015 on distribution regression) are missing, weakening the paper’s positioning in the literature.  \n\n4. **Experimental Analysis:**  \n   - Claims about model interpretability (e.g., partisan crossover voting) are supported by conjecture rather than quantitative evidence.  \n   - Validation on synthetic data and comparisons to state-of-the-art ecological inference methods (e.g., R×C methods) are absent, limiting the generality of the results.  \n\nWhile the application to real-world election data is compelling, the paper’s unresolved methodological and presentational issues outweigh its strengths. Addressing these concerns would require substantial revisions, including expanded baselines, deeper analysis, and improved clarity. As such, the paper is not yet suitable for acceptance at a top-tier venue."
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question aims to bridge the gap between meta-learning theory and practice by introducing regularization terms based on theoretical assumptions. The reviewers have a mix of opinions, but there's a trend towards rejection. Let me break down the key points.\n\nReviewer 1's main issue is that the proposed regularizations aren't novel. They compare the spectral normalization and Frobenius norm regularization to existing methods like spectral normalization in GANs and standard weight decay. If the authors are just applying known regularization techniques under a new context (meta-learning), the contribution might be incremental. Also, they mention that other simple tricks in prior work (like HOW TO TRAIN YOUR MAML) have shown improvements, questioning the value here.\n\nReviewer 2 points out inconsistencies between the theoretical setup and the practical few-shot learning setting. For example, the lack of a validation set in the equations and the unclear task distribution. These could be critical because if the theory's assumptions don't align with the experimental setup, the results might not hold. Also, the meta-learning loss in Eq.4 differs from standard approaches like MAML, which uses a few gradient steps instead of full convergence. This discrepancy might affect the validity of the results.\n\nReviewer 3 appreciates the attempt to connect theory and practice but raises concerns about the statistical significance of the improvements. If the gains are within the standard deviation range, they might not be meaningful. They also suggest using synthetic experiments to validate whether the regularization helps when assumptions hold and hurts when they don't. This is a valid point for strengthening the paper's claims.\n\nReviewer 4 questions the novelty of the second regularization term, equating it to standard L2 regularization. They also note the lack of comparison with recent methods and technical details on computing subgradients for singular values. The application to more complex models like LEO is another concern, as the current approach is limited to linear layers.\n\nPutting this together, the main issues are:\n\n1. **Novelty**: The regularization methods are seen as not sufficiently novel, being similar to existing techniques. The authors need to clearly differentiate their approach from spectral normalization and weight decay. If the theoretical grounding is the main contribution, but the implementation is just applying known regularizers, the novelty might be lacking.\n\n2. **Theoretical and Experimental Alignment**: The assumptions in the theory (e.g., task distribution, validation set usage) might not hold in the experiments. The paper needs to address how these discrepancies affect the results. For example, if the tasks in benchmarks don't satisfy the theoretical assumptions, does the regularization still help?\n\n3. **Empirical Validation**: The improvements are marginal and possibly within the noise level. More rigorous experiments, including synthetic data where assumptions can be controlled, would strengthen the claims. Comparisons with recent methods are also lacking, making it hard to assess the true impact.\n\n4. **Technical Details**: The computation of subgradients for singular values is non-trivial and not adequately explained. Without proper implementation details, reproducibility is a concern.\n\nThe strengths include a well-organized paper and a valid motivation to connect theory with practice. However, the weaknesses highlighted by the reviewers, especially regarding novelty and empirical validation, are significant. For a top-tier conference, contributions need to be both novel and substantiated with clear, significant improvements. The current paper seems to fall short in these areas based on the reviews. The authors might need to address these concerns through more thorough experimentation and clearer differentiation from existing methods. Given the reviewers' consensus leaning towards reject, especially with Reviewer 1 and 4 explicitly recommending rejection, the paper might not meet the high bar for acceptance in its current form.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a well-motivated attempt to bridge meta-learning theory and practice by incorporating theoretical assumptions as regularization terms. However, critical flaws undermine its suitability for a top-tier conference:  \n1. **Lack of Novelty:** The proposed regularization methods (spectral and Frobenius norm penalties) are closely related to existing techniques like spectral normalization and standard weight decay, raising questions about technical originality.  \n2. **Weak Empirical Validation:** Improvements over baselines are marginal (often within standard deviation ranges) and lack comparisons with recent state-of-the-art methods. The absence of synthetic experiments to validate the necessity of theoretical assumptions further weakens the claims.  \n3. **Theoretical-Experimental Misalignment:** Key theoretical assumptions (e.g., task distribution properties, validation set usage) are not rigorously aligned with the experimental setup, casting doubt on the practical relevance of the theory.  \n4. **Technical Gaps:** Critical implementation details (e.g., subgradient computation for singular values) are omitted, and the method’s applicability to complex architectures (e.g., LEO) remains unaddressed.  \n\nWhile the paper’s motivation and theoretical grounding are commendable, the combination of incremental technical contributions, insufficient empirical rigor, and unresolved methodological concerns justifies rejection."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what the paper is about. The title is αVIL: Learning to Leverage Auxiliary Tasks for Multitask Learning. The main idea seems to be dynamically adjusting task weights in multitask learning using model parameter updates between epochs. The abstract mentions that existing methods rely on heuristics or extensive search, and their approach uses task-specific updates directly.\n\nLooking at the reviewers' comments, there are some common concerns. Reviewer 1 points out the lack of theoretical justification and limited baselines. The experimental results aren't consistently better than the baselines, and the improvements might not be significant. Reviewer 2 questions the novelty, comparing it to meta-learning approaches like MAML, and notes that the results are mixed. They also have questions about the experimental setup and clarity. Reviewer 3 is concerned about the algorithm's clarity, the experimental setup (especially the number of auxiliary tasks), and the statistical significance of results. All reviewers mention that the improvements are minor and possibly not significant.\n\nThe authors' response would be crucial here, but since we don't have that, I have to go by the reviewers' points. The key issues are:\n\n1. **Theoretical Foundation**: The method is intuitive but lacks theoretical analysis. Without this, it's hard to assess why it works or under what conditions.\n\n2. **Experimental Rigor**: The baselines are limited (only DIW and standard MTL), and the results don't show clear, consistent superiority. The MultiMNIST results are within a standard deviation, and NLU tasks have mixed outcomes. Also, using only one auxiliary task in MultiMNIST might not fully test the method's capability.\n\n3. **Novelty and Clarity**: Reviewers question how different αVIL is from existing meta-learning approaches. The algorithm's steps, like subtracting 1 from weights, aren't well-justified. There's confusion about the distinction between alphas and task weights.\n\n4. **Statistical Significance and Reporting**: The standard deviations for NLU tasks aren't provided, making it hard to judge if improvements are meaningful. The choice of datasets and the number of auxiliary tasks could be better justified.\n\nConsidering the standards of a top-tier conference, the paper needs to demonstrate clear novelty, solid theoretical or empirical contributions, and thorough experimentation. While the idea of using model updates for task weighting is interesting, the current presentation has gaps. The lack of theoretical backing and marginal empirical gains make it hard to justify acceptance. The reviewers' consensus leans towards rejection due to these shortcomings. The authors might need to address these issues with more analysis, broader experiments, and clearer differentiation from existing methods.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper proposes αVIL, a method for dynamically adjusting task weights in multitask learning by leveraging task-specific parameter updates. While the idea is intuitive and addresses an important challenge in multitask learning, several critical issues prevent it from meeting the standards of a top-tier conference:\n\n1. **Lack of Theoretical Justification**:  \n   The method is presented as a heuristic algorithm without formal analysis or theoretical grounding. Reviewer 1 and 3 emphasize the need for a theoretical framework to explain why parameter update differences are a valid proxy for task importance or how αVIL guarantees convergence. Without this, the method’s validity remains unclear.\n\n2. **Marginal Empirical Improvements**:  \n   The experimental results are not compelling. On MultiMNIST, αVIL’s gains over baselines (e.g., DIW) are within standard deviation ranges, suggesting statistical insignificance. On NLU tasks, performance is mixed, with no clear dominance over existing methods. Reviewer 3 notes that improvements are minor and inconsistent, failing to demonstrate a meaningful advantage.\n\n3. **Limited Baseline Comparison and Scope**:  \n   The baselines are sparse (single-task, standard MTL, and DIW), omitting stronger meta-learning approaches (e.g., MAML, Meta-Weight-Net) that also dynamically adjust task weights. Reviewer 2 highlights that αVIL’s novelty is questionable, as it resembles meta-learning techniques. Additionally, experiments use only one auxiliary task in MultiMNIST, which does not fully validate the method’s scalability to more complex multitask settings.\n\n4. **Algorithmic Clarity and Design Choices**:  \n   Reviewers raise concerns about the ad hoc nature of the algorithm. For example, subtracting 1 from task weights (Algorithm 1, line 11) lacks justification, and the distinction between α and task weights is unclear (Reviewer 3). These choices weaken the method’s credibility.\n\n5. **Incomplete Reporting and Analysis**:  \n   Standard deviations for NLU tasks are omitted, making it impossible to assess significance. Reviewer 2 requests sanity checks (e.g., learned weights for degenerate tasks) and trajectory analyses, which are absent. The authors also do not address dataset balancing or training protocols for the NLU tasks, raising reproducibility concerns.\n\n6. **Writing and Presentation Issues**:  \n   Minor but persistent issues (e.g., formatting, inconsistent terminology) detract from readability. The introduction and related work sections are underdeveloped, failing to contextualize αVIL within the broader literature.\n\n**Final Decision:**  \nWhile αVIL introduces a novel angle for task weighting, the lack of theoretical analysis, marginal empirical gains, and methodological ambiguities outweigh its contributions. The paper does not sufficiently advance the state of the art to warrant acceptance at a top-tier venue. Addressing these issues would require substantial revisions, including theoretical grounding, expanded experiments, and comparisons to stronger baselines.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this paper review. The paper is about using CycleGAN for automatic music accompaniment by treating Mel-spectrograms as images. The reviewers have a mix of positive and negative points. \n\nStarting with the strengths: The problem of automatic music accompaniment is definitely relevant and under-explored, especially in the audio domain. Using CycleGAN for unpaired image-to-image translation is a creative approach. The authors also proposed a new evaluation metric combining human judgment and objective measures, which is good because evaluating generative music models is tricky. The experiments on two tasks (bass to drums and voice to full song) show some promise, even if the results are mixed.\n\nBut the reviewers pointed out several issues. The title is misleading because \"music production\" is broader than what's actually done (accompaniment generation). The literature review misses key works in symbolic music generation, which is a big oversight. The method section lacks details on Demucs for source separation and the CycleGAN setup—without these, reproducibility is a problem. Also, the evaluation is shaky: the metrics aren't well-justified, the human evaluation has small sample size and potential biases, and there's no comparison to other models. The phase reconstruction from Mel-spectrograms isn't explained, which is crucial for audio quality.\n\nReviewer 3 mentions that using Demucs on FMA might introduce artifacts, affecting the training data quality. They also question the necessity of including FMA data without ablation studies. The technical details about Mel-spectrogram parameters (like windowing, quantization) are missing, making it hard to replicate. The justification for raw audio over symbolic methods isn't convincing, and the rebuttal of other representations seems dismissive without evidence.\n\nThe reviewers agree that while the idea is novel, the execution has flaws. The paper needs more rigorous experiments, better comparisons, clearer methodology, and addressing the impact of source separation artifacts. The evaluation section, especially the human metrics, needs more robustness. Without these, the paper doesn't meet the standards of a top-tier conference. Even though the concept is interesting, the current version has too many unresolved issues and missing details for acceptance.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel application of CycleGAN for automatic music accompaniment by treating Mel-spectrograms as images. While the problem is relevant and underexplored, significant methodological and experimental shortcomings prevent it from meeting top-tier conference standards.\n\n1. **Scope & Motivation:**  \n   - The title overpromises (\"Automatic Music Production\") compared to the actual focus on accompaniment generation. This misalignment weakens the paper's clarity.  \n   - The justification for raw audio over symbolic methods (e.g., MIDI) is insufficient. Claims that symbolic representations are inherently inferior for artistic purposes lack empirical or theoretical support and ignore successful hybrid approaches in music technology.\n\n2. **Technical Rigor:**  \n   - **Source Separation:** The use of Demucs for training data preparation is inadequately described. Potential artifacts from source separation and their impact on model performance are not addressed, raising concerns about data quality.  \n   - **CycleGAN Implementation:** Critical details (architecture, hyperparameters, training protocol) are omitted, hindering reproducibility.  \n   - **Phase Reconstruction:** The method for inverting Mel-spectrograms to audio is not explained, despite phase retrieval being crucial for perceptual quality. This omission undermines the technical validity of the results.\n\n3. **Experiments & Evaluation:**  \n   - **Datasets:** The filtering process for the FMA dataset is unclear, and the lack of song IDs or duration statistics limits reproducibility.  \n   - **Baselines:** No comparisons to existing symbolic or hybrid music generation models (e.g., MAESTRO, LakhNES) are provided, making it impossible to assess the novelty or superiority of the approach.  \n   - **Metrics:** The proposed evaluation metric (based on four subjective attributes) lacks theoretical grounding. The small expert panel (N=3) and absence of inter-rater agreement metrics weaken the reliability of human evaluations.  \n   - **Task Limitations:** Experiments are restricted to bass→drums and vocals→full arrangements. The exclusion of reverse tasks (e.g., drums→bass) and other instrument combinations limits the paper’s generalizability.\n\n4. **Ethical & Practical Considerations:**  \n   - The impact of source separation errors on downstream tasks (e.g., bleed-through in training data) is not quantified, which could invalidate the model’s real-world applicability.  \n   - Computational efficiency claims are unsubstantiated, with no discussion of runtime, hardware requirements, or scalability.\n\n5. **Reviewer Consensus:**  \n   All reviewers acknowledge the paper’s novelty but highlight critical gaps in methodology, evaluation, and reproducibility. The lack of ablation studies (e.g., FMA vs. MusDB-only training), insufficient technical details, and weak empirical validation render the contributions preliminary.\n\n**Final Decision:** Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes MQES, a method for efficient exploration in continuous RL by maximizing mutual information related to the optimal Q distribution. The reviewers have a mix of positive and negative points, but there are several recurring issues that need careful consideration.\n\nReviewer 1 points out that the practical implementation isn't clear, especially regarding how policies are parameterized and how the covariance adapts during training. They also mention that the experiments are preliminary, lacking ablation studies and hyperparameter analysis. Reviewer 2 and 3 echo the need for more detailed explanations of the target policy and algorithm implementation. They also question the experimental setup, like the horizon length and the choice of baselines. Reviewer 3 notes that the performance improvement over DSAC isn't significant except in specific environments, which might question the method's effectiveness. Reviewer 4 criticizes the writing and the lack of ablations, while Reviewer 5 raises clarity issues, missing baselines like VIME, and conceptual concerns about the exploration approach being tied to Q*.\n\nThe main strengths of the paper are the novel idea of combining information-theoretic exploration with distributional RL and the empirical results showing some improvement. However, the weaknesses are significant. The clarity of the method's presentation is a major issue, with multiple reviewers confused about the equations and algorithm details. The experiments are not thorough enough; the lack of ablation studies, hyperparameter sensitivity analysis, and comparisons to key baselines like OAC and VIME makes it hard to assess the method's true contribution. Additionally, the theoretical derivations have inconsistencies and unclear justifications, which undermines confidence in the approach.\n\nFor a top-tier conference, the paper needs to address these issues. The reviewers' concerns about the significance of results, missing comparisons, and unclear methodology are critical. Without addressing these, the paper doesn't meet the high standards required. The authors might need to revise the paper to include more comprehensive experiments, clarify their methodology, and better situate their work within existing literature. However, given the current state as per the reviews, the paper isn't ready for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a novel information-theoretic exploration strategy (MQES) for continuous RL, combining distributional Q-learning with mutual information maximization. While the core idea is promising, the paper faces significant criticisms that outweigh its strengths for a top-tier conference:  \n\n1. **Clarity and Presentation Issues:**  \n   - Multiple reviewers highlight unclear notation, inconsistent definitions (e.g., posterior vs. optimal policy in Eq. 9), and insufficient derivations. Key components like policy parameterization and adaptive covariance updates are not adequately explained.  \n   - The connection between mutual information maximization and exploration is not intuitively motivated, making the method’s theoretical foundation hard to validate.  \n\n2. **Incomplete Empirical Evaluation:**  \n   - Results lack statistical rigor (e.g., no p-values, only 5 seeds) and fail to demonstrate significant improvements over baselines like DSAC in most environments.  \n   - Critical ablations (e.g., isolating aleatoric vs. epistemic uncertainty contributions) and hyperparameter sensitivity analyses (e.g., β, N) are missing.  \n   - Comparisons to relevant baselines (e.g., OAC, VIME) are omitted, weakening the claim of state-of-the-art performance.  \n\n3. **Conceptual Concerns:**  \n   - The exploration strategy’s reliance on Q* (which depends on extrinsic rewards) raises doubts about its effectiveness in sparse-reward settings, a key motivation for advanced exploration.  \n   - Theoretical steps (e.g., Proposition 1) lack justification, and practical approximations (e.g., Eq. 20) are not rigorously validated.  \n\n4. **Reproducibility and Scope:**  \n   - Algorithm details (e.g., target policy updates in Algorithm 2) are relegated to appendices, and experiment settings (e.g., horizon length=100) deviate from standard benchmarks without justification.  \n\nWhile the idea of unifying epistemic/aleatoric uncertainty for exploration is novel, the paper’s unresolved clarity issues, limited empirical validation, and lack of conceptual depth make it unsuitable for acceptance in its current form. Addressing these concerns would require substantial revisions, including expanded theory, clearer exposition, and more comprehensive experiments."
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper introduces PERIL, a method combining meta-RL and imitation learning. The reviewers have a mix of positive and negative feedback. Let me break down the key points.\n\nReviewer 1 likes the idea of using demonstrations for meta-RL in sparse reward settings and the empirical results showing PERIL outperforms baselines. However, they're concerned about the method's complexity with multiple loss terms, unclear notation, and lack of clarity in the mathematical formulation. They also question whether all components are necessary and suggest more baselines/ablations. The problem formulation mismatch (POMDP vs fixed task) is a significant issue.\n\nReviewer 2 points out related work omissions, especially in combining Meta-IL and Meta-RL. They find the method's explanation unclear, especially the equations and how parts fit together. They stress the need for comparisons with prior works like Mendonca or Zhou. The experiments' setup (sparse vs dense rewards) is confusing, and some sections lack explanation.\n\nReviewer 3 acknowledges the motivation and implementation quality but criticizes the lack of comparison to prior works, insufficient experiments (especially robotics benchmarks), and unclear claims like zero-shot learning. They note the method's complexity and lack of statistical significance in results. The use of SAC experts for demonstrations is questioned for realism.\n\nReviewer 4 sees potential but highlights the strong assumption of expert trajectories conditioned on the learned latent z, which might not be practical. Clarity issues in problem statement and notation are major concerns. They also mention missing citations and the need for more exploration-heavy tasks to validate claims.\n\nCommon themes across reviewers:\n1. **Clarity and Formulation Issues**: Multiple reviewers found the notation inconsistent, equations unclear, and problem setup confusing (mixing POMDP with fixed task). The method's complexity with many loss terms isn't well-explained.\n2. **Experimental Weaknesses**: Lack of sufficient baselines (especially prior Meta-IL/Meta-RL hybrids), insufficient statistical analysis (few seeds, no confidence intervals), and experiments not fully validating claims (e.g., zero-shot, generalization).\n3. **Related Work and Citations**: Missing key citations, especially for Meta-IL/Meta-RL combinations and context-based meta-RL. Some prior works are not properly acknowledged.\n4. **Methodological Concerns**: The assumption of expert data conditioned on z is unrealistic. Some components (loss terms) lack justification through ablations. The use of SAC-generated demos might not reflect real-world scenarios.\n\nStrengths of the paper:\n- Novel approach combining meta-RL and IL with promising results.\n- Addresses important challenges in sparse rewards and fast adaptation.\n- Provides some empirical validation showing improved performance over PEARL and MetaIL.\n\nHowever, the weaknesses are significant. The clarity issues make it hard to assess the method's novelty and contribution. The lack of thorough experiments and comparisons with relevant baselines leaves doubts about the method's advantages. The methodological assumptions (expert data on z, SAC demos) might limit practical applicability. Without addressing these points, the paper doesn't meet the standards of a top-tier conference. The reviewers' concerns about the problem formulation mismatch and insufficient ablation studies are particularly critical. These issues are fundamental and would require major revisions, which might not be feasible in a conference review cycle.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel approach combining meta-reinforcement learning (meta-RL) and imitation learning (IL) to address sparse-reward adaptation, with promising empirical results. However, critical flaws hinder its readiness for publication in a top-tier conference:  \n1. **Clarity and Formulation Issues**: The problem setup conflates POMDP dynamics with fixed-task assumptions, creating confusion. Mathematical notation is inconsistent, and key components (e.g., loss terms) lack clear justification. The relationship to prior works like PEARL and MetaIL is underexplained, obscuring technical novelty.  \n2. **Insufficient Experiments**:  \n   - Missing comparisons to relevant baselines (e.g., Mendonca et al., Zhou et al.) that combine meta-RL and IL.  \n   - Limited statistical rigor (e.g., few seeds, no confidence intervals).  \n   - Ablations to validate the necessity of complex components (e.g., auxiliary losses) are absent.  \n3. **Unrealistic Assumptions**: The reliance on expert demonstrations conditioned on the learned latent variable \\( z \\) (which is unavailable to real-world experts) and SAC-generated \"expert\" data undermines practical applicability.  \n4. **Overstated Claims**: Assertions of \"zero-shot\" adaptation and generalization to unseen task families lack robust empirical support. The experiments focus on simple 2D/3D tasks and omit robotics benchmarks (e.g., MetaWorld).  \n5. **Related Work Gaps**: Key citations for meta-RL/IL hybrids (e.g., Ghasemipour et al.) and context-based meta-RL are missing, weakening the paper’s positioning.  \n\nWhile the core idea is compelling, the unresolved clarity issues, methodological limitations, and insufficient empirical validation prevent the paper from meeting the high standards of a top-tier venue. Major revisions addressing these concerns are needed."
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviewers' comments. The paper proposes CanvasEmb, a transformer-based model for layout representation learning using a large dataset of presentation slides. The main contributions seem to be the dataset and the pre-training approach. \n\nReviewer 1 points out that the dataset construction details are lacking. They mention the need for data statistics like the number of elements per slide and how the properties were parsed. This is important because without knowing the dataset's specifics, it's hard to assess the model's effectiveness. Also, the pretraining and evaluation tasks might overlap, which could mean the model isn't truly generalizing. The reviewer also suggests including more baselines like LayoutGAN and Neural Design Network, which are relevant for layout tasks.\n\nReviewer 2 raises issues about originality and significance. They note that while the task isn't new, using transformers is a different approach. However, the motivation for transformers over other models like GNNs isn't clear. The reviewer also points out missing comparisons with key prior works like Cao et al. 2019 and others. The evaluation is considered weak, and there's a need for more thorough experiments to show the model's advantages. The lack of explanation on why transformers are better than GNNs is a significant concern.\n\nReviewer 3 focuses on the evaluation's shortcomings. They mention that the baselines used (like decision trees) are too simple, and the tasks (element role labeling and image captioning) might not be challenging enough to justify the model's complexity. The auto-completion task is criticized for being too narrow, and the retrieval evaluation's manual scoring is seen as arbitrary. The reviewer suggests testing on more complex tasks and comparing with methods like convnets or existing layout generation models.\n\nAll three reviewers agree that the evaluation is insufficient. The paper lacks comparisons with important prior works, uses weak baselines, and doesn't adequately demonstrate the model's superiority. The dataset, while large, isn't described in enough detail, and there's a lack of clarity on how the pre-training tasks differ from the evaluation tasks. The motivation for using transformers over other architectures isn't well-justified, which weakens the paper's contribution.\n\nThe strengths of the paper include the large dataset and the application of transformers to layout representation. However, the weaknesses in evaluation, missing comparisons, and insufficient methodological details are major issues. For a top-tier conference, the paper needs to address these concerns to show a significant advancement over existing methods. Without these improvements, the paper's contributions aren't sufficiently validated.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel approach (CanvasEmb) for layout representation learning using transformer-based pre-training on a large-scale dataset of presentation slides. While the work has potential, several critical issues identified by the reviewers hinder its readiness for acceptance at a top-tier conference:\n\n1. **Dataset Transparency and Methodology Concerns**:\n   - Insufficient details about dataset construction (e.g., parsing methods, element statistics, template diversity) raise reproducibility concerns. The lack of clarity on whether evaluation tasks overlap with pretraining data (e.g., role labeling subset) undermines claims of generalizability.\n\n2. **Weak Evaluation and Baselines**:\n   - The evaluation tasks (element role labeling, image captioning) are not sufficiently challenging to justify the model’s complexity. Baselines like decision trees perform well, suggesting simpler methods might suffice. Comparisons to state-of-the-art layout models (e.g., Cao et al. 2019, LayoutGAN, READ) are missing, leaving the paper’s technical advancement unclear.\n\n3. **Motivation for Transformer Architecture**:\n   - The choice of transformers over graph neural networks (GNNs) or convolutional networks is not rigorously justified. GNNs, which naturally model relational data like layouts, are dismissed without empirical comparison, weakening the paper’s technical rationale.\n\n4. **Task Design and Impact**:\n   - Tasks like layout auto-completion are narrowly defined (masking single attributes) and fail to align with broader applications (e.g., full layout generation). The retrieval evaluation relies on ad-hoc human scoring, which lacks rigor.\n\n5. **Missing Analysis**:\n   - Critical details (e.g., maximum elements per slide, training hyperparameters) are omitted. The absence of qualitative results (e.g., visualization of generated layouts) limits insight into the model’s practical utility.\n\n**Strengths**:\n- The large-scale dataset (1M slides) is a valuable contribution, assuming it is released publicly.\n- The application of transformers to layout representation is novel and aligns with trends in self-supervised learning.\n\n**Weaknesses**:\n- The evaluation does not convincingly demonstrate superiority over existing methods. Key experiments (e.g., comparisons to GNNs, Cao et al.) are missing, and the tasks are not representative of real-world layout design challenges.\n\n**Conclusion**:\nWhile the paper explores an important direction in layout representation learning, the unresolved methodological gaps, weak evaluation, and insufficient justification for architectural choices prevent it from meeting the standards of a top-tier conference. Addressing these issues would require significant revisions, including expanded experiments, dataset transparency, and comparisons to stronger baselines.\n\n**Final Decision**: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper and the reviewers' comments. The paper proposes that the cerebellum acts as a decoupled neural interface (DNI) to solve the credit assignment problem in the brain, similar to how DNIs work in deep learning. They tested this with a model called CC-DNI on various tasks and compared it to models without the cerebellar component. The results show that CC-DNI performs better, especially in tasks that are cerebellar-dependent.\n\nReviewer 1 found the hypothesis intriguing but had concerns about clarity, especially regarding the focus on feedforward vs. recurrent settings and the comparison to existing DNI work. They also pointed out some technical details that need clarification. Reviewer 2 was more critical, arguing that the paper doesn't add new insights to either neuroscience or deep learning, just combines existing ideas. Reviewer 3 appreciated the novelty but wanted more experimental validation and concrete predictions, plus addressing how real gradients are computed in the brain.\n\nThe key issues here are: \n\n1. **Novelty and Contribution**: Does the paper provide new insights, or is it just combining existing concepts? Reviewer 2 thinks it's the latter, while Reviewer 1 and 3 see some potential but want more depth.\n\n2. **Technical Clarity and Focus**: The paper's explanation of how DNI applies to the cerebellum in different network settings (feedforward vs. recurrent) is confusing. The methods need clearer exposition, especially regarding bootstrapping and comparisons to prior DNI work.\n\n3. **Empirical Validation**: The link to biological cerebellum is weak. The authors need more direct comparisons with experimental data and concrete predictions. Reviewer 3 emphasizes this, pointing out that the model's predictions aren't well-supported by existing data.\n\n4. **Addressing Critical Questions**: The problem of how real gradients are computed in the brain isn't tackled, which Reviewer 3 considers a major oversight.\n\nConsidering the standards of a top-tier conference, the paper must make a significant contribution to either neuroscience or machine learning. The idea of linking cerebellum to DNI is novel, but the execution might be lacking. The experiments show that CC-DNI works better than truncated BPTT, but without comparing to original DNI, it's hard to assess its advantage. Also, the neuroscience predictions are too vague, which limits the impact.\n\nThe authors need to address how their model provides new insights beyond existing DNI work and validate their hypotheses with biological data. Without these, the paper might not meet the high bar for acceptance. However, the interdisciplinary approach and the potential to bridge deep learning with neuroscience are strengths. But given the reviewers' concerns, especially about novelty and empirical validation, the paper might be better suited for revision and resubmission rather than acceptance as is.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents an innovative hypothesis linking the cerebellum to decoupled neural interfaces (DNIs), proposing a novel cortico-cerebellar framework (CC-DNI) to address credit assignment and locking problems in neural computation. While the interdisciplinary approach is commendable and the experimental results demonstrate the utility of CC-DNI in tasks ranging from motor control to caption generation, several critical issues limit its readiness for publication in a top-tier conference:\n\n1. **Novelty and Contribution**:  \n   - The core idea—aligning cerebellar function with DNI principles—is intriguing but risks being perceived as a conceptual juxtaposition rather than a transformative insight. Reviewer 2’s critique that the paper does not advance either neuroscience or deep learning significantly is valid. While the framework is novel, the experimental results largely validate known advantages of DNI (e.g., handling temporal delays, sparsity benefits) rather than providing new theoretical or empirical breakthroughs. Comparisons to the original DNI architecture (Jaderberg et al.) are absent, making it unclear whether CC-DNI offers unique advantages over prior work.\n\n2. **Technical Clarity and Biological Plausibility**:  \n   - The conflation of feedforward and recurrent locking problems (Reviewer 1) creates confusion about the model’s scope. Clarifying the role of bootstrapping in training the cerebellar synthesiser (as in Jaderberg et al.) is essential for reproducibility and biological plausibility.  \n   - The biological link to the cerebellum remains underdeveloped. While the model incorporates cerebellar architectural features (e.g., sparse connectivity), it does not rigorously test predictions against experimental data (Reviewer 3). For example, claims about cerebellar deficits in tasks are not directly tied to empirical lesion studies or neural recordings.\n\n3. **Empirical Validation and Predictions**:  \n   - The experiments demonstrate that CC-DNI outperforms truncated BPTT, but this is insufficient to establish its neuroscientific relevance. Concrete predictions (e.g., cerebellar silence in well-learned tasks) are mentioned but lack empirical validation or alignment with existing data. The absence of direct comparisons to cerebellar patient data or neural activity limits the model’s impact on neuroscience.\n\n4. **Critical Unaddressed Questions**:  \n   - The mechanism for computing and transmitting real gradients in the brain (e.g., via the inferior olive) is dismissed as “outside the scope,” yet this is central to the biological feasibility of the proposed framework (Reviewer 3). The mismatch between the number of Purkinje cells and cortical neurons further challenges the model’s scalability.\n\n**Strengths**:  \n- The interdisciplinary synthesis of deep learning and cerebellar neuroscience is timely and could stimulate cross-domain collaboration.  \n- The sparse CC-DNI variant’s performance improvements (via decorrelation) are practically relevant for efficient learning architectures.  \n- The framework’s applicability to cognitive tasks (e.g., caption generation) broadens its potential impact.\n\n**Weaknesses**:  \n- Insufficient novelty in the DNI implementation and lack of comparison to baseline DNI models.  \n- Weak empirical grounding in neuroscience, with limited testable predictions or validation against biological data.  \n- Technical ambiguities in the locking problem’s framing and gradient-computation mechanisms.\n\n**Conclusion**:  \nWhile the paper’s hypothesis is compelling and the experiments are competently executed, the current version does not fully meet the high bar for a top-tier conference. The lack of direct comparisons to prior DNI work, underdeveloped biological validation, and unresolved technical questions about gradient computation in the brain diminish its impact. However, the interdisciplinary premise and experimental framework have significant potential. A major revision addressing these concerns—particularly by incorporating cerebellar patient data, clarifying the locking problem’s scope, and benchmarking against Jaderberg et al.—could make this work suitable for publication.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about using Fourier convolutional networks for 3D snapshot microscopy. The main idea is that traditional UNet-based decoders can't handle the non-local optical encoding required for this task, so they propose a Fourier-based approach which they show works better in simulations and on a lensless imaging dataset.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns but upgraded to a 6 after the rebuttal. Reviewer 2 is positive. Reviewer 3 is on the fence, saying it's marginally above acceptance but maybe better suited for an application-specific venue. Reviewer 4 points out that the technical novelty is limited compared to prior work like Rippel et al., and wants more comparisons and implementation details. Reviewer 5 is a weak reject, concerned about evaluation on real data and some inconsistencies in results. Reviewer 6 has some concerns about real-world applicability but thinks it's acceptable. Reviewer 7 finds the application strong but the ML novelty limited.\n\nThe main issues seem to be around technical novelty and empirical validation. The Rippel et al. paper is a key point here—how much does this work build on that? The authors addressed some of this in the rebuttal by adding references, but reviewers still feel the novelty is marginal. Also, the experiments are mostly on simulated data, which is a limitation. The lensless imaging results are there but maybe not directly applicable to the main microscopy claim.\n\nOn the positive side, the method works better than existing approaches in their tests, and the application is impactful. The paper is well-written and the limitations are discussed. However, top-tier conferences like ICLR (assuming that's the venue here) often prioritize technical novelty in ML. If the core idea is an application of existing Fourier methods without significant algorithmic innovation, that might be a problem. \n\nReviewer 3 mentions that the main ML ideas are from Rippel, so unless the authors can show that their global kernels are a novel extension, the technical contribution might not be enough. The application to microscopy is novel, but if the method is a straightforward application of prior work, then the significance for a ML conference could be questioned.\n\nThe empirical results are strong in simulation but lack real-world validation for microscopy. The lensless imaging part helps but isn't the main focus. Some reviewers question the fairness of comparisons (like parameter counts or training times), but the authors addressed some in the rebuttal.\n\nConsidering all this, the paper's strengths are in the application and thorough evaluation within simulations. The weaknesses are in technical novelty and real-world validation. For a top-tier conference, especially one focused on ML like ICLR, the lack of significant technical novelty might lead to rejection. However, if the application is deemed sufficiently important and the method's adaptation is non-trivial, it could be accepted. The reviewers are split, with some leaning towards accept (after revisions) and others towards reject. The consensus seems to be borderline. Given that some reviewers upgraded their scores post-rebuttal and the application's potential impact, maybe it's a borderline accept. But the technical novelty concerns are strong. I think the final decision would hinge on how the committee views the balance between application impact and ML novelty. Since the technical contribution is seen as incremental by several reviewers, rejection might be more likely for a top-tier venue.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper presents a well-executed application of Fourier convolutional networks to 3D snapshot microscopy and lensless imaging, demonstrating improved performance over UNet-based methods in simulations. However, key concerns raised by reviewers limit its suitability for a top-tier conference:  \n1. **Technical Novelty**: The core methodology builds heavily on prior work (e.g., Rippel et al., 2015), with insufficient differentiation in terms of machine learning innovation. While the global kernel application is novel in microscopy, reviewers noted that the Fourier-domain parameterization and spectral cropping techniques were previously explored, weakening the technical contribution.  \n2. **Empirical Validation**: The 3D microscopy experiments rely solely on simulated data, with no validation on real-world systems. This raises concerns about practical applicability, as optical aberrations and calibration errors (acknowledged in the limitations) could significantly impact performance. The lensless imaging results, while promising, are tangential to the paper’s primary focus.  \n3. **Comparison Fairness**: Questions about parameter counts, training times, and architectural choices (e.g., UNet’s poor performance) were not fully resolved. Reviewer 5 highlighted inconsistencies in speed comparisons, and Reviewer 4 requested clearer evidence that improvements stem from global kernels rather than Fourier optimization alone.  \n4. **Venue Fit**: Reviewer 3 and others noted that the work’s application-specific focus might be better suited for domain-specific conferences/journals rather than a machine learning venue, where methodological novelty is prioritized.  \n\nWhile the paper addresses an important problem and is rigorously executed, the incremental technical contributions and lack of real-world validation for the main application make it fall short of the high bar for acceptance at a top-tier ML conference."
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about improving model calibration on out-of-distribution (OOD) data using a post-hoc method that involves outlier exposure. The main idea is to calibrate predictions by leveraging a set of known corruptions in the calibration data. The reviewers have a mix of positive and negative points, so I need to weigh these carefully.\n\nReviewer 1's main criticism is that the approach assumes the OOD distributions are known, which they argue simplifies the problem unrealistically. They also mention missing related work. However, the authors provided a rebuttal, and the reviewer adjusted their score but still has concerns about incomplete baselines. Reviewer 2 finds the method simple and effective but points out some unclear motivations for the multiple image method and suggests additional experiments. Reviewer 3 is concerned about terminology clarity and the method's applicability to different corruptions. Reviewer 4 initially had concerns but increased their rating after seeing new experiments, though they still want more analysis on why the method works across different corruptions.\n\nThe paper's strengths include addressing an important problem (model calibration on OOD data), proposing a simple yet effective post-hoc method, and showing improved results on standard benchmarks. The weaknesses revolve around assumptions about known corruptions, potential lack of generalization to truly unseen OOD data, and clarity in presentation and methodology.\n\nThe key issues to consider are:\n1. **Assumption of Known Corruptions**: If the method requires knowing possible corruptions in advance, it might not handle truly novel OOD scenarios, which is a common real-world challenge. This could limit the practical applicability despite good benchmark results.\n2. **Comparison to Baselines**: Some reviewers note that existing methods (like those in cited works) weren't adequately compared, especially in scenarios where only a single image is available. The rebuttal added experiments, but there's still doubt about baseline completeness.\n3. **Clarity and Methodology**: Unclear definitions (e.g., p_max) and the rationale behind choosing specific corruptions (like contrast) could hinder reproducibility and understanding.\n4. **Generalization to Unseen Corruptions**: While the method works on tested corruptions, there's limited evidence it works on completely different OOD types (e.g., SVHN for CIFAR-10 models), which is a standard test in the field.\n\nThe authors addressed some concerns in the rebuttal, adding experiments and clarifications, which led some reviewers to raise their scores. However, the core issue remains whether the approach's assumption of known corruptions undermines its contribution. Top-tier conferences usually seek methods that advance the field significantly. If the method's success heavily relies on prior knowledge of possible shifts, it might be seen as incremental rather than groundbreaking.\n\nMoreover, the lack of thorough comparison with recent related works (as pointed out by Reviewer 1) is a red flag. Properly situating the work within existing literature is crucial for acceptance. The method's simplicity is a plus, but if it's only effective under restrictive conditions, its impact is limited.\n\nOn the positive side, the empirical results are strong on the tested corruptions, and the approach is post-hoc, making it widely applicable. The improved calibration metrics are a solid contribution. However, the unresolved issues regarding truly novel OOD scenarios and incomplete baselines might hold it back.\n\nIn conclusion, while the paper has merits, the concerns about assumptions and baseline comparisons are significant. For a top-tier conference, these issues might warrant rejection unless the authors can convincingly address them, which the rebuttal partially did but not entirely. The split among reviewers (some raising scores post-rebuttal, others remaining critical) suggests it's on the borderline. However, given the importance of the problem and the demonstrated effectiveness within their setup, it might still be a candidate for acceptance if the committee values practical improvements over theoretical novelty. But considering the remaining flaws highlighted, especially the fundamental critique from Reviewer 1 about problem setup, rejection seems more likely.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses a significant problem—improving calibration on OOD data—and proposes a simple post-hoc method with empirical improvements on benchmark datasets. However, critical flaws remain unresolved:  \n1. **Assumption of Known Corruptions**: The method assumes access to a predefined set of corruptions during calibration, which simplifies the problem and limits applicability to truly novel, unanticipated OOD scenarios. This undermines its relevance to real-world deployment, a key motivation for OOD calibration.  \n2. **Incomplete Baselines and Comparisons**: Reviewers noted missing comparisons to recent calibration methods under distribution shift (e.g., Park et al. 2020, Wang et al. 2020). While the rebuttal added experiments, concerns persist about whether the method outperforms existing approaches in single-image settings or generalizes beyond contrast-based corruptions.  \n3. **Clarity and Generalization**: The paper lacks clear definitions (e.g., \\( p_{\\text{max}} \\)) and theoretical intuition for why contrast-based calibration transfers to other corruptions. Limited evaluation on truly novel OOD data (e.g., SVHN for CIFAR-10 models) weakens claims of robustness.  \n4. **Marginal Novelty**: The core idea—using outlier exposure for calibration—builds directly on prior work (e.g., Ovadia et al. 2019). The incremental improvement, while empirically validated, does not constitute a conceptual breakthrough for a top-tier venue.  \n\nWhile the method shows promise, the unresolved issues and restrictive assumptions make it more suitable for a specialized workshop or mid-tier conference. For a top-tier publication, the paper needs stronger theoretical grounding, broader evaluation, and clearer differentiation from existing approaches."
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding the paper's main contribution. The authors propose Block Minifloat (BM), a new floating-point format for training DNNs with 4-8 bits. The key idea is introducing a shared exponent bias across a block, allowing smaller exponents and more efficient hardware, specifically using a Kulisch accumulator. The hardware results show significant area and energy savings compared to FP8 and FP32 while maintaining accuracy.\n\nReviewer 1 had concerns about the novelty of shared exponents but acknowledged the hardware benefits. They asked for more details on why smaller exponents help the accumulator and how denorm handling works. The authors addressed some points in their response, but Reviewer 1's initial concerns might still linger, especially regarding the accumulator's impact on convergence and denorm handling complexity.\n\nReviewer 2 initially thought the BM concept was a simple extension of BFP but recognized the hardware contribution as strong. They were impressed by the synthesis results but had a major issue with using different formats for forward/backward passes, which the authors clarified. However, the reviewer still noted that other methods use the same format throughout, so the overhead of multiple formats needs proper discussion. The minor issues about power measurement details and the sudden introduction of log-BM are valid but might not be critical.\n\nReviewer 3 praised the generality and thorough exploration but found the presentation confusing. They wanted more mathematical details on exponent alignment and block size determination. The tables were also unclear, which could hinder reproducibility. The additional comments about equation clarity and comparisons to Flexpoint are important for readers to grasp the work's context.\n\nNow, considering the strengths: The paper presents a novel approach with significant hardware efficiency gains, validated through extensive experiments and RTL synthesis. The results are compelling, showing BM6/8 matching FP32 accuracy with better hardware metrics. Addressing the major concern about multiple formats in hardware (as per author response) strengthens the case.\n\nWeaknesses include some lack of clarity in explaining key concepts (like the accumulator's dependency on exponent size and denorm handling) and presentation issues in tables and equations. However, the core contribution—efficient hardware implementation with proven benefits—is solid. The reviewers' concerns were partially addressed, and the methodology seems rigorous despite minor flaws.\n\nTop-tier conferences value both technical innovation and thorough evaluation. The hardware synthesis and comparison to existing formats are strong points. While the paper has presentation issues, the technical merits and potential impact on efficient DNN training are significant. The reviewers' initial concerns were mitigated, especially after author clarifications, leading to an overall positive assessment.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper introduces Block Minifloat (BM), a novel floating-point representation for low-precision DNN training, emphasizing hardware efficiency via shared exponent bias and Kulisch accumulators. The work demonstrates competitive accuracy with 4-8 bit formats and significant hardware improvements (e.g., 4.1× smaller area and 2.3× lower energy vs. FP8). Below is an analysis of strengths and weaknesses based on reviewer feedback:\n\n### **Strengths:**\n1. **Technical Contribution:**  \n   - BM’s shared exponent bias reduces exponent bitwidth, enabling dense integer-like hardware for FMA operations. This addresses a critical bottleneck in low-precision training hardware.  \n   - The hardware evaluation (RTL synthesis, area/power metrics) is rigorous and exceeds typical quantization papers, showing practical viability.  \n   - The exploration of design space (e.g., block size, exponent/mantissa configurations) and validation across models (ResNet, VGG, Transformer) strengthens generality.\n\n2. **Empirical Results:**  \n   - BM6 achieves near-FP32 accuracy on ImageNet with ResNet-18, and BM8 matches FP32 accuracy while improving computational density. These results are compelling for deployment in resource-constrained settings.  \n   - Hardware metrics (area, energy) are convincingly quantified, with BM6/BM8 outperforming FP8/FP32.\n\n3. **Addressing Reviewer Concerns:**  \n   - The authors clarified that hardware overhead for supporting multiple formats (forward/backward passes) is accounted for in their synthesis, alleviating Reviewer 2’s major concern.  \n   - The discussion of denorm handling (Reviewer 1) and comparisons to FlexPoint (Reviewer 3) adds context, though some details remain underdeveloped.\n\n### **Weaknesses:**\n1. **Clarity and Presentation:**  \n   - Key concepts (e.g., exponent bias alignment, Kulisch accumulator benefits) lack sufficient mathematical and intuitive explanation (Reviewers 1, 3).  \n   - Tables and equations (e.g., undefined columns in Table 2, abrupt introduction of Equation 6) are confusing, hindering reproducibility.  \n   - The role of log-BM in VGG-16 and the justification for multiple BM formats per training step need clearer rationale.\n\n2. **Methodological Gaps:**  \n   - The impact of Kulisch accumulator settings on model convergence (Reviewer 1) and power measurement methodology (Reviewer 2) require more detail.  \n   - While the authors claim minimal denorm-handling overhead, the hardware complexity for addends and result normalization is not fully addressed (Reviewer 1).\n\n3. **Novelty vs. Prior Work:**  \n   - BM builds on block floating-point (BFP) by introducing shared exponent bias. While the hardware implementation is novel, the core idea of shared exponents is incremental (Reviewer 2). However, the integration with Kulisch accumulators and end-to-end training validation distinguishes it.\n\n### **Overall Assessment:**  \nThe paper’s technical merits—novel hardware design, rigorous evaluation, and practical efficiency gains—outweigh its presentational and methodological shortcomings. The work advances low-precision training by demonstrating a feasible path to efficient hardware without sacrificing accuracy. While the exposition could be improved, the contributions are significant for both the machine learning and systems communities.\n\n**Final Decision: Accept**"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about reducing the number of neurons in ReLU networks using a new algorithm based on regularization theory. The reviewers have a mix of concerns and some positive points. Let me start by summarizing the key points from each section.\n\nFirst, the abstract says they introduce a reduction algorithm that significantly cuts down neurons with minimal function change. The keywords include pruning, compression, etc. The reviewers have several issues. Reviewer 1 initially had concerns but retracted some after author response, but still wants more motivation and better presentation. Reviewer 2 and 3 are more negative, pointing out poor writing, lack of comparisons to SOTA, and weak experiments (only toy and MNIST). Reviewer 3 also mentions that the method is similar to highway networks. Reviewer 4 has major clarity issues, insufficient comparisons, and questions about the bottleneck architecture. Reviewer 5 and 6 also highlight problems with referencing unpublished work, lack of derivation for the method, and experiments not being convincing.\n\nLooking at the common themes: the main issues are around the experiments being too limited (only simple datasets), lack of comparison to existing pruning methods, unclear presentation, and reliance on unpublished work. The strengths are the theoretical grounding and a new algorithm, but the execution in validation and presentation is lacking.\n\nFor a top-tier conference, experiments on MNIST alone are usually not enough. They need to show results on more complex datasets like CIFAR or ImageNet. Also, comparing to existing pruning techniques (like magnitude pruning, etc.) is crucial to demonstrate the method's advantage. The reviewers pointed out that the authors didn't do this, which is a big weakness.\n\nThe references to unpublished work (especially their own) make it hard to verify the theoretical claims. If the theory is essential to the method, then the paper can't be properly evaluated without it. This is a red flag for reviewers. Also, the presentation issues like typos and unclear explanations further hinder understanding.\n\nThe authors did respond to some concerns, like clarifying equations, but didn't address the major experimental shortcomings. The reviewers still find the experiments insufficient. Without comparisons to SOTA and more complex datasets, the contribution's impact is unclear.\n\nAnother point is the similarity to existing architectures like highway networks. If the proposed architecture isn't sufficiently novel, that's a problem. The reviewers note this, which could indicate a lack of novelty in the approach.\n\nIn terms of the algorithm's steps, combining always-active neurons might be known in literature, which reduces the perceived novelty. The third step using weighted k-means isn't well-explained, and the reviewers question the derivation and correctness here.\n\nConsidering all these factors, even though the idea has potential, the paper as it stands doesn't meet the standards for a top-tier conference. The lack of thorough experimentation, comparisons, clarity, and reliance on unpublished theory are major issues. The reviewers' consensus leans towards rejection due to these unresolved problems.\n</think>\n\nThe paper proposes a novel algorithm for pruning ReLU networks by leveraging regularization theory, aiming to reduce neuron count while preserving function approximation. However, significant concerns raised by reviewers warrant rejection:\n\n1. **Theoretical Validation & Unpublished Work**: The paper heavily relies on an unpublished work by the authors, making it impossible to verify foundational claims. Key sections (e.g., Section 3) lack rigor and clarity, with critical definitions and proofs omitted. This undermines the paper’s credibility and reproducibility.\n\n2. **Experimental Weaknesses**: \n   - **Limited Scope**: Experiments are restricted to toy datasets and MNIST, which are insufficient for a top-tier conference. More complex benchmarks (e.g., CIFAR, ImageNet) are absent.\n   - **Lack of Baselines**: No comparisons to state-of-the-art pruning methods (e.g., magnitude-based, gradient/Hessian-aware pruning) are provided, making it impossible to assess the method’s novelty or effectiveness.\n   - **Statistical Significance**: Results lack error margins or statistical validation, raising doubts about robustness.\n\n3. **Presentation & Clarity**: \n   - The paper is poorly organized, with vague contributions and excessive reliance on footnotes for critical details. Key concepts (e.g., \"P-functional,\" \"stack network\") are inadequately defined.\n   - Typos and grammatical errors further hinder readability.\n\n4. **Novelty Concerns**: \n   - The proposed architecture resembles highway networks (Srivastava et al., 2015), and steps like pruning always-active neurons are deemed \"well-known\" by reviewers. The weighted k-means clustering step lacks derivation, leaving its theoretical basis unclear.\n\n5. **Methodological Gaps**: \n   - The computational complexity of the algorithm is unaddressed, and hyperparameters (e.g., cluster counts) are not tested for sensitivity.\n   - The connection between the theoretical framework (Section 3) and the practical algorithm is tenuous, with approximations introduced without justification.\n\nWhile the core idea of combining regularization theory with pruning is promising, the paper fails to demonstrate its superiority over existing methods, validate claims rigorously, or provide sufficient experimental evidence. The unresolved issues in presentation, theory, and experimentation make it unsuitable for a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about reducing the number of neurons in ReLU networks using a new algorithm based on regularization theory. The reviewers have a mix of concerns and some positive points. \n\nFirst, looking at the abstract and the reviewers' comments. The main contribution is a reduction algorithm that prunes neurons with minimal loss in accuracy. The reviewers point out several issues. Reviewer 1 initially had concerns but retracted some after author responses, but still mentions the need for more detailed explanations and experiments. Reviewer 2 and 3 are more critical, citing poor organization, lack of comparisons with state-of-the-art methods, and experiments only on toy datasets and MNIST. Reviewer 3 also mentions that the paper heavily relies on unpublished work, which is a problem for verification. Reviewer 4 has major clarity issues and insufficient comparisons. Reviewer 5 and 6 echo similar concerns about the reliance on unpublished work, lack of motivation, and insufficient experiments.\n\nThe key issues seem to be:\n\n1. **Reliance on Unpublished Work**: The paper references the authors' own unpublished work, making it hard for reviewers to verify the theoretical foundations. This is a big red flag because the core theory isn't accessible for peer review.\n\n2. **Experimental Evaluation**: The experiments are limited to simple datasets (toy data and MNIST) without comparisons to existing pruning methods. For a top-tier conference, more rigorous testing on complex datasets like CIFAR or ImageNet is expected. The lack of statistical significance measures (like standard errors) also weakens the results.\n\n3. **Clarity and Presentation**: Multiple reviewers note the paper is poorly written, with typos, unclear explanations, and sections that are hard to follow. The method's derivation isn't adequately explained, especially the clustering step, which seems crucial but isn't well-motivated.\n\n4. **Novelty and Comparison**: The pruning steps (removing inactive neurons, combining always-active ones) are considered by some reviewers as not novel. The paper doesn't position itself well against existing pruning techniques, which is critical in a crowded field like neural network compression.\n\n5. **Technical Details**: Questions about the architecture (e.g., bottleneck layers) and the computational complexity of the algorithm aren't sufficiently addressed. The theoretical claims (like the equivalence of equations) need more precise proofs or references.\n\nOn the positive side, the algorithm does show some effectiveness in reducing neurons without significant accuracy loss on the tested datasets. However, given the standards of a top-tier conference, the issues raised by the reviewers are substantial. The reliance on unpublished work and the lack of thorough experimentation and comparisons are major drawbacks. Even if the theoretical underpinnings are correct, the presentation and experimental validation don't meet the expected bar. The reviewers' consensus leans towards rejection, with most pointing out that the paper isn't ready for publication in its current state.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper proposes a neuron reduction algorithm for ReLU networks based on regularization theory, aiming to prune neurons while preserving accuracy. However, significant concerns raised by reviewers warrant rejection:\n\n1. **Theoretical Validity and Unpublished Work**:  \n   The paper heavily relies on theoretical results from the authors’ unpublished work, making it impossible to verify foundational claims. Reviewer 5 notes that Section 3, which connects the method to theory, cannot be validated due to inaccessible references. This undermines the paper’s credibility, as critical theoretical assumptions (e.g., equivalence of optimization problems in Eqs. 1–2) remain unverified.\n\n2. **Insufficient Experiments**:  \n   Experiments are limited to toy datasets and MNIST, which are insufficient for a top-tier conference. Reviewer 3 and 5 emphasize the need for evaluations on more complex benchmarks (e.g., CIFAR) and comparisons to state-of-the-art pruning methods (e.g., magnitude-based, Hessian-aware pruning). The lack of statistical significance measures (e.g., error bars) further weakens the results.\n\n3. **Novelty and Positioning**:  \n   Reviewers 2, 3, and 6 argue that core components of the method (e.g., pruning always-active neurons) are not novel and resemble existing techniques like highway networks. The paper fails to differentiate itself from prior work or justify its unique contributions convincingly.\n\n4. **Clarity and Presentation**:  \n   Multiple reviewers highlight poor organization, undefined terms (e.g., \"P-functional\"), and unresolved typos. Key steps (e.g., the weighted k-means clustering in Step 3) lack motivation and derivation, leaving the method’s mechanics unclear. Reviewer 4 criticizes the use of footnotes for critical definitions, which disrupts readability.\n\n5. **Technical Gaps**:  \n   The computational complexity of the algorithm is unaddressed, and the role of bottleneck layers (Figures 4–5) is unexplained. Reviewer 4 points out that the proof for neuron reduction via Carathéodory’s theorem is oversimplified, omitting width requirements (e.g., \\(n_j \\geq d+1\\)).\n\n**Strengths**:  \nThe algorithm demonstrates promising neuron reduction rates (90–99%) on MNIST with minimal accuracy loss, suggesting practical potential. The theoretical framework, if validated, could offer insights into regularization and network compression.\n\n**Conclusion**:  \nWhile the idea of leveraging ReLU properties for pruning is interesting, the paper’s reliance on inaccessible theory, limited experiments, and unclear presentation make it unsuitable for a top-tier venue. Addressing these issues would require substantial revisions, including rigorous comparisons, expanded experiments, and self-contained theoretical derivations.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes a method called MIMO, which trains a single neural network with multiple inputs and outputs to create subnetworks that act like an ensemble. The main claim is that this approach achieves robustness and uncertainty benefits without the computational cost of multiple forward passes. \n\nReviewer 1 points out strengths like minimal parameter increase and the use of MIMO for efficiency but has concerns about the \"free\" claim since there's still a 1% parameter increase. They also mention missing details on where the parameters come from and suggest more datasets. \n\nReviewer 2 praises the thorough experiments and clear writing but wants more discussion on related work like BatchEnsemble and a deeper understanding of why subnetworks become independent. They also note that the independence might be context-dependent.\n\nReviewer 3 finds the idea clever and the results compelling but is concerned about the lack of theoretical analysis, the practical limitation of M being small, and the testing setup where the same input is replicated. They also question the effect of mixing training examples.\n\nReviewer 4 has mixed feelings. They acknowledge the method's strengths in experiments and clarity but criticize the lack of computational cost metrics (like FLOPs) and insufficient related work citations. They also argue that the originality is low due to existing multi-branch architectures and question the significance, pointing out that the batch size trade-off isn't properly addressed.\n\nNow, considering all this, the paper's main contributions are the MIMO approach for efficient ensembling and empirical demonstration of subnetworks' independence. The strengths include strong experimental results across multiple datasets and metrics, simplicity of the method, and efficient inference. However, the weaknesses are the lack of theoretical grounding, insufficient comparison with related works, and some methodological concerns like parameter increase and testing setup.\n\nThe reviewers generally agree the experiments are solid, which is crucial for a top-tier conference. However, the concerns about originality and significance might be more critical. Reviewer 4's point about existing multi-branch architectures and missing citations could undermine the paper's novelty. Also, the practical impact is questioned because the method's advantage over traditional ensembles might not be dramatic if M is limited. \n\nThe authors addressed some concerns in their responses, but the reviews don't mention specific rebuttals. For a top-tier conference, theoretical analysis and thorough related work are often expected. The lack of these elements might be a red flag. However, the empirical results are strong, which can sometimes compensate for theoretical gaps, especially in applied ML conferences.\n\nBalancing all factors, the paper presents a novel and practical approach with solid empirical validation but has some shortcomings in theoretical explanation and related work. Given that top conferences value innovation and empirical results, and considering that the method's efficiency gain is significant (single forward pass), the paper might still be acceptable. However, the concerns about originality and the batch size trade-off need careful consideration. If the authors adequately addressed these in their response (which we don't see here), it could tip the scales. Without seeing the author response, but based on the reviews, the paper has merits but also notable flaws. However, since the reviews are mixed but lean towards strengths in experiments and method, I'd lean towards acceptance with the expectation that the authors address the concerns in the final version.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper presents a novel approach (MIMO) for training subnetworks within a single model to achieve ensemble-like robustness and uncertainty benefits with minimal computational overhead. The strengths and weaknesses highlighted by reviewers are analyzed as follows:\n\n1. **Strengths:**\n   - **Empirical Robustness:** The method demonstrates strong performance across multiple benchmarks (CIFAR10/100, ImageNet, OOD variants) in accuracy, calibration, and uncertainty metrics, surpassing single-forward-pass baselines and approaching traditional ensemble performance.\n   - **Efficiency:** By enabling ensembling in a single forward pass, the method reduces inference latency significantly compared to multi-model ensembles, a practical advantage for real-world applications.\n   - **Subnetwork Independence Analysis:** The paper provides empirical evidence (e.g., loss landscapes, parameter projections) that subnetworks behave like independent models, a compelling contribution to understanding neural network dynamics.\n\n2. **Weaknesses and Concerns:**\n   - **Theoretical Underpinnings:** Reviewers note the lack of theoretical analysis explaining why subnetworks become independent or how input mixing during training generalizes to replicated test inputs. This limits mechanistic understanding.\n   - **Originality and Related Work:** The paper insufficiently discusses connections to prior multi-branch architectures (e.g., BatchEnsemble) and multi-input/output strategies in robustness/uncertainty literature. This weakens claims of novelty.\n   - **Practical Trade-offs:** While parameter growth is minimal (1%), computational costs (FLOPs) and the impact of reduced effective batch size during inference (due to input replication) are not rigorously quantified. Reviewer 4 criticizes this as \"deceptive.\"\n   - **Scope of Evaluation:** Experiments are limited to image classification; testing on non-vision tasks (e.g., NLP) or more diverse datasets (e.g., OpenImages) could strengthen generalizability claims.\n\n3. **Reconciling Concerns:**\n   - The empirical results are robust and reproducible, a hallmark of top-tier conference standards. While theoretical gaps exist, the practical benefits (efficiency gains, strong performance) are compelling.\n   - The lack of computational cost metrics (FLOPs) and detailed comparisons with related methods (e.g., BatchEnsemble) could be addressed in a revision, but the core idea remains novel and impactful.\n\n**Final Decision:** Accept  \n**Rationale:** Despite methodological and presentation flaws, the paper’s empirical contributions—demonstrating that a single network can embed diverse, independent subnetworks for efficient ensembling—are significant and align with the innovation expected at top conferences. The concerns (e.g., theoretical gaps, related work) are addressable in revisions and do not outweigh the practical advance."
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. So, the paper is about applying boosting techniques from supervised learning to reinforcement learning (RL), specifically in Markov Decision Processes (MDPs) with large state spaces. The main claim is that their algorithm's sample complexity doesn't depend on the number of states, which is a big deal because that's a common bottleneck. \n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 1 initially had a 3 but moved to 5 after author response, but still wants more clarity. Reviewer 2 and 3 point out that the technical novelty might be incremental compared to prior work like Hazan and Singh 2021. Reviewer 4 and 5 have major issues with clarity, undefined notations, and lack of experiments. Reviewer 6 acknowledges the interesting idea but also mentions the paper is hard to follow and needs more motivation for definitions.\n\nThe strengths mentioned by reviewers include the interesting application of boosting to RL, solid theoretical analysis, and the state-space independent sample complexity. Weaknesses are around limited novelty compared to existing work, presentation issues, lack of experiments, and unclear definitions/notations.\n\nThe technical correctness seems okay, with most reviewers giving 3 or 4. The main issues are clarity and novelty. The paper's theoretical contributions are there, but if the presentation is so poor that readers can't follow, that's a problem. Also, the lack of experiments when prior work (Hazan & Singh) included them is a downside. \n\nTop-tier conferences usually expect both theoretical and empirical contributions unless the theory is groundbreaking. Here, the reviewers are split on the novelty. Some see it as incremental, others as significant. The clarity issues are a red flag. If the paper is too dense and notation-heavy without proper explanations, it's hard to assess its true impact. \n\nThe authors might need a major rewrite to address the clarity problems, better differentiate from prior work, and perhaps add experiments. But given that this is a conference submission, maybe they can do a shepherding process? However, the reviewers' concerns about the presentation and incremental contribution might outweigh the strengths. \n\nIn the end, if the core idea is novel enough and the technical results are solid, but the presentation is fixable, maybe a weak accept. But given that multiple reviewers point out significant clarity issues and incremental novelty, and the lack of experiments, it might lean towards rejection. Especially since top-tier conferences have high standards, and even a good idea can be rejected if not well-presented or if the contribution isn't clearly beyond prior work.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper presents a theoretically grounded approach to applying boosting techniques in reinforcement learning (RL), with a focus on achieving state-space-independent sample complexity. While the core idea of leveraging boosting in RL is promising and aligns with current interests in scalable RL methods, the paper faces significant challenges that outweigh its strengths in its current form:\n\n1. **Clarity and Presentation Issues:**  \n   Multiple reviewers (Reviewers 1, 4, 5, 6) highlight severe readability problems, including undefined notation, inconsistent terminology, and overly dense mathematical exposition. For example, Reviewer 4 notes that critical terms like $\\mathbb{\\Lambda}$, $d^{\\pi}$, and $R_{\\mathcal{W}}(M)$ are not properly defined, making the technical contributions inaccessible. Reviewer 5 describes the paper as \"extremely dense and hard to read,\" with insufficient intuition or narrative coherence. These issues hinder the ability to assess the validity and significance of the results, a critical requirement for acceptance at a top-tier venue.\n\n2. **Limited Novelty and Incremental Contribution:**  \n   Reviewers 2, 3, and 6 question the technical novelty, noting similarities to prior work (e.g., Hazan & Singh 2021). While the non-convex Frank-Wolfe adaptation and gradient boosting integration are acknowledged as interesting, the paper does not sufficiently differentiate itself from existing methods for boosted contextual bandits or address the unique challenges of RL (e.g., exploration, non-convexity) in a way that clearly advances the state of the art. Reviewer 2 explicitly states that the contributions appear \"incremental.\"\n\n3. **Lack of Empirical Validation:**  \n   Despite the theoretical focus, the absence of experiments (highlighted by Reviewers 2, 5, and 6) weakens the case for practical relevance. Prior work (e.g., Hazan & Singh 2021) includes empirical results, and the omission here leaves open questions about the algorithm’s feasibility and performance in real-world scenarios. Even simple experiments (e.g., grid-world MDPs) could have provided critical intuition.\n\n4. **Assumptions and Practicality Concerns:**  \n   Reviewers raise concerns about the practicality of key assumptions, such as access to a weak learner that optimizes linear functions over policies (Reviewer 3) and the distribution mismatch parameter $C_\\infty$ (Reviewer 4). The paper does not clarify how these assumptions might be satisfied in practice or their implications for real-world applicability.\n\n5. **Inconsistent and Undefined Notation:**  \n   The frequent misuse of notation (e.g., undefined $P$, $M$, $\\eta_{2,t}$ in Algorithm 1) and inconsistent definitions (e.g., $d_0$ vs. $\\mu$) create confusion and undermine the technical rigor. Reviewer 6 notes that even the policy class $\\Pi$ is ambiguously defined, making it difficult to interpret the main theorem.\n\n**Conclusion:**  \nWhile the paper tackles an important problem and introduces a theoretically interesting approach, the combination of clarity issues, incremental novelty, and lack of empirical validation makes it unsuitable for acceptance at a top-tier conference in its current form. Addressing these concerns would require a substantial revision, including a rewrite for clarity, explicit comparisons to prior work, and empirical validation."
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper is about analyzing Feedback Alignment (FA) in deep linear networks, providing convergence guarantees and studying implicit regularization. The reviewers have mixed opinions. \n\nReviewer 1 thinks the one-dimensional dynamics might oversimplify the problem and questions the significance, especially since the convergence for deeper networks requires special initialization. They also wonder if the implicit anti-regularization is observed with common initializations. The technical and empirical novelty scores are low here.\n\nReviewer 2 points out that the convergence results have strong assumptions, like co-diagonalizable matrices and deterministic layer initializations, which might not align with practice. They also note the lack of exploration into weight alignment, a key aspect of FA. However, they find the anti-regularization phenomenon interesting.\n\nReviewer 3 is more positive, highlighting the interesting implicit bias findings and clear structure. They recommend acceptance despite some minor questions about the discrete updates and presentation. Their technical novelty score is high.\n\nReviewer 4 acknowledges the clear writing and interesting results but is concerned about the restrictive spectral initialization and lack of connection to practical settings. They suggest more experiments with non-spectral initializations and better visualization, leading to a weak reject.\n\nThe main strengths are the theoretical analysis in a simplified setting, identification of implicit regularization/anti-regularization, and clear presentation. Weaknesses include the restrictive assumptions (diagonalization, spectral initialization) that limit practical relevance, insufficient empirical validation under common initializations, and questions about the broader significance beyond the specific setup.\n\nTop-tier conferences usually look for both theoretical and practical contributions. While the theoretical results are solid and novel in the context of FA, the applicability to real-world scenarios is unclear. The reviewers' concerns about the assumptions and lack of empirical evidence in more general settings are significant. However, Reviewer 3's strong support and the identification of a novel phenomenon (implicit anti-regularization) add weight. \n\nThe paper contributes to understanding FA's dynamics, which is an active area. The results might inspire further research, even if the current analysis is under restrictive conditions. However, the majority of reviewers (1, 2, 4) have reservations about the significance and generality, which are critical for a top-tier conference. The technical novelty is acknowledged but may not be sufficient to overcome the limitations highlighted.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper provides a rigorous theoretical analysis of Feedback Alignment (FA) in deep linear networks, offering convergence guarantees and insights into implicit regularization/anti-regularization. While the technical execution is solid and the identification of anti-regularization is novel, the work faces significant limitations that reduce its suitability for a top-tier conference:  \n1. **Overly Restrictive Assumptions:** The analysis relies heavily on co-diagonalizable matrices and spectral initialization, which are not representative of practical neural network training (e.g., random, independent layer initialization). This limits the applicability of the results to real-world scenarios.  \n2. **Limited Empirical Validation:** The numerical experiments are confined to simplified settings, and there is insufficient evidence that the observed phenomena (e.g., anti-regularization) generalize to non-spectral initializations or nonlinear networks.  \n3. **Marginal Practical Significance:** While the theoretical contributions advance understanding of FA dynamics, the gap between the idealized setting and practical implementations weakens the broader impact.  \n4. **Mixed Reviewer Sentiment:** Three reviewers raised concerns about significance and generality, with two explicitly recommending weak rejection. Reviewer 3’s support highlights the paper’s merits but does not outweigh the critical limitations noted by others.  \n\nFor a top-tier venue, the paper’s narrow scope and lack of connection to practical training paradigms make it more suitable for a specialized journal or workshop."
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper's main contribution is showing that all global optima for two-layer ReLU networks can be found via a convex program, building on prior work by Pilanci and Ergen. The reviewers have a mix of opinions, but mostly positive.\n\nReviewer 1 points out some technical issues like typos and equation errors but acknowledges the novelty and significance. They gave a 3 for correctness and technical novelty. The weaknesses are about the single-layer focus and some technical details needing fixes. But the author responses addressed the minor issues, so the reviewer kept their score.\n\nReviewer 2 is very positive, calling it the best submission they've reviewed. They mention missing references but still recommend acceptance. They gave high scores except for technical and empirical novelty being 3, probably because of prior work. The main weakness is the lack of experiments, but since it's a theoretical paper, maybe that's acceptable.\n\nReviewer 3 also notes the connection to prior work and the lack of experiments. They mention missing related works and the need for more comparisons. They also question the extension to deeper networks. However, after the rebuttal, they kept their score, which was a 3 in correctness and novelty. The typos and clarity issues were addressed.\n\nReviewer 4 is very supportive, recommending a clear accept. They highlight the complete characterization of global minima and the algorithm for testing optimality. The weaknesses are about the two-layer limitation but see it as a solid contribution.\n\nThe common themes are: significant theoretical contributions extending prior work, some technical issues addressed in rebuttal, and questions about generalization to deeper networks or other activations. However, the paper's focus is on two-layer ReLU, which is a valid and important case. The lack of experiments isn't a major issue here since it's a theoretical analysis. The technical correctness seems solid after addressing the minor issues. The novelty is there, even if building on previous work. \n\nConsidering all this, the strengths in theoretical contributions and the positive aspects from most reviewers, especially Reviewer 4's strong endorsement, outweigh the limitations. The paper provides a rigorous analysis and new insights into the convexity of neural network landscapes, which is valuable for the community. The minor issues were addressed, so acceptance seems appropriate.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a significant theoretical advancement by fully characterizing the global optima of two-layer ReLU networks through a convex optimization framework, extending prior work (e.g., Pilanci & Ergen) with novel insights. Key strengths include:  \n1. **Completeness of Analysis:** It provides an exact mapping between optimal neural networks and convex program solutions, addressing a gap in prior work that only constructed individual solutions.  \n2. **Algorithmic Contributions:** A polynomial-time algorithm for verifying global optimality and guarantees for gradient-based methods (e.g., no spurious valleys with sufficient width) are impactful.  \n3. **Theoretical Depth:** The analysis of minimal/nearly minimal networks and invariant transformations enriches understanding of the loss landscape.  \n\nWhile reviewers noted limitations (e.g., restriction to two-layer networks, minor technical errors, and missing references), these are outweighed by the paper’s contributions:  \n- **Technical Correctness:** Most claims are well-supported, and authors addressed minor issues in rebuttal.  \n- **Novelty:** Though building on prior convex formulations, the characterization of all global optima and invariant transformations is a distinct advance.  \n- **Impact:** The framework offers tools for analyzing neural network training dynamics, with potential to inspire extensions to deeper architectures.  \n\nReviewers 2 and 4 strongly endorse acceptance, emphasizing the paper’s clarity and theoretical value. While the lack of empirical validation is noted, the paper’s theoretical focus justifies this omission. Overall, the work meets the standards of a top-tier conference."
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about improving the sampling efficiency of diffusion models. The main contributions are the introduction of Generalized Gaussian Diffusion Models (GGDM) and Differentiable Diffusion Sampler Search (DDSS). The authors claim that their method can generate high-quality samples with fewer steps compared to existing methods like DDPM and DDIM, which is a big deal because diffusion models are notoriously slow at sampling.\n\nLooking at the reviews, there's a mix of positive and negative points. Reviewer 1 initially had concerns about the theoretical grounding but was satisfied after the authors added Theorem 1, which links the ELBO of their model to DDPMs. They raised the score from 3 to 6. Reviewer 2 was more critical, pointing out issues with motivation and theoretical justification but also increased their score post-rebuttal. Reviewer 3 was positive, impressed by the empirical results despite some concerns about practicality. Reviewer 4 and 5 had mixed feelings, with concerns about theoretical guarantees and dataset diversity but acknowledged the empirical strengths.\n\nThe key strengths highlighted by reviewers include the practical improvement in sample quality with fewer steps, compatibility with pre-trained models without retraining, and the use of gradient rematerialization to handle memory issues. The weaknesses revolve around the lack of theoretical depth, clarity in the paper's exposition, and limited experimentation on larger datasets.\n\nThe authors addressed some concerns in the rebuttal, particularly around Theorem 1, which helped clarify the connection to DDPMs. However, some issues remain, like the theoretical justification for using perceptual loss instead of ELBO and the need for more diverse datasets. The empirical results are strong, especially the FID scores on LSUN church, which are significantly better than baselines. But the reviewers note that the improvements might be due to optimizing for FID directly, which could be seen as overfitting the metric.\n\nThe technical novelty is somewhat debated. Some reviewers see GGDM as a natural extension of existing samplers like DDIM, while others appreciate the optimization approach of DDSS. The empirical results are compelling enough to suggest that this is a meaningful contribution, even if the theoretical underpinnings aren't fully fleshed out.\n\nIn top-tier conferences, empirical success can sometimes outweigh theoretical limitations, especially if the method offers a clear practical benefit. The paper's ability to work with pre-trained models and reduce inference steps significantly is a strong point. However, the concerns about clarity and theoretical grounding are valid and could affect the paper's acceptance. \n\nBalancing all factors, the paper presents a solid contribution with practical improvements but has some shortcomings in explanation and theory. Given that the rebuttal addressed some concerns and the empirical results are strong, it might be borderline but acceptable. However, the mixed reviews and remaining issues might lead to rejection in a highly competitive setting. But considering the post-rebuttal score increases and the significance of the problem addressed, leaning towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper introduces a novel method (DDSS) and sampler family (GGDM) that significantly improve the sampling efficiency of diffusion models, achieving state-of-the-art FID scores with fewer steps. While reviewers initially raised concerns about theoretical grounding and clarity, the authors addressed key issues in the rebuttal (e.g., Theorem 1 linking GGDM to DDPMs’ ELBO). Empirical results are compelling, particularly the FID improvements on LSUN (11.6 with 10 steps vs. 51.1 for DDPM), demonstrating practical value. Although theoretical motivations remain underdeveloped and dataset diversity is limited, the method’s compatibility with pre-trained models and its empirical advances justify acceptance at a top-tier conference. The post-rebuttal score increases from reviewers (e.g., Reviewer 1 and 2 raising scores to 6) further support this decision."
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about target propagation (TP) using regularized inversion. The main idea is to compute targets instead of gradients and propagate them backward. They propose a method that's supposed to be easier to implement with differentiable programming. The reviewers have mixed opinions. \n\nReviewer 1 is critical, pointing out that the proposed TP loses some key features that make TP attractive, like avoiding weight transport, which is important for neuromorphic hardware. They also mention the computational cost is higher than backpropagation (BP), and the theoretical guarantees are weak. The experiments aren't convincing enough. Reviewer 2 is more positive, highlighting the autograd compatibility and some theoretical bounds but notes the empirical results are a bit shaky. Reviewer 3 and 4 both have concerns about the experiments not being strong enough, especially on real datasets, and the lack of comparison with other RNN optimization methods. Reviewer 4 also wants more theoretical exploration, like the connection to Gauss-Newton methods.\n\nThe authors' response would be crucial here, but since we don't have that, I have to go by the reviews. The main issues seem to be:\n\n1. **Technical Novelty**: Reviewers 1, 2, 3, and 4 all rate technical novelty as low (2 or 3). The method uses regularized inversion, but if it's similar to prior TP work (like Lee et al. 2015), the novelty might be incremental. The reliance on weight transport is a step back from what makes TP interesting in some contexts.\n\n2. **Empirical Evaluation**: The experiments are on synthetic and image datasets but with low accuracy (like CIFAR10 at <20%). This is a red flag. If the method can't show competitive performance even on standard benchmarks, it's hard to argue for its effectiveness. Also, lack of comparisons with other RNN optimization techniques (e.g., Hessian-Free) is a weakness.\n\n3. **Theoretical Contributions**: The theoretical analysis is present but not strong enough. The bound in Lemma 3.1 is loose, and the connection to Gauss-Newton isn't fully fleshed out. Without solid theoretical guarantees, the method's advantages over BP aren't clear.\n\n4. **Practical Impact**: The computational cost is higher than BP, which is a downside unless there's a significant benefit. The reviewers don't see enough evidence that TP here is better in scenarios where BP struggles, like vanishing gradients. Also, the method's applicability in distributed settings is unclear due to weight transport.\n\nPutting this together, the paper's contributions are incremental and the empirical results aren't convincing. For a top-tier conference, the bar is high. Even though the idea is interesting, the execution in terms of theory and experiments seems lacking. The reviewers' concerns about novelty, empirical strength, and theoretical depth suggest that the paper isn't ready for acceptance. The weaknesses pointed out by multiple reviewers, especially the lack of compelling experiments and unclear advantages over existing methods, make me lean towards rejection.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper proposes a target propagation (TP) variant using regularized inversion, aiming to provide a practical alternative to backpropagation (BP) for training recurrent neural networks (RNNs). While the idea of simplifying TP via analytical layer inversion is intriguing, the reviews highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n1. **Limited Technical Novelty**:  \n   Reviewers unanimously noted that the technical contributions are incremental. The proposed method relies on weight transport—a departure from TP’s original appeal in avoiding biologically implausible weight symmetry. This limits its applicability to neuromorphic hardware and distributed systems, key motivations for TP research. The theoretical analysis (e.g., Lemma 3.1) is deemed insufficient to establish clear advantages over BP, and connections to Gauss-Newton methods remain underdeveloped.\n\n2. **Weak Empirical Validation**:  \n   Experiments on image datasets (e.g., CIFAR-10) show poor performance (<20% accuracy), raising doubts about the method’s scalability and practical utility. The lack of comparisons with modern RNN optimization techniques (e.g., Hessian-Free optimization) and failure to demonstrate robustness across tasks weaken the empirical case. Reviewer 3 explicitly questions why the method was not tested on standard architectures like CNNs if it is a “generic” BP alternative.\n\n3. **Theoretical Gaps**:  \n   The paper does not provide convergence guarantees or a rigorous analysis of how TP’s descent directions differ from BP’s gradients. Claims about mitigating vanishing/exploding gradients are contradicted by the reliance on Jacobian inverses, which inherit similar stability issues. The relationship to Gauss-Newton methods is ambiguously presented, with Reviewer 4 noting that regularization is consistent with Gauss-Newton but the authors dismiss this connection without justification.\n\n4. **Methodological Concerns**:  \n   Reviewer 1 raises critical questions about the handling of non-injective activations (e.g., ReLU) and the 0th-order approximation claim, which the paper does not adequately address. These issues cast doubt on the algorithm’s correctness in common scenarios.\n\n**Conclusion**:  \nWhile the paper explores a theoretically interesting direction, it falls short of the standards for a top-tier conference. The incremental technical contributions, unconvincing experiments, and unresolved theoretical questions outweigh the potential benefits of the proposed method. Significant revisions to the theoretical grounding, empirical evaluation, and scope of comparison are needed to justify publication."
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper's title is about recognizing and overcoming the greedy nature of learning in multi-modal DNNs. The abstract states that multi-modal models tend to rely on one modality, which hurts generalization. They propose a method called balanced multi-modal learning and test it on three datasets.\n\nLooking at the reviewers' comments, there's a mix of positive and negative feedback. Reviewer 4 and 5 have some positive points, noting the interesting topic and structured approach. However, most reviewers have significant concerns. The main issues seem to be around the experimental validation and support for the claims. \n\nReviewer 1 mentions that the authors gave rough answers without detailed results. Reviewer 2 points out inconsistencies in Table 1 and insufficient validation of the main claims. Reviewer 3 suggests the experiments are limited to visual datasets and lack comparison with methods on natural modalities like audio-visual. Reviewer 4 criticizes the weak empirical support, especially on datasets other than Colored MNIST. Reviewer 5 raises technical concerns about the validity of the conditional learning speed definition. Reviewer 6 and 7 note the lack of theoretical analysis, insufficient comparison with existing methods, and missing ablation studies on parameters like alpha.\n\nThe authors' responses don't seem to have fully addressed these concerns. For example, Reviewer 2 mentions changes in Table 1 without explanation, which could indicate errors or lack of clarity. The experiments on Colored MNIST might be too synthetic, and the other datasets (ModelNet40, NVGesture) don't show significant improvements. The proposed method isn't compared against state-of-the-art approaches like RUBi or others mentioned by Reviewer 6 and 7, which is a big issue for a top-tier conference. \n\nTechnical novelty is questioned by some reviewers. While the idea of balancing modality learning speeds is interesting, the connection between the proxy metric (conditional learning speed) and the actual utilization rate isn't convincingly established. Reviewer 5's concern about the validity of the metric's definition could undermine the method's foundation. \n\nMoreover, the lack of theoretical analysis and ablation studies makes it hard to assess the method's robustness. Parameters like alpha are crucial but not explored, which is a common expectation in top-tier papers. The reliance on visual-only datasets also limits the generalizability of the findings, as natural multi-modal datasets (e.g., audio-visual) are more complex and could behave differently.\n\nIn summary, while the paper addresses an important problem and has some empirical results, the concerns about insufficient validation, lack of comparisons with SOTA, technical weaknesses in metric definitions, and missing theoretical grounding are significant. These issues are critical for acceptance at a top-tier conference where rigorous methodology and strong empirical evidence are required.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem in multi-modal learning—imbalanced modality utilization—and proposes a novel training algorithm. However, critical weaknesses highlighted by reviewers undermine its suitability for a top-tier conference:  \n1. **Insufficient Empirical Validation**: The main claim that greedy learning harms generalization is weakly supported. Results on synthetic datasets (e.g., Colored MNIST) dominate the validation, while improvements on real-world datasets (ModelNet40, NVGesture) are marginal or inconsistent. Key metrics like conditional utilization rate lack rigorous statistical significance testing.  \n2. **Lack of Theoretical Foundation**: The connection between conditional learning speed (proxy) and utilization rate is asserted but not rigorously proven. Reviewer 5 questions the validity of the proxy metric’s definition, which weakens the method’s theoretical grounding.  \n3. **Incomplete Comparisons**: The proposed method is not benchmarked against state-of-the-art baselines (e.g., RUBi, modality regularization methods). This omission raises doubts about its novelty and effectiveness relative to existing solutions.  \n4. **Technical Ambiguities**: Reviewers note unclear implementation details (e.g., re-balancing steps, parameter settings like α) and inconsistencies in experimental results (e.g., Table 1 revisions). These issues hinder reproducibility and trust in the findings.  \n5. **Limited Dataset Diversity**: Experiments focus on visual modalities, neglecting natural multi-modal settings (e.g., audio-visual data), which limits the generality of the conclusions.  \n\nWhile the topic is timely and the approach has potential, the paper does not meet the high bar for technical rigor, empirical thoroughness, and theoretical coherence expected at a top-tier conference. Addressing these issues would require substantial revisions and additional experimentation."
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is titled \"A Frequency Perspective of Adversarial Robustness\" and focuses on analyzing adversarial examples through the lens of frequency analysis. The abstract claims that adversarial examples are dataset-dependent rather than strictly high or low-frequency, challenging a common misconception. The authors also propose a frequency-based explanation for the accuracy-robustness trade-off in adversarial training.\n\nNow, looking at the reviewers' comments, there's a mix of opinions. Reviewers 4 and 6 recommend acceptance, highlighting the paper's interesting experiments and insights, especially the dataset dependency finding and the practical implications for adversarial training. However, they also note some weaknesses like presentation issues and the need for more comprehensive experiments.\n\nReviewers 1, 2, 3, and 5 are more critical. Reviewer 1 points out that the authors didn't adequately address concerns about universal adversarial perturbations (UAPs) and their frequency characteristics. Reviewer 2 questions the technical novelty, especially regarding the contributions of the proposed methods compared to existing work like TRADES. They also feel the authors didn't clearly highlight their unique contributions. Reviewer 3 raises several issues: the motivation might be overstated since prior work already questioned the high-frequency assumption, the dataset dependency isn't novel, limited datasets tested, and methodological concerns like using only PGD attacks. Reviewer 5 is confused about the contributions and finds the claims insufficiently supported.\n\nThe main strengths of the paper seem to be the empirical analysis showing dataset dependency and the frequency-based approach to adversarial training. However, the weaknesses revolve around novelty, methodology, and clarity of contributions. The paper's claim to challenge a \"common misconception\" is contested because other works have already addressed this. The technical contributions, like the adversarial training method, are critiqued for not demonstrating clear improvements over existing methods. Additionally, the limited scope of datasets and attacks (only PGD) might undermine the generalizability of the findings.\n\nThe rebuttal process didn't fully alleviate the concerns. For instance, Reviewer 1 still feels the UAP aspect isn't addressed, and Reviewer 2 remains unconvinced about the uniqueness of the contributions. The overlap with concurrent work (Bernhard et al. 2021) is a significant concern, as it could diminish the paper's novelty.\n\nIn a top-tier conference, novelty, technical soundness, and clear contributions are crucial. While the paper provides some interesting empirical insights, the concerns about prior work overlap, insufficient methodological rigor (e.g., limited datasets and attacks), and unclear differentiation from existing techniques are substantial. The mixed reviews, with some recommending acceptance and others rejection, indicate that the paper is on the borderline. However, given the high standards of top-tier venues, the unresolved issues regarding novelty and contribution clarity might tip the balance towards rejection. The authors need to address these points more thoroughly, possibly through additional experiments and clearer articulation of their unique contributions, which might be better suited for a revision and resubmission rather than acceptance in the current state.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a frequency-based analysis of adversarial examples, challenging the notion that they are strictly high-frequency phenomena and proposing dataset dependency. While the empirical analysis and experiments on CIFAR-10/ImageNet are thorough, the reviewers raise critical concerns that undermine its suitability for a top-tier conference:  \n1. **Lack of Novelty:** Multiple reviewers note that the core claim—adversarial examples are dataset-dependent—is not novel, as prior work (e.g., Bernhard et al. 2021, Tsuzuku & Sato 2019) already questioned the high-frequency assumption. The overlap with concurrent work diminishes the paper’s originality.  \n2. **Methodological Gaps:** The analysis is limited to PGD attacks and lacks validation on diverse datasets (e.g., MNIST, CIFAR-100) and architectures (e.g., ViTs). Reviewer 3 highlights that the dataset dependency conclusion is under-supported without broader experimentation.  \n3. **Unclear Contributions:** Reviewers 2 and 5 criticize the paper for failing to articulate its unique technical contributions. The proposed frequency-based adversarial training method lacks empirical superiority over existing approaches (e.g., TRADES), and its practical utility is unclear.  \n4. **Rebuttal Shortcomings:** Key concerns (e.g., UAPs, comparison to prior work) were inadequately addressed in the rebuttal. Reviewer 1’s downgrade reflects unresolved issues.  \n\nWhile the frequency perspective and experimental insights are valuable, the paper does not meet the high bar for novelty, technical rigor, or clarity required by top-tier venues. Significant revisions—expanding experiments, clarifying contributions, and addressing prior work—are needed."
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about understanding overparameterization in recursive value estimation methods like TD learning, FVI, and RM. The main contributions seem to be theoretical analysis showing that these methods converge to different fixed points even when overparameterized, a unified interpretation via optimization constraints, and some new regularizers for deep models.\n\nLooking at the reviews, there's a mix of positive and negative feedback. Reviewer 7 was excited about the topic but pointed out issues with scholarship, like not properly crediting prior work for some theorems. They also mentioned that the connection to deep RL isn't very strong, which is a concern because the paper's motivation is about deep models. Reviewer 9 and 10 raised correctness issues, like errors in equations and proofs, which could undermine the technical validity. However, some reviewers increased their scores after the rebuttal, indicating the authors addressed some concerns.\n\nThe technical novelty is debated. Some reviewers think the results are incremental, especially since some theorems (like Theorem 4) are seen as trivial or known in optimization literature. The empirical evaluation is considered weak, with simple environments and unclear impact on deep RL. However, the proposed regularizers do show some promise in the experiments.\n\nThe main strengths are the theoretical insights into overparameterized RL, which is timely given the use of deep networks. But weaknesses include potential lack of proper attribution, technical errors, and limited empirical validation. The authors need to address the scholarship issues and ensure correctness. If the errors in proofs are fixable and the prior work is properly cited, the paper's contributions might be sufficient for a top-tier conference. However, given the mixed reviews and the remaining concerns about technical soundness and relevance to deep RL, it's a borderline case. The rebuttal helped some reviewers, but others still have reservations. Considering the theoretical contribution's potential significance despite the flaws, leaning towards acceptance with the expectation that the authors will address the issues in the final version.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a timely theoretical analysis of overparameterized linear representations in recursive value estimation, addressing a gap in understanding deep RL models. While reviewers raised valid concerns about technical correctness, scholarship, and empirical scope, the authors addressed several issues in rebuttals, leading some reviewers to increase scores. Key strengths include:\n\n1. **Theoretical Contributions:** The analysis of fixed-point differences between TD, FVI, and RM in overparameterized regimes offers novel insights, even if some results (e.g., Theorem 4) are rooted in optimization literature. The unification via norm minimization constraints and generalization bounds (Theorem 5) are valuable additions.\n\n2. **Algorithmic Impact:** The proposed regularizers, though tested in simple environments, demonstrate practical utility and align with theoretical findings, suggesting potential for broader applicability.\n\n3. **Relevance to Deep RL:** While the connection to deep models is not fully resolved, the work provides foundational insights into overparameterization, a critical aspect of modern RL systems.\n\n**Weaknesses and Mitigations:**  \n- **Scholarship:** Concerns about insufficient credit to prior work (e.g., Theorem 4’s relation to least-norm solutions) require minor revisions but do not invalidate the core contributions.  \n- **Technical Errors:** Reviewer 9’s equation corrections and proof issues need addressing but appear fixable.  \n- **Empirical Scope:** Experiments are limited but sufficient for a theory-focused paper; deeper empirical validation is left for future work.  \n\nGiven the paper’s theoretical novelty, alignment with modern RL challenges, and rebuttal improvements, it meets the bar for acceptance in a top-tier conference, contingent on addressing remaining technical and scholarly concerns in the final version."
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about understanding the implicit biases in neural networks, specifically looking at how architecture and gradient descent (GD) contribute to generalization. The authors present theoretical tools to analyze the average test error of NNGP posteriors and argue that GD's ability to select functions with larger margins explains the gap between their bounds and empirical results.\n\nThe reviews are mixed. Some reviewers acknowledge the theoretical contributions, especially the new bounds and estimator for NNGP, but many have concerns. The main issues seem to be around the experimental validation and the claims about GD's implicit bias related to margin maximization. \n\nReviewer 1 and others point out that the experiments in Section 5 are not convincing. The bounds are loose, and the comparison between GD and NNGP might be specific to the optimizer used (Nero) rather than GD in general. There's also criticism about the lack of comparisons with prior work, especially Valle-Pérez et al. (2019), and the applicability to different architectures like CNNs. \n\nReviewers also question the margin analysis. They argue that existing results on margin maximization are under specific conditions (like exponential loss), but the paper uses squared loss, making the claims less rigorous. The connection between GD and margin improvement isn't clearly established, and some feel it's more about overfitting than implicit bias.\n\nOn the theoretical side, the new PAC-Bayes bound is seen as incremental by some, building on previous work without significant novelty. The estimator's practical utility is questioned, especially since Monte Carlo sampling might be just as effective. \n\nHowever, there are strengths. The paper is well-written, and the theoretical analysis of NNGP's implicit bias is considered a solid contribution. The idea that both architecture and GD contribute to generalization is interesting and relevant. Some reviewers upgraded their scores after the rebuttal but remained uncertain.\n\nThe key issues are whether the technical contributions are novel enough for a top-tier conference and if the empirical evidence supports the claims. The authors addressed some concerns in the rebuttal, but reviewers still found the experiments lacking in breadth (e.g., only MLPs, limited datasets) and rigor (e.g., optimizer choice, margin analysis). The margin section's theoretical foundation is shaky, and the GD vs. NNGP comparison isn't sufficiently validated across settings.\n\nGiven that top-tier conferences require both theoretical novelty and strong empirical support, the paper's weaknesses in experiments and some theoretical aspects might outweigh its strengths. While the theoretical tools are a step forward, the incremental nature and unresolved issues in the empirical evaluation make it a borderline case. The majority of reviewers lean towards weak reject or borderline accept, but the concerns about the margin analysis and experimental setup are significant. \n\nIn conclusion, despite some merits, the paper doesn't fully meet the high bar for acceptance due to unresolved methodological concerns and insufficient empirical validation.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents a nuanced investigation into the implicit biases of neural network architectures and gradient descent (GD), offering theoretical tools to analyze NNGP posteriors and proposing that GD's margin maximization bridges the gap between theoretical bounds and empirical performance. While the theoretical contributions (e.g., analytical bounds and consistent estimators for NNGP error) are non-trivial, several critical weaknesses undermine its suitability for a top-tier venue:  \n\n1. **Weak Empirical Validation**:  \n   - Experiments in Section 5, which aim to demonstrate GD’s margin-seeking behavior, are criticized for relying on the non-standard \"Nero\" optimizer, limited datasets (e.g., synthetic data), and lack of ablation studies (e.g., comparisons with SGD, NTK, or vanilla GD). The results are seen as overly specific and insufficient to generalize claims about GD’s implicit bias.  \n   - The bounds derived for NNGP are empirically loose, and comparisons to prior work (e.g., Valle-Pérez et al.) are absent, raising doubts about their practical significance.  \n\n2. **Unconvincing Margin Analysis**:  \n   - The link between GD and margin maximization is not rigorously established. Reviewers note that existing margin theory applies to specific losses (e.g., exponential loss), whereas the paper uses squared loss without justification. Claims about GD’s implicit bias are conflated with overfitting, and the analysis lacks formal connections to NTK or prior margin-maximization literature.  \n\n3. **Limited Scope and Incremental Contributions**:  \n   - The theoretical tools focus on MLPs, with no extension to architectures like CNNs, despite their practical relevance. The PAC-Bayes bound, while novel, is viewed as incremental over prior work (Valle-Pérez et al.), and the estimator’s advantage over Monte Carlo sampling is unclear.  \n\n4. **Reviewer Consensus**:  \n   - While some reviewers upgraded scores post-rebuttal, concerns about premature claims, insufficient experiments, and theoretical gaps persisted. The majority of reviewers rated technical/empirical novelty as marginal or incremental, aligning with a \"weak reject\" outcome.  \n\n**Conclusion**:  \nThough the paper addresses an important question and introduces useful technical tools, the lack of robust empirical validation, unresolved theoretical ambiguities, and incremental contributions relative to prior work render it unsuitable for a top-tier conference in its current form."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper presents AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The main contributions seem to be the implementation of existing MOBO algorithms in a modular framework, a novel Believer-Penalizer (BP) strategy for asynchronous batch experiments, a GUI for non-experts, and real-world hardware integration.\n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 1 initially had concerns about performance and novelty but raised the score to 5. Reviewer 2 and 4 point out that the paper is more of a software manual with limited methodological novelty. They mention that the BP strategy isn't sufficiently justified theoretically and that the empirical results are overstated. Reviewer 3 also questions the technical contribution, noting the lack of theoretical proof for BP. Reviewer 5 is more positive, highlighting the problem's motivation and the platform's usability but still notes minor issues.\n\nThe key issues here are novelty and empirical validation. The BP method is a combination of existing techniques (KB and LP) with a threshold, but the reviewers argue that the combination isn't novel enough and lacks theoretical backing. The experiments show mixed results, with AutoOED not consistently outperforming other methods. Also, the scalability to more than 3 objectives is limited, which the authors acknowledge but might limit the platform's applicability.\n\nThe software aspect is a strength, but the reviewers compare it to existing BO packages, suggesting it's \"yet another\" tool without standout features except for the GUI and multi-objective focus. However, the GUI and real-world application are practical contributions, especially for non-experts. But in a top-tier conference, technical novelty is crucial. The BP strategy's incremental improvement might not suffice, especially with the empirical results not strongly supporting its superiority.\n\nAnother point is the self-citation issue mentioned by Reviewer 2. Not citing their own prior work on arXiv could be a problem, indicating possible oversight or redundancy. The paper's claims about being a generic platform are also questioned since it's limited to 2-3 objectives, which contradicts the initial statement of handling m>=2.\n\nConsidering the conference standards, even though the tool is useful, the lack of significant methodological innovation and the mixed empirical results might not meet the high bar. The reviewers' scores lean towards borderline or rejection, with most technical and empirical novelty scores being low. The authors' responses might have addressed some concerns, but the core issues of novelty and empirical support remain.\n\nIn conclusion, while the platform has practical value, the paper's contributions are more engineering-focused rather than advancing the scientific state-of-the-art sufficiently for a top-tier conference. The BP strategy's novelty and effectiveness aren't convincingly demonstrated, and the empirical evaluation doesn't strongly back the claims. Therefore, the paper might be better suited for a software or application-focused venue rather than a methodology-centric top conference.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper presents AutoOED, a platform for multi-objective Bayesian optimization (MOBO) with a novel asynchronous batch strategy (Believer-Penalizer, BP) and a user-friendly GUI. While the tool has practical utility, the reviewers identified critical shortcomings that undermine its suitability for a top-tier conference:  \n\n1. **Limited Technical Novelty**:  \n   - The core methodological contribution, BP, is critiqued as an incremental combination of existing techniques (Kriging Believer and Local Penalization) without theoretical justification. Reviewers noted the lack of rigorous analysis or proof of BP’s effectiveness beyond empirical results, which were mixed (e.g., BP underperforms baselines in some benchmarks).  \n   - The platform primarily integrates established MOBO algorithms, making it a \"modular framework\" rather than a significant methodological advance.  \n\n2. **Overstated Empirical Claims**:  \n   - Results in Figures 4–5 show inconsistent performance: BP is not consistently superior to baselines, and AutoOED underperforms existing tools on half the benchmarks. Claims of \"state-of-the-art performance\" and robustness are unsupported by the data.  \n   - The real-world experiment lacks depth, comparing only against random search and omitting comparisons with other MOBO tools.  \n\n3. **Scope and Presentation Issues**:  \n   - The platform’s limitation to 2–3 objectives contradicts the paper’s initial claim of handling \"m≥2 objectives,\" misleading readers about its generality.  \n   - The omission of a key prior arXiv paper by the authors (Tian et al., 2021) raises concerns about redundancy or oversight.  \n\n4. **Positioning as a Scientific Contribution**:  \n   - Reviewers emphasized that the paper reads more like a software manual or engineering report than a research contribution. While the GUI and modular design are valuable, they do not meet the bar for technical novelty expected at a top conference.  \n\n**Strengths**:  \n- The GUI and real-world integration are praiseworthy for democratizing MOBO.  \n- The platform’s modularity and open-source nature could benefit the community.  \n\nHowever, these strengths are overshadowed by the lack of foundational innovation and insufficient empirical validation. For a top-tier venue, the paper’s incremental engineering contributions and overstated claims do not justify acceptance. It would be better suited for a software-focused or applied conference.  \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is titled \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation.\" The authors propose using a VAE with specific decoder components for time series generation, emphasizing interpretability, domain knowledge integration, and reduced training times. They compare their method against existing GAN-based approaches and show results on similarity and prediction tasks.\n\nLooking at the reviewers' comments, there's a mix of opinions but leaning towards rejection. Reviewer 1 points out issues with hyperparameters, lack of ablation studies, and insufficient evidence on interpretability. Reviewer 2 questions the technical novelty, suggesting the approach is just a standard VAE applied to time series without significant modifications. Reviewer 3 raises concerns about the literature review, methodology description, and experimental setup, especially the lack of evaluation on interpretability and well-known datasets. Reviewer 4 also notes the absence of discussion on interpretability and limited performance gains.\n\nThe main strengths mentioned by reviewers are the paper's clarity and the interesting idea of incorporating classical time-series components. However, the weaknesses are substantial. The technical novelty is doubted, with reviewers pointing out that the architecture doesn't introduce enough innovation beyond existing VAEs. The empirical evaluation is criticized for lacking ablation studies, not using established datasets, and not properly addressing the interpretability claims. Additionally, there are methodological concerns, such as the use of a single latent vector for the entire sequence and the absence of comparison with autoregressive models.\n\nThe authors' response to these critiques isn't provided here, but based on the reviewers' summaries, it seems the paper hasn't adequately addressed these issues. For a top-tier conference, the paper needs to demonstrate clear technical novelty, thorough experimentation, and validation of all claimed contributions. The lack of evidence supporting interpretability, insufficient ablation studies, and questions about the architecture's novelty are significant drawbacks. While the idea has potential, the current execution doesn't meet the high standards required for acceptance. Therefore, the paper should be rejected in its present form.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper proposes TimeVAE, a VAE-based architecture for multivariate time-series generation, emphasizing interpretability, domain knowledge integration, and reduced training time. While the idea of combining VAEs with classical time-series components (e.g., trend, seasonality) is promising, the reviewers identified critical shortcomings that undermine its readiness for publication at a top-tier conference:  \n\n1. **Lack of Technical Novelty:**  \n   - Multiple reviewers (2, 3, 4) argue that the core architecture resembles a standard or conditional VAE applied to time-series data without substantial innovation. The use of convolutional layers instead of recurrent architectures or state-space models is questioned, and the absence of novel loss functions or mechanisms to explicitly model temporal dependencies weakens claims of superiority over GAN-based methods like TimeGAN.  \n\n2. **Insufficient Empirical Validation:**  \n   - **Interpretability Claims Unsupported:** A recurring critique (Reviewers 1, 3, 4) is the lack of experiments or ablation studies demonstrating the interpretability of the proposed decoder components (e.g., trend/seasonality blocks). This undermines a key contribution.  \n   - **Limited Benchmarking:** Experiments focus on synthetic/non-standard datasets, making comparisons to prior work difficult. The omission of autoregressive models (e.g., DeepAR) and state-space VAEs (e.g., GP-VAE) raises concerns about the completeness of the evaluation.  \n   - **Ambiguous Results:** Reviewer 1 highlights counterintuitive results (e.g., worse performance with larger training sets), and Reviewer 4 notes minimal performance gains over baselines except on synthetic data.  \n\n3. **Methodological Weaknesses:**  \n   - The use of a single global latent vector for entire sequences (Reviewer 3) is criticized as limiting compared to per-time-step latent variables in state-space models.  \n   - The explanation of the VAE framework and loss function is deemed nonstandard and incomplete, with unclear justification for weighting terms (Reviewer 2).  \n\n4. **Literature Review and Presentation Issues:**  \n   - The background section mischaracterizes generative models (e.g., conflating autoregressive models as deterministic) and lacks engagement with relevant work in state-space models and particle filtering (Reviewer 3).  \n\nWhile the paper’s clarity and potential for interpretability are strengths, the unresolved technical and empirical gaps prevent it from meeting the high bar of a top-tier conference. Addressing these issues would require a major revision, including rigorous ablation studies, comparisons to broader baselines, and validation of interpretability claims."
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper introduces MILAN, a method to generate natural language descriptions of neurons in deep vision models. The main idea is to use mutual information to find descriptions that align with image regions where neurons activate. The reviewers generally find the work well-executed and impactful, with strengths in technical soundness, clarity, and practical applications. \n\nReviewer 2 highlights the method's ability to capture various feature levels and its generalizability across architectures. They raise valid points about dataset quality and potential scaling with existing datasets. Reviewer 3 appreciates the thorough testing but wants more on the approach's limits. Reviewer 4, an NLP expert, questions the scope (since it's image-focused) and suggests more empirical validation against captioning models. They also mention the need for real-world bias examples instead of contrived tasks.\n\nThe technical novelty is rated as somewhat new by most reviewers, which is a concern for a top-tier conference. However, the empirical results and applications (analysis, auditing, editing) seem strong. The authors addressed inter-annotator agreement by using majority voting, which Reviewer 2's comment might not have covered. The dataset's quality control is a point that needs clarification, but the code and data release help reproducibility.\n\nReviewer 4's point about using zero-shot captioning models as a baseline is important. If the paper didn't compare with such models, that's a gap. However, the method's focus on neuron-level descriptions might not be directly comparable to standard captioning. The applications shown (like improving robustness by removing text-sensitive neurons) are practical and demonstrate the method's utility.\n\nThe reviewers' scores are mostly positive, with correctness rated high. The main weaknesses are around dataset validation, scope, and comparison with existing methods. The authors' response would need to address these, but since this is a review without author responses, I have to assume they might not have fully addressed them yet. However, the strengths in methodology, thorough evaluation, and clear applications likely outweigh the concerns. For a top-tier conference, the paper's contributions in interpretability and practical use cases make it a strong candidate for acceptance despite some limitations.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents MILAN, a novel method for generating natural language descriptions of neuron activations in vision models, with compelling applications in model analysis, auditing, and editing. While reviewers noted minor concerns about dataset validation, scope, and comparisons to existing methods, the work’s strengths—technical rigor, generalizability across architectures, and actionable insights—outweigh these limitations. Key strengths include:  \n1. **Technical Soundness**: The method is well-designed, leveraging mutual information optimization and a human-annotated dataset (MILANNOTATIONS) to produce interpretable, compositional descriptions.  \n2. **Empirical Validation**: Extensive experiments demonstrate MILAN’s superiority over baselines in human alignment and its applicability to diverse architectures (e.g., ResNet, AlexNet) and tasks (classification, anonymization).  \n3. **Practical Impact**: Applications like detecting spurious correlations (e.g., text in images) and improving model robustness highlight the method’s utility for real-world model debugging.  \n4. **Clarity and Reproducibility**: The paper is well-written, with code and data released for reproducibility.  \n\nReviewers’ concerns (e.g., dataset quality control, limited scope to images, and comparisons to captioning models) are valid but do not undermine the core contributions. The work advances interpretability research meaningfully and aligns with top-tier conference standards."
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about using a weighted mean trick to optimize higher-order moments of the loss distribution for robustness. The reviewers have a mix of opinions, but there are some recurring themes. \n\nFirst, the theoretical contributions are under scrutiny. Reviewers 1, 4, 5, and 6 point out issues with the choice of weights (lambda), especially the use of negative values. They argue that allowing both positive and negative lambas might make the approach too flexible, leading to overfitting rather than genuine robustness. Reviewer 5 also questions the convexity claims when weights depend on theta, which isn't properly addressed. The algorithm's correctness is a big concern—Reviewers 2, 3, and 5 note that the gradient descent step only uses part of the gradient, which might mean the algorithm isn't actually minimizing the intended objective. This is a critical flaw because if the method isn't optimizing the right loss, the empirical results could be misleading.\n\nOn the empirical side, while the experiments show comparable performance to existing methods, several reviewers mention that the improvements might just be due to hyperparameter tuning. Reviewer 1 points out that without theoretical guidance on choosing lambdas, the results aren't convincing. Reviewer 4 also questions the practical aspects, like handling unknown expectations and the number of hyperparameters introduced. The lack of convergence guarantees for Algorithm 1 is another red flag.\n\nThe technical novelty is debated. Reviewers 5 and 6 find the theoretical results straightforward or incremental, while Reviewer 7 sees potential but notes the theorems are simple. The connection to prior work like Duchi & Namkoong (2019) and others isn't sufficiently differentiated, which weakens the case for novelty. \n\nWriting and clarity issues are mentioned by Reviewers 4 and 6, with notation inconsistencies and unclear explanations. These could hinder reproducibility and understanding, which are crucial for a top-tier conference.\n\nOn the positive side, Reviewer 7 appreciates the observation's potential and votes to accept. However, the majority of reviewers (especially 4, 5, 6) have significant concerns about theoretical soundness, algorithm correctness, and novelty. The paper's core contribution—the weighted mean trick—is questioned in terms of both its theoretical foundation and practical implementation. \n\nGiven the high standards of a top-tier conference, the unresolved theoretical issues (especially regarding the gradient computation and convexity), the lack of clear novelty beyond existing methods, and the methodological concerns raised by multiple reviewers, the paper doesn't meet the acceptance threshold. The potential highlighted by Reviewer 7 isn't enough to outweigh the critical flaws pointed out by others.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a weighted mean approach to optimize higher-order loss moments for robustness. While the idea is interesting, several critical issues preclude acceptance at a top-tier venue:  \n\n1. **Theoretical Concerns:**  \n   - **Algorithm Correctness:** Multiple reviewers (2, 3, 5) highlight that the proposed algorithm does not compute the full gradient of the weighted loss, raising doubts about whether it minimizes the intended objective. This undermines the method’s theoretical validity.  \n   - **Convexity Claims:** The convexity analysis (Lemma 2) is questioned for ignoring the dependence of weights on parameters (θ), which is central to the method. Reviewer 5 argues that the theoretical results become trivial when this dependence is overlooked.  \n   - **Ambiguity in Weight Selection:** The use of both positive and negative λ values (Reviewers 1, 4, 6) introduces conflicting effects (e.g., amplifying vs. suppressing outliers) without a principled way to choose λ, weakening the theoretical motivation.  \n\n2. **Empirical Limitations:**  \n   - **Hyperparameter Sensitivity:** The improvements may stem from tuning λ rather than the method itself (Reviewer 1). No convergence guarantees for Algorithm 1 are provided (Reviewer 4).  \n   - **Comparisons to Prior Work:** The experiments do not sufficiently differentiate the approach from existing robust methods (e.g., trimmed mean, MoM minimization), limiting empirical novelty (Reviewer 4).  \n\n3. **Novelty and Significance:**  \n   - The theoretical contributions are deemed incremental, with Reviewer 5 noting the derivations are “basic computations” and Reviewer 6 criticizing the lack of technical depth. While Reviewer 7 finds the observation valuable, the majority view is that the paper does not advance the field meaningfully beyond prior work (e.g., Duchi & Namkoong, 2019).  \n\n4. **Clarity and Presentation:**  \n   - Notation inconsistencies, undefined terms (e.g., λ_min in Figure 1), and unclear explanations (Reviewers 4, 6) hinder reproducibility and understanding.  \n\nWhile the core idea has potential, unresolved theoretical flaws, methodological ambiguities, and insufficient empirical differentiation from existing approaches make the paper unsuitable for a top-tier conference in its current form."
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by looking at the paper's contributions. The authors propose two methods: Computation Redistribution (CR) and Sample Redistribution (SR). CR adjusts where computation happens in the model's backbone, neck, and head. SR augments training samples where needed. They use a random search in a designed space to implement SCRFD. The results on WIDER FACE show SOTA trade-off between accuracy and efficiency, with SCRFD-34GF outperforming TinaFace by 4.78% AP on the hard set while being 3x faster.\n\nReviewers have mixed opinions. Reviewer 1, 2, and 3 seem satisfied with the responses, but their comments are brief. Reviewer 4 points out that the search strategies (CR) are straightforward and not very novel. They also mention a lack of comparison with other network search methods, which makes it unclear if the improvements are due to the authors' contributions or just the search. Reviewer 5 notes the effectiveness of CR and SR but questions the broader applicability beyond objects with wide scale ranges. Reviewer 6 praises the results and innovation but asks about comparisons with evolutionary methods and the choice of TinaFace as the base. Reviewer 7 highlights missing details in methodology and insufficient comparisons on cross-dataset evaluations.\n\nThe main strengths are the effective methods (CR and SR) leading to improved performance, especially on small faces, and thorough ablation studies. The weaknesses revolve around novelty and methodology. The search approach is seen as too simple, and there's a lack of comparison with other NAS methods. Some reviewers question the technical novelty, rating it as marginal or somewhat new. However, empirical results are strong, showing significant improvements.\n\nThe key points to consider are: Does the paper present enough technical novelty despite using a straightforward search method? Are the empirical results strong enough to compensate for potential lack of novelty? The paper's practical impact is clear with code availability and efficiency gains, which is important for real-world applications. However, top-tier conferences often prioritize both novelty and strong results. The reviewers' concerns about comparisons with other NAS methods and the simplicity of the search strategy might be critical. Yet, the performance gains are substantial, and the methods are well-validated on standard benchmarks. The mixed technical novelty scores (some 2s and 3s) versus high empirical scores (3s and 4s) suggest that while the approach isn't groundbreaking, the application and results are significant. Given that the paper addresses a real problem (efficient small face detection) with effective solutions and solid experiments, it might still be acceptable. However, the lack of comparison with other NAS approaches and the simplicity of the search could be seen as major flaws. But since the results are strong and the methods are practical, leaning towards acceptance if the community values application impact alongside some novelty.\n</think>\n\nThe paper presents SCRFD, a method combining Computation Redistribution (CR) and Sample Redistribution (SR) to improve efficiency and accuracy in face detection, particularly for small faces. While the empirical results are strong (e.g., 4.78% AP improvement over TinaFace with 3× speedup), reviewers raised critical concerns about novelty and methodology. \n\n**Strengths**:  \n1. **Effective Techniques**: CR and SR demonstrably improve small-face detection, validated by ablation studies and SOTA results on WIDER FACE.  \n2. **Practical Impact**: Code is released, and the method achieves significant speed-accuracy trade-offs, appealing for real-world deployment.  \n3. **Scale-Specific Optimization**: Addresses the challenge of detecting small faces (78.93% <32×32 in WIDER FACE), a critical edge case.  \n\n**Weaknesses**:  \n1. **Limited Novelty**: The search strategies (random search in a constrained space) are deemed straightforward and less innovative compared to advanced NAS methods (e.g., evolutionary algorithms).  \n2. **Incomplete Comparisons**: Lack of direct comparisons with other NAS approaches weakens claims about the uniqueness of the contributions.  \n3. **Methodological Gaps**: Insufficient details on SR implementation and cross-dataset benchmarking (e.g., missing comparisons with BFBox/HAMBox on Pascal/FDDB).  \n\n**Assessment**:  \nWhile the technical novelty is modest (CR/SR build on established concepts like computation allocation and scale augmentation), the empirical contributions are substantial. The paper’s practical advancements in efficiency and accuracy for a high-impact problem (face detection) outweigh its methodological simplicity. However, the absence of comparisons with alternative NAS methods and incomplete cross-dataset evaluations leave room for skepticism.  \n\n**Final Decision**: Accept  \nThe paper’s strong empirical results, practical relevance, and well-validated techniques justify acceptance despite moderate novelty. Addressing reviewer concerns in a rebuttal (e.g., clarifying methodological details, acknowledging NAS baselines) would strengthen the work further."
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question is about improving the sampling efficiency of diffusion models. The main contributions are the Generalized Gaussian Diffusion Models (GGDM) and the Differentiable Diffusion Sampler Search (DDSS). The authors claim that by optimizing the sampler parameters through gradient descent on sample quality metrics like FID, they achieve better results with fewer steps compared to existing methods like DDPM and DDIM.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 1 initially had concerns about the theoretical grounding but raised their score after the authors addressed some points, especially Theorem 1. Reviewer 2 was critical about the motivation and theoretical justification but also moved from a reject to a borderline score. Reviewer 3 was more positive, highlighting the empirical results and practicality despite computational costs. Reviewer 4 pointed out issues with theoretical guarantees and dataset diversity, while Reviewer 5 mentioned clarity issues but acknowledged the empirical strengths.\n\nThe key points to consider are:\n\n1. **Theoretical Foundation**: Some reviewers were concerned about the lack of theoretical guarantees, especially regarding the ELBO and the justification for using perceptual losses. However, Theorem 1 seems to have alleviated some of these concerns by showing equivalence under certain conditions. Still, there's a lingering question about whether optimizing for FID might not align with the original model's objectives.\n\n2. **Empirical Results**: The FID scores presented are significantly better than baselines, especially with very few steps (e.g., 10 steps). This is a strong point, as efficiency in sampling is a major hurdle for diffusion models. However, some reviewers wanted more experiments on larger datasets (like high-resolution LSUN or ImageNet) to demonstrate scalability.\n\n3. **Clarity and Presentation**: Multiple reviewers found the paper's clarity lacking, particularly in explaining the model's structure and the optimization process. The use of complex notation and insufficient explanation of terms like the double subscripts in equations was a common issue. The authors need to improve the exposition to make the method more accessible.\n\n4. **Novelty and Significance**: The idea of differentiating through sample quality to optimize samplers is seen as novel, though some reviewers noted similarities to prior work like DDIM. The non-Markovian approach in GGDM adds flexibility, but its necessity beyond enabling DDSS is questioned. The practical impact of reducing sampling steps is significant, which is a strong point in favor.\n\n5. **Technical Soundness**: The use of gradient rematerialization to handle backpropagation through the sampling process is technically sound, though computationally intensive. The method's compatibility with pre-trained models without retraining is a practical advantage.\n\nBalancing these factors, the paper presents a method that empirically works well, addresses an important problem, and introduces a novel approach. However, the theoretical underpinnings and clarity issues are notable weaknesses. The reviewers' scores post-rebuttal are mostly in the borderline range (around 6), with some concerns remaining but significant improvements acknowledged. Given the empirical strength and the practical importance of faster sampling in diffusion models, the paper's contributions might outweigh its shortcomings for a top-tier conference, especially if the authors commit to addressing clarity and theoretical points in the final version.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper introduces a novel method (DDSS) and sampler family (GGDM) that significantly improve the efficiency of diffusion models by optimizing sampler parameters via gradient descent on perceptual metrics. Despite some theoretical and clarity concerns, the empirical results are compelling, demonstrating state-of-the-art FID scores with very few sampling steps (e.g., 4.82 FID on LSUN church with 20 steps vs. 14.9 for DDIM). Key strengths include:\n\n1. **Empirical Impact**: The method achieves substantial improvements in sample quality with fewer steps, addressing a critical limitation of diffusion models. Results on CIFAR-10 and ImageNet 64×64 are robust, and the approach is compatible with pre-trained models without retraining.\n\n2. **Technical Novelty**: DDSS’s use of gradient rematerialization to backpropagate through the sampling process and optimize non-Markovian samplers (GGDM) is a novel contribution. While GGDM’s design is partly motivated by enabling DDSS, the combination of these ideas is innovative.\n\n3. **Theoretical Progress**: Post-rebuttal updates, including Theorem 1, partially address concerns about ELBO alignment and justify the use of pre-trained models. However, lingering questions about the statistical properties of the optimized samples remain.\n\n4. **Practical Relevance**: Reducing inference steps is a high-impact problem for real-world applications of diffusion models. The method’s compatibility with existing models enhances its utility.\n\n**Weaknesses Noted but Insufficient for Rejection**:  \n- Theoretical justification for optimizing perceptual loss (vs. ELBO) is incomplete but mitigated by empirical success.  \n- Clarity issues (e.g., notation, motivation) are addressable in revisions.  \n- Limited high-resolution dataset experiments, but results on LSUN 128×128 (FID 11.6 with 10 steps) suggest scalability.\n\nGiven the paper’s empirical advancements and alignment with conference standards for impactful, novel work, acceptance is warranted with expectations for revisions to improve clarity and theoretical discussion."
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about the divergence of SGD with momentum (SGDm) under covariate shift, particularly focusing on resonance phenomena. The abstract mentions that existing work on SGDm convergence typically assumes iid data, but this paper explores scenarios with non-iid data, like continual learning and RL. They model SGDm as a parametric oscillator and use ODE analysis to show divergence under certain conditions. The experiments extend beyond the theoretical setup to include non-periodic shifts and neural networks.\n\nNow, looking at the reviewers' comments. Reviewer 1 didn't add much after the authors' response. Reviewer 2 raises concerns about technical novelty, arguing that the proofs rely too much on existing literature without adding new techniques. They also question the intuitive explanation of resonance and the relevance of the theory to actual SGDm since the analysis uses expected gradients (removing the stochastic element). Reviewer 3 points out that the theoretical contributions are limited, with Proposition 1 being straightforward and Proposition 2 similar to prior work. They also mention presentation issues and the disconnect between the problem setting and experiments. Reviewer 4 is positive, praising the structure and empirical validation but notes uncertainty about novelty. Reviewer 5 suggests improvements in the theoretical analysis, like analyzing discrete dynamics directly, and wants more concrete examples. Reviewer 6 likes the thoroughness but questions practical relevance and suggests adding real-world data examples.\n\nThe main themes from the reviews are: \n\n1. **Technical Novelty**: Several reviewers (2, 3, 5) question the novelty of the theoretical contributions, noting reliance on existing methods like Floquet theory and operator splitting. The authors claim technical novelty in handling time-varying ODEs, but reviewers argue this is not sufficiently original.\n\n2. **Empirical Strength vs. Practical Relevance**: The experiments show resonance in broader settings, but some reviewers (6, 5) question if this phenomenon is relevant in real-world scenarios with more complex data distributions. Reviewer 6 suggests testing on real datasets.\n\n3. **Presentation and Clarity**: Reviewers 2, 3, and 5 mention that the theoretical sections, especially Theorem 1, are hard to follow and lack intuitive explanations. The example in Section 4.1 isn't sufficiently explained.\n\n4. **Stochasticity in Analysis**: Reviewers 2 and 4 note that the theory uses expected gradients, which removes the \"S\" in SGDm, making the connection to stochastic settings tenuous. The empirical results with stochasticity (Section 4.3) show dampened resonance, which might limit practical implications.\n\n5. **Potential Impact**: The identification of resonance as a divergence mechanism in non-iid settings is seen as interesting (Reviewers 3, 4, 6), but the practical significance is debated. Some reviewers think it's a niche scenario (high momentum, low noise), while others see broader implications.\n\nConsidering the criteria for a top-tier conference, the paper needs to present significant technical novelty and impactful contributions. The theoretical part, while correct, seems to build heavily on existing frameworks without introducing new methodologies. The empirical results are extensive but might not convincingly demonstrate real-world applicability. The mixed reviews (some positive, some negative) suggest that while the paper has merits, the concerns about novelty and practical relevance are substantial. However, Reviewer 4 and 6's positive outlooks highlight the paper's strengths in systematically exploring an understudied phenomenon. The key question is whether the resonance phenomenon's identification and the empirical validations in varied settings are enough to outweigh the theoretical novelty concerns. Given that top conferences value both theoretical and empirical contributions, and the paper does provide a new perspective on optimization dynamics in non-iid scenarios, it might still be a candidate for acceptance. However, the concerns about the theoretical depth and practical impact could tip the balance towards rejection if the committee prioritizes technical novelty highly.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper identifies an intriguing phenomenon—resonance-driven divergence in SGDm under covariate shift—and provides empirical validation across diverse settings. However, key weaknesses undermine its suitability for a top-tier conference:  \n1. **Limited Technical Novelty:** The theoretical analysis heavily relies on existing frameworks (e.g., Floquet theory, operator splitting), with insufficient novel methodology. Reviewers noted that the claimed \"technical novelty\" in handling time-varying ODEs is overstated, as the proofs largely defer to prior results.  \n2. **Disconnect Between Theory and Practice:** The analysis uses expected gradients, effectively removing stochasticity (\"S\") from SGDm. While experiments show resonance persists with stochasticity, the theoretical claims about SGDm are weakened, raising doubts about their practical relevance.  \n3. **Presentation Gaps:** The resonance mechanism and its connection to parameters (e.g., why specific frequencies cause divergence) are not intuitively explained, limiting accessibility and impact.  \n4. **Niche Applicability:** The divergence occurs under restrictive conditions (e.g., high momentum, periodic shifts), and reviewers questioned whether this generalizes to real-world scenarios with broader frequency spectra or normalization techniques.  \n\nWhile the empirical work is thorough, the theoretical contributions lack sufficient novelty and depth for a top-tier venue. The paper would benefit from a more direct analysis of discrete-time SGDm, clearer intuitive explanations, and validation on real-world non-iid datasets."
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper proposes a new benchmark for compositional learning behaviors using meta-referential games and a Symbolic Continuous Stimulus (SCS) representation. The reviewers have several concerns. \n\nFirst, all reviewers mention issues with clarity. The paper is hard to follow, with unclear explanations of key concepts and insufficient details in the methodology. For example, Reviewer 2 points out that the introduction is cluttered with related work without a clear problem statement, and Reviewer 3 notes duplicated text between sections. This lack of clarity makes it difficult to assess the contributions.\n\nNext, the technical novelty of the SCS representation is questioned. Reviewers 1, 3, and 4 argue that the benefits of SCS over other representations (like one-hot or continuous) aren't convincingly demonstrated. Reviewer 4 suggests comparing with fully continuous representations and existing benchmarks like SCAN. The experiments are also criticized for being insufficient—only a few baselines are tested, with limited seeds and no ablation studies. The results show low performance (29% accuracy), which isn't compelling, and there's no evidence that SCS is better than alternatives in broader contexts.\n\nThe benchmark's significance is another concern. While the idea of a new benchmark is good, reviewers feel it's not well-detailed or differentiated from existing ones. Reviewer 2 asks for more emphasis on what makes this benchmark unique and how it measures compositionality. Without this, the benchmark's value isn't clear.\n\nMethodological issues are also highlighted. Reviewer 4 mentions that the experimental setup lacks details on model architectures and training regimes. The choice of hyperparameters and the poor performance of DNC compared to LSTM raise questions about whether the task is inherently hard or just poorly optimized.\n\nReviewers also point out grammatical errors and repetitive sections, which affect readability. The literature review is seen as insufficient, missing key works on compositionality and representations, which weakens the paper's positioning.\n\nConsidering all these points, the paper has potential but falls short in its current form. The core issues are clarity, insufficient experimentation, and lack of convincing evidence for the proposed methods. The reviewers unanimously rate technical and empirical novelty as low, and correctness as problematic. Given the requirements of a top-tier conference, the paper needs significant revisions to address these concerns. Therefore, the decision is to reject.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper introduces a novel benchmark for compositional learning via meta-referential games and proposes a Symbolic Continuous Stimulus (SCS) representation. While the problem of compositional generalization is important and timely, the paper suffers from critical shortcomings that render it unsuitable for acceptance at a top-tier conference in its current form:  \n\n1. **Clarity and Presentation Issues:**  \n   - Multiple reviewers highlight the paper’s lack of clarity, with repetitive, verbose explanations and undefined key concepts (e.g., \"positional disentanglement metric,\" \"systematic generalization\").  \n   - Critical details about the benchmark, experimental setup (e.g., model architectures, training regimes), and SCS’s applicability are missing. For example, Reviewer 2 notes that the benchmark is only briefly mentioned without a clear comparison to existing benchmarks, and Reviewer 4 criticizes the lack of clarity on how communication actions are structured.  \n\n2. **Weak Technical Novelty and Validation:**  \n   - The SCS representation is not sufficiently justified. Reviewers question its advantages over one-hot encodings (OHE) or fully continuous representations (Reviewer 4), especially since OHE outperforms SCS in initial experiments (Figure 2). The authors fail to address why SCS is preferable in realistic scenarios where discrete vocabularies (like human language) succeed.  \n   - The benchmark’s novelty is undercut by limited empirical validation. Results show poor performance (e.g., 29% accuracy in Figure 4), and experiments lack ablation studies, comparisons to continuous representations, or tests on established benchmarks like SCAN/gSCAN. Reviewer 4 argues that low performance might stem from poor hyperparameter tuning rather than task difficulty.  \n\n3. **Insufficient Empirical Rigor:**  \n   - Experiments are underdeveloped: only two seeds are used for some models (Reviewer 1), no statistical significance is reported, and key hyperparameters (e.g., vocabulary size, \\(N_{dim}\\)) are omitted. Reviewer 3 notes that failure cases and model limitations are not analyzed.  \n   - The paper does not engage with foundational literature on compositionality (e.g., Smolensky, Chalmers) or modern approaches (e.g., Santoro et al.), weakening its theoretical grounding.  \n\n4. **Marginal Contributions:**  \n   - Reviewers unanimously rate technical and empirical novelty as \"marginally significant\" (2/4). The SCS representation and benchmark are not shown to enable progress beyond the paper’s narrow setup, and the baseline results do not convincingly demonstrate a community challenge.  \n\nWhile the problem is compelling, the paper’s lack of clarity, weak empirical validation, and limited novelty outweigh its potential. Significant revisions—including clearer exposition, rigorous experiments, and deeper theoretical engagement—are needed to meet top-tier standards."
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by looking at the paper's contributions. The authors propose PEARL, a new framework for differentially private data synthesis using deep generative models. The main idea is to sanitize data in a one-shot manner, avoiding the privacy degradation issues common in gradient sanitization methods. They use characteristic functions and adversarial re-weighting, which seems novel. \n\nReviewer 1 is positive, highlighting the approach's novelty, theoretical backing, and favorable empirical results. However, they note the lack of thorough benchmarking, suggesting more datasets and comparisons, especially with PATE-GAN for tabular data and more complex image datasets like CIFAR-10. The minor comment about moving Algorithm 1 to the main paper is easy to address.\n\nReviewer 2 has some confusion about the privacy budget allocation and sensitivity calculations. They also question why performance doesn't improve with higher epsilon and suggest comparisons with other methods like PATE-GAN and CTGAN. These are valid technical concerns that need clarification. If the authors can address these points, it would strengthen the paper.\n\nReviewer 3 points out missing related work (PrivSyn, DPT, etc.), which is a significant issue. Not citing key recent works could undermine the paper's credibility. They also mention the need for more datasets, baselines, complexity analysis, and additional experiments. The lack of these elements makes the empirical evaluation seem incomplete. The reviewer also notes that the privacy-preserving optimization details are vague, which is a critical aspect for a DP paper. Addressing these would require substantial work but is necessary for a top-tier conference.\n\nAll reviewers agree the technical novelty is present but not entirely groundbreaking, as aspects exist in prior work. The empirical evaluations are somewhat limited. The correctness is mostly there, but some claims need better support. \n\nThe main strengths are the novel framework, theoretical guarantees, and initial positive results. Weaknesses include insufficient benchmarking, missing related work, and unclear technical details. For a top-tier conference, the paper needs to thoroughly address these issues. However, given the reviewers' scores (mostly 3s) and the potential of the approach, if the authors can adequately respond to the reviewers' concerns (especially adding more comparisons, clarifying privacy mechanisms, and expanding related work), the paper might be acceptable. But since the current version has notable gaps, especially in empirical validation and related work, it might lean towards rejection unless major revisions are made. However, the reviewers didn't outright reject it, and Reviewer 3 suggests a possible accept with revisions. But considering the conference's high standards, the paper in its current state might not meet the bar. The decision hinges on whether the authors can convincingly address the issues, but based on the provided reviews, there's a risk that the current submission isn't sufficient.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper presents a novel framework (PEARL) for differentially private data synthesis with theoretical guarantees and promising empirical results. However, critical weaknesses highlighted by reviewers prevent acceptance at a top-tier conference in its current form:  \n1. **Insufficient Empirical Validation:** The experiments lack breadth (e.g., limited datasets, missing comparisons to key baselines like PATE-GAN, PrivBayes, and CTGAN) and depth (e.g., no analysis of computational complexity or performance saturation at varying privacy budgets).  \n2. **Missing Related Work:** Omission of recent, highly relevant papers (e.g., PrivSyn, DPT) undermines the paper’s positioning and novelty claims.  \n3. **Technical Ambiguities:** Reviewers raised unresolved concerns about privacy budget allocation, sensitivity calculations, and the one-shot sanitization mechanism, which are critical for a privacy-focused contribution.  \n4. **Incomplete Analysis:** The lack of complexity analysis and limited evaluation tasks (e.g., no downstream task performance beyond basic metrics) weaken the empirical contribution.  \n\nWhile the core idea is promising, the paper does not yet meet the rigor expected of a top-tier conference. Addressing these issues would require significant revisions, including expanded experiments, clearer technical explanations, and engagement with recent literature."
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper presents AutoOED, a platform for automated optimal experimental design using multi-objective Bayesian optimization. The main contributions seem to be the platform itself, a modular framework for MOBO algorithms, a novel BP strategy for asynchronous batch experiments, and a GUI for accessibility. \n\nReviewers have mixed opinions. Reviewer 1 initially had concerns about performance and novelty but raised the score to 5. Reviewer 2 and 4 point out that the work is more of a software tool with limited methodological novelty. They mention that the BP strategy isn't well theoretically justified and the empirical results are mixed. Reviewer 3 also questions the scientific contribution, noting the lack of theoretical backing for BP. Reviewer 5 is more positive, highlighting the problem's importance and the platform's usability but notes minor issues.\n\nThe key issues here are novelty and empirical validation. The BP method is a combination of existing techniques (KB and LP) with a threshold, but the paper doesn't provide a strong theoretical foundation for why this combination works. The experiments show that BP isn't consistently better than other methods, which undermines the claim of it being state-of-the-art. Additionally, the platform's scalability is limited to 2-3 objectives, which the authors acknowledge but might limit its applicability. \n\nThe comparison with existing tools is another weak point. Reviewer 4 mentions missing several relevant BO packages in the related work, which could affect the perceived significance. The GUI is a plus for accessibility, but if the core technical contribution is minor, it might not be enough for a top-tier conference. \n\nOn the positive side, the platform's modularity and real-world application (controlling hardware experiments) are strengths. However, the real-world benchmark was only compared against random search, which isn't a strong baseline. \n\nGiven that top-tier conferences like ICLR or NeurIPS prioritize significant technical novelty and strong empirical results, the paper's contributions might fall short. The BP strategy's lack of theoretical justification and inconsistent empirical performance, combined with the platform's incremental improvements over existing tools, suggest that the paper doesn't meet the high bar required. The reviewers' scores (mostly 3s and 2s in technical and empirical novelty) support this. \n\nFinal Decision: Reject\n</think>\n\nThe paper introduces AutoOED, a platform for multi-objective Bayesian optimization (MOBO) with a focus on automated experimental design. While the tool demonstrates practical utility through modularity, a user-friendly GUI, and real-world hardware integration, its suitability for a top-tier conference hinges on technical novelty, empirical robustness, and alignment with academic standards. Here's the analysis:\n\n### **Strengths**\n1. **Practical Utility**: AutoOED addresses a clear need for accessible, user-friendly tools in experimental design. The GUI and modular framework lower barriers for non-experts and researchers alike.\n2. **Real-World Application**: The demonstration of fully automated hardware experiments is compelling and highlights the platform’s potential impact in applied settings.\n3. **Engineering Effort**: The integration of existing MOBO algorithms and asynchronous batch optimization (via the Believer-Penalizer strategy) reflects significant engineering work.\n\n### **Weaknesses**\n1. **Limited Technical Novelty**: The core methodological contribution—the Believer-Penalizer (BP) strategy—is critiqued as a straightforward combination of existing techniques (Kriging Believer and Local Penalization) without theoretical justification. Reviewers note that the threshold-based switching between KB and LP lacks rigorous motivation or analysis.\n2. **Overstated Empirical Claims**: The empirical results are inconsistent. BP does not consistently outperform baselines (e.g., underperforms on DTLZ3 in Figure 4), and AutoOED is outperformed on half the benchmarks in Figure 5. Comparisons with random search in real-world experiments are insufficient to demonstrate superiority over state-of-the-art tools.\n3. **Scalability and Scope**: The platform’s restriction to 2–3 objectives contradicts the initial claim of handling “m≥2” objectives, limiting its generality. Many real-world problems require many-objective optimization, which AutoOED does not address.\n4. **Incomplete Related Work**: Key open-source BO tools (e.g., AX, Optuna) are omitted, weakening the positioning of AutoOED’s contributions. The authors also fail to cite their own prior arXiv preprint on AutoOED, raising concerns about originality.\n\n### **Reviewer Consensus**\nReviewers acknowledge the platform’s usability but unanimously criticize its lack of methodological depth. The BP strategy is seen as incremental, and the empirical evaluation fails to substantiate claims of superiority. While the system is well-engineered, it is perceived as “yet another BO package” with minor novelty. Scores for technical/empirical significance (mostly 2–3/5) reflect this.\n\n### **Conclusion**\nAutoOED’s strengths lie in its practical implementation and accessibility, but these are insufficient for acceptance at a top-tier conference. The paper does not meet the high bar for technical innovation or empirical rigor expected in such venues. The BP strategy’s weak theoretical grounding, inconsistent results, and limited scope undermine its contribution. While the tool is valuable for practitioners, the work is better suited for a software-focused journal or workshop.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper presents a novel approach to neural network compression using tropical geometry, specifically focusing on the Hausdorff distance of tropical zonotopes. The main theoretical contribution is an approximation theorem linking the error between two ReLU networks to the Hausdorff distance of their tropical zonotopes. The authors also propose compression methods using K-means and evaluate them on several datasets.\n\nReviewer 1 acknowledges the mathematical solidity of the analysis and the improvement over a tropical baseline but points out issues with outdated architectures (LeNet), lack of complexity analysis, and overclaims in the abstract. The experiments are on small networks, raising questions about scalability. Reviewer 2, while outside their expertise, notes the novelty but criticizes the limited empirical evaluation on small datasets. They also mention the paper's alignment with the conference. Reviewer 3 questions the practical utility of the error bounds and the experimental comparisons, suggesting the need for modern architectures and SOTA comparisons. Reviewer 4 finds the idea novel and well-explained but highlights missing comparisons with non-tropical methods and reproducibility issues.\n\nThe common themes are: theoretical novelty but empirical weaknesses, especially in experiments (small datasets, outdated models, lack of SOTA comparisons), and some overclaims in the abstract. The technical contribution is recognized, but the practical impact is uncertain. The authors' response addresses some points, like the proof-of-concept nature of experiments and the focus on theoretical groundwork. However, the reviewers still have reservations, especially regarding empirical significance.\n\nFor a top-tier conference, both theoretical and empirical contributions are important. The theoretical part seems solid and novel, which is a strength. However, the empirical evaluation is limited, using outdated models and not comparing with the latest methods. The reviewers also note that the error bounds might not be practically useful. While the theoretical contribution is significant, the empirical part doesn't convincingly demonstrate practical applicability or superiority over existing methods beyond tropical baselines. The overclaims in the abstract (points 2 and 3) weaken the paper's credibility. Given that top conferences expect both theoretical and empirical rigor, the weaknesses in the experiments and validation might outweigh the theoretical strengths. The paper might be better suited for a theory-focused venue, but given the current reviews and the mixed feedback, rejection seems likely.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper presents a novel theoretical contribution by linking neural network approximation to tropical geometry, specifically through Hausdorff distances of tropical zonotopes. While the theoretical framework is mathematically rigorous and offers a fresh perspective on neural network compression, the empirical evaluation and practical validation fall short of the standards expected at a top-tier conference. Key issues include:\n\n1. **Empirical Limitations**: \n   - Experiments are conducted on outdated architectures (LeNet, VGG, AlexNet) rather than modern networks (e.g., ResNets, Transformers), limiting the relevance of the results.\n   - Comparisons with state-of-the-art (SOTA) pruning methods are absent. For instance, ThiNet (2017) is used as a baseline, but newer methods from recent surveys are not included.\n   - The datasets (MNIST, Fashion-MNIST) are small-scale, and results on larger benchmarks (e.g., ImageNet) are missing, raising doubts about scalability.\n\n2. **Overclaims in the Abstract**: \n   - The assertion that the method \"advances baseline pruning methods\" and has \"competitive performance against modern pruning techniques\" is unsupported. The experiments only demonstrate superiority over a tropical baseline (Smyrnis & Maragos) and do not convincingly outperform non-tropical methods like ThiNet in most cases.\n\n3. **Technical Concerns**:\n   - The error bounds (Propositions 4–5) are noted to decrease with fewer retained weights, which is counterintuitive and undermines their practical utility.\n   - Computational complexity, runtime, and memory usage are not analyzed, making it difficult to assess the method’s efficiency.\n\n4. **Reproducibility and Clarity**:\n   - Pseudo-code and implementation details are insufficient, hindering reproducibility.\n   - The experimental setup lacks clarity (e.g., whether convolutional layers were pruned in CNN experiments).\n\n**Strengths**:\n- The theoretical contribution (Theorem 2) is novel and well-motivated, offering a geometric interpretation of neural network approximation.\n- The tropical geometry perspective is underexplored and could inspire future theoretical work.\n\n**Conclusion**:\nWhile the theoretical insights are valuable, the empirical evaluation does not meet the bar for a top-tier conference. The experiments are limited in scope, and the claims about practical performance are overstated. Addressing these issues would require significant revisions, including modern benchmarks, SOTA comparisons, and a more rigorous analysis of computational trade-offs. As such, the paper is better suited for a specialized theoretical venue or a revision cycle with expanded empirical validation."
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is about improving RNNs for hierarchical motor control using insights from neuroscience, specifically the thalamocortical architecture. The main claims are that their biologically inspired model allows for better continual learning of motor motifs and zero-shot transfer to new chains without additional training.\n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 2 and 3 have significant concerns. Reviewer 2 questions the biological plausibility and computational necessity of the resetting mechanism, suggesting that simpler inhibitory inputs might suffice. They also rate technical and empirical novelty as low. Reviewer 3 is critical of the small scale of the experiments (only 10 motifs, 300 units) and the lack of testing alternative approaches, like training with end states or more motifs. They argue that the problem setup is too synthetic and not convincing for broader ML relevance. Reviewer 4 is more positive, noting the simplicity and biological inspiration but mentions uncertainty about the appendices. Reviewer 5 points out issues with the problem formalization, writing clarity, and the existence of a trivial solution (hard resetting), which the authors didn't adequately address.\n\nThe authors addressed some concerns in their revisions, but key issues remain. The main criticisms revolve around the novelty and significance of the approach, the experimental scale, and the lack of comparison with alternative methods. The reviewers also question the biological motivation and whether the proposed mechanism is truly necessary or just one of many possible solutions.\n\nFor a top-tier conference, the paper needs to demonstrate clear technical novelty, empirical validation on non-trivial tasks, and solid grounding in theory or application. The reviewers' concerns about the toy problem scale and lack of comparison with alternative approaches (like training with more data or different initializations) weaken the empirical contribution. The biological inspiration is a plus, but the paper doesn't convincingly show that the thalamocortical mechanism is the key factor rather than a simpler reset. Additionally, the writing issues and unclear problem formalization mentioned by Reviewer 5 could hinder the paper's impact and reproducibility.\n\nWhile the idea of integrating neuroscience insights into RNNs is interesting, the combination of methodological concerns, limited empirical validation, and presentation issues makes the paper's contributions marginal for a top-tier venue. The reviewers' scores (mostly 2s and 3s in technical and empirical novelty) suggest that the paper doesn't meet the high bar required. Therefore, despite some strengths, the paper should be rejected.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper proposes a thalamocortical-inspired RNN architecture to address continual learning of motor motifs and zero-shot chaining. While the biological inspiration and focus on robustness are commendable, the reviewers identified critical flaws that undermine its suitability for a top-tier conference:  \n\n1. **Lack of Empirical Rigor and Scale**:  \n   - The experiments are limited to a synthetic task with only 10 motifs and small networks (300 units). This scale is deemed insufficient to demonstrate generalizability or relevance to real-world ML challenges (Reviewers 2, 3).  \n   - The authors did not rigorously test alternative hypotheses (e.g., training with end states, noise injection, or larger motif libraries), which weakens claims about the necessity of their proposed inductive bias (Reviewers 2, 3, 5).  \n\n2. **Unconvincing Biological and Computational Motivation**:  \n   - The thalamocortical reset mechanism’s biological plausibility is questioned. Reviewers argue that simpler inhibitory mechanisms (e.g., basal ganglia disinhibition) or generic state resets could achieve similar results (Reviewers 2, 3).  \n   - Claims about the superiority of the proposed architecture over standard RNNs rely on weak empirical evidence, with insufficient ablation studies to isolate the contribution of the preparatory module (Reviewers 2, 4).  \n\n3. **Presentation and Methodological Clarity**:  \n   - The problem formalization and task description are fragmented and lack mathematical rigor, making it difficult to assess technical contributions (Reviewer 5).  \n   - Critical details (e.g., hyperparameters, training protocols) are buried in appendices, while the main text lacks a coherent narrative (Reviewers 4, 5).  \n\n4. **Marginal Novelty and Impact**:  \n   - The work builds on established continual learning and RNN literature (e.g., Schuessler et al., 2020) but does not advance the field significantly. The proposed architecture is seen as incremental rather than transformative (Reviewers 2, 3, 5).  \n   - The synthetic task and small-scale experiments limit the broader relevance to machine learning or neuroscience communities (Reviewers 2, 3).  \n\nWhile the paper explores an interesting intersection of neuroscience and ML, the combination of methodological limitations, insufficient empirical validation, and unclear presentation outweigh its potential contributions. For a top-tier conference, the work does not meet the threshold for technical novelty, empirical significance, or generalizability."
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's try to figure out whether this paper should be accepted or rejected. The paper proposes a method called BiLAW, which uses a bi-level optimization approach (inspired by MAML) to reweight training samples for adversarial robustness. The main idea is to learn a parametric function that assigns weights based on the multi-class margin of samples. The authors claim improvements in both clean and robust accuracy compared to existing methods.\n\nLooking at the reviews, there's a mix of opinions. Some reviewers acknowledge the technical soundness and the empirical results, while others have concerns about novelty, evaluation completeness, and the significance of improvements. Let's break down the key points.\n\nFirst, the strengths mentioned by reviewers include the novel application of meta-learning (MAML) to adversarial training, the use of multi-class margins, and comprehensive experiments with AutoAttack. The method is well-written and the approach is methodologically sound. Some reviewers found the results convincing enough for publication, especially Reviewer 7 and 10, who recommend acceptance based on the empirical improvements.\n\nHowever, major concerns are raised. Reviewer 1 points out that the novelty is limited since it's a direct adaptation of MAML to adversarial training. They also criticize the lack of evaluation against adaptive attacks, which could undermine the robustness claims. Reviewer 3 and others question the choice of input encoding for the meta-network (multi-class margin vs. other features) and the lack of ablation studies to justify this design. Reviewer 6 and others note that the improvements are marginal when BiLAW is used alone and that combining it with TRADES might be the real reason for better performance. There's also a concern about outdated datasets (MNIST/F-MNIST) and small network architectures.\n\nThe authors' responses to some of these concerns don't fully satisfy the reviewers. For example, Reviewer 3 feels their questions about input encoding weren't adequately addressed, and Reviewer 1 insists that adaptive attack evaluation is necessary. The rebuttal about computational costs and input dimensionality for the meta-network is seen as insufficient without empirical backing.\n\nOn the technical side, the paper's contributions are seen as incremental by several reviewers. The core idea of using MAML for reweighting isn't entirely novel, even if applied to a new context. The empirical gains, while present, are not groundbreaking. The lack of comparison with SOTA methods like AWP and insufficient results on CIFAR-100 weaken the claims. Additionally, the absence of theoretical justification for the multi-class margin's superiority is a gap.\n\nThe split among reviewers is evident. Some see the empirical results as sufficient for acceptance, while others find the novelty and evaluation lacking. For a top-tier conference, the bar is high. The concerns about adaptive attacks and the incremental nature of the contribution might tip the balance. If the method's improvements are mainly due to combining with existing techniques (like TRADES) rather than the novel component itself, the significance is reduced.\n\nIn conclusion, while the paper presents a solid application of meta-learning to adversarial training, the concerns about novelty, incomplete evaluations (especially adaptive attacks), and marginal improvements without sufficient ablation studies make it fall short of the standards for a top-tier conference. The mixed reviews, with some strong rejections based on methodology and evaluation, suggest that the paper isn't ready for acceptance in its current form.\n</think>\n\nFinal Decision: Reject \n\n**Detailed Reasoning:**  \nThe paper proposes a bi-level optimization framework (BiLAW) for adversarial training, leveraging meta-learning (MAML) to reweight samples based on multi-class margins. While the idea of integrating meta-learning into adversarial training is promising, the reviews highlight critical shortcomings that undermine its suitability for a top-tier venue:  \n\n1. **Limited Novelty**: The core methodology is perceived as an incremental adaptation of MAML to adversarial training rather than a fundamentally novel contribution. Reviewers note that prior works (e.g., Meta-Weight-Net) have explored similar bi-level optimization for sample reweighting in non-adversarial settings, and the extension to adversarial robustness lacks sufficient technical innovation.  \n\n2. **Incomplete Evaluation**:  \n   - **Adaptive Attacks**: A major weakness is the lack of evaluation against adaptive attacks targeting the reweighting module. Reviewer 1 and others argue convincingly that omitting this analysis invalidates the robustness claims, as adaptive attacks are standard in adversarial ML.  \n   - **Dataset and Architecture Limitations**: Experiments on MNIST/F-MNIST are deemed outdated, and results on CIFAR-100 are missing. The use of small CNNs (e.g., WRN-10-32) instead of standard architectures (e.g., WRN-34-10) raises questions about scalability and relevance to SOTA comparisons.  \n\n3. **Marginal Empirical Gains**: While BiLAW shows improvements over some baselines, reviewers highlight that gains under AutoAttack are modest and often rely on combining with TRADES. The standalone performance of BiLAW is not compelling, and comparisons to recent SOTA methods (e.g., AWP) are absent.  \n\n4. **Theoretical and Ablation Gaps**:  \n   - The justification for using multi-class margins as input to the meta-network is underdeveloped. Reviewers criticize the lack of ablation studies comparing alternative input encodings (e.g., raw data, activations) to validate the design.  \n   - The connection between the theoretical motivation (multi-class margin) and the empirical results is tenuous, with insufficient analysis of how reweighting impacts robustness.  \n\n5. **Author Responses**: Several reviewers found the authors’ responses inadequate, particularly regarding adaptive attacks and input encoding. For instance, Reviewer 3 notes that ablation studies in the appendix do not address core concerns about the meta-network’s inductive bias.  \n\n**Conclusion**: While the paper introduces a technically sound approach and demonstrates moderate empirical improvements, the concerns about novelty, evaluation gaps, and insufficient justification for design choices outweigh its strengths. For a top-tier conference, the incremental nature of the contribution and unresolved methodological questions justify rejection. The authors should address adaptive attacks, expand experiments (e.g., CIFAR-100, SOTA comparisons), and provide deeper theoretical/ablation analysis in a future submission."
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper is about epoch-wise double descent in neural networks, using a linear teacher-student model to analyze the phenomenon. The abstract claims they derived closed-form expressions for generalization error over time, attributing double descent to features learned at different scales. \n\nReviewer 1 is positive, noting the work's insight into non-monotonic generalization error in simple models. They appreciate the clear writing and theory but have questions about extending to more scales, differences between model-wise and epoch-wise descent, and the peak interpretation. They also mention a possible figure color issue. Their summary is very favorable, suggesting the paper is a valuable contribution.\n\nReviewer 2 finds the work solid but points out connections to prior work on implicit regularization (like Ridge regression) and suggests discussing more existing literature. They question if the analysis in high-dimensional settings (d >> p) relates to Ridgeless regression studies. Their main concern is about situating the work within existing frameworks.\n\nReviewer 3 has mixed feelings. They acknowledge the strengths of the analysis and intuition but highlight weaknesses like typos, technical novelty, and connections to prior work. They also question the technical execution, such as handling of randomness and equation derivations. The reviewer is unsure about the technical advancements and wants more discussion on related works.\n\nNow, considering all three reviews, the main points are:\n\n1. **Technical Correctness and Clarity**: All reviewers find the claims mostly correct, but Reviewer 3 notes several typos and ambiguities in equations, which could affect reproducibility. The color issue in Figure 2 (Reviewer 1) and equation errors (Reviewer 3) need addressing but seem fixable.\n\n2. **Novelty and Significance**: Reviewers 1 and 2 rate technical and empirical novelty as 3 (significant but somewhat new), while Reviewer 3 gives 2 (marginally significant). The key is whether the insights into epoch-wise double descent via multi-scale features in linear models are novel enough. The connection to prior work (Heckel & Yilmaz 2020, Stephenson & Lee 2021, Tibshirani's works) needs more discussion. The paper's contribution seems to be providing a theoretical framework for a known phenomenon in DNNs using a simpler model, which is valuable but may overlap with existing analyses.\n\n3. **Empirical Validation**: The ResNet experiments on CIFAR10 are mentioned, but Reviewer 3 questions the depth of connection to real networks. The empirical part isn't the main focus but supports the theory.\n\n4. **Addressing Literature**: Both Reviewers 2 and 3 emphasize the need to better situate the work within existing literature. If the authors can address these connections in revisions, the paper's contribution would be clearer.\n\n5. **Potential Impact**: The work helps explain training dynamics in DNNs through linear models, which is useful for the community. The analytical results could guide future research, even if the model is simple.\n\nConsidering the reviews, the paper has merits but needs to address clarity, typos, and literature connections. The reviewers' concerns are mostly about presentation and context rather than fundamental flaws. Since the technical contributions are solid and the topic is relevant, the paper is likely acceptable with revisions. However, the decision hinges on how well the authors can address the reviewers' points, especially regarding prior work and technical clarifications. Given that the conference setting usually allows for author responses to address these issues, and assuming the authors can adequately respond, the paper should be accepted.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper provides a theoretically grounded analysis of epoch-wise double descent through a linear teacher-student model, offering closed-form expressions for generalization dynamics. While reviewers raised valid concerns about technical clarity, connections to prior work, and minor errors, the core contributions—linking multi-scale feature learning to double descent and demonstrating qualitative alignment with DNN behavior—are novel and impactful for understanding deep learning dynamics.  \n\n1. **Strengths:**  \n   - The analytical framework successfully captures epoch-wise double descent, a phenomenon observed in deep networks, using a tractable model.  \n   - The intuition of multi-scale feature learning as a driver of non-monotonic test error is compelling and aligns with empirical validations.  \n   - The work bridges theory and practice, with results that could inform future research on training dynamics.  \n\n2. **Weaknesses Addressed in Revisions:**  \n   - **Clarity and Typos:** Reviewer 3’s concerns about equation derivations, figure labels, and terminology require revision but do not invalidate the results.  \n   - **Connections to Prior Work:** The authors must explicitly discuss relationships with Ridge regularization (Reviewer 2) and multi-scale analyses (Reviewer 3) to clarify novelty.  \n   - **Technical Justification:** Expanding on the replica method’s application and addressing stochasticity in SGD dynamics (Reviewer 3) would strengthen the manuscript.  \n\n3. **Impact:**  \n   - The paper advances the theoretical understanding of double descent, a critical topic in machine learning. Its simplicity and analytical tractability make it a valuable reference for future studies.  \n\nGiven the reviewers’ overall positive assessment of significance and the feasibility of addressing concerns via revisions, acceptance is justified."
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's dive into this paper review. The paper in question is titled \"Behavior Proximal Policy Optimization\" and focuses on applying online on-policy algorithms to offline reinforcement learning (RL). The main claim is that the inherent conservatism of on-policy methods like PPO makes them suitable for offline RL without needing extra constraints. The authors propose BPPO, a modified version of PPO, and show it outperforms state-of-the-art methods on the D4RL benchmark.\n\nFirst, looking at the reviewers' comments. Reviewer 1 points out strengths in the theoretical analysis and empirical results but raises concerns about clarity, missing experiments (like showing overestimation isn't an issue), and whether the method is truly on-policy. They also question the comparison with Onestep BPPO and the necessity of certain hyperparameters. Reviewer 2 highlights potential overlaps with prior work like GePPO and questions the novelty of the theoretical contributions. They also mention issues with the advantage approximation and missing comparisons with recent SOTA methods. Reviewer 3 finds the paper well-written but has questions about the advantage model and the algorithm's iterative steps. Reviewer 4 acknowledges the simplicity and theoretical insight but is unconvinced by the empirical gains and points out the lack of analysis on dataset quality. After rebuttal, they upgraded to a weak accept but still have reservations. Reviewer 5 criticizes the strong assumptions and the empirical scope, suggesting the method might not handle sparse rewards well.\n\nThe key issues here are novelty, technical soundness, and empirical validation. The theoretical contributions are contested, especially regarding their overlap with existing works like GePPO. If the theorems are indeed a specific case of prior work, that's a major hit on novelty. The authors need to clearly differentiate their approach. The assumption about the behavior policy (Assumption 1) is another sticking point. If it's too strong and not validated empirically, that undermines the theory's applicability.\n\nEmpirically, while the results on D4RL are positive, the improvements over baselines are marginal. Reviewers note that the benchmarks are saturated, and the lack of comparison with newer methods (like EDAC, LAPO) is a problem. The rebuttal added some results but didn't fully address all concerns. The simplicity of BPPO is a plus, but if existing methods like TD3+BC are equally simple and perform similarly, the practical impact is lessened.\n\nThe reviewers' scores are mixed. Reviewers 1 and 2 have significant concerns about technical novelty and soundness, giving lower scores. Reviewers 3 and 4 are more positive but still note issues. Reviewer 5 is critical. The rebuttal addressed some points but didn't fully resolve the major concerns, especially around novelty and empirical significance.\n\nIn a top-tier conference, novelty and clear contributions are crucial. If the core theory isn't sufficiently novel compared to prior work, and the empirical results aren't groundbreaking, the paper might not meet the acceptance threshold. The authors need to better position their work against existing literature and provide more compelling evidence of BPPO's advantages. Without that, despite some strengths, the paper may not be strong enough for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents an interesting approach by adapting on-policy PPO for offline RL, with theoretical analysis and empirical validation. However, key concerns raised by reviewers undermine its suitability for a top-tier conference:  \n1. **Lack of Technical Novelty:** The theoretical contributions (monotonic improvement bounds) overlap significantly with prior work (e.g., GePPO), which the authors did not adequately address. Reviewer 2 highlights that the theorems are specific instances of existing generalized policy improvement frameworks, diminishing the paper’s novelty.  \n2. **Weak Empirical Significance:** While BPPO achieves competitive results on D4RL, the improvements over baselines (e.g., CQL, TD3+BC) are marginal and not consistently demonstrated across all environments. The experiments lack comparisons with newer SOTA methods (e.g., EDAC, RORL), and the rebuttal’s additional results did not resolve concerns about practical impact.  \n3. **Unresolved Theoretical Concerns:** Assumption 1 (exact knowledge of the behavior policy) is unrealistic and unvalidated in experiments. The mismatch between theoretical guarantees (e.g., Theorem 4) and practical approximations (e.g., clipped advantage estimates) raises doubts about the method’s soundness.  \n4. **Methodological Ambiguities:** Reviewers questioned the algorithm’s iterative design (e.g., the role of *K* steps, clipping decay) and the lack of ablation studies for critical hyperparameters. These gaps weaken the reproducibility and interpretability of the results.  \n\nWhile the paper’s simplicity and insight into on-policy conservatism are commendable, the unresolved issues in novelty, theoretical rigor, and empirical conclusiveness make it fall short of the high bar for acceptance at a top-tier venue."
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper in question is titled \"Variational Neural Cellular Automata\" and proposes a generative model inspired by biological cellular processes. The abstract highlights that the model is a probabilistic generative model, which is a step beyond previous works. They evaluate it on standard datasets like MNIST, Noto Emoji, and CelebA. The key strengths mentioned are the self-organizing nature and robustness to perturbations, but they admit the performance is below state-of-the-art.\n\nNow, looking at the reviewers' comments. Reviewer 1 is leaning towards rejection due to underwhelming experimental results and the lack of practical interest in the resilience to damage. Reviewer 2 points out that some aspects are not novel, referencing prior work on ResNets with constant weights, but acknowledges the bridging of NCA and ResNet literature. Reviewer 3 was initially positive and increased their score after the rebuttal. Reviewer 4 is on the fence, noting the paper's clarity and honesty but questioning the motivation and experimental results. Reviewer 5 initially had concerns but raised their score after the authors addressed comments. Reviewer 6 is critical, arguing that the work lacks novelty as the decoder can be seen as a standard convolutional network. Reviewer 7 finds the approach interesting but incremental and suggests more comparisons.\n\nThe main themes from the reviews are: \n\n1. **Novelty and Technical Contribution**: Some reviewers (2, 6, 7) question the novelty, pointing out prior work on ResNets and NCAs. The authors' rebuttal addressed some of these by adding references and clarifications, but there's still a debate on whether the NCA perspective adds significant value beyond existing architectures.\n\n2. **Experimental Results**: The results are consistently noted as underwhelming, especially on CelebA. While MNIST works well, the performance on more complex datasets isn't competitive. However, the paper is honest about these limitations.\n\n3. **Clarity and Reproducibility**: The paper is well-written and reproducible, which is a plus. Reviewers 3 and 5 appreciated the authors' responses and improvements.\n\n4. **Potential Impact**: The self-organizing aspect and robustness are highlighted as interesting, but their practical utility is questioned. Some reviewers see value in exploring NCAs for generative models despite the performance gap.\n\nConsidering the conference's top-tier standards, the paper's acceptance hinges on whether the novelty and potential insights outweigh the subpar empirical results. The authors made efforts to address reviewer concerns, but key issues like the gap in performance and questions about the true novelty remain. While the bridging of NCA with VAEs is a contribution, the lack of SOTA results and the argument that the architecture isn't fundamentally different from existing models might tip the balance towards rejection. However, some reviewers upgraded their scores post-rebuttal, indicating that the paper has merit. The community might benefit from the exploration of self-organizing models even if they aren't yet competitive. Given the mixed reviews but with some strong advocates post-rebuttal, the paper might be a borderline case. However, in a top-tier conference where technical novelty and strong empirical results are critical, the underwhelming performance and remaining doubts about novelty could lead to rejection.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces a novel integration of Neural Cellular Automata (NCA) with variational inference, proposing the VNCA as a probabilistic generative model. While the work is well-written, methodologically sound, and explores an underexplored direction (self-organizing generative processes), several critical issues justify rejection at a top-tier conference:  \n\n1. **Limited Technical Novelty**: Multiple reviewers (2, 6, 7) highlight that the core architecture can be reinterpreted as a deep convolutional network with weight sharing, aligning with prior work on ResNets and neural ODEs. The NCA framing, while interesting, does not sufficiently differentiate the approach or demonstrate unique advantages over existing frameworks.  \n\n2. **Underwhelming Empirical Results**: The model lags significantly behind state-of-the-art generative methods on standard benchmarks (e.g., CelebA), with likelihoods and sample quality deemed uncompetitive. While the authors transparently acknowledge this gap, the results do not convincingly validate the practical utility of the proposed method.  \n\n3. **Motivational and Impact Concerns**: Reviewers (4, 6, 7) question the problem being solved. The self-organizing properties and damage robustness, while theoretically intriguing, are not shown to enable applications where existing models fail. The work does not establish a compelling use case for NCAs in generative modeling beyond incremental exploration.  \n\n4. **Mixed Reviewer Consensus**: Though some reviewers raised scores post-rebuttal (e.g., Reviewer 5), others maintained concerns about novelty and significance. In a competitive setting, the unresolved doubts about foundational contributions and practical relevance outweigh the paper’s strengths.  \n\nWhile the paper contributes to the growing literature on NCAs and offers insights into self-organizing systems, it does not meet the high bar for technical innovation or empirical performance required by top-tier venues."
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes using graph structures and topological properties for OOD detection, leveraging object detection models to create semantic graphs. The abstract claims high AUROC scores on LSUN, comparable to SOTA methods. \n\nReviewer 1 points out strengths like clear intuition and extensive comparisons but criticizes vague terms like \"common sense,\" lack of detailed methodology, and insufficient validation against SOTA. They also mention grammatical issues. The correctness is rated 3, which is okay, but technical and empirical novelty are only 3 and 2, suggesting the contributions are somewhat new but not groundbreaking.\n\nReviewer 2 is harsher, calling the paper poorly prepared with confused notations and lack of comparison to existing methods. They note the literature review is missing and experiments are not convincing. They give the lowest scores (1s) across correctness and novelty, recommending strong rejection.\n\nReviewer 3 finds the idea interesting but not novel, as it combines existing methods. They criticize missing comparisons to SOTA, lack of details on the object detector, and no ablation studies. The scores here are 2s and 2s, indicating marginal significance.\n\nReviewer 4 acknowledges the novel idea of semantic graphs for interpretability but points out reliance on a pre-trained object detector, which may not work for all data types. They also mention missing experiments, poor writing, and insufficient related work. Their scores are mixed (3s and 2s), suggesting some merit but major flaws.\n\nPutting this together, the main issues are:\n\n1. **Novelty**: The approach combines existing techniques (object detection, graph embeddings) without a clear novel contribution. Reviewers question if this is more than an incremental improvement.\n\n2. **Methodology and Clarity**: The paper lacks detailed descriptions, making reproducibility hard. Examples and step-by-step explanations are missing. Writing issues and unclear notations confuse readers.\n\n3. **Experiments**: The experiments are limited to LSUN without comparisons to key SOTA methods. No ablation studies to show the impact of each component. The reliance on a pre-trained detector's accuracy isn't analyzed.\n\n4. **Literature Review**: Important prior work, especially recent surveys and methods, are not adequately discussed, making it hard to position the work within the field.\n\nWhile the idea of using semantic graphs for OOD detection is interesting and could be a good direction, the execution falls short of top-tier conference standards. The lack of thorough experimentation, comparisons, and methodological clarity are significant drawbacks. Even though some reviewers see potential, the consensus leans towards rejection due to these critical issues. The paper needs substantial revisions, more comprehensive experiments, and better articulation of its contributions to be considered for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces an intriguing approach leveraging graph structures for OOD detection, with potential for interpretability. However, critical flaws prevent acceptance at a top-tier conference:  \n1. **Lack of Technical Novelty**: The method primarily combines existing components (pre-trained object detectors, graph embeddings) without a distinct technical innovation. Reviewers noted that contributions are incremental and overlap with prior work.  \n2. **Insufficient Empirical Validation**:  \n   - No direct comparisons to established SOTA OOD detection methods (e.g., Mahalanobis distance-based approaches).  \n   - Limited experiments on a single dataset (LSUN) without validation on standard benchmarks (e.g., CIFAR, ImageNet).  \n   - Absence of ablation studies to isolate the impact of graph structures versus other components.  \n3. **Methodological Ambiguity**: Key details (e.g., object detector architecture, graph construction parameters) are missing, hindering reproducibility. Claims about mimicking \"child cognition\" and \"common sense\" lack rigorous grounding.  \n4. **Writing and Presentation Issues**: Poorly defined notations, grammatical errors, and incomplete literature review (e.g., ignoring foundational OOD detection surveys) undermine clarity and scholarly rigor.  \n\nWhile the concept of semantic graphs for interpretable OOD detection is promising, the paper does not meet the bar for technical novelty, empirical thoroughness, or presentation quality expected at a top-tier venue. Major revisions addressing these issues are required."
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. Let's start by going through the reviews and the paper details.\n\nThe paper proposes an online knowledge distillation method using a specialist ensemble with label prior shift and post-compensation aggregation. The main idea is to diversify the teachers in the ensemble by skewing their label distributions, which should help the student learn better. They also use importance sampling and a new aggregation method. The experiments show improvements in error rate, NLL, and calibration error.\n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 points out that the setup is specialized (online, peer networks) and questions why the method isn't applied to offline distillation. They also mention that the baselines aren't well-explained and suggest more ablations. The novelty is considered limited. Reviewer 2 is concerned about inconsistent results with a previous method (MVCL) and thinks the novelty isn't sufficient. Reviewer 3 is more positive, praising the framework and thorough experiments but notes some typos. Reviewer 4 and 5 both mention missing comparisons with recent works and question the significance of the results. Reviewer 5 even points out a missing reference that performs better.\n\nThe main issues seem to be around novelty and empirical validation. The technical approach combines existing ideas (label shift, PC-Softmax) into a framework, but reviewers feel this isn't sufficiently novel. The experiments, while extensive, might not compare against the latest methods, and some results are questioned (like MVCL's replication). The improvements over baselines are marginal in some cases (e.g., 0.05% on ImageNet), which might not be compelling enough for a top-tier conference.\n\nAnother point is the lack of thorough ablation studies. Reviewers wanted to see the impact of each component (like PC-Softmax, importance sampling vs. other sampling methods) and whether the peer network setup is necessary. Without these, it's hard to assess the actual contribution of each part of the method.\n\nThe paper's strengths are the clear writing, the framework's structure, and the focus on calibration error, which is a notable aspect. However, the weaknesses in novelty and empirical comparisons are significant. Top-tier conferences usually require both strong technical novelty and empirical results that clearly surpass existing methods. If the improvements are minor and the approach is seen as an incremental combination of existing techniques, it might not meet the bar.\n\nConsidering all this, even though the paper has merits, the concerns about novelty, missing comparisons, and marginal improvements might lead to a rejection. The reviewers' scores (mostly 2s and 3s in technical/empirical novelty) suggest that the contributions aren't seen as significant enough. Additionally, the replication issues with MVCL and the lack of response to some methodological questions could undermine the paper's credibility.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper proposes an online knowledge distillation framework leveraging label prior shift and specialist ensembles to enhance teacher diversity. While the idea of inducing diversity through label shifts and aggregating via post-compensation is interesting, the reviews highlight critical shortcomings that diminish its suitability for a top-tier conference:\n\n1. **Limited Novelty:**  \n   Reviewers consistently noted that the core components (label prior shift, PC-Softmax, ensemble diversification) are not novel. The framework combines existing techniques without a clear justification for why this specific integration constitutes a significant advancement. For instance, label prior shift has been explored in other contexts (e.g., domain adaptation), and PC-Softmax is borrowed from prior work. The incremental nature of the contributions fails to meet the high novelty bar expected at top conferences.\n\n2. **Insufficient Empirical Validation:**  \n   - **Marginal Improvements:** The reported gains (e.g., 0.05% improvement over PCL on ImageNet) are minimal and unlikely to justify publication in a highly competitive venue.  \n   - **Missing Comparisons:** Key baselines like CGL, PCL, and L-MCL are omitted or insufficiently addressed, raising doubts about the method’s competitiveness. Reviewer 5 highlights a missing reference (MVCL) that reportedly outperforms the proposed method.  \n   - **Reproducibility Concerns:** Reviewer 2 questions the consistency of MVCL results, suggesting potential flaws in experimental replication.  \n\n3. **Incomplete Ablation Studies:**  \n   Critical ablations—such as isolating the impact of PC-Softmax, comparing importance sampling to alternative sampling strategies, and evaluating the necessity of peer networks—are absent. Without these, the paper cannot convincingly demonstrate the necessity or superiority of its components.\n\n4. **Methodological Ambiguities:**  \n   - The rationale for restricting the method to online distillation (vs. offline) is unclear.  \n   - The aggregation strategy (uniform averaging) ignores potential benefits of weighting teachers based on their specialization, as noted by Reviewer 1.  \n\n5. **Writing and Presentation Issues:**  \n   While the paper is generally clear, Reviewer 3 identifies distracting typos, and Reviewer 1 critiques the lack of motivation for key design choices (e.g., label prior shift’s connection to MoEs).\n\n**Conclusion:**  \nThough the paper addresses an important problem and demonstrates a structured framework, its incremental technical contributions, marginal empirical gains, and unresolved methodological questions render it insufficient for acceptance at a top-tier conference. Addressing these issues would require substantial revisions, including deeper theoretical grounding, broader empirical comparisons, and rigorous ablation studies."
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes Collaborative Adversarial Training (CAT), which uses two models trained with different adversarial methods, exchanging adversarial examples and logits to improve robustness. The reviewers have mixed opinions. \n\nReviewer 1 acknowledges the promising results but points out computational cost, lack of theory, and missing comparisons with ensemble methods. They also mention typos and the need to cite prior work like ensemble adversarial training. Reviewer 2 and 3 are harsher, criticizing the novelty, lack of theoretical analysis, and insufficient experiments (like not using larger datasets or more architectures). Reviewer 4 is more positive, praising the intuition and results but also concerned about computational costs and the need for ablations.\n\nThe main strengths are the improved robustness on CIFAR benchmarks and the intuitive approach. However, weaknesses include high computational cost, missing comparisons with key baselines (like ensemble methods), lack of theoretical grounding, and limited empirical evaluation (only small datasets, no ImageNet). The reviewers also note that the idea of combining methods isn't entirely novel, as prior work on ensembles exists.\n\nThe paper's technical novelty is questioned because similar ideas have been explored. The empirical evaluation, while showing SOTA results on CIFAR-10, is seen as incomplete without testing on larger datasets or more network architectures. The missing comparison with training a single model using multiple attacks is a significant oversight. Also, the computational efficiency concern is valid since training two models doubles the cost, which is a practical drawback.\n\nThe authors need to address why their method is better than existing ensemble approaches and provide more thorough experiments. The lack of theoretical analysis makes it hard to understand why CAT works. Without these elements, the contribution might not be sufficient for a top-tier conference. While the results are good, the paper's current state has notable gaps that lower its impact and novelty.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes an intuitive collaborative adversarial training (CAT) framework that leverages complementary robustness from dual models. While the approach achieves improved robustness on CIFAR benchmarks, significant issues prevent acceptance at a top-tier conference:  \n\n1. **Limited Novelty:** Multiple reviewers note that the core idea of combining adversarial training methods resembles prior ensemble-based defenses (e.g., ensemble adversarial training, diversity-promoting methods). The authors fail to adequately differentiate CAT from these works or cite key related literature, weakening the novelty claim.  \n\n2. **Insufficient Empirical Validation:**  \n   - Experiments are restricted to small-scale datasets (CIFAR-10/100), with no validation on larger benchmarks like ImageNet, raising doubts about scalability.  \n   - Critical baselines are missing (e.g., training a single model with multiple attacks/objectives, ensemble methods like [1,2], or integrating CAT with AWP).  \n   - Ablation studies (e.g., using two identical methods like TRADES-TRADES) and analysis of prediction discrepancies between methods are absent, undermining motivation.  \n\n3. **Lack of Theoretical Analysis:** The paper provides no theoretical justification for why logit alignment between models improves robustness, leaving the mechanism unclear.  \n\n4. **Practical Concerns:** Doubling computational costs (training two models) is a significant drawback, yet efficiency trade-offs are not rigorously analyzed.  \n\n5. **Presentation Issues:** Typos and missing definitions (e.g., ALP) reduce clarity, and comparisons to recent SOTA methods (e.g., AWP) are incomplete.  \n\nWhile the results on CIFAR-10 are promising, the unresolved questions, lack of theoretical grounding, and incomplete empirical evaluation make the contribution incremental rather than transformative for a top-tier venue. Addressing these gaps would require substantial revisions beyond the current scope."
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question examines the order of Batch Normalization (BN) and activation functions, specifically focusing on bounded activations like Tanh. The main finding is that placing BN after the activation (Swap model) leads to better performance compared to the conventional order. The authors attribute this improvement to asymmetric saturation and increased sparsity in the activations.\n\nLooking at the reviews, there's a mix of opinions. Reviewer 2 and 3 find the topic interesting but have concerns about the analysis and practical significance. Reviewer 1 wants more data on skewness in different layers. Reviewer 4 questions the novelty and suggests more experiments. Reviewer 5 has several technical concerns about the claims and experimental setup.\n\nThe strengths of the paper include a novel observation about BN placement with bounded activations and the exploration of saturation and sparsity. However, the weaknesses are significant. The reviewers point out that the analysis isn't comprehensive enough. For example, the saturation metrics might not fully capture the phenomena, and the connection between asymmetric saturation and generalization isn't convincingly established. There's also a lack of practical impact since the Swap model with Tanh doesn't outperform ReLU-based models, which are standard. \n\nThe rebuttal attempts to address some issues, like adding figures for VGG16_11's skewness, but other concerns remain. The technical novelty is questioned, as prior work exists on BN placement. The empirical results, while interesting, don't surpass existing methods, limiting their significance. Additionally, the paper's focus on architectures without residual connections reduces its applicability to modern networks.\n\nConsidering the reviews, the paper presents an intriguing phenomenon but falls short in providing a robust theoretical foundation and practical applicability. The arguments linking asymmetric saturation to improved performance are not fully substantiated, and the experiments are limited in scope. For a top-tier conference, the paper needs stronger evidence, broader analysis, and clearer practical implications. Therefore, despite the interesting findings, the paper doesn't meet the high bar required for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper presents an intriguing empirical observation that swapping the order of Batch Normalization (BN) and bounded activation functions (e.g., Tanh) improves performance, attributed to asymmetric saturation and sparsity. However, significant weaknesses undermine its suitability for a top-tier conference:  \n1. **Weak Theoretical Foundation**: The causal link between asymmetric saturation, sparsity, and generalization remains unconvincing. Reviewers highlighted inconsistencies in saturation/sparsity metrics, incomplete layer-wise analysis (e.g., shallow vs. deep layers), and insufficient justification for why saturation—typically considered harmful—improves performance.  \n2. **Limited Practical Impact**: The Swap model with bounded activations underperforms conventional ReLU-based models (Table 1), reducing its practical relevance. The analysis is restricted to non-residual architectures (e.g., VGG), excluding modern networks like ResNet, which limits applicability.  \n3. **Methodological Gaps**: Key claims lack rigorous validation. For example, the effect of weight decay on sparsity (Reviewer 5), the role of initialization in asymmetric saturation (Reviewer 4), and the exclusion of sigmoid (despite potential adjustments) are unresolved.  \n4. **Presentation Issues**: Terms like \"center of the function\" and \"non-linearity in everyplace\" are ambiguously defined, and acronyms (e.g., NWDBN) are poorly introduced. The rebuttal partially addresses Reviewer 1’s request for additional figures but leaves other concerns unaddressed.  \n5. **Marginal Novelty**: While the Swap-BN observation is novel for bounded activations, prior work has explored BN-activation ordering (e.g., with ReLU). The paper’s empirical findings, though interesting, do not significantly advance the field’s theoretical understanding or practical toolkit.  \n\nFor a top-tier venue, the paper requires stronger theoretical grounding, broader empirical validation (e.g., residual architectures, deeper layer analysis), and clearer practical implications. As it stands, the contributions are incremental and inadequately supported."
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes a new contrastive learning objective to address text degeneration in language models. The main idea is to adjust the loss function to not only promote correct tokens but also suppress negative ones, specifically those that lead to repetition. The reviewers have a mix of opinions, but there are some common concerns.\n\nFirst, the novelty is questioned by multiple reviewers. They mention that the approach is too similar to unlikelihood (UL) training. Reviewer 1 points out that the authors misrepresented UL's formulation, which already includes both positive and negative terms. If the core idea is just a tweak on UL, then the novelty might indeed be thin. However, the authors argue that their method handles negative and irrelevant tokens differently. But if UL already considers both, the differentiation might not be as significant as claimed.\n\nNext, the experimental setup is a big issue. Reviewer 2 and 4 note that the experiments are on GPT-2 small, which is considered outdated. Larger models might not exhibit the same issues with repetition, so the results might not generalize. Also, the metrics used (like perplexity and dist-1) are criticized. Perplexity isn't a reliable measure of generation quality, and using dist-1 instead of higher n-grams (like dist-2 or 3) might not capture true diversity. The human evaluation results also lack statistical significance, which weakens the claims of improved fluency.\n\nAnother point is the lack of comparison with other tasks. The authors mention that their method could benefit tasks like summarization or translation, but they didn't test it there. Including those could have strengthened the paper's impact. Reviewer 1 specifically calls this out as a missed opportunity.\n\nReproducibility seems okay since the code is provided, but the technical contribution's significance is in question. If the method is a simple modification of existing approaches without substantial empirical gains, it might not meet the bar for a top-tier conference.\n\nBalancing the strengths and weaknesses: The idea is intuitive and the paper is well-written, which are positives. However, the incremental nature of the work and insufficient experimentation are major drawbacks. The reviewers' concerns about novelty and empirical validation are significant. For a top-tier conference, the paper needs to demonstrate clear technical novelty and strong empirical results across varied setups, which this paper seems to lack based on the reviews.\n\nFinal Decision: Reject\n</think>\n\nFinal Decision: Reject  \n\n**Reasoning:**  \nThe paper proposes a contrastive token (CT) objective to mitigate text degeneration in language models by balancing positive and negative token learning. While the idea is intuitive and the paper is well-written, several critical flaws highlighted by reviewers make it unsuitable for acceptance at a top-tier conference:  \n\n1. **Thin Novelty:**  \n   - The method is perceived as incremental over unlikelihood (UL) training. Reviewers (1, 3, 4) argue that the core idea of penalizing negative tokens (e.g., repetitions) overlaps significantly with UL, and the authors misrepresent UL’s formulation by omitting its explicit consideration of both positive and negative tokens. The distinction between \"negative\" and \"irrelevant\" tokens in CT is not sufficiently novel to justify a major contribution.  \n\n2. **Insufficient Experiments:**  \n   - **Model Scale:** Experiments are limited to GPT-2 small, which is outdated compared to modern large LMs. Larger models (e.g., GPT-2/3 medium/large) might inherently reduce repetition, making the proposed method’s utility unclear (Reviewers 2, 4).  \n   - **Metrics:** Reliance on perplexity (criticized as unreliable for text quality) and dist-1 (less informative than dist-2/3/4) weakens claims of improved diversity. Human evaluations also lack statistical significance (Reviewers 2, 4).  \n   - **Task Scope:** The authors mention broader applications (e.g., summarization, translation) but provide no experiments, missing a chance to demonstrate generalizability (Reviewer 1).  \n\n3. **Technical Misrepresentation:**  \n   - Reviewer 1 highlights a critical issue: the paper incorrectly claims UL ignores positive tokens, whereas UL explicitly combines likelihood and unlikelihood terms. This undermines the credibility of the technical comparison.  \n\n4. **Marginal Empirical Gains:**  \n   - While results show improvements over baselines, the gains are not transformative. Human evaluations do not strongly favor CT over UL, and the lack of statistical significance further weakens the case (Reviewer 4).  \n\n**Conclusion:**  \nDespite the method’s simplicity and potential, the paper fails to meet the high bar for novelty, technical rigor, and empirical validation expected at a top-tier conference. The incremental contribution, limited experiments, and misrepresentation of prior work justify rejection."
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper is about regression with label differential privacy. The authors propose a mechanism using randomized response on bins, claiming it's optimal under a given regression loss. They also provide theoretical proofs and empirical evaluations.\n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewer 1 initially had a misunderstanding but the authors clarified, leading to a score increase. Reviewer 2 also increased their score after the authors addressed their concerns. Reviewer 3 points out that the paper is clear and has practical use but questions the comparison with prior work like Ghazi et al. 2021. Reviewer 4 praises the novelty and optimality proof but raises a weakness about the interaction between the two terms in Equation (1). Reviewer 5 acknowledges the generalization of Ghazi's work but thinks the technical novelty is marginal. Reviewer 6 finds the problem interesting but notes it's an extension of existing methods for classification to regression.\n\nThe main strengths highlighted are clarity, theoretical contributions (optimality proof), and empirical validation. Weaknesses include comparisons with existing label-DP methods, potential limitations in considering the full error term (Equation 1), and questions about technical novelty.\n\nThe technical novelty seems to be a point of contention. Some reviewers see it as a significant extension (Reviewer 4), while others view it as incremental over Ghazi's work (Reviewer 5). The empirical results are solid but could be more comprehensive by including more comparisons, as noted by Reviewer 3.\n\nThe authors addressed some concerns in their responses, like adding experiments to the appendix. However, there's still a need for discussion on why certain comparisons are missing and deeper analysis of the error terms. The paper's contribution to the field is present but might be seen as an incremental step rather than groundbreaking, depending on the perspective.\n\nConsidering top-tier conference standards, the paper needs to have both technical soundness and significant novelty. The theoretical optimality proof and extension to regression are plus points. However, the mixed reviews on novelty and the need for more thorough comparisons might be a drawback. If the authors adequately addressed the reviewers' concerns in their revisions, especially around the discussion of prior work and error analysis, the paper could be acceptable. But given the split in reviewer opinions and the remaining questions, it's a borderline case. However, since the majority of reviewers lean towards positive scores (Reviewers 2, 3, 4, 6 are somewhat positive, while 1 and 5 have mixed), and the technical contributions are solid, I'd lean towards acceptance with the expectation that the authors address the weaknesses in the final version.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper presents a theoretically grounded approach to label differential privacy (DP) for regression tasks, introducing an optimal mechanism via \"randomized response on bins.\" While reviewers raised valid concerns about novelty (e.g., comparisons to Ghazi et al. 2021) and technical scope (e.g., interaction between error terms in Equation (1)), the strengths outweigh the weaknesses:  \n1. **Theoretical Contribution:** The optimality proof under a general regression loss framework is a clear technical advance over prior work focused on classification.  \n2. Empirical Validation: Experiments demonstrate efficacy across datasets, though broader comparisons with label-DP baselines would strengthen the work.  \n3. Clarity and Impact:** The paper is well-written and addresses a practical problem in DP, with potential applications in privacy-sensitive regression tasks.  \n\nWhile some reviewers noted incrementalism, the extension to regression and the mechanism’s optimality under general loss functions represent meaningful progress. Reviewer concerns (e.g., error term analysis, bin count limitations) are addressable in revisions, and the authors have already engaged constructively with feedback (e.g., adding experiments to the appendix). For a top-tier conference, this work meets the bar for acceptance, provided minor revisions are incorporated."
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. So, the paper is about combining Transformers and Monte-Carlo Tree Search (MCTS) for offline reinforcement learning, specifically applied to SameGame. The reviewers have a mix of opinions, but mostly leaning towards rejection. Let me go through their points step by step.\n\nFirst, Reviewer 1 thinks the idea is promising but the execution is lacking. The main issues are unclear writing, limited evaluation (only one domain, SameGame), and unfair comparisons. They mention that the experiments don't compare with Decision Transformers or Online DTs, which are relevant. Also, the MCTS baseline might not use a learned policy, making comparisons unfair. The technical correctness is rated 2, which is a problem.\n\nReviewer 2 is harsher, saying the novelty is very low because it's similar to AlphaGo/AlphaZero but with a Transformer instead of ResNet. They also point out that the experiments are unfair (comparing different simulation counts) and that comparing to a random player isn't meaningful. The technical and empirical novelty scores are both 1 here, which is a red flag.\n\nReviewer 3 also criticizes the limited novelty, as it's just combining existing methods. They suggest more comparisons with prior work on SameGame and other methods like AlphaZero. The experiments are not thorough enough, and there are presentation issues like typos. However, they acknowledge the reproducibility.\n\nSo, the common themes are: lack of novelty, insufficient experiments (single domain, missing baselines), unclear writing/presentation, and questionable experimental fairness. The idea of combining Transformers and MCTS is interesting, but the execution seems lacking. \n\nThe paper's main claim is that their approach outperforms both Transformer-based and MCTS-based solutions in SameGame. But if the MCTS baseline isn't using a learned policy like AlphaZero does, then the comparison isn't fair. Also, using only SameGame, which is a toy problem, makes it hard to generalize the results. Top-tier conferences usually expect evaluations on multiple benchmarks, especially more complex ones like Atari or Minigo. \n\nThe authors didn't compare against key baselines like Decision Transformers or AlphaZero variants, which is a big issue. Without these comparisons, it's unclear if their method actually improves upon existing approaches. Reviewer 2's point about the structure being too similar to AlphaZero but with a different neural network architecture (Transformer vs. ResNet) suggests that the technical novelty is minimal. \n\nThe writing and clarity issues are another concern. If the method isn't explained clearly, it's hard for readers to understand the contributions. Figures with small fonts and unclear captions add to the problem. \n\nReproducibility is mentioned as possible, but without code, it's still a hurdle. However, since the method is similar to existing ones, maybe it's easier to reproduce, but that's not a strong point.\n\nIn terms of significance, combining Transformers and MCTS could be useful, but the paper doesn't convincingly show why this combination is better than prior approaches. The empirical results are limited and don't address the broader applicability. \n\nGiven all these points, even though the idea has potential, the current version has too many flaws in execution, evaluation, and presentation. The reviewers' criticisms about novelty and empirical validation are significant. For a top-tier conference, the paper needs more thorough experiments, clearer writing, and fair comparisons. As it stands, it's not ready for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper proposes an intriguing integration of Transformers and MCTS for offline RL, but critical flaws in novelty, empirical validation, and clarity undermine its suitability for a top-tier conference. While the core idea—combining Transformers with MCTS—has potential, the execution falls short in several areas:  \n\n1. **Limited Novelty**: The method resembles AlphaGo/AlphaZero’s architecture (learned policy/value network guiding MCTS) but substitutes ResNet with Transformers. This substitution alone is insufficient to claim technical novelty without deeper algorithmic or theoretical innovation. Reviewers unanimously criticized the lack of originality, with two reviewers scoring technical novelty as marginal (2/3) or minimal (1/3).  \n\n2. **Weak Empirical Evaluation**:  \n   - **Single Domain**: Experiments are confined to SameGame, a \"toy-ish\" domain, without validation on standard benchmarks (e.g., Atari, MiniGo).  \n   - **Missing Baselines**: No comparisons to Decision Transformers, Online DTs, or AlphaZero-like MCTS with learned networks, making claims of superiority unsupported.  \n   - **Unfair Comparisons**: MCTS baselines may lack learned priors (unlike AGZ/MuZero), and simulation counts/time limits are not standardized, raising fairness concerns.  \n\n3. **Clarity and Presentation**: The paper suffers from unclear exposition, underspecified figures (e.g., Figure 1’s unexplained components), and typographical errors. These issues hinder reproducibility and reader comprehension.  \n\n4. **Marginal Significance**: While the idea of using Transformers for MCTS rollouts is interesting, the paper fails to demonstrate why this combination is uniquely effective. Critical ablations (e.g., Transformer vs. other networks in MCTS) and domain-general results are absent.  \n\n**Conclusion**: Though the concept has promise, the paper’s lack of rigorous evaluation, insufficient novelty, and presentation flaws make it unsuitable for acceptance in its current form. Addressing these issues—e.g., expanding experiments, clarifying methodology, and strengthening baselines—could elevate the work to a publishable standard."
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviews. The paper's main contribution is showing that swapping the order of batch normalization (BN) and activation functions (specifically bounded ones like Tanh) leads to better performance. They argue that this is due to asymmetric saturation and increased sparsity. \n\nThe reviewers have a mix of opinions. Strengths mentioned include the interesting observation about the order swap and the analysis of saturation and sparsity. However, there are several weaknesses. A common concern is that the practical significance is limited because the swapped models with bounded activations don't outperform ReLU-based models. Also, the technical explanations and justifications for why asymmetric saturation helps are not fully convincing. Some reviewers question the metrics used, the layer-wise variations in saturation, and the lack of analysis on residual connections.\n\nReviewer 1 wants more figures to support the skewness argument. Reviewer 2 and others are confused about the link between saturation, sparsity, and performance. They also mention that the analysis is limited to non-residual architectures, which are less common now. Reviewer 3 points out that the swapped models still underperform compared to ReLU, which might reduce the paper's impact. Reviewer 4 questions the novelty and asks for more experiments, like varying hyperparameters. Reviewer 5 has concerns about the rigor of claims and experimental setup.\n\nThe authors' rebuttal might address some points, but the reviews suggest that even after revisions, there are unresolved issues. The technical novelty is considered moderate, and empirical significance is debated. The paper's findings are interesting but may not be sufficiently supported or impactful enough for a top-tier conference. The weaknesses in the analysis and the practical applicability (since ReLU still performs better) might lead to a reject decision despite the interesting observation.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper investigates the impact of swapping the order of batch normalization (BN) and bounded activation functions (e.g., Tanh), demonstrating improved performance in this configuration. While the core observation—asymmetric saturation and sparsity leading to better generalization—is intriguing, the paper faces significant criticisms that undermine its suitability for a top-tier conference:\n\n1. **Weak Empirical Significance**:  \n   A recurring critique is that the swapped models with bounded activations (e.g., Tanh) still underperform compared to conventional ReLU-based models (Table 1). This raises questions about the practical utility of the findings. While the authors highlight relative improvements between swapped and conventional orders for bounded activations, the lack of superiority over ReLU diminishes the broader impact of the work.\n\n2. **Unconvincing Causal Links**:  \n   The connection between asymmetric saturation, sparsity, and generalization is not rigorously established. Reviewers note contradictions (e.g., high saturation in shallow layers vs. low saturation in deeper layers) and insufficient evidence to rule out alternative explanations (e.g., training dynamics, layer-wise effects). The metrics for saturation and sparsity are also critiqued as incomplete or ambiguous.\n\n3. **Limited Scope and Applicability**:  \n   The analysis excludes architectures with residual connections, which dominate modern deep learning. The authors’ claim that Tanh cannot follow residual design principles (due to non-linearity at the origin) is inadequately justified, and experiments are restricted to VGG-like networks. This limits the generality of the conclusions.\n\n4. **Technical and Presentation Issues**:  \n   Key terms (e.g., \"center of the function\") are poorly defined, and acronyms (e.g., NWDBN) are introduced without explanation. The rebuttal responses do not fully resolve concerns about layer-wise saturation patterns or the exclusion of sigmoid activations. Additionally, the code lacks clarity in implementing critical metrics (saturation, skewness).\n\n5. **Mixed Novelty Assessment**:  \n   While the swapped BN-activation order is novel in the context of bounded functions, reviewers argue that prior work has explored BN-activation ordering (e.g., with ReLU). The empirical findings, though interesting, are seen as incremental rather than transformative.\n\n**Final Decision**: Reject  \nWhile the paper identifies a novel interaction between BN and bounded activations, the unresolved methodological flaws, limited practical relevance, and insufficiently supported claims prevent it from meeting the high bar of a top-tier conference. The work would benefit from deeper theoretical analysis, broader empirical validation (including residual architectures and direct comparisons to ReLU), and clearer causal explanations."
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the reviews and the paper details. The paper proposes a method to maintain diverse playing styles in self-play RL using a bi-objective optimization model. The main idea is to balance skill level and style diversity, which they test on Pong and Justice Online. \n\nReviewer 1 has some major concerns. They mention missing key literature like Lanctot et al.'s work, which covers self-play algorithms like Fictitious Play. Not comparing to these could be a problem. Also, the experimental setup in Table 3 might unfairly favor their method since EMOGI and BiO have more agents in the pool. If opponents are sampled from this pool, those methods might face agents they trained with, giving them an advantage. Plus, PBT is still competitive despite this, which questions BiO's real benefit.\n\nReviewer 2 points out that reducing playing style to a single scalar is a big simplification. They also mention some unclear parts in the methodology, like the variance calculation and reward weights. The experiments need more details on confidence intervals and whether the agents shown were cherry-picked. However, they find the paper mostly clear and the approach promising.\n\nReviewer 3 notes the lack of comparison to quality-diversity algorithms like MAP-Elites, which are relevant. They also mention the experiments are limited to two domains, with Pong being simple. The different environment testing (starting positions) might not be compared to domain randomization, a common baseline. The scalability to higher-dimensional styles is questioned, and there's a missing matchup table for opponents.\n\nAll reviewers agree the idea has merit but has significant gaps in literature coverage and experimental rigor. The technical novelty is considered marginal by two reviewers, and empirical results are seen as not sufficiently validated across domains or against key baselines. The missing comparison to quality-diversity methods and potential flaws in the experimental setup (like opponent sampling bias) are critical issues.\n\nThe paper's strengths are the clear writing and promising results in the tested scenarios. However, the weaknesses in literature review and experimental design are substantial. For a top-tier conference, the paper needs to address these gaps to demonstrate both novelty and robustness. Without comparisons to established baselines like Fictitious Play or MAP-Elites, and with potential experimental flaws, the contributions might not be significant enough. The reviewers' concerns about correctness and novelty, especially the technical and empirical aspects being rated low, suggest that the paper in its current form isn't ready for acceptance. It needs more thorough comparisons and validation to meet the high standards.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper introduces a bi-objective optimization approach to maintain diverse playing styles in self-play reinforcement learning, with promising empirical results in two game environments. However, significant concerns raised by reviewers undermine its suitability for a top-tier conference:  \n\n1. **Literature Gaps and Baselines:**  \n   - The omission of key works like Lanctot et al. (2017) on self-play frameworks and quality-diversity algorithms (e.g., MAP-Elites) weakens the paper’s positioning. Reviewer 1 and 3 highlight that established baselines (e.g., Fictitious Play, AlphaStar, domain randomization) are missing, making it unclear whether the proposed method offers meaningful improvements over existing approaches.  \n\n2. **Experimental Design Issues:**  \n   - The opponent sampling strategy in Table 3 disproportionately favors the authors’ method (BiO) and EMOGI, as they dominate the agent pool. This introduces bias, yet PBT remains competitive, raising doubts about BiO’s novelty.  \n   - The evaluation lacks statistical rigor (e.g., confidence intervals, agent cherry-picking in visualizations) and scalability analysis for high-dimensional style spaces.  \n\n3. **Technical and Empirical Novelty:**  \n   - While the bi-objective formulation is straightforward, reviewers argue that combining multi-objective optimization with RL is not novel (e.g., quality-diversity literature). The scalar representation of \"playing style\" is overly simplistic and under-discussed.  \n\n4. **Reproducibility and Scope:**  \n   - Though the appendix provides implementation details, the limited domain evaluation (only Pong and Justice Online) and absence of code reduce confidence in broader applicability.  \n\nWhile the paper addresses an important problem and shows potential, the unresolved methodological flaws, insufficient comparisons, and marginal technical novelty do not meet the high bar of a top-tier conference. Addressing these issues would require significant revisions, including expanded baselines, rigorous statistical analysis, and broader empirical validation."
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what this paper is about. The title is \"Proximal Validation Protocol,\" and the abstract talks about addressing the trade-off between having a good validation set and using as much data as possible for training. The main idea is to create a validation set through data augmentation and a distribution-consistent sampling method instead of splitting the training data. The authors claim this approach avoids wasting data and improves model evaluation and performance.\n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 1 points out that the experiments might not properly show model selection via hyperparameter tuning, which is a key use of validation sets. They also mention clarity issues in the results and the need for more thorough evaluations, like varying data augmentation methods. Reviewer 2 is concerned about the datasets being too small and the lack of theoretical backing. Reviewer 3 likes the problem's practicality but wants more experiments, especially on real-world scenarios and different models. Reviewer 4 criticizes the lack of theoretical justification and questions if the validation set truly represents the data distribution.\n\nThe strengths of the paper include addressing an important problem (validation set trade-off) with a novel approach using data augmentation and sampling. The method seems applicable across different data types (images, text, tabular), which is a plus. However, the weaknesses are significant. The experiments don't convincingly demonstrate that PVP leads to better model selection, which is crucial. The datasets used are small, and there's no theoretical analysis to support why the proximal set is a valid substitute. Additionally, the clarity of the results presentation is lacking, making it hard to assess the claims.\n\nReviewer 3's borderline recommendation suggests that with improvements, the paper could be acceptable, but the other reviewers are more critical. The lack of theoretical foundation and insufficient empirical evidence on critical aspects like model selection and real-world applicability are major red flags. Top-tier conferences usually require both solid theoretical contributions and strong empirical validation. This paper's empirical evaluations are incomplete and the methodology's validity isn't sufficiently proven. Therefore, despite the interesting idea, the current version doesn't meet the standards for acceptance.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**  \nThe paper introduces a novel validation protocol (PVP) aimed at resolving the train/validation split trade-off by constructing a proximal validation set via data augmentation and distribution-consistent sampling. While the problem is timely and the idea is innovative, the paper falls short of the standards for a top-tier conference due to critical weaknesses highlighted by reviewers:  \n\n1. **Insufficient Empirical Validation**:  \n   - The experiments fail to demonstrate that PVP improves *model selection* (e.g., hyperparameter tuning), which is the primary purpose of validation sets. Results focus on fixed models, limiting the practical relevance of the claims.  \n   - Evaluations rely on small or non-standard datasets (e.g., CIFAR-LT instead of ImageNet, DistilBERT instead of BERT), raising concerns about scalability and generalizability.  \n   - Key ablations (e.g., sensitivity to augmentation strategies, sampling hyperparameters) are missing, weakening the robustness of the conclusions.  \n\n2. **Lack of Theoretical Foundation**:  \n   - The paper provides no theoretical justification for why the proximal validation set approximates the true data distribution or why it should generalize better than traditional splits. Reviewer 4 correctly notes that label corruption could artificially reduce the test-validation gap without meaningful guarantees.  \n\n3. **Clarity and Presentation Issues**:  \n   - Metrics like \"F1 deterioration\" and \"T-V gap\" are ambiguously defined, and results in Tables 3–4 are poorly explained. This undermines reproducibility and trust in the findings.  \n   - The writing contains informal language (e.g., \"plague,\" \"combo\") and unclear technical phrasing, reducing professionalism.  \n\n4. **Limited Scope**:  \n   - The method is evaluated only on classification tasks, with no discussion of regression, self-supervised learning, or other modalities. Reviewer 3 highlights the need for real-world industrial validation, which is absent.  \n\nWhile the problem is significant and the core idea has potential, the paper’s empirical and theoretical gaps, combined with unclear presentation, prevent it from meeting the bar for acceptance. Addressing these issues would require substantial revisions, including rigorous model selection experiments, theoretical analysis, and validation on larger-scale, real-world benchmarks."
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about Functional Risk Minimization (FRM), which challenges the traditional assumption that data comes from a single function with additive noise. Instead, they model each data point as coming from its own function. The abstract says FRM subsumes ERM and shows better performance in small experiments. The reviewers have mixed opinions. \n\nReviewer 1 points out several issues. The main weaknesses are the lack of clarity in connecting the algorithm to theory, scalability concerns, and no formal arguments for why FRM outperforms ERM. They also mention presentation problems, like confusing terminology. The technical novelty is high, but correctness is rated low. \n\nReviewer 2 is confused about the relationship with hierarchical Bayesian models and the inconsistency between training and test assumptions. They also note the lack of experiments on the generative model part. The technical and empirical scores are moderate.\n\nReviewer 3 is more positive, praising the novelty and writing quality, but mentions computational costs and suggests discussing why FRM is better than modifying ERM. They recommend acceptance but it's a marginal call.\n\nSo, the key issues are clarity, theoretical grounding, scalability, and addressing the Bayesian comparison. The paper's idea is novel, which is a strength, but the presentation and technical details are lacking. Top-tier conferences usually require both novelty and solid technical validation. The reviewers' concerns about the algorithm's derivation, scalability, and theoretical guarantees are significant. Without these, even a novel idea might not be sufficient. Also, the experiments are small-scale, which Reviewer 1 and 2 point out as a problem. \n\nThe authors need to better explain how FRM differs from existing methods like Bayesian approaches and provide formal proofs or arguments for their claims. The confusion around ERM terminology and the algorithm's connection to the theory needs clarification. Since two reviewers have major concerns and only one leans towards acceptance, the paper might not meet the high bar for acceptance in its current form. The lack of theoretical support and unclear presentation are critical issues that would need revision. Therefore, despite the interesting idea, the paper should probably be rejected.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper introduces a novel framework (FRM) that challenges traditional assumptions in supervised learning by modeling each data point with its own function. While the concept is innovative and has high potential significance, several critical issues prevent acceptance at a top-tier conference:\n\n1. **Lack of Theoretical Rigor:**  \n   - Reviewer 1 highlights the absence of formal arguments connecting FRM’s objectives to performance guarantees (e.g., low loss, finite-sample behavior). Without proofs or theoretical analysis, the framework’s validity remains unsubstantiated.  \n   - The connection between the proposed algorithm (Equations 7–8) and the theoretical motivation is unclear, raising doubts about correctness and reproducibility.\n\n2. **Scalability and Practicality:**  \n   - The algorithm relies on approximations (e.g., Laplace approximation) and Hessian computations, which are computationally expensive for large models. The experiments are limited to small-scale tasks (regression, RL), failing to demonstrate scalability to real-world problems. Reviewer 1 and 3 both question the practicality of the method.\n\n3. **Clarity and Terminology:**  \n   - The paper conflates ERM with additive noise models, misaligning with established definitions (Vapnik, 1991). Reviewer 1 notes that this creates confusion, weakening the narrative.  \n   - The distinction between FRM (the learning objective) and FGM (the generative model) is poorly articulated, further muddying the contribution.\n\n4. **Conceptual Gaps:**  \n   - Reviewer 2 raises critical questions about the inconsistency between training (per-data-point functions) and test-time assumptions (single parameter). This undermines the framework’s coherence and raises concerns about generalization.  \n   - The relationship to hierarchical Bayesian models is underexplored, leaving ambiguity about technical novelty.\n\n5. **Empirical Limitations:**  \n   - Experiments focus on synthetic or small-scale tasks, with no validation on the proposed generative model (FGM). Reviewer 2 emphasizes this as a major weakness.  \n   - While FRM outperforms ERM in non-uniform noise settings, the gains are not rigorously contextualized against alternative approaches (e.g., modified ERM objectives).\n\n**Conclusion:**  \nThe paper’s core idea—rethinking noise through functional variation—is compelling and novel. However, the lack of theoretical grounding, scalability evidence, and clarity in presentation prevent it from meeting the standards of a top-tier conference. Addressing these issues would require significant revisions, including formal proofs, large-scale experiments, and clearer articulation of the framework’s mechanics and novelty. As such, rejection is warranted in its current form."
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about protein sequence and structure co-design using an equivariant translation approach. The main claim is that their method, ProtSeed, outperforms existing methods like autoregressive or diffusion models in terms of both performance and inference speed. \n\nLooking at the reviews, there's a mix of positive and negative points. Reviewer 1 and 3 seem satisfied after the rebuttal, especially with clarifications made. Reviewer 4 raises concerns about novelty, questioning if they're the first in co-design and comparing to Jin's work. Reviewer 5 points out several weaknesses: lack of hyperparameter details, code release, and functional evaluation. They also question the use of AF2 for evaluation. Reviewer 6 is critical about technical novelty, noting similarities to AlphaFold2 and unclear method parts. Reviewer 7 thinks the empirical results are strong but wants more comparisons. Reviewer 8 finds the method clear but notes that \"co-design\" might be misleading since it's conditioned on existing structural info.\n\nThe strengths highlighted by reviewers include strong empirical results, faster inference, and a well-written paper. Weaknesses are around novelty claims, technical details missing, evaluation metrics not reflecting functionality, and reproducibility concerns without code release.\n\nThe authors addressed some issues in the rebuttal, like clarifying claims and adding details, but some reviewers still have concerns. For example, Reviewer 4 is still not convinced about the novelty compared to prior work. The technical novelty is debated, with some reviewers seeing it as incremental. The lack of code and hyperparameters is a red flag for reproducibility, which is important for top-tier conferences. Also, the evaluation relying on metrics like RMSD and AAR without functional tests might not fully validate the method's real-world applicability.\n\nOn balance, the paper presents a solid contribution with improved performance and efficiency. However, the concerns about novelty, especially in the context of existing co-design methods, and the reproducibility issues could be significant. Top-tier conferences usually require both technical novelty and thorough evaluation. If the authors addressed the rebuttal points adequately, maybe acceptance is possible. But given the remaining concerns from multiple reviewers about novelty and the need for more detailed methodology, it's a tough call. The empirical results are strong, which is a big plus. However, the technical criticisms and reproducibility might tip it towards rejection unless the authors sufficiently addressed these in the final version. But since some reviewers upgraded their scores post-rebuttal and the method's efficiency is a clear advantage, maybe acceptance is warranted.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a novel approach to protein sequence-structure co-design, addressing computational inefficiencies in existing methods (autoregressive/diffusion models) through an equivariant translation framework. Key strengths include:  \n1. **Technical Contribution:** The integration of a trigonometry-aware encoder and roto-translation equivariant decoder enables efficient one-shot updates, significantly reducing inference time while maintaining high fidelity.  \n2. **Empirical Performance:** Strong results across multiple benchmarks (e.g., structural antibody design, fixed-backbone tasks) demonstrate superiority over state-of-the-art baselines in metrics like RMSD and AAR.  \n3. **Clarity and Impact:** The method is well-motivated, with clear applications in protein engineering, and the rebuttal addressed several concerns (e.g., clarifying claims, adding algorithm details).  \n\n**Addressing Weaknesses:**  \n- While concerns about novelty (e.g., overlap with AlphaFold2, Jin et al.) and reproducibility (missing hyperparameters/code) were raised, the authors’ rebuttal mitigated these by clarifying technical distinctions and committing to revisions (e.g., adding SeqIPA equations).  \n- Functional evaluation limitations (e.g., reliance on AF2) remain, but the paper’s focus on computational efficiency and structural accuracy aligns with conference standards for methodological advances.  \n\n**Conclusion:**  \nDespite minor flaws, the paper’s technical innovation, empirical rigor, and potential impact on protein design justify acceptance at a top-tier conference. The authors’ responsiveness to reviewer feedback further strengthens the case for acceptance."
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about analyzing the convergence rates of gradient descent for deep linear neural networks. The main claims are that they get sharp convergence rates that don't depend on the type of random initialization or the depth of the network, provided the layers are wide enough. The reviewers have a mix of opinions. \n\nReviewer 1 and 3 are concerned about novelty. They mention that some results, like the O(κ log 1/ε) rate, were already achieved in prior works like Du & Hu (2019) and Hu et al. (2020). The authors extended these to more general loss functions and different initializations, but the reviewers question how technically challenging these extensions were. Reviewer 1 also points out that the comparison regarding the operator P(t) might not be fair, suggesting the novelty here is overstated.\n\nReviewer 2 and 4 recommend acceptance, praising the clarity and the extension to different initializations and loss functions. Reviewer 4 does note that the analysis is local and lacks intuitive understanding but still finds the work solid. Reviewer 5 also has novelty concerns, thinking the extension to general convex losses isn't sufficiently novel. Reviewer 6 recommends rejection due to lack of discussion on overparametrization and other insights, though they think it's fixable.\n\nThe authors responded to some points, adding a new section (Section 4) which Reviewer 1 found helpful but still had reservations. The main issues seem to revolve around whether the technical contributions are novel enough beyond prior work. The trajectory result (Theorem 3.3) is highlighted as novel by Reviewer 3, which could be a key contribution. However, other theorems (B.1, B.2, B.3) are seen as extensions of existing work.\n\nThe paper's strengths are clear writing and generalization of prior results. Weaknesses include potential lack of novelty in some theorems and insufficient discussion on overparametrization's role. The rebuttal needs to address whether the technical extensions required significant new insights. If the authors can convincingly argue that their extensions (like varying layer widths, different initializations) required overcoming non-trivial challenges, that would boost the case for acceptance. However, some reviewers remain unconvinced, and Reviewer 6's concerns about overclaims in the title/abstract and lack of discussion are valid. \n\nThe paper's correctness is generally accepted (most reviewers gave 4/4), so the main issue is significance and novelty. The trajectory result (Theorem 3.3) seems to be a novel contribution that hasn't been shown before, which could tip the balance. However, the majority of the results are seen as incremental. Given that top-tier conferences require both technical soundness and significant novelty, the mixed reviews on novelty might lead to a borderline decision. If the authors addressed the concerns in the rebuttal (like clarifying technical challenges, adjusting claims, adding discussions), acceptance could be justified. But since the reviews are split and some concerns remain unresolved (like overclaiming in the title), the decision might lean towards rejection unless the authors made substantial revisions. However, since two reviewers recommend acceptance and others are on the fence with possible revisions, maybe acceptance is possible if the authors adequately addressed the issues in the response. But given the current state (as per the reviews without seeing the authors' full response beyond the initial rebuttal), the lack of clear consensus and remaining concerns about novelty might lead to a reject. Alternatively, if the trajectory result is sufficiently novel and the paper's contributions are framed correctly, acceptance could be warranted. The key is whether the trajectory analysis and the generalization of initializations/loss functions are seen as meaningful advances. Since some reviewers found the trajectory result novel and the paper well-executed, and others are more critical, the final decision could go either way. However, given that three reviewers (1,3,5,6) have concerns about novelty and significance, while two (2,4) are positive, and one (6) is negative but open to change, the paper might be on the borderline. In such cases, if the trajectory result is a significant addition, and the authors can adjust the title/abstract to be more precise (as per Reviewer 6's point), then acceptance is possible. But without those adjustments, the overclaiming in the title and abstract might be a red flag. The final decision would depend on how well the authors addressed these in the rebuttal. Since the initial reviews indicate that some concerns were partially addressed but others remain, and given the split in recommendations, I think the paper might be accepted with the understanding that the authors will make the necessary revisions, especially clarifying the contributions and toning down certain claims. However, if the committee feels the novelty isn't sufficient, it might be rejected. But based on the trajectory result and generalization across initializations, which some reviewers found novel, I'll lean towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**  \nThe paper presents a rigorous analysis of gradient descent convergence for deep linear neural networks, extending prior work by generalizing results to different initializations, loss functions, and layer widths. While concerns about novelty were raised (e.g., similarities to Du & Hu 2019), the trajectory alignment result (Theorem 3.3) is a distinct and novel contribution, providing insights into how non-convex optimization mirrors convex dynamics. Reviewers acknowledged the paper’s clarity, technical correctness, and significance in understanding overparameterized networks. Key weaknesses (e.g., overclaims in the title, insufficient discussion of overparameterization) were addressable through revisions. Given the paper’s theoretical contributions and alignment with the conference’s standards, acceptance is justified with minor revisions to clarify novelty and scope."
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper in question is about improving adversarial robustness by applying more regularization to less robust samples. The reviewers have a mix of opinions, but there's a lot of back-and-forth, especially regarding the theoretical contributions and the empirical results.\n\nFirst, looking at the theoretical aspects. The main criticism from several reviewers (like Reviewer 5, 9, 16) is that the theoretical contribution, specifically Theorem 1, is not novel enough. They argue that it's an extension of TRADES to multi-class problems but with a loose bound. The authors claim their theorem provides a tighter upper bound, but some reviewers point out that for binary classification, it's essentially the same as TRADES, and for multi-class, the bound is not tight. The definition of z(x) also seems problematic, with confusion about whether it's a constant, a set, or something else. This lack of clarity in the mathematical definitions might undermine the theorem's validity.\n\nThen there's the practical part. The method proposed, ARoW, shows improvements over existing methods like TRADES and HAT in experiments, but the margins are small. Some reviewers note that the improvements are consistent but not substantial. Also, when extra data is used, ARoW doesn't clearly outperform HAT, which is a concern. The datasets used (CIFAR-10, F-MNIST, SVHN) are smaller, and there's a question about scalability to larger datasets. However, the experiments are extensive and include ablation studies, which is a plus.\n\nReviewers 13, 14, and 15 are more positive, highlighting the clear writing, theoretical motivation, and solid experiments. They acknowledge the marginal improvements but still see value in the work. However, the critical reviews (like 16) are very concerned about the theoretical claims being overblown and the empirical gains being too minor.\n\nThe key issues here are: \n1. The theoretical contribution's novelty and correctness. If the theorem is just a trivial extension or based on loose bounds, it might not be sufficient for a top-tier conference.\n2. Empirical results: While there are improvements, they're small, and some baselines might not be optimally compared, especially with extra data. The datasets are standard but not the largest, which could limit the perceived impact.\n\nThe rebuttal attempts to address some concerns, like adding experiments and clarifying the relation to TRADES. However, the fundamental issues about the theorem's depth and the practical significance of the results remain. \n\nIn top-tier conferences, both theoretical and empirical contributions are scrutinized. If the theory is seen as incremental or not robust, and the experiments only show minor gains, the paper might not meet the high bar. However, some reviewers upgraded their scores after the rebuttal, indicating that the authors addressed some concerns. But others remained unconvinced, especially regarding the theoretical aspects.\n\nBalancing all this, the paper might be on the borderline. The mixed reviews suggest that while there's some value, the concerns about the theoretical contribution's significance and the empirical improvements' magnitude could lead to a rejection. However, the fact that some reviewers found the method effective and the experiments thorough might push towards acceptance. But given the depth of the theoretical criticisms and the potential overclaiming, I think the paper might lean towards rejection. The theoretical flaws seem significant enough to question the paper's contribution, and the empirical results aren't strong enough to compensate.\n</think>\n\nFinal Decision: Reject\n\n**Reasoning:**  \nThe paper proposes a novel adversarial training method (ARoW) with theoretical motivation and empirical validation. While the approach shows consistent improvements over baselines, key concerns from reviewers undermine its suitability for a top-tier conference:  \n\n1. **Theoretical Contribution:**  \n   - Theorem 1, the core theoretical result, is critiqued as a trivial extension of TRADES to multi-class settings with loose bounds. Reviewers argue the theorem lacks novelty and rigor, relying on poorly defined terms (e.g., \\(z(x)\\)) and loose approximations (e.g., replacing indicator functions with KL divergence). The theoretical claims are deemed insufficient to justify a \"new theorem\" and fail to meaningfully advance the field.  \n\n2. **Empirical Results:**  \n   - While ARoW achieves state-of-the-art results on small datasets (CIFAR-10, SVHN), improvements over baselines like TRADES and HAT are marginal. Performance gains diminish with extra data, and comparisons to stronger baselines (e.g., AWP-TRADES) are missing. The lack of evaluation on larger datasets (e.g., ImageNet) further limits the perceived impact.  \n\n3. **Methodological Concerns:**  \n   - Discrepancies between theoretical definitions (e.g., \\(z(x)\\) as an adversarial example) and practical implementations (KL divergence) weaken the connection between theory and method. Reviewers highlight this as a critical flaw, suggesting the theoretical motivation is tenuous.  \n\n4. **Mixed Reviewer Consensus:**  \n   - While some reviewers acknowledge the paper’s clarity and incremental contributions, the majority of in-depth critiques (especially regarding theory) raise fundamental doubts. The rebuttal partially addressed empirical concerns but failed to resolve core theoretical objections.  \n\nGiven the high standards of top-tier conferences, the paper’s theoretical weaknesses and marginal empirical gains outweigh its strengths. The work may be better suited for a venue with a focus on incremental algorithmic improvements rather than theoretical breakthroughs."
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper proposes using binary labels as an auxiliary task alongside the standard one-hot labels in a multi-task learning setup. The main idea is that these binary labels, which are randomly generated, can help the model learn better by grouping classes in different ways. The reviewers have a mix of opinions, but there are some common concerns.\n\nFirst, looking at the strengths: The idea of exploring label representations is novel and understudied, which is a plus. The method shows improved performance in some cases, especially with limited data and on more challenging datasets. The approach is simple to implement, which is practical.\n\nNow the weaknesses. Reviewer 1 points out that the experimental section is weak. They didn't compare enough baselines, like SOTA methods, and the improvements are sometimes marginal. Also, there's a lack of analysis on when and why the method works. For example, what dataset characteristics make the binary labels effective? Reviewer 2 mentions issues with notation and clarity, and that the technical contribution is limited. They also note that the paper doesn't adequately discuss related work, like error-correcting output codes, which seems relevant. Reviewer 3 and 4 echo similar concerns about missing related work and insufficient empirical evidence, especially on larger datasets like ImageNet. There's also a reproducibility issue because some implementation details are missing, and the role of Metabalance isn't explained.\n\nThe reviewers also question the motivation behind binary labels. Why do random groupings help? The authors need to provide more intuition or theoretical backing. The lack of analysis on the variance due to different random binary label generations is another gap. Without showing standard deviations or multiple runs, it's hard to know if the results are consistent.\n\nRegarding technical novelty, while the specific use of binary labels as an auxiliary task is new, related concepts like error-correcting codes and unsupervised meta-learning have explored similar ideas. The authors need to better position their work within existing literature.\n\nThe empirical results are mixed. Some datasets show improvement, but without comparison to SOTA methods, it's unclear if the gains are meaningful. The data efficiency claim is interesting, but as Reviewer 4 points out, data efficiency might not be the best metric here. Also, missing experiments on large-scale benchmarks like ImageNet weaken the generalizability claim.\n\nClarity issues are a problem. Terms like \"dense labels\" are introduced without explanation, and the method's details (like how Metabalance is used) are unclear. The equations have formatting issues, and there are typos. These make the paper hard to follow and reproduce.\n\nIn terms of significance, the idea has potential but the current presentation and evaluation don't fully support a top-tier conference acceptance. The improvements are modest, and the analysis is lacking. The reviewers' concerns about missing baselines, insufficient analysis, and clarity are major red flags.\n\nFinal decision hinges on whether the contributions are significant enough for a top-tier venue. Given the mixed reviews, especially the concerns about technical novelty, empirical strength, and clarity, the paper might not meet the high bar. The authors need to address the weaknesses through more thorough experiments, better analysis, and clearer writing. As it stands, the paper is promising but not yet ready for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces an intriguing concept of using binary label representations as an auxiliary task in multi-task learning to improve classification performance. While the idea is novel and shows promise in specific scenarios (e.g., data efficiency, challenging datasets), significant weaknesses prevent it from meeting the standards of a top-tier conference:  \n1. **Insufficient Empirical Validation**: The experiments lack comparisons to state-of-the-art baselines (e.g., error-correcting output codes, advanced multi-task frameworks) and fail to demonstrate consistent, large-scale improvements. Key datasets like ImageNet are omitted, raising questions about generalizability.  \n2. **Limited Analysis and Clarity**: The paper does not adequately explain why binary labels improve performance, how variance in random label generation affects results, or how task weighting is optimized. Critical implementation details (e.g., Metabalance, hyperparameters) are unclear, hindering reproducibility.  \n3. **Weak Positioning in Related Work**: Prior work on label representations (e.g., ordinal regression, meta-learning with random labels) is under-discussed, weakening the novelty claim.  \n4. **Marginal Technical Contribution**: The core idea—adding a simple auxiliary task with random binary labels—lacks sufficient technical depth or theoretical grounding to justify its significance compared to existing methods.  \n\nWhile the concept has potential, the current presentation and evaluation do not convincingly demonstrate a substantial advance over prior work. Addressing these issues would require significant revisions, including expanded experiments, deeper analysis, and clearer connections to the literature."
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's dive into this. The paper in question tackles online adaptation of machine learning models when both label shift and conditional shift occur. The reviewers have a mix of opinions, but there's a clear trend towards rejection. Let me break down the key points.\n\nFirst, the strengths. The problem is well-motivated. Real-world deployments often face multiple distribution shifts, so combining label and conditional shifts is relevant. The paper's honesty about inconclusive results is a plus, and the empirical setup seems reproducible. However, the weaknesses are significant. \n\nReviewer 1 points out the lack of a systematic approach to distribution shifts. The paper frames the problem as label shift with conditional shift, but in reality, this is a general shift scenario. Without a structured framework, the experiments feel scattered. The empirical study is modest, with limited datasets and baselines. For a top-tier conference, the experiments need to be more comprehensive. The paper also doesn't adequately address prior work on regression label shift or confusion matrix inversion, which weakens its technical foundation.\n\nReviewer 2 and 3 note that the proposed heuristics don't show significant improvements over existing methods. Heuristic 3, for example, is a common practice and lacks novelty. The reliance on an OOD validation set is a practical limitation, and the connection to Bayesian methods isn't clearly explained. Reviewer 4 criticizes the lack of theoretical justification and the paper's clarity, making it hard to evaluate the contributions.\n\nThe technical novelty is rated low by all reviewers. The heuristics are incremental and not well-supported by theory. The empirical results are mixed, with some cases showing minor improvements and others none. For a top-tier conference, the bar is high, and the paper doesn't meet it in terms of novelty or significance.\n\nClarity issues are another problem. Key definitions like \"conditional shift\" are missing, and some sections are confusing. The Bayesian discussion in Section 4 is unclear, and the regression extension isn't sufficiently explored. Reproducibility is a concern due to missing details in the methodology.\n\nIn conclusion, while the problem is important, the paper's execution falls short. The incremental contributions, lack of theoretical grounding, and modest empirical results make it unsuitable for a top-tier venue. The reviewers' consensus leans towards rejection, and I agree.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem of adapting models to distribution shifts (label and conditional) in online settings, which is highly relevant for real-world deployments. However, significant shortcomings prevent it from meeting the standards of a top-tier conference:  \n\n1. **Lack of Technical Novelty and Theoretical Foundation:**  \n   - The proposed heuristics (e.g., OOD validation, hyperparameter tuning for confusion matrices) are incremental and lack theoretical justification. Heuristic 3 (confusion matrix regularization) is noted as a common practice, diminishing its novelty.  \n   - The Bayesian framework in Section 4 is underdeveloped and disconnected from the empirical results, failing to provide actionable insights.  \n\n2. **Insufficient Empirical Rigor:**  \n   - Experiments are limited in scope (3 synthetic/2 real datasets) and fail to compare against critical baselines (e.g., domain adaptation/generalization methods like CORAL, optimal fixed classifiers).  \n   - Results are inconclusive, with marginal or inconsistent improvements (e.g., no clear gains on WILDS datasets, minor degradation in iWildCam F1). The paper’s honesty about these limitations underscores the preliminary nature of the findings.  \n\n3. **Clarity and Methodological Gaps:**  \n   - Key concepts (e.g., “conditional shift,” OOD validation protocol) are poorly defined, and notation is inconsistent (e.g., misuse of \\( f \\), undefined terms in Bayesian equations).  \n   - The regression extension and confusion matrix handling lack depth, with minimal discussion of prior work in these areas.  \n\n4. **Reviewer Consensus:**  \n   All reviewers highlight weak technical/empirical novelty (scores 1–2/4) and correctness issues. While the problem is well-motivated, the paper does not advance the field significantly beyond Wu et al. (2021) and lacks the rigor expected for a top-tier venue.  \n\nGiven these issues, the paper does not meet the threshold for acceptance at a prestigious conference. A rejection is warranted, encouraging the authors to address these gaps in future work."
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "<think>\nOkay, let's start by going through all the information provided. The paper introduces DeepSeaProbLog, which extends DeepProbLog to handle both discrete and continuous variables using weighted model integration (WMI). The reviewers have a mix of positive and negative points. \n\nReviewers 3 and 4 highlight the novelty and the technical contribution, especially the use of WMI to handle continuous variables. However, they also point out weaknesses in the empirical evaluation, such as limited experiments and lack of comparisons in pure discrete settings. Reviewer 2 and 5 mention the need for more applications and scalability analysis. Reviewer 6 and 7 question the technical novelty, suggesting the work is incremental and the experiments are not convincing. Reviewer 6 also raises concerns about handling discrete variables and the clarity of the method's contributions.\n\nThe strengths of the paper include addressing a clear gap in existing NPLP systems by supporting continuous variables, which is a significant step for real-world applications. The integration of WMI and neural networks is noted as novel by some reviewers. The implementation and theoretical foundation seem solid, with correctness scores mostly high.\n\nOn the downside, the empirical evaluation is criticized for being limited to toy examples and lacking thorough comparisons. The reviewers wanted more benchmarks, ablation studies, and scalability tests. Some reviewers felt the technical details were not sufficiently explained in the main text, and the experiments didn't fully demonstrate the method's advantages over existing approaches. The perceived incremental nature of the work (building on DeepProbLog) lowers the novelty score for some reviewers.\n\nConsidering this is a top-tier conference, the paper needs to demonstrate both significant technical novelty and strong empirical validation. While the technical contribution is there, the empirical section's weaknesses might be a major issue. However, the problem addressed is important, and the approach seems promising. The mixed reviews suggest that the paper is on the borderline. Given that some reviewers acknowledge the technical soundness and the potential impact despite the empirical limitations, and considering that the authors might address some concerns in a revision, the paper might be a candidate for acceptance. However, the lack of extensive experiments and comparisons could be a deal-breaker for a top-tier venue. The reviewers' scores vary, but the majority lean towards acceptance with reservations. Balancing all factors, the paper's contribution to enabling continuous variables in NPLP might justify acceptance, provided the authors address the reviewers' concerns in the final version.\n</think>\n\nFinal Decision: Accept \n\n**Reasoning:**  \nThe paper introduces DeepSeaProbLog, a novel extension of neural probabilistic logic programming (NPLP) to handle mixed discrete-continuous domains via weighted model integration (WMI). This addresses a significant limitation in existing systems like DeepProbLog, which are restricted to discrete variables. The technical contributions—semantics, implementation, and integration of WMI with gradient-based learning—are solid and well-supported, with reviewers acknowledging correctness (most scored 4/4) and technical novelty (mixed scores, but several 3–4/4). The work fills a clear gap in NeSy research, enabling richer probabilistic modeling for real-world applications.\n\nHowever, the empirical evaluation is a weakness. Reviewers criticized the limited scope of experiments (e.g., \"toy applications,\" lack of pure discrete benchmarks, and insufficient ablation studies) and the absence of scalability analysis. While the MNIST subtraction task demonstrates potential, it was deemed contrived by some. Additionally, discussions about WMI solver choices and comparisons to discrete-only baselines were missing, weakening the validation of the method’s advantages.\n\nDespite these issues, the core technical contribution—supporting continuous variables in NPLP—is impactful and aligns with the conference’s focus on advancing foundational methods. Reviewers 3, 4, and 6 acknowledged the novelty and significance, while others’ concerns about incrementalism (e.g., Reviewer 5) were balanced by the clear extension of DeepProbLog’s capabilities. The paper’s clarity and reproducibility were generally praised, though some requested more details in the main text.\n\nGiven the importance of enabling mixed discrete-continuous reasoning in NeSy and the technical rigor demonstrated, the paper merits acceptance. The authors should address empirical limitations (e.g., broader benchmarks, scalability) in future work, but the foundational contribution justifies publication in a top-tier venue."
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to work through this. So, the paper is about data leakage in federated learning, specifically targeting tabular data. They propose a method called TabLeak that addresses challenges unique to tabular data like mixed discrete-continuous features. The reviewers have mixed opinions. \n\nReviewer 1 and 2 are pretty critical. They mention that the methods are not novel—softmax and ensembling are standard techniques. Reviewer 1 also points out writing issues, which could be a problem for clarity. Reviewer 2 says the technical contributions are weak and that there are better optimization methods available. But Reviewer 3 is more positive, highlighting the application's novelty to tabular data and the thorough experiments, though they have some concerns about the claims and methodology.\n\nThe main points to consider are novelty, technical contribution, and presentation. The paper's strength is applying existing techniques to a new domain (tabular data in FL), which Reviewer 3 finds significant. However, Reviewers 1 and 2 don't see the technical methods as novel enough. The empirical results are strong, showing improvements over baselines, which is a plus. But if the core ideas are just applying known methods to a new problem without significant innovation, that might not be enough for a top-tier conference.\n\nAlso, the writing issues from Reviewer 1 could be a red flag. Poor presentation can hinder the paper's acceptance even if the content is good. However, Reviewer 3 says the paper is well-written, so maybe Reviewer 1's concerns are more about specific sections. The authors might need to clarify those parts.\n\nThe technical correctness seems okay. Reviewer 2 gives a 4 for correctness, and Reviewer 3 a 3. The main issues are around novelty and the justification of certain claims. The rebuttal would need to address the concerns about whether the mixed data problem is uniquely challenging and if their approach is truly novel compared to existing optimization techniques.\n\nIn top-tier conferences, even incremental contributions can be accepted if they address an important problem. Tabular data in FL is a high-stakes area, so showing vulnerabilities here is significant. The empirical results showing effectiveness could make the paper impactful despite the method's simplicity. However, if the reviewers feel the technical novelty is too low, it might get rejected.\n\nBalancing all this, the paper has a meaningful application and solid experiments but might be held back by perceived lack of technical novelty and writing issues. If the authors can convincingly address the reviewers' concerns in a rebuttal, especially regarding the novelty of their approach compared to existing methods and better justifying their claims, it might have a chance. But given the split reviews and the high standards of top conferences, it's a borderline case. However, since Reviewer 3 is more positive and the empirical results are strong, maybe leaning towards acceptance with major revisions. But the reviewers' scores (especially technical novelty being low) might push towards rejection.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper addresses an important problem in federated learning (FL) by focusing on data leakage in tabular data, a high-stakes domain with privacy-critical applications. The proposed method, TabLeak, introduces techniques to handle mixed discrete-continuous optimization, reduce reconstruction variance, and assess reconstruction quality. However, the reviews highlight critical concerns:\n\n1. **Technical Novelty**:  \n   - Reviewers 1 and 2 argue that the core techniques (softmax relaxation, ensembling) are not novel. Softmax relaxations and pooling strategies are well-established in other domains, and alternative optimization approaches (e.g., mixed-integer programming, Gumbel tricks) are not sufficiently explored. This weakens the perceived technical contribution.  \n   - Reviewer 3 acknowledges the novelty of applying gradient inversion attacks to tabular data but questions whether the challenges posed by mixed features are uniquely addressed compared to NLP/text settings.\n\n2. **Empirical Evaluation**:  \n   - While the experiments demonstrate improvements over baselines (e.g., 10% accuracy gain on Adult dataset), reviewers note weaknesses in benchmarking (e.g., weak baselines like random selection) and justification. For instance, the claim that mixed features inherently increase variance lacks empirical validation.  \n   - Reviewer 3 highlights missing analyses (e.g., effect of network architecture, comparison to linear models) and ambiguities in reproducibility (e.g., batch alignment during evaluation).\n\n3. **Presentation and Clarity**:  \n   - Reviewer 1 criticizes the writing quality, citing grammatical errors and inconsistent tense, which could hinder readability. Reviewer 3 notes minor notation inconsistencies but finds the paper generally well-structured. These issues may reduce the paper’s accessibility despite its technical merits.\n\n4. **Significance**:  \n   - The application to tabular data is timely and relevant, given its prevalence in sensitive domains. However, the reviewers’ concerns about novelty and methodological rigor raise doubts about whether the work advances the field beyond incremental progress.\n\n**Final Decision**: Reject  \n\nWhile the paper tackles an important problem and demonstrates empirical success, the lack of clear technical novelty and unresolved methodological concerns (e.g., insufficient justification for design choices, weak baselines) make it unsuitable for a top-tier conference. Addressing these issues would require substantial revisions to strengthen the theoretical foundation and experimental validation."
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper and the reviewers' comments. The paper's title is \"Improving Protein Interaction Prediction using Pretrained Structure Embedding.\" The abstract says they're using pretrained structure embeddings from OmegaFold and applying graph neural networks (GNNs) for PPI prediction. The main claim is that their method improves over sequence and network-based methods and shows transferability across species.\n\nReviewer 1 points out that the novelty is limited because they're using existing OmegaFold embeddings and just applying standard GNNs. The mean pooling of embeddings might lose local structural info. Also, there are inconsistencies in the results across tables. Reviewer 2 and 3 echo the lack of novelty, saying the approach is too simple and the improvements are marginal. They also mention not comparing to state-of-the-art methods like dMaSIF or AlphaFold-Multimer. Reviewer 4 is critical of the weak baselines and the model design, questioning why a GNN is necessary instead of pairwise models. They also mention missing comparisons to relevant works.\n\nThe authors' response isn't provided here, but based on the reviews, the main issues are:\n\n1. **Novelty and Contribution**: The method uses existing embeddings (OmegaFold) without introducing new pre-training. The application of GNNs isn't novel either. The technical contribution seems minimal.\n\n2. **Empirical Evaluation**: The improvements over baselines are small (around 5%), and the baselines might not be strong enough. Missing comparisons to SOTA methods like AlphaFold-Multimer or dMaSIF. Also, the use of AUC without confidence intervals and unclear setup (like negative sampling) raises questions about result reliability.\n\n3. **Methodology Concerns**: Using mean pooling might not capture local interactions. The choice of GNN over pairwise models isn't justified. The model's design isn't clearly advantageous.\n\n4. **Clarity and Presentation**: Issues with table formatting, missing details on MSA usage in OmegaFold, and some typos. The motivation is good, but the execution in writing and presentation needs improvement.\n\nFor a top-tier conference, the paper needs to demonstrate significant technical novelty and empirical advancement. Here, the approach seems incremental. The reviewers consistently rate technical and empirical novelty as low (scores 1-2). The correctness is also questioned, especially regarding the experimental setup and baselines.\n\nEven though the problem is important, the lack of novelty and insufficient empirical validation against proper baselines are major red flags. The paper might be better suited for a workshop or a lower-tier venue where incremental improvements are acceptable. Given the reviewers' consensus on limited contributions, rejection seems appropriate.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper addresses an important problem in protein-protein interaction (PPI) prediction by incorporating structural embeddings from OmegaFold into a graph neural network (GNN) framework. While the motivation is well-founded—leveraging structural information to overcome limitations of sequence and network-based methods—the reviewers unanimously highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n1. **Limited Novelty and Contribution**:  \n   - The core methodology relies on **directly using pretrained OmegaFold embeddings** without modifying or extending the pretraining process. The application of standard GNN architectures (e.g., GraphSAGE, GAT) to these embeddings is not novel, as similar approaches have been explored in other domains.  \n   - The work does not propose a new structural embedding method or a novel architecture tailored to PPI prediction. This lack of technical innovation makes the contribution incremental rather than groundbreaking.\n\n2. **Weak Empirical Validation**:  \n   - **Marginal Improvements**: The reported performance gains (e.g., ~5% over baseline GNNs) are modest and lack statistical significance due to missing confidence intervals.  \n   - **Inadequate Baselines**: Key state-of-the-art methods for PPI prediction (e.g., AlphaFold-Multimer, dMaSIF) are not compared, raising doubts about the method’s competitiveness.  \n   - **Unresolved Inconsistencies**: Reviewer 1 notes inconsistent results across datasets (e.g., PSE4PPI-SAGE vs. PSE4PPI-GAT performance in Tables 2 and 3), which are not addressed in the current submission.  \n\n3. **Methodological Concerns**:  \n   - **Simplistic Embedding Aggregation**: Mean pooling of OmegaFold embeddings may fail to capture local structural features critical for interaction prediction, as noted by Reviewer 1.  \n   - **Unjustified GNN Design**: The choice to frame PPI prediction as a graph-level task (with negative sampling) instead of a pairwise prediction problem is not rigorously motivated. Reviewer 4 questions whether the GNN itself contributes meaningfully over simpler pairwise models.  \n\n4. **Clarity and Reproducibility Issues**:  \n   - The paper lacks clarity in defining evaluation metrics (e.g., how AUC is computed for PPI) and experimental setup (e.g., negative sampling details).  \n   - Tables are poorly formatted, and key implementation details (e.g., MSA generation for OmegaFold) are omitted.  \n\n**Conclusion**:  \nWhile the idea of integrating structural embeddings into PPI prediction is promising, the paper does not meet the bar for a top-tier conference due to its incremental technical contribution, insufficient empirical validation, and unresolved methodological concerns. The reviewers’ critiques—particularly the lack of novelty, weak baselines, and marginal improvements—align with common rejection criteria for such venues. The work might be better suited for a specialized workshop or journal after addressing these limitations.  \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what the paper is about. The title is \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning.\" The main idea seems to be splitting a neural network into two parts: an encoder on the client side and a classifier on the server side. The goal is to anonymize data sent to the server to protect sensitive attributes while maintaining task accuracy. They use center loss to minimize sensitive information in the encoded data.\n\nLooking at the reviewers' comments, Reviewer 1 thinks the idea is incremental, with limited experiments and baselines. They mention that splitting the network isn't new and that the datasets used are too simplistic. Also, they didn't compare with methods like homomorphic encryption. Reviewer 2 points out serious technical flaws, like the server having access to labels and class centers, which could allow reconstruction of the encoder or data. They also question the threat model, as the server could disable the center loss. Reviewer 3 is concerned about the lack of a defined security model and that features might still leak information, especially in other domains like NLP.\n\nThe authors' approach might have novelty in using center loss for privacy, but the reviewers highlight significant issues. The threat model seems flawed because if the server is adversarial, trusting it to apply the center loss correctly isn't realistic. The experiments are limited to simple datasets and models, which might not generalize. Also, not comparing with more relevant baselines like split learning or other privacy techniques weakens the empirical validation.\n\nThe technical novelty is questioned by all reviewers. The core idea of splitting the network is similar to existing methods, and the added component (center loss) might not sufficiently address the privacy concerns. The empirical evaluation is insufficient, with only two datasets and shallow models. The reviewers also note that the privacy claims aren't well-supported, especially against stronger attacks.\n\nConsidering the standards of a top-tier conference, the paper needs strong technical novelty, rigorous experiments, and a clear threat model. The flaws in the threat model and the lack of thorough experimentation are major red flags. Even if the idea has potential, the current presentation and validation don't meet the high bar required. The reviewers' concerns about the approach's effectiveness and the limited empirical evidence suggest that the paper isn't ready for acceptance.\n</think>\n\n**Detailed Reasoning:**\n\nThe paper proposes a privacy-preserving MLaaS framework (Scrunch) using split neural networks and center loss to anonymize client data. However, based on reviewer feedback and analysis, the work has critical shortcomings that undermine its suitability for a top-tier conference:\n\n1. **Technical Flaws in Threat Model and Security Assumptions**  \n   - The threat model is inconsistent and unrealistic. Reviewer 2 highlights that the server (assumed adversarial) could bypass the center loss entirely, rendering the privacy mechanism ineffective. The server’s access to labels, class centers, and encoder parameters also enables model inversion attacks, contradicting the paper’s privacy claims.  \n   - The defense is limited to inference phases, leaving training-phase vulnerabilities unaddressed (e.g., surrogate encoder reconstruction).\n\n2. **Incremental Novelty**  \n   - The core idea of splitting networks for privacy builds on prior work (e.g., split learning), which is not acknowledged. The addition of center loss is insufficient to claim novelty, as similar mutual-information minimization techniques exist in privacy literature. Reviewers unanimously rated technical and empirical novelty as marginal.\n\n3. **Weak Empirical Validation**  \n   - Experiments are limited to shallow models (LeNet-5, VGG-16) and simplistic datasets (MNIST, CelebA). Results on MNIST lack baseline comparisons, and CelebA’s binary tasks (gender/smiling) are not representative of real-world privacy challenges.  \n   - No comparison to critical baselines (e.g., homomorphic encryption, differential privacy) or evaluation on modern architectures (ResNets, Transformers) weakens the case for practical relevance.\n\n4. **Ambiguous Security Model and Privacy Guarantees**  \n   - Reviewer 3 notes the absence of a formal security model, and all reviewers question whether encoded features truly prevent leakage. The paper fails to address reconstruction risks (e.g., time-series trends in wearables, NLP token inference) or provide theoretical privacy bounds.\n\n5. **Writing and Reproducibility Issues**  \n   - Grammatical errors and ambiguous statements (e.g., conflating intra/inter-class distance) reduce clarity. Reproducibility is hampered by missing training details (optimizers, epochs).\n\n**Conclusion:**  \nWhile the problem addressed is important, the paper’s technical flaws, lack of rigorous evaluation, and incremental contributions do not meet the standards of a top-tier conference. The reviewers’ concerns about the threat model, novelty, and empirical validity are decisive.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper's title is \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics.\" The abstract emphasizes studying the average performance of learning dynamics beyond just convergence, which is an important issue. The keywords include q-replicator dynamics and potential games, which are established areas in game theory and machine learning.\n\nLooking at the reviewers' comments, there's a mix of opinions. Reviewer 1 acknowledges the motivation but points out that prior work (PP16) might have initiated similar studies, and the formal results here are partial. They also mention some technical clarifications needed, like the distinction between \"whenever\" and \"if and only if\" in Theorem 4.4, and citation formatting issues.\n\nReviewer 2 raises a significant concern about insufficient credit to PP16, which they claim already addressed similar problems. They note that the results here seem incremental, especially since the main theorems are limited to 2x2 games, and the technical novelty is questioned. However, after discussion, they mention that added comparisons with PP16 could improve the paper, but their initial inclination was rejection.\n\nReviewer 3 finds the problem interesting but feels the results are preliminary. They point out that even in the 2x2 case, the analysis is incomplete (only comparing q=0 and q=1, no formal proof for general q). They also mention issues with notation and the abstract's accuracy. The empirical approach is seen as promising but not fully developed.\n\nReviewer 4 supports the paper, highlighting the importance of the problem and the clarity of the writing. However, they note the restriction to 2x2 games and suggest the authors explain why their methods can't extend further. Post-rebuttal, they agree the contributions need clearer presentation but still lean towards acceptance.\n\nThe main themes from the reviews are:\n1. **Novelty and Comparison to Prior Work (PP16):** Several reviewers felt the paper didn't sufficiently differentiate itself from PP16, which already studied similar metrics. The authors need to clarify how their work extends beyond PP16, especially in generalizing to QRD and providing a framework for comparison.\n2. **Technical Depth and Scope:** The results are seen as limited to 2x2 games, with partial analysis even within that scope. Theorems 4.4 and 4.6 are specific, and the lack of generalization to higher dimensions or other game classes is a weakness. The empirical evidence for monotonicity isn't formally proven, which undermines the claims.\n3. **Clarity and Presentation:** Issues with citation formatting, terminology (e.g., \"best response contained in a point\"), and ambiguous statements in theorems and the abstract need addressing. The abstract's claim of a general framework might be overstated given the paper's focus on 2x2 games.\n4. **Significance:** While the problem is important, the incremental progress and narrow focus reduce the perceived impact. However, some reviewers acknowledge the potential if expanded.\n\nConsidering the standards of a top-tier conference, the paper must demonstrate significant technical novelty, rigorous analysis, and clear contributions beyond existing work. The authors have addressed some concerns in the rebuttal (e.g., adding discussions about PP16), but key issues remain. The restriction to 2x2 games and partial results even there may not meet the high bar for acceptance. The framework's generality is not fully established, and the empirical results without formal proofs weaken the theoretical contribution. While the problem is relevant, the execution seems preliminary. Therefore, despite some strengths, the paper might not yet be ready for a top-tier venue and could benefit from more comprehensive results and clearer differentiation from prior work.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem—evaluating the average-case performance of learning dynamics beyond mere convergence—but falls short of the standards for a top-tier conference due to several critical weaknesses:  \n1. **Incremental Technical Contributions:** The results, while solid, are perceived as incremental extensions of prior work (e.g., PP16). Theorems 4.4 and 4.6 are limited to 2x2 symmetric coordination games, and the analysis of general q-replicator dynamics lacks formal proofs (e.g., monotonicity of regions of attraction).  \n2. **Underdeveloped Framework:** The abstract and introduction promise a \"systematic comparison of different optimal learning dynamics in arbitrary games,\" but the technical results focus narrowly on a restricted class of games. The lack of generalization to higher dimensions or broader game classes undermines the claimed framework.  \n3. **Insufficient Engagement with Prior Work:** Reviewers highlighted inadequate discussion of PP16, which already studied regions of attraction and average price of anarchy in similar settings. This weakens the perceived novelty.  \n4. **Ambiguities and Presentation Issues:** Technical terms (e.g., \"best response contained in a point\") and theorem statements (e.g., Theorem 4.4’s \"whenever\" vs. \"if and only if\") lack rigor. The abstract’s generality is misleading given the paper’s narrow focus.  \n\nWhile the problem is compelling and the empirical approach shows promise, the paper’s limited scope, partial results, and unresolved comparisons to prior work make it better suited for a specialized venue or a revision with expanded formal analysis and broader applicability."
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let me try to figure out whether this paper should be accepted or rejected. So, the paper is about identifying phase transition thresholds in permuted linear regression using message passing. The reviewers have a mix of opinions, but there's a lot of criticism here. Let me go through each reviewer's points and see the common themes.\n\nReviewer 1 says the paper is interesting but poorly written. They mention issues with incremental results and lack of clarity. The paper relies heavily on prior works (Semerjian et al. 2020 and Zhang et al. 2020), making it hard to follow without reading those. The notation is heavy, and the presentation isn't self-contained. They suggest major reorganization and expanding explanations, especially for the non-oracle case. They also note numerous typos and unclear sections.\n\nReviewer 2 points out that the main contribution is the non-oracle case threshold, but the oracle case is not practical. They criticize the writing as messy and non-rigorous, with unclear motivations for partial permutation recovery. There are issues with assumptions (like Assumption 1) not being justified, and technical details in theorems not being properly explained. They also mention problems with numerical result presentation.\n\nReviewer 3 finds the paper interesting but notes standard techniques and typos. They're less familiar with message passing but think the results are novel. However, they mention reproducibility issues since code isn't provided.\n\nReviewer 4 praises the paper's clarity and technical contribution but points out that the phase transition analysis builds on prior work (Semerjian et al.), limiting novelty. They have questions about the presentation, reproducibility, and some technical details. They also note the need for code release and better discussion of limitations.\n\nReviewer 5 is very critical, calling the writing terrible and the mathematical presentation confusing. They highlight contradictions (precise vs. approximate thresholds), unrealistic assumptions (like B being identity), and vague claims. They find sections like the message passing review and technical explanations poorly written and hard to follow. They recommend rejection due to inaccessibility and lack of clarity.\n\nLooking at the scores: Reviewers 1, 2, 4, and 5 have significant concerns. Reviewer 3 is more positive but still notes issues. The main issues across all are:\n\n1. **Clarity and Writing**: Almost all reviewers mention poor writing, heavy notation, lack of self-containment, and confusing explanations. The paper is hard to follow without prior knowledge of specific works.\n\n2. **Technical Novelty**: Some reviewers (1, 2, 4) note that the work builds heavily on prior methods (Semerjian et al., Zhang et al.), making the contributions incremental. The phase transition analysis for the non-oracle case is the main novel part but is not fully rigorous.\n\n3. **Empirical Support**: There are concerns about the empirical results, like gaps between theory and experiments, unclear motivation for partial recovery, and lack of code for reproducibility.\n\n4. **Assumptions and Rigor**: Assumption 1 is criticized for being vague. The oracle case's practicality is questioned, and some technical derivations lack proper justification.\n\nGiven that top-tier conferences require both technical novelty and clear presentation, this paper seems to fall short. The significant issues with clarity and organization, as highlighted by multiple reviewers, make the current version unsuitable. Even though there's technical merit in identifying phase transitions, the reliance on prior works and the lack of self-contained explanations are major drawbacks. The paper might need substantial revisions to improve clarity, better justify assumptions, and elaborate on the non-oracle case's contributions. However, as it stands, the consensus leans towards rejection.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper addresses an important problem in permuted linear regression, aiming to identify precise phase transition thresholds via message passing. While the technical contributions—particularly the analysis of the non-oracle case—are non-trivial, the paper faces significant criticism across multiple dimensions:  \n\n1. **Clarity and Organization**:  \n   - The paper is heavily criticized for poor writing, excessive notation, and reliance on prior works (e.g., Semerjian et al. 2020, Zhang et al. 2020) without sufficient self-contained exposition. Reviewers note that the presentation is inaccessible to readers unfamiliar with these works, and key sections (e.g., message passing derivation, phase transition analysis) lack clarity.  \n\n2. **Technical Novelty**:  \n   - The phase transition analysis for the oracle case is seen as incremental, building directly on existing frameworks. While the non-oracle case introduces new insights, reviewers question the rigor of the approximation methods and the justification for assumptions (e.g., Assumption 1).  \n\n3. **Empirical and Practical Relevance**:  \n   - The motivation for partial permutation recovery is unclear, and empirical results (e.g., gaps between theoretical and numerical thresholds) are insufficiently explained. The practicality of the oracle setting (known signal matrix) is also debated.  \n\n4. **Reproducibility**:  \n   - The absence of code and incomplete experimental details hinder reproducibility, a critical concern for top-tier conferences.  \n\nWhile the problem is theoretically interesting, the paper’s current state—marked by organizational flaws, unclear technical contributions, and unresolved methodological questions—does not meet the standards for acceptance. Major revisions (e.g., restructuring for clarity, addressing assumptions, improving empirical validation) are necessary before the work could be reconsidered."
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by going through the paper details and the reviewers' comments. The paper proposes FedPA, a parameterized aggregator using meta-learning to address client and period drift in federated learning. The main claims are that existing methods neglect period drift and that FedPA improves convergence by learning to aggregate parameters better.\n\nReviewer 1 points out several weaknesses. The first is that the paper doesn't sufficiently differentiate between client drift and period drift. Without clear theoretical or experimental distinction, the motivation for period drift as a novel concept is weak. Also, the aggregation strategy uses a proxy dataset, which limits applicability and is similar to existing methods like FedET and DS-FL. The lack of comparison with these works is a problem. Additionally, the experiments are on simple datasets (EMNIST and MovieLens) and lack ablation studies on proxy data distribution. The technical novelty is rated low, and there's no code provided, affecting reproducibility.\n\nReviewer 2 acknowledges the identification of period drift as a strength but criticizes the lack of in-depth discussion on why meta-learning is suited for this problem. They also mention that the dynamic systems analogy is underdeveloped and question the need for two neural networks. The reviewer suggests more comparisons with prior work and leans towards rejection due to insufficient justification of the method's unique advantages.\n\nReviewer 3 finds the idea interesting but raises concerns about the proxy dataset assumption and privacy implications, as accessing individual client updates might conflict with FL principles. They also question the experimental results, noting lower accuracy compared to other works and a lack of ablation studies. The reviewer is concerned about the method being ad-hoc and suggests simpler alternatives. Reproducibility is an issue without code.\n\nReviewer 4 highlights the importance of addressing two levels of drift but points out the need for testing on more datasets and quantifying server overhead. They note minor writing issues and the need for clarity on how the drift problem in FL is different from non-FL scenarios. The technical novelty is considered moderate.\n\nSynthesizing these points, the main issues are:\n\n1. **Novelty and Differentiation**: The concept of period drift isn't sufficiently justified. Without clear differentiation from client drift and prior work (like FedET, DS-FL, and others), the novelty is questionable. The dynamic systems analogy isn't effectively utilized, making the approach seem less grounded.\n\n2. **Methodological Concerns**: The reliance on a proxy dataset is a significant limitation, especially in FL where data privacy is paramount. The architecture choice (two NNs) isn't well-explained, and simpler alternatives (like weighted averaging) aren't explored. The connection between period drift and the proposed solution via meta-learning isn't clearly established.\n\n3. **Experimental Weaknesses**: The datasets used are not challenging enough (CIFAR-100 and Stack Overflow were suggested). Results on FEMNIST are lower than existing methods, raising doubts about effectiveness. Lack of ablation studies on proxy data distribution and hyperparameters undermines the empirical validation.\n\n4. **Reproducibility and Clarity**: No code is provided, and the writing has grammatical issues and unclear sections. The dynamic systems part is confusing, and some technical details (like Equation 7) need clarification.\n\nWhile the idea of learning an aggregator is interesting, the paper doesn't convincingly address these critical issues. The reviewers consistently note that the contributions are marginal in novelty and significance. The experimental setup is insufficient for a top-tier conference, and the methodological justification is lacking. Given these concerns, especially the lack of strong empirical validation and differentiation from prior work, the paper doesn't meet the high standards required for acceptance.\n</think>\n\nFinal Decision: Reject \n\n**Reasoning:**  \nThe paper introduces FedPA, a meta-learning-based aggregator to address client and period drift in federated learning (FL). While the idea of learning to aggregate parameters is promising, the paper faces critical shortcomings that undermine its suitability for a top-tier conference:  \n\n1. **Weak Novelty and Motivation**:  \n   - The concept of \"period drift\" is not rigorously defined or differentiated from existing notions of client drift. Reviewers highlight that the distinction between intra-round (client drift) and inter-round (period drift) heterogeneity is unclear, both theoretically and empirically.  \n   - The proposed method’s connection to meta-learning lacks justification. Prior works (e.g., FedET, DS-FL) already explore knowledge distillation or proxy data for aggregation, and the paper fails to demonstrate how FedPA meaningfully advances beyond these approaches.  \n\n2. **Methodological and Technical Concerns**:  \n   - The reliance on a proxy dataset raises privacy and practicality issues in FL, a setting where server-side data access is often prohibited. This assumption limits real-world applicability.  \n   - Architectural choices (e.g., dual neural networks for weights and updates) are not adequately motivated. Simpler alternatives (e.g., scalar weighting) are not explored, leaving the design’s necessity unproven.  \n   - The dynamic systems analogy (Section 3) is underdeveloped and does not contribute meaningfully to the technical narrative.  \n\n3. **Insufficient Empirical Validation**:  \n   - Experiments are limited to small-scale datasets (EMNIST, MovieLens) and shallow models, failing to demonstrate scalability. Reviewer 1 explicitly requests results on CIFAR-100 and Stack Overflow, which are standard benchmarks for FL robustness.  \n   - Performance claims are questionable: FedPA’s accuracy on FEMNIST is lower than state-of-the-art methods (e.g., Adaptive Federated Optimization).  \n   - Critical ablation studies (e.g., proxy data distribution impact, hyperparameter sensitivity) are missing, weakening the empirical case.  \n\n4. **Reproducibility and Presentation Issues**:  \n   - No code is provided, and implementation details (e.g., hyperparameters, training protocols) are insufficiently documented.  \n   - The writing suffers from grammatical errors, inconsistent citations, and unclear explanations (e.g., Equation 7, dynamic systems analogy).  \n\n**Conclusion**:  \nWhile the problem of aggregation bias in FL is important, the paper does not convincingly establish the novelty, technical soundness, or empirical superiority of FedPA. The lack of rigorous comparisons with relevant baselines, weak theoretical grounding, and limited experimental scope render it unsuitable for a top-tier venue. Addressing these issues would require substantial revisions, including redefining the problem’s novelty, expanding experiments, and justifying design choices."
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "<think>\nOkay, let's start by understanding what the paper is about. The title is \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability.\" The main contribution seems to be a new dataset that adds a 3D component to precipitation measurements, which existing datasets lack. They collected data from 30 radars in Russia over two years, including multiple altitude levels and other features like orography. They tested some baseline models and provided the dataset publicly.\n\nLooking at the reviewers' comments, Reviewer 1 points out that the dataset is attractive but the methods used are low-cost. They question how more complex models like transformers would perform and the dataset's usability compared to advanced forecasting devices. The technical and empirical novelty scores are low here.\n\nReviewer 2 praises the dataset's multi-dimensional data and coverage but criticizes the baseline methods as too naive. They want more experiments on other ML tasks, cross-evaluation with other zones, more related work details, better evaluation metrics beyond MSE, and results from previous methods on the new dataset. The technical novelty is rated as 3, but empirical is 2.\n\nReviewer 3 appreciates the dataset's richness and comparison with others but notes the dataset's coarseness (2 km grid) might limit its utility. They also mention no methodological novelties and a simple uncertainty estimation. They think the dataset is too similar to existing ones for a top conference like ICLR, despite being well-written and clear.\n\nAll reviewers agree the dataset is a solid contribution but lack technical novelty. The main issues are the simplicity of the baselines, lack of comparison with state-of-the-art models, and insufficient exploration of the dataset's broader applications. The paper's strength is the dataset, but in top-tier conferences, dataset papers need to show significant novelty and impact beyond just introducing data. The authors did provide code and data, which is good for reproducibility.\n\nHowever, the reviewers point out that the empirical evaluations are lacking. For example, using only MSE might not capture nowcasting accuracy adequately. They didn't test the dataset on other tasks like anomaly detection as claimed. Also, the baselines are not cutting-edge; using older models like optical flow and ConvLSTM without comparing to recent transformers or other advanced architectures makes the benchmarking less compelling.\n\nThe key question is whether the dataset's introduction alone is enough for acceptance. While the dataset adds 3D data and covers diverse climates, which is valuable, the paper doesn't sufficiently demonstrate its unique benefits over existing datasets. The experiments don't fully leverage the dataset's features, like vertical reflectivity changes. Without showing that the new data leads to better model performance or enables new research directions through thorough experimentation, the contribution's significance is limited.\n\nMoreover, top conferences often look for both a novel dataset and novel methodologies or insights derived from it. The authors provided baselines but didn't push the state-of-the-art in nowcasting. The uncertainty quantification part is mentioned but seems underdeveloped. The reviewers' concerns about the dataset's grid resolution and lack of cross-validation with other regions also weaken the case.\n\nIn conclusion, while the dataset is a good contribution, the paper doesn't meet the high bar for technical and empirical novelty required by top-tier conferences. The experiments are insufficient to highlight the dataset's advantages, and the methodological aspects are lacking. Therefore, the paper should be rejected.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\nThe paper introduces the RuDar dataset, a 3D weather radar dataset for precipitation nowcasting, which addresses a gap in existing datasets by incorporating vertical reflectivity data and geographical variability. While the dataset is a valuable contribution with potential community impact, several critical issues prevent acceptance at a top-tier conference:\n\n1. **Limited Technical and Empirical Novelty:**  \n   The dataset itself, though comprehensive, is deemed only marginally novel by reviewers. Existing datasets (e.g., HKO-7, NEXRAD) already provide multi-source radar data, and the paper does not sufficiently justify how RuDar’s 3D structure or geographical coverage uniquely advances the field beyond incremental improvements. The evaluation of baseline methods (e.g., optical flow, ConvLSTM) lacks comparison to state-of-the-art models (e.g., transformers, diffusion models), weakening claims about the dataset’s utility for cutting-edge research.\n\n2. **Insufficient Empirical Validation:**  \n   - The evaluation relies heavily on MSE, which is criticized as inadequate for capturing nowcasting accuracy. Metrics like Critical Success Index (CSI) or probability-based scores are standard in meteorology but omitted.  \n   - Claims about the dataset’s applicability to tasks like anomaly detection or data shift studies are unsupported by experiments.  \n   - Cross-region validation (e.g., testing models trained on RuDar with data from other regions) is absent, limiting insights into generalizability.\n\n3. **Methodological Weaknesses:**  \n   The baseline experiments are described as “naïve” and lack depth. For instance, the uncertainty quantification analysis is simplistic (e.g., seed variation for ensembles) and does not explore advanced techniques like Bayesian neural networks. The paper also fails to demonstrate how the 3D vertical data improves nowcasting compared to 2D baselines.\n\n4. **Comparison to Prior Work:**  \n   While the dataset is compared in terms of spatial/temporal resolution and parameters (Table 1), the discussion does not clarify how RuDar’s 3D structure leads to better model performance or novel scientific insights compared to existing datasets. The 2 km spatial resolution is noted as a potential limitation, but this is not rigorously addressed.\n\n5. **Reproducibility and Impact:**  \n   Though the dataset and code are publicly available, the paper’s impact is constrained by its narrow focus on precipitation nowcasting without broader ML applications. Reviewer 3 notes that dataset papers in top venues require groundbreaking novelty or transformative potential, which RuDar does not convincingly achieve.\n\n**Conclusion:**  \nWhile the RuDar dataset is a commendable effort and a useful resource, the paper does not meet the high bar for technical novelty, empirical rigor, or demonstrated impact expected at a top-tier conference. The lack of advanced baselines, limited validation, and insufficient exploration of the dataset’s unique capabilities undermine its contribution. Addressing these issues would require substantial revisions beyond the current scope."
    }
  ]
}