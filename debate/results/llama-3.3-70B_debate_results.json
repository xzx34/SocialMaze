{
  "summary": {
    "model": "llama-3.3-70B",
    "total_papers": 100,
    "correct_predictions": 65,
    "accuracy": 65.0,
    "confusion_matrix": [
      [
        43,
        33
      ],
      [
        2,
        22
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 56.57894736842105,
        "precision": 95.55555555555556
      },
      "Accept": {
        "total": 24,
        "recall": 91.66666666666666,
        "precision": 40.0
      }
    }
  },
  "results": [
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel meta-learning framework called NORML, which uses an LSTM-based meta-learner to perform neuron-wise optimization on a learner for efficient task learning. The authors claim that NORML can potentially scale to learner networks with a large number of parameters and that it learns to make effective weight updates using information from previous data points and update steps.\n\nHowever, after carefully analyzing the paper and the reviewer comments, several concerns and issues arise that cast doubt on the paper's contribution and validity.\n\nFirstly, the reviewers question the novelty of the proposed approach, pointing out that it is similar to previous work, such as Ravi & Larochelle's Meta-Learner LSTM. The authors claim that NORML has fewer parameters than previous approaches, but the reviewers argue that this is not entirely accurate. Specifically, Reviewer 1 points out that Ravi & Larochelle's approach also uses parameter sharing, which reduces the number of parameters in the meta-learner.\n\nSecondly, the experimental results are not convincing, and the reviewers request more comparisons with previous work, such as MAML and Meta-Learner LSTM. Reviewer 3 suggests that the comparison with MAML is not fair, as the authors use a different base learner and do not consider other versions of MAML, such as MAML++.\n\nThirdly, the reviewers criticize the writing style, pointing out numerous grammatical mistakes, inconsistencies in citations, and unclear explanations. Reviewer 2 states that the writing is \"well beyond the ICLR level\" and requires a \"massive overhaul.\"\n\nLastly, the reviewers question the applicability of the proposed approach, as it is limited to fully connected networks and may not be suitable for convolutional networks, which are commonly used in few-shot learning tasks.\n\nGiven these concerns and issues, it is challenging to evaluate the significance of the proposed approach. The authors' claims about the benefits of NORML are not adequately supported by the experimental results, and the reviewers' requests for more comparisons and clarifications are not addressed in the author response.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I conclude that the paper should be rejected due to its limited novelty, inadequate experimental results, and numerous writing and presentation issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" proposes a novel approach to reduce the training time of deep neural networks by utilizing a multi-precision strategy. The authors introduce a precision-switching mechanism that decides at runtime when to switch between different precision levels, allowing for a balance between training time and accuracy.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 questions the motivation behind the switching mechanism and requests a more detailed explanation. Reviewer 2 points out that the proposed approach does not match the description in the paper and suggests that the authors clarify the setting of quantized precision levels and the presentation of the precision switching policy. Reviewer 3 recommends a weak reject, stating that the core idea of the manuscript is not shown to be a necessary condition for good classification results.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and clarifications. They have also updated the manuscript to address the concerns raised by the reviewers.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has several strengths. The proposed approach is novel and has the potential to reduce the training time of deep neural networks. The authors have demonstrated the effectiveness of their approach on several networks and datasets, and the results show a significant reduction in training time without compromising accuracy.\n\nHowever, the paper also has some weaknesses. The switching mechanism is not fully explained, and the authors could provide more insight into the motivation behind it. Additionally, the presentation of the precision switching policy is confusing, and the notation could be improved.\n\nDespite these weaknesses, I believe that the paper has the potential to make a significant contribution to the field of deep learning. The authors have demonstrated the effectiveness of their approach, and the results are promising. With some revisions to address the concerns raised by the reviewers, I believe that the paper could be improved significantly.\n\nTherefore, I recommend that the paper be accepted for publication, but with some revisions to address the concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper in question, \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH,\" proposes an improvement to Monte Carlo Tree Search (MCTS) agents, a method that has been successful in games like chess, shogi, and Go, particularly through the approach inspired by Alpha Zero. The abstract suggests a focus on the coordination of episode generation, treating the entire system as a game tree search to balance exploitation and exploration. However, the paper faces significant criticisms from all reviewers, which can be summarized into several key areas:\n\n1. **Lack of Clarity and Poor Writing**: All reviewers agree that the paper is poorly written, with grammatical issues, incomprehensible passages, and a general lack of clarity. This makes it difficult for reviewers to understand the proposed method, its contributions, and its significance. The abstract, which is supposed to provide a clear and concise overview of the paper, is particularly criticized for its vagueness and lack of coherence.\n\n2. **Absence of References**: The paper does not include any references, which is a critical omission in academic writing. References are essential for situating the research within the existing body of knowledge, acknowledging previous work, and demonstrating the novelty and contribution of the proposed method. The author's response that they did not know how to use a.bib file and therefore removed all references is not a valid excuse, especially in a field where academic integrity and proper citation are fundamental.\n\n3. **Lack of Scientific Rigor and Detail**: Reviewers mention that the paper lacks the preliminary elements of scientific writing, including undefined mathematical symbols, poorly explained algorithm boxes, and large sections of blank space. This suggests a lack of rigor in the presentation of the research and its methods, making it impossible to evaluate the paper's technical contributions.\n\n4. **Insufficient Evidence and Evaluation**: While the abstract mentions experiments on a small problem showing robust performance compared to Alpha Zero, the reviewers do not have enough information to assess the validity and significance of these claims. There is no detailed description of the experimental setup, the metrics used for evaluation, or how the proposed method improves upon existing approaches.\n\nGiven these criticisms, the author's response acknowledges some of the issues, such as the need to improve writing skills and the lack of references. However, the response does not address the fundamental concerns about the paper's clarity, scientific rigor, and contribution to the field. The author expresses a desire to learn and improve, which is commendable, but this does not rectify the paper's current shortcomings.\n\nIn conclusion, based on the information provided and the severe criticisms from all reviewers, the paper in its current form does not meet the standards expected of a top-tier conference. The issues with clarity, references, scientific rigor, and the overall presentation are too significant to overlook.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" proposes a novel generative model, ROOTS, for unsupervised object-wise 3D scene decomposition and rendering. The model is based on the Generative Query Network (GQN) framework but provides object-oriented representation decomposition, which is a significant contribution.\n\nThe reviewers' comments highlight several strengths and weaknesses of the paper. The main strengths include:\n\n1. The paper proposes a novel and interesting idea, and the qualitative results are impressive.\n2. The model achieves disentanglement, compositionality, and generalization, which are important abilities in deep representation learning.\n3. The authors have made significant efforts to improve the readability of the paper and address the reviewers' comments.\n\nHowever, the reviewers also point out several weaknesses, including:\n\n1. The paper was initially poorly written, with many typos, inconsistencies, and missing explanations.\n2. The claims made in the paper are not always supported by the experiments, and some results are not clearly explained.\n3. The model is complex, and the notation is not always consistent or clear.\n4. The authors do not provide a thorough comparison with existing works in 3D computer vision, which makes it difficult to understand the significance of their contribution.\n\nAfter carefully considering the reviewers' comments and the authors' responses, it is clear that the authors have made significant efforts to address the weaknesses and improve the paper. The revised paper is more readable, and the authors have provided additional explanations and comparisons with existing works.\n\nHowever, some concerns still remain, such as the complexity of the model and the lack of a thorough ablation study. Nevertheless, the authors have provided a clear and detailed response to the reviewers' comments, and their contributions are significant and interesting.\n\nIn conclusion, based on the strengths and weaknesses of the paper, as well as the authors' efforts to address the reviewers' comments, I believe that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning disentangled representations in a hierarchical manner using a variational autoencoder (VAE) framework. The authors propose a progressive learning strategy that starts with learning the most abstract representation and then incrementally adds new representations at different levels of abstraction.\n\nThe paper has undergone significant revisions based on the reviewer comments, addressing concerns regarding the clarity of the purpose, the relationship between hierarchical and disentangled representations, and the comparison with existing methods. The authors have also provided additional experiments and results to support their claims, including ablation studies and comparisons with other state-of-the-art methods.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewer 1 and Reviewer 3 provide positive feedback, acknowledging the novelty and potential of the proposed approach. Reviewer 2, however, raises several concerns, including the lack of clarity in the purpose, the comparison with existing methods, and the limitation of the proposed metric.\n\nAfter carefully considering the reviewer comments and the author responses, it is clear that the authors have made significant efforts to address the concerns and improve the paper. The additional experiments and results provide stronger evidence for the effectiveness of the proposed approach, and the clarifications and revisions have improved the clarity and coherence of the paper.\n\nHowever, some concerns still remain, such as the comparison with existing methods and the limitation of the proposed metric. The authors have acknowledged these concerns and provided some additional discussions and results to address them.\n\nIn conclusion, based on the significant revisions and improvements made by the authors, the paper has demonstrated its potential and strengths. While some concerns still remain, the authors have provided sufficient evidence and discussions to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Score and Lyrics-Free Singing Voice Generation\" presents a novel approach to generating singing voices without the need for pre-assigned scores and lyrics. The authors propose three different schemes for singing voice generation: free singer, accompanied singer, and solo singer. The paper is well-structured, and the authors have made a significant effort to address the comments and concerns raised by the reviewers.\n\nThe reviewers have raised several concerns, including the lack of motivation for the three task settings, the absence of comparison with other methods, and the weakness of the evaluation metrics. However, the authors have responded to these concerns by revising the paper to include clear motivations, adding baseline methods for comparison, and expanding the evaluation metrics.\n\nThe authors have also addressed the concerns regarding the training data, including the lack of information about the data and the potential differences between the training and test domains. The authors have provided more details about the training data and have clarified the differences between the training and test domains.\n\nThe reviewers have also suggested that the paper could be improved by including more literature reviews, particularly on neural network-based singing voice synthesis. The authors have expanded the literature review to include more relevant papers.\n\nOne of the main concerns raised by the reviewers is the weakness of the evaluation metrics. The authors have addressed this concern by adding more objective metrics, including vocalness, matchness, and average pitch. The authors have also conducted subjective evaluations through MOS user studies, which provide more comprehensive evaluations of the generated singing voices.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the authors have made significant efforts to address the concerns and improve the paper. The paper presents a novel approach to singing voice generation, and the authors have provided a comprehensive evaluation of their method.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" presents a novel approach for training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. The authors propose three key ideas: minimizing an upper bound to the GMM log likelihood, introducing a new regularizer to prevent convergence to pathological local minima, and enforcing constraints inherent to GMM training when using SGD.\n\nThe reviewers raise several concerns and suggestions for improvement. Reviewer 1 criticizes the paper for being largely incremental and lacking understanding of the history of online EM. They also question the motivation for the max-component approximation, the use of a standard representation of multinoulli without reference, and the gridding approach for smoothing. Reviewer 2 points out the lack of guarantees for convergence and the need for excellent empirical results. They also raise concerns about the experimental evaluation, including the choice of datasets and the comparison to EM. Reviewer 3 finds the paper to be serious and well-written but notes that the main contribution is the description of a simple method for learning GMMs from random initialization. They also suggest that the paper falls short in showing improvement compared to other baselines.\n\nThe authors respond to the reviewers' comments, addressing concerns about the max-component approximation, the regularizer, and the experimental evaluation. They clarify that their approach is not online EM but rather SGD, which is a different optimization method. They also acknowledge the lack of guarantees for convergence and the need for empirical results.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I conclude that the paper has some merits but also significant weaknesses. The authors' approach to training GMMs using SGD is novel, and their responses to the reviewers' comments demonstrate a good understanding of the concerns raised. However, the paper lacks a clear and compelling motivation for the proposed approach, and the experimental evaluation is limited and not entirely convincing.\n\nThe main concerns that prevent me from accepting the paper are:\n\n1. Lack of clear motivation: The authors do not provide a strong motivation for why SGD is a better choice than EM for training GMMs in high-dimensional spaces.\n2. Limited experimental evaluation: The authors only evaluate their approach on a few datasets, and the comparison to EM is not comprehensive.\n3. Lack of guarantees for convergence: The authors do not provide any guarantees for convergence, which is a significant concern for any optimization method.\n\nGiven these concerns, I believe that the paper requires significant revisions to address the reviewers' comments and improve the overall quality of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Extreme Classification via Adversarial Softmax Approximation\" presents a novel approach to improve the efficiency of softmax regression in extreme classification settings, where the number of classes is very large. The authors propose an adversarial softmax approximation method that generates negative samples from an adversarial model, which mimics the data distribution. This approach aims to reduce the gradient variance and improve the convergence rate of the training process.\n\nThe paper has several strengths. Firstly, the authors provide a clear and well-motivated problem statement, and their proposed approach is simple yet effective. The experimental results on two large-scale datasets demonstrate the superiority of their method over several competitive baselines. Additionally, the authors provide a mathematical proof that their adversarial sampling mechanism minimizes the gradient variance, which adds to the paper's theoretical contributions.\n\nHowever, the paper also has some weaknesses. The reviewers have pointed out that the authors do not adequately discuss related work, particularly in the context of non-uniform sampling methods. The authors have responded to these comments by acknowledging the omission and promising to add a discussion of the relevant literature in the final version of the paper. Additionally, the reviewers have suggested that the authors should compare their method with other state-of-the-art approaches, such as Slice and DiSMEC, and evaluate their method on smaller datasets where the full softmax loss can be optimized.\n\nDespite these weaknesses, the paper's strengths outweigh its limitations. The authors have made a significant contribution to the field of extreme classification, and their proposed approach has the potential to improve the efficiency of softmax regression in large-scale classification problems. The paper is well-written, and the authors have provided a clear and concise explanation of their approach.\n\nBased on the reviewer comments and the author responses, it is clear that the authors have addressed the concerns raised by the reviewers and have provided a revised version of the paper that addresses the weaknesses identified by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Improved Training of Certifiably Robust Models\" proposes two regularizers to improve the training of certifiably robust neural networks. The regularizers are designed to reduce the gap between the certifiable and empirical robustness of neural networks. The paper presents experimental results showing that the proposed regularizers can improve the certified accuracy of neural networks on MNIST and CIFAR10 datasets.\n\nThe reviewer comments raise several concerns about the paper, including:\n\n1. Clarity and presentation: Reviewer 1 and Reviewer 2 find the analysis section confusing and suggest that it needs to be re-thought. Reviewer 3 mentions some presentation issues.\n2. Technical correctness: Reviewer 1 points out several technical errors, including the incorrect statement that IBP cannot certify L2 adversaries and the incorrect claim that the convex relaxation of Ehlers is optimal.\n3. Comparison to baselines: Reviewer 1 suggests that the paper should include the results of Gowal et al. in Table 1 for better context. Reviewer 2 asks whether the method of Wong et al. uses the looser convex relaxation or the tight convex relaxation.\n4. Experimental results: Reviewer 3 finds the experimental results not entirely convincing due to the lack of certain baselines.\n\nThe authors respond to the reviewer comments by:\n\n1. Revising the analysis section to clarify the technical details.\n2. Correcting the technical errors pointed out by Reviewer 1.\n3. Adding new experimental results, including comparisons to stronger baselines.\n4. Providing more context and explanations for the experimental results.\n\nAfter carefully considering the reviewer comments and the author responses, I believe that the paper has addressed the major concerns raised by the reviewers. The authors have revised the analysis section, corrected technical errors, and added new experimental results to strengthen the paper. While some minor issues remain, such as presentation issues and the lack of experiments on additional datasets, I believe that the paper makes a significant contribution to the field of certifiably robust neural networks.\n\nThe paper's strengths include:\n\n1. The proposal of two new regularizers that can improve the certified accuracy of neural networks.\n2. The presentation of experimental results showing the effectiveness of the proposed regularizers.\n3. The authors' responsiveness to reviewer comments and their efforts to address the concerns raised.\n\nThe paper's weaknesses include:\n\n1. Some minor presentation issues.\n2. The lack of experiments on additional datasets.\n3. Some technical details that could be further clarified.\n\nOverall, I believe that the paper's strengths outweigh its weaknesses, and I recommend acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a novel out-of-distribution detection method, MALCOM, which utilizes the normalized compression distance to extract informative sequential patterns from feature maps of convolutional neural networks. The method is designed to work without requiring out-of-distribution samples for validation or retraining the model, making it more practical for real-world applications.\n\nThe reviewers have raised several concerns and suggestions, which the authors have addressed in their response. Reviewer 1 points out that the comparison with prior methods is not entirely fair and suggests that the authors should compare their method with the Mahalanobis detector validated by adversarial samples. Reviewer 2 questions the necessity of the strict constraints imposed by the authors and suggests that the method should be compared with other works that are not as tightly constrained. Reviewer 3 suggests that the experiments could be done in a more complex setting and that the authors should provide theoretical guarantees for the proposed method.\n\nThe authors have responded to these comments by providing additional experiments and justifications for their approach. They have also polished the writing, corrected typos, and added missing related work. The authors have also conducted additional experiments to compare their method with other works that are not as tightly constrained, and the results show that their method, MALCOM++, outperforms other methods in most cases.\n\nAfter carefully analyzing the paper, the reviewer comments, and the author responses, I believe that the paper has addressed the major concerns raised by the reviewers. The authors have provided additional experiments and justifications for their approach, and the results show that their method is effective and outperforms other methods in most cases.\n\nHowever, I still have some reservations about the paper. The authors have not fully addressed the concern raised by Reviewer 1 about the comparison with prior methods, and the results in Table 2 show that the performance of the Mahalanobis detector is lower than reported in the original paper. Additionally, the authors have not provided theoretical guarantees for the proposed method, which is a concern raised by Reviewer 3.\n\nDespite these reservations, I believe that the paper has made significant contributions to the field of out-of-distribution detection and that the authors have addressed the major concerns raised by the reviewers. The paper is well-written, and the experiments are thorough and well-designed.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of the impact of activation functions on the training of highly overparametrized neural networks. The authors provide a comprehensive study of the effect of different activation functions on the minimum eigenvalue of the Gram matrix, which is crucial for determining the convergence rate of training.\n\nThe paper's strengths include its thorough theoretical analysis, which separates the results into two cases: non-smooth activation functions (e.g., ReLU, SELU, ELU) and smooth activation functions (e.g., tanh, swish, polynomial). The authors show that non-smooth activations have large minimum eigenvalues, while smooth activations can have small or even zero eigenvalues, depending on the dimensionality of the data and the depth of the network. The paper also includes empirical experiments that validate the theoretical results.\n\nThe reviewers' comments highlight the paper's strengths and weaknesses. Reviewer 1 appreciates the authors' effort in providing a thorough theoretical analysis but criticizes the paper's length and organization. Reviewer 2 votes to accept the paper, praising its thorough theoretical study and well-written presentation.\n\nThe authors' response addresses the reviewers' concerns, including the length and organization of the paper. They have added a table of contents to the appendix, reorganized some sections, and improved section names to make the paper easier to navigate. They have also clarified some minor concerns, such as the notation and assumptions.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper should be accepted for publication. The paper's strengths, including its thorough theoretical analysis and empirical validation, outweigh its weaknesses, which have been addressed by the authors in their response. The paper's contributions to the understanding of the effect of activation functions on the training of overparametrized neural networks are significant, and its presentation is clear and well-organized.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" presents a novel approach to segmenting trajectories into sub-skills using weak supervision. The authors propose a method inspired by multiple instance learning (MIL) to learn reusable skills from a collection of human demonstrations. The paper is well-motivated, and the problem statement is of clear value.\n\nHowever, the reviewers have raised several concerns that need to be addressed. Reviewer 1 points out that the paper's framing and motivation do not correspond to the results and experiments reported, which seems misleading. The reviewer also notes that the paper is limited in scope and requires further experiments and comparisons to relevant baselines to support the claims made.\n\nReviewer 2 finds the novelty and results of the paper to be minor and suggests that the log-sum-exp pooling approach has been used successfully in other domains. The reviewer also notes that the results are underwhelming, with validation accuracy ranging from 35-60%. The reviewer suggests that the authors should compare their method to a fully-supervised oracle method and provide more thorough baselines on the behavior task.\n\nReviewer 3 likes the idea of identifying skills in the proposed manner but has concerns about the feasibility of the approach when a large number of skills are present. The reviewer notes that the classification results seem extremely poor and suggests that the authors should provide more analysis, such as confusion matrices or precision and recall metrics, to assess the quality of the segmentations.\n\nThe authors have responded to the reviewers' comments, addressing some of the concerns raised. However, the responses do not fully alleviate the concerns, and some of the reviewers' comments remain unaddressed.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper has some merits, but the concerns raised by the reviewers are significant. The paper's limitations, lack of thorough baselines, and poor classification results are major concerns that need to be addressed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns from epigenetic marks. The authors propose a bidirectional LSTM RNN model that outperforms traditional machine learning models, demonstrating the importance of sequential DNA states for chromatin folding.\n\nThe reviewer comments highlight several concerns, including:\n\n1. **Lack of methodological novelty**: Reviewer 3 points out that the paper is similar to previously published works and lacks methodological novelty. The authors respond by highlighting the differences between their work and previous studies, but it is unclear whether these differences are significant enough to warrant publication in a top-tier conference.\n2. **Unclear evaluation**: Reviewer 1 and Reviewer 3 express concerns about the evaluation methodology, including the use of a custom weighted MSE metric and the lack of comparison to other baseline models. The authors respond by adding more evaluation metrics and comparing their model to other baseline models, but it is unclear whether this addresses the reviewers' concerns.\n3. **Lack of generalizability**: Reviewer 1 and Reviewer 3 point out that the paper lacks generalizability, as the model is only tested on a single dataset and organism. The authors respond by stating that they do not expect the underlying mechanism of TADs formation to be the same for other cell types and organisms, but this does not address the concern about generalizability.\n4. **Clarity and presentation**: Reviewer 2 and Reviewer 3 comment on the lack of clarity and presentation issues, including the absence of equations, unclear notation, and poor figure labeling. The authors respond by addressing these issues, but it is unclear whether the revised paper is significantly improved.\n\nDespite these concerns, the paper has some strengths, including:\n\n1. **Interesting application**: The paper presents an interesting application of machine learning to a biological problem, which could be of interest to the conference audience.\n2. **Good performance**: The authors demonstrate good performance of their model on the tested dataset, which suggests that the approach may be useful for predicting DNA folding patterns.\n\nHowever, after careful consideration of the reviewer comments and author responses, I believe that the paper has not adequately addressed the concerns raised by the reviewers. The lack of methodological novelty, unclear evaluation, and lack of generalizability are significant concerns that outweigh the paper's strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"SGD Learns One-Layer Networks in WGANs\" presents a theoretical analysis of the convergence of stochastic gradient descent-ascent (SGDA) in the context of Wasserstein Generative Adversarial Networks (WGANs) with a one-layer generator network. The authors claim to provide the first result showing convergence to global equilibrium points for non-linear generators, which is a significant contribution to the field.\n\nHowever, the reviewers have raised several concerns about the paper. The main criticisms are:\n\n1. **Simplistic discriminator**: The reviewers argue that the quadratic discriminator used in the paper is too simple and does not accurately represent the WGAN setting. They suggest that a more complex discriminator, such as a two-layer neural network, would be more realistic and challenging to analyze.\n2. **Limited applicability**: The reviewers question the applicability of the results to more complex generator architectures and discriminator classes. They argue that the analysis is limited to a specific setting and may not generalize to more practical scenarios.\n3. **Lack of clarity**: Some reviewers found the paper difficult to understand, particularly with regards to the choice of discriminator class and the relationship between the quadratic discriminator and the WGAN setting.\n\nThe authors have responded to these concerns, providing additional explanations and justifications for their choices. They argue that:\n\n1. **Quadratic discriminator is sufficient**: The authors claim that the quadratic discriminator has sufficient distinguishing power to learn the family of distributions parametrized by their generator class. They also argue that using a larger discriminator family would increase the sample complexity.\n2. **Focus on one-layer generator**: The authors emphasize that their goal is to understand the dynamics of SGDA with WGAN on learning a one-layer generator, which does not require a complex discriminator.\n3. **Future work**: The authors acknowledge that more complex discriminators may be necessary for future work and suggest that recent developments in neural network training, such as Neural Tangent Kernel (NTK) results, could be used to analyze the discriminator dynamics.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has some merits, but the concerns raised by the reviewers are significant. While the authors have provided some justifications for their choices, the limitations of the analysis and the simplicity of the discriminator class are still a concern.\n\nThe paper's contribution to the field is notable, as it provides a theoretical analysis of the convergence of SGDA in WGANs. However, the applicability and generality of the results are limited by the simplistic discriminator and the focus on a specific generator architecture.\n\nTherefore, I would recommend rejecting the paper in its current form, but encouraging the authors to revise and resubmit with additional analysis and experiments to address the concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel connection between the expressivity of Deep Neural Networks (DNNs) and Sharkovsky's Theorem from dynamical systems. The authors leverage this connection to characterize the depth-width trade-offs of ReLU networks for representing functions with periodic points. The paper provides a lower bound on the depth necessary to represent periodic functions, which is a significant contribution to the understanding of DNNs.\n\nThe reviewer comments are overwhelmingly positive, with Reviewer 2 stating that the paper is a \"delight to read\" and strongly suggesting acceptance. Reviewer 1 also provides positive feedback, and Reviewer 3, while criticizing the structure of the paper, acknowledges that it is \"serious and well-written\" and that the application of Sharkovsky's results is \"clever and well in place.\"\n\nThe author response addresses the reviewer comments and suggestions, providing additional examples, discussions, and experiments to support the results. The authors also clarify the usefulness of their results, acknowledging that assessing the period of a given function is a difficult question, but providing intuition and examples to illustrate the significance of their characterization result.\n\nThe paper's strengths include:\n\n1. Novel connection between DNNs and Sharkovsky's Theorem, providing new insights into the expressivity of DNNs.\n2. Clear and well-written exposition, making the paper accessible to a broad audience.\n3. Significant contribution to the understanding of depth-width trade-offs in DNNs.\n4. Strong positive feedback from reviewers, indicating that the paper is well-received by the community.\n\nThe paper's weaknesses include:\n\n1. Some reviewers found the structure of the paper to be overloaded with general theory, which could be improved by reorganizing the presentation.\n2. The usefulness of the results in practice is still an open question, as assessing the period of a given function is a challenging problem.\n\nHowever, the authors have addressed these weaknesses in their response, providing additional examples and discussions to support the results and clarifying the usefulness of their characterization result.\n\nIn conclusion, based on the positive reviewer comments, the authors' thorough response, and the significant contribution of the paper to the understanding of DNNs, I believe that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to coordinating multiple agents with pre-learned adaptable skills using a modular framework. The framework first trains sub-skills of each end-effector with skill behavior diversification and then learns to coordinate end-effectors using diverse behaviors of the skills.\n\nThe reviewers have provided constructive feedback, highlighting both the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they also raise concerns about the notations, the design choice of the fixed horizon of the low-level skills, and the high variance in performance.\n\nReviewer 2 questions the novelty of the work, asking whether the problem of temporal abstraction under the multi-agent setting is different from the single-agent setting. They also ask about the choice of the DIAYN method and the design choice of the fixed horizon of the low-level skills.\n\nReviewer 3 provides a weak acceptance, praising the approach as simple and scalable but criticizing the reliance on specifying subtasks in advance. They suggest that the paper would be improved by including a strong baseline that uses DIAYN only, without a priori subtasks.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional experiments and analysis. They have added two baselines, centralized and decentralized policies with Skill Behavior Diversification, to evaluate the importance of expert knowledge in the final performance. They have also provided more details on the design choice of the fixed horizon of the low-level skills and the size of the latent skill embedding.\n\nAfter carefully analyzing the paper, the reviewer comments, and the author responses, I believe that the paper has addressed the concerns raised by the reviewers. The authors have provided additional experiments and analysis to support their claims, and the paper presents a novel and interesting approach to coordinating multiple agents with pre-learned adaptable skills.\n\nThe paper's strengths include:\n\n* A novel approach to coordinating multiple agents with pre-learned adaptable skills\n* Empirical results that outperform the baselines\n* A clear and well-structured presentation\n\nThe paper's weaknesses include:\n\n* Some notations are inconsistent and confusing\n* The design choice of the fixed horizon of the low-level skills may not be optimal\n* The high variance in performance may be a concern\n\nHowever, the authors have addressed these weaknesses in their responses, and the paper has been improved as a result.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to coordinating multiple agents with pre-learned adaptable skills. The authors propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification and then learns to coordinate end-effectors using diverse behaviors of the skills.\n\nThe reviewers have provided constructive feedback, highlighting both the strengths and weaknesses of the paper. Reviewer 1 praises the paper for addressing an interesting problem and presenting empirical results that outperform the baselines. However, they also raise concerns about the notation, the design choice of the fixed horizon of the low-level skills, and the high variance in performance.\n\nReviewer 2 questions the need for treating temporal abstraction under the multi-agent setting differently from the single-agent setting and asks for clarification on the design choice of the fixed horizon of the low-level skills. They also request more discussion on alternative multi-agent methods.\n\nReviewer 3 suggests that the paper would be improved with a strong baseline that uses DIAYN only (no a priori subtasks) to evaluate the importance of expert knowledge to the final performance. They also ask for more information on the size of the latent skill embedding.\n\nThe authors have responded to the reviewers' comments, addressing the concerns and providing additional experiments and analysis. They have added two baselines, centralized and decentralized policies with Skill Behavior Diversification, to evaluate the importance of expert knowledge. They have also provided more information on the size of the latent skill embedding and conducted additional experiments to investigate the effect of the skill selection interval.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field of multi-agent reinforcement learning. The authors have presented a novel approach to coordinating multiple agents with pre-learned adaptable skills, and the empirical results demonstrate the effectiveness of the proposed framework.\n\nWhile the reviewers have raised some concerns, the authors have addressed them in their response, providing additional experiments and analysis to clarify the design choices and improve the paper. The paper has been revised to provide consistent notation, and the authors have added more discussion on alternative multi-agent methods.\n\nIn conclusion, based on the analysis of the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field and has addressed the concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" proposes a novel method for defending against adversarial attacks on deep neural networks. The method, called AE-GAN+sr, uses an autoencoder-based approach to purify input images by searching for a closest natural reconstruction in the latent coding space.\n\nThe paper has several strengths. Firstly, the proposed method shows promising results in defending against various types of adversarial attacks, including white-box, black-box, and gray-box attacks. The method is also shown to be computationally efficient, outperforming the Defense-GAN method in terms of computational cost. Additionally, the authors have conducted extensive experiments to evaluate the performance of their method, including comparisons with state-of-the-art adversarial training defenses such as PGD adversarial training.\n\nHowever, the paper also has some weaknesses. One of the main concerns is that the method relies heavily on the autoencoder's ability to detect adversarial samples, which can be susceptible to white-box attacks. The authors have addressed this concern by conducting additional experiments to demonstrate the effectiveness of their method against white-box attacks, but more work may be needed to fully address this issue. Another concern is that the paper could benefit from more clarity and readability, with some phrases and sentences being reworded for better understanding.\n\nThe reviewers have provided detailed and constructive feedback, highlighting both the strengths and weaknesses of the paper. Reviewer 1 praises the paper for its promising results and computational efficiency, but notes that the method may not be robust against stronger white-box attacks. Reviewer 2 suggests that the paper could benefit from more comparisons with state-of-the-art adversarial training defenses and notes that the method may not be effective against more complex datasets. Reviewer 3 raises concerns about the reliance on the autoencoder's ability to detect adversarial samples and suggests that more work may be needed to address this issue. Reviewer 4 notes that the paper could benefit from more clarity and readability and suggests that the authors provide more details about the implementation and training of the models.\n\nIn response to the reviewers' comments, the authors have provided detailed and thoughtful responses, addressing each of the concerns raised. They have also provided additional experiments and results to demonstrate the effectiveness of their method against various types of adversarial attacks.\n\nOverall, based on the strengths and weaknesses of the paper, as well as the reviewers' feedback and the authors' responses, I believe that the paper has the potential to make a significant contribution to the field of adversarial defense. While there are some concerns and areas for improvement, the authors have demonstrated a good understanding of the issues and have provided thoughtful responses to the reviewers' comments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Differentiable Hebbian Consolidation for Continual Learning\" proposes a novel approach to address the problem of catastrophic forgetting in neural networks. The authors introduce a Differentiable Hebbian Consolidation model, which combines a Differentiable Hebbian Plasticity (DHP) Softmax layer with a fixed (slow changing) parameter component. This approach enables the network to retain learned representations for a longer timescale and demonstrates flexibility by integrating well-known task-specific synaptic consolidation methods.\n\nThe reviewer comments and author responses provide valuable insights into the strengths and weaknesses of the paper. Reviewer 1 suggests that the empirical evaluation could be stronger, and the authors respond by adding more experiments, including the Split CIFAR-10/100 benchmark. Reviewer 2 raises questions about the notation and terminology used in the paper, and the authors provide clarifications and additions to improve the manuscript. Reviewer 3 praises the paper's well-written nature and the method's simplicity but raises concerns about the experimental section, which the authors address by adding more experiments and discussions.\n\nThe paper's strengths include:\n\n1. Novel approach: The proposed Differentiable Hebbian Consolidation model offers a fresh perspective on addressing catastrophic forgetting in neural networks.\n2. Flexibility: The method can be integrated with other task-specific synaptic consolidation methods, demonstrating its versatility.\n3. Improved performance: The authors show that their approach outperforms comparable baselines in various benchmarks, including Permuted MNIST, Split MNIST, and Vision Datasets Mixture.\n4. Well-written manuscript: The paper is well-organized, and the authors provide clear explanations of their method and experiments.\n\nHowever, the paper also has some weaknesses:\n\n1. Limited empirical evaluation: Although the authors add more experiments in response to reviewer comments, some reviewers still express concerns about the comprehensiveness of the empirical evaluation.\n2. Comparison to other methods: The authors could provide more detailed comparisons to other CLS-based approaches and rehearsal-based techniques to further demonstrate the effectiveness of their method.\n3. Hyperparameter tuning: The authors mention that the initial value of the Hebbian learning rate  is the only hyperparameter introduced by DHP Softmax, but they do not provide an extensive analysis of its impact on the results.\n\nAfter carefully considering the reviewer comments, author responses, and the paper's strengths and weaknesses, I conclude that the paper has the potential to make a valuable contribution to the field of continual learning. While there are some areas for improvement, the authors have addressed many of the concerns raised by the reviewers, and the paper's novelty, flexibility, and improved performance make it a strong candidate for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning to Transfer via Modelling Multi-level Task Dependency\" proposes a novel multi-task learning framework that models both general task dependency and data-specific task dependency using an attention mechanism. The authors claim that their approach can capture the hierarchical structure in text and graph data, which is not fully utilized by previous works.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. Reviewer 1 and Reviewer 2 appreciate the novelty of the approach, but point out that the paper requires thorough proof-reading to improve clarity and grammar. Reviewer 3 and Reviewer 4 raise concerns about the lack of clear distinction between the proposed approach and previous works, and the need for more experimental comparisons to demonstrate the effectiveness of the proposed method.\n\nThe author response addresses some of the concerns raised by the reviewers, providing more details on the motivation, methodology, and experimental setup. The authors also acknowledge the need for clearer presentation and more experimental comparisons.\n\nHowever, despite the author response, several concerns remain unaddressed. For example, Reviewer 4 points out that the paper is unaware of critically related prior work, such as \"Taskonomy: Disentangling Task Transfer Learning\", and that the proposed approach has strong similarities with this work. The author response does not fully address this concern, and the distinction between the proposed approach and previous works remains unclear.\n\nAdditionally, the experimental evaluation is limited, and the authors do not provide sufficient comparisons with other state-of-the-art methods. The results are also not thoroughly analyzed, and the authors do not provide a clear discussion of the implications of their findings.\n\nIn terms of the standards of a top-tier conference, the paper falls short in several areas. The writing is not clear and concise, and the presentation is not well-organized. The methodology is not thoroughly explained, and the experimental evaluation is limited. The paper also lacks a clear and concise conclusion, and the implications of the findings are not fully discussed.\n\nTherefore, based on the analysis of the paper and the reviewer comments, I conclude that the paper requires significant revisions to address the concerns raised by the reviewers and to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" has been submitted for publication at a top-tier conference. After carefully analyzing the paper's title, abstract, reviewer comments, and author responses, several critical issues have been identified that significantly impact the paper's suitability for publication.\n\nFirstly, the paper lacks clarity and readability, as highlighted by all three reviewers. Reviewer 1 describes the paper as \"less than half-baked\" and criticizes the absence of references. Reviewer 2 had a hard time understanding the paper due to grammatical issues and unclear writing, while Reviewer 3 found the paper to be \"very poorly written\" with many incomprehensible passages. The author's response acknowledges the writing issues and expresses a desire to improve, but this does not address the current state of the paper.\n\nSecondly, the paper fails to provide a proper literature review, which is a crucial component of academic research. The absence of references, as noted by Reviewer 1 and Reviewer 3, indicates a lack of engagement with existing research in the field. This omission makes it difficult to assess the paper's contribution and novelty.\n\nThirdly, the paper's technical content is unclear and lacks rigor. Reviewer 2 and Reviewer 3 mention the presence of undefined mathematical symbols, difficult-to-follow algorithm boxes, and a general lack of coherence. The author's response does not provide any clarification or defense of the technical aspects of the paper.\n\nLastly, the author's response, while appreciative of the reviewers' feedback, does not provide a convincing plan for addressing the paper's shortcomings. The author mentions a desire to improve writing skills and conduct more research but does not offer any specific solutions or revisions to address the current issues.\n\nGiven these significant concerns, it is clear that the paper is not ready for publication at a top-tier conference. The issues with writing quality, lack of references, unclear technical content, and insufficient author response all contribute to this conclusion.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Domain Adaptive Multibranch Networks\" proposes a novel approach to unsupervised domain adaptation, which is a crucial problem in computer vision. The authors introduce a deep learning framework that allows each domain to undergo a different sequence of operations, enabling more complex domains to go through more computations than others. This approach is in contrast to state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. Reviewer 1 praises the paper's motivation, related work, and experimental settings, but suggests minor improvements, such as linking to more transfer learning techniques and avoiding name clashes with established concepts. Reviewer 2 finds the idea of adaptive computation graphs for domain adaptation novel and technically sound, but raises concerns about the lack of experimentation with different amounts of computation per domain and the limited comparison to state-of-the-art methods. Reviewer 3 appreciates the paper's novelty and potential for multi-task and multi-source/multi-target domain adaptation, but criticizes the insufficient experiments and lack of comparison to state-of-the-art methods.\n\nThe authors' responses address the reviewers' concerns, providing additional experiments and comparisons to state-of-the-art methods. They also clarify the usage of terms, rename their approach to avoid name clashes, and update their manuscript to reflect the changes.\n\nThe paper's strengths include its novel approach to domain adaptation, its potential for handling multiple domains simultaneously, and its well-written and easy-to-follow presentation. The authors' responses demonstrate their willingness to address the reviewers' concerns and improve their work.\n\nHowever, the paper's weaknesses include the limited experimentation with different amounts of computation per domain, the lack of comparison to state-of-the-art methods, and the potential for name clashes with established concepts. Although the authors have addressed these concerns to some extent, further experimentation and comparison to state-of-the-art methods would strengthen the paper.\n\nConsidering the paper's strengths and weaknesses, as well as the authors' responses to the reviewers' comments, I believe that the paper has the potential to make a significant contribution to the field of computer vision. The authors have demonstrated their ability to address the reviewers' concerns and improve their work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents a machine learning approach to predict DNA folding patterns using Recurrent Neural Networks (RNNs), specifically bidirectional Long Short-Term Memory (LSTM) models. The authors compare the performance of their model with traditional machine learning methods, such as linear models and Gradient Boosting, and demonstrate that the LSTM model outperforms these methods.\n\nThe reviewers have raised several concerns and suggestions for improvement, which the authors have addressed in their response. The main concerns were:\n\n1. Lack of methodological novelty: Reviewer 3 pointed out that the paper is similar to previously published works and lacks methodological novelty. The authors responded by highlighting the differences between their work and previous studies, and emphasizing that their focus is on predicting the information that characterizes the 3D chromatin structure, rather than the Hi-C full reconstruction.\n2. Unclear description of data and methods: Reviewer 3 also pointed out that the description of the data and methods is not clear enough. The authors responded by expanding the description of the data and methods, and providing more details about the features used as input to the models.\n3. Limited evaluation: Reviewer 1 suggested that the evaluation is limited, and that the authors should consider additional baseline models and evaluation metrics. The authors responded by adding a broader set of evaluation metrics, including MSE, MAE, R^2, and weighted MSE score, and comparing their model with additional baseline models.\n4. Lack of generalizability: Reviewer 1 also pointed out that the approach lacks illustration of generalizability, and that the authors should consider testing their model on other Hi-C datasets or predicting other epigenetic features. The authors responded by explaining that they did not test their model on other datasets due to biological reasons, and that their focus is on predicting the information that characterizes the 3D chromatin structure in Drosophila.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has been improved significantly. The authors have addressed the concerns raised by the reviewers, and have provided more details about their methods and evaluation. The paper presents a clear and well-motivated approach to predicting DNA folding patterns using RNNs, and the results demonstrate the effectiveness of the proposed model.\n\nHowever, I still have some concerns about the generalizability of the approach, and the lack of comparison with other state-of-the-art methods. The authors should consider testing their model on other datasets, and comparing their results with other methods that have been proposed for predicting DNA folding patterns.\n\nIn conclusion, while the paper has been improved significantly, I still have some concerns about the generalizability and comparison with other methods. Therefore, I would like to see further revisions and improvements before making a final decision.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" presents a novel approach to the problem of Mahalanobis metric learning, formulating it as an optimization problem with the objective of minimizing the number of violated similarity/dissimilarity constraints. The authors propose a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time, leveraging tools from the theory of linear programming in low dimensions. The paper also includes experimental results on synthetic and real-world data sets, demonstrating the algorithm's performance and its ability to handle adversarial noise.\n\nSeveral strengths of the paper are noteworthy:\n1. **Novel Formulation and Approach**: The paper introduces a fresh perspective on Mahalanobis metric learning by directly minimizing the number of violated constraints, which is a highly non-convex problem. This approach differs from previous works that minimize an error function penalizing violations.\n2. **Theoretical Guarantees**: The authors provide a provable guarantee on the number of violated constraints for arbitrary inputs, which is a significant contribution. The use of LP-type problems and the application of Har-peled's result are innovative in this context.\n3. **Experimental Validation**: The inclusion of experimental results, especially those showing the algorithm's performance under adversarial conditions and its comparison with state-of-the-art methods like ITML and LMNN, strengthens the paper.\n\nHowever, there are also several concerns and areas for improvement raised by the reviewers:\n1. **Clarity and Presentation**: Reviewers noted that the paper could benefit from clearer explanations, especially regarding the novelty of the approach, the state-of-the-art in the field, and specific technical points such as the definition of combinatorial dimension and the proof of Lemma 2.1.\n2. **Comparison with State-of-the-Art**: While the paper mentions ITML and LMNN as benchmarks, a more detailed comparison with recent works (e.g., Verma and Branson, Ye et al.) and a clearer discussion on how the proposed method advances the state-of-the-art are needed.\n3. **Computational Hardness**: The reviewers suggested discussing the computational hardness of the problem, which the authors suspect to be NP-hard for unbounded dimension d. Clarifying this aspect would provide a better understanding of the problem's complexity and the significance of the proposed approximation algorithm.\n4. **Conclusion and Future Work**: The absence of a conclusion section and discussions on future work directions were noted. Adding these would enhance the paper's completeness and provide readers with a broader perspective on the research's implications and potential avenues for further investigation.\n\nThe authors' responses to the reviewers' comments address many of the concerns, providing additional context, clarifying technical points, and acknowledging areas where the paper could be improved. They highlight the novelty of their approach, the significance of achieving a provable guarantee on the number of violated constraints, and the distinction of their work from previous studies.\n\nConsidering the strengths of the paper, including its novel approach, theoretical contributions, and experimental validation, along with the authors' thoughtful responses to the reviewers' comments, it appears that the paper has the potential to make a significant contribution to the field of metric learning. While there are areas for improvement, particularly in terms of clarity, comparison with recent state-of-the-art methods, and discussion of computational hardness, these can be addressed through revisions.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Interaction Processes for Time-Evolving Graphs\" proposes a novel approach to modeling continuous time-evolving graphs using a temporal point process framework. The authors introduce a principled deep neural approach that models the dependency between latent dynamic representations of each node, and they define a mixture of temporal cascades to capture the interactions between nodes.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. Reviewer 1 praises the paper's empirical performance on multiple datasets but raises concerns about the novelty and justification of the contributions. Reviewer 2 notes that the paper's contributions, while promising, are not particularly novel, and that the experimental results, although interesting, require more detailed analysis. Reviewer 3 finds the paper difficult to understand and requests more clarification on the temporal point process framework and the handling of growing/shrinking graph nodes and changing interactions.\n\nThe authors' response addresses many of the reviewers' concerns, providing additional explanations, justifications, and experimental results. They clarify the novelty of their approach, provide more details on the temporal point process framework, and offer insights into the performance of their method.\n\nAfter carefully analyzing the paper, reviewer comments, and author responses, I conclude that the paper has several strengths:\n\n1. **Novel approach**: The paper proposes a novel approach to modeling time-evolving graphs, which combines temporal point processes with a recurrent architecture.\n2. **Empirical performance**: The paper demonstrates improved empirical performance on multiple datasets, including a real-world financial application.\n3. **Clear response**: The authors provide a clear and detailed response to the reviewer comments, addressing many of the concerns and providing additional explanations and justifications.\n\nHowever, I also identify some weaknesses:\n\n1. **Lack of novelty**: Some reviewers question the novelty of the paper's contributions, noting that similar approaches have been proposed in prior work.\n2. **Complexity**: The paper's approach is complex, and some reviewers find it difficult to understand.\n3. **Limited analysis**: Some reviewers request more detailed analysis of the model and its performance.\n\nConsidering these strengths and weaknesses, I believe that the paper has the potential to make a significant contribution to the field. While the novelty of the approach may be debated, the empirical performance and clear response from the authors demonstrate the paper's value.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Complex Query Answering with Neural Link Predictors\" proposes a novel framework for efficiently answering complex queries on incomplete Knowledge Graphs (KGs). The authors introduce Continuous Query Decomposition (CQD), a method that translates each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. The paper presents two solutions to the optimization problem: gradient-based and combinatorial search.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The reviewers praise the paper's elegance, efficiency, and state-of-the-art results. However, they also raise concerns about the paper's clarity, the choice of neural link predictor, and the timing results.\n\nAfter carefully analyzing the paper and the reviewers' comments, I identify the following strengths and weaknesses:\n\nStrengths:\n\n1. **Novel approach**: The paper proposes a novel framework for complex query answering, which is different from existing methods that embed queries into a low-dimensional space.\n2. **State-of-the-art results**: The paper presents state-of-the-art results on several benchmarks, outperforming existing methods with less training data.\n3. **Explainability**: The paper provides aspects of explainability, which is essential for understanding the model's decisions.\n4. **Efficient optimization**: The paper introduces two efficient optimization methods, gradient-based and combinatorial search, which can be used to answer complex queries.\n\nWeaknesses:\n\n1. **Clarity**: Some reviewers found the paper's notation and mathematical formalism excessive and unclear.\n2. **Choice of neural link predictor**: The paper uses ComplEx as the neural link predictor, which may not be the best choice for all scenarios. The authors should provide more justification for this choice or experiment with other predictors.\n3. **Timing results**: The paper lacks detailed timing results, which are essential for understanding the model's efficiency.\n4. **Comparison to existing methods**: The paper could provide more detailed comparisons to existing methods, including GQE and Q2B, to demonstrate the advantages of the proposed approach.\n\nTo address these weaknesses, the authors have provided additional experiments, clarifications, and justifications in their response. They have also updated their paper to include more detailed timing results, comparisons to existing methods, and explanations for their choice of neural link predictor.\n\nBased on the analysis of the paper and the reviewers' comments, I conclude that the paper has made significant contributions to the field of complex query answering on incomplete KGs. The authors have addressed the reviewers' concerns and provided additional experiments and clarifications to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Coherent Exploration For Continuous Control\" presents a novel exploration framework for deep reinforcement learning (RL) algorithms on continuous control tasks. The framework, called Deep Coherent Exploration, generalizes step-based and trajectory-based exploration by modeling the last layer parameters of the policy network as latent variables and using a recursive inference step within the policy update.\n\nThe reviewers have provided detailed comments on the paper, highlighting both its strengths and weaknesses. The main strengths of the paper include:\n\n1. The paper proposes a novel and scalable exploration framework for deep RL algorithms, which is a significant contribution to the field.\n2. The framework is well-motivated and clearly explained, with a good introduction to the relevant concepts and a thorough discussion of the related work.\n3. The empirical results are strong, showing that Deep Coherent Exploration improves the speed and stability of learning of A2C, PPO, and SAC on several continuous control tasks.\n4. The paper provides detailed ablation studies, which help to understand the contribution of each component of the proposed method.\n\nHowever, the reviewers have also raised some concerns, including:\n\n1. The paper's relationship to prior work, particularly van Hoof et al. (2017), is not fully clear. The authors could have done a better job of explaining how their work builds upon or differs from prior work.\n2. The experimental results for SAC are not as strong as those for A2C and PPO, which raises questions about the effectiveness of the proposed method for off-policy learning.\n3. The paper could benefit from more detailed explanations of some of the technical aspects, such as the motivation for the SAC setting and the connection to equation (5).\n4. The experiments are limited to MuJoCo tasks, which may not be representative of all continuous control tasks.\n\nIn response to the reviewers' comments, the authors have provided a detailed rebuttal, addressing each of the concerns raised. They have also updated the manuscript to clarify some of the technical aspects and to provide more detailed explanations.\n\nBased on the reviewers' comments and the authors' rebuttal, I believe that the paper has made a significant contribution to the field of reinforcement learning and that the strengths of the paper outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents a study on reducing the number of supervised synaptic updates required to achieve a brain-like model of the primate ventral stream. The authors propose three complementary strategies: reducing the number of supervised updates, improving the random distribution of synaptic connectivity, and training only a subset of model synapses.\n\nThe reviewers have raised several concerns and questions about the paper, including the interpretation of the Brain-Score metric, the biological plausibility of the proposed methods, and the significance of the results. The authors have responded to these concerns, providing additional explanations and justifications for their approach.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has several strengths, including:\n\n1. The paper addresses an important problem in the field of computational neuroscience, namely the need to reduce the number of supervised synaptic updates required to achieve a brain-like model of the primate ventral stream.\n2. The authors propose a novel approach to initializing the weights of a neural network, which shows promise in reducing the number of supervised updates required.\n3. The paper presents a thorough analysis of the results, including statistical tests and comparisons to other methods.\n\nHowever, the paper also has some weaknesses, including:\n\n1. The interpretation of the Brain-Score metric is not entirely clear, and the authors could provide more explanation and justification for their use of this metric.\n2. The biological plausibility of the proposed methods is not fully established, and the authors could provide more discussion and analysis of the potential limitations and implications of their approach.\n3. Some of the results and figures could be improved for clarity and readability.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field of computational neuroscience, but it requires some revisions to address the concerns and questions raised by the reviewers.\n\nThe authors have provided a detailed response to the reviewer comments, which addresses many of the concerns and questions raised. They have also provided additional explanations and justifications for their approach, which helps to clarify the strengths and limitations of their method.\n\nBased on the analysis of the paper and the reviewer comments, I believe that the paper should be accepted for publication, but with some revisions to address the concerns and questions raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review proposes a generalized probability kernel (GPK) on discrete distributions with finite support, which generalizes existing discrepancy statistics such as maximum mean discrepancy (MMD) and kernel Stein discrepancy (KSD). The authors aim to connect the fields of discrete distribution-property estimation and kernel-based hypothesis testing.\n\nHowever, upon careful analysis of the paper, reviewer comments, and author responses, several major concerns arise that significantly impact the paper's quality and validity.\n\n1. **Lack of Clarity and Consistency**: Reviewers have pointed out numerous instances of confusing notation, unclear definitions, and inconsistencies in the presentation. For example, the use of \"K\" to represent different concepts (kernel between distributions, kernel between values, and gram matrix) is confusing. Although the authors have attempted to address these issues in their response, the lack of clarity and consistency in the initial submission raises concerns about the paper's overall quality.\n\n2. **Theoretical Flaws and Missing Proofs**: Reviewers have identified several theoretical flaws, including the missing proof of Theorem 5, unclear definitions (e.g., Definition 2 and Definition 6), and the lack of analysis showing the usefulness of the proposed estimators. The authors' response acknowledges some of these issues, such as the incorrect proof of Theorem 7 related to KSD, which has been removed. However, the removal of significant parts of the paper without adequate replacement or clarification weakens the submission.\n\n3. **Insufficient Experimental Evaluation**: The paper lacks experimental evaluation, which is a critical component of any research paper, especially in the field of machine learning and statistics. The authors' response does not provide a clear plan for addressing this significant omission.\n\n4. **Questionable Motivation and Relevance**: Some reviewers have questioned the motivation behind the proposed GPK, particularly in the context of discrete distributions where labels may not relate to any notion of distance or ordering. The authors' response attempts to address this by mentioning the word2vec technique in NLP tasks, but this example may not be directly relevant to the broader applicability of GPK.\n\n5. **Typos, Imprecisions, and Lack of Rigor**: The paper is criticized for having numerous typos, imprecisions, and a lack of rigor in introducing mathematical concepts. While the authors acknowledge these issues and promise to correct them, the initial submission's poor quality reflects poorly on the authors' attention to detail and professionalism.\n\nGiven these significant concerns, it is clear that the paper, in its current form, does not meet the standards expected of a top-tier conference. The authors have attempted to address some of the reviewers' comments, but the revisions and responses provided do not adequately alleviate the major issues identified.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Disentangling Representations of Text by Masking Transformers\" presents a novel approach to learning disentangled representations from pre-trained language models. The method involves learning binary masks over transformer weights or hidden units to identify subnetworks that encode distinct, complementary aspects of the representation. The authors evaluate their approach on two tasks: disentangling syntax and semantics, and sentiment from genre in movie reviews.\n\nThe reviewer comments highlight several strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novel approach**: The paper proposes a new and interesting approach to disentangling representations, which is a key challenge in natural language processing.\n2. **Well-written and easy to follow**: The paper is well-organized, and the authors provide clear explanations of their method and experiments.\n3. **Convincing results**: The authors present convincing results on two tasks, demonstrating the effectiveness of their approach in disentangling representations.\n\nHowever, the reviewers also raise several concerns and questions, including:\n\n1. **Lack of generality**: The authors only evaluate their approach on two tasks and two datasets, which may not be representative of the broader range of NLP tasks and datasets.\n2. **Comparison to previous work**: The authors do not provide a comprehensive comparison to previous work on disentangling representations, including variational autoencoders and adversarial training.\n3. **Experimental setup**: The authors' experimental setup is not entirely convincing, and some reviewers question the choice of datasets and tasks.\n4. **Interpretability of results**: Some reviewers find the results difficult to interpret, particularly with regards to the degree of disentanglement achieved by the proposed approach.\n\nThe authors respond to these concerns and questions in their rebuttal, providing additional context and clarification on their approach and experiments. They also acknowledge the limitations of their work and propose potential future directions for improvement.\n\nAfter carefully considering the paper, reviewer comments, and author responses, I believe that the paper has several strengths, including its novel approach, well-written presentation, and convincing results. However, the paper also has some weaknesses, including the limited scope of the evaluation and the lack of comprehensive comparison to previous work.\n\nTo address these weaknesses, I would suggest that the authors:\n\n1. **Conduct additional experiments**: Evaluate the proposed approach on a broader range of NLP tasks and datasets to demonstrate its generality and effectiveness.\n2. **Provide a comprehensive comparison**: Compare the proposed approach to previous work on disentangling representations, including variational autoencoders and adversarial training, to provide a more complete understanding of its strengths and limitations.\n3. **Improve the experimental setup**: Consider alternative datasets and tasks to evaluate the proposed approach, and provide more detailed information about the experimental setup and hyperparameter choices.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field of natural language processing, but requires some revisions to address the concerns and questions raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method for multitask learning, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), which dynamically adjusts task weights during model training. The method is based on the idea of using task-specific updates of the model's parameters to estimate the positive or negative influence of auxiliary tasks on the target task.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical justification for the proposed algorithm, the limited experimental results, and the similarity to existing methods. Reviewer 1 suggests that the paper needs more theoretical analysis and justification for the proposed methodology, and that the experimental results are not convincing enough to justify the proposed method. Reviewer 2 questions the novelty of the proposed method and suggests that it is similar to existing methods, such as meta-learning of task weights. Reviewer 3 is not convinced by the proposed algorithm and suggests that it is not superior to existing methods.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and justifications for their method. They acknowledge the lack of theoretical justification and suggest that they will remedy this in the camera-ready version. They also provide additional experimental results and comparisons to existing methods, which show that their method can outperform existing methods in certain settings.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has some merits, but it also has some significant limitations. The proposed method is novel and has some potential, but it lacks theoretical justification and the experimental results are limited. The authors have not fully addressed the concerns raised by the reviewers, and the paper needs more work to be considered for publication in a top-tier conference.\n\nThe main concerns that need to be addressed are:\n\n1. Lack of theoretical justification: The authors need to provide a more detailed theoretical analysis of their method, including proofs and derivations, to justify its effectiveness.\n2. Limited experimental results: The authors need to provide more extensive experimental results, including comparisons to existing methods, to demonstrate the effectiveness of their method.\n3. Similarity to existing methods: The authors need to provide a more detailed comparison to existing methods, such as meta-learning of task weights, to demonstrate the novelty and superiority of their method.\n\nIn conclusion, while the paper has some potential, it needs significant revisions to address the concerns raised by the reviewers. The authors need to provide more theoretical justification, more extensive experimental results, and a more detailed comparison to existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"New Bounds For Distributed Mean Estimation and Variance Reduction\" presents a novel approach to distributed mean estimation and variance reduction, leveraging lattice theory to achieve improved bounds on communication complexity. The authors provide a thorough analysis of the problem, including a clear description of the setting, related work, and technical contributions.\n\nThe paper's main strengths include:\n\n1. **Novel approach**: The authors introduce a new method for distributed mean estimation, exploiting the structure of lattices to reduce communication complexity. This approach is innovative and has the potential to impact the field of distributed machine learning.\n2. **Theoretical guarantees**: The paper provides rigorous theoretical analysis, including upper and lower bounds on communication complexity, demonstrating the optimality of the proposed algorithm.\n3. **Experimental evaluation**: The authors conduct experiments to demonstrate the practical effectiveness of their approach, showing improvements over prior methods.\n\nHowever, the paper also has some weaknesses and areas for improvement:\n\n1. **Clarity and presentation**: Some reviewers found the writing to be unclear or dense, suggesting that the authors could improve the presentation to make the paper more accessible to a broader audience.\n2. **Comparison to prior work**: While the authors discuss related work, some reviewers felt that the comparison to prior approaches could be more thorough, particularly with regard to the straightforward approach suggested by Reviewer 1.\n3. **Experimental setup**: The experimental evaluation is limited to a specific setup, and some reviewers suggested that additional experiments with larger numbers of machines or different scenarios would be beneficial.\n\nIn response to the reviewer comments, the authors have provided a detailed and thoughtful rebuttal, addressing the concerns and suggestions raised by the reviewers. They have clarified the presentation, provided additional context and explanations, and acknowledged the limitations of their experimental evaluation.\n\nConsidering the strengths and weaknesses of the paper, as well as the authors' response to the reviewer comments, I believe that the paper has the potential to make a significant contribution to the field of distributed machine learning.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents a novel approach to integrating linguistic and visual information for image segmentation tasks based on referring expressions. The authors argue that using language to modulate both top-down and bottom-up visual processing can improve performance over traditional methods that only consider top-down attention. The paper includes experiments on several datasets, demonstrating significant improvements when language is used to control filters for bottom-up visual processing in addition to top-down attention.\n\nThe reviewer comments provide a range of perspectives on the paper's strengths and weaknesses. Reviewer 1 suggests that the novelty and contribution of the paper are thin, citing a lack of exploration of different language modeling approaches. However, they also provide constructive suggestions for improving the paper, including adding figures to explain key concepts and citing relevant papers. Reviewer 2 raises concerns about the empirical results, noting that the full model struggles to match existing works on certain metrics and suggesting possible over-tuning. Reviewer 3 is more positive, praising the paper's well-written presentation, important question, and impressive qualitative examples. Reviewer 4 votes for rejection, citing poor conceptual motivation, insufficient error analysis, and a lack of clear understanding of the model's contributions.\n\nThe authors' responses to the reviewer comments address several of the concerns raised. They provide additional explanations for their model design choices, clarify their experimental methodology, and offer to conduct further analyses to better understand the interaction between language and visual processing. However, some reviewers remain unconvinced, questioning the significance of the empirical results and the originality of the model.\n\nAfter carefully considering the paper and the reviewer comments, several key points emerge:\n\n1. **Novelty and Contribution**: While some reviewers question the novelty of the approach, the authors' use of language to modulate both top-down and bottom-up visual processing represents a distinct contribution to the field. The ablation studies demonstrating the importance of both modalities for improved performance support this claim.\n\n2. **Empirical Results**: The paper presents competitive or state-of-the-art results on several datasets, which is a significant achievement. However, concerns about over-tuning and the variability of results across different evaluation sets are valid and should be addressed through additional analyses or significance tests.\n\n3. **Conceptual Motivation and Error Analysis**: The authors could strengthen their paper by providing a clearer conceptual motivation for their model design, potentially drawing more explicitly from cognitive science literature on language and visual processing. Additionally, more detailed error analyses or visualizations of how language influences visual processing would enhance the paper's impact.\n\n4. **Originality and Comparison to Existing Work**: The comparison to Step-ConvRNN and other models is crucial. The authors argue that their model is distinct and more suitable for ablation studies, which is a reasonable point. However, a more detailed comparison, potentially including architectural diagrams or a more in-depth discussion of how their model differs from and improves upon existing approaches, would be beneficial.\n\nConsidering these points, the decision to accept or reject the paper hinges on the balance between its contributions, the quality of its empirical results, and the clarity of its conceptual motivation and analysis. While the paper has its weaknesses, particularly in terms of providing a deeper understanding of how language modulates visual processing and addressing concerns about over-tuning, its strengths in terms of novelty, empirical performance, and the importance of its research question are significant.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Cortico-cerebellar networks as decoupled neural interfaces\" presents a novel hypothesis that the cerebellum acts as a decoupled neural interface (DNI) to facilitate learning in the cerebral cortex. The authors propose a framework, called cortico-cerebellar-DNI (CC-DNI), which combines the concepts of DNIs and cerebellar forward and inverse models to address the credit assignment problem in neural networks.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewer 1 praises the paper's intriguing hypothesis and potential to stimulate discussion, but raises concerns about clarity, particularly in the presentation of the model and its relationship to existing work on DNIs. Reviewer 2 is more critical, arguing that the paper does not offer new insights into either the cerebellum or DNIs, and that the results are largely a juxtaposition of existing knowledge. Reviewer 3 provides a balanced assessment, acknowledging the paper's clarity and the potential of the CC-DNI model, but also highlighting the need for more comparisons with experimental findings and concrete testable predictions.\n\nThe authors' response addresses many of the reviewers' concerns, including the need for clearer presentation, more explicit discussion of bootstrapping, and additional experiments to demonstrate the model's predictions. The authors provide new figures and a new section that highlight the model's predictions and compare them to existing experimental findings.\n\nAfter careful consideration of the paper, the reviewers' comments, and the authors' response, I believe that the paper has been substantially improved. The authors have addressed the major concerns raised by the reviewers, and the revised paper provides a clearer and more compelling presentation of the CC-DNI model and its potential contributions to the field.\n\nThe paper's strengths include its novel hypothesis, clear writing, and potential to stimulate discussion and new research directions. The authors' response demonstrates a willingness to engage with criticism and improve the paper, which is a positive sign.\n\nHowever, I also acknowledge that the paper may not entirely satisfy the concerns of all reviewers, particularly Reviewer 2, who remains skeptical about the paper's novelty and impact. Nevertheless, I believe that the paper's strengths outweigh its weaknesses, and that it has the potential to make a significant contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Structure and randomness in planning and reinforcement learning\" introduces a novel method called Shoot Tree Search (STS), which aims to balance the trade-off between depth and breadth of search in planning and reinforcement learning. The authors claim that STS can achieve better performance than standard Monte Carlo Tree Search (MCTS) and other baseline methods in various domains.\n\nThe reviewers have raised several concerns and questions about the paper, including:\n\n1. Technical novelty: Some reviewers question whether the idea of multi-step expansion in STS is truly novel, citing existing works that have explored similar concepts.\n2. Comparison to MCTS: Reviewers have asked for more thorough comparisons between STS and MCTS, including experiments with different hyperparameters and settings.\n3. Intuition and explanation: Reviewers have requested more intuitive explanations for why STS works better than MCTS in certain scenarios.\n4. Related work: Reviewers have pointed out that the paper could benefit from a more thorough discussion of related work, including existing methods that combine MCTS with rollouts.\n5. Experimental results: Reviewers have asked for more detailed and thorough experimental results, including ablation studies and comparisons to other baseline methods.\n\nThe authors have responded to these concerns and questions, providing additional experimental results and explanations. They have also acknowledged the similarities between STS and existing methods, but argue that their approach is distinct and offers advantages in certain scenarios.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has some merits, but also has some significant weaknesses. The idea of multi-step expansion in STS is interesting, and the authors have provided some evidence that it can lead to improved performance in certain domains. However, the paper could benefit from more thorough comparisons to existing methods, including MCTS and other baseline approaches. Additionally, the authors could provide more intuitive explanations for why STS works better than MCTS in certain scenarios.\n\nFurthermore, I am concerned that the paper may not be suitable for a top-tier conference, given the weaknesses and limitations identified by the reviewers. While the authors have made an effort to address these concerns, I believe that the paper requires more significant revisions and improvements to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"GG-GAN: A Geometric Graph Generative Adversarial Network\" proposes a novel approach to graph generation using a geometric perspective and a Wasserstein GAN architecture. The paper is well-written, and the authors provide a clear and concise explanation of their methodology.\n\nHowever, upon analyzing the reviewer comments, several concerns arise that need to be addressed. Reviewer 1 raises concerns about the numerical evidence section, specifically the small size of the datasets used and the lack of convincing results in Table 1. The reviewer suggests that the authors should compare their method with other graph generators, such as NetGAN, and provide more comprehensive results on medium and large-sized graphs.\n\nReviewer 2 has several concerns, including the lack of discussion on mode collapse/generation diversity, the inability of the proposed method to model complex edge dependencies, and the weakness of Proposition 1. The reviewer also suggests that the authors should provide more evidence to support their claims and clarify the sampling procedure.\n\nReviewer 3 finds the paper inspiring and well-organized, but Reviewer 4 raises concerns about the paper's contributions, literature review, and experimental results. The reviewer suggests that the authors should provide a more comprehensive literature review and clarify the key innovations of their method.\n\nReviewer 5 raises concerns about the experiments, specifically the small dimension of the graphs used and the lack of convincing results. The reviewer also suggests that the authors should clarify the introduction of  and its implications.\n\nGiven the concerns raised by the reviewers, it is clear that the paper has some significant weaknesses that need to be addressed. The authors should provide more comprehensive numerical evidence, clarify their methodology, and address the concerns raised by the reviewers.\n\nHowever, the paper also has some strengths, such as its novelty and the fact that it is well-written. The authors propose a new approach to graph generation, and their method has the potential to be useful in practice.\n\nOverall, based on the reviewer comments and the paper's content, I believe that the paper needs significant revisions to address the concerns raised by the reviewers. Therefore, my final decision is:\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper $\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning proposes a novel method for multitask learning that dynamically adjusts task weights during model training. The reviewers have provided detailed comments and suggestions, which the authors have addressed in their response.\n\nThe main concerns raised by the reviewers are:\n\n1. Lack of theoretical justification: The reviewers have pointed out that the paper lacks a clear theoretical motivation for the proposed algorithm. The authors have acknowledged this and intend to remedy it in the camera-ready version.\n2. Weak results: The reviewers have noted that the results are not convincing, with minor improvements over the baselines. The authors have provided some context, explaining that improvements in multitask learning on NLU tasks are often small, and that their method is less prone to overfitting on the development data.\n3. Need for more experiments and ablation studies: The reviewers have suggested that additional experiments and ablation studies are needed to demonstrate the efficacy of the proposed method. The authors have agreed to add these to the camera-ready version or appendices.\n\nThe authors have provided a clear and well-written response, addressing the concerns and suggestions raised by the reviewers. They have also provided some additional context and explanations for their results.\n\nHowever, despite the authors' efforts to address the concerns, the reviewers' comments and the authors' response still raise some significant issues. The lack of theoretical justification and the weak results are major concerns that need to be addressed. While the authors have provided some context and explanations, they have not fully alleviated these concerns.\n\nFurthermore, the reviewers have pointed out that the proposed method is not significantly different from existing methods, such as dynamic task weighting and meta-learning algorithms. The authors have attempted to distinguish their method from these existing approaches, but the reviewers' comments suggest that the differences may not be substantial.\n\nIn light of these concerns and the reviewers' comments, I believe that the paper requires significant revisions to address the theoretical justification, experimental results, and differentiation from existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method for multitask learning, called $\\alpha$-Variable Importance Learning ($\\alpha$VIL), which adjusts task weights dynamically during model training. The method is based on the idea of using task-specific updates of the underlying model's parameters between training epochs to estimate the positive or negative influence of auxiliary tasks on the target task.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical justification for the proposed algorithm, the limited experimental results, and the similarity to existing methods. Reviewer 1 suggests that the paper needs more theoretical analysis and justification for the proposed methodology, and that the experimental results are not convincing enough to justify the proposed method. Reviewer 2 questions the novelty of the proposed method and suggests that it is similar to existing methods, such as meta-learning of task weights. Reviewer 3 is not convinced by the proposed algorithm and suggests that it is not superior to previous methods.\n\nThe authors have responded to the reviewers' comments, providing additional explanations and justifications for their method. They acknowledge the lack of theoretical justification and suggest that they will remedy this in the camera-ready version. They also provide additional experimental results and comparisons to existing methods, which show that their method can outperform other multitask learning approaches in certain settings.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has some merits, but it also has some significant limitations. The proposed method is novel and has the potential to be useful in certain settings, but it lacks theoretical justification and the experimental results are limited. The authors have provided some additional explanations and justifications, but more work is needed to fully address the reviewers' concerns.\n\nTherefore, I would recommend rejecting the paper in its current form. However, I would encourage the authors to revise and resubmit the paper, addressing the concerns raised by the reviewers and providing more theoretical justification and experimental results to support their method.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Ecological Inference\" presents an interesting approach to ecological inference, a problem of significant importance in political science and other fields. The authors propose a deep learning framework to approximate ecological inference, which enables the estimation of individual labels from aggregated data. The paper evaluates the proposed model using real-world data from the Maryland 2018 midterm elections and reports promising results.\n\nHowever, upon careful analysis of the reviewer comments, several concerns arise that impact the paper's suitability for publication in a top-tier conference. \n\n1. **Clarity and Readability**: Multiple reviewers (Reviewer 1, Reviewer 2, and Reviewer 4) mention that the paper is not easy to follow, with suggestions for improving the structure, particularly in the introduction of key concepts and the description of input data and evaluation tasks. This lack of clarity hampers the understanding and assessment of the paper's contributions.\n\n2. **Experimental Analysis and Baselines**: Reviewer 2 criticizes the paper for incomplete and unclear experiments, lacking key analyses and baselines. For instance, the comparison to a standard multi-level model (MLM) as a baseline is missing, which is crucial for understanding the improvements brought by the deep MLM. Additionally, the paper lacks detailed experimental settings, such as how train/test splits are created, the specifics of neural network architectures, and the choice of hyperparameters.\n\n3. **Reproducibility**: The absence of detailed experimental settings and hyperparameter choices, as pointed out by Reviewer 2 and Reviewer 4, hampers the reproducibility of the results. This is a significant concern for any research paper, especially in a field like machine learning where experimental verification is crucial.\n\n4. **Novelty and Related Work**: Reviewer 4 questions the novelty of the proposed approach, suggesting that it may not be new and that related work is not adequately cited. This includes connections to other problems like learning with label proportions, distribution regression, multiple-instance learning, and collective graphical models. A thorough discussion of related work and how the proposed method advances the state-of-the-art is essential.\n\n5. **Typos and Minor Errors**: Several reviewers mention the presence of typos and minor writing issues throughout the paper. While these are not critical, they reflect a lack of thorough editing and attention to detail.\n\nGiven these concerns, despite the paper's interesting approach and important research area, the issues with clarity, experimental analysis, reproducibility, novelty, and presentation are significant enough to warrant rejection. The authors need to address these concerns through revisions that improve the paper's clarity, provide more comprehensive and clear experimental analyses, ensure reproducibility, adequately discuss related work, and correct minor errors.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical insights from few-shot learning and the practical performance of meta-learning algorithms. The authors propose two regularization terms based on recent theoretical advances in meta-learning and demonstrate their effectiveness in improving the generalization capacity of popular meta-learning algorithms on few-shot classification benchmarks.\n\nThe reviewers have raised several concerns and questions about the paper, including the novelty of the proposed regularization terms, the significance of the improvement over vanilla meta-learning, the consistency of the theoretical setting with the few-shot learning setting, and the lack of comparison with recent methods.\n\nThe authors have responded to these concerns, clarifying that their goal is not to propose a novel regularization that outperforms state-of-the-art few-shot classification methods but rather to study whether current theoretical results agree with the real-world behavior of popular meta-learning algorithms. They have also provided additional explanations and justifications for their approach, including the difference between their regularization terms and existing methods like weight decay and spectral normalization.\n\nAfter carefully considering the paper, the reviewer comments, and the author responses, I believe that the paper makes a valuable contribution to the field of meta-learning by providing a rigorous analysis of the theoretical foundations of few-shot learning and demonstrating the practical effectiveness of these insights. While the reviewers have raised some valid concerns, the authors have adequately addressed these concerns and provided additional clarifications and justifications for their approach.\n\nThe paper's strengths include its clear and well-organized presentation, its thorough analysis of the theoretical foundations of few-shot learning, and its demonstration of the practical effectiveness of the proposed regularization terms. The paper's weaknesses include the lack of novelty in the proposed regularization terms and the limited comparison with recent methods. However, the authors have acknowledged these limitations and provided additional explanations and justifications for their approach.\n\nOverall, I believe that the paper makes a significant contribution to the field of meta-learning and provides valuable insights into the theoretical foundations of few-shot learning. While the paper may not be perfect, its strengths outweigh its weaknesses, and it has the potential to inspire further research and improvements in the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel method, PERIL, which combines meta-reinforcement learning and imitation learning to enable efficient adaptation to new tasks. The method uses probabilistic embeddings to precondition exploration policies on demonstrations, allowing for improved adaptation rates in unseen tasks.\n\nThe reviewers have provided detailed and thoughtful feedback, highlighting both the strengths and weaknesses of the paper. The main strengths of the paper include:\n\n1. The proposed method, PERIL, shows promising results in adapting to new tasks, especially in sparse reward settings.\n2. The paper addresses an important problem in reinforcement learning, which is the ability to adapt to new tasks with limited data and exploration.\n3. The authors have provided a clear and well-organized paper, with a good introduction to the problem and related work.\n\nHowever, the reviewers have also identified several weaknesses and areas for improvement:\n\n1. The paper assumes that an expert distribution is available, which may not always be the case in real-world scenarios.\n2. The problem formulation mixes the approach with the problem setting, making it unclear what is a constraint due to the setting and what is a design choice for the approach.\n3. The paper lacks clarity in some sections, particularly in the methods and experiments sections.\n4. The reviewers have requested additional baselines and ablations to further understand the contributions of the different components in PERIL.\n5. The paper could benefit from more detailed explanations of the related work and a clearer comparison to existing methods.\n\nConsidering the feedback from the reviewers, it is clear that the paper has potential but requires significant revisions to address the weaknesses and areas for improvement. The authors have acknowledged the feedback and have decided to postpone their submission to incorporate the suggested changes.\n\nBased on the analysis of the paper and the feedback from the reviewers, I conclude that the paper is not yet ready for publication in its current form. However, with significant revisions to address the weaknesses and areas for improvement, the paper has the potential to be a strong contribution to the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel framework for automatic music accompaniment using generative adversarial networks (GANs) and Mel-spectrograms. The authors claim that their approach can generate high-quality music accompaniments for given melodies. However, the paper has several limitations and areas for improvement, which are highlighted by the reviewer comments.\n\nOne of the main concerns is the lack of clarity and specificity in the paper's title, abstract, and introduction. The title is misleading, and the abstract does not accurately reflect the content of the paper. The introduction lacks a clear motivation for the research and does not provide a thorough overview of the related work in the field.\n\nThe methodology section is also lacking in detail, particularly with regards to the Demucs algorithm used for source separation and the CycleGAN architecture. The authors do not provide sufficient information about the hyperparameters, training procedures, and evaluation metrics used in their experiments.\n\nThe experimental results are promising, but the evaluation metrics used are not well-justified, and the results are not thoroughly analyzed. The authors also do not provide a clear comparison with existing methods or a thorough discussion of the limitations and potential biases of their approach.\n\nThe reviewer comments also highlight several other issues, including the lack of reproducibility, the limited scope of the experiments, and the need for more rigorous evaluation and analysis.\n\nIn response to the reviewer comments, the authors have provided additional explanations and justifications for their approach. However, they have not fully addressed all the concerns raised by the reviewers.\n\nConsidering the limitations and areas for improvement highlighted by the reviewer comments, I believe that the paper requires significant revisions before it can be considered for publication.\n\nThe authors need to address the following issues:\n\n1. Clarify and specify the title, abstract, and introduction to accurately reflect the content and contributions of the paper.\n2. Provide more detailed information about the methodology, including the Demucs algorithm, CycleGAN architecture, and evaluation metrics.\n3. Conduct more thorough experiments and analysis, including a comparison with existing methods and a discussion of the limitations and potential biases of their approach.\n4. Improve the reproducibility of the results by providing more details about the datasets, experimental settings, and hyperparameters used.\n5. Address the concerns raised by the reviewers regarding the evaluation metrics, experimental design, and potential limitations of the approach.\n\nUntil these issues are addressed, I believe that the paper is not ready for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to training deep neural networks using a block minifloat representation, which combines the benefits of minifloats and block floating-point representations. The authors demonstrate the effectiveness of their approach through extensive experiments on various deep learning models and datasets, including ResNet and ImageNet.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. Reviewer 1 raises several questions and concerns, including the novelty of the shared-exponent bias concept, the impact of the accumulator setting on model convergence, and the hardware overhead for denorm support. The authors address these concerns in their response, providing additional explanations and clarifications.\n\nReviewer 2 praises the paper's contribution to the hardware implementation of block minifloats and the use of Kulisch accumulators for minifloat dot products. However, they also raise several minor issues, including the lack of detail on power modeling and the need for more information on the hardware evaluation methodology. The authors respond to these comments, providing additional details and clarifications.\n\nReviewer 3 highlights the paper's strengths, including the proposed representation's generality and the exhaustive exploration of the design space. However, they also raise several concerns, including the need for more precise explanations of certain aspects of the work and the presentation of results in a confusing way. The authors address these concerns in their response, providing additional explanations and clarifications.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' responses, I conclude that the paper has several strengths, including:\n\n1. Novelty: The paper presents a novel approach to training deep neural networks using a block minifloat representation, which combines the benefits of minifloats and block floating-point representations.\n2. Extensive experiments: The authors demonstrate the effectiveness of their approach through extensive experiments on various deep learning models and datasets.\n3. Hardware implementation: The paper provides a detailed hardware implementation of block minifloats, including the use of Kulisch accumulators for minifloat dot products.\n\nHowever, the paper also has some weaknesses, including:\n\n1. Lack of clarity: Some aspects of the paper, such as the presentation of results and the explanation of certain concepts, could be improved for better clarity.\n2. Limited novelty: Some reviewers raised concerns about the novelty of certain aspects of the work, such as the shared-exponent bias concept.\n\nOverall, I believe that the paper's strengths outweigh its weaknesses, and it provides a valuable contribution to the field of deep learning and computer architecture.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Training independent subnetworks for robust prediction\" presents a novel approach to improving the robustness and uncertainty estimation of neural networks by training multiple independent subnetworks within a single network. The authors propose a multi-input multi-output (MIMO) configuration, which allows for the training of multiple subnetworks that learn to ignore each other's inputs, resulting in diverse predictions.\n\nThe paper has several strengths, including:\n\n1. **Novel approach**: The MIMO configuration is a new and interesting approach to training independent subnetworks, which can lead to improved robustness and uncertainty estimation.\n2. **Thorough experimental evaluation**: The authors provide a thorough experimental evaluation of their approach, including results on several benchmark datasets and comparisons to other state-of-the-art methods.\n3. **Improved performance**: The results show that the MIMO approach can lead to significant improvements in robustness and uncertainty estimation, with minimal increases in computational cost.\n\nHowever, the paper also has some weaknesses, including:\n\n1. **Lack of theoretical analysis**: The authors do not provide a thorough theoretical analysis of their approach, which makes it difficult to understand why the MIMO configuration leads to independent subnetworks.\n2. **Limited evaluation**: The authors only evaluate their approach on a limited set of datasets and tasks, which makes it difficult to generalize the results to other domains.\n3. **Comparison to other methods**: The authors do not provide a thorough comparison to other state-of-the-art methods, which makes it difficult to understand the strengths and weaknesses of their approach.\n\nThe reviewers' comments provide additional insights into the paper's strengths and weaknesses. Reviewer 1 highlights the paper's contribution to improving uncertainty estimation and robustness, but raises concerns about the lack of detailed evaluation on the number of parameters. Reviewer 2 praises the paper's experimental evaluation, but raises concerns about the originality of the approach and the lack of theoretical analysis. Reviewer 3 raises concerns about the practical potential of the approach and the lack of theoretical analysis. Reviewer 4 raises concerns about the originality of the approach and the lack of comparison to other state-of-the-art methods.\n\nThe authors' response addresses some of the reviewers' concerns, including the lack of detailed evaluation on the number of parameters and the originality of the approach. However, the response does not fully address the concerns about the lack of theoretical analysis and the limited evaluation.\n\nIn conclusion, while the paper presents a novel and interesting approach to improving the robustness and uncertainty estimation of neural networks, it has some significant weaknesses, including the lack of theoretical analysis and limited evaluation. Therefore, I would recommend rejecting the paper in its current form, but encouraging the authors to revise and resubmit with additional theoretical analysis and evaluation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Uncertainty for deep image classifiers on out of distribution data\" proposes a simple post-hoc calibration method to improve the predictive uncertainty of deep image classifiers on out-of-distribution (OOD) data. The method uses outlier exposure to properly calibrate the model probabilities.\n\nThe reviewers have raised several concerns and questions about the paper, including the assumption of knowing the possible novel distributions, the simplicity of the approach, and the lack of related work. However, the authors have addressed these concerns in their response, providing additional explanations, experiments, and references to related work.\n\nThe reviewers have also praised the paper for its importance, simplicity, and effectiveness. Reviewer 2 notes that the method is \"simple and intuitive\" and that the improvements are \"fairly consistent\". Reviewer 3 states that the method \"could be applied post-hoc to a variety of models\" and that it \"seems fast to compute\". Reviewer 4 notes that the method \"performs well\" on completely OOD data.\n\nAfter carefully analyzing the paper and the reviewer comments, I believe that the paper has addressed the concerns and questions raised by the reviewers. The authors have provided additional explanations, experiments, and references to related work, which have strengthened the paper. The paper's importance, simplicity, and effectiveness make it a valuable contribution to the field.\n\nThe paper's strengths include:\n\n* Importance: The paper addresses a crucial problem in deep learning, which is the estimation of predictive uncertainty on OOD data.\n* Simplicity: The method is simple and intuitive, making it easy to understand and implement.\n* Effectiveness: The method has been shown to be effective in improving the predictive uncertainty of deep image classifiers on OOD data.\n\nThe paper's weaknesses include:\n\n* Assumptions: The method assumes that the possible novel distributions are known, which may not always be the case.\n* Lack of related work: The paper could benefit from a more comprehensive review of related work in the field.\n\nHowever, the authors have addressed these weaknesses in their response, providing additional explanations and references to related work.\n\nIn conclusion, based on the analysis of the paper and the reviewer comments, I believe that the paper has made a valuable contribution to the field and has addressed the concerns and questions raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel algorithm for reducing the number of neurons in a deep ReLU network based on the theory of implicit and explicit regularization. The algorithm consists of three steps: removing dead ReLU neurons, combining neurons that are always active, and using weighted k-means to combine other neurons. The authors claim that their algorithm can reduce the number of neurons by 90% to 99% without introducing sparsity and without significant loss of accuracy.\n\nHowever, the paper has several major issues that need to be addressed. Firstly, the presentation is unclear and lacks detailed explanations, making it difficult for readers to understand the algorithm and its underlying theory. The authors rely heavily on an unpublished work, which is not available for review, and this makes it challenging to verify the correctness of the proposed technique.\n\nSecondly, the experimental evaluation is insufficient. The authors only conduct experiments on simple toy datasets and MNIST, which is not enough to demonstrate the effectiveness of the algorithm on more complex datasets. The experiments also lack comparison to state-of-the-art pruning techniques, making it difficult to assess the performance of the proposed algorithm.\n\nThirdly, the paper lacks motivation and clarity on the proposed technique. The authors do not provide a clear explanation of why their algorithm is better than existing pruning methods, and the connection between the theory and the algorithm is not well-established.\n\nLastly, the paper contains several typos, grammatical errors, and inconsistencies in notation, which further detract from its overall quality.\n\nThe reviewers have raised several concerns and questions, which the authors have attempted to address in their response. However, the authors' response also highlights the need for significant revisions to the paper, including improving the presentation, providing more detailed explanations, and conducting more comprehensive experiments.\n\nIn light of these issues, it is clear that the paper is not ready for publication in its current form. While the authors' response provides some clarification and insight into their work, it also underscores the need for significant revisions to address the concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\" proposes a novel exploration strategy for continuous reinforcement learning, which combines the principles of optimism in the face of uncertainty and information-theoretic exploration. The authors introduce a new method called Max-Q Entropy Search (MQES), which formulates the exploration policy to maximize the information about the globally optimal distribution of the Q function.\n\nThe reviewers have provided detailed comments and suggestions for improvement. The main concerns raised by the reviewers include:\n\n1. Clarity and presentation: Several reviewers have pointed out that the paper is hard to follow, and the notation and terminology used are not clear or well-explained.\n2. Theoretical contributions: Some reviewers have questioned the novelty and significance of the theoretical contributions, and have requested more detailed derivations and justifications.\n3. Experimental evaluation: The reviewers have suggested that the experimental evaluation is limited, and that more comparisons with other state-of-the-art methods are needed.\n4. Hyperparameter tuning: Some reviewers have pointed out that the hyperparameters used in the experiments are not well-justified, and that more ablation studies are needed to understand the impact of the hyperparameters on the performance.\n\nThe authors have responded to the reviewers' comments and have made revisions to the paper. The revised paper includes:\n\n1. Improved clarity and presentation: The authors have rewritten the paper to make it easier to follow, and have added more explanations and justifications for the notation and terminology used.\n2. More detailed theoretical derivations: The authors have provided more detailed derivations and justifications for the theoretical contributions, including the proof of Proposition 2.\n3. Additional experimental evaluations: The authors have conducted more experiments, including comparisons with other state-of-the-art methods, and have added more ablation studies to understand the impact of the hyperparameters on the performance.\n4. Hyperparameter tuning: The authors have provided more justification for the hyperparameters used in the experiments, and have conducted more ablation studies to understand the impact of the hyperparameters on the performance.\n\nBased on the revised paper and the authors' responses to the reviewers' comments, I believe that the paper has been significantly improved. The authors have addressed the main concerns raised by the reviewers, and have provided more detailed and clear explanations of the theoretical contributions and experimental evaluations.\n\nHowever, I still have some concerns about the paper. Specifically, I think that the paper could benefit from more comparisons with other state-of-the-art methods, and that the authors could provide more insights into the strengths and limitations of the proposed method.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field of reinforcement learning, and that the authors have made a good effort to address the concerns raised by the reviewers.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel algorithm for reducing the number of neurons in a deep ReLU network based on the theory of implicit and explicit regularization. The algorithm consists of three steps: removing dead neurons, combining neurons that are always active, and using weighted k-means to combine other neurons. The authors claim that their algorithm can reduce the number of neurons by 90% to 99% without introducing sparsity and without significant loss of accuracy.\n\nHowever, the paper has several major issues that need to be addressed. Firstly, the presentation is unclear and lacks detail, making it difficult for reviewers to understand the algorithm and its theoretical foundation. The authors rely heavily on an unpublished work, which makes it impossible for reviewers to verify the claims made in the paper. The experiments are also limited, with only two datasets used (a toy dataset and MNIST), and no comparison to state-of-the-art pruning techniques.\n\nThe reviewers have raised several concerns, including the lack of motivation for the proposed technique, insufficient experimental evaluation, and unclear presentation. Reviewer 1 suggests that the method should be verified experimentally on more complex datasets and compared to state-of-the-art pruning techniques. Reviewer 2 states that the paper is not properly written, and the experiments are not convincing. Reviewer 3 believes that the paper is not ready for publication, citing the lack of detail and unclear presentation.\n\nThe authors' response acknowledges some of these issues, including the need for clearer explanations and more experiments. However, they do not provide sufficient evidence to address the concerns raised by the reviewers. The authors claim that their algorithm has a theoretical foundation, but this foundation is not clearly explained in the paper, and the reviewers are unable to verify the claims made.\n\nIn conclusion, while the paper proposes an interesting idea, it has several major issues that need to be addressed before it can be considered for publication. The presentation is unclear, the experiments are limited, and the theoretical foundation is not well-explained. Therefore, I recommend rejecting the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" proposes a novel approach to learning layout representations for graphic design using a Transformer-based model and a large-scale dataset of over 1 million slides. The authors demonstrate the effectiveness of their approach on several downstream tasks, including element role labeling, image captioning, layout auto-completion, and layout retrieval.\n\nThe reviewers have raised several concerns and suggestions for improvement, which can be summarized as follows:\n\n1. **Lack of comparison to previous works**: The reviewers have pointed out that the paper does not provide a thorough comparison to previous works in the field, including the SIGGRAPH 2019 paper, Li et al. 2020b, and READ (CVPR 2020). The authors should provide a more detailed comparison to these works to demonstrate the novelty and significance of their approach.\n2. **Weak evaluation**: The reviewers have criticized the evaluation methodology used in the paper, suggesting that the tasks used to evaluate the model are not sufficiently challenging and that the baselines used are not strong enough. The authors should consider using more challenging tasks and stronger baselines to evaluate their model.\n3. **Lack of clarity and detail**: The reviewers have pointed out that the paper lacks clarity and detail in several areas, including the description of the Transformer model, the dataset, and the evaluation methodology. The authors should provide more detail and clarity in these areas to make the paper more readable and reproducible.\n4. **Need for more systematic evaluation**: The reviewers have suggested that the authors should conduct a more systematic evaluation of their model, including a more thorough analysis of the results and a comparison to other state-of-the-art models.\n\nIn response to these concerns, the authors have provided a detailed response outlining their plans to address these issues, including:\n\n1. **Adding more comparisons to previous works**: The authors plan to add more comparisons to previous works, including the SIGGRAPH 2019 paper, Li et al. 2020b, and READ (CVPR 2020).\n2. **Improving the evaluation methodology**: The authors plan to improve the evaluation methodology used in the paper, including using more challenging tasks and stronger baselines.\n3. **Providing more detail and clarity**: The authors plan to provide more detail and clarity in several areas, including the description of the Transformer model, the dataset, and the evaluation methodology.\n4. **Conducting a more systematic evaluation**: The authors plan to conduct a more systematic evaluation of their model, including a more thorough analysis of the results and a comparison to other state-of-the-art models.\n\nBased on the reviewers' comments and the authors' response, it is clear that the paper has several strengths, including:\n\n1. **Novel approach**: The paper proposes a novel approach to learning layout representations for graphic design using a Transformer-based model and a large-scale dataset.\n2. **Large-scale dataset**: The paper introduces a large-scale dataset of over 1 million slides, which is a significant contribution to the field.\n3. **State-of-the-art results**: The paper demonstrates state-of-the-art results on several downstream tasks, including element role labeling, image captioning, layout auto-completion, and layout retrieval.\n\nHowever, the paper also has several weaknesses, including:\n\n1. **Lack of comparison to previous works**: The paper does not provide a thorough comparison to previous works in the field.\n2. **Weak evaluation**: The paper's evaluation methodology is not sufficiently challenging, and the baselines used are not strong enough.\n3. **Lack of clarity and detail**: The paper lacks clarity and detail in several areas, including the description of the Transformer model, the dataset, and the evaluation methodology.\n\nOverall, while the paper has several strengths, its weaknesses outweigh its strengths. The paper requires significant revisions to address the concerns raised by the reviewers, including adding more comparisons to previous works, improving the evaluation methodology, providing more detail and clarity, and conducting a more systematic evaluation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Global Node Attentions via Adaptive Spectral Filters\" presents a novel approach to graph neural networks (GNNs) by introducing a global self-attention mechanism defined using learnable spectral filters. This allows the model to attend to any nodes, regardless of distance, which is particularly useful for disassortative graphs where local homophily does not hold.\n\nThe paper's strengths include:\n\n1. **Novelty**: The paper proposes a new approach to GNNs, which is a significant contribution to the field. The use of learnable spectral filters is a unique aspect of the paper.\n2. **Theoretical motivation**: The paper provides a solid theoretical foundation for the proposed method, drawing on concepts from graph wavelets and spectral filtering.\n3. **Empirical evaluation**: The paper presents a thorough empirical evaluation of the proposed method on six benchmark datasets, demonstrating its effectiveness on both assortative and disassortative graphs.\n\nHowever, the paper also has some weaknesses:\n\n1. **Complexity**: Some reviewers have raised concerns about the computational complexity of the proposed method, particularly with regards to the computation of the attention weights.\n2. **Comparison to existing methods**: Some reviewers have questioned the novelty of the paper, citing existing methods that use attention mechanisms or spectral filters.\n3. **Limited evaluation**: Some reviewers have suggested that the paper's evaluation is limited, with a focus on node classification tasks and a lack of comparison to other baselines.\n\nIn response to these concerns, the authors have provided additional explanations and justifications for their approach, including:\n\n1. **Clarification on complexity**: The authors have provided further details on the computational complexity of their method, arguing that it is efficient and scalable.\n2. **Comparison to existing methods**: The authors have highlighted the key differences between their approach and existing methods, emphasizing the novelty of their learnable spectral filters.\n3. **Additional evaluation**: The authors have committed to conducting additional experiments and evaluations to address the concerns of the reviewers.\n\nAfter carefully considering the paper and the reviewers' comments, I believe that the paper makes a significant contribution to the field of graph neural networks. While there are some concerns about complexity and evaluation, the authors have provided sufficient responses to address these issues.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents a novel approach to 3D snapshot microscopy by utilizing Fourier convolutional neural networks for end-to-end optimization of optical encoders and image reconstruction. The authors demonstrate the effectiveness of their method through simulations and comparisons with existing state-of-the-art architectures.\n\nThe strengths of the paper include:\n\n1. **Novel application of Fourier convolutional networks**: The authors apply Fourier convolutional networks to the problem of 3D snapshot microscopy, demonstrating their potential in handling highly non-local optical encoders.\n2. **End-to-end optimization**: The paper presents a framework for end-to-end optimization of optical encoders and image reconstruction, which is a significant contribution to the field of computational microscopy.\n3. **Comparisons with state-of-the-art architectures**: The authors provide thorough comparisons with existing architectures, such as UNets, demonstrating the superiority of their approach in terms of reconstruction quality and computational efficiency.\n\nHowever, the paper also has some weaknesses:\n\n1. **Limited technical novelty**: Some reviewers pointed out that the idea of using Fourier convolutional networks is not entirely new, and that similar concepts have been explored in previous works (e.g., Rippel et al., 2015).\n2. **Lack of real-world experiments**: The paper relies solely on simulations, which may not fully capture the complexities of real-world experiments.\n3. **Some unclear results and comparisons**: A few reviewers noted that some results, such as the comparison with UNet in Table 3, were unclear or inconsistent with the claims made in the paper.\n\nThe authors' response to the reviewers' comments addresses some of the concerns, providing additional context and clarifications. They acknowledge the prior work on Fourier convolutional networks and emphasize the novelty of their application to 3D snapshot microscopy. They also provide more details on the simulations and comparisons, which helps to alleviate some of the concerns.\n\nConsidering the strengths and weaknesses of the paper, as well as the authors' response to the reviewers' comments, I believe that the paper makes a significant contribution to the field of computational microscopy. While the technical novelty may not be entirely new, the application of Fourier convolutional networks to 3D snapshot microscopy is a valuable contribution. The paper's strengths, including the end-to-end optimization framework and the thorough comparisons with state-of-the-art architectures, outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Boosting Approach to Reinforcement Learning\" presents a novel approach to reinforcement learning by leveraging boosting techniques from supervised learning. The authors propose an algorithm that combines weak learners to generate a near-optimal policy, with a sample complexity guarantee that does not depend on the number of states. This is a significant contribution, as it addresses the challenge of large state spaces in reinforcement learning.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novel approach**: The paper introduces a new approach to reinforcement learning by applying boosting techniques, which is a significant contribution to the field.\n2. **Theoretical guarantees**: The authors provide theoretical guarantees for their algorithm, including a sample complexity bound that does not depend on the number of states.\n3. **Technical novelty**: The paper presents a novel proof for a variant of the Frank-Wolfe optimization technique, adapted to non-convex and gradient-dominated function classes.\n\nHowever, the reviewers also raise several concerns, including:\n\n1. **Clarity and readability**: Many reviewers comment on the paper's density and lack of clarity, making it difficult to follow and understand the results.\n2. **Notation overload**: The paper introduces a large number of notations, which can be overwhelming for the reader.\n3. **Lack of empirical evaluation**: The paper does not include empirical experiments to validate the effectiveness of the proposed algorithm.\n4. **Comparison to prior work**: Some reviewers question the novelty of the approach, comparing it to existing work, such as Hazan and Singh (2021).\n\nThe authors' response addresses some of these concerns, including:\n\n1. **Clarifying notation**: The authors agree to add a notation list and elaborate on the intuition behind the algorithm.\n2. **Providing examples**: The authors will provide examples of valid weak learners and clarify the usage of certain assumptions.\n3. **Comparing to prior work**: The authors highlight the differences between their approach and existing work, such as the ability to handle non-convex costs.\n\nAfter carefully considering the reviewers' comments and the authors' response, I conclude that the paper has significant potential, but its presentation and clarity need improvement. While the authors have addressed some of the concerns, the paper still requires revisions to make it more readable and accessible to a broader audience.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to consider several factors, including the novelty and significance of the contributions, the correctness and clarity of the paper, and the quality of the experimental results.\n\nThe paper proposes a new version of target propagation (TP) based on regularized inversion of network layers, which can be easily implemented in a differentiable programming framework. The authors compare the computational complexity of the algorithm with backpropagation (BP) and show that it can be used to train recurrent neural networks (RNNs) with long sequences on various sequence modeling problems.\n\nThe reviewer comments highlight both strengths and weaknesses of the paper. Reviewer 1 is not enthusiastic about the paper, citing the loss of features that make TP attractive as an alternative to BP, the lack of theoretical guarantees, and the limited experimental results. Reviewer 2 finds the proposed scheme novel and interesting, with a natural interpretation, but notes that the numerical results are confusing. Reviewer 3 thinks the paper explores an interesting alternative to BP but is not fully convinced of its superiority. Reviewer 4 believes the manuscript proposes an interesting algorithm but needs to more fully explain its theoretical properties and provide more detailed experimentation and comparisons.\n\nA common criticism among the reviewers is the lack of strong theoretical guarantees and the limited experimental results. The paper's claims are not always well-supported, and some statements require small changes to be made correct. The empirical evidence is not extensive, and the improvements over BP are not fully explained.\n\nOn the other hand, the paper is clear and well-structured, and the proposed algorithm has a natural interpretation. The authors demonstrate the effectiveness of the algorithm on synthetic and image benchmarks, although the results are not always convincing.\n\nConsidering the above factors, the paper has some merits, but the limitations and weaknesses outweigh the strengths. The lack of strong theoretical guarantees, the limited experimental results, and the marginal significance of the contributions are major concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"On the Implicit Biases of Architecture & Gradient Descent\" presents a theoretical analysis of the generalization properties of neural networks, focusing on the roles of architecture and gradient descent. The authors derive a non-vacuous PAC-Bayes bound for neural networks, which depends analytically on the architecture and training data. They also investigate the implicit bias of gradient descent and its relationship to margin.\n\nThe reviewers' comments and the authors' responses provide a comprehensive discussion of the paper's strengths and weaknesses. The main strengths of the paper include:\n\n1. **Novel theoretical results**: The authors derive a non-vacuous PAC-Bayes bound for neural networks, which is a significant contribution to the field.\n2. **Insightful analysis of margin**: The paper provides a thoughtful analysis of the relationship between margin and generalization, which is a crucial aspect of neural network performance.\n3. **Well-written and clear presentation**: The authors' writing is clear and concise, making the paper easy to follow.\n\nHowever, the reviewers also raise several concerns and criticisms, including:\n\n1. **Limited experimental evaluation**: Some reviewers feel that the experimental evaluation is limited and does not provide sufficient evidence to support the authors' claims.\n2. **Lack of comparison to existing works**: A few reviewers note that the authors do not adequately compare their results to existing works in the field, which makes it difficult to assess the significance of their contributions.\n3. **Unclear terminology and notation**: Some reviewers find the terminology and notation used in the paper to be unclear or misleading, which can make it difficult to understand the authors' arguments.\n\nAfter carefully considering the reviewers' comments and the authors' responses, it is clear that the paper has both strengths and weaknesses. While the authors have made significant contributions to the field, there are also areas where the paper could be improved.\n\nThe authors have addressed some of the reviewers' concerns in their responses, providing additional clarification and context. However, some concerns remain, particularly regarding the limited experimental evaluation and the lack of comparison to existing works.\n\nUltimately, the decision to accept or reject the paper depends on the trade-off between the paper's strengths and weaknesses. While the paper has some significant flaws, it also presents novel and insightful results that contribute to the field's understanding of neural network generalization.\n\nThe final decision is:\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical analysis of the Feedback Alignment (FA) algorithm, an alternative to backpropagation for training neural networks. The authors provide convergence guarantees with rates for deep linear networks, both in continuous and discrete dynamics, and study incremental learning phenomena for shallow linear networks. They also identify an implicit anti-regularization phenomenon under certain initializations and propose initialization schemes that provide a form of implicit regularization.\n\nThe reviewer comments are mixed, with some reviewers praising the paper's clarity, technical novelty, and significance, while others raise concerns about the paper's limitations, such as the assumption of spectral initialization, the focus on linear networks, and the lack of comparison with Gradient Descent (GD).\n\nReviewer 1 and Reviewer 2 express concerns about the paper's significance and technical novelty, citing the restrictive assumption of spectral initialization and the lack of comparison with GD. Reviewer 3, on the other hand, finds the paper to be a nice theoretical development, extending the theory of linear neural networks to the case of FA, and recommends acceptance. Reviewer 4 raises concerns about the significance of the results, the lack of experiments under non-spectral initialization, and the presentation of the paper, but still finds the paper to be clearly written and the results to be interesting.\n\nThe authors respond to the reviewer comments, addressing the concerns and providing additional explanations and justifications for their work. They clarify the assumption of spectral initialization, provide additional experiments under non-spectral initialization, and discuss the implications of their results for non-linear networks.\n\nAfter carefully considering the reviewer comments and the author responses, I believe that the paper makes a significant contribution to the understanding of the FA algorithm and its convergence properties. While the paper has some limitations, such as the focus on linear networks and the assumption of spectral initialization, the authors provide a clear and well-written presentation of their results, and their responses to the reviewer comments address many of the concerns raised.\n\nThe paper's technical novelty and significance are evident in its rigorous analysis of the FA algorithm, which provides new insights into the convergence properties of the algorithm and its behavior under different initializations. The identification of the implicit anti-regularization phenomenon is a notable contribution, and the proposed initialization schemes provide a useful tool for practitioners.\n\nIn conclusion, while the paper is not perfect and has some limitations, I believe that its strengths outweigh its weaknesses, and it makes a significant contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper presents AutoOED, an Automated Optimal Experimental Design platform that utilizes machine learning to accelerate the discovery of solutions with optimal objective trade-offs. The platform implements popular multi-objective Bayesian optimization (MOBO) algorithms and introduces a novel strategy called Believer-Penalizer (BP) for batch experiments acceleration. AutoOED also features a graphical user interface (GUI) for intuitive visualization and guidance of experiment design.\n\nThe reviewer comments highlight both positive and negative aspects of the paper. Reviewers praise the paper's clarity, organization, and the potential impact of AutoOED on the field of optimal experimental design. However, they also express concerns about the paper's scientific contribution, the lack of theoretical proof for the BP strategy, and the limited novelty of the implemented methods.\n\nThe authors' response addresses the reviewers' concerns, providing additional explanations and justifications for the design choices and contributions of AutoOED. They emphasize the platform's ease of use, modular design, and the introduction of asynchronous MOBO, which is a novel contribution to the field. The authors also acknowledge the limitations of their work and provide plans for future improvements.\n\nAfter carefully analyzing the paper, reviewer comments, and author responses, I conclude that the paper has both strengths and weaknesses. The strengths include the potential impact of AutoOED on the field, the clarity and organization of the paper, and the authors' efforts to address the reviewers' concerns. The weaknesses include the limited scientific contribution, the lack of theoretical proof for the BP strategy, and the limited novelty of the implemented methods.\n\nHowever, considering the standards of a top-tier conference, I believe that the paper's weaknesses outweigh its strengths. The paper's scientific contribution is not significant enough, and the lack of theoretical proof for the BP strategy is a major concern. Additionally, the paper's novelty is limited, and the authors' efforts to address the reviewers' concerns, while appreciated, do not fully alleviate the concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" presents an interesting and relevant problem in the field of multi-modal learning. The authors propose a hypothesis that multi-modal deep neural networks tend to rely on one modality while under-utilizing the other modalities, leading to poor generalization performance. They introduce a metric called conditional utilization rate to measure the imbalance in modality utilization and propose a proxy metric called conditional learning speed to estimate it efficiently during training.\n\nThe authors conduct experiments on three datasets, Colored MNIST, Princeton ModelNet40, and NVIDIA Dynamic Hand Gesture Dataset, and demonstrate that their proposed balanced multi-modal learning algorithm improves the model's generalization performance. The experimental results show that the algorithm addresses the issue of greedy learning behavior, which is a significant contribution to the field.\n\nHowever, the reviewers raise several concerns about the paper. Reviewer 1 and Reviewer 2 point out that the experimental results are not convincing, and the differences between the vanilla and guided algorithms are not statistically significant in some datasets. Reviewer 3 and Reviewer 4 suggest that the paper lacks theoretical analysis and comparison with other methods that solve the imbalance problem. Reviewer 5 and Reviewer 6 raise concerns about the definition of conditional learning speed and the re-balancing step method.\n\nThe authors respond to the reviewers' comments and provide additional explanations and justifications for their work. They acknowledge the limitations of their study and propose to address them in future work. The authors also provide additional experimental results and comparisons with other methods, which strengthens their claims.\n\nAfter carefully analyzing the paper and the reviewers' comments, I conclude that the paper has some merits, but it also has significant limitations. The authors' hypothesis and proposed algorithm are interesting and relevant, but the experimental results are not convincing, and the paper lacks theoretical analysis and comparison with other methods.\n\nThe authors' responses to the reviewers' comments are helpful, but they do not fully address the concerns raised. The additional experimental results and comparisons provided by the authors are useful, but they are not sufficient to fully support the authors' claims.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"A Frequency Perspective of Adversarial Robustness\" presents a comprehensive analysis of adversarial examples from a frequency perspective, challenging the common misconception that these examples are primarily high-frequency noise. The authors provide theoretical and empirical evidence to support their claim that adversarial examples are neither high-frequency nor low-frequency but are dataset-dependent. They also explore the properties of training robust models with frequency constraints and propose a frequency-based explanation for the accuracy vs. robustness trade-off.\n\nThe reviewer comments highlight both the strengths and weaknesses of the paper. Reviewers 4 and 6 find the paper's insights beneficial to the community and recommend acceptance, with Reviewer 6 noting the paper's well-written nature and the provision of extensive supplementary material. Reviewer 4 suggests minor improvements, such as addressing grammatical errors and reordering figures for better presentation.\n\nHowever, other reviewers raise several concerns. Reviewer 1 suggests that the paper's contributions are not well-addressed in the rebuttal and recommends considering universal adversarial perturbations (UAPs) for a more comprehensive analysis. Reviewer 2 questions the novelty of the paper's contributions, given the existence of prior work on similar topics, and seeks clearer explanations of the paper's unique ideas. Reviewer 3 criticizes the paper for not providing sufficiently new insights, as the idea that adversarial examples are dataset-dependent is already accepted in the community. Reviewer 3 also points out the limitation of the paper's experiments to only PGD attacks and suggests that more datasets and attack methods should be considered. Reviewer 5 decides to reject the paper due to confusion about the level of contribution and the lack of clear explanations for the paper's claims.\n\nThe authors' response addresses many of the reviewers' concerns, providing additional experiments, clarifying their methodology, and discussing the differences between their work and prior studies. They acknowledge the suggestions for improvement and express willingness to incorporate them into an updated version of the paper.\n\nGiven the detailed analysis and the authors' thorough response to the reviewers' comments, it is clear that the paper contributes to the understanding of adversarial robustness from a frequency perspective. While there are valid criticisms regarding the novelty and completeness of the contributions, the authors have made a significant effort to address these concerns.\n\nThe final decision should consider the overall quality of the paper, the significance of its contributions, and the potential impact on the field. The paper's strengths, including its insightful analysis and well-conducted experiments, outweigh its weaknesses, particularly when considering the authors' willingness to revise and improve the paper based on the reviewers' feedback.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a novel analysis of the optimization landscape of two-layer ReLU neural networks. The authors provide a convex optimization program with cone constraints that can be used to find all globally optimal two-layer ReLU neural networks. The paper also establishes connections between the convex optimization program and the neural network optimization landscape, including the characterization of optimal solutions, the construction of a continuous path between any neural network and the global minimum, and the analysis of the minimal size of the hidden layer required to avoid spurious valleys.\n\nThe reviewer comments are overall positive, with Reviewer 2 stating that the paper is \"the best among all submissions I have reviewed in ICLR 2022\" and Reviewer 4 recommending a \"clear accept\". The reviewers appreciate the novelty and significance of the contributions, the clarity of the presentation, and the technical correctness of the results.\n\nHowever, the reviewers also raise some concerns and suggest improvements. Reviewer 1 points out some minor issues with the paper's claims and suggests that the analysis is not entirely new, as some aspects of the contributions exist in prior work. Reviewer 2 asks about the possibility of extending the results to neural networks with other activation functions and to deep neural networks. Reviewer 3 suggests that the paper could be improved by providing experimental validations and discussing the relationship between the convex optimization program and the neural network optimization landscape in more detail.\n\nThe authors respond to the reviewer comments, addressing the minor issues and providing additional explanations and motivations for their results. They also discuss the possibility of extending their results to deep neural networks and other activation functions, and provide references to related work.\n\nBased on the reviewer comments and the author responses, it appears that the paper makes significant contributions to the understanding of the optimization landscape of two-layer ReLU neural networks. The results are technically correct, and the presentation is clear and well-organized. While the reviewers raise some concerns and suggest improvements, the authors have addressed these issues in their responses.\n\nTherefore, considering the standards of a top-tier conference, which emphasize novelty, technical correctness, and significance of the contributions, as well as the clarity and quality of the presentation, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper proposes a novel method, Differentiable Diffusion Sampler Search (DDSS), to optimize fast samplers for pre-trained diffusion models. The method uses a non-Markovian hierarchical generative model, Generalized Gaussian Diffusion Models (GGDM), and optimizes the sampler variances by minimizing a perceptual loss. The paper demonstrates the effectiveness of the proposed method on various datasets, including CIFAR-10 and ImageNet 64x64, and shows that it outperforms existing methods, such as DDPM and DDIM, in terms of sample quality and inference speed.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The reviewers praise the paper's novelty, empirical results, and the potential impact of the proposed method on the field of generative models. However, they also raise concerns about the paper's clarity, motivation, and theoretical guarantees.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has made significant contributions to the field of generative models. The proposed method, DDSS, is novel and has shown promising results on various datasets. The authors have also addressed the reviewers' concerns and provided additional explanations and results to support their claims.\n\nHowever, I also agree with the reviewers that the paper could be improved in terms of clarity and motivation. The authors should provide more detailed explanations of the proposed method and its connections to existing work in the field. Additionally, the authors should consider providing more theoretical guarantees and analysis of the proposed method to strengthen its contributions.\n\nOverall, I believe that the paper has the potential to make a significant impact on the field of generative models, and with some revisions to address the reviewers' concerns, it can be a strong contribution to the conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"The weighted mean trick  optimization strategies for robustness\" presents a novel approach to optimizing the higher-order moments of the loss distribution, which can lead to improved robustness against outliers. The authors provide theoretical results, including Theorem 1, which shows that minimizing a weighted mean results in optimizing the higher-order moments of the loss distribution. They also propose an iterative algorithm, Algorithm 1, to perform the weighted Empirical Risk Minimization (ERM).\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The main concerns raised by the reviewers include:\n\n1. Theoretical contribution: Some reviewers felt that the theoretical results are not significant or novel enough.\n2. Clarity and notation: Several reviewers found the notation and writing to be confusing, which made it difficult to understand the paper.\n3. Algorithm 1: Reviewers questioned the correctness and convergence of Algorithm 1.\n4. Experimental results: Some reviewers felt that the experimental results are not convincing or comprehensive enough.\n\nThe authors have addressed these concerns in their responses, providing additional explanations, clarifications, and justifications for their approach. They have also updated the paper to improve notation, clarity, and readability.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' responses, I conclude that the paper has some merits, but it also has significant weaknesses that need to be addressed.\n\nThe paper's strengths include:\n\n1. Novel approach: The weighted mean trick is a new and interesting approach to optimizing the higher-order moments of the loss distribution.\n2. Theoretical results: The paper provides some theoretical results, including Theorem 1, which shows the connection between minimizing a weighted mean and optimizing the higher-order moments of the loss distribution.\n3. Potential applications: The paper has potential applications in robust learning and outlier detection.\n\nHowever, the paper's weaknesses include:\n\n1. Lack of significant theoretical contribution: The theoretical results, although novel, may not be significant or substantial enough to warrant publication in a top-tier conference.\n2. Clarity and notation: The paper's notation and writing are still confusing, which may hinder the reader's understanding of the paper.\n3. Algorithm 1: The correctness and convergence of Algorithm 1 are still questionable, which may affect the paper's validity.\n4. Limited experimental results: The experimental results are not comprehensive or convincing enough to demonstrate the effectiveness of the proposed approach.\n\nConsidering these points, I believe that the paper requires significant revisions to address the concerns raised by the reviewers. While the paper has some merits, its weaknesses outweigh its strengths, and it is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" presents a novel analysis of the behavior of Stochastic Gradient Descent with Momentum (SGDm) under non-iid sampling, specifically covariate shift. The authors identify a resonance phenomenon that can lead to divergence of SGDm and provide theoretical and empirical evidence to support their claims.\n\nThe paper's strengths include:\n\n1. **Novelty**: The paper introduces a new perspective on the behavior of SGDm under non-iid sampling, which is a common scenario in many machine learning applications.\n2. **Theoretical foundations**: The authors provide a solid theoretical foundation for their claims, using techniques from numerical analysis and differential equations to analyze the behavior of SGDm.\n3. **Empirical evidence**: The paper presents extensive empirical evidence to support the theoretical claims, including experiments on synthetic and real-world datasets.\n4. **Importance**: The paper addresses an important problem in machine learning, as non-iid sampling is a common issue in many applications, including reinforcement learning and online learning.\n\nHowever, the paper also has some weaknesses:\n\n1. **Technical novelty**: Some reviewers questioned the technical novelty of the paper, arguing that the mathematical techniques used are not new and that the paper's contributions are mostly incremental.\n2. **Limited scope**: The paper focuses on a specific setting (linear regression with covariate shift) and may not be directly applicable to other scenarios.\n3. **Lack of practical implications**: Some reviewers felt that the paper's results may not have significant practical implications, as the resonance phenomenon may not be a common issue in practice.\n\nAfter carefully considering the reviews and the authors' responses, I believe that the paper's strengths outweigh its weaknesses. The paper presents a novel and important contribution to the field of machine learning, and its theoretical and empirical evidence provide a solid foundation for the claims made. While the technical novelty of the paper may be limited, the paper's importance and potential impact on the field justify its publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" proposes a novel architecture for synthetically generating time-series data using Variational Auto-Encoders (VAEs). The authors claim that their approach has several distinct properties, including interpretability, the ability to encode domain knowledge, and reduced training times.\n\nUpon analyzing the reviewer comments, several concerns and weaknesses are raised. Reviewer 1 points out that the introduced decoder building blocks seem to introduce new hyper-parameters without providing details on how to select them or perform model selection. Additionally, the reviewer questions the interpretability of the newly introduced building blocks, as there is no experiment or ablation study to provide evidence for this claim.\n\nReviewer 2 expresses concerns that the proposed model is not substantially original and does not seem to present any significant novelty. The reviewer also questions why the proposed \"base\" model is not just a standard VAE applied to time-series data and why the architecture includes convolutional layers instead of recurrent layers.\n\nReviewer 3 raises several issues, including the lack of general context in the literature, unclear technical contribution, and substantial limitations in the current version of the paper. The reviewer also points out that the description of the methods and background on generative models is odd and that the explanation of VAEs is non-standard.\n\nReviewer 4 doubts the contribution of the proposed method to the machine learning community and points out that there is no discussion about interpretability in the results session. The reviewer also notes that there is no obvious better performance for the proposed model besides the dataset \"sine\".\n\nDespite these concerns, Reviewer 4 rates the correctness of the paper as 4, indicating that all claims and statements are well-supported and correct. However, the technical novelty and significance, as well as empirical novelty and significance, are rated as 3 by Reviewer 4 and 1, 2, or 3 by the other reviewers, indicating that the contributions are not significantly novel or original.\n\nConsidering the reviewer comments and the ratings provided, it appears that the paper has several weaknesses and limitations, including the lack of novelty, unclear technical contribution, and insufficient evaluation of the proposed model. While the paper is well-written, the concerns raised by the reviewers outweigh the strengths of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach to improving the sampling efficiency of diffusion models, a class of generative models that have shown promising results in image synthesis tasks. The authors propose a method called Differentiable Diffusion Sampler Search (DDSS), which optimizes the parameters of a non-Markovian sampler to minimize the perceptual loss between the generated samples and the real data. The paper also introduces a new family of samplers called Generalized Gaussian Diffusion Models (GGDM), which can be used in conjunction with DDSS to achieve better sample quality.\n\nThe reviewers have raised several concerns about the paper, including the lack of theoretical guarantees, the choice of perceptual loss, and the limited experimental evaluation. However, the authors have addressed these concerns in their rebuttal, providing additional theoretical insights, justifying the choice of perceptual loss, and promising to include more experimental results in the camera-ready version.\n\nThe reviewers have also praised the paper for its novelty, empirical results, and potential impact on the field. Reviewer 3 finds the paper \"very interesting\" and notes that the results are \"quite impressive.\" Reviewer 5 thinks that the paper \"makes a novel contribution\" and has \"good empirical performance.\"\n\nAfter carefully considering the reviews and the authors' rebuttal, I believe that the paper has made significant contributions to the field of diffusion models and has the potential to improve the efficiency and quality of image synthesis tasks.\n\nThe authors have provided a clear and well-motivated approach to improving the sampling efficiency of diffusion models, and their experimental results demonstrate the effectiveness of their method. While there are some limitations to the paper, the authors have addressed the concerns raised by the reviewers and have provided additional insights and justifications for their approach.\n\nTherefore, based on the reviews and the authors' rebuttal, I believe that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Learning Sample Reweighting for Adversarial Robustness\" proposes a novel approach to improve the robustness of neural network classifiers against adversarial attacks. The authors introduce a bi-level optimization framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin. The paper is well-written, and the authors provide extensive experiments to demonstrate the effectiveness of their approach.\n\nThe reviewers' comments and the authors' responses provide valuable insights into the paper's strengths and weaknesses. The main concerns raised by the reviewers include:\n\n1. **Novelty**: Some reviewers question the novelty of the approach, as it is based on existing meta-learning algorithms. However, the authors argue that their formulation and implementation are new and provide additional value.\n2. **Evaluation**: Reviewers suggest that the authors should evaluate their approach against more modern baselines and provide a more detailed analysis of the results.\n3. **Adaptive attacks**: Some reviewers raise concerns about the potential vulnerability of the approach to adaptive attacks, which could exploit the learned reweighting scheme.\n4. **Ablation studies**: Reviewers request more ablation studies to understand the contribution of individual components of the approach.\n\nThe authors respond to these concerns by:\n\n1. **Providing additional context**: The authors clarify the novelty of their approach and highlight the differences with existing methods.\n2. **Offering additional experiments**: The authors provide new experiments and results to address the reviewers' concerns, including evaluations against more modern baselines and ablation studies.\n3. **Addressing adaptive attacks**: The authors discuss the potential vulnerability to adaptive attacks and provide preliminary results to demonstrate the robustness of their approach.\n4. **Clarifying the methodology**: The authors provide more details on their methodology and respond to questions about the input encoding, capacity of the weighting network, and weight-adaptive attacks.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' responses, I conclude that the paper has several strengths, including:\n\n1. **Well-written and clear**: The paper is well-organized, and the authors provide a clear explanation of their approach.\n2. **Extensive experiments**: The authors provide a comprehensive evaluation of their approach, including comparisons with existing methods.\n3. **Novel formulation**: The authors introduce a new formulation of the reweighting scheme, which is based on a class-conditioned margin.\n\nHowever, I also identify some weaknesses, including:\n\n1. **Limited novelty**: Some reviewers question the novelty of the approach, and the authors could provide more context to clarify the differences with existing methods.\n2. **Potential vulnerability to adaptive attacks**: The authors acknowledge the potential vulnerability to adaptive attacks, and more research is needed to fully address this concern.\n\nOverall, I believe that the paper provides a valuable contribution to the field of adversarial robustness and that the authors have addressed the reviewers' concerns to a large extent.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Sample and Computation Redistribution for Efficient Face Detection\" presents a novel approach to face detection, focusing on optimizing computation distribution and scale augmentation for improved efficiency and accuracy, particularly for small faces in low-resolution images. The authors propose two methods: Computation Redistribution (CR) and Sample Redistribution (SR), which are combined into the Sample and Computation Redistribution for Face Detection (SCRFD) framework. The paper includes extensive experiments on the WIDER FACE dataset, demonstrating state-of-the-art accuracy-efficiency trade-offs.\n\nSeveral strengths of the paper are highlighted by the reviewers, including:\n- The effectiveness of CR and SR in improving detection accuracy, especially for small faces.\n- The simplicity and practicality of the proposed methods, making them appealing for real-world applications.\n- The comprehensive experimental evaluation, which shows significant improvements over existing state-of-the-art methods.\n\nHowever, reviewers also point out several weaknesses and areas for improvement:\n- The search strategies implemented are considered straightforward and not particularly novel or interesting by some reviewers.\n- There is a lack of comparison with other state-of-the-art network search methods, which is seen as necessary to fully evaluate the contributions of the paper.\n- Some reviewers question the generality of the approach, suggesting it might be limited to object detection with a wide range of object scales.\n\nThe authors have provided detailed responses to the reviewers' comments, addressing the concerns and suggestions for improvement. They have added comparisons with evolutionary search methods and other state-of-the-art network search approaches, demonstrating the superiority of their method. They have also clarified the novelty and significance of their contributions, highlighting the practicality and effectiveness of their approach for face detection and its potential insights for efficient network design.\n\nConsidering the strengths and weaknesses, the detailed responses from the authors, and the overall quality of the paper, it is clear that the paper makes significant contributions to the field of face detection and efficient network design. The authors have adequately addressed the reviewers' concerns, providing additional comparisons and clarifications that strengthen the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Meta-Referential Games to Learn Compositional Learning Behaviours\" proposes a novel benchmark to investigate state-of-the-art artificial agents' abilities to exhibit compositional learning behaviors. The authors introduce a Symbolic Continuous Stimulus (SCS) representation and cast the problem of learning compositional behaviors as a meta-reinforcement learning problem.\n\nThe reviewers have raised several concerns about the paper, including:\n\n1. **Clarity and readability**: Reviewers 1, 2, and 4 have mentioned that the paper is difficult to follow, with long and repetitive sentences, and lacks clear explanations of key concepts, such as the SCS representation and the meta-referential games.\n2. **Lack of detail**: Reviewers 2, 3, and 4 have pointed out that the paper lacks sufficient detail about the experimental setup, model architectures, and training regimes, making it hard to understand and replicate the results.\n3. **Limited evaluation**: Reviewers 1, 2, and 4 have noted that the experimental evaluation is limited, with few ablation studies and no comparison to other representations, such as one-hot encodings.\n4. **Technical novelty and significance**: Reviewers 1, 2, 3, and 4 have questioned the technical novelty and significance of the paper, with some arguing that the contributions are only marginally significant or novel.\n5. **Empirical novelty and significance**: Reviewers 1, 2, 3, and 4 have also questioned the empirical novelty and significance of the paper, with some arguing that the results are not compelling and do not demonstrate the value of the SCS representation.\n\nDespite these concerns, the reviewers have also acknowledged that the paper has some promising ideas and that the problem of compositional learning is an important and interesting challenge.\n\nHowever, after carefully considering the reviewers' comments, I believe that the paper requires significant revisions to address the concerns raised. The lack of clarity, detail, and comprehensive evaluation, combined with the limited technical and empirical novelty, make it difficult to accept the paper in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents a theoretical analysis of the convergence properties of three classical value estimation algorithms (TD, FVI, and RM) in the overparameterized linear case. The authors provide a unified interpretation of these algorithms as minimizing the Euclidean norm of the weights subject to alternative constraints. They also propose two regularizers to improve the stability of TD and FVI and demonstrate their effectiveness in experiments.\n\nThe reviewers' comments and the authors' responses suggest that the paper makes a respectable contribution to the theory of temporal difference learning. However, some reviewers raised concerns about the novelty and significance of the results, the connection to deep RL, and the experimental evaluation.\n\nAfter carefully analyzing the paper and the reviewers' comments, I believe that the paper has several strengths:\n\n1. The paper provides a novel perspective on the convergence properties of TD, FVI, and RM in the overparameterized linear case.\n2. The authors provide a unified interpretation of these algorithms, which can help to better understand their behavior.\n3. The proposed regularizers are shown to be effective in improving the stability of TD and FVI.\n\nHowever, the paper also has some weaknesses:\n\n1. The connection to deep RL is not fully clear, and some reviewers questioned the relevance of the overparameterized linear case to deep RL.\n2. The experimental evaluation is limited to simple domains, and some reviewers suggested that more extensive experiments are needed to demonstrate the effectiveness of the proposed regularizers.\n3. Some reviewers raised concerns about the novelty and significance of the results, suggesting that similar results may have been obtained in prior work.\n\nDespite these weaknesses, I believe that the paper makes a valuable contribution to the field of reinforcement learning. The authors have addressed the reviewers' concerns in their response, and the paper has been improved as a result.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper presents AutoOED, an Automated Optimal Experimental Design platform that utilizes machine learning to accelerate the discovery of solutions with optimal objective trade-offs. The platform implements popular multi-objective Bayesian optimization (MOBO) algorithms and introduces a novel strategy called Believer-Penalizer (BP) for asynchronous batch experiments. The authors also provide a graphical user interface (GUI) for users without coding experience.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewers praise the paper's clarity, organization, and the potential impact of AutoOED on the field of optimal experimental design. However, they also raise concerns about the paper's scientific contributions, the lack of theoretical proof for the BP strategy, and the limited novelty of the implemented methods.\n\nReviewer 1 is not convinced by the overall performance of AutoOED, citing a mixed picture in Figure 5. Reviewer 2 views the paper as more of a software manual than a scientifically methodological paper, with limited methodology contributions. Reviewer 3 questions the scientific contribution of the paper, stating that it is more engineering-oriented and lacks convincing theoretical work. Reviewer 4 acknowledges the paper's engineering contributions but finds the novelty to be minor and the claims in the experiments to be overstated. Reviewer 5 is more positive, appreciating the paper's clarity and the motivation of the problem, but also points out minor issues.\n\nThe authors' response addresses the reviewers' concerns, providing additional explanations and updates to the paper. They clarify the advantages of AutoOED, including its user-friendliness, modular design, and support for asynchronous MOBO. They also acknowledge the limitations of the paper, such as the lack of theoretical proof for BP, and provide more intuitive explanations and analyses.\n\nAfter carefully considering the reviewers' comments and the authors' response, I conclude that the paper has both strengths and weaknesses. While the paper presents a well-organized and clear description of AutoOED, its scientific contributions are limited, and the novelty of the implemented methods is not significant. The lack of theoretical proof for the BP strategy and the limited comparison with other existing BO packages are notable weaknesses.\n\nHowever, the paper's engineering contributions, including the development of a user-friendly GUI and a modular framework, are significant. The authors' response addresses many of the reviewers' concerns, and the updated paper provides more clarity and explanations.\n\nGiven the mixed evaluation, I would like to see more significant scientific contributions and a more thorough comparison with existing methods. However, considering the paper's potential impact on the field of optimal experimental design and the authors' efforts to address the reviewers' concerns, I believe that the paper deserves a chance to be presented at the conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review presents a novel approach to improving the accuracy and robustness of Recurrent Neural Networks (RNNs) in the context of hierarchical control of continually learned autonomous motor motifs. The authors draw inspiration from neuroscience, specifically the thalamocortical circuit, to design a preparatory module that enhances the performance of RNNs in generating flexible sequences of motor motifs.\n\nThe reviewers' comments highlight several strengths and weaknesses of the paper. On the positive side, the paper tackles an important problem in the field of continual learning and motor control, and the proposed approach shows promising results. The use of insights from neuroscience to inform the design of the preparatory module is also seen as a strength. Additionally, the paper provides a clear and well-structured presentation of the results, and the authors have made an effort to address the reviewers' comments and concerns.\n\nHowever, the reviewers also raise several concerns and criticisms. Some of the main issues include:\n\n1. **Lack of comparison to alternative approaches**: The reviewers feel that the authors have not adequately compared their approach to other possible solutions, such as training the RNNs to generalize from end states or using a reset strategy to a fixed state.\n2. **Limited scalability**: The reviewers question the scalability of the approach, given the small size of the networks and the limited number of motifs used in the experiments.\n3. **Biological plausibility**: Some reviewers express concerns about the biological plausibility of the preparatory module, suggesting that it may not accurately reflect the neural mechanisms underlying motor control.\n4. **Writing and presentation**: The reviewers criticize the writing and presentation of the paper, finding it confusing and difficult to follow at times.\n\nIn response to these concerns, the authors have provided additional explanations and justifications for their approach. They argue that their method is designed to address the specific challenges of continual learning and motor control, and that it offers advantages over alternative approaches. They also acknowledge the limitations of their current implementation and suggest potential avenues for future work.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I believe that the paper has potential but requires significant revisions to address the concerns raised. The authors have made a good effort to address the reviewers' comments, but some issues remain unresolved.\n\nThe main strengths of the paper, including its novel approach and promising results, are outweighed by the significant weaknesses, including the lack of comparison to alternative approaches, limited scalability, and biological plausibility concerns. While the authors have provided some additional explanations and justifications, these concerns require more thorough addressing.\n\nTherefore, based on the analysis of the paper and the reviewers' comments, I conclude that the paper requires major revisions to address the concerns raised.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper under review presents a theoretical analysis of the epoch-wise double descent phenomenon in neural networks, using a linear teacher-student model. The authors derive closed-form expressions for the evolution of the generalization error over training time and demonstrate that double descent can be attributed to distinct features being learned at different scales.\n\nThe reviewer comments are overall positive, with Reviewer 1 praising the paper as \"insightful\" and \"well-motivated\", and Reviewer 2 describing it as \"solid\" with \"theoretical analysis and empirical evaluation\". Reviewer 3 also finds the paper \"illuminating\" and notes that the topic is of general interest.\n\nThe authors have addressed the reviewer comments in their response, providing additional explanations, clarifications, and references to related work. They have also corrected typos and errors, and improved the writing and terminology.\n\nThe paper's strengths include:\n\n1. Novel theoretical analysis: The paper presents a new theoretical framework for understanding epoch-wise double descent, which is a phenomenon that has been observed in neural networks but not fully understood.\n2. Clear and well-motivated writing: The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach.\n3. Empirical evaluation: The paper includes empirical evaluations of the theoretical results, which helps to validate the authors' claims.\n\nThe paper's weaknesses include:\n\n1. Limited novelty: Reviewer 2 notes that the paper's contributions are \"significant and somewhat new\", but that aspects of the contributions exist in prior work.\n2. Technical issues: Reviewer 3 points out several technical issues, including typos, errors, and inconsistencies in notation.\n3. Limited connection to realistic settings: Reviewer 3 notes that the paper could improve by making a stronger connection to realistic settings, such as deep neural networks with label noise on training data.\n\nOverall, the paper's strengths outweigh its weaknesses. The authors have made a significant contribution to the understanding of epoch-wise double descent, and their theoretical analysis and empirical evaluation provide valuable insights into this phenomenon.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper presents a novel approach to neural network compression using tropical geometry, specifically by representing neural networks as tropical polynomials and applying zonotope reduction techniques. The authors provide a theoretical foundation for their method, including bounds on the approximation error, and demonstrate its effectiveness through experiments on several datasets and architectures.\n\nThe reviewers have raised several concerns and suggestions for improvement, including:\n\n1. **Limited experimental evaluation**: The experiments are mainly proof-of-concept and focus on small-scale datasets and architectures. Reviewers have suggested evaluating the method on larger datasets and more modern architectures.\n2. **Lack of comparison to state-of-the-art methods**: The authors compare their method to a few baseline methods, but reviewers have suggested comparing to more recent and state-of-the-art pruning techniques.\n3. **Unclear scope and contribution**: Reviewers have noted that the paper's scope and contribution are not entirely clear, and that the authors could better emphasize the theoretical significance of their work.\n4. **Minor issues and typos**: Reviewers have pointed out several minor issues, including typos, unclear notation, and missing information.\n\nThe authors have responded to these concerns, providing additional context and clarification on their method and experiments. They have also addressed the minor issues and typos pointed out by the reviewers.\n\nOverall, the paper presents a novel and interesting approach to neural network compression, with a strong theoretical foundation. While the experimental evaluation is limited, the authors have demonstrated the effectiveness of their method on several datasets and architectures. The reviewers' concerns and suggestions have been largely addressed by the authors, and the paper has been improved as a result.\n\nHowever, considering the standards of a top-tier conference, the paper's limitations in terms of experimental evaluation and comparison to state-of-the-art methods are significant. Additionally, the paper's scope and contribution, while interesting and novel, may not be entirely clear to all readers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Natural Language Descriptions of Deep Visual Features\" presents a novel approach to interpreting deep learning models for computer vision by generating natural language descriptions of neurons in the model. The method, called MILAN, uses mutual information to guide the linguistic annotation of neurons, resulting in fine-grained descriptions that capture categorical, relational, and logical structure in learned features.\n\nThe paper is well-written, and the authors provide sufficient technical details for readers to understand and reproduce their work. The experiments demonstrate the effectiveness of MILAN in generating descriptions that agree with human-generated feature descriptions across a diverse set of model architectures and tasks. The authors also highlight three applications of natural language neuron descriptions: analysis, auditing, and editing.\n\nThe reviewer comments are generally positive, with some suggestions for improvement. Reviewer 2 raises questions about the inter-annotator agreement and the potential for scaling up the method using larger datasets. Reviewer 3 suggests that the paper could benefit from a more detailed analysis of the limits of the approach. Reviewer 4 raises concerns about the scope of the work and the potential for applying the method to non-vision tasks.\n\nThe author response addresses these concerns and provides additional information about the methodology and results. The authors clarify that they did not implement special controls for bias in the dataset but acknowledge that the dataset may inherit biases from the underlying datasets. They also discuss the potential for applying the method to non-vision tasks and the challenges involved in doing so.\n\nOverall, the paper presents a significant contribution to the field of explainable AI and deep learning. The methodology is sound, and the results are impressive. While there are some limitations and potential biases in the dataset, the authors acknowledge these and provide a clear discussion of the limitations of their approach.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" proposes a novel framework for synthesizing data using deep generative models in a differentially private manner. The reviewers have provided detailed comments and suggestions, which the authors have addressed in their response.\n\nThe strengths of the paper include:\n\n1. Novel approach: The paper proposes a new framework for private data synthesis, which is a significant contribution to the field.\n2. Theoretical guarantees: The paper provides theoretical guarantees of performance, which is a strong aspect of the work.\n3. Empirical evaluations: The paper presents empirical evaluations on multiple datasets, which demonstrate the effectiveness of the proposed approach.\n4. Well-written: The paper is well-written, and the authors have made an effort to address the reviewers' comments and suggestions.\n\nThe weaknesses of the paper include:\n\n1. Lack of thorough benchmarking: The reviewers have pointed out that the paper lacks thorough benchmarking, particularly for tabular data and image data.\n2. Limited comparison to other methods: The paper could benefit from a more comprehensive comparison to other methods, such as PATE-GAN and CTGAN.\n3. Need for more justifications: The reviewers have requested more justifications for the novelty claims and the choice of methods.\n4. Lack of complexity analysis: The paper could benefit from a more detailed analysis of the computational complexity of the proposed algorithm.\n\nThe authors have addressed some of the weaknesses in their response, including:\n\n1. Adding more baselines and datasets: The authors have added more baselines and datasets, which strengthens the empirical evaluations.\n2. Providing more justifications: The authors have provided more justifications for their methods and novelty claims.\n3. Analyzing computational complexity: The authors have provided a detailed analysis of the computational complexity of the proposed algorithm.\n\nHowever, some weaknesses remain, such as:\n\n1. Limited comparison to other methods: The paper could still benefit from a more comprehensive comparison to other methods.\n2. Lack of evaluation on more complex datasets: The paper could benefit from evaluations on more complex datasets, such as CIFAR-10.\n\nOverall, the paper has made significant contributions to the field of private data synthesis, and the authors have addressed some of the weaknesses in their response. However, some weaknesses remain, and the paper could benefit from further improvements.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Variational Neural Cellular Automata\" proposes a novel generative model that combines the concepts of neural cellular automata (NCA) and variational autoencoders (VAE). The authors demonstrate the ability of their model, called Variational NCA (VNCA), to learn complex data distributions through morphogenesis and show resilience to damage despite computational constraints.\n\nThe reviewer comments provide a comprehensive evaluation of the paper, highlighting both strengths and weaknesses. Reviewers praise the paper's clarity, reproducibility, and the interesting idea of combining NCA and VAE. However, some reviewers express concerns about the experimental results, particularly the underwhelming performance on certain datasets and the lack of comparison to other methods.\n\nThe authors' response to the reviewer comments is thorough and addresses many of the concerns raised. They provide additional experiments, clarify misunderstandings, and acknowledge the limitations of their approach. The new experiments, such as the damage recovery analysis and the latent space analysis, add significant value to the paper and demonstrate the potential of the VNCA model.\n\nOne of the main concerns raised by reviewers is the comparison to other methods. The authors acknowledge that their model may not be state-of-the-art in terms of generative performance but argue that their approach has unique strengths, such as the ability to model complex data distributions through morphogenesis and resilience to damage. While the comparison to other methods is limited, the authors provide a fair and unbiased evaluation of their model, which is essential for advancing the field.\n\nAnother concern is the novelty of the approach. Some reviewers argue that the idea of combining NCA and VAE is not entirely new, and that similar concepts have been explored in other works. However, the authors clarify that their contribution lies in the specific implementation of the VNCA model and the demonstration of its ability to model complex data distributions through morphogenesis.\n\nOverall, the paper presents a novel and interesting approach to generative modeling, and the authors' response to the reviewer comments demonstrates their willingness to address concerns and improve the paper. While the experimental results may not be overwhelming, the paper's strengths, including its clarity, reproducibility, and potential for future research, outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Behavior Proximal Policy Optimization\" presents a novel approach to offline reinforcement learning (RL) by leveraging the concept of monotonic policy improvement. The authors propose a simple yet effective algorithm, BPPO, which builds upon the Proximal Policy Optimization (PPO) framework. The key insight is that online on-policy algorithms can be adapted to solve offline RL problems by exploiting the inherent conservatism of these algorithms.\n\nThe paper's strengths include:\n\n1. **Theoretical contributions**: The authors provide a thorough analysis of offline monotonic policy improvement, which is a significant contribution to the field.\n2. **Simple and effective algorithm**: BPPO is a straightforward extension of PPO, making it easy to implement and understand.\n3. **Empirical evaluations**: The authors conduct extensive experiments on the D4RL benchmark, demonstrating the competitiveness of BPPO with state-of-the-art offline RL algorithms.\n\nHowever, the paper also has some weaknesses and areas for improvement:\n\n1. **Assumption 1**: The original assumption 1 is too strong, and the authors have replaced it with a more reasonable alternative. However, this change may affect the theoretical results, and further clarification is needed.\n2. **Lack of ablation studies**: Some reviewers pointed out the need for additional ablation studies to better understand the impact of hyperparameters and the importance of certain components in the algorithm.\n3. **Comparison to prior work**: While the authors compare BPPO to several baseline algorithms, some reviewers suggested including more state-of-the-art methods and providing a more detailed analysis of the results.\n4. **Code availability**: The authors promise to make their code public after the final decision, which is essential for reproducibility and further research.\n\nAfter carefully considering the reviewer comments and the authors' responses, I conclude that the paper has addressed most of the concerns and has made significant improvements. The theoretical contributions, simplicity, and effectiveness of the algorithm, as well as the empirical evaluations, demonstrate the paper's value to the research community.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper proposes a novel online knowledge distillation framework that utilizes a label prior shift to induce diversity among teachers and an aggregation strategy that combines post-compensation in specialist outputs with conventional model averaging. The authors demonstrate the effectiveness of their approach through extensive experiments on various datasets, including CIFAR-10/100 and ImageNet.\n\nThe reviewers have raised several concerns and suggestions, which the authors have addressed in their responses. The main concerns include:\n\n1. **Novelty and significance**: Reviewers have questioned the novelty and significance of the proposed approach, suggesting that it combines existing methods without clear explanations. The authors have responded by highlighting the unique aspects of their approach, such as the use of label prior shift and importance sampling in online knowledge distillation.\n2. **Comparison to existing methods**: Reviewers have requested more comprehensive comparisons to existing methods, including PCL, MVCL, and CGL. The authors have provided additional experimental results, demonstrating the superiority of their approach over these methods in various settings.\n3. **Experimental results**: Reviewers have pointed out inconsistencies in the reported results and suggested additional experiments to validate the effectiveness of the proposed approach. The authors have conducted these experiments and provided new results, which support their claims.\n4. **Writing and clarity**: Reviewers have suggested improvements to the writing and clarity of the paper, including corrections to typos and ambiguous sentences. The authors have addressed these concerns and clarified their writing.\n\nAfter carefully analyzing the paper, reviewer comments, and author responses, I conclude that the paper has addressed the major concerns and demonstrated the effectiveness of the proposed approach. The authors have provided additional experimental results, clarified their writing, and highlighted the novelty and significance of their contribution.\n\nHowever, I also note that some reviewers have expressed concerns about the marginal significance and novelty of the contributions, which might be a limitation of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" proposes a novel framework that combines Monte-Carlo Tree Search (MCTS) with the Transformer architecture in offline reinforcement learning. The idea of combining these two approaches is intriguing, and the paper presents some promising results on the SameGame domain.\n\nHowever, upon closer examination, several concerns arise that impact the paper's overall quality, novelty, and significance. \n\n1. **Lack of Clarity and Presentation**: Reviewer 1 criticizes the paper's writing and presentation, stating that it is \"extremely unclear\" and requires multiple passes to understand. This lack of clarity can hinder the reader's ability to fully appreciate the method and its contributions. Although Reviewer 3 finds the presentation easy to follow despite typos and grammar errors, the consensus among reviewers highlights the need for improvement in this area.\n\n2. **Limited Empirical Evaluation**: The paper's empirical evaluation is limited to the SameGame domain, which Reviewer 1 describes as \"relatively toy-ish.\" This limited scope makes it challenging to assess the method's generalizability and effectiveness across different domains. Reviewers 1, 2, and 3 all suggest comparing the MCTransformer with other state-of-the-art methods, including Decision Transformers, Online DTs, AlphaGo, AlphaZero, and MuZero, to provide a more comprehensive understanding of its performance.\n\n3. **Novelty Concerns**: Reviewers 2 and 3 express concerns about the paper's novelty, suggesting that the combination of MCTS and Transformers, while interesting, does not significantly deviate from existing approaches like AlphaGo/AlphaZero. The method's originality is questioned, with Reviewer 2 stating that the structure is \"identical\" to AlphaGo/AlphaZero's setting, except for replacing the neural network with a Transformer.\n\n4. **Methodological Concerns**: Reviewer 2 points out that the comparison between MCTransformer and MCTS-only solutions might be unfair, as the MCTS-based program could execute more simulation counts without using neural networks. This raises questions about the fairness and convincingness of the empirical evaluation.\n\n5. **Reproducibility**: While Reviewer 3 mentions that the paper provides enough information for reproducibility, the lack of source code and detailed experimental settings (e.g., specific hyperparameters, computational resources) could hinder exact replication.\n\nGiven these considerations, the paper faces significant challenges in terms of clarity, empirical evaluation, novelty, and methodological soundness. While the idea of combining MCTS with Transformers is promising, the current presentation and evaluation do not sufficiently support the paper's claims of superiority over existing methods or demonstrate significant novelty.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration\" proposes a new contrastive token learning objective to address the issue of text degeneration in autoregressive language models. The paper is well-written, easy to follow, and presents a clear motivation for the proposed approach. However, after carefully analyzing the reviewer comments, several concerns arise that impact the paper's overall quality, novelty, and significance.\n\nFirstly, the novelty of the paper is questioned by multiple reviewers. Reviewer 1 states that the core idea of penalizing previously generated tokens has been proposed before, and the authors' claim that unlikelihood training only considers the probabilities of negative tokens is disputed. Reviewer 3 also mentions that the idea is very similar to \"unlikelihood training.\" This lack of novelty is a significant concern, as top-tier conferences typically expect innovative and groundbreaking research.\n\nSecondly, the experimental setup and results are criticized by several reviewers. Reviewer 1 suggests that the experiments are insufficient and that the metrics used (e.g., perplexity) are not reliable for evaluating text generation tasks. Reviewer 2 points out that the experimental setup is weak, using only GPT2-small fine-tuned on WikiText, and that the results are not convincing. Reviewer 4 also questions the importance of the repetition issue for better or larger models and suggests that scaling experiments could help resolve this question.\n\nThirdly, the human evaluation results are not statistically significant, which raises concerns about the effectiveness of the proposed approach. Reviewer 4 mentions that the human evaluation does not show a preference for the proposed method over one of the baselines (UL-TS).\n\nLastly, the author response is missing, which would have provided an opportunity for the authors to address the reviewers' concerns and clarify any misunderstandings.\n\nConsidering these concerns, the paper's overall quality, novelty, and significance are not sufficient to meet the standards of a top-tier conference. While the paper is well-written and presents an interesting idea, the lack of novelty, insufficient experiments, and non-statistically significant human evaluation results are significant drawbacks.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" proposes a novel approach to detecting out-of-distribution (OOD) data by leveraging graph structures and topological properties. The authors claim that their method achieves high performance on both far-OOD and near-OOD detection tasks, with AUROC scores of 97.95% and 98.79%, respectively.\n\nHowever, upon careful analysis of the reviewer comments, several concerns and weaknesses are raised that cast doubt on the paper's suitability for publication at a top-tier conference. Here are the key issues:\n\n1. **Lack of clarity and poor writing**: Multiple reviewers (Reviewer 1, Reviewer 2, Reviewer 3, and Reviewer 4) criticize the paper's writing style, noting that it is poorly written, lacks detail, and contains grammatical mistakes. This makes it difficult for readers to understand the methodology and results.\n2. **Insufficient experimental validation**: Reviewers 2 and 4 point out that the paper lacks comprehensive experimental comparisons with state-of-the-art methods, which is a crucial aspect of evaluating the effectiveness of a new approach. The authors only compare their method to some graph embedding algorithms, which is not sufficient.\n3. **Limited novelty**: Reviewers 1, 3, and 4 question the novelty of the paper, suggesting that the approach is not significantly new or innovative. The method combines existing techniques, and the authors fail to provide a clear and rigorous demonstration of their claims.\n4. **Overreliance on pre-trained object detection network**: Reviewer 4 notes that the proposed method relies heavily on a pre-trained object detection network, which may not be suitable for all types of in-distribution data (e.g., medical images). This limitation is not adequately addressed in the paper.\n5. **Lack of ablation studies and interpretability analysis**: Reviewer 4 suggests that the paper lacks essential ablation studies to analyze the impact of hyperparameters and the interpretability of the OOD detection results using semantic graphs.\n\nGiven these concerns, it is clear that the paper requires significant revisions to address the issues raised by the reviewers. The authors need to improve the writing style, provide more comprehensive experimental comparisons, and demonstrate the novelty and significance of their approach more rigorously.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" provides a convergence analysis of gradient descent for deep linear neural networks with different random initialization. The paper extends existing results to general strongly convex and smooth objectives, and it shows that the convergence rate does not depend on the types of random initialization. The depth of the network does not affect the optimal rate of convergence, provided that the width of each hidden layer is appropriately large.\n\nThe reviewers have provided detailed comments on the paper, and the authors have responded to these comments. The main concerns of the reviewers are:\n\n1. Novelty: Some reviewers question the novelty of the paper, as the results are similar to those in existing papers, such as Du and Hu (2019).\n2. Technical extension: The authors claim that their results are a technical extension of existing results, but some reviewers are not convinced.\n3. Lack of insights: Some reviewers feel that the paper lacks insights and discussions, particularly on the convergence region part.\n4. Overclaims: Some reviewers point out that the title and abstract of the paper are overly broad and do not accurately reflect the results.\n\nThe authors have responded to these concerns by:\n\n1. Providing more details on the technical extension and how it differs from existing results.\n2. Adding a new section to discuss the insights and implications of their results.\n3. Clarifying the title and abstract to better reflect the results.\n4. Addressing the concerns about the convergence region and the minimum width requirement.\n\nAfter carefully analyzing the paper and the reviewer comments, I conclude that the paper provides a significant contribution to the field of deep learning. The authors have addressed the concerns of the reviewers and have provided a clear and detailed response to the comments.\n\nThe paper's strengths include:\n\n1. The paper provides a sharp convergence analysis of gradient descent for deep linear neural networks.\n2. The results are novel and extend existing results to general strongly convex and smooth objectives.\n3. The paper provides insights into the convergence region and the minimum width requirement.\n\nThe paper's weaknesses include:\n\n1. The paper could benefit from more discussions and insights into the results.\n2. The title and abstract could be improved to better reflect the results.\n\nOverall, I believe that the paper is well-written, and the authors have provided a clear and detailed response to the reviewer comments. The paper provides a significant contribution to the field of deep learning, and I recommend acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting observation about the interaction between batch normalization and bounded activation functions. The authors claim that swapping the order of batch normalization and activation functions can lead to improved performance when using bounded activation functions like Tanh. They also investigate the reasons behind this improvement and propose that asymmetric saturation and sparsity are key factors.\n\nThe reviewers have raised several concerns and questions about the paper, including the clarity of the claims, the experimental setup, and the analysis of the results. The authors have responded to these concerns and provided additional explanations and experiments to support their claims.\n\nAfter carefully analyzing the paper and the reviewer comments, I have identified several strengths and weaknesses. The strengths include:\n\n* The paper presents an interesting and novel observation about the interaction between batch normalization and bounded activation functions.\n* The authors have conducted extensive experiments to support their claims and have provided additional explanations and experiments to address the concerns raised by the reviewers.\n* The paper has the potential to contribute to the understanding of deep neural networks and the design of more effective architectures.\n\nThe weaknesses include:\n\n* The paper's claims and analysis are not always clear and rigorous, and some of the reviewer comments have highlighted potential flaws in the reasoning and methodology.\n* The experimental setup and analysis are limited to a specific set of architectures and datasets, and it is not clear how generalizable the results are to other settings.\n* Some of the reviewer comments have raised concerns about the potential for overfitting and the lack of comparison to other architectures and techniques.\n\nOverall, while the paper has some strengths and presents an interesting observation, the weaknesses and concerns raised by the reviewers suggest that the paper is not yet ready for publication in its current form. The authors need to address the concerns and flaws highlighted by the reviewers and provide more rigorous and comprehensive analysis and experiments to support their claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" proposes a new adversarial training algorithm that applies more regularization to data vulnerable to adversarial attacks. The algorithm is theoretically motivated and empirically superior to other existing algorithms.\n\nThe paper's strengths include:\n\n1. **Theoretical motivation**: The paper provides a theoretical justification for the proposed algorithm, which is based on minimizing an upper bound of the robust risk.\n2. **Empirical superiority**: The paper presents extensive experimental results showing that the proposed algorithm outperforms other state-of-the-art methods in terms of both clean and robust accuracy.\n3. **Improved fairness**: The paper demonstrates that the proposed algorithm improves fairness in class-wise robustness compared to other methods.\n\nHowever, the paper also has some weaknesses:\n\n1. **Marginal improvements**: Some reviewers noted that the improvements over existing methods are marginal, which might not be significant enough to warrant publication in a top-tier conference.\n2. **Similarity to existing methods**: The proposed algorithm is similar to existing methods, such as MART, which might raise concerns about its novelty and originality.\n3. **Theoretical contributions**: Some reviewers questioned the significance and novelty of the theoretical contributions, particularly with regards to the upper bound of the robust risk.\n\nAfter carefully analyzing the paper and the reviewer comments, I conclude that the paper's strengths outweigh its weaknesses. The paper provides a theoretically motivated algorithm that empirically outperforms existing methods, and its improvements in fairness are a significant contribution. While the improvements may be marginal in some cases, the paper's overall quality and significance warrant publication in a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" presents an interesting observation that swapping the order of batch normalization and bounded activation functions can lead to improved performance in certain architectures. The authors provide empirical evidence to support their claims and propose metrics to measure saturation and sparsity.\n\nHowever, the paper has several weaknesses that need to be addressed. Firstly, the claims about the benefits of asymmetric saturation and its relationship to generalization performance are not clearly supported by the experiments. The authors' argument that asymmetric saturation leads to higher sparsity, which in turn improves generalization, is not convincing. The experiments show that the relationship between saturation, sparsity, and generalization performance is more complex and depends on various factors, such as the depth of the layers and the specific architecture used.\n\nSecondly, the paper lacks a clear and rigorous definition of the \"center of the function\" and its implications for the analysis. The authors' response to the reviewers' comments clarifies some of these issues, but the paper could benefit from a more detailed and precise explanation of these concepts.\n\nThirdly, the experimental setup and analysis have some limitations. The authors only consider a specific set of architectures and activation functions, and the results may not generalize to other settings. The use of a large model (VGG16) on a small dataset (CIFAR-100) may lead to overparameterization, which could affect the results.\n\nLastly, the paper could benefit from more detailed and thorough comparisons with existing work, particularly in the context of residual connections and the use of bounded activation functions.\n\nDespite these weaknesses, the paper has some strengths, such as the interesting observation about the benefits of swapping the order of batch normalization and bounded activation functions, and the proposal of new metrics to measure saturation and sparsity.\n\nAfter carefully considering the paper's strengths and weaknesses, I conclude that the paper requires significant revisions to address the concerns raised by the reviewers. While the paper has some interesting ideas and contributions, the current version lacks clarity, rigor, and convincing evidence to support its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper \"CAT: Collaborative Adversarial Training\" should be accepted or rejected for publication at a top-tier conference, we need to consider several factors based on the provided information, including the paper's content, reviewer comments, and the standards of a top-tier conference.\n\n1. **Novelty and Originality**: The reviewers have mixed opinions on the novelty of the proposed method. Some consider it marginally novel, while others see it as not particularly new, citing existing ensemble methods for adversarial training. The lack of a clear, groundbreaking contribution might be a concern.\n\n2. **Technical Quality and Significance**: The paper demonstrates improved robustness against AutoAttack on CIFAR-10 and CIFAR-100 datasets, which is a positive aspect. However, the reviewers have raised several questions about the theoretical analysis, the necessity of training two models from scratch, and the comparison with other baselines, especially ensemble methods and data augmentation approaches.\n\n3. **Empirical Evidence and Reproducibility**: The experiments are extensive but limited to small-scale datasets (CIFAR-10 and CIFAR-100). Reviewers have suggested that larger datasets (like ImageNet) and more network architectures should be considered for a more comprehensive evaluation. The reproducibility of the results is also questioned, with some reviewers suggesting that the code should be made available.\n\n4. **Clarity and Writing Quality**: The paper is generally considered well-written and easy to follow, which is a positive aspect. However, there are suggestions for improvement, including addressing typos and providing clearer explanations for some concepts.\n\n5. **Reviewer Consensus**: The reviewers have identified several weaknesses, including the lack of theoretical analysis, increased computational cost, limited experimental analysis, and the need for comparison with more recent and stronger baselines. While some reviewers see the approach as interesting and marginally novel, others are more critical about its novelty and significance.\n\nConsidering these factors, the paper has some merits, such as its well-written nature and the demonstration of improved robustness on certain benchmarks. However, it also has significant drawbacks, including concerns about novelty, the lack of comprehensive theoretical and empirical analysis, and the need for more rigorous comparisons with existing methods.\n\nGiven the standards of a top-tier conference, which typically require high novelty, significant technical contributions, thorough analysis, and comprehensive evaluation, this paper falls short in several key areas. The concerns raised by the reviewers about the paper's novelty, the lack of detailed analysis, and the limited scope of the experiments are significant and impact the paper's potential for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" proposes a bi-objective optimization model to simultaneously optimize the agents' skill level and playing style in self-play reinforcement learning. The model is designed to maintain a population of agents with diverse playing styles and high skill levels throughout the training process.\n\nThe reviewers have provided detailed comments on the paper, highlighting both strengths and weaknesses. Reviewer 1 points out that the paper misses an important part of the literature, specifically the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al (2017), which could be used as a baseline in the experiments. Reviewer 1 also criticizes the experimental design, arguing that it gives an advantage to the proposed method (BiO) and EMOGI, as they represent a larger share of the pool of agents. Reviewer 2 finds the paper to be well-written and easy to read, but requires more clarification on some parts, such as the description of \"playing style\" and the experimental results. Reviewer 3 appreciates the conceptual simplicity of the algorithm but finds the empirical results to be narrow, with only two domains evaluated and no comparison to quality-diversity algorithms.\n\nDespite the criticisms, the reviewers agree that the paper deals with a challenging and important problem, and the proposed method shows promising results. However, the lack of comparison to relevant baselines, such as quality-diversity algorithms, and the narrow scope of the empirical evaluation are significant weaknesses.\n\nConsidering the comments and criticisms from the reviewers, I believe that the paper requires significant revisions to address the weaknesses and improve the overall quality. The authors should provide a more comprehensive literature review, including the paper by Lanctot et al (2017), and compare their method to relevant baselines, such as quality-diversity algorithms. Additionally, the authors should provide more detailed experimental results, including confidence intervals and matchup tables, to support their claims.\n\nIn conclusion, while the paper shows promise, it requires significant revisions to address the weaknesses and improve the overall quality. Therefore, I recommend rejecting the paper in its current form, but encouraging the authors to revise and resubmit.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Regression with Label Differential Privacy\" presents a novel approach to training regression models with label differential privacy (DP) guarantees. The authors propose a label DP randomization mechanism, called \"randomized response on bins,\" which is optimal under a given regression loss function. The paper provides a thorough theoretical analysis, including proofs of the mechanism's optimality, and demonstrates its efficacy through extensive experiments on several datasets.\n\nThe reviewer comments are generally positive, with most reviewers praising the paper's clarity, technical novelty, and empirical significance. Reviewer 3 notes that the paper is easy to follow and has the potential for a positive impact on the DP community. Reviewer 4 praises the paper's writing, novelty, and experimental results. Reviewer 5 finds the paper well-written and enjoyable to read, although they raise some concerns about the practicality of the approach and the comparison to prior work.\n\nSome reviewers raise minor concerns and suggestions for improvement. Reviewer 1 questions the distinction between regression and classification, but the authors provide a clear response clarifying the difference. Reviewer 2 suggests including an additional experiment in the appendix, which the authors agree to do. Reviewer 3 asks about the comparison to other label-DP methods, and the authors provide a response discussing the differences between their approach and prior work.\n\nReviewer 4 raises two weaknesses: (1) the proposed mechanism may not be optimal when considering the second term in Equation (1), which represents the Bayes optimal error; and (2) the number of optimal bins may be smaller than the number of different labels. The authors respond to these concerns, acknowledging the potential for future work on the first point and providing empirical evidence for the second point.\n\nReviewer 5 notes that the paper's contributions are not entirely novel, as they build upon prior work by Ghazi et al. (2021). However, the authors argue that their approach is more general and can be applied to both classification and regression problems, whereas Ghazi et al.'s work is specific to classification.\n\nAfter carefully considering the reviewer comments and the authors' responses, I believe that the paper makes a significant contribution to the field of differential privacy and regression analysis. The authors have addressed the minor concerns and suggestions raised by the reviewers, and the paper is well-written and easy to follow.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Functional Risk Minimization\" presents a novel framework for supervised learning problems, challenging the classic assumption of data coming from a single function followed by some noise in output space. Instead, it models each data point as coming from its own function, leading to the derivation of Functional Risk Minimization (FRM), a general framework for scalable training objectives. The paper shows that FRM can subsume Empirical Risk Minimization (ERM) for many common loss functions and provides an avenue for more realistic noise processes.\n\nThe reviewers have identified several strengths and weaknesses of the paper. The main strengths include:\n\n1. **Novelty and Potential Impact**: The paper presents a very interesting and novel take on representing noise in a learning framework, which could be highly impactful as it may allow for more easily dealing with the variation in real datasets than standard MLE in additive noise models.\n2. **Empirical Results**: The empirical results on linear regression with non-uniform noise seem to be strong, demonstrating that as the noise distribution changes, FRM begins to outperform ERM.\n3. **Well-written Article**: The paper is well-written, with examples given to show the motivation and clarify some concepts.\n\nHowever, the reviewers have also identified several weaknesses, including:\n\n1. **Lack of Clarity**: The presentation is confusing, with non-standard use of terms like \"ERM\" and unclear distinction between the FGM modeling assumption and the FRM learning objective.\n2. **Scalability Concerns**: The implementable algorithm as proposed does not seem to be scalable, requiring significant approximation and Hessian information that could be difficult to obtain for large models.\n3. **Lack of Theoretical Guarantees**: There is no formal argument made as to why or when FRM will outperform ERM, and no arguments showing that the FRM minimizer will achieve low loss or that the finite-sample version of FRM approximates the population variant.\n\nThe authors have responded to these concerns, providing additional experiments and clarifications on scalability. They have shown that FRM can be scalable to real datasets and neural networks, and that it can lead to better representations in the bottleneck layer of a VAE. They have also addressed concerns about the scalability of FRM, discussing the accuracy and scalability of the approximation and highlighting the potential for FRM to lead to radically increasing the scalability of NN training.\n\nAfter carefully considering the strengths and weaknesses of the paper, as well as the authors' responses to the reviewers' concerns, I believe that the paper has the potential to make a significant contribution to the field of machine learning. While there are still some concerns about clarity and scalability, the authors have made a good faith effort to address these concerns and provide additional evidence to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a novel scheme, dubbed Proximal Validation Protocol (PVP), aimed at resolving the issue of validation set construction in machine learning development. This protocol involves creating a proximal set as a substitute for the traditional validation set through dense data augmentation followed by a novel distributional-consistent sampling algorithm. The authors claim that PVP works better than existing validation protocols on three data modalities (images, text, and tabular data), demonstrating its feasibility towards ML production.\n\nHowever, upon careful analysis of the reviewer comments, several concerns and weaknesses are highlighted that impact the paper's overall quality, novelty, and significance. \n\n1. **Lack of Clarity and Presentation**: Reviewer 1 and Reviewer 3 mention that the paper is not presented clearly, particularly in the experiments section. This lack of clarity makes it hard to evaluate the paper's contributions properly. For instance, the definition of 'F1' score and the representation of values in tables 3 and 4 are not clear, which could lead to misunderstandings about the results.\n\n2. **Insufficient Empirical Evaluation**: Reviewers 1, 2, and 3 point out the need for more thorough empirical evaluations. This includes testing PVP on larger datasets (e.g., original CIFAR dataset or ImageNet), exploring the impact of different data augmentation choices, and conducting ablation studies on key hyperparameters. The current evaluation seems limited, which raises questions about the robustness and generalizability of PVP.\n\n3. **Theoretical Guarantees**: Reviewer 2 and Reviewer 4 emphasize the lack of theoretical analysis or guarantees to support the claims made about PVP. This is a significant concern because, without theoretical backing, it's challenging to understand why PVP should work or trust its performance in various scenarios.\n\n4. **Applicability and Generalizability**: Reviewer 3 questions the applicability of PVP to tasks beyond classification, such as object detection, semantic segmentation, or unsupervised learning regimes. This raises concerns about the protocol's versatility and whether it addresses a broad range of ML development needs.\n\n5. **Statistical Significance**: Reviewer 3 suggests that presenting statistical significance results, especially in tables 1 and 2, would strengthen the paper. This omission makes it difficult to assess the reliability of the reported improvements.\n\nGiven these concerns, while the paper tackles an important issue in machine learning and proposes a novel approach, the lack of clarity, insufficient empirical evaluation, absence of theoretical guarantees, limited applicability, and omission of statistical significance analyses are significant drawbacks. These issues undermine the paper's overall quality, novelty, and significance, making it less suitable for publication in a top-tier conference in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" proposes a novel method for predicting protein-protein interactions (PPIs) by leveraging pretrained structure embeddings and graph neural networks (GNNs). The authors claim that their approach leads to significant improvements in PPI prediction compared to sequence and network-based methods.\n\nHowever, upon careful analysis of the reviewer comments, several concerns and limitations are raised. Reviewer 1 notes that the contribution and novelty of the paper are limited, as the pretrained structural embedding is directly adopted from OmegaFold, and the main contribution is using GNNs with structural embeddings as inputs for PPI prediction. Reviewer 1 also points out inconsistencies in the results and raises questions about the protein-level embedding used in the paper.\n\nReviewer 2 agrees that the novelty is too limited, as the approach combines existing methods without significant modifications. Reviewer 3 finds the paper unclear, with marginal improvements in empirical results and limited comparison to state-of-the-art methods. Reviewer 4 raises concerns about the baseline choices, model design, and comparison to other PPI prediction approaches, such as AlphaFold-Multimer.\n\nThe reviewers' comments suggest that the paper has several weaknesses, including:\n\n1. Limited novelty and contribution: The paper builds upon existing methods without introducing significant new ideas or modifications.\n2. Poor baseline choices: The comparison to weak baselines makes it difficult to evaluate the performance of the proposed model.\n3. Unclear model design: The advantage of using both structural embeddings and GNN aggregation is not well-supported by the results.\n4. Limited comparison to state-of-the-art methods: The paper does not compare its approach to other recent and relevant PPI prediction methods.\n\nGiven these concerns and limitations, it is challenging to justify accepting the paper for publication at a top-tier conference like ICLR. The paper's contributions and novelty are not significant enough to warrant acceptance, and the limitations and weaknesses raised by the reviewers need to be addressed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper presents a novel approach to protein sequence and structure co-design, leveraging a trigonometry-aware encoder and a roto-translation equivariant decoder. The method demonstrates significant improvements over previous state-of-the-art baselines in terms of performance metrics such as PPL, RMSD, and AAR. The authors have also conducted extensive experiments to evaluate their model, including ablation studies to investigate the effectiveness of different components.\n\nOne of the primary strengths of the paper is its ability to address the long-standing challenge of protein design, which has significant implications for bioengineering and related fields. The proposed method, ProtSeed, shows promise in generating high-fidelity protein sequences and structures, outperforming existing methods.\n\nHowever, several reviewers have raised concerns regarding the paper's clarity, novelty, and reproducibility. Some of the concerns include:\n\n1. **Lack of specific hyperparameter and training details**: Reviewers have pointed out that the paper lacks detailed information about hyperparameters and training settings, which could affect the reproducibility of the results.\n2. **Discrepancy with prior work**: The paper reports different performance metrics for the diffusion baseline compared to the original paper, which could be due to differences in dataset processing or implementation.\n3. **Novelty and significance**: Some reviewers have questioned the novelty and significance of the contributions, suggesting that the method builds upon existing approaches and may not be substantially new.\n4. **Evaluation metrics**: Reviewers have suggested that the current evaluation metrics may not fully capture the quality of the designed proteins and that additional functional-aware metrics could be more informative.\n\nIn response to these concerns, the authors have provided detailed explanations, revisions, and additional experiments to address the reviewers' comments. They have:\n\n1. **Expanded discussions on hyperparameters and training details**: The authors have provided more information about the training settings and hyperparameters in the appendix.\n2. **Addressed discrepancies with prior work**: The authors have explained the differences in dataset processing and implementation that may have contributed to the discrepancy in performance metrics.\n3. **Clarified the novelty and significance**: The authors have emphasized the unique aspects of their approach, including the use of a trigonometry-aware encoder and a roto-translation equivariant decoder, and the ability to cross-condition on sequence and structure.\n4. **Added ablation studies and discussions on evaluation metrics**: The authors have conducted additional experiments to investigate the effectiveness of different components and discussed the limitations of current evaluation metrics.\n\nConsidering the authors' responses and revisions, the paper has addressed many of the concerns raised by the reviewers. The method presents a significant improvement over existing approaches, and the authors have provided detailed explanations and additional experiments to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Data Leakage in Tabular Federated Learning\" presents a novel approach to reconstructing tabular data from gradients in federated learning, addressing the unique challenges posed by mixed discrete-continuous data. The authors propose a method called TabLeak, which utilizes a softmax structural prior, pooled ensembling, and an entropy measure to assess reconstruction quality.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewer 1 criticizes the paper for being poorly written, with obscure sentences and grammatical errors, but acknowledges the interesting topic. Reviewer 2 finds the paper well-written but argues that the technical contributions are weak, as the ideas are not innovative, and the benchmark models are fairly weak. Reviewer 3 provides a detailed and balanced review, praising the paper's clarity, quality, and empirical novelty, while also raising concerns about the claims regarding the specificity of the problems tackled for tabular data.\n\nThe authors' response addresses the reviewers' comments, providing additional experiments and clarifications to support their claims. They demonstrate the effectiveness of TabLeak, showing significant improvements over baseline methods, and provide evidence for the increased variance in the reconstruction problem due to the mix of discrete and continuous features.\n\nConsidering the reviewers' comments and the authors' response, the paper's strengths outweigh its weaknesses. The paper tackles an important and interesting problem, and the proposed method, TabLeak, shows promising results. While the technical contributions may not be entirely novel, the application of existing techniques to the tabular data setting is a significant contribution. The authors' response has addressed many of the reviewers' concerns, providing additional evidence and clarifications to support their claims.\n\nHowever, the paper's writing quality and presentation are areas for improvement. The authors should revise the paper to address the grammatical errors, obscure sentences, and inconsistencies in notation, as pointed out by Reviewer 1.\n\nIn conclusion, based on the provided information, the paper has made significant contributions to the field of federated learning, and the authors have demonstrated the effectiveness of their proposed method. While there are areas for improvement, the paper's strengths outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper under review explores the problem of online black-box adaptation to label-shift in the presence of conditional-shift. The authors propose several heuristics to improve the performance of existing label-shift adaptation methods in this setting. The paper is well-motivated, and the problem it addresses is relevant to real-world applications.\n\nHowever, the reviewers have raised several concerns about the paper. One major issue is the lack of a systematic approach to understanding the types of distribution shifts considered. The paper focuses on general distribution shifts, but the experiments do not provide clear insights into when the proposed methods can be expected to improve upon baselines. The reviewers have suggested that a more comprehensive empirical study is necessary to ascertain the usefulness of the proposed methods.\n\nAnother concern is the lack of theoretical justification for the proposed heuristics. The authors acknowledge this limitation and view their work as an empirical study. However, the reviewers have emphasized the importance of providing a more principled approach to the problem.\n\nThe paper has also been criticized for its writing quality, with some sections being unclear or difficult to follow. The authors have responded to these concerns by revising their draft and providing additional explanations and clarifications.\n\nThe reviewers have also pointed out that the proposed heuristics are not significantly novel or distinguishable from existing methods. The authors have acknowledged this limitation and emphasized the importance of their empirical study in providing insights into the practical implications of the strong assumptions typically made in experimentation.\n\nAfter carefully considering the reviewers' comments and the authors' responses, I conclude that the paper has some merits, but its limitations and weaknesses outweigh its strengths. The lack of a systematic approach to understanding the types of distribution shifts, the lack of theoretical justification for the proposed heuristics, and the limited novelty of the proposed methods are significant concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi Task Learning of Different Class Label Representations for Stronger Models\" proposes a novel approach to improve image classification performance by using binary labels as an auxiliary task in a multi-task learning setting. The authors demonstrate that this approach can lead to higher accuracy across various datasets and architectures, especially when training data is limited.\n\nHowever, the reviewers have raised several concerns that need to be addressed. Reviewer 1 points out that the paper lacks clarity, with unclear explanations of key concepts such as \"dense labels\" and \"Metabalance.\" The reviewer also notes that the experimental section is weak, with insufficient comparisons to baselines and limited analysis of the results. Reviewer 2 criticizes the paper for poor notation, calculation errors, and a lack of insight into the algorithm. Reviewer 3 suggests that the contribution may not be significant enough due to the small empirical improvements and the lack of further analysis. Reviewer 4 notes that the technical contribution is limited, and the paper does not clearly explain how the binary labels improve the learning.\n\nDespite these concerns, the reviewers also acknowledge the novelty and potential of the approach. Reviewer 1 notes that the idea is interesting and agrees that label representations are an understudied aspect of machine learning. Reviewer 3 finds the paper to be clearly written and easy to understand, and suggests that the authors expand on the results by exploring some of the questions listed in the conclusions section.\n\nTo determine whether the paper should be accepted or rejected, we need to weigh the strengths and weaknesses of the paper. The paper's novelty and potential are significant strengths, but the concerns raised by the reviewers need to be addressed. The lack of clarity, insufficient experimental analysis, and limited technical contribution are significant weaknesses.\n\nGiven the current state of the paper, it seems that the weaknesses outweigh the strengths. The authors have not provided a response to address the concerns raised by the reviewers, which suggests that they may not be able to adequately address the issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether this paper should be accepted or rejected for publication, let's analyze the key points from the reviews and the author's response.\n\n1. **Technical Novelty and Significance**: Reviewers 1, 2, 3, and 4 acknowledge that the paper presents significant and somewhat new contributions, although aspects of these contributions exist in prior work. This indicates that the paper does offer some level of novelty and significance to the field.\n\n2. **Clarity, Quality, and Reproducibility**: Several reviewers (1, 2, and 5) criticize the paper for being poorly written, with issues in clarity, quality, and reproducibility. Reviewer 5 states that the paper is inaccessible to an ordinary reader due to its suboptimal writing. However, Reviewer 4 suggests that the paper is generally pleasant to read, although they also mention areas for improvement in clarity and reproducibility.\n\n3. **Correctness**: The reviews are mixed regarding the correctness of the paper's claims. Reviewer 1 gives a score of 3, indicating some minor issues, while Reviewer 2 scores 2, suggesting several claims are incorrect or not well-supported. Reviewer 3 scores 4, indicating all claims are well-supported, but Reviewer 5 scores 1, stating the main claims are incorrect or not supported.\n\n4. **Empirical and Technical Contributions**: The paper presents numerical experiments that, according to the authors, confirm their theoretical predictions. However, some reviewers question the practicality of certain assumptions (e.g., knowing the signal matrix) and the comparison between theoretical and empirical results.\n\n5. **Response to Reviews**: The authors acknowledge the criticisms and suggest they will revise the paper to address issues of clarity, organization, and reproducibility. They also clarify some of the technical points raised by the reviewers.\n\nConsidering these points, the decision to accept or reject the paper hinges on the balance between its technical contributions and the issues with clarity, quality, and reproducibility. While the paper appears to offer significant and somewhat new contributions, the criticisms regarding its accessibility and the support for some of its claims are substantial.\n\nGiven the mixed reviews and the fact that several reviewers have highlighted significant issues with the paper's presentation and some of its technical aspects, but also acknowledging the potential novelty and significance of its contributions, the most appropriate decision would be to reject the paper in its current form. However, the authors should be encouraged to revise their work, addressing the clarity, quality, and reproducibility concerns, and potentially resubmitting it for further consideration.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" proposes a deep learning system to create anonymized representations for data, aiming to maintain accuracy for the targeted MLaaS task while preventing privacy issues. The reviewers have provided detailed comments on the paper's strengths, weaknesses, clarity, quality, novelty, and reproducibility.\n\nReviewer 1 points out that the idea of splitting a neural network into two parts is not new, and the paper's experimental analysis is limited, considering only two datasets and a single baseline. The reviewer also mentions grammatical errors and questions the paper's novelty, stating that it is an incremental effort over state-of-the-art methods.\n\nReviewer 2 raises severe concerns about the proposed approach, stating that it is flawed due to the requirement of fine-tuning the encoder component, which can only be done during the training phase. The reviewer also points out that the server can easily recreate a \"surrogate\" encoder, making the approach vulnerable to privacy leakage. Additionally, the reviewer criticizes the paper's threat model, stating that it is completely messed up, and the server can turn off the center loss to zero and backpropagate only the cross-entropy loss, making the client unable to verify it.\n\nReviewer 3 also raises concerns about the paper's security model, stating that it is not defined, and using middle features of neural networks is still risky for information leakage. The reviewer also questions the paper's ability to guarantee privacy and points out that the proposed model may not work for NLP applications.\n\nAll reviewers agree that the paper's technical novelty and significance are only marginally significant or novel. They also agree that the paper's empirical novelty and significance are limited, and the contributions are not well-supported.\n\nConsidering the reviewers' comments, the paper's limitations, and the lack of an author response, it is clear that the paper has significant flaws and weaknesses. The proposed approach is vulnerable to privacy leakage, and the paper's experimental analysis is limited. The paper's novelty and significance are also questioned by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics\" presents a study on the average-case performance of no-regret learning dynamics, specifically q-replicator dynamics, in potential games. The authors aim to quantify the outcome of optimal learning dynamics via average performance metrics, which couple the regions of attraction with the quality of each attracting point.\n\nThe reviewers have provided detailed comments on the paper, highlighting both strengths and weaknesses. The main concerns raised by the reviewers include:\n\n1. **Lack of comparison with prior work**: Reviewer 2 points out that the paper does not give enough credit to the prior work PP16, which studied the average-case performance of replicator dynamics in potential games. The authors have responded to this comment by adding a paragraph explaining the similarities and differences with PP16.\n2. **Limited scope**: Reviewers 2 and 3 note that the paper's results are limited to 2x2 symmetric coordination games, which is a restricted case. The authors acknowledge this limitation but argue that their results are a necessary step towards proving positive average price of anarchy (APoA) bounds in larger instances.\n3. **Lack of formal proofs**: Reviewer 3 comments that the paper's results are partial and feel preliminary, with no formal proof for the general case. The authors respond that their empirical analysis provides evidence for their claims, but they acknowledge the need for more formal proofs.\n4. **Technical novelty and significance**: Reviewers 2 and 3 rate the paper's technical novelty and significance as marginal, while Reviewer 4 rates it as significant.\n\nDespite these concerns, the reviewers also praise the paper's strengths, including:\n\n1. **Well-written and easy to read**: Reviewers 2 and 3 comment that the paper is well-written and easy to read.\n2. **Interesting problem**: Reviewers 2 and 3 note that the problem studied in the paper is interesting and challenging.\n3. **Solid results**: Reviewer 2 comments that the results are solid, and Reviewer 4 rates the paper's correctness as 4 (out of 5).\n\nAfter carefully considering the reviewers' comments and the authors' responses, I conclude that the paper has made a significant contribution to the study of average-case performance of no-regret learning dynamics. While the paper's scope is limited, and more formal proofs are needed, the authors have provided a clear and well-written presentation of their results.\n\nThe authors have also addressed the reviewers' concerns, including the lack of comparison with prior work and the limited scope of the results. The added paragraph explaining the similarities and differences with PP16 provides a clearer understanding of the paper's contribution.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "To determine whether the paper should be accepted or rejected for publication at a top-tier conference, we need to carefully evaluate the paper's strengths and weaknesses based on the provided information, including the abstract, reviewer comments, and the absence of an author response.\n\n1. **Novelty and Significance**: The paper proposes a novel aggregation strategy, FedPA, which uses a parameterized aggregator to debias aggregation in cross-device federated learning. This addresses both client drift and period drift, issues that are significant in federated learning due to the heterogeneity of decentralized data. Reviewers acknowledge the importance of tackling these drifts but question the novelty and significance of the proposed method compared to existing works.\n\n2. **Technical Quality and Correctness**: Reviewers have pointed out several technical issues, including the lack of theoretical justification for the effectiveness of FedPA, the similarity of the proposed method to other knowledge distillation-based techniques, and the need for more ablation studies to understand the impact of different components of the algorithm. Additionally, there are concerns about the experimental settings being too simple and not comprehensive enough.\n\n3. **Empirical Evidence**: The paper presents experimental results showing that FedPA can achieve competitive performances compared to conventional baselines. However, reviewers suggest that the experimental settings are not strong, with too simple datasets and models, and there's a need for results on more complex datasets like CIFAR-100 and Stack Overflow.\n\n4. **Clarity, Quality, and Reproducibility**: Reviewers have noted issues with the paper's clarity and quality, including grammatical mistakes, small font sizes in figures, and a lack of code availability, which hampers reproducibility. There are also suggestions for improving the writing and providing more details on hyperparameter tuning.\n\n5. **Comparison with Existing Works**: The reviewers emphasize the need for a more thorough comparison with existing methods, especially those that also address client and period drift in federated learning. This includes justifying why a meta-learning approach is preferred over regularization-based approaches.\n\nGiven these considerations, the paper has some promising aspects, such as addressing an important problem in federated learning and showing competitive performance in some experiments. However, it also has significant weaknesses, including technical issues, a lack of comprehensive experimental validation, and concerns about novelty and significance compared to existing works.\n\nThe absence of an author response means that the authors have not addressed the reviewers' concerns, which is a critical aspect of the review process, especially for a top-tier conference where the standards for acceptance are high.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" presents a novel approach to neural-symbolic AI, introducing DeepSeaProbLog, a language that supports discrete and continuous random variables. The authors provide a solid theoretical foundation, proving the correctness of their approach and its differentiability. They also demonstrate the effectiveness of DeepSeaProbLog through experiments on various tasks.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. Reviewers 1, 3, and 4 appreciate the novelty and significance of the contributions, while Reviewers 2, 5, and 6 raise concerns about the incrementality of the work, the lack of extensive empirical evaluations, and the need for more clarity on the handling of discrete random variables.\n\nThe authors' response addresses these concerns, providing additional explanations, references, and experimental results. They clarify the novelty of their approach, the handling of discrete random variables, and the scalability of their implementation. They also acknowledge the limitations of their work and propose future research directions.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' response, I conclude that the paper makes significant contributions to the field of neural-symbolic AI. The introduction of DeepSeaProbLog, its theoretical foundation, and the experimental evaluations demonstrate the potential of this approach. While the paper could benefit from more extensive empirical evaluations and clearer explanations of certain aspects, the authors' response addresses these concerns to a large extent.\n\nThe paper's strengths, including its novelty, theoretical significance, and experimental evaluations, outweigh its weaknesses. The authors' willingness to address the reviewers' concerns and provide additional explanations and results demonstrates their commitment to improving the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper presents a new weather radar dataset, RuDar, which includes 3D radar echo observations and covers multiple climate zones in the European part of Russia. The dataset is comprehensive, with over 50,000 timestamps and 100 GB of data, and is publicly available along with the code for data processing and model preparation. The authors evaluate several baseline models, including optical flow and neural network models, on the proposed dataset and also investigate uncertainty quantification.\n\nThe reviewers' comments highlight both the strengths and weaknesses of the paper. The main strengths are:\n\n1. The dataset is attractive and comprehensive, covering multiple climate zones and including 3D radar echo observations.\n2. The paper is well-written and easy to understand.\n3. The authors provide a thorough evaluation of baseline models on the proposed dataset.\n\nHowever, the reviewers also point out some weaknesses:\n\n1. The methods used are relatively low-cost, and the authors do not evaluate more heavy-cost models, such as transformers.\n2. The uncertainty estimation is rather simple.\n3. The paper lacks methodological novelties.\n4. The evaluation metric only uses MSE, which may not reflect the accuracy of nowcasting.\n\nThe authors respond to the reviewers' comments by:\n\n1. Adding experimental results of the Earthformer model, a recent and heavy-cost model, on the proposed dataset.\n2. Acknowledging the simplicity of the uncertainty estimation and suggesting that a deeper analysis is an interesting topic for future studies.\n3. Clarifying that the paper is focused on describing the RuDar dataset and evaluating baseline models on it, rather than introducing methodological novelties.\n4. Providing additional evaluation metrics, including F1 measure, to reflect the accuracy of nowcasting.\n\nAfter carefully analyzing the paper, the reviewers' comments, and the authors' responses, I conclude that the paper has both strengths and weaknesses. However, the strengths outweigh the weaknesses, and the authors have addressed the reviewers' comments satisfactorily.\n\nThe RuDar dataset is a significant contribution to the field of precipitation nowcasting, and the paper provides a thorough evaluation of baseline models on the proposed dataset. While the methods used may not be novel, the paper is well-written, and the authors have provided additional experimental results and evaluation metrics to address the reviewers' comments.\n\nTherefore, based on the analysis, I conclude that the paper should be accepted for publication.\n\nFinal Decision: Accept"
    }
  ]
}