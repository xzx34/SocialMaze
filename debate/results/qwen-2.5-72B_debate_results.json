{
  "summary": {
    "model": "qwen-2.5-72B",
    "total_papers": 100,
    "correct_predictions": 66,
    "accuracy": 66.0,
    "confusion_matrix": [
      [
        43,
        33
      ],
      [
        1,
        23
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 56.57894736842105,
        "precision": 97.72727272727273
      },
      "Accept": {
        "total": 24,
        "recall": 95.83333333333334,
        "precision": 41.07142857142857
      }
    }
  },
  "results": [
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper introduces NORML, a meta-learning framework that uses an LSTM to perform neuron-wise optimization on a learner network. The key claim is that this approach scales better with the number of learner parameters compared to previous methods. However, the novelty and contribution of the paper are questionable for several reasons:\n\n1. **Comparison to Ravi & Larochelle (2017)**:\n   - The paper claims that the LSTM meta-learner in Ravi & Larochelle (2017) would require an enormous number of parameters, which is not accurate. Ravi & Larochelle use parameter sharing across coordinates, meaning the number of parameters in the LSTM meta-learner is constant relative to the size of the learner network.\n   - The paper should clearly state and explain the actual differences and benefits of NORML compared to Ravi & Larochelle (2017). The current comparison is misleading and needs to be corrected.\n\n2. **Technical Novelty**:\n   - The technical novelty of the paper is minimal. The idea of using an LSTM to control learning updates is not new, and the specific implementation in NORML does not seem to introduce significant new insights or techniques.\n\n#### Experimental Evaluation\nThe experimental results are not convincing and have several issues:\n\n1. **Comparison to Baselines**:\n   - The paper does not provide a direct comparison to Ravi & Larochelle (2017) with the same experimental settings, which is crucial for validating the claimed benefits.\n   - The comparison to MAML is also limited. The paper should include a version of MAML where a separate learning rate is learned per hidden unit and per inner loop step, as well as a comparison to MAML++ (Antoniou et al.).\n   - The results on Mini-ImageNet are not compared to the results from Sun et al., which achieved better or similar performance. This raises questions about the effectiveness of NORML.\n\n2. **Applicability**:\n   - NORML is designed for fully connected networks, which limits its applicability. Most few-shot learning methods use convolutional networks, and the paper does not demonstrate how NORML can be applied to convolutional networks.\n\n#### Writing and Presentation\nThe writing and presentation of the paper have several issues:\n\n1. **Clarity and Grammar**:\n   - The paper contains numerous grammatical errors and unclear statements. The writing needs significant improvement to meet the standards of a top-tier conference.\n   - The flow of ideas and the organization of the paper are not optimal, making it difficult to follow the arguments and contributions.\n\n2. **Inaccurate Descriptions**:\n   - The related work section contains inaccurate descriptions of prior work, which can mislead readers and reviewers. These inaccuracies need to be corrected.\n\n### Final Decision\nGiven the limited novelty, issues with experimental evaluation, and problems with writing and presentation, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel training strategy, MuPPET, which employs a multi-precision approach to reduce training time and energy consumption while maintaining accuracy. The key innovation lies in the precision-switching mechanism, which dynamically decides when to increase the precision of computations based on gradient diversity. This approach is a significant step forward from existing mixed-precision methods, which typically use fixed precision levels.\n\n#### Technical Soundness\nThe paper is technically sound in its approach and methodology. The authors provide a clear explanation of the precision-switching mechanism and the rationale behind it. They address the heuristic nature of the switching mechanism by providing a detailed explanation and empirical justification. The algorithm is well-defined, and the authors have addressed the reviewer's concerns about the clarity of the algorithm and the precision levels.\n\n#### Experimental Evaluation\nThe experimental results are comprehensive and demonstrate the effectiveness of MuPPET. The authors show that MuPPET can achieve the same accuracy as full-precision training with an average training-time speedup of 1.28Ã— across multiple networks and datasets. The addition of the accuracy-time trade-off plot (Fig. 3) in the revised manuscript further strengthens the case for MuPPET by showing its superiority over random and pre-defined switching points.\n\n#### Clarity and Presentation\nThe paper is well-written and the authors have made significant improvements in response to the reviewers' comments. The introduction is now more concise and clearly states the novel contributions. The figures and tables are more readable, and the authors have provided additional details in the appendix. The authors have also addressed the minor issues pointed out by the reviewers, such as typos and unclear notations.\n\n#### Addressing Reviewer Concerns\nThe authors have effectively addressed the major and minor points raised by the reviewers. They have provided a more detailed explanation of the precision-switching mechanism, the choice of precision levels, and the impact of the hyperparameters. The addition of the training and validation curves in the appendix and the updated discussion in Section 4.4 further strengthen the paper.\n\n#### Comparative Analysis\nWhile the results are not superior to state-of-the-art methods in all cases, the authors have demonstrated that MuPPET can achieve comparable performance with significant runtime benefits. The authors have also discussed the limitations and potential areas for improvement, which adds to the robustness of the paper.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper presents a novel and technically sound approach to reducing training time and energy consumption in deep neural networks. The authors have effectively addressed the reviewers' concerns and provided a comprehensive evaluation of their method. The contributions are significant and the paper is well-written, making it suitable for publication in a top-tier conference."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Quality of Writing and Clarity\nThe paper is severely lacking in clarity and coherence. Both Reviewer 2 and Reviewer 3 highlight significant grammatical issues and incomprehensible passages. The abstract, which is crucial for setting the stage for the research, is poorly written and difficult to understand. This makes it challenging for readers to grasp the main contributions and objectives of the paper.\n\n#### Lack of References\nThe absence of references is a major red flag. Reviewer 1 and Reviewer 3 both point out that the paper does not cite any previous research. This is a critical oversight, as it fails to situate the work within the existing literature and does not provide a foundation for the claims made in the paper. The author's response indicates that they removed references due to a technical issue with the .bib file, but this does not excuse the lack of proper citation.\n\n#### Technical Content and Novelty\nThe paper's technical content is not well-defined. Reviewer 1 and Reviewer 3 both express confusion about the proposed method and its novelty. The author's response suggests that they are aware of similar ideas in their country but have not found international papers on the topic. However, the lack of clear explanation and justification for the proposed method makes it difficult to assess its significance and contribution to the field.\n\n#### Experimental Results\nThe experiments described in the paper are limited to a small problem (tic-tac-toe), which is not a strong test case for demonstrating the robustness and performance of the proposed method. The results are not well-documented, and there is no comparison with existing methods like AlphaZero, which is crucial for evaluating the effectiveness of the proposed approach.\n\n#### Overall Evaluation\nThe paper fails to meet the standards of a top-tier conference in several key areas:\n- **Clarity and Writing Quality**: The paper is poorly written and difficult to understand.\n- **References and Literature Review**: The lack of references and a proper literature review undermines the credibility of the work.\n- **Technical Content and Novelty**: The proposed method is not clearly defined, and its novelty and significance are not well-justified.\n- **Experimental Results**: The experiments are limited and not well-documented, making it difficult to assess the performance of the proposed method.\n\n### Final Decision\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper proposes a generative model, ROOTS, for unsupervised object-wise 3D scene decomposition and rendering. The key contributions are:\n1. **Object-Oriented Representation**: ROOTS provides a 3D object-oriented representation, which is a significant step forward from the scene-level representation of GQN.\n2. **Hierarchical Representation**: The model offers a hierarchical representation at both the 3D global-scene level and the 2D local-image level.\n3. **Viewpoint Invariance**: The inferred object representations are viewpoint invariant, similar to GQN's full scene representation.\n\nThese contributions are novel and address important challenges in 3D scene understanding and representation learning. The ability to disentangle objects in a 3D scene and represent them in a modular, 3D-viewpoint-invariant manner is a significant advancement in the field.\n\n#### Clarity and Writing\nThe paper has been criticized for poor writing and notation inconsistencies. The authors have acknowledged these issues and have made substantial revisions to improve clarity, notation consistency, and readability. The revised version is reported to be much more readable and clear, which is a positive step.\n\n#### Experiments and Results\nThe experiments demonstrate the model's ability to disentangle objects, handle occlusion, and generate high-quality images. The qualitative results are impressive, and the authors have added additional experiments to address the reviewers' concerns. However, the quantitative results, particularly the NLL and MSE, are not significantly better than GQN. The authors argue that the primary goal is not to improve generation quality but to achieve object-oriented 3D representation while maintaining comparable performance.\n\n#### Related Work\nThe paper initially lacked a comprehensive discussion of related work in 3D computer vision. The authors have addressed this by adding a significant amount of related work and clarifying the position of their work in the context of existing literature. This is an important improvement that helps readers understand the unique contributions of the paper.\n\n#### Ablation Studies\nThe reviewers have raised concerns about the lack of ablation studies. The authors argue that the purpose of the paper is not to improve a performance metric but to achieve object-oriented 3D representation. They claim that the hierarchical structure is necessary to enable this representation, and the benefits are clear from the qualitative results. While ablation studies would provide more detailed insights, the authors' argument is reasonable given the nature of their contribution.\n\n#### Minor Issues\nThe paper has several minor issues, such as typos and missing explanations, which have been addressed in the revision. These issues, while important for the final quality of the paper, do not significantly impact the overall contribution.\n\n### Final Decision\nGiven the significant contributions in terms of object-oriented 3D representation, the improvements in clarity and writing, and the additional experiments and related work, the paper should be accepted for publication. The authors have addressed the major concerns raised by the reviewers, and the paper now meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel approach to learning disentangled hierarchical representations using a progressive learning strategy. The key contributions include:\n1. **Progressive Learning Strategy**: The model starts with the most abstract representations and progressively grows the network to introduce new representations at different levels of abstraction.\n2. **Hierarchical Representations**: The model uses a ladder architecture to capture hierarchical representations, which is a significant improvement over traditional VAEs.\n3. **New Disentanglement Metric**: The authors propose a new metric, MIG-sup, to complement existing metrics and better capture the one-to-one mapping of generative factors to latent dimensions.\n\n#### Strengths\n1. **Fresh and Well-Explained Idea**: The concept of progressive learning for disentanglement is novel and well-motivated. The paper is clear in explaining the motivation and the technical details.\n2. **Thorough Experiments**: The authors provide both qualitative and quantitative results on benchmark datasets, demonstrating the effectiveness of their approach.\n3. **Ablation Studies**: The authors conducted ablation studies to investigate the impact of different implementation strategies, which adds robustness to their claims.\n4. **Comparisons with Baselines**: The paper compares the proposed method with existing models (e.g., VLAE, vanilla VAE) and shows improvements in disentanglement metrics.\n5. **Additional Experiments**: The authors addressed the reviewers' concerns by conducting additional experiments and providing detailed results in the appendix.\n\n#### Weaknesses\n1. **Clarity of Purpose**: The paper could be clearer in distinguishing between the goals of learning hierarchical representations and disentangled representations. The relationship between these two concepts should be better explained.\n2. **Robustness to Hyperparameters**: The model's performance is sensitive to the hyperparameter beta, which is a limitation. The authors should discuss this more thoroughly and provide strategies to mitigate this issue.\n3. **Comparison with Related Work**: The paper should more thoroughly discuss the relationship with related work, particularly [3] (Burgess et al., 2017), to highlight the unique contributions of the proposed method.\n4. **Quantitative Information Flow**: While the authors provided additional experiments on information flow, a more detailed quantitative analysis would strengthen the paper.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Raised valid concerns about the training of the first layer and the need for additional experiments to compare mutual information with VLAE. The authors addressed these concerns with additional experiments.\n- **Reviewer 2**: Criticized the clarity of the paper's purpose and the relationship between hierarchical and disentangled representations. The authors clarified these points and provided additional experiments.\n- **Reviewer 3**: Praised the paper's novelty and thorough experiments but suggested additional quantitative experiments on information flow. The authors addressed these suggestions.\n- **Reviewer 4**: Provided positive feedback and suggested ablation studies. The authors conducted these studies and included the results.\n\n### Final Decision\nGiven the novel contributions, thorough experiments, and the authors' efforts to address the reviewers' concerns, the paper should be accepted for publication. However, the authors should consider the remaining suggestions to further improve the clarity and robustness of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel and challenging problem: generating singing voices without pre-assigned scores and lyrics. This is a significant contribution to the field of generative models for music, as it explores a less-studied and more complex scenario. The authors propose three different schemes (free singer, accompanied singer, and solo singer) and provide a detailed pipeline for tackling these tasks. The novelty of the problem and the innovative approach are strong points in favor of the paper.\n\n#### Methodology\nThe authors have done extensive work in creating their own dataset, designing their tasks, and implementing their models. They use a GAN-based approach and provide a detailed description of their architecture and training process. The use of source separation and transcription models for data preparation is well-thought-out and necessary for the task. The inclusion of two baseline methods (Sinsy and Synthesizer V) in the revised version significantly strengthens the evaluation.\n\n#### Evaluation\nThe initial evaluation was criticized for being weak and lacking context. The authors have addressed these concerns by:\n1. Adding two baseline methods (Sinsy and Synthesizer V) to the MOS study.\n2. Expanding Table 1 to include the objective metrics of the training data and the Sinsy baseline.\n3. Providing a more detailed discussion of the metrics and their significance.\n4. Including a new user study that compares their model with the baseline methods.\n\nThese additions provide a more comprehensive and context-rich evaluation, making it easier to assess the performance of the proposed models.\n\n#### Motivation and Clarity\nThe revised version includes a clearer motivation and discussion of the potential applications of the proposed methods. The authors have also addressed the confusion regarding the training and test data, providing a detailed description of the datasets used and their characteristics. The paper is now more structured and easier to follow.\n\n#### Addressing Reviewer Concerns\nThe authors have responded to the reviewers' comments in a thorough and constructive manner. They have made significant revisions to the paper, addressing most of the concerns raised. The paper is now more robust and well-supported.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper addresses a novel and challenging problem, provides a well-designed and implemented solution, and has significantly improved its evaluation and clarity in response to reviewer feedback. The contributions are substantial and the paper is now suitable for publication in a top-tier conference."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\nThe paper proposes a method for training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. The main contributions are:\n1. **Max-Component Approximation**: Minimizing an upper bound to the GMM log likelihood for numerical stability.\n2. **Regularizer**: A novel regularizer to prevent convergence to pathological local minima.\n3. **Covariance Structure**: A method for enforcing constraints and a simplified covariance structure to reduce memory usage.\n\nWhile the paper introduces these ideas, the novelty is somewhat limited. The max-component approximation is a known technique in numerical stability, and the regularizer is more akin to annealing, which is a well-established concept. The covariance structure simplification is a practical but not groundbreaking contribution.\n\n#### Technical Soundness\nThe technical soundness of the paper is questionable based on the reviewer comments:\n- **Numerical Stability**: The max-component approximation is not clearly justified over the log-sum-exp trick, which is a standard method for numerical stability.\n- **Regularizer**: The motivation for the gridding approach is not well-explained, and the term \"regularizer\" might be misleading.\n- **Covariance Constraints**: The requirement \\(\\Sigma_{i,j} \\geq 0\\) is incorrect and should be clarified to ensure positive definiteness.\n- **Experimental Evaluation**: The datasets used (MNIST, SVHN) are not extremely high-dimensional, and the experimental setup lacks details and comparisons to other methods.\n\n#### Clarity and Presentation\nThe paper is generally well-written, but there are several issues:\n- **Clarity**: Some sections are unclear, such as the motivation for the max-component approximation and the details of the covariance structure.\n- **Repetition**: The experimental details are repeated, which is a minor but noticeable issue.\n- **Figures and Results**: The selection criteria for images in Figures 2 and 4 are not clear, and the comparison to EM in Figure 3 lacks necessary details.\n\n#### Relevance and Impact\nThe topic of training GMMs in high-dimensional spaces is relevant, but the paper does not convincingly demonstrate the practical advantages of the proposed method over existing techniques like EM with K-means initialization. The authors' response indicates that the method is intended for online, incremental settings, but this is not clearly stated in the paper.\n\n### Final Decision\nGiven the limited novelty, technical issues, and lack of convincing experimental results, the paper does not meet the standards of a top-tier conference. The authors have provided some clarifications, but significant improvements are needed to address the reviewers' concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Theoretical Contributions:**\n   - The paper provides a rigorous theoretical foundation for the intuition that \"hard\" negative samples are \"better\" by formalizing and proving this concept. This is a significant contribution to the understanding of negative sampling.\n   - Theorem 2, which formalizes the signal-to-noise ratio and proves its optimality for hard negative samples, is a novel and valuable contribution.\n\n2. **Practical Impact:**\n   - The proposed method significantly reduces training time by an order of magnitude compared to several competitive baselines, which is a substantial practical improvement.\n   - The adversarial sampling mechanism is simple and efficient, with a computational cost that scales logarithmically with the number of classes, making it scalable for large datasets.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and accessible, even to non-experts. The derivations and explanations are clear and didactic, which is a strength for a top-tier conference.\n\n#### Negative Aspects:\n1. **Lack of Comprehensive Baselines:**\n   - The paper does not compare against recent state-of-the-art methods like Slice [1] and DiSMEC [2], which are important for a complete evaluation. This omission weakens the experimental section.\n   - The inclusion of these methods would provide a more comprehensive comparison and a better understanding of the proposed method's performance relative to the current state of the art.\n\n2. **Experimental Evaluation:**\n   - The experimental evaluation is limited to two datasets and lacks comparisons on a wider range of datasets, including smaller ones like EURLex, where full softmax can be computed.\n   - The performance on tail-labels and metrics other than accuracy (e.g., MacroF1) are not evaluated, which is crucial for understanding the method's robustness and effectiveness in real-world scenarios.\n\n3. **Multi-Label Setting:**\n   - The paper does not explore the extension of the proposed method to the multi-label setting, which is a common scenario in extreme classification. This limits the applicability and relevance of the method.\n\n4. **Related Work:**\n   - The paper does not sufficiently acknowledge and discuss relevant prior work, particularly in the context of non-uniform sampling and sampled softmax. This is a significant oversight that needs to be addressed.\n\n#### Author Response:\n- The authors have acknowledged the need to include more comprehensive baselines and have committed to adding these comparisons in the final version of the paper.\n- They have also addressed the theoretical contributions and the practical implications of their method, which are strong points of the paper.\n- The authors have committed to running additional experiments on smaller datasets and evaluating performance on tail-labels, which will strengthen the experimental section.\n\n### Final Decision\nGiven the significant theoretical contributions, practical improvements, and the authors' commitment to addressing the reviewers' concerns, the paper has the potential to make a valuable contribution to the field. However, the current lack of comprehensive baselines and a more thorough experimental evaluation is a notable weakness.\n\nFinal Decision: Accept\n\nThe authors should ensure that the promised improvements and additional experiments are included in the final version of the paper to address the reviewers' concerns fully."
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Problem Relevance and Novelty**:\n   - The paper addresses a significant problem in the field of adversarial robustness, specifically the gap between certifiable and empirical robustness in neural networks.\n   - The proposed regularizers aim to improve the tightness of convex relaxations, which is a novel and relevant approach.\n\n2. **Empirical Results**:\n   - The authors provide empirical results that demonstrate the effectiveness of their proposed regularizers. They achieve state-of-the-art certified accuracies on both MNIST and CIFAR10, which is a strong indicator of the practical utility of their method.\n   - The inclusion of results on CIFAR10, a more challenging dataset, adds credibility to the claims.\n\n3. **Theoretical Analysis**:\n   - The paper includes a detailed theoretical analysis, including propositions and proofs, which provide a solid foundation for the proposed regularizers.\n   - The illustrative example in the appendix helps clarify some of the complex concepts and addresses some of the reviewers' concerns.\n\n4. **Response to Reviewer Comments**:\n   - The authors have made significant revisions to address the reviewers' comments. They have clarified several points, corrected typos, and added new experimental results.\n   - The responses are thorough and demonstrate a deep understanding of the subject matter.\n\n#### Weaknesses:\n1. **Clarity and Presentation**:\n   - Some sections of the paper, particularly in the theoretical analysis, are still somewhat confusing. The authors have made efforts to improve clarity, but further refinement might be necessary.\n   - The distinction between the original non-convex problem and its convex relaxation is not always clear, which can lead to misunderstandings.\n\n2. **Experimental Scope**:\n   - While the results on MNIST and CIFAR10 are strong, the paper could benefit from additional experiments on other datasets to further validate the robustness of the proposed method.\n   - The comparison with other baselines, such as IBP and CROWN-IBP, is thorough, but more detailed comparisons with state-of-the-art methods would strengthen the paper.\n\n3. **Theoretical Justification**:\n   - The theoretical justification for the regularizers, particularly the second regularizer, could be more robust. The authors have addressed some of the concerns, but a more detailed explanation of why the regularizers work and under what conditions would be beneficial.\n\n### Final Decision\nDespite the remaining issues with clarity and the need for additional experiments, the paper presents a novel and effective approach to improving the robustness of neural networks. The empirical results are strong, and the authors have made significant efforts to address the reviewers' concerns. The contributions are valuable and relevant to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Setting and Novelty:**\n   - The paper addresses a practical and important problem in AI safety: out-of-distribution (OOD) detection without the need for OOD samples or retraining the model. This is a significant constraint that makes the problem more challenging and relevant to real-world applications.\n   - The proposed method, MALCOM, introduces a novel approach using the normalized compression distance (NCD) to capture spatial patterns in feature maps, which is a unique and interesting idea.\n\n2. **Experimental Evaluation:**\n   - The authors provide a thorough evaluation of their method on several benchmarks and different neural network architectures. They compare their method with existing state-of-the-art techniques, including Mahalanobis and ODIN.\n   - The authors have conducted additional experiments to address the reviewers' concerns, including comparisons with methods that use adversarial samples for validation. These experiments show that MALCOM and its variant, MALCOM++, perform well even under these more relaxed constraints.\n\n3. **Clarity and Presentation:**\n   - The paper is well-structured and clearly explains the motivation, methodology, and experimental results. The authors have addressed the reviewers' comments and made significant revisions to improve the clarity and robustness of their claims.\n   - The addition of Figure 1 and the detailed appendices (B and C) enhance the understanding of the proposed method and the experimental results.\n\n#### **Weaknesses of the Paper:**\n1. **Comparison with Baselines:**\n   - The initial comparison with baselines was not entirely fair, as the authors did not use adversarial samples for validation, which is a common practice in the literature. However, the authors have addressed this in their response by conducting additional experiments and showing that their method still performs well.\n   - The performance of the replication of prior methods was lower than reported, which could be due to implementation issues. The authors have acknowledged this and provided both the original and replicated numbers, which is a good practice.\n\n2. **Theoretical Guarantees:**\n   - The paper lacks theoretical guarantees showing the exhaustiveness of the proposed method in detecting all possible OOD examples. This is a common limitation in OOD detection research, but it would be beneficial to have some theoretical insights or analysis.\n\n3. **Practical Relevance:**\n   - While the problem setting is practical, the authors could provide more concrete examples or scenarios where the constraints (no OOD samples, no retraining) are particularly important. This would strengthen the motivation and relevance of the work.\n\n4. **Writing and Typos:**\n   - The paper could benefit from additional proofreading to correct minor grammatical issues and typos. The authors have addressed some of these issues, but a thorough review is still recommended.\n\n### Final Decision\n\nDespite the initial issues with the comparison to baselines and the lack of theoretical guarantees, the authors have made significant improvements in their response. The paper addresses a practical and important problem, introduces a novel method, and provides a thorough evaluation. The additional experiments and revisions have addressed the reviewers' concerns and strengthened the paper.\n\nFinal Decision: **Accept**"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper provides a thorough theoretical analysis of the impact of activation functions on the training of overparametrized neural networks. This is a significant contribution to the field, especially given the increasing interest in understanding the theoretical underpinnings of deep learning.\n2. **Comprehensive Analysis**: The authors cover a wide range of activation functions, including both smooth and non-smooth ones, and provide clear conditions under which these functions affect the training dynamics. This comprehensive approach is valuable for both theoretical and practical insights.\n3. **Empirical Validation**: The paper includes empirical experiments on both synthetic and real data (CIFAR10), which support the theoretical findings. While there is a gap between theory and experiment on CIFAR10, the authors acknowledge this and discuss it as a strength.\n4. **Clarity and Structure**: The paper is well-written and well-referenced. The authors provide a clear roadmap and detailed proofs in the appendix, which is essential for a theoretical paper of this nature.\n5. **Relevance**: The topic is highly relevant to the current research in deep learning, particularly in the context of overparametrized models and the role of activation functions.\n\n#### Weaknesses:\n1. **Length and Organization**: The appendix is quite long, which can make the paper difficult to navigate for a general audience. However, the authors have addressed this by adding a table of contents and improving section names, which should help.\n2. **Complexity**: The paper is highly technical and may be challenging for readers who are not deeply familiar with the mathematical concepts involved. This is a common issue for theoretical papers but is a valid concern for a top-tier conference.\n3. **Empirical Gaps**: The empirical results on CIFAR10 do not fully align with the theoretical predictions, which could be a point of criticism. However, the authors acknowledge this and discuss it as a strength, which is a positive approach.\n\n#### Reviewer Feedback:\n- **Reviewer 1**: Generally positive, with a focus on the theoretical contributions and empirical validation. The main concern is the length and organization of the appendix, which the authors have addressed.\n- **Reviewer 2**: Also positive, highlighting the thorough theoretical analysis and the interesting use of polynomial approximations. The reviewer suggests some improvements in clarity and organization, which the authors have addressed.\n\n### Final Decision\nGiven the significant theoretical contributions, the comprehensive analysis, and the empirical validation, the paper is a strong candidate for acceptance. The authors have addressed the main concerns raised by the reviewers, particularly regarding the organization and clarity of the appendix. The paper is well-written, relevant, and provides valuable insights into the role of activation functions in overparametrized neural networks.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Problem Relevance**: The problem of learning reusable skills from weakly supervised data is highly relevant and important in the field of robotics and reinforcement learning. The ability to segment trajectories into meaningful sub-tasks without detailed supervision can significantly reduce the burden on human annotators and make the learning process more scalable.\n2. **Methodological Contribution**: The paper proposes a novel approach inspired by multiple instance learning (MIL) to segment trajectories into sub-skills. The use of log-sum-exp pooling is a reasonable choice and has shown some success in other domains.\n3. **Empirical Evaluation**: The authors evaluate their method across multiple environments, including both simulation and real-world robots, which demonstrates the versatility of their approach. The inclusion of zero-shot transfer experiments is a strong point, showing the potential for the learned skills to be reused in new tasks.\n\n#### Weaknesses of the Paper\n1. **Limited Scope and Experimental Design**:\n   - **Scope**: The paper is limited in the number of skills and environments it considers. The reviewer points out that the experiments are restrictive, with at most 4-6 primitive skills in each environment. This limits the generalizability of the results.\n   - **Experimental Design**: The experimental setup is not clearly described, and important details such as the number of demonstrations per task/skill, architectural choices, and hyper-parameters are lacking. This makes it difficult to reproduce the results and understand the experimental design.\n2. **Insufficient Baselines and Comparisons**:\n   - **Baselines**: The paper lacks comparisons with fully-supervised methods and other relevant baselines, such as Policy Sketches. This makes it hard to assess the performance of the proposed method relative to existing approaches.\n   - **Unsupervised Clustering**: The reviewers suggest that comparisons with unsupervised clustering methods would be useful. The authors mention preliminary experiments but do not provide detailed results.\n3. **Ambiguous and Poor Classification Results**:\n   - **Classification Accuracy**: The classification results are poor, with validation accuracies ranging from 35% to 60%. The authors explain that this is due to ambiguous timesteps, but this raises questions about the practical utility of the method.\n   - **Confusion Matrices and Precision/Recall**: The lack of confusion matrices, precision, and recall metrics makes it difficult to assess the performance of the classifier in more detail.\n4. **Theoretical and Practical Feasibility**:\n   - **Dataset Requirements**: The reviewers raise concerns about the feasibility of the approach when a large number of skills are present. The paper does not provide theoretical bounds or clear guidelines on the dataset requirements for successful learning.\n   - **Smooth Predictions**: The smooth predictions in the dial jaco videos experiment are not well-explained, and it is unclear whether this is due to filtering or the model's inherent behavior.\n\n#### Author Responses\n- **Addressing Reviewer Concerns**: The authors provide some clarifications and explanations, such as the minor effect of Gaussian smoothing and the challenges of ambiguous timesteps. However, these responses do not fully address the major concerns raised by the reviewers.\n- **Future Work**: The authors mention adding fully-supervised oracle results and feature-based clustering comparisons in the final version. While these additions would strengthen the paper, they are not currently present in the submitted version.\n\n### Final Decision\nGiven the significant weaknesses in the experimental design, insufficient baselines, and poor classification results, the paper does not meet the standards of a top-tier conference. While the problem is important and the method is novel, the current state of the paper does not provide enough evidence to support its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Relevance and Novelty**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning, specifically focusing on Topologically Associating Domains (TADs) in Drosophila melanogaster.\n   - The use of bidirectional LSTM RNNs to capture the sequential nature of DNA is a novel approach in this context.\n\n2. **Methodological Soundness**:\n   - The authors compare their LSTM model with several other machine learning models (linear models, gradient boosting, etc.), demonstrating the superiority of the LSTM model.\n   - The weighted Mean Squared Error (wMSE) is a well-motivated loss function that addresses the specific characteristics of the data.\n\n3. **Clarity and Presentation**:\n   - The paper is generally well-written and provides a clear explanation of the methods and results.\n   - The authors have addressed many of the reviewer comments, improving the clarity and robustness of the paper.\n\n#### Weaknesses of the Paper:\n1. **Generalizability**:\n   - The authors acknowledge that their model is specific to Drosophila melanogaster and may not generalize to other organisms or cell types. This is a valid limitation, but it should be clearly stated in the paper.\n   - The choice of the wMSE loss function is specific to the dataset and may not be generalizable to other Hi-C datasets or other types of epigenetic data (e.g., ATAC-seq).\n\n2. **Comparative Analysis**:\n   - The paper lacks a comparison with other recurrent models (e.g., GRUs) and non-recurrent models (e.g., fully connected or convolutional networks). This is important to understand if the performance gain is due to the LSTM architecture or the loss function.\n   - The authors should provide a more detailed comparison with existing literature, especially papers that have been presented at workshops or in preprints.\n\n3. **Feature Importance**:\n   - The method for feature importance extraction using feature dropout is not as robust as correlating LSTM activations with input features. The authors should consider this for future work.\n\n4. **Evaluation Metrics**:\n   - While the authors have added additional evaluation metrics (MSE, MAE, R^2), it would be beneficial to include more standard metrics used in the field for a comprehensive evaluation.\n\n#### Addressing Reviewer Comments:\n- **Reviewer 1**: The authors have addressed the concern about the wMSE loss function by using it for all models, which is a fair comparison. However, they should still consider comparing with other loss functions to ensure the robustness of their results.\n- **Reviewer 2**: The authors have made several clarifications and corrections, improving the readability and clarity of the paper.\n- **Reviewer 3**: The authors have expanded the literature review and provided more details about the data and methods. However, the lack of comparison with other recurrent and non-recurrent models remains a significant concern.\n\n### Final Decision\nDespite the improvements made by the authors, the paper still has some significant weaknesses that need to be addressed before it can be considered for publication in a top-tier conference. The lack of generalizability, the need for a more comprehensive comparison with other models, and the limited evaluation metrics are critical issues that need to be resolved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Contributions and Novelty\nThe paper claims to provide the first result showing the convergence of stochastic gradient descent-ascent (SGD) to global equilibrium points for non-linear generators in the context of Wasserstein GANs (WGANs). This is a significant theoretical contribution, especially given the complexity of the min-max optimization problem in GANs. The authors also demonstrate that their choice of quadratic discriminators simplifies the dynamics while maintaining sufficient discriminating power, and they achieve a nearly optimal sample complexity.\n\n#### Criticisms and Responses\n1. **Simplicity of Discriminator**:\n   - **Reviewer Concerns**: Reviewers argue that the quadratic and linear discriminators are too simple and do not capture the true nature of WGANs, which typically use more complex neural networks.\n   - **Author Response**: The authors justify their choice by explaining that the quadratic discriminator has sufficient discriminating power for the one-layer generator and that using a more complex discriminator would increase the sample complexity. They also mention that their goal is to show that SGD can learn the ground truth generating distribution with minimal requirements for the discriminator class.\n   - **Evaluation**: While the simplicity of the discriminator is a limitation, the authors provide a valid rationale for their choice. The theoretical insights gained from this simplified setting are valuable and can serve as a foundation for future work on more complex discriminators.\n\n2. **Relevance to Practical WGANs**:\n   - **Reviewer Concerns**: The reviewers question the practical relevance of the results, given that the discriminators used are not typical in practical WGANs.\n   - **Author Response**: The authors acknowledge this limitation but emphasize that their focus is on understanding the dynamics of SGD in a simplified setting. They suggest that the insights gained can be extended to more complex discriminators using recent developments in neural network training.\n   - **Evaluation**: The practical relevance is indeed limited, but the theoretical insights are valuable. The paper can serve as a stepping stone for more advanced analyses.\n\n3. **Terminology and Clarity**:\n   - **Reviewer Concerns**: There is confusion about the terminology \"one-layer generator\" and the claim that the results can be misleading.\n   - **Author Response**: The authors clarify that by \"one-layer generator,\" they mean a linear model with an elementwise monotonic transform. They also emphasize that their goal is to understand the dynamics of SGD in the WGAN setting.\n   - **Evaluation**: The authors should improve the clarity of their terminology and claims to avoid confusion. However, the core contributions remain strong.\n\n4. **Sample Complexity**:\n   - **Reviewer Concerns**: The sample complexity results are questioned, particularly the claim that they match the upper bound provided in (Wu et al., 2019).\n   - **Author Response**: The authors provide references and explanations to support their sample complexity results.\n   - **Evaluation**: The sample complexity analysis is a strong point of the paper and adds to its theoretical value.\n\n#### Overall Evaluation\nThe paper makes a significant theoretical contribution by providing the first result on the convergence of SGD to global equilibrium points for non-linear generators in WGANs. The choice of quadratic discriminators is justified, and the results are valuable for understanding the dynamics of SGD in a simplified setting. While the practical relevance is limited, the theoretical insights are strong and can serve as a foundation for future work. The authors have addressed the reviewers' concerns to a large extent, and the paper is well-written and technically sound.\n\n### Final Decision\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **1. Novelty and Significance:**\nThe paper introduces a novel connection between the expressivity of ReLU networks and Sharkovsky's Theorem from dynamical systems theory. This connection is significant because it provides a new perspective on the depth-width trade-offs in neural networks, particularly for functions with periodic points. The paper addresses an open question posed by Telgarsky regarding the representational power of deep versus shallow networks, which is a fundamental issue in deep learning and approximation theory.\n\n#### **2. Technical Soundness:**\nThe technical content of the paper is strong. The proofs are well-written and appear to be correct, as noted by Reviewer 2. The application of Sharkovsky's Theorem to derive depth-width trade-offs is both clever and well-executed. The lower bounds on the width needed to represent periodic functions are derived rigorously, and the theoretical background is well-explained.\n\n#### **3. Clarity and Exposition:**\nThe paper is well-written and easy to follow, as highlighted by Reviewer 2. The authors have taken the feedback from Reviewer 3 and restructured the paper to present the main results and contributions to ML earlier, which improves the overall flow and readability. The inclusion of examples and intuition for definitions (e.g., Definition 3) further enhances the clarity.\n\n#### **4. Empirical Validation:**\nThe authors have addressed the request for empirical validation by performing experiments on a synthetic dataset generated by triangle functions. They have plotted the classification error as a function of depth, which provides a concrete illustration of the theoretical results. This empirical evidence helps to contextualize the theoretical findings and makes the paper more compelling.\n\n#### **5. Relevance and Impact:**\nThe paper is highly relevant to the field of deep learning and approximation theory. It contributes to a deeper understanding of the benefits of depth in neural networks, which is a critical issue for both theoretical and practical applications. The insights provided by the paper could inform the design of more efficient and effective neural network architectures.\n\n#### **6. Addressing Reviewer Comments:**\nThe authors have responded thoughtfully to the reviewer comments. They have restructured the paper to improve clarity, provided additional examples and intuition, and included empirical results to support their theoretical findings. They have also addressed specific questions about the impact of bias terms and the brittleness of periodic points, demonstrating a thorough understanding of the subject matter.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper is a significant contribution to the field, with novel insights, rigorous technical content, and clear exposition. The authors have effectively addressed the reviewer comments, and the empirical validation adds value to the theoretical results. This paper is suitable for publication in a top-tier conference."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Problem Relevance and Novelty:**\n   - The paper addresses a significant and relevant problem in multi-agent reinforcement learning (MARL) by focusing on the coordination of multiple agents with pre-learned skills. This is a novel and important direction in the field, as it bridges the gap between skill learning and multi-agent coordination.\n   - The modular framework proposed is well-motivated and addresses the challenge of coordinating complex tasks by leveraging diverse behaviors of individual skills.\n\n2. **Empirical Evaluation:**\n   - The authors provide a thorough empirical evaluation on a variety of challenging tasks, including manipulation and locomotion tasks. The results demonstrate that the proposed method outperforms baselines, which is a strong indicator of the method's effectiveness.\n   - The inclusion of additional baselines (centralized SBD and decentralized SBD) in response to reviewer feedback further strengthens the empirical evaluation.\n\n3. **Clarity and Presentation:**\n   - The paper is generally well-written and well-structured. The authors have addressed many of the reviewer comments, improving the clarity and consistency of the paper.\n   - The addition of more detailed explanations, such as the analysis of the effect of \\( T_{low} \\) and the inclusion of success rate plots, enhances the paper's readability and robustness.\n\n#### **Weaknesses:**\n1. **Notational Consistency:**\n   - While the authors have made efforts to improve notational consistency, some inconsistencies still exist. This could be a minor issue but can affect the readability of the paper.\n   - The authors should ensure that all notations are consistent and clearly defined, especially in the diagrams and equations.\n\n2. **Design Choices and Hyperparameters:**\n   - The choice of a fixed \\( T_{low} \\) is a design decision that has been justified, but the authors should provide more detailed analysis on the sensitivity of the method to this hyperparameter. While the additional experiments (Figure 7) are helpful, a more comprehensive sensitivity analysis would be beneficial.\n   - The size of the latent skill embedding (5) is now reported, but the authors should also discuss the impact of different sizes on the performance of the method.\n\n3. **Comparison with Other Skill Discovery Methods:**\n   - The authors have addressed the question of whether other skill discovery methods can be used, but a more detailed comparison with other methods (e.g., Hindsight Experience Replay, Option-Critic) would provide a more comprehensive understanding of the method's strengths and limitations.\n   - The authors should also discuss the potential benefits and drawbacks of using different skill discovery methods in the context of multi-agent coordination.\n\n4. **High Variance in Performance:**\n   - The high variance in performance, as noted by Reviewer 1, is a concern. While the authors have addressed this by running more seeds and using moving averages, the variance should be further investigated to ensure the robustness of the method.\n   - The authors should provide a more detailed analysis of the sources of variance and discuss potential strategies to reduce it.\n\n#### **Final Decision:**\nDespite the identified weaknesses, the paper makes a significant contribution to the field of multi-agent reinforcement learning by addressing a novel and important problem. The empirical evaluation is thorough, and the authors have addressed most of the reviewer comments effectively. The paper is well-written and presents a clear and compelling argument for the proposed method.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution**\nThe paper introduces a modular framework for coordinating multiple agents by first learning diverse sub-skills and then using a meta-policy to coordinate these skills. The key novelty lies in the combination of skill behavior diversification and the coordination of these skills in a multi-agent setting. This approach is a significant step forward in the field of multi-agent reinforcement learning (MARL) and addresses a gap in the literature where most existing methods focus on single-agent settings or simple multi-agent tasks.\n\n#### **2. Technical Soundness**\n- **Skill Behavior Diversification**: The use of the DIAYN method to learn diverse skills is well-justified and technically sound. The authors provide a clear explanation of how the variational lower bound is used to approximate the posterior distribution \\( q(z|s) \\).\n- **Meta-Policy**: The meta-policy is designed to select and coordinate skills, which is a crucial aspect of the paper. The authors address the concern about the fixed horizon \\( T_{low} \\) and provide experimental evidence to show that smaller \\( T_{low} \\) values are more effective for their tasks.\n- **Baseline Comparisons**: The authors have added two additional baselines (centralized SBD and decentralized SBD) to demonstrate the importance of predefined subtasks. These baselines fail to solve the tasks, which supports the claim that domain expert knowledge is critical for complex tasks.\n\n#### **3. Empirical Evaluation**\n- **Experiments**: The paper presents empirical results on a variety of tasks, including manipulation and locomotion. The tasks are challenging and well-suited to evaluate the proposed method. The results show that the proposed method outperforms the baselines, which is a strong indicator of its effectiveness.\n- **High Variance**: The authors have addressed the issue of high variance in the training curves by adding moving averages and running more seeds. This improves the reliability of the results.\n- **Hyperparameter Sensitivity**: The analysis of the hyperparameters (Section 4.5) is thorough and provides insights into the sensitivity of the method to different settings.\n\n#### **4. Clarity and Presentation**\n- **Notations and Diagrams**: The authors have revised the paper to address inconsistent notations and have improved Figure 2 to better explain the framework. This enhances the clarity of the paper.\n- **Discussion of Related Work**: The authors have provided a more in-depth discussion of related multi-agent methods, which helps to contextualize their contributions. They have also addressed the question of whether temporal abstraction should be treated differently in multi-agent settings.\n\n#### **5. Addressing Reviewer Concerns**\n- **Reviewer 1**: The authors have addressed the concerns about notations, the fixed horizon \\( T_{low} \\), and the high variance in performance. They have also added more seeds to improve the reliability of the results.\n- **Reviewer 2**: The authors have added the suggested baselines and provided a detailed explanation of why DIAYN skills without predefined subtasks fail to solve the tasks. They have also addressed the question of whether the method can work with other skill discovery methods.\n- **Reviewer 3**: The authors have addressed the concern about the importance of predefined subtasks and the design choice of the fixed horizon \\( T_{low} \\). They have also provided additional experiments to support their claims.\n\n### Final Decision\nThe paper addresses an important problem in multi-agent reinforcement learning and presents a novel and effective solution. The technical soundness is strong, and the empirical evaluation is thorough. The authors have addressed the concerns raised by the reviewers, which has improved the quality of the paper. Therefore, I recommend that this paper be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Novelty and Contribution**:\n   - The paper proposes a novel method, AE-GAN+sr, which combines an auto-encoder and a GAN to purify adversarial examples. The method is designed to be computationally efficient compared to existing methods like Defense-GAN.\n   - The authors provide a clear explanation of the method, including the use of the encoder to initialize the search process in the latent space, which reduces computational cost.\n\n2. **Experimental Results**:\n   - The paper includes experiments on MNIST and Fashion-MNIST, showing that the proposed method performs well against various adversarial attacks, including white-box, black-box, and gray-box attacks.\n   - The authors have added results on the CelebA dataset, demonstrating the method's effectiveness on more complex datasets.\n\n3. **Comparisons**:\n   - The authors have included comparisons with state-of-the-art methods like PGD adversarial training, showing that their method can achieve comparable or better performance with fewer computations.\n   - The authors have also addressed the concerns raised by the reviewers by including additional experiments and comparisons, such as the BPDA attack and the l2_attack.\n\n4. **Clarity and Presentation**:\n   - The authors have made significant efforts to improve the clarity and presentation of the paper, addressing the typos and rewording confusing phrases.\n   - They have provided detailed explanations of the training and testing processes, including pseudo-codes and visualizations.\n\n#### Weaknesses of the Paper:\n1. **Incremental Contribution**:\n   - Some reviewers have noted that the contribution is incremental, particularly in comparison to Defense-GAN. However, the authors have shown that their method is more computationally efficient and can achieve comparable or better performance.\n   - The authors have also addressed the incremental nature by including additional results and comparisons, which strengthen the paper's contribution.\n\n2. **Complexity and Robustness**:\n   - The method relies heavily on the auto-encoder's ability to detect and purify adversarial examples, which can be susceptible to white-box attacks. The authors have conducted additional experiments to demonstrate the robustness of their method against such attacks.\n   - The authors have also addressed the issue of the latent distribution and the prior distribution, although they admit that ensuring the similarity between the two distributions is a topic for future research.\n\n3. **CIFAR-10 Experiments**:\n   - The paper lacks experiments on CIFAR-10, which is a more complex dataset. While the authors have included results on CelebA, including results on CIFAR-10 would further strengthen the paper's claims.\n   - The authors have committed to including these results in a future revision, but the current version does not have them.\n\n4. **Writing and Presentation**:\n   - Despite the improvements, the paper still has some issues with clarity and readability. The authors should ensure that the final version is thoroughly revised to meet the standards of a top-tier conference.\n\n### Final Decision\nGiven the strengths of the paper, particularly the novel method, strong experimental results, and the authors' efforts to address the reviewers' concerns, the paper should be accepted for publication. However, the authors should ensure that the final version includes the promised results on CIFAR-10 and further improves the clarity and presentation.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Innovative Approach**: The paper introduces a novel method, Differentiable Hebbian Consolidation (DHC), which combines Hebbian learning with synaptic plasticity to address catastrophic forgetting in continual learning. This approach is well-motivated and draws inspiration from complementary learning systems (CLS) theory, which is a significant contribution to the field.\n\n2. **Clarity and Motivation**: The paper is well-written and provides a clear motivation for the proposed method. The authors effectively explain the background and the need for their approach, making it accessible to readers who may not be familiar with Hebbian learning.\n\n3. **Empirical Evaluation**: The authors conduct a thorough empirical evaluation on several benchmarks, including Permuted MNIST, Split MNIST, and a 5-vision dataset mixture. They also introduce an imbalanced variant of Permuted MNIST, which adds to the novelty and relevance of their work.\n\n4. **Complementary with Other Methods**: The paper demonstrates that DHC can be effectively combined with other task-specific synaptic consolidation methods (e.g., EWC, SI, MAS) to further improve performance. This is a valuable insight for the continual learning community.\n\n5. **Simplicity and Scalability**: The method is simple to implement and does not introduce significant computational overhead. The authors also show that DHC can be easily integrated into existing differentiable programming setups, which is a practical advantage.\n\n#### Weaknesses of the Paper:\n1. **Comparative Performance**: While DHC shows improvement over naive finetuning and in some cases outperforms other methods, it does not consistently outperform state-of-the-art methods like Synaptic Intelligence (SI) on all benchmarks. This is a concern, especially on more challenging datasets like Split CIFAR-10/100.\n\n2. **Limited Scope of Experiments**: The authors have addressed some of the reviewers' concerns by adding experiments on Split CIFAR-10/100, but the paper could benefit from more extensive evaluations on a wider range of datasets and tasks, such as ILSVRC or CORe50. This would provide a more comprehensive understanding of the method's strengths and limitations.\n\n3. **Clarity of Technical Details**: Some reviewers found the technical details, particularly the notation and terminology, to be confusing. The authors have addressed this by adding a figure and clarifying the notation, but further improvements in the clarity of the technical sections could enhance the paper's readability.\n\n4. **Hyperparameter Sensitivity**: The authors mention that DHC introduces a single hyperparameter, the Hebbian learning rate \\(\\eta\\), which is learned during training. However, a more detailed sensitivity analysis and discussion of the impact of this hyperparameter on performance would be beneficial.\n\n### Final Decision\nDespite the weaknesses, the paper introduces a novel and well-motivated approach to continual learning that addresses a significant challenge in the field. The authors have made substantial efforts to address the reviewers' concerns and have provided additional experiments and clarifications. The method's simplicity and potential for integration with other techniques make it a valuable contribution to the continual learning literature.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\n1. **Multi-level Task Dependency**: The paper introduces a novel approach to multi-task learning by modeling both general task dependency and data-specific task dependency. This is a significant contribution, especially for tasks involving hierarchical data like text and graphs. The authors provide a clear motivation for why this is necessary and how it differs from existing methods.\n2. **Parameter Efficiency**: The authors propose a decomposition method to reduce the parameter complexity from \\(O(T^2)\\) to \\(O(T)\\), which is a practical improvement over existing methods that suffer from quadratic complexity.\n3. **Interaction Between Tasks**: The model enables interaction between tasks, which is a key feature not present in some existing methods. This can lead to better performance, especially when tasks are not strongly related.\n\n#### Clarity and Presentation\n1. **Grammar and Readability**: The paper has several grammatical errors and readability issues. While the authors have made efforts to improve the language, the paper still requires thorough proofreading to ensure clarity and coherence.\n2. **Motivation and Explanation**: The motivation for the multi-level task dependency is well-explained, but the paper could benefit from more detailed explanations of the technical components, such as the \"Position-wise Mutual Attention Mechanism\" and the decomposition method.\n\n#### Experimental Evaluation\n1. **Dataset and Baselines**: The experiments are conducted on several public datasets, and the results show significant improvements over current methods. However, the paper lacks a thorough comparison with recent works, particularly \"Taskonomy: Disentangling Task Transfer Learning\" and \"End-to-End Multi-Task Learning with Attention.\" Adding these comparisons would strengthen the paper.\n2. **Additional Baselines**: The authors have added a method that does not involve attention (i.e., only uses D) to the evaluation, which is a good step. However, more baselines, especially those that model task relationships, should be included to demonstrate the effectiveness of the proposed method.\n\n#### Addressing Reviewer Comments\n1. **Reviewer 1 and 2**: The authors have addressed the concerns about grammar and clarity. They have also provided more details on the motivation and the decomposition method.\n2. **Reviewer 3 and 4**: The authors have clarified the distinctions between their work and previous works, particularly \"Taskonomy.\" They have also addressed the issues related to the general task dependency matrix and the position-wise mutual attention mechanism.\n\n### Final Decision\nWhile the paper presents a novel and significant contribution to multi-task learning, it still has some areas that need improvement, particularly in clarity and experimental evaluation. The authors have made efforts to address the reviewer comments, but the paper would benefit from a more thorough revision to ensure it meets the standards of a top-tier conference.\n\nFinal Decision: **Reject**\n\nHowever, the authors are encouraged to revise the paper based on the feedback and resubmit it to a future conference."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Writing Quality**\n- **Abstract and Overall Writing**: The abstract and the entire paper are poorly written, with numerous grammatical errors and unclear sentences. This makes it extremely difficult to understand the research objectives, methodology, and results.\n- **Reviewer Comments**: All three reviewers have highlighted the poor writing quality, with Reviewer 2 and Reviewer 3 emphasizing that they could not understand the paper despite spending significant time on it.\n\n#### 2. **Scientific Rigor and Methodology**\n- **Methodology**: The paper proposes a method to improve Monte Carlo Tree Search (MCTS) using a \"master game tree,\" but the details are not clearly explained. The lack of clarity makes it impossible to evaluate the scientific rigor of the proposed method.\n- **Experiments and Results**: The experiments are described as being conducted on a small problem (tic-tac-toe), but the results and their implications are not clearly presented. There is no detailed analysis of the performance compared to existing methods like AlphaZero.\n\n#### 3. **Literature Review and References**\n- **References**: The paper lacks any references, which is a significant issue. A comprehensive literature review is essential to place the research in the context of existing work and to justify the novelty of the proposed method.\n- **Related Work**: The author mentions finding a similar idea in their country but not internationally. However, without references, it is impossible to verify this claim or understand the contributions of the paper.\n\n#### 4. **Author Response**\n- **Acknowledgment of Issues**: The author acknowledges the poor writing quality and the lack of references. They express a willingness to improve and seek proofreading help in the future.\n- **Research Context**: The author mentions finding a similar idea in their country but not internationally. This suggests that the idea might have some merit, but the current state of the paper does not support this claim.\n\n### Conclusion\nThe paper is severely lacking in clarity, scientific rigor, and proper referencing. The writing quality is so poor that it is difficult to understand the research objectives, methodology, and results. The lack of references and a comprehensive literature review further undermines the paper's credibility. While the author shows a willingness to improve, the current state of the paper is not suitable for publication in a top-tier conference.\n\n### Final Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novelty and Innovation:**\n   - The paper introduces a novel approach to unsupervised domain adaptation (UDA) by allowing different domains to undergo different sequences of operations. This is a significant contribution as it addresses the issue of information asymmetry between domains, which is a common challenge in UDA.\n   - The concept of Domain-Adaptive Multibranch Networks (DAMN) is innovative and provides a flexible framework for handling multiple domains simultaneously.\n\n2. **Technical Soundness:**\n   - The method is technically sound and well-motivated. The use of adaptive computation graphs and learnable weights for different domains is a sensible approach.\n   - The authors have provided a clear explanation of the architecture and the training process, making the paper easy to follow.\n\n3. **Experimental Results:**\n   - The experimental results are convincing and demonstrate the effectiveness of the proposed method. The authors have shown improvements over baseline methods like RPT and DANN.\n   - The authors have addressed the reviewers' concerns by including additional experiments, such as using different capacities for the flows and incorporating additional flows.\n\n4. **Potential Impact:**\n   - The proposed method has the potential to be applied to a wide range of UDA problems and can be combined with other state-of-the-art techniques to further improve performance.\n   - The authors have also mentioned the potential of the method for multi-source and multi-target domain adaptation, which is an important direction for future research.\n\n#### **Weaknesses:**\n1. **Comparison with State-of-the-Art Methods:**\n   - The paper initially only compared the proposed method with RPT and DANN, which are not state-of-the-art UDA methods. This was a significant limitation in the original submission.\n   - However, the authors have addressed this issue in their response by including experiments with the Maximum Classifier Discrepancy (MCD) method, which is a more recent and advanced technique. They have shown that their method consistently outperforms MCD when combined with it.\n\n2. **Ablation Studies and Hyperparameter Sensitivity:**\n   - The original submission lacked detailed ablation studies and hyperparameter sensitivity analyses, which are crucial for understanding the behavior and robustness of the proposed method.\n   - The authors have addressed this by including experiments on the number of parallel flows, the impact of different capacities for the flows, and the behavior of the gates. These additional experiments provide valuable insights into the method.\n\n3. **Clarity and Terminology:**\n   - The term \"flow network\" was a source of confusion, as it is a well-established concept in algorithmics. The authors have addressed this by renaming their approach to Domain-Adaptive Multibranch Networks (DAMN).\n   - The authors have also clarified the behavior of the gates and the parameter sharing in the network, addressing the reviewers' concerns about the counter-intuitive results in Figure 4.\n\n4. **Code and Reproducibility:**\n   - The authors have committed to making their code publicly available, which is a positive step towards reproducibility. This will allow other researchers to build upon their work and verify the results.\n\n### Final Decision\nGiven the novel and innovative approach, the technical soundness of the method, the convincing experimental results, and the authors' thorough response to the reviewers' concerns, I believe this paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper:\n1. **Relevance and Novelty**:\n   - The paper addresses a significant problem in computational biology: predicting DNA folding patterns using machine learning, specifically focusing on Topologically Associating Domains (TADs) in Drosophila melanogaster.\n   - The use of bidirectional LSTM RNNs to leverage the sequential nature of DNA is a novel approach in this context.\n\n2. **Methodological Soundness**:\n   - The authors compare their LSTM model with several other machine learning models (linear models with regularization, Gradient Boosting, etc.), demonstrating the superiority of the LSTM model.\n   - The weighted Mean Squared Error (wMSE) is introduced as a custom loss function, which is a thoughtful approach to handle the specific characteristics of the data.\n\n3. **Biological Insights**:\n   - The paper identifies informative epigenetic features, providing biological insights into the factors that influence chromatin folding.\n\n4. **Clarity and Presentation**:\n   - The paper is well-written and structured, making it easy to follow the methodology and results.\n   - The authors have addressed many of the reviewer comments, improving the clarity and robustness of the paper.\n\n#### Weaknesses of the Paper:\n1. **Generalizability**:\n   - The authors acknowledge that the model is specific to Drosophila melanogaster and may not generalize to other organisms or cell types. This is a limitation that should be clearly stated in the paper.\n   - The choice of the wMSE loss function is specific to the dataset and may not be generalizable to other Hi-C datasets or other types of epigenetic data (e.g., ATAC-seq).\n\n2. **Methodological Comparisons**:\n   - The paper lacks a fair comparison with other loss functions for the baseline models. The authors should use the same loss function for all models to ensure a fair comparison.\n   - The authors should also compare their model with non-recurrent models (e.g., fully connected or convolutional networks) to demonstrate the unique advantage of using RNNs.\n\n3. **Feature Representation**:\n   - The description of how input features were represented (e.g., ChIP-Seq properties) is not entirely clear. The authors should provide more details on the preprocessing steps and feature representation.\n\n4. **Hyperparameter Tuning**:\n   - The authors should provide more details on the hyperparameter tuning process for the LSTM model and baseline models.\n\n5. **Literature Review**:\n   - The literature review is somewhat limited. The authors should cite and discuss more related works, especially those that have used machine learning for similar tasks, even if they were presented at workshops.\n\n#### Addressing Reviewer Comments:\n- **Reviewer 1**: The authors have addressed the concern about the wMSE loss function by using it for all models and providing additional evaluation metrics. However, they should still consider using other loss functions for a more comprehensive comparison.\n- **Reviewer 2**: The authors have made several improvements to the clarity and presentation of the paper, including fixing equations and labels.\n- **Reviewer 3**: The authors have expanded the literature review and provided more details on the data and methods. However, they should still consider comparing their model with non-recurrent models and providing more details on hyperparameter tuning.\n\n### Final Decision\nDespite the weaknesses, the paper presents a novel and relevant approach to predicting DNA folding patterns using machine learning. The authors have made significant improvements in response to the reviewer comments, enhancing the clarity and robustness of the paper. The biological insights and methodological soundness are strong points that make the paper suitable for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\n- **Novelty**: The paper claims to be the first to provide a polynomial-time algorithm with a provable near-optimal guarantee on the number of violated constraints for Mahalanobis metric learning. This is a significant theoretical contribution, especially given the adversarial nature of the inputs. The use of linear programming (LP) techniques in this context is novel and leverages well-known results from computational geometry.\n- **State-of-the-Art**: The paper positions itself well against existing methods like ITML and LMNN, which are widely cited and used. The authors provide a clear comparison and highlight the limitations of these methods in handling adversarial inputs.\n\n#### Technical Soundness\n- **Theoretical Guarantees**: The paper provides a rigorous theoretical analysis, including the proof of the FPTAS. The use of LP-type problems and the results from Har-Peled are well-cited and appropriately applied.\n- **Proof Clarity**: There are some issues with the clarity of the proofs, particularly in Lemma 2.1. The authors have addressed this in their response, indicating that they will rephrase the proof to make it clearer.\n- **Computational Hardness**: The authors acknowledge that the exact complexity of the problem is not known but suspect it to be NP-hard for unbounded dimensions. They also provide a theoretical result for constant dimensions, which is a valuable contribution.\n\n#### Experimental Results\n- **Performance**: The experimental results show that the proposed algorithm performs favorably, especially in the presence of adversarial noise. The authors demonstrate that their method can correctly learn the ground truth in scenarios where previous methods fail.\n- **Running Time**: The running time analysis is somewhat unclear, particularly in Figure 4. The authors explain that PCA can affect the combinatorial structure, but this explanation could be more detailed in the final version.\n\n#### Clarity and Presentation\n- **Clarity**: The paper is generally well-written, but there are some areas that need improvement. The authors have addressed several points raised by the reviewers, such as defining combinatorial dimension and clarifying the proof of Lemma 2.1.\n- **Conclusion**: The paper lacks a conclusion, which is a significant omission. The authors should add a conclusion to summarize their findings and discuss future work.\n\n#### Comparison with Recent Work\n- **Related Work**: The authors have addressed the need to compare their work with recent papers by Verma and Branson and Ye et al. They have clearly explained the differences in the settings and the contributions of their work.\n\n### Final Decision\nThe paper makes a significant theoretical contribution to the field of Mahalanobis metric learning by providing the first polynomial-time algorithm with provable guarantees on the number of violated constraints for adversarial inputs. The use of LP-type problems is novel and well-executed. While there are some issues with clarity and the experimental results, these can be addressed in the final version. The paper's contributions are strong enough to warrant acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\nThe paper proposes a deep neural approach to model continuous time-evolving graphs using a temporal point process (TPP) framework. The main contributions include:\n1. **Temporal Dependency Graph (TDG)**: A novel concept to capture complex dependencies among time-evolving interactions.\n2. **Graph-Structured LSTM with Time Gates (DIP Units)**: An extension of LSTM to handle irregular time intervals in dynamic graphs.\n3. **Selection Mechanism**: A method to weight important interactions in k-hop history subgraphs.\n4. **Stacked LSTM with Fusion**: A deep architecture to capture multiple time resolutions and integrate information from different layers.\n\nWhile the individual components (LSTM, attention, and stacking) are not entirely novel, the combination and application to time-evolving graphs are innovative. The authors provide a clear and detailed explanation of how these components work together to address the problem.\n\n#### Technical Soundness\nThe paper is technically sound in its approach and methodology. The use of TPP to model interaction dynamics is well-justified, and the proposed DIP units effectively handle the temporal and structural aspects of the problem. The selection mechanism and fusion of multiple layers add significant value to the model.\n\nHowever, there are some concerns:\n1. **Log-likelihood Function**: The initial formulation of the log-likelihood function was incorrect, but the authors have corrected it in their response.\n2. **Ablation Studies**: The ablation studies provided are somewhat limited. More detailed analysis, such as the effect of different k values and the impact of removing specific components, would strengthen the paper.\n3. **Comparison with JODIE**: The authors have added a comparison with JODIE, which is a significant improvement. However, a more detailed discussion of the differences and performance comparisons would be beneficial.\n\n#### Experimental Results\nThe experimental results are promising, showing superior performance on interaction prediction and classification tasks. The authors have included a real-world financial application, which adds practical relevance. The inclusion of JODIE in the comparison is a positive step, but more detailed analysis of the results and insights into why the proposed method outperforms baselines would enhance the paper.\n\n#### Clarity and Presentation\nThe paper is generally well-written and structured. The addition of an overview diagram (Figure 2) and a more detailed explanation of the pipeline (Figure 3) significantly improves readability. However, some notations and equations could be more consistent, and a more detailed discussion of the results would be beneficial.\n\n#### Addressing Reviewer Comments\nThe authors have addressed most of the reviewer comments effectively:\n1. **Corrected Log-likelihood Function**: The authors have corrected the log-likelihood function and provided a clear explanation.\n2. **Ablation Studies**: Additional ablation studies have been provided, though more detailed analysis would be beneficial.\n3. **Comparison with JODIE**: The authors have included JODIE in the comparison and provided some analysis.\n4. **Clarity and Readability**: The addition of figures and a more detailed explanation of the pipeline have improved clarity.\n\n### Final Decision\nDespite some minor issues, the paper presents a novel and effective approach to modeling continuous time-evolving graphs. The technical soundness, experimental results, and addressing of reviewer comments are strong. The contributions are significant and the paper is well-positioned for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Contribution**\nThe paper introduces a novel framework, Continuous Query Decomposition (CQD), for answering complex queries on incomplete Knowledge Graphs (KGs). The key contributions are:\n- **Elegant Solution**: The method translates complex queries into an end-to-end differentiable objective, leveraging pre-trained neural link predictors.\n- **State-of-the-Art Results**: The proposed approach outperforms existing methods, achieving significant improvements in Hits@3 with much less training data.\n- **Explainability**: The model provides insights into the intermediate solutions, enhancing interpretability.\n\n#### **2. Technical Soundness**\n- **Methodology**: The paper is technically sound, with a clear and logical flow. The use of t-norms and t-conorms for logical operations is well-justified.\n- **Optimization Techniques**: Both continuous and combinatorial optimization methods are well-explained, and the authors provide a detailed analysis of their performance.\n- **Neural Link Predictors**: The authors address the dependency on the choice of neural link predictors and provide additional experiments with DistMult, showing that the improvements are not solely due to the choice of ComplEx.\n\n#### **3. Experimental Evaluation**\n- **Comprehensive Experiments**: The experiments are extensive and well-designed, covering a variety of query types and datasets.\n- **Comparison with Baselines**: The paper compares CQD with state-of-the-art methods (GQE, Q2B) and demonstrates superior performance.\n- **Ablation Studies**: The authors include ablation studies to understand the impact of different components, such as the choice of t-norms and embedding sizes.\n\n#### **4. Clarity and Presentation**\n- **Writing Quality**: The paper is well-written and easy to follow. The authors have addressed most of the reviewer comments, improving the clarity and presentation.\n- **Figures and Tables**: The figures and tables are clear and informative, although some minor issues (e.g., notation consistency) have been pointed out and addressed.\n\n#### **5. Addressing Reviewer Comments**\n- **Reviewer 1**: The author's response cleared the reviewer's doubts.\n- **Reviewer 2**: The authors addressed the need for more qualitative discussions and provided additional insights into the query types and their importance.\n- **Reviewer 3**: The authors provided detailed responses to the technical questions, including the use of different neural link predictors and timing results.\n- **Reviewer 4**: The authors clarified the use of 1-hop queries for training and provided additional experiments with DistMult.\n- **Reviewer 5**: The authors addressed the concerns about the embedding size and the use of the target embedding, and provided more detailed timing results.\n\n### Final Decision\nThe paper presents a novel and effective method for answering complex queries on incomplete KGs. It addresses a significant challenge in the field, provides state-of-the-art results, and includes thorough experimental validation. The authors have also responded well to the reviewer comments, addressing most of the concerns and improving the clarity of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Summary of the Paper**\nThe paper introduces \"Deep Coherent Exploration,\" a method that generalizes step-based and trajectory-based exploration for deep reinforcement learning (RL) algorithms in continuous control tasks. The key contributions are:\n1. Simplifying the exploration by modeling only the last layer parameters of the policy network.\n2. Using a recursive, analytical expression for marginalizing over the last-layer parameters.\n3. Providing detailed recipes for integrating the method with on-policy (A2C, PPO) and off-policy (SAC) methods.\n\n#### **Strengths**\n1. **Theoretical Contribution**: The paper builds on prior work (van Hoof et al., 2017) and extends it to deep RL, which is a significant step forward in the field.\n2. **Empirical Results**: The method shows strong performance improvements for on-policy methods (A2C, PPO) on several continuous control tasks.\n3. **Clarity and Reproducibility**: The paper is well-written, and the detailed appendix provides clear steps for reproducing the method.\n4. **Mathematical Soundness**: The derivations and mathematical formulations are solid and well-explained.\n\n#### **Weaknesses**\n1. **Off-Policy Methods (SAC)**: The integration of the method with SAC is less robust and heuristic. The empirical results for SAC are not as strong as those for on-policy methods, and in some cases, performance degrades.\n2. **Limited Scope of Experiments**: The experiments are primarily conducted on Mujoco environments, which, while diverse, might not be sufficient to fully validate the method's effectiveness across a broader range of tasks.\n3. **Hyperparameter Sensitivity**: The performance of the method with NoisyNet and PSNE is sensitive to hyperparameters, which could be a limitation in practical applications.\n4. **Comparison with State-of-the-Art**: The method does not significantly improve the performance of SAC, which is currently the state-of-the-art for continuous control tasks.\n\n#### **Reviewer Feedback and Author Responses**\n1. **Reviewer 1**: Positive about the on-policy results but concerned about the off-policy (SAC) results. Suggested reframing the paper as an on-policy method.\n2. **Reviewer 2**: Highlighted the novelty and mathematical soundness but raised concerns about the empirical performance with SAC and the limited scope of experiments.\n3. **Reviewer 3**: Praised the clarity and theoretical contributions but suggested moving some technical details to the appendix and improving the presentation.\n4. **Reviewer 4**: Acknowledged the strong performance with on-policy methods but expressed concerns about the heuristic nature of the SAC integration and the need for additional experiments.\n\n#### **Author Responses**\nThe authors have addressed several concerns:\n1. **SAC Integration**: They are working on a new formulation for SAC that is more consistent with the on-policy methods and expect to provide better results.\n2. **Clarity and Presentation**: They plan to move technical details to the appendix and improve the presentation.\n3. **Hyperparameter Tuning**: They will consider tuning the hyperparameters of all methods to ensure fair comparisons.\n4. **Additional Experiments**: They will conduct more experiments to validate the method's effectiveness across a broader range of tasks.\n\n### Final Decision\nThe paper presents a significant theoretical contribution and strong empirical results for on-policy methods. While there are concerns about the off-policy (SAC) results and the limited scope of experiments, the authors have shown a willingness to address these issues and are actively working on improvements. Given the importance of the problem and the quality of the work, the paper should be accepted with the condition that the authors address the remaining concerns in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Addressing an Important Problem**:\n   - The paper addresses a significant issue in the field: the discrepancy between the number of supervised updates required by deep neural networks and the biological feasibility of such updates in the primate visual system. This is a critical gap that needs to be addressed to improve the biological plausibility of computational models.\n\n2. **Innovative Methods**:\n   - The paper introduces three complementary strategies to reduce the number of supervised updates: reduced training, improved initialization (Weight Compression), and selective training (Critical Training). These methods are novel and thought-provoking, especially in the context of biological relevance.\n\n3. **Thorough Experiments**:\n   - The experiments are well-designed and comprehensive. The authors provide a detailed analysis of the performance of their methods across different models and conditions, which adds to the robustness of their findings.\n\n4. **Relevance to the Field**:\n   - The work is highly relevant to both computational neuroscience and machine learning. It bridges the gap between these fields by proposing methods that could lead to more biologically plausible models of the visual system.\n\n5. **Clarity and Writing**:\n   - The paper is well-written and clearly structured. The authors have made efforts to address the concerns raised by the reviewers, particularly in clarifying the biological assumptions and the limitations of the Brain-Score metric.\n\n#### Weaknesses of the Paper\n\n1. **Biological Assumptions**:\n   - The biological assumptions underlying the proposed methods, particularly Weight Compression, are not fully justified. The authors acknowledge this and clarify that their methods are exploratory, but a more detailed discussion of the biological plausibility of these methods would strengthen the paper.\n\n2. **Interpretation of Brain-Score**:\n   - The interpretation of Brain-Score as a measure of biological similarity is a significant concern. The reviewers have raised valid points about the limitations of Brain-Score and the need to be cautious in interpreting the results. The authors have addressed this to some extent, but further clarification and possibly additional analyses would be beneficial.\n\n3. **Generalization and Robustness**:\n   - While the authors show that their methods generalize to different architectures (ResNet-50 and MobileNet), more extensive validation on a broader range of datasets and models would strengthen the claims. Additionally, the robustness of the results to different initializations and training conditions should be further explored.\n\n4. **Theoretical Foundation**:\n   - The theoretical foundation of the proposed methods is somewhat lacking. The authors should provide a more detailed explanation of the biological mechanisms that their methods aim to model and how these mechanisms align with current understanding in neuroscience.\n\n#### Addressing Reviewer Concerns\n\n- **Reviewer 1**:\n  - The authors have addressed the concerns about the interpretation of Brain-Score and the biological significance of the results. They have provided additional statistical analyses and clarified the limitations of their methods.\n\n- **Reviewer 2**:\n  - The authors have addressed the concerns about the biological significance of the results and the limitations of Brain-Score. They have also provided additional details about the experimental setup and the robustness of the results.\n\n- **Reviewer 3**:\n  - The authors have addressed the concerns about the biological assumptions and the theoretical foundation of their methods. They have also clarified the scope of their work and the limitations of the current models.\n\n- **Reviewer 4**:\n  - The authors have addressed the minor comments and provided additional details about the experimental setup and the results.\n\n- **Reviewer 5**:\n  - The authors have addressed the concerns about the biological assumptions and the interpretation of the results. They have also provided additional details about the experimental setup and the robustness of the results.\n\n### Final Decision\n\nDespite the weaknesses, the paper makes a significant contribution to the field by addressing an important problem and introducing novel methods. The authors have made substantial efforts to address the concerns raised by the reviewers, and the paper is well-written and well-structured. The work is relevant and has the potential to inspire further research in both computational neuroscience and machine learning.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Positive Aspects:\n1. **Novelty and Contribution**:\n   - The paper proposes a generalized probability kernel (GPK) that extends existing discrepancy measures like MMD and KSD. This is a significant contribution as it unifies and generalizes these measures.\n   - The introduction of power-MMD as a specific instance of GPK is a novel and potentially useful extension of MMD.\n\n2. **Theoretical Framework**:\n   - The authors provide a theoretical framework for GPK, including definitions and properties. They also discuss the convergence bounds and unbiased estimators for the proposed measures.\n   - The connection between GPK and existing methods (MMD, KSD) is well-explained, which can be valuable for researchers in the field.\n\n3. **Potential Impact**:\n   - The work connects the fields of discrete distribution-property estimation and kernel-based hypothesis testing, which could open up new research directions and applications.\n\n#### Negative Aspects:\n1. **Clarity and Presentation**:\n   - The paper has numerous typos, grammatical errors, and notational inconsistencies. These issues significantly impact the readability and clarity of the work.\n   - The notation is confusing, with the same symbol (e.g., \\( K \\)) used to represent different concepts (kernel between distributions, kernel between values, and gram matrix).\n\n2. **Theoretical Rigor**:\n   - The initial version had several theoretical issues, including incorrect proofs and missing details. While the authors have addressed some of these in the revision, the paper still lacks a rigorous and complete theoretical foundation.\n   - The proof of Theorem 5 is still missing, and the authors have removed the discussion of KSD due to an incorrect proof. This raises concerns about the robustness of the theoretical results.\n\n3. **Experimental Evaluation**:\n   - The paper lacks any experimental evaluation. The absence of empirical results makes it difficult to assess the practical utility and performance of the proposed methods.\n   - The authors do not provide any analysis showing that the proposed estimators are useful in any specific context or application.\n\n4. **Motivation and Relevance**:\n   - The motivation for the proposed GPK is not clearly articulated. The authors need to provide more concrete examples and applications to demonstrate the relevance and utility of their work.\n   - The setting where the labels of discrete variables lie in \\(\\mathbb{R}^k\\) and satisfy a notion of distance is not well-motivated, and the authors should provide at least one relevant example to justify this setting.\n\n### Final Decision\nGiven the significant issues with clarity, presentation, and the lack of experimental evaluation, the paper is not ready for publication in a top-tier conference. While the idea of generalizing MMD and KSD is interesting and has potential, the current version of the paper does not meet the high standards required for such a venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Novelty and Contribution:**\n   - The paper introduces a novel method for disentangling representations in pre-trained transformers by learning binary masks over weights or hidden units. This is a significant contribution as it sidesteps the need to train a disentangled model from scratch, which is computationally expensive.\n   - The method is straightforward and easy to understand, which is a plus for reproducibility and adoption by the community.\n\n2. **Experimental Setup:**\n   - The experiments are well-designed to evaluate the disentanglement of syntax and semantics, and sentiment from genre. The use of a spurious correlation (e.g., genre and sentiment in movie reviews) is a strong test of the method's robustness.\n   - The paper shows that the proposed method outperforms baselines, including VAEs and adversarial training, in terms of disentanglement and robustness.\n\n3. **Robustness and Generalization:**\n   - The paper demonstrates that the method can help models be less reliant on spurious correlations, which is a crucial aspect of robustness in NLP models.\n   - The experiments on different tasks (syntax, semantics, sentiment, and genre) provide a good range of evaluations, showing the method's potential generalizability.\n\n4. **Clarity and Presentation:**\n   - The paper is well-written and the ideas are clearly explained. The figures and tables are well-presented and support the claims made in the paper.\n\n#### **Weaknesses and Concerns:**\n1. **Generalizability:**\n   - While the experiments cover different tasks, the focus on specific genres (Drama and Horror) and the limited number of domains (movie reviews and syntactic/semantic tasks) raise questions about the method's generalizability. More diverse datasets and tasks would strengthen the paper.\n   - The reviewer comments highlight the need for more extensive experiments to ensure the method's robustness across a broader range of scenarios.\n\n2. **Comparison to Baselines:**\n   - The paper mentions outperforming VAEs and adversarial training but does not provide a comprehensive comparison in all experiments. Adding more baselines, especially in the sentiment/genre experiment, would strengthen the claims.\n   - The reviewer comments suggest that the fine-tuned baseline performs well in some tasks, which needs further discussion and analysis.\n\n3. **Technical Details:**\n   - The paper lacks some technical details, such as the specific layers and weights being masked. These details are important for reproducibility and understanding the method's limitations.\n   - The reviewer comments raise valid questions about the computational efficiency of the method, especially regarding the number of parameters and the impact of masking on model performance.\n\n4. **Further Experiments:**\n   - The reviewers suggest additional experiments, such as training masks after fine-tuning, exploring L1 penalties for sparsity, and considering more attributes. These experiments would provide a more comprehensive evaluation of the method.\n\n#### **Author Responses:**\n- The authors have addressed many of the reviewer comments and provided additional experiments and clarifications. They have included VAE baselines in the sentiment/genre experiment and clarified the experimental setup.\n- The authors have also committed to conducting further experiments and adding more details to the paper, which is a positive step.\n\n### Final Decision\nDespite the weaknesses and concerns, the paper presents a novel and interesting approach to disentangling representations in pre-trained transformers. The method shows promising results in terms of robustness and disentanglement, and the authors have addressed many of the reviewer comments. The paper has the potential to contribute valuable insights to the field of NLP and representation learning.\n\nFinal Decision: Accept\n\nHowever, the authors should address the remaining concerns and suggestions in the camera-ready version to strengthen the paper further."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, for dynamic task weighting in multitask learning. The idea of using task-specific updates of the model parameters to adjust task weights is innovative and well-motivated.\n   - The method is flexible and can be generalized to different models and tasks, which is a significant advantage.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow. The authors provide a clear summary of related work and a detailed explanation of the proposed methodology.\n   - The empirical evaluation is performed on both computer vision and natural language understanding tasks, demonstrating the method's versatility.\n\n3. **Empirical Evaluation**:\n   - The experiments on MultiMNIST show that $\\alpha$VIL outperforms other methods in terms of mean performance, which is a positive result.\n   - The authors provide a detailed analysis of the results and discuss the implications of the findings.\n\n#### Weaknesses:\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation. The authors do not provide a formal analysis or theoretical justification for the proposed method, which is a significant concern for a top-tier conference.\n   - The intuition behind certain steps in the algorithm, such as subtracting 1 from the alpha parameters, is not clearly explained.\n\n2. **Empirical Results**:\n   - The experimental results on NLU tasks are mixed and not consistently better than the baselines. The improvements are often marginal and sometimes within the standard deviation of the baselines.\n   - The choice of baselines is somewhat limited, and the authors could have included more competitive methods for a more comprehensive comparison.\n\n3. **Experimental Setup**:\n   - The choice of the MultiMNIST dataset with only one auxiliary task is not very convincing for a multi-task learning setup. More complex datasets with multiple auxiliary tasks would have provided a stronger evaluation.\n   - The authors should provide more ablation studies and sanity checks to validate the effectiveness of the proposed method.\n\n4. **Statistical Significance**:\n   - The authors do not provide statistical significance scores for the results, which is crucial for validating the improvements over the baselines.\n   - The standard deviations for the NLU tasks are not reported, which makes it difficult to assess the robustness of the results.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, the lack of a strong theoretical foundation and the mixed empirical results are significant concerns. While the method is novel and well-motivated, the current state of the paper does not meet the high standards of a top-tier conference. The authors should address the theoretical and empirical gaps in a revised version of the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **Problem Relevance and Contribution**\nThe paper addresses the problem of distributed mean estimation (DME) and variance reduction, which are fundamental tasks in distributed machine learning. The authors propose a novel method using lattice-based quantization to achieve error bounds that depend on the distance between inputs rather than their norms. This is a significant improvement over previous methods, especially in scenarios where the inputs are concentrated around the mean but have large norms. The problem is well-motivated, and the contribution is substantial.\n\n#### **Technical Soundness and Novelty**\nThe technical approach is sound and novel. The use of lattice theory to achieve the desired error bounds is an innovative idea. The paper provides both theoretical guarantees and practical algorithms. The authors also provide matching upper and lower bounds, which is a strong theoretical contribution. The practical algorithm, while not achieving the optimal theoretical bounds, is straightforward to implement and shows practical improvements over existing methods.\n\n#### **Experimental Evaluation**\nThe experimental evaluation is adequate but could be improved. The experiments are primarily conducted with \\( n = 2 \\), which is a limiting setup. However, the authors do provide some experiments with larger \\( n \\) in the appendix, and they plan to include more in the revision. The experiments show that the proposed method outperforms existing methods, which is a positive sign.\n\n#### **Clarity and Presentation**\nThe paper is generally well-written and well-structured. The problem is clearly defined, and the main ideas are explained in a clear and concise manner. However, there are some minor issues with the presentation, such as typos and unclear explanations in certain sections. The authors have addressed these issues in their response and plan to make the necessary corrections in the revision.\n\n#### **Reviewer Comments and Author Responses**\nThe reviewers have provided constructive feedback, and the authors have responded thoughtfully. They have addressed most of the major concerns, such as the practicality of the algorithms, the need for more experiments, and the clarity of certain definitions. The authors have also committed to making the necessary changes in the revision.\n\n#### **Strengths and Weaknesses**\n**Strengths:**\n- Addresses a fundamental problem in distributed machine learning.\n- Novel use of lattice theory to achieve improved error bounds.\n- Provides both theoretical guarantees and practical algorithms.\n- Strong experimental results showing practical improvements.\n\n**Weaknesses:**\n- Some experiments are limited to \\( n = 2 \\).\n- The practical algorithm does not match the optimal theoretical bounds.\n- Minor issues with clarity and presentation.\n\n### Final Decision\nGiven the significant contribution to the field, the soundness of the technical approach, and the authors' commitment to addressing the reviewers' concerns, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\n- **Novelty**: The paper introduces a novel approach to integrating language throughout the visual pathway, both in bottom-up and top-down processing. This is a significant departure from the common practice of using language only for top-down attention. The idea of modulating both pathways with language is innovative and addresses an important open problem in multimodal tasks.\n- **Contribution**: The paper provides a well-argued case for the necessity of integrating language at multiple levels of visual processing. The experimental results show that this approach leads to significant improvements over baselines that only use top-down or bottom-up modulation. The ablation studies further support the importance of both pathways.\n\n#### Quality of Experiments and Results\n- **Empirical Results**: The results are generally strong, showing state-of-the-art or competitive performance on several referring expression datasets. However, there are some inconsistencies and concerns:\n  - **Variability Across Datasets**: The performance varies across different datasets, which raises questions about the robustness of the model.\n  - **Statistical Significance**: The lack of statistical significance tests is a notable weakness. While the authors provide some mean and standard deviation values, these are not sufficient to establish the robustness of the results.\n  - **Overfitting Concerns**: Reviewer 2's concern about overfitting on the validation set is valid and needs to be addressed.\n\n#### Clarity and Presentation\n- **Clarity**: The paper is generally well-written and easy to follow. The introduction and motivation are clear, and the model is well-described.\n- **Figures and Tables**: The figures and tables are mostly clear, but some improvements are suggested by the reviewers (e.g., adding arrows for top-down and bottom-up processing, providing more space between branches).\n\n#### Conceptual and Theoretical Depth\n- **Conceptual Motivation**: The paper lacks a strong conceptual motivation for the model. While there are references to cognitive science, the paper does not provide a detailed explanation of how language affects low-level visual processing. This is a significant gap that needs to be addressed.\n- **Error Analysis**: The lack of a detailed error analysis is a major weakness. The authors need to provide more insights into how the model works and why it performs better than baselines.\n\n#### Originality and Related Work\n- **Originality**: The model is original in its approach to integrating language at multiple levels of visual processing. However, the innovation over existing models like Step-ConvRNN is not entirely clear. The authors need to provide a more detailed comparison to highlight the unique contributions of their model.\n- **Related Work**: The related work section is comprehensive, but the authors should address the specific points raised by the reviewers, such as the missing reference to Mei et al. (2018).\n\n### Final Decision\nDespite the novel and important contribution, the paper has several weaknesses that need to be addressed before it can be considered for publication in a top-tier conference. The lack of statistical significance tests, the need for a more detailed error analysis, and the unclear conceptual motivation are significant issues that need to be resolved.\n\nFinal Decision: [Reject]"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces a novel framework that links the cerebellum's function to decoupled neural interfaces (DNI) in deep learning. This is a significant contribution because it bridges the gap between systems neuroscience and deep learning, offering a new perspective on how the cerebellum might solve the credit assignment problem in the brain. The authors propose that the cerebellum acts as a decoupling mechanism, similar to DNIs, which is a compelling and novel idea.\n\n#### Clarity and Presentation\nThe paper is generally well-written and clear. The authors have made substantial revisions in response to the reviewers' comments, adding new figures and a new section to highlight specific predictions and experimental findings. The revised version is more focused and addresses the confusion about the temporal feedback problem and the use of bootstrapping. The clarity of the model and its biological relevance has been improved.\n\n#### Experimental Validation\nThe authors have conducted a range of experiments to validate their model, including motor and cognitive tasks. They have also performed ablation studies to show the importance of the cerebellar component in their model. The results are robust and demonstrate the effectiveness of the CC-DNI model in various tasks. The addition of new figures (Fig. 5, 6, 7) and the extended discussion on predictions are particularly valuable, as they provide concrete testable hypotheses that can be compared with experimental data.\n\n#### Scientific Impact\nThe paper has the potential to stimulate further research in both neuroscience and deep learning. The model's predictions about cerebellar function and its role in learning and credit assignment are novel and could lead to new experimental studies. The authors have also highlighted the potential for extending DNI models inspired by the cerebellum, which could have practical applications in deep learning.\n\n#### Addressing Reviewer Concerns\nThe authors have effectively addressed the main concerns raised by the reviewers:\n- **Lack of new insights**: The revised version includes new experiments and predictions that provide specific insights into cerebellar function.\n- **Clarity and focus**: The paper now clearly focuses on the temporal feedback problem and explains the use of bootstrapping.\n- **Comparisons with experimental data**: The authors have added more comparisons with experimental findings and made concrete predictions.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper presents a novel and well-supported framework that bridges systems neuroscience and deep learning. The revisions have significantly improved the clarity, experimental validation, and scientific impact of the work. The model's predictions and experimental findings are valuable and could stimulate further research in both fields. Therefore, the paper should be accepted for publication at a top-tier conference."
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Contribution\n1. **Novelty**: The paper introduces Shoot Tree Search (STS), a method that extends Monte Carlo Tree Search (MCTS) by allowing multi-step expansion. While the idea of multi-step expansion is not entirely new, the authors argue that the context of using neural network value function approximations and the specific implementation details (e.g., \"mega-backpropagation\") make their approach novel. The authors also provide a thorough comparison with existing methods, including MCTS and random shooting, and show that STS can outperform these methods in certain scenarios.\n2. **Contribution**: The main contribution of the paper is the algorithmic innovation and the empirical evaluation showing that STS can achieve better performance in challenging domains like Sokoban and Google Research Football. The authors also provide insights into the bias-variance trade-off and the efficiency of the search tree construction.\n\n#### Empirical Evaluation\n1. **Experiments**: The paper includes a comprehensive set of experiments on Sokoban and Google Research Football, demonstrating that STS outperforms MCTS and other baselines. The authors have also conducted additional experiments in response to reviewer comments, including comparisons with AlphaGo-style leaf evaluation and experiments with different backpropagation schemes.\n2. **Ablation Studies**: The authors have performed ablation studies to understand the impact of different components of STS, such as the multi-step expansion and the backpropagation scheme. These studies provide valuable insights into the algorithm's behavior and help to validate the claims made in the paper.\n\n#### Clarity and Presentation\n1. **Writing Quality**: The paper is generally well-written and easy to understand. However, there are some areas where the presentation could be improved. For example, the reviewer comments highlight issues with the clarity of certain sections, such as the explanation of the algorithm and the experimental setup.\n2. **Figures and Tables**: The figures and tables are generally clear, but some of the results (e.g., Figure 7 and 8) are described as \"weird\" by one of the reviewers. The authors have addressed some of these issues in their response, but further improvements could be made in the final version.\n\n#### Addressing Reviewer Concerns\n1. **Technical Novelty**: The authors have addressed the concerns about the novelty of the multi-step expansion by providing a detailed comparison with existing work and highlighting the differences in the context of using neural network value function approximations.\n2. **Empirical Details**: The authors have provided additional experimental results and clarified some of the experimental details, such as the hyperparameter settings and the initialization of the value network.\n3. **Clarity and Presentation**: The authors have acknowledged the need for improvements in the presentation and have committed to making these changes in the revised version.\n\n### Final Decision\nGiven the technical novelty, the thorough empirical evaluation, and the authors' responsiveness to reviewer comments, the paper should be accepted for publication. However, the authors should address the remaining issues with clarity and presentation in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Novelty and Contribution\nThe paper introduces GG-GAN, a geometric graph generative adversarial network, which aims to address key challenges in graph generation, such as modeling complex relations, isomorphic graphs, and fully exploiting the latent distribution. The authors claim that GG-GAN is permutation equivariant and can scale to generate graphs with tens of thousands of nodes. While the idea is novel and the theoretical foundation is solid, the novelty and contribution need to be critically evaluated against existing work.\n\n#### Literature Review and Context\nThe paper lacks a comprehensive literature review, which is crucial for a top-tier conference. The reviewers point out that similar properties (isomorphism, expressive power, scalability) have been addressed in previous works like GraphRNN, NetGAN, and TagGen. The authors need to clearly articulate the unique contributions of GG-GAN and how it builds upon or improves existing methods.\n\n#### Theoretical Analysis\nThe paper includes several theorems and propositions to support the proposed method. However, Reviewer 2 and Reviewer 4 raise concerns about the clarity and relevance of these theoretical results. For instance, the existence of distribution \\(\\mathcal{D}_x\\) and the sampling procedure need more rigorous justification. The theoretical analysis should be more tightly integrated with the practical aspects of the model.\n\n#### Experimental Evaluation\nThe experimental section is a significant weak point. Reviewer 1 and Reviewer 4 highlight the following issues:\n- **Dataset Size**: The datasets used (graphs with 9 and 20 vertices) are too small to convincingly demonstrate the scalability and effectiveness of GG-GAN. Larger and more diverse datasets are needed to validate the claims.\n- **Baselines**: The comparison with baselines is not comprehensive. Key methods like NetGAN and simpler models (Watts-Strogatz, BarabÃ¡siâ€“Albert) should be included for a fair comparison.\n- **Performance Metrics**: The performance metrics (MMD) show limited improvement over baselines. The authors need to provide more convincing evidence of the model's superiority.\n\n#### Specific Concerns\n- **Mode Collapse**: Reviewer 2 raises the issue of mode collapse, which is a common problem in GANs. The authors should provide evidence that GG-GAN avoids this issue.\n- **Edge Dependencies**: The claim that GG-GAN models complex edge dependencies is questionable. The sampling procedure for edges is based on independent Bernoulli distributions, which does not capture dependencies.\n- **Initial Configuration**: The necessity of the initial fixed point configuration and unique sampled \\(z\\) for each node is not well justified. More details and theoretical analysis are needed.\n\n#### Writing and Clarity\nThe paper is generally well-written and easy to follow. However, some sections (e.g., Section 2.2.3 on avoiding collisions) are unclear and need more explanation. The introduction of \\(\\phi\\) and its implications are also confusing.\n\n### Final Decision\nGiven the significant concerns raised by the reviewers, particularly the lack of comprehensive experimental evaluation, limited dataset size, and unclear theoretical justifications, the paper does not meet the standards of a top-tier conference. The authors need to address these issues to strengthen the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, which dynamically adjusts task weights during training by leveraging task-specific model updates. This is a unique approach that has not been explored before in the context of multitask learning.\n   - The method is well-motivated and intuitive, addressing the challenge of estimating the positive or negative influence of auxiliary tasks on the target task.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and easy to follow, with clear explanations of the proposed methodology and related work.\n   - The authors provide a clear summary of the related work and the intuition behind their method.\n\n3. **Empirical Evaluation**:\n   - The paper includes experiments on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating the versatility of the proposed method.\n   - The authors report mean, min, max, and standard deviations for 20 runs on MultiMNIST, which is a strong point in terms of robustness and reproducibility.\n\n#### Weaknesses:\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical foundation or formal analysis of the proposed methodology. This is a significant concern, as it leaves the reader questioning the robustness and generalizability of the method.\n   - The authors acknowledge this in their response and plan to address it in the camera-ready version, but the current lack of theoretical support is a major weakness.\n\n2. **Empirical Results**:\n   - The experimental results are not consistently strong. On MultiMNIST, the proposed method outperforms baselines, but the improvements are within a standard deviation of the baselines.\n   - On NLU tasks, the results are mixed, and the proposed method does not consistently outperform the baselines. The improvements are minor and sometimes lower, which raises questions about the effectiveness of the method.\n   - The authors provide a reasonable explanation for the small improvements in NLU tasks, but this does not fully address the concern that the method may not be significantly better than existing approaches.\n\n3. **Experimental Setup**:\n   - The choice of baselines is somewhat limited. While the authors compare against standard multitask learning and Discriminative Importance Weighting (DIW), they could have included more diverse baselines to strengthen their claims.\n   - The experimental setup for NLU tasks, particularly the sampling of 25% of data for each task, could be more clearly explained and justified.\n\n4. **Additional Experiments and Ablations**:\n   - The paper could benefit from additional experiments and ablation studies to further validate the proposed method. For example, experiments with more auxiliary tasks, different datasets, and a more detailed analysis of the learned task weights would strengthen the paper.\n\n#### Author Response:\n- The authors address some of the concerns raised by the reviewers, particularly the theoretical justification and the experimental setup. They plan to add more theoretical analysis and additional experiments in the camera-ready version.\n- The authors provide a clear explanation for the choice of MultiMNIST and the behavior of task weights, which helps to clarify some of the reviewer's concerns.\n- However, the authors' response does not fully address the concerns about the empirical results, particularly the lack of consistent improvements over baselines.\n\n### Final Decision\nGiven the novel and intuitive approach, the well-written presentation, and the authors' commitment to addressing the weaknesses in the camera-ready version, the paper has the potential to make a valuable contribution to the field. However, the current lack of strong theoretical justification and consistent empirical results is a significant concern.\n\nFinal Decision: **Reject**\n\nThe paper needs substantial improvements in both theoretical and empirical aspects to meet the standards of a top-tier conference. The authors should focus on providing a stronger theoretical foundation, more diverse and robust experimental results, and additional ablation studies to fully validate the proposed method."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novelty and Intuition**:\n   - The paper introduces a novel method, $\\alpha$VIL, for dynamic task weighting in multitask learning. The method leverages task-specific updates of the model parameters to adjust task weights, which is a unique and intuitive approach.\n   - The method is well-motivated and the algorithm is clearly described.\n\n2. **Empirical Evaluation**:\n   - The paper provides empirical evaluations on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, demonstrating the method's applicability across different domains.\n   - The experiments are well-designed, and the results are presented clearly.\n\n3. **Clarity and Readability**:\n   - The paper is well-written and easy to follow. The related work is summarized clearly, and the methodology is well-explained.\n\n#### Weaknesses of the Paper\n1. **Theoretical Justification**:\n   - The paper lacks a strong theoretical justification for the proposed method. While the intuition is clear, a formal analysis or theoretical guarantees would strengthen the paper significantly.\n   - The reviewers have pointed out the need for a more rigorous theoretical foundation.\n\n2. **Empirical Results**:\n   - The empirical results are not consistently strong. On the MultiMNIST dataset, the proposed method outperforms baselines, but the improvements are marginal and within the standard deviation of the baselines.\n   - On the NLU tasks, the results are mixed, and the proposed method does not consistently outperform the baselines. The improvements are minor and sometimes lower, which raises questions about the method's effectiveness.\n   - The choice of baselines is somewhat limited, and the paper could benefit from comparing with more recent and competitive multitask learning methods.\n\n3. **Experimental Setup**:\n   - The choice of the MultiMNIST dataset with only one auxiliary task is somewhat limiting. The paper could be stronger with experiments on datasets with multiple auxiliary tasks.\n   - The paper could benefit from additional ablation studies and sanity checks to validate the method's effectiveness and robustness.\n\n4. **Statistical Significance**:\n   - The paper does not provide statistical significance scores for the results, which is crucial for evaluating the robustness of the empirical findings.\n\n#### Author Response\n- The authors have addressed some of the reviewers' concerns, particularly regarding the intuition behind the algorithm and the choice of the MultiMNIST dataset.\n- They have acknowledged the need for more theoretical justification and additional experiments, and have committed to addressing these issues in the camera-ready version.\n- However, the authors have not provided new experimental results or significant additional analysis in their response, which leaves some of the reviewers' concerns unaddressed.\n\n### Final Decision\nGiven the novel and intuitive approach, the well-written and clear presentation, and the authors' commitment to addressing the weaknesses, the paper has potential. However, the lack of strong empirical results and the need for more theoretical justification are significant concerns. The paper would benefit from additional experiments, ablation studies, and a more rigorous theoretical analysis.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Problem Relevance**: The paper addresses a significant and practical problem in ecological inference, which is crucial for various applications, including political science and election analysis.\n2. **Methodological Innovation**: The paper introduces an efficient approximation to the loss function for ecological inference, enabling the use of linear models, deep neural networks, and Bayesian neural networks. This is a novel and valuable contribution.\n3. **Real-World Application**: The paper applies the proposed methods to real-world data from the Maryland 2018 midterm elections, demonstrating practical utility and providing insights into voting patterns.\n4. **Multi-Task Learning**: The paper shows that joint learning across multiple races improves the accuracy of ecological inference, which is a significant finding.\n\n#### Weaknesses\n1. **Clarity and Structure**: The paper is not easy to follow, with several structural issues and typos. Key concepts and methods are not introduced early enough, making the paper difficult to understand for readers not familiar with the domain.\n2. **Experimental Details**: The paper lacks crucial experimental details, such as the implementation of baselines, train/test splits, and hyperparameters. This hampers reproducibility and the ability to verify the results.\n3. **Baseline Comparisons**: The paper does not compare the proposed methods to standard multi-level models (MLMs), which is essential to demonstrate the added value of the deep learning approach.\n4. **Analysis Depth**: The analysis is incomplete and lacks depth. Claims about the performance of the models are not well-supported with quantitative evidence, and some results, such as the best performance of the linear regression, are not discussed.\n5. **Related Work**: The paper does not adequately cite related work, particularly in the areas of learning with label proportions (LLP), distribution regression, multiple-instance learning, and collective graphical models. This makes it difficult to assess the novelty and significance of the proposed methods.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Highlights the importance of the problem and the interesting direction of the paper but criticizes the clarity, structure, and lack of experimental details.\n- **Reviewer 2**: Emphasizes the importance of the research area and the value of the datasets but points out the incomplete and unconvincing experiments, lack of baseline comparisons, and need for deeper analysis.\n- **Reviewer 3**: Finds the paper interesting and potentially publishable but suggests additional work on hyperparameter optimization and synthetic data experiments.\n- **Reviewer 4**: Criticizes the lack of new technical contributions, inadequate related work, and poor organization of the experiments.\n\n### Final Decision\nGiven the significant strengths of the paper, including its relevance, methodological innovation, and real-world application, it has the potential to make a valuable contribution to the field. However, the weaknesses in clarity, experimental details, and depth of analysis are substantial and need to be addressed. The paper would benefit from substantial revisions to improve its structure, provide more detailed experimental information, and conduct a more thorough analysis.\n\nFinal Decision: **Reject** with the strong recommendation for the authors to revise and resubmit the paper after addressing the identified issues."
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Contribution and Novelty\n1. **Theoretical Integration**: The paper aims to bridge the gap between theoretical insights and practical applications in meta-learning, specifically focusing on few-shot learning. This is a valuable contribution as it attempts to validate theoretical assumptions through practical experiments.\n2. **Regularization Terms**: The paper proposes two regularization terms based on theoretical assumptions from recent meta-learning theory. While the reviewers point out that these terms are similar to existing techniques (e.g., spectral normalization and weight decay), the authors argue that the combination and application of these terms in the context of meta-learning are novel and serve a different purpose. The authors provide additional experiments and explanations to support this claim.\n3. **Empirical Validation**: The paper includes empirical results on standard few-shot learning benchmarks, showing improvements over vanilla meta-learning methods. However, the improvements are sometimes within the standard deviation, which raises questions about their significance.\n\n#### Technical Soundness\n1. **Assumptions and Theoretical Consistency**: The reviewers raise valid concerns about the assumptions made in the theoretical framework and their practical implications. The authors address these concerns by providing additional explanations and synthetic experiments, which help clarify the practical relevance of the theoretical assumptions.\n2. **Experimental Setup**: The experimental setup is well-designed, and the authors have addressed some of the reviewers' concerns by adding more baselines and hyperparameter tuning. However, the inclusion of more recent and complex methods would strengthen the paper further.\n3. **Implementation Details**: The authors address the concern about the computation of subgradients through SVD, which is supported by existing literature. This ensures the technical soundness of the implementation.\n\n#### Clarity and Presentation\n1. **Writing and Organization**: The paper is well-organized and clearly written. The authors have made efforts to address the reviewers' comments and improve the clarity of the paper.\n2. **Narrative and Goals**: The authors have clarified that their primary goal is not to propose a novel meta-learning algorithm but to validate theoretical insights. This aligns well with the paper's contributions and should be reflected more clearly in the narrative.\n\n#### Impact and Relevance\n1. **Relevance to the Field**: The paper addresses a relevant and important problem in the field of meta-learning and few-shot learning. The integration of theoretical insights into practical applications is a significant step forward.\n2. **Potential Impact**: While the improvements in performance are sometimes marginal, the paper's contribution lies in its methodological approach and the validation of theoretical assumptions. This can inspire further research and practical applications in the field.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper makes a valuable contribution by integrating theoretical insights into practical applications in meta-learning. While there are some concerns about the novelty and significance of the proposed regularization terms, the authors have addressed these concerns through additional experiments and explanations. The paper is well-written, technically sound, and relevant to the field. Therefore, it should be accepted for publication at a top-tier conference."
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in reinforcement learning: combining imitation learning and meta-reinforcement learning to improve sample efficiency and adaptability to new tasks. This is a relevant and important area of research.\n\n2. **Methodological Innovation:**\n   - The proposed method, PERIL, introduces a novel approach by combining ideas from meta-RL (specifically PEARL) and imitation learning. The dual inference strategies and the use of demonstrations to precondition exploration policies are innovative and promising.\n\n3. **Empirical Results:**\n   - The empirical results show that PERIL outperforms baselines on a set of 2D and 3D tasks. The method demonstrates strong performance in tasks with sparse rewards, which is a challenging scenario for many RL algorithms.\n\n4. **Clarity and Organization:**\n   - The paper is generally well-organized, and the motivation for the proposed method is clear. The visualizations and figures are helpful in understanding the method.\n\n#### **Weaknesses of the Paper:**\n1. **Clarity and Notation:**\n   - The paper suffers from significant clarity issues. The notation is inconsistent, and many terms and equations are not clearly defined. This makes it difficult for readers to follow the method and understand the contributions.\n   - The problem formulation and the proposed solution are not clearly separated, leading to confusion about what is part of the setting and what is part of the approach.\n\n2. **Complexity and Ablation Studies:**\n   - The method is quite complex, with many different components and loss terms. The paper lacks sufficient ablation studies to understand the contributions of each component. This makes it hard to determine which parts of the method are necessary and which are not.\n   - The inclusion of multiple loss terms without clear justification or ablation studies weakens the paper's claims about the effectiveness of the method.\n\n3. **Related Work and Citations:**\n   - The related work section is limited and does not adequately acknowledge many prior works in context-based meta-RL and meta-IL. This makes it difficult to understand how the proposed method builds on or differs from existing approaches.\n   - The paper underestimates the contributions of closely related works, such as PEARL and meta-IL, and does not clearly explain the differences and improvements.\n\n4. **Experimental Design:**\n   - The experimental design has several issues. The paper lacks multiple seeds and statistical significance tests, making it difficult to trust the results.\n   - The baselines are not comprehensive. The paper does not compare to other methods that combine meta-IL and meta-RL, such as Mendonca et al. or Zhou et al.\n   - The claim of zero-shot learning is not well-supported. The paper does not provide a clear definition of zero-shot learning and does not convincingly demonstrate this capability.\n\n5. **Assumptions and Robustness:**\n   - The paper makes strong assumptions about the availability of expert demonstrations conditioned on the learned latent $z$. This assumption is restrictive and not realistic in many real-world scenarios.\n   - The robustness of the method to noisy or imperfect demonstrations is not thoroughly evaluated.\n\n#### **Author Response:**\n- The authors have acknowledged the feedback and have committed to making significant improvements to the paper. They plan to clarify the problem formulation, improve the related work section, add ablation studies, and include more comprehensive baselines.\n- The authors have also committed to addressing the issues with notation, clarity, and experimental design.\n\n### Final Decision\nGiven the significant weaknesses in clarity, notation, experimental design, and related work, and the need for substantial improvements, the paper is not ready for publication in its current form. However, the authors have shown a willingness to address these issues and improve the paper. Therefore, the best course of action is to reject the paper for the current submission but encourage the authors to resubmit after making the necessary improvements.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Title and Abstract\n- **Title**: The title \"Automatic Music Production Using Generative Adversarial Networks\" is indeed misleading. The paper focuses on automatic music accompaniment, which is a specific subset of music production. The authors have acknowledged this and provided a justification for keeping the title, but it remains a significant issue. A more accurate title would be \"Automatic Music Accompaniment Using Generative Adversarial Networks.\"\n- **Abstract**: The abstract is generally clear but could benefit from more precise language. The authors have addressed some of the issues, such as rephrasing the sentence about consistent demands from producers and artists and clarifying the use of Mel-spectrograms. However, the abstract still lacks a clear motivation for the specific task of automatic accompaniment.\n\n#### Introduction\n- **Motivation**: The introduction provides a good overview of the problem but lacks a clear motivation for why automatic accompaniment is important. The authors have addressed this by rephrasing the sentence to highlight the importance of the task for producers, artists, and companies.\n- **Literature Review**: The literature review is comprehensive but could benefit from more citations to the extensive literature on music generation in the symbolic domain. The authors have added relevant papers, which is a positive step.\n\n#### Method\n- **Source Separation**: The authors have provided more details about the Demucs algorithm, which is a significant improvement. However, the impact of source separation artifacts on the generated audio is still not fully addressed.\n- **CycleGAN**: The details about the CycleGAN architecture and training process are now more comprehensive, which is a positive change. However, the method for recovering the waveform from the generated Mel spectrograms is still not fully explained. The authors have added a detailed explanation, but it would be beneficial to include more technical details and equations.\n\n#### Experiments\n- **Dataset**: The authors have provided more details about the dataset, including the number of songs used from the MusDB18 dataset and the IDs of the songs in the subset. However, the impact of using automatically separated signals from the FMA dataset is still not fully evaluated. The authors have acknowledged this limitation and provided a manual inspection, but a quantitative evaluation would be more convincing.\n- **Evaluation Metrics**: The authors have added a correlation matrix for the annotators and rephrased the justification for the subjective evaluation. However, the choice of the four attributes (quality, euphony, coherence, and intelligibility) is still not fully justified. The authors have explained that these attributes were chosen based on discussions with evaluators, but a more rigorous justification would be beneficial.\n- **Results**: The results are promising, but the mixed performance on the vocal-to-full arrangement task is a concern. The authors have acknowledged this and provided a possible explanation, but more in-depth analysis would be beneficial.\n\n#### Minor Comments\n- **Clarity and Reproducibility**: The authors have addressed many of the minor comments, such as rephrasing sentences and adding citations. However, the paper still lacks some technical details, such as the specific method for phase retrieval and the choice of window function in the short-time Fourier transform.\n\n### Final Decision\nWhile the paper has made significant improvements in addressing the reviewers' comments, there are still some critical issues that need to be addressed before it can be considered for publication in a top-tier conference. The title and abstract need to be more accurate, the method for phase retrieval and waveform recovery needs to be fully explained, and a more rigorous evaluation of the impact of source separation artifacts is required. Additionally, the motivation for the specific task of automatic accompaniment and the justification for the evaluation metrics need to be more clearly articulated.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths of the Paper:**\n1. **Innovative Representation:**\n   - The paper introduces Block Minifloat (BM), a novel representation for training DNNs with low precision (4-8 bits). This is a significant contribution, especially given the growing interest in efficient hardware for deep learning.\n2. **Hardware Implementation:**\n   - The authors provide a detailed hardware implementation of BM, including synthesis results for area and power. This is a strong point, as it demonstrates the practical feasibility of the proposed format.\n3. **Comprehensive Evaluation:**\n   - The paper evaluates BM on a variety of models and datasets, including ResNet-18 on ImageNet. The results show that BM can achieve comparable accuracy to FP32 while significantly reducing hardware costs.\n4. **Kulisch Accumulator:**\n   - The use of a Kulisch accumulator for error-free accumulation is a key technical contribution. The authors explain how BM can reduce the exponent length, making the accumulator more practical.\n\n#### **Weaknesses and Concerns:**\n1. **Clarity and Explanation:**\n   - Several reviewers noted that the paper lacks clarity in explaining key concepts, such as the alignment of minifloat distribution and the derivation of Equation 6. The authors have addressed these issues in the revised draft, but the original submission was somewhat lacking.\n2. **Hybrid Representation:**\n   - The use of different BM formats for forward and backward passes (e.g., (2e,5)/(4e,3)) is a significant overhead that was not fully addressed in the original submission. The authors have clarified this in the revised draft, but it remains a potential issue for hardware implementation.\n3. **Denormal Support:**\n   - The hardware overhead for supporting denormal numbers was not well-explained in the original submission. The authors have provided more details in the revised draft, but the complexity of this process should be more thoroughly discussed.\n4. **Larger Networks:**\n   - The paper primarily evaluates BM on smaller networks like ResNet-18. While the authors mention that they started ResNet50, the lack of results for larger networks is a limitation. This could be addressed in future work.\n\n#### **Reviewer Feedback:**\n- **Reviewer 1:**\n  - Raised concerns about the clarity of the exponent bias and Kulisch accumulator. These issues were addressed in the revised draft.\n- **Reviewer 2:**\n  - Highlighted the importance of the hardware implementation and the need to support multiple formats. The authors have clarified that the hardware area results take into account the need to support multiple formats.\n- **Reviewer 3:**\n  - Emphasized the need for a more detailed explanation of the minifloat distribution alignment and the derivation of Equation 6. The authors have provided more details in the revised draft.\n\n### Final Decision\n\nThe paper introduces a novel and practical representation for training DNNs with low precision, backed by comprehensive hardware implementation and evaluation. While the original submission had some clarity issues, the authors have addressed these concerns in the revised draft. The paper's contributions are significant and align well with the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Innovative Approach**: The paper introduces a novel method (MIMO) for training multiple subnetworks within a single model, which can improve robustness and uncertainty estimation without a significant increase in parameters or computational cost. This is a significant contribution to the field of efficient ensembling.\n2. **Empirical Validation**: The authors provide extensive empirical results on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet) and their out-of-distribution variants. The results show significant improvements in negative log-likelihood, accuracy, and calibration error.\n3. **Clarity and Presentation**: The paper is well-written and easy to follow. The method is clearly explained, and the experimental setup is thorough.\n4. **Theoretical Insights**: The authors provide some theoretical insights into why the subnetworks become independent, supported by empirical evidence (loss landscape, t-SNE plots, conditional variance analysis).\n\n#### Weaknesses:\n1. **Originality Concerns**: While the method is novel, the authors could have provided a more detailed comparison with related work, such as BatchEnsemble. This would help to better understand the unique contributions of MIMO.\n2. **Computational Costs**: The authors claim that the computational cost is equivalent to a standard deep neural network at test time, but this claim could be more rigorously supported. The increase in parameters (1%) and the potential impact on batch size should be more thoroughly discussed.\n3. **Theoretical Depth**: The paper could benefit from a deeper theoretical analysis of why the subnetworks become independent. While the authors provide some empirical evidence, a more rigorous theoretical framework would strengthen the paper.\n4. **Dataset Diversity**: The paper could be improved by including experiments on more diverse datasets, such as OpenImages, to demonstrate the generalizability of the method.\n\n### Author Response\nThe authors have addressed many of the concerns raised by the reviewers:\n- They clarified the source of the 1% increase in model parameters and provided FLOPs measurements.\n- They expanded on the relationship between MIMO and BatchEnsemble.\n- They provided more detailed explanations of the independence of subnetworks and the theoretical insights.\n- They addressed the notation and formatting issues.\n\n### Final Decision\nDespite the weaknesses, the paper presents a novel and significant contribution to the field of efficient ensembling. The empirical results are strong, and the authors have addressed many of the concerns raised by the reviewers. The paper is well-written and provides valuable insights into the behavior of subnetworks within a single model.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Problem and Contribution\nThe paper addresses the important problem of calibrating predictive uncertainties for out-of-distribution (OOD) data in deep image classifiers. This is a significant issue because models are often overconfident on OOD data, leading to unreliable predictions. The authors propose a post-hoc calibration method that uses outlier exposure to improve the calibration of model probabilities. The method is designed to be simple and effective, and it can be applied to various models without requiring additional training.\n\n#### Strengths\n1. **Important Problem**: The problem of model calibration on OOD data is crucial for practical applications, especially in safety-critical domains.\n2. **Simple and Effective Method**: The proposed method is straightforward and computationally efficient, which is a significant advantage.\n3. **Empirical Results**: The authors provide extensive empirical results on standard benchmarks, showing that their method outperforms existing approaches.\n4. **Generalization**: The method is shown to be effective across different types of corruptions, even when the corruption used for calibration is different from the test corruptions.\n5. **Clarity and Presentation**: The paper is well-written, and the authors have made significant efforts to address the reviewers' concerns, including adding new experiments and clarifying the methodology.\n\n#### Weaknesses\n1. **Assumptions and Generalization**: The method relies on a pre-specified set of corruptions for calibration. While the authors show that the method generalizes to other types of corruptions, it is important to thoroughly evaluate its performance on completely OOD data.\n2. **Related Work**: The paper could benefit from a more comprehensive review of related work, particularly in the area of unsupervised domain adaptation and calibration under distribution shift.\n3. **Notation and Terminology**: Some notation and terminology are unclear, which could be improved for better clarity.\n4. **Single vs. Multiple Image Method**: The motivation for the multiple image method is less clear and could be better explained.\n\n#### Addressing Reviewer Concerns\n1. **Reviewer 1**: The reviewer's main concern was that the method assumes a small set of possible distributions. The authors have clarified that the method is evaluated on a wide range of corruptions, not just the ones used for calibration. They have also added results on completely OOD data (SVHN, Fashion-MNIST, Not-MNIST) to address this concern.\n2. **Reviewer 2**: The reviewer suggested additional experiments and baseline comparisons. The authors have added these experiments and provided more detailed explanations of their methods.\n3. **Reviewer 3**: The reviewer pointed out issues with notation and terminology. The authors have addressed these concerns by clarifying the notation and providing more detailed explanations.\n4. **Reviewer 4**: The reviewer suggested additional experiments and baseline comparisons. The authors have added these experiments and provided more detailed explanations of their methods.\n\n### Final Decision\nGiven the importance of the problem, the effectiveness of the proposed method, and the thoroughness of the empirical evaluation, the paper should be accepted for publication. The authors have addressed the reviewers' concerns and provided additional experiments to strengthen their claims. While there are some minor issues with notation and terminology, these can be addressed in the final version of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Clarity and Presentation**\nThe paper suffers from significant issues in clarity and presentation. Multiple reviewers have pointed out that the paper is hard to read, poorly organized, and contains numerous typos and grammatical errors. The authors have acknowledged these issues and have promised to improve the presentation. However, the current state of the paper makes it difficult for readers to follow the arguments and understand the contributions.\n\n#### 2. **Theoretical Foundation**\nThe paper heavily relies on an unpublished work by the authors, which is a significant issue. The reviewers have pointed out that the correctness of the proposed technique cannot be verified due to the lack of access to this unpublished work. This is a critical flaw, as the theoretical foundation of the paper is not independently verifiable. The authors have acknowledged this and suggested that they might publish the other paper first, but this does not address the current issue.\n\n#### 3. **Experimental Evaluation**\nThe experimental evaluation is insufficient and not convincing. The experiments are limited to toy datasets and MNIST, which are too simple to showcase the effectiveness of the proposed method. The authors have acknowledged the need for more experiments and have promised to add comparisons with state-of-the-art pruning techniques. However, the current experimental results do not provide strong evidence of the method's effectiveness.\n\n#### 4. **Novelty and Contribution**\nThe paper claims to introduce a novel algorithm for pruning ReLU networks based on a theoretical foundation. However, the reviewers have pointed out that some of the ideas, such as removing dead neurons and combining always active neurons, are not particularly novel. The authors have emphasized the importance of the third step of their algorithm, which involves clustering neurons based on a specific vector representation. While this step is mathematically interesting, the lack of a clear theoretical foundation and insufficient experimental validation makes it difficult to assess its novelty and contribution.\n\n#### 5. **Author Responses**\nThe authors have provided detailed responses to the reviewers' comments, acknowledging the issues and promising to make improvements. They have also provided some additional explanations and clarifications. However, these responses do not address the fundamental issues with the paper, such as the lack of access to the unpublished work and the insufficient experimental evaluation.\n\n### Conclusion\nThe paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference. The clarity and presentation need substantial improvement, the theoretical foundation is not independently verifiable, and the experimental evaluation is insufficient. While the authors have acknowledged these issues and promised to make improvements, the current state of the paper does not meet the standards of a top-tier conference.\n\n### Final Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### **Original Contribution and Novelty**\nThe paper introduces a novel exploration strategy, Max-Q Entropy Search (MQES), for continuous reinforcement learning (RL). The method aims to maximize the information about the globally optimal distribution of the Q function, which is a significant and interesting contribution. The approach combines elements of information-theoretic principles and distributional RL, which is a novel and promising direction in the field.\n\n#### **Clarity and Presentation**\nThe paper is generally well-written, but there are several areas where the clarity and presentation can be improved:\n- **Terminology and Notation**: The use of terms like \"acquisition functions\" and \"heteroscedastic aleatoric uncertainty\" is not well-explained, which can make the paper difficult to follow for a general RL audience. The authors have addressed this in their response by removing the introduction of acquisition functions and providing more detailed explanations of the uncertainties.\n- **Equations and Derivations**: Several equations and derivations are confusing or lack clarity. For example, Eq. 8 and Eq. 9 were initially unclear, but the authors have revised these equations to make them more rigorous and easier to understand.\n- **Algorithm Details**: The target policy and its update mechanism were not clearly explained in the main paper. The authors have acknowledged this and mentioned that they will move Algorithm 2 to the main paper if possible.\n\n#### **Empirical Evaluation**\nThe empirical evaluation is a mixed bag:\n- **Baseline Comparisons**: The paper compares MQES with SAC and DSAC, but the inclusion of other relevant baselines like OAC and VIME (Houthooft et al., 2016) is missing. The authors have addressed this by including OAC in their revised version and discussing the differences with VIME.\n- **Ablation Studies**: The authors have added ablation studies to explore the sensitivity of hyper-parameters and the impact of distinguishing between epistemic and aleatoric uncertainties. This is a significant improvement.\n- **Sparse Reward Environments**: The authors have conducted experiments on sparse reward environments, which is a good step. However, the performance improvements over DSAC are not always significant, and the authors need to provide more detailed analysis and discussion of these results.\n\n#### **Theoretical Justification**\nThe theoretical foundation of MQES is sound, but some steps in the derivation are not clearly justified. The authors have addressed this by rewriting the equations and providing more detailed explanations. However, the theoretical contribution could be further strengthened by providing more rigorous proofs and derivations.\n\n#### **Impact and Significance**\nThe proposed method has the potential to improve exploration in continuous RL, especially in environments with sparse rewards. The authors have demonstrated some improvements over existing methods, but the significance of these improvements is somewhat limited. The method's ability to handle sparse reward environments is a strong point, but more extensive experiments and comparisons are needed to fully validate its effectiveness.\n\n### Final Decision\nGiven the novel contribution, the improvements made in response to the reviewers' comments, and the potential impact of the method, the paper should be accepted for publication. However, the authors should address the remaining issues and provide more detailed analysis and discussion of the empirical results.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### 1. **Clarity and Presentation**\nThe paper suffers from significant issues in clarity and presentation. Multiple reviewers have pointed out that the paper is hard to read, poorly organized, and contains numerous typos and grammatical errors. The authors have acknowledged these issues and have promised to improve the presentation. However, the current state of the paper makes it difficult for readers to understand the contributions and the theoretical foundations.\n\n#### 2. **Theoretical Foundations**\nThe paper heavily relies on an unpublished work by the authors, which is a significant issue. The theoretical results from this unpublished work are crucial for understanding the proposed algorithm, but they cannot be verified. This lack of transparency and verifiability is a major concern for a top-tier conference. The authors have acknowledged this and suggested that they might publish the other paper first, but this does not address the current issue.\n\n#### 3. **Experimental Evaluation**\nThe experimental evaluation is insufficient and not convincing. The experiments are limited to toy datasets and MNIST, which are not complex enough to showcase the effectiveness of the proposed method. The authors have acknowledged the need for more experiments and have promised to add comparisons with state-of-the-art pruning techniques. However, the current experiments do not provide enough evidence to support the claims made in the paper.\n\n#### 4. **Novelty and Contribution**\nThe novelty of the proposed method is questionable. The authors claim that their method is based on a novel theoretical foundation, but the specific contributions are not clearly articulated. The steps of the algorithm, particularly the clustering step, are not well-motivated or derived. The authors have provided some clarifications in their response, but these clarifications are not reflected in the current version of the paper.\n\n#### 5. **Comparison with State-of-the-Art**\nThe paper lacks a thorough comparison with existing pruning techniques. The authors have acknowledged this and have promised to add such comparisons. However, the current lack of comparison makes it difficult to assess the practical value of the proposed method.\n\n#### 6. **Reviewer Feedback**\nThe reviewers have provided detailed feedback, and the authors have responded to some of the concerns. However, the responses do not fully address the major issues identified by the reviewers. The authors have promised to make improvements, but the current state of the paper is not ready for publication.\n\n### Conclusion\nThe paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference. The theoretical foundations are not verifiable, the experimental evaluation is insufficient, and the presentation is poor. While the authors have acknowledged these issues and have promised to make improvements, the current state of the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Dataset Contribution**: The paper introduces a large-scale dataset of over one million slides, which is a significant contribution to the field. This dataset can be a valuable resource for future research in layout representation and graphic design intelligence.\n2. **Innovative Approach**: The use of a Transformer-based model for layout representation is novel and aligns with recent trends in deep learning, particularly in self-supervised pre-training. This approach has the potential to generalize well to various downstream tasks.\n3. **Downstream Tasks**: The paper proposes and evaluates the model on two novel tasksâ€”element role labeling and image captioningâ€”which are relevant to layout understanding and graphic design.\n4. **State-of-the-Art Performance**: The model achieves state-of-the-art performance on the proposed tasks, which is a strong indicator of its effectiveness.\n\n#### Weaknesses:\n1. **Evaluation**: The evaluation is somewhat limited and lacks thoroughness. The paper does not compare against several relevant baselines, including recent works like Cao et al. (SIGGRAPH 2019), Li et al. (2020b), and READ (CVPR 2020). This makes it difficult to assess the true novelty and effectiveness of the proposed method.\n2. **Dataset Details**: The paper lacks sufficient details about the dataset, such as the distribution of elements, templates, and the completeness of properties. This information is crucial for understanding the dataset's richness and the model's applicability.\n3. **Technical Depth**: The paper could benefit from more detailed explanations of the Transformer model and the attention mechanism, especially for readers who are not familiar with these concepts.\n4. **Higher Complexity Tasks**: The paper does not evaluate the model on more complex tasks that could better demonstrate its capabilities. For example, layout auto-completion and layout retrieval are simplified and do not align with previous work in these areas.\n5. **Clarity and References**: The \"Related Work\" section could be improved to better differentiate the paper from prior works. There are also some minor issues with references and writing clarity that need to be addressed.\n\n#### Author Response:\nThe authors have acknowledged the key issues and have committed to improving the evaluation, adding more baselines, and providing more dataset details. They also plan to refine the \"Related Work\" section and address the technical depth of the Transformer model. These improvements, if implemented, could significantly strengthen the paper.\n\n### Final Decision\nGiven the significant contributions of the dataset and the novel approach, along with the authors' commitment to addressing the weaknesses, the paper has the potential to be a valuable addition to the conference. However, the current version lacks the thorough evaluation and technical depth required for a top-tier conference.\n\nFinal Decision: **Reject** with the strong recommendation to resubmit after addressing the identified issues."
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Contribution\n- **Novelty**: The paper introduces a novel GNN model with a global self-attention mechanism defined using learnable spectral filters. This is a significant contribution because it addresses the limitation of existing GNNs, which often assume local homophily. The authors argue that their model can generalize well to both assortative and disassortative graphs, which is a valuable advancement in the field.\n- **Comparison with Existing Work**: The authors address the concerns raised by Reviewer 1 about the novelty of their approach compared to ChevNet and other methods. They provide a detailed explanation of how their model differs from ChevNet in terms of adaptability and the ability to handle heterophilic networks. The use of learnable spectral filters and multi-head attention is a key innovation that sets their work apart.\n\n#### Technical Soundness\n- **Complexity**: The authors have addressed the complexity concerns raised by Reviewer 1 and Reviewer 4. They explain that the attention mechanism is computed using Chebyshev polynomials, which reduces the complexity to \\(O(p \\times |E|)\\). They also provide empirical evidence of the model's efficiency through runtime comparisons with other baselines.\n- **Sparsification**: The authors have introduced two sparsification techniques to improve runtime efficiency and have provided experimental results to support their claims. This is a practical and important aspect of the model, especially for larger graphs.\n\n#### Empirical Evaluation\n- **Datasets**: The authors have expanded their experiments to include larger disassortative networks, such as Chameleon, and are working on additional datasets like Actor and Squirrel. This addresses the concern raised by Reviewer 3 about the limited size of the disassortative networks used in the initial experiments.\n- **Performance**: The proposed model outperforms state-of-the-art baselines on disassortative graphs and performs comparably on assortative graphs. This is a strong empirical validation of the model's effectiveness.\n\n#### Clarity and Presentation\n- **Exposition**: The paper is well-written and clearly explains the motivation, methodology, and experimental results. Reviewer 2 and Reviewer 3 both noted the clarity of the exposition, which is crucial for a top-tier conference.\n- **Figures and Tables**: The authors have added figures and tables to support their claims, including Figure 3 to demonstrate the impact of attention sparsification on runtime efficiency.\n\n#### Additional Considerations\n- **Theoretical Results**: While the paper does not provide theoretical results, the empirical evaluation is thorough and convincing. The lack of theoretical results is a minor concern given the strong empirical performance and the novelty of the approach.\n- **Future Work**: The authors have acknowledged the need to evaluate the model on other tasks such as graph reconstruction and link prediction, which is a good direction for future work.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper introduces a novel and effective GNN model that addresses a significant limitation in the field. The authors have addressed the concerns raised by the reviewers and provided strong empirical evidence to support their claims. The paper is well-written and clearly presented, making it a valuable contribution to a top-tier conference."
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n- **Fourier Convolutional Networks (FCNs):** The paper introduces the use of Fourier convolutional networks for 3D snapshot microscopy, which is a novel application of FCNs. While the concept of Fourier-domain convolutions is not entirely new (as noted by Reviewer 4 and 6), the specific application to 3D snapshot microscopy and the integration with differentiable wave optics simulation is a significant contribution.\n- **End-to-End Optimization:** The paper demonstrates the effectiveness of FCNs in end-to-end optimization of both the optical encoder and the deep learning decoder. This is a significant advancement over existing methods that struggle with highly non-local optical encodings.\n- **Comparison with UNet:** The paper shows that FCNs outperform UNets in both simulation and real-world lensless imaging tasks. This is a strong empirical validation of the proposed method.\n\n#### Empirical Novelty and Significance\n- **Simulation-Based Optimization:** The paper provides a thorough simulation-based evaluation of the proposed method, demonstrating its effectiveness in optimizing highly non-local PSFs for 3D snapshot microscopy. The use of a large, unique dataset of zebrafish images is a significant strength.\n- **Lensless Imaging:** The paper also evaluates the proposed method on a lensless imaging dataset, showing that FCNs outperform state-of-the-art methods. This demonstrates the general utility of the proposed architecture.\n\n#### Weaknesses and Concerns\n- **Real-World Validation:** While the paper provides a thorough simulation-based evaluation, it lacks real-world validation for the 3D snapshot microscopy application. This is a significant limitation, as the domain gap between simulation and reality can be substantial.\n- **Technical Details:** Some reviewers noted that the paper lacks important technical details, such as the implementation of conjugate symmetry in the Fourier domain and the memory requirements for the lensless imaging experiments. These details are crucial for reproducibility and understanding the fairness of the comparisons.\n- **Comparison with Rippel et al.:** The paper should provide a more detailed comparison with the work of Rippel et al. (2015) to clearly demonstrate the specific aspects of the proposed method that provide meaningful improvements.\n\n#### Strengths\n- **Well-Written and Clear:** The paper is well-written and easy to understand, with a thorough limitations section.\n- **Thorough Evaluation:** The paper provides a comprehensive evaluation of the proposed method, including both simulation and real-world lensless imaging experiments.\n- **Potential Impact:** The proposed method has the potential to significantly impact the field of computational imaging, particularly in the context of 3D snapshot microscopy and lensless imaging.\n\n### Final Decision\nDespite the limitations and concerns, the paper makes a significant contribution to the field of computational imaging. The introduction of Fourier convolutional networks for 3D snapshot microscopy and the thorough evaluation of the proposed method are strong points. The paper addresses a challenging problem and provides a novel solution that outperforms existing methods. The lack of real-world validation for 3D snapshot microscopy is a concern, but the simulation-based evaluation is thorough and the lensless imaging results are strong.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper introduces a novel approach to reinforcement learning (RL) by applying boosting techniques, specifically using a non-convex variant of the Frank-Wolfe method. This is a significant theoretical contribution, as it addresses the challenge of large state spaces in RL without explicitly depending on the number of states.\n2. **Algorithmic Innovation**: The proposed algorithm is designed to convert weak learners into effective policies, which is a valuable contribution to the field of RL. The use of the Frank-Wolfe method to handle the non-convexity of the value function is innovative.\n3. **Relevance**: The problem of handling large state spaces in RL is a critical issue, and the paper's approach is relevant to both the RL and optimization communities.\n\n#### Weaknesses:\n1. **Clarity and Readability**: Multiple reviewers have pointed out that the paper is dense, notation-heavy, and difficult to follow. This is a significant issue, as clarity is crucial for the paper's impact and readability. The authors have acknowledged this and promised to add a notation list and improve the clarity in the final version.\n2. **Empirical Validation**: The paper lacks empirical results, which would have strengthened the claims and provided practical insights. While the focus is on theoretical contributions, some initial experiments would have been beneficial.\n3. **Comparison with Prior Work**: The paper does not adequately discuss the differences between designing boosted contextual bandits and boosted RL, which is a relevant comparison. Additionally, it misses some important related work on state compression techniques.\n4. **Technical Novelty**: Some reviewers have noted that the technical novelty is limited, as the techniques are similar to those in Hazan and Singh 2021. The authors have addressed this by highlighting the critical difference in handling non-convex costs, but this needs to be more clearly articulated in the paper.\n5. **Practicality**: There are concerns about the practicality of the proposed algorithm, particularly regarding the computation of certain terms and the assumptions made. The authors have provided some clarifications, but these need to be more thoroughly addressed in the final version.\n\n### Final Decision\n\nDespite the significant theoretical contributions and the innovative use of boosting techniques in RL, the paper has several critical issues that need to be addressed. The clarity and readability of the paper are major concerns, and the lack of empirical validation weakens the overall impact. However, the authors have shown a willingness to improve the paper, and the theoretical contributions are valuable.\n\nGiven the importance of the problem and the potential impact of the proposed approach, I believe the paper should be given the opportunity to be revised and improved. Therefore, my final decision is:\n\nFinal Decision: Accept\n\nHowever, the authors must address the following points in the final version:\n1. Improve the clarity and readability of the paper, including adding a notation list and more intuitive explanations.\n2. Provide some initial empirical results to validate the effectiveness of the proposed algorithm.\n3. Clearly discuss the differences between designing boosted contextual bandits and boosted RL.\n4. Address the practicality concerns and provide more detailed explanations of the assumptions and computational steps."
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Technical Novelty and Significance\n- **Reviewer 1** and **Reviewer 3** both note that the contributions are only marginally significant or novel. They highlight that the proposed algorithm loses many of the key features that make target propagation (TP) attractive, such as the ability to avoid weight transport and the potential for decentralized optimization.\n- **Reviewer 2** and **Reviewer 4** provide a more balanced view, acknowledging the novelty of the regularized inversion approach and its potential for practical implementation. However, they also point out that the theoretical and empirical support for the algorithm is lacking.\n\n#### Empirical Novelty and Significance\n- **Reviewer 1** and **Reviewer 3** are critical of the empirical results, noting that the experiments are not convincing and that the performance on realistic datasets is weak.\n- **Reviewer 2** and **Reviewer 4** suggest that the empirical evidence is lacking in depth and breadth, with a need for more detailed and extensive experimentation to validate the claims.\n\n#### Correctness\n- **Reviewer 1** and **Reviewer 3** raise several issues with the correctness of the paper, including questions about the interpretation of certain equations and the relationship to existing work.\n- **Reviewer 2** and **Reviewer 4** generally find the claims to be well-supported, but they also point out areas where the theoretical analysis is incomplete or needs further development.\n\n#### Clarity and Structure\n- **Reviewer 2** and **Reviewer 4** praise the clarity and structure of the paper, noting that it is easy to read and well-organized.\n- **Reviewer 1** and **Reviewer 3** also find the paper clear but raise specific questions and concerns that need to be addressed.\n\n### Summary of Reviewer Feedback\n- **Strengths:**\n  - The proposed algorithm is novel and easily implementable in a differentiable programming framework.\n  - The paper is well-structured and clear.\n  - The algorithm has a natural interpretation and is computationally tractable.\n- **Weaknesses:**\n  - The theoretical guarantees are not strong enough.\n  - The empirical results are not convincing and lack depth.\n  - The algorithm loses key features that make TP attractive.\n  - There are issues with the correctness of some claims and interpretations.\n\n### Final Decision\nGiven the mixed feedback, the paper has some novel and interesting aspects but falls short in several critical areas. The theoretical and empirical support for the proposed algorithm is not strong enough to justify acceptance at a top-tier conference. The paper would benefit from more extensive experimentation, a deeper theoretical analysis, and addressing the specific concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions**:\n   - The paper introduces a novel PAC-Bayes bound that is non-vacuous and analytically tractable. This is a significant contribution, as non-vacuous bounds are rare in the context of deep learning.\n   - The authors provide an unbiased estimator for the NNGPâ€™s generalization error, which is a valuable tool for understanding the implicit biases of neural architectures.\n\n2. **Insightful Analysis**:\n   - The paper offers a detailed analysis of the implicit biases of both the architecture and gradient descent. The authors show that large margin functions generalize better, both in the context of NNGP and finite-width neural networks trained by gradient descent.\n   - The connection between large margin functions and the concentration of the GP posterior on its mean is a novel and insightful observation.\n\n3. **Clarity and Presentation**:\n   - The paper is well-written and clearly structured, making it easy to follow the theoretical developments and experimental results.\n   - The authors provide intuitive explanations and visualizations (e.g., Figure 3) that help in understanding the complex theoretical concepts.\n\n#### Weaknesses of the Paper\n\n1. **Experimental Scope**:\n   - The experimental validation is limited to MLPs and a specific optimizer (Nero). While the authors argue that similar results hold for other optimizers like vanilla GD and Adam, more extensive experiments with different architectures (e.g., CNNs, ResNets) and datasets would strengthen the paper.\n   - The use of Nero, a relatively new optimizer, raises questions about the generalizability of the results. The authors should provide more ablation studies to isolate the effects of the optimizer.\n\n2. **Margin Analysis**:\n   - The analysis of the implicit bias of gradient descent with respect to margin is somewhat limited. The authors claim that gradient descent with certain loss functions targets specific margins, but this claim is not rigorously supported by theoretical analysis.\n   - The experimental evidence for the margin claims is not as strong as it could be. More detailed experiments and comparisons with other methods (e.g., NTK) would be beneficial.\n\n3. **Comparison with Prior Work**:\n   - The paper does not adequately compare its results with prior work, particularly in the context of NNGP and NTK. The authors should provide a more comprehensive discussion of how their findings relate to existing literature.\n   - The bounds derived in the paper are not compared to those from previous works (e.g., Valle-PÃ©rez et al. (2019)), which could help in understanding the significance of the new bounds.\n\n4. **Practical Significance**:\n   - The bounds and estimators derived in the paper are not as tight as they could be, and there is a significant gap between the theoretical bounds and empirical performance. While the authors argue that this gap is due to the large margin effect, more evidence is needed to support this claim.\n   - The practical significance of the bounds is limited, as they are not directly applicable to real-world datasets and architectures.\n\n### Final Decision\n\nDespite the significant theoretical contributions and insightful analysis, the paper has several weaknesses that need to be addressed. The experimental scope is limited, and the margin analysis is not as rigorous as it could be. Additionally, the paper could benefit from a more comprehensive comparison with prior work and a more detailed discussion of the practical significance of the results.\n\nGiven these considerations, the paper is not yet ready for publication in a top-tier conference. However, the authors have shown a strong foundation and a promising direction for future work. With additional experiments, a more thorough comparison with prior work, and a clearer discussion of the practical implications, the paper could be a strong submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Theoretical Contributions:**\n   - The paper provides a rigorous theoretical analysis of the Feedback Alignment (FA) algorithm for deep linear networks. It offers convergence guarantees for both continuous and discrete dynamics, which is a significant contribution to the field.\n   - The authors introduce the concept of implicit anti-regularization, which is a novel and interesting phenomenon. This adds to the understanding of the dynamics of FA and could have broader implications for other learning algorithms.\n\n2. **Clarity and Structure:**\n   - The paper is well-structured and clearly written. The authors are transparent about their assumptions and provide detailed proofs.\n   - The numerical experiments support the theoretical findings and are well-presented.\n\n3. **Relevance:**\n   - The study of linear networks is a crucial starting point for understanding more complex non-linear networks. The paper builds on a solid foundation of existing work in linear networks and provides valuable insights into the FA algorithm.\n\n#### Weaknesses of the Paper\n\n1. **Assumptions and Generalizability:**\n   - The analysis is heavily dependent on the assumption that the data matrices, FA matrices, and initial weight matrices can be diagonalized simultaneously. This assumption may oversimplify the problem and limit the generalizability of the results.\n   - The convergence results are primarily derived under specific initializations, which may not reflect common practices in practical applications. The authors acknowledge this limitation and provide some experiments with random initialization, but more extensive empirical validation would strengthen the paper.\n\n2. **Comparison with Gradient Descent (GD):**\n   - While the paper provides a theoretical analysis of FA, it does not directly compare the performance of FA with GD in a comprehensive manner. The authors argue that the gap between diagonal dynamics and high-dimensional matrix dynamics is non-trivial, but this comparison is crucial for understanding the practical advantages of FA.\n\n3. **Empirical Evidence:**\n   - The numerical experiments are limited to simple settings and do not fully explore the behavior of FA under various initialization schemes and network architectures. More extensive empirical validation, especially with non-spectral initialization, would provide stronger support for the theoretical findings.\n\n#### Author Responses\n\n- The authors have addressed several of the reviewers' concerns, particularly regarding the clarity of the assumptions and the inclusion of visual tools to better illustrate the dynamics.\n- They have also provided additional experiments with non-spectral initialization, which helps to address the concern about the practical relevance of the results.\n- However, the authors have not fully addressed the need for a more comprehensive comparison with GD and the generalizability of the results to non-linear networks.\n\n### Final Decision\n\nDespite the limitations, the paper makes significant theoretical contributions to the understanding of the FA algorithm. The introduction of implicit anti-regularization and the detailed analysis of convergence are novel and valuable. The paper is well-written and structured, and the authors have made efforts to address the reviewers' concerns.\n\nHowever, the assumptions and the limited empirical validation raise concerns about the generalizability of the results. The paper would benefit from more extensive empirical validation and a more comprehensive comparison with GD.\n\nGiven the significance of the theoretical contributions and the clarity of the presentation, I believe the paper should be accepted, but with the recommendation for the authors to address the remaining concerns in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Technical Novelty**: The paper introduces the Believer-Penalizer (BP) strategy for asynchronous multi-objective Bayesian optimization (MOBO). While the idea is straightforward, it addresses a practical issue in MOBO and is the first to empirically evaluate this approach. However, the novelty is somewhat limited, as it builds on existing techniques (KB and LP) and does not provide a strong theoretical foundation.\n- **Scientific Contribution**: The paper's primary scientific contribution is the introduction and empirical evaluation of the BP strategy. However, the claims about its performance are somewhat overstated, and the empirical results show mixed performance. The paper also lacks a thorough theoretical analysis of the BP strategy.\n- **Engineering Contribution**: The platform itself is well-engineered and user-friendly, with a modular design that allows for easy integration and customization. The graphical user interface (GUI) is a significant advantage for non-expert users. The platform supports a wide range of optimization scenarios, including single-objective, multi-objective, and many-objective optimization.\n\n#### **2. Empirical Evaluation**\n- **Benchmarking**: The empirical results are mixed. While the paper claims that BP outperforms other methods, the results in Figure 5 show that BP is not consistently the best. The authors have updated the paper to reflect a more balanced and fair statement, but the empirical significance is still limited.\n- **Real-World Application**: The paper demonstrates the platform's ability to control and guide real-world hardware experiments, which is a valuable contribution. However, the real-world example is more of a proof of concept and lacks a comprehensive evaluation.\n\n#### **3. Clarity and Presentation**\n- **Writing and Structure**: The paper is well-written and well-structured, making it easy to follow. The authors have addressed many of the minor issues pointed out by the reviewers, such as typos and redundant specifications.\n- **Documentation and User-Friendliness**: The authors have provided a video introduction and download links for executable files, which enhances the user experience. The GUI and modular design make the platform accessible to non-expert users.\n\n#### **4. Addressing Reviewer Concerns**\n- **Reviewer 1**: The reviewer is still not convinced by the overall performance of AutoOED and the novelty of the BP strategy. The author's response addresses some of these concerns but does not fully resolve them.\n- **Reviewer 2**: The reviewer is concerned about the limited methodological contribution and the misleading claims about the number of objectives. The authors have addressed these concerns by updating the paper to support any number of objectives.\n- **Reviewer 3**: The reviewer is concerned about the lack of theoretical proof for the BP strategy and the cherry-picking of results. The authors have updated the paper to reflect a more balanced and fair statement.\n- **Reviewer 4**: The reviewer is concerned about the overstatement of the empirical results and the lack of comparison with other platforms. The authors have addressed these concerns by updating the paper with more comparisons.\n- **Reviewer 5**: The reviewer is generally positive about the paper but points out minor issues. The authors have addressed these issues in the updated version.\n\n### Final Decision\nWhile the paper has significant engineering contributions and a well-designed platform, the scientific novelty and empirical significance are somewhat limited. The claims about the performance of the BP strategy are overstated, and the empirical results show mixed performance. However, the platform's user-friendliness and modular design are valuable contributions that can benefit a wide range of users.\n\nFinal Decision: **Reject**\n\nThe paper would benefit from further theoretical analysis and more robust empirical evaluation to strengthen its scientific contributions. Additionally, addressing the concerns about the performance claims and the limitations of the BP strategy would make the paper more suitable for a top-tier conference."
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Relevance and Interest**: The paper addresses a relevant and interesting problem in multi-modal deep learning, specifically the issue of greedy learning where models tend to rely on one modality while under-utilizing others. This is a significant issue in the field, and the topic is worthy of exploration.\n2. **Empirical Validation**: The authors provide empirical evidence to support their hypothesis through experiments on multiple datasets (Colored MNIST, Princeton ModelNet40, and NVIDIA Dynamic Hand Gesture Dataset). The experiments are well-designed and provide insights into the greedy learning behavior.\n3. **Proposed Method**: The proposed balanced multi-modal learning algorithm is a novel approach to address the greedy learning issue. The method is straightforward and easy to understand, which is a strength in terms of practical applicability.\n4. **Clarity and Readability**: The paper is generally well-written and easy to follow, which is important for a top-tier conference.\n\n#### Weaknesses of the Paper\n1. **Lack of Theoretical Support**: The paper primarily relies on empirical validation and lacks a strong theoretical foundation to support the greedy learner hypothesis. While empirical results are valuable, a more rigorous theoretical analysis would strengthen the paper.\n2. **Inconsistent Results**: Some reviewers noted inconsistencies in the experimental results, particularly in Table 1, where the performance differences between the vanilla and guided methods are not statistically significant in some datasets (ModelNet40, NVGesture-scratch, and NVGesture-pretrained). This raises questions about the robustness of the proposed method.\n3. **Limited Comparison with State-of-the-Art Methods**: The paper does not compare the proposed method with other state-of-the-art methods that address similar issues, such as RUBi and other techniques for balancing modalities. This limits the paper's impact and makes it difficult to assess the novelty and effectiveness of the proposed method.\n4. **Focus on Visual Modalities**: The experiments are primarily conducted on datasets with visual modalities, and the paper does not include experiments with more natural modalities like audio-visual datasets. This limits the generalizability of the findings.\n5. **Hyperparameter Sensitivity**: The authors did not provide a thorough analysis of the sensitivity of the proposed method to hyperparameters, particularly the imbalance tolerance parameter \\(\\alpha\\). This is crucial for understanding the robustness and practical applicability of the method.\n6. **Clarity of Re-balancing Step**: The re-balancing step is not clearly explained in the main text, and some reviewers found it confusing. The authors should provide a more detailed and clear explanation of this step in the main part of the paper.\n\n#### Author Responses\nThe authors have addressed some of the reviewers' concerns by:\n- Providing additional experiments and comparisons with RUBi.\n- Conducting sensitivity analysis for hyperparameters.\n- Augmenting the related work section with additional references.\n- Fixing typos and improving the clarity of the paper.\n\nHowever, the responses do not fully address all the weaknesses, particularly the lack of theoretical support and the limited comparison with state-of-the-art methods.\n\n### Final Decision\nGiven the strengths of the paper in addressing a relevant problem and providing empirical validation, but considering the significant weaknesses in theoretical support, inconsistent results, and limited comparison with state-of-the-art methods, the paper does not meet the high standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Frequency-Based Understanding of Adversarial Examples:**\n   - The paper provides a novel and comprehensive analysis of adversarial examples from a frequency perspective. It challenges the common misconception that adversarial examples are high-frequency noise and shows that they are dataset-dependent.\n   - The use of noise gradients and extensive empirical results across different datasets (CIFAR-10, TinyImageNet, ImageNet) and architectures (ResNet18, VGG16, DenseNet-121, ViT) supports the claims.\n\n2. **Insights for Adversarial Training:**\n   - The paper introduces a frequency-based approach to adversarial training, which can help control the trade-off between clean accuracy and robustness. This is a significant contribution as it provides a new tool for the community to better understand and manage this trade-off.\n\n3. **Clarity and Presentation:**\n   - The paper is well-written and easy to follow, with clear explanations and extensive supplementary material. The experimental design is well-thought-out, and the results are presented in a structured manner.\n\n4. **Community Impact:**\n   - The insights provided in the paper are valuable for the adversarial machine learning community. The frequency-based perspective offers new dimensions for thinking about adversarial examples and can inform future research.\n\n#### Weaknesses of the Paper\n\n1. **Overlap with Prior Work:**\n   - The paper's main contribution, that adversarial examples are dataset-dependent, has been partially explored in prior works. However, the authors have addressed this by highlighting the differences and providing additional evidence and insights.\n   - The authors have also conducted experiments with other attacks (Auto-Attack) and datasets (MNIST, Fashion-MNIST, CIFAR-100) to strengthen their claims.\n\n2. **Clarity of Contributions:**\n   - Some reviewers noted that the contributions were not clearly highlighted in the introduction. The authors have addressed this in their response by providing a clearer enumeration of their contributions.\n\n3. **Scope and Limitations:**\n   - The paper primarily focuses on natural images and standard datasets. While the authors have acknowledged this limitation and suggested further experiments on other types of images (e.g., radar signals), this is a valid point that should be addressed in future work.\n   - The authors have also discussed the limitations of their approach, such as the choice of network architectures and optimization properties, which adds to the paper's credibility.\n\n4. **Minor Issues:**\n   - There are minor grammatical errors and issues with figure placement, which the authors have committed to addressing.\n\n### Final Decision\n\nDespite the overlap with prior work and some minor issues, the paper provides significant and novel insights into the frequency properties of adversarial examples. The comprehensive analysis, extensive experiments, and practical implications for adversarial training make it a valuable contribution to the field. The authors have also addressed the concerns raised by the reviewers in their responses, further strengthening the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Theoretical Contributions**:\n   - The paper provides a novel and comprehensive analysis of the optimization landscape for two-layer ReLU neural networks with L2 regularization.\n   - It extends the work of Pilanci and Ergen (2020) by characterizing all global optima via a convex optimization program, which is a significant theoretical advance.\n   - The paper introduces the concepts of minimal and nearly minimal neural networks, which provide a clear structure for understanding the set of optimal solutions.\n   - It provides a polynomial-time algorithm for checking if a neural network is a global minimum, which is a practical and important contribution.\n\n2. **Clarity and Presentation**:\n   - The paper is well-written and organized, making the technical content accessible and easy to follow.\n   - The authors provide clear and detailed proofs, which are essential for a theoretical paper of this nature.\n\n3. **Relevance and Impact**:\n   - The results have the potential to influence the understanding of the optimization landscape in neural networks, which is a fundamental problem in deep learning.\n   - The convex perspective on non-convex problems can inspire further research and practical applications.\n\n#### Weaknesses of the Paper\n1. **Scope**:\n   - The analysis is limited to two-layer ReLU networks, which is a significant limitation. While the authors acknowledge this and discuss potential extensions, the current scope is narrow.\n   - The extension to multiple layers and other activation functions is not fully explored, which is a common criticism from the reviewers.\n\n2. **Technical Issues**:\n   - There are some minor technical issues and typos that need to be addressed, as pointed out by the reviewers.\n   - The positive homogeneity of the ReLU activation function is a key assumption, and the authors need to clarify how this affects the generalizability of their results to other activation functions.\n\n3. **Empirical Validation**:\n   - The paper lacks empirical validation, which would strengthen the practical relevance of the theoretical findings. While the theoretical contributions are strong, empirical results could provide additional insights and support.\n\n#### Reviewer Feedback\n- **Reviewer 1** and **Reviewer 3** both highlight the significance of the theoretical contributions but point out the limitations in scope and some technical issues.\n- **Reviewer 2** is highly positive, emphasizing the novel and elegant approach and the potential for further research.\n- **Reviewer 4** also provides a strong positive review, noting the completeness and clarity of the theoretical analysis.\n\n### Final Decision\nDespite the limitations in scope and some minor technical issues, the paper makes significant theoretical contributions that are well-supported and clearly presented. The novel insights and the potential for future research make it a valuable addition to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Problem Relevance and Importance**:\n   - The paper addresses a significant issue in diffusion models: the high computational cost of generating high-quality samples. This is a critical problem in the field of generative models, and any improvement in this area can have a substantial impact.\n\n2. **Technical Contributions**:\n   - **Generalized Gaussian Diffusion Models (GGDM)**: The introduction of a non-Markovian family of samplers is a novel contribution. This family of models is more flexible and can potentially lead to better sample quality with fewer steps.\n   - **Differentiable Diffusion Sampler Search (DDSS)**: The method of optimizing the parameters of the sampler using perceptual losses is innovative. The use of gradient rematerialization to handle the computational challenges is a practical solution.\n\n3. **Empirical Results**:\n   - The paper demonstrates strong empirical results on several datasets, including CIFAR-10 and ImageNet 64x64. The FID scores are significantly improved, especially in the few-step regime, which is a key contribution.\n\n4. **Theoretical Grounding**:\n   - The authors have addressed the concerns about the theoretical grounding of their perceptual loss by connecting it to the Kernel Inception Distance (KID). This provides a clearer theoretical justification for their approach.\n\n5. **Clarity and Presentation**:\n   - The revised version of the paper has addressed many of the clarity issues raised by the reviewers. The paper is now more accessible and easier to understand, with better explanations and more detailed derivations.\n\n#### Weaknesses of the Paper\n\n1. **Theoretical Guarantees**:\n   - While the authors have provided a clearer connection to KID, there is still a lack of strong theoretical guarantees about the distribution that the samples are optimized to match. This is a valid concern, but the empirical results are strong enough to mitigate this issue.\n\n2. **Scalability**:\n   - The computational cost of the DDSS optimization is high, especially for larger datasets. However, the authors argue that the gains in sample quality justify this cost, and the method is still practical for smaller datasets.\n\n3. **Generalization to Larger Datasets**:\n   - The paper primarily focuses on smaller datasets like CIFAR-10 and ImageNet 64x64. While the authors promise to include results on larger datasets in the camera-ready version, the current results are limited in scope.\n\n4. **Comparison to Baselines**:\n   - The paper could benefit from more extensive comparisons to other baselines, especially on larger datasets. However, the current comparisons are sufficient to demonstrate the effectiveness of the proposed method.\n\n#### Reviewer Feedback\n\n- **Reviewer 1 and 2**: Both reviewers have raised concerns about the theoretical grounding and the motivation of the perceptual loss. However, they have been satisfied with the revisions and have raised their scores.\n- **Reviewer 3**: This reviewer found the paper interesting and the results compelling, despite some concerns about the practicality of the method.\n- **Reviewer 4**: This reviewer has concerns about the theoretical guarantees and the generalization to larger datasets. However, the reviewer acknowledges the empirical improvements.\n- **Reviewer 5**: This reviewer has concerns about clarity and the need for more detailed explanations. The reviewer is satisfied with the revisions but suggests further improvements.\n\n### Conclusion\n\nThe paper addresses a significant problem in diffusion models and provides a novel and effective solution. The empirical results are strong, and the authors have addressed the major concerns raised by the reviewers. While there are still some minor issues, the overall quality and impact of the paper are high.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Theoretical Insights**: The paper provides a clear and mathematically rigorous framework for understanding how weighted empirical risk minimization (WERM) can be used to penalize higher-order moments of the loss distribution. Theorems 1 and 3, along with Lemmas 2 and 4, offer valuable insights into the relationship between the weighted mean and the moments of the loss distribution.\n2. **Empirical Validation**: The paper includes experimental results that demonstrate the effectiveness of the proposed weighted mean trick in improving robustness against outliers. The experiments are well-designed and provide a strong empirical basis for the theoretical claims.\n3. **Clarity and Structure**: The revised version of the paper is more structured and clear, with a new section on notation and updated theoretical results. The authors have addressed many of the minor comments and have made the paper more accessible to readers.\n\n#### Weaknesses of the Paper\n1. **Algorithmic Concerns**: Reviewer 2 and Reviewer 5 raised significant concerns about the algorithm's implementation, specifically that the proposed algorithm does not perform a full gradient descent on the weighted loss function. This is a critical issue that needs to be addressed to ensure the algorithm is correctly minimizing the desired objective.\n2. **Theoretical Limitations**: Reviewer 4 and Reviewer 6 pointed out several issues with the theoretical results, including the assumption that the first moment of the loss exists, the interpretation of Lemma 2, and the behavior of the weights when they are clipped to zero. These issues need to be clarified and potentially addressed in the theoretical framework.\n3. **Practical Applicability**: The paper does not provide a clear method for choosing the hyperparameters $\\lambda_i$, which is crucial for practical implementation. The flexibility of the framework, while a strength, also introduces a significant challenge in determining the appropriate values for these parameters.\n4. **Comparison with Existing Work**: The paper could benefit from a more detailed comparison with existing robust loss functions and methods, such as those mentioned by Reviewer 4 and Reviewer 7. This would help to better position the proposed method in the context of existing literature.\n\n#### Addressing Reviewer Concerns\n- **Algorithmic Issues**: The authors should provide a more detailed explanation of the algorithm and address the concerns raised by Reviewer 2 and Reviewer 5. This could involve a more rigorous analysis of the gradient updates and a discussion of the conditions under which the algorithm converges.\n- **Theoretical Clarity**: The authors should clarify the assumptions and conditions under which their theoretical results hold. This includes addressing the issues raised by Reviewer 4 and Reviewer 6, particularly regarding the behavior of the weights and the convexity of the objective function.\n- **Practical Guidance**: The paper should include more practical guidance on how to choose the hyperparameters $\\lambda_i$. This could involve empirical studies or heuristics for selecting these parameters in different scenarios.\n- **Comparative Analysis**: The authors should provide a more detailed comparison with existing robust loss functions and methods. This could involve additional experiments or theoretical analysis to demonstrate the advantages and limitations of the proposed method.\n\n### Final Decision\nGiven the significant theoretical and algorithmic concerns raised by the reviewers, and the need for further clarification and improvement in the paper, I recommend that the paper be **rejected** for publication at this top-tier conference. However, the authors are encouraged to address the identified issues and resubmit the paper for consideration in a future conference or journal.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Novel Insight**: The paper identifies a novel phenomenon, namely the resonance-driven divergence of SGD with momentum under non-i.i.d. sampling. This insight is significant because it highlights a potential failure mode of a widely used optimization algorithm in settings that are common in practical applications like continual learning and reinforcement learning.\n\n2. **Theoretical Foundation**: The paper provides a rigorous theoretical foundation for understanding the behavior of SGD with momentum under covariate shift. The use of parametric oscillators and Floquet theory to analyze the system is a novel and elegant approach.\n\n3. **Empirical Validation**: The empirical results are comprehensive and well-designed. The authors start with a simple linear setting and progressively relax assumptions to show that the resonance phenomenon persists even in more complex settings, including non-linear models and different optimizers.\n\n4. **Clarity and Structure**: The paper is well-written and structured. The \"phenomenon â†’ hypothesis â†’ analysis\" format is clear and logical, making the paper easy to follow.\n\n#### Weaknesses of the Paper\n1. **Technical Novelty**: Some reviewers have raised concerns about the technical novelty of the paper. While the application of Floquet theory and parametric oscillators to the analysis of SGD with momentum is novel, the proof techniques themselves are not entirely new. The authors have acknowledged this and are willing to soften their claims of technical novelty.\n\n2. **Presentation of Theoretical Results**: The presentation of the theoretical results, particularly Theorem 1, could be improved. The connection between the mathematical objects and the resonance behavior is not immediately clear, and more intuitive explanations would enhance the paper.\n\n3. **Relevance to Stochastic Gradients**: The analysis is based on expected gradients, which some reviewers find questionable. The authors have addressed this by providing additional experiments with fully stochastic settings, showing that the resonance phenomenon persists. However, this point remains a concern for some reviewers.\n\n4. **Practical Relevance**: The paper focuses on a synthetic setting with periodic covariate shift, which may not be common in real-world datasets. The authors acknowledge this and are planning future work to explore more realistic settings, including naturally oscillatory RL environments.\n\n5. **Convergence Analysis**: The paper is unable to prove convergence of SGD with momentum when the continuous dynamics converge, which is a limitation. The authors suggest that this is due to the difficulty of analyzing the discrete dynamics, but this remains a gap in the theoretical analysis.\n\n#### Addressing Reviewer Concerns\n- **Technical Novelty**: The authors have acknowledged the concerns and are willing to soften their claims. The novelty lies in the application of existing mathematical tools to a new domain, which is valuable.\n- **Presentation**: The authors have provided a more intuitive explanation of the theoretical results and are willing to include this in the paper.\n- **Stochastic Gradients**: The authors have provided additional experiments with fully stochastic settings, which align well with the theoretical predictions.\n- **Practical Relevance**: The authors have acknowledged the limitations and are planning future work to explore more realistic settings.\n\n### Final Decision\nGiven the novel insight, rigorous theoretical foundation, and comprehensive empirical validation, the paper makes a significant contribution to the understanding of SGD with momentum under non-i.i.d. sampling. While there are some valid concerns, the authors have addressed them adequately, and the overall quality of the paper is strong.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Well-Written and Clear**: The paper is well-written and clearly conveys its main points and contributions. This is a significant strength as it ensures that the ideas are accessible to the readers.\n2. **Interesting Approach**: The use of VAEs for time series generation, particularly with the inclusion of interpretable components, is an interesting and novel approach. This could be valuable in applications where interpretability is crucial.\n3. **Empirical Evaluation**: The authors provide a thorough empirical evaluation on four datasets, comparing their method to several state-of-the-art data generation methods. This helps to validate the effectiveness of the proposed method.\n\n#### Weaknesses of the Paper\n1. **Lack of Hyperparameter Tuning and Model Selection**: The introduced decoder building blocks introduce new hyperparameters, but the paper lacks details on how to select these hyperparameters and perform model selection. This is a significant gap in the methodology.\n2. **Limited Interpretability Evaluation**: While interpretability is claimed as a key contribution, the paper does not provide any experiments or ablation studies to demonstrate the interpretability of the proposed building blocks. This weakens the claim and the overall contribution.\n3. **Comparison to Related Work**: The paper does not adequately compare its method to other relevant works, such as GP-VAE and probabilistic autoregressive models. This makes it difficult to assess the novelty and significance of the proposed method.\n4. **Experimental Methodology**: The experiments are conducted on synthetic or novel datasets, which makes it challenging to compare the results with other works in the literature. Additionally, the next-step prediction evaluation involves post-hoc learning of an LSTM model, which is not a standard approach.\n5. **Technical Clarity**: The paper contains some imprecise or non-standard explanations of VAEs and generative models. For example, the categorization of generative models and the explanation of the VAE loss function could be improved for clarity and precision.\n\n#### Reviewer Comments\n- **Reviewer 1**: Points out the lack of hyperparameter tuning and model selection, and the absence of interpretability evaluation. Suggests the paper is not ready for publication at ICLR.\n- **Reviewer 2**: Questions the originality of the proposed method and the lack of discussion on the VAE loss function. Suggests the paper lacks substantial originality.\n- **Reviewer 3**: Criticizes the lack of general context in the literature review, imprecise technical explanations, and the experimental methodology. Suggests major revisions are needed.\n- **Reviewer 4**: Questions the practical significance of the proposed method and the lack of interpretability evaluation. Suggests the paper does not achieve significant performance compared to other methods.\n\n### Conclusion\nThe paper presents an interesting and novel approach to generating multivariate time series data using VAEs with interpretable components. However, the paper has several significant weaknesses that need to be addressed before it can be considered for publication at a top-tier conference. These include the lack of hyperparameter tuning and model selection, limited evaluation of interpretability, inadequate comparison to related work, and issues with the experimental methodology.\n\nGiven these weaknesses, the paper is not currently ready for publication at ICLR. The authors should address the reviewers' comments and make substantial revisions to strengthen the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Problem Relevance**: The paper addresses a significant issue in diffusion models, which is the high computational cost associated with generating high-quality samples. This is a relevant and important problem in the field of generative models.\n\n2. **Novelty and Technical Contribution**:\n   - **Generalized Gaussian Diffusion Models (GGDM)**: The introduction of a more flexible family of samplers that can use multiple previous steps for generation is a novel contribution. This flexibility allows for more efficient sampling.\n   - **Differentiable Diffusion Sampler Search (DDSS)**: The optimization procedure for finding the parameters of the sampler is innovative. The use of perceptual losses and gradient rematerialization to optimize the sampler is a significant technical contribution.\n\n3. **Empirical Results**:\n   - The paper demonstrates strong empirical results on image generation tasks, particularly on CIFAR-10 and ImageNet 64x64. The FID scores are significantly improved with fewer inference steps compared to existing methods like DDPM and DDIM.\n   - The results on LSUN church 128x128 are also impressive, showing a substantial improvement in FID scores with very few steps.\n\n4. **Theoretical Grounding**:\n   - The authors have addressed the concerns about the theoretical grounding of their perceptual loss by aligning it with the Kernel Inception Distance (KID). This provides a clear and theoretically sound basis for their optimization objective.\n   - The inclusion of a formal proof for Theorem 1 in the appendix strengthens the theoretical foundation of the paper.\n\n5. **Clarity and Presentation**:\n   - The authors have made significant improvements in the clarity of the paper, addressing the feedback from reviewers. The revised version is more accessible and easier to understand.\n\n#### Weaknesses of the Paper\n\n1. **Clarity and Motivation**:\n   - While the authors have made improvements, some reviewers still find the motivation and clarity of the paper lacking. The connection between the perceptual loss and the theoretical guarantees could be further clarified.\n   - The paper could benefit from a more detailed discussion on the practical implications of using perceptual losses instead of optimizing the ELBO.\n\n2. **Experimental Scope**:\n   - The paper primarily focuses on smaller datasets like CIFAR-10 and ImageNet 64x64. While the results are compelling, the inclusion of experiments on larger, high-resolution datasets like LSUN 256x256 or CelebA would strengthen the empirical validation.\n   - The authors have promised to include results on another challenging dataset in the camera-ready version, which is a positive step.\n\n3. **Theoretical Insights**:\n   - Some reviewers are concerned about the theoretical insights of the proposed method. While the KID-based perceptual loss provides a clear objective, the paper could benefit from a more detailed analysis of the statistical properties of the GGDM and the implications of optimizing for perceptual losses.\n\n4. **Practicality**:\n   - The computational cost of the DDSS optimization procedure is a concern. While the authors argue that the gains in sample quality justify the computational cost, a more detailed discussion on the practicality and scalability of the method would be beneficial.\n\n### Final Decision\n\nDespite the remaining concerns, the paper makes a significant contribution to the field of diffusion models. The introduction of GGDM and DDSS, along with the strong empirical results, addresses a critical problem in generative models. The authors have made substantial improvements in addressing the reviewers' concerns, particularly in terms of theoretical grounding and clarity.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Technical Novelty and Significance\n- **Novelty**: The paper introduces a novel approach to reweighting samples in adversarial training using a bilevel optimization framework and a meta-learning approach. While the use of MAML for reweighting is not entirely novel, the specific application to adversarial training and the introduction of the multi-class margin as an input to the reweighting network is a significant contribution.\n- **Significance**: The method shows consistent improvements in both clean and robust accuracy compared to existing baselines. However, the improvements are marginal in some cases, especially when compared to state-of-the-art methods that use larger architectures or additional data.\n\n#### Empirical Evaluation\n- **Strengths**: The paper provides a thorough evaluation on multiple datasets (MNIST, FMNIST, CIFAR-10, and CIFAR-100) and against various attacks (PGD, AutoAttack). The results are generally positive, showing improvements over baselines.\n- **Weaknesses**: \n  - **Adaptive Attacks**: The paper does not evaluate the method against adaptive attacks, which is a significant concern. The authors argue that this is out of scope, but the reviewers' points are valid. The reweighting network, if known to the attacker, could be exploited, and this should be addressed.\n  - **State-of-the-Art Comparison**: The paper claims to outperform state-of-the-art baselines, but the comparison is not comprehensive. The authors should clarify their position relative to methods that use larger architectures or additional data, such as AWP.\n  - **Ablation Studies**: While the paper includes some ablation studies, they are not as extensive as some reviewers suggest. More detailed ablation studies, especially on the input encoding to the reweighting network, would strengthen the paper.\n\n#### Theoretical Justification\n- **Strengths**: The paper provides a clear motivation for using the multi-class margin and the bilevel optimization framework. The theoretical background is well-explained, and the method is sound.\n- **Weaknesses**: The paper lacks a deeper theoretical analysis of why the multi-class margin is superior to other inputs. The authors should provide more justification for their choices and explore alternative inputs in the ablation studies.\n\n#### Clarity and Writing\n- **Strengths**: The paper is well-written and easy to follow. The methodology is clearly explained, and the experiments are well-documented.\n- **Weaknesses**: The paper could benefit from more detailed discussions and ablation studies. The authors should also address the minor typos and notational inconsistencies pointed out by the reviewers.\n\n#### Additional Considerations\n- **Computational Cost**: The authors provide a reasonable explanation of the computational overhead, but more detailed comparisons with other methods would be beneficial.\n- **Generalization**: The authors have added a new experiment to explore the generalization of the weighting network, which is a positive step. However, more extensive experiments on different datasets and architectures would further validate the method.\n\n### Final Decision\nGiven the strengths and weaknesses of the paper, the marginal improvements over state-of-the-art methods, and the lack of evaluation against adaptive attacks, the paper does not meet the high standards of a top-tier conference. However, the method is novel and shows promising results, and the authors have addressed many of the reviewers' concerns in their response.\n\nFinal Decision: Reject\n\nThe authors should consider addressing the major concerns, particularly the evaluation against adaptive attacks and a more comprehensive comparison with state-of-the-art methods, before resubmitting to a future venue."
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Effective Methods:**\n   - The proposed Computation Redistribution (CR) and Sample Redistribution (SR) methods are shown to be effective in improving face detection accuracy, especially for small faces.\n   - The ablation studies and comparative experiments on the WIDER FACE dataset demonstrate significant improvements over the baseline and state-of-the-art methods.\n\n2. **State-of-the-Art Performance:**\n   - The SCRFD-34GF model outperforms the best competitor, TinaFace, by 4.78% in AP on the hard set while being more than 3 times faster on GPUs with VGA-resolution images.\n   - The method also achieves state-of-the-art performance on other datasets (AFW, Pascal, FDDB) with lower computational costs.\n\n3. **Practicality:**\n   - The methods are practical and can be applied to real-world applications, such as face detection on mobile devices.\n   - The proposed approach is general and can be adapted to other object detection and segmentation tasks.\n\n4. **Detailed Experiments:**\n   - The paper includes extensive experiments and ablation studies, providing a thorough evaluation of the proposed methods.\n   - The authors have addressed the reviewers' comments and added necessary details, such as a figure illustrating the framework and an algorithm describing the computation redistribution method.\n\n#### **Weaknesses:**\n1. **Search Strategy:**\n   - The search strategies (CR) are described as straightforward and not particularly novel. However, the authors argue that the effectiveness of the method lies in the empirical bootstrap and the elimination of low-quality architectures from the search space.\n   - The paper could benefit from a more detailed comparison with other network search methods, such as evolutionary approaches.\n\n2. **Limited Scope:**\n   - The methods are primarily designed for detecting small faces in low-resolution images, which might limit their applicability to other object detection tasks.\n   - The paper could provide more insights into how the methods can be generalized to other object detection scenarios.\n\n3. **Clarity and Detail:**\n   - Some aspects of the paper, such as the sample distribution and the binary probability model, could be explained more clearly.\n   - The authors have addressed these issues in their response, but the main paper could benefit from additional details and figures to enhance clarity.\n\n#### **Reviewer Feedback:**\n- **Positive Feedback:**\n  - Reviewers generally agree that the proposed methods are effective and achieve state-of-the-art performance.\n  - The authors have addressed most of the reviewers' comments and provided additional details to improve the paper.\n\n- **Constructive Criticism:**\n  - Some reviewers suggest that the search strategies are straightforward and not highly novel.\n  - There is a need for more detailed comparisons with other network search methods and a clearer explanation of the sample distribution and binary probability model.\n\n### Final Decision\nDespite the minor weaknesses, the paper presents a significant contribution to the field of face detection. The proposed methods are effective, practical, and achieve state-of-the-art performance. The authors have addressed the reviewers' comments and provided additional details to enhance the clarity and robustness of the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### 1. **Clarity and Readability**\n- **Reviewer 1 and 2**: Both reviewers found the paper difficult to follow and lacking in clarity. The introduction and the explanation of the proposed Symbolic Continuous Stimulus (SCS) representation are not well-explained, making it hard for readers to understand the significance and utility of the proposed method.\n- **Reviewer 3 and 4**: These reviewers also pointed out that the paper is unclear in several places, with long and repetitive sentences, and missing details in the experimental setup and results.\n\n#### 2. **Technical Novelty and Significance**\n- **Reviewer 1 and 2**: The reviewers noted that the contributions are only marginally significant or novel. The SCS representation is not well-justified, and its utility outside the specific experimental setting is not clear.\n- **Reviewer 3 and 4**: These reviewers echoed similar concerns, emphasizing that the SCS representation's applicability and novelty are limited. They also suggested that the paper should compare SCS with fully continuous representations to justify its value.\n\n#### 3. **Empirical Novelty and Significance**\n- **Reviewer 1 and 2**: The experiments are not sufficient to support the claimed contributions. The authors did not compare SCS with other representations, such as one-hot encodings, and the results are not robust (e.g., using only 2 seeds for some experiments).\n- **Reviewer 3 and 4**: These reviewers pointed out that the results are barely above chance performance and that the experimental setup lacks necessary details and ablation studies. They also suggested that the paper should demonstrate the value of SCS on existing benchmarks for compositional generalization.\n\n#### 4. **Correctness**\n- **Reviewer 1 and 2**: The paper has minor issues with some claims and statements that are not well-supported.\n- **Reviewer 3 and 4**: These reviewers noted more significant issues with the correctness of the paper, including the poor performance of the DNC model and the lack of ablation studies.\n\n#### 5. **Author Response**\n- The authors acknowledged the reviewers' comments and promised to improve the clarity and readability of the paper. However, they did not provide specific details on how they will address the technical and empirical issues raised by the reviewers.\n\n### Conclusion\nThe paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference. The clarity and readability of the paper are poor, the technical contributions are not well-justified, and the empirical results are insufficient to support the claimed contributions. The authors' response does not provide enough detail to address these concerns adequately.\n\n### Final Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Technical Novelty and Significance:**\n- **Theoretical Contributions:**\n  - The paper provides a novel analysis of the fixed points of Temporal Difference (TD), Fitted Value Iteration (FVI), and Residual Minimization (RM) in the overparameterized linear setting. This is a significant contribution because it addresses a gap in the literature where most existing work focuses on underparameterized settings.\n  - The unified interpretation of these algorithms as minimizing the Euclidean norm of the weights subject to different constraints is a novel and insightful perspective. This interpretation helps in understanding the implicit biases of these algorithms in the overparameterized regime.\n  - The paper also provides a generalization error bound for FVI, which is a valuable theoretical result.\n\n- **Connection to Deep RL:**\n  - The authors establish a connection between overparameterized linear models and deep neural networks using the Neural Tangent Kernel (NTK). This connection is important because it allows the theoretical results to be extended to more practical settings.\n  - However, some reviewers have raised concerns about the practical relevance of the linear setting to deep RL, where the number of parameters is often much larger than the number of data points. The authors address this by showing how the results can be applied to NTK, but this connection could be further strengthened.\n\n#### **2. Correctness and Robustness:**\n- **Mathematical Derivations:**\n  - The paper contains several mathematical derivations and proofs. While the authors have addressed most of the concerns raised by the reviewers, some issues remain:\n    - The diagonalizability assumption in the matrix decomposition is a minor issue but should be clearly stated.\n    - The proof of Corollary 2 had a minor error, which has been fixed in the revision.\n  - The derivations in Theorem 1, 2, and 3 are robust and well-supported.\n\n- **Empirical Validation:**\n  - The empirical results are primarily designed to validate the proposed regularizers. While the experiments are on toyish domains, the authors have added new results on Mujoco control problems in the revision, which strengthens the empirical validation.\n  - However, the experiments could be more extensive and better motivated to fully demonstrate the practical impact of the theoretical findings.\n\n#### **3. Clarity and Presentation:**\n- **Writing and Structure:**\n  - The paper is generally well-written and easy to follow. The authors have addressed most of the grammatical and clarity issues raised by the reviewers.\n  - The introduction and motivation are clear, and the paper provides a good overview of the related work.\n  - However, some sections could benefit from more detailed explanations, especially the connection between the overparameterized linear setting and deep RL.\n\n- **Figures and Tables:**\n  - The figures and tables are clear, but the font size in some figures is small, which could be improved.\n\n#### **4. Impact and Relevance:**\n- **Theoretical Impact:**\n  - The paper makes a significant theoretical contribution by providing a deeper understanding of value estimation algorithms in the overparameterized regime. This is a crucial step towards better understanding modern deep RL algorithms.\n  - The unified view of the algorithms and the generalization error bounds are valuable contributions to the field.\n\n- **Practical Impact:**\n  - While the practical impact is somewhat limited by the focus on the linear setting, the proposed regularizers show promise in improving the stability and performance of value estimation algorithms in deep RL.\n  - The connection to NTK and the extension to more complex environments in the revision enhance the practical relevance.\n\n### Final Decision\nGiven the significant theoretical contributions, the robustness of the derivations, and the improvements made in the revision, the paper should be accepted for publication. However, the authors should address the remaining minor issues and further strengthen the practical relevance of the results.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n\n1. **Motivation and Problem Relevance**:\n   - The paper addresses a significant and relevant problem in the field of multi-objective optimization, particularly in the context of optimal experimental design (OED). The need for a user-friendly and efficient platform for OED is well-motivated, especially for users who are not experts in coding or machine learning.\n\n2. **Engineering Contributions**:\n   - **User-Friendly GUI**: The platform is designed with a graphical user interface (GUI) that makes it accessible to users with little to no coding experience. This is a significant contribution as it lowers the barrier to entry for using advanced optimization techniques.\n   - **Modular Design**: The modular framework allows for easy integration and testing of different components, which is valuable for both users and researchers. This design facilitates the development and evaluation of new algorithms.\n   - **Support for Multiple Domains**: AutoOED supports a wide range of design variables (continuous, discrete, binary, categorical, and mixed types), which is a significant improvement over existing platforms.\n\n3. **Scientific Contributions**:\n   - **Asynchronous MOBO**: The paper introduces and evaluates asynchronous multi-objective Bayesian optimization (MOBO), which is a novel and important contribution. The Believer-Penalizer (BP) strategy is a practical solution to the challenges of asynchronous optimization.\n   - **Empirical Evaluations**: The authors provide extensive empirical evaluations on 12 benchmark problems, demonstrating the robust performance of AutoOED. The ablation studies and comparisons with other platforms are thorough and well-documented.\n\n4. **Real-World Applications**:\n   - The paper demonstrates the practical utility of AutoOED by showing its application in real-world hardware experiments. This is a strong point as it validates the platform's effectiveness in real-world scenarios.\n\n#### Weaknesses of the Paper\n\n1. **Technical Novelty**:\n   - While the paper introduces the BP strategy, the overall technical novelty is somewhat limited. The BP strategy is a combination of existing ideas (Kriging Believer and Local Penalization) and lacks a strong theoretical foundation.\n   - The paper is more engineering-oriented, and the scientific contributions are not as strong as those in purely algorithmic papers.\n\n2. **Empirical Results**:\n   - The empirical results show mixed performance, and the claims of outperforming other methods are overstated. The authors have addressed this in their response by rephrasing the results to be more accurate.\n   - The real-world example is a proof of concept rather than a comprehensive benchmark, which limits the strength of the empirical validation.\n\n3. **Limitations**:\n   - The initial limitation of handling only 2 or 3 objectives has been addressed in the author response, but the platform's performance on many-objective optimization is not thoroughly evaluated.\n   - The paper could benefit from a more detailed comparison with other state-of-the-art platforms and a more comprehensive theoretical analysis of the BP strategy.\n\n### Final Decision\n\nDespite the limitations, the paper makes significant contributions in terms of engineering and practical utility. The introduction of a user-friendly platform for multi-objective optimization, the modular design, and the empirical evaluations on real-world problems are valuable contributions to the field. The authors have addressed many of the reviewers' concerns and have made improvements to the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **Strengths of the Paper:**\n1. **Problem Relevance:**\n   - The paper addresses a significant problem in the field of continual learning and hierarchical motor control, which is relevant for both neuroscience and machine learning.\n   - The problem of learning and chaining motor motifs without interference is a well-defined and important challenge.\n\n2. **Biological Inspiration:**\n   - The authors draw inspiration from the thalamocortical circuit, which is a novel and potentially valuable approach to improving RNN performance.\n   - The biological plausibility of the preparatory module is supported by recent experimental evidence (Nashef et al., 2021).\n\n3. **Technical Contributions:**\n   - The proposed architecture with a preparatory module shows significant improvements in zero-shot transfer and robustness compared to baseline RNNs.\n   - The authors provide a clear and detailed description of the experimental setup and results, which is essential for reproducibility.\n\n4. **Empirical Validation:**\n   - The paper includes extensive empirical results that demonstrate the effectiveness of the proposed method.\n   - The authors have addressed some of the reviewers' concerns by providing additional evidence and clarifications in the revised manuscript.\n\n#### **Weaknesses of the Paper:**\n1. **Technical Novelty and Significance:**\n   - Some reviewers argue that the contributions are only marginally significant or novel. While the preparatory module is a novel idea, the overall approach and the specific technical details may not be groundbreaking.\n   - The paper could benefit from a more thorough comparison with existing methods and a clearer demonstration of the unique advantages of the proposed approach.\n\n2. **Scale and Relevance:**\n   - The scale of the experiments (300 units, 10 motifs) is relatively small, which may limit the generalizability of the results to larger, more complex tasks.\n   - The relevance of the results to real-world applications and larger-scale problems is not fully established.\n\n3. **Clarity and Presentation:**\n   - The paper suffers from some issues in clarity and presentation, which make it difficult to follow and understand the contributions.\n   - The writing could be improved to better integrate the technical details and make the paper more accessible to a broader audience.\n\n4. **Alternative Approaches:**\n   - The authors have not thoroughly tested or discussed alternative approaches, which weakens the argument for the necessity of the proposed method.\n   - The paper could benefit from a more comprehensive evaluation of different strategies for motif chaining and zero-shot transfer.\n\n### Final Decision\nDespite the significant contributions and the biological inspiration, the paper has several weaknesses that need to be addressed. The technical novelty and significance are not as strong as they could be, and the scale of the experiments is relatively small. Additionally, the clarity and presentation of the paper need improvement, and the authors should consider testing alternative approaches to strengthen their claims.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Theoretical Insights**:\n   - The paper provides a clear and insightful analysis of epoch-wise double descent using a linear teacher-student model. The authors derive closed-form expressions for the generalization error, which is a significant theoretical contribution.\n   - The use of the replica method and random matrix theory to study the dynamics of feature learning is novel and well-executed.\n\n2. **Empirical Validation**:\n   - The authors validate their theoretical findings with numerical experiments, showing a good match between the theory and empirical results. This adds credibility to their theoretical claims.\n   - The connection to real-world deep neural networks (ResNet18 on CIFAR-10) is demonstrated, showing that the insights from the simple model are relevant to more complex settings.\n\n3. **Clarity and Presentation**:\n   - The paper is well-written and the results are neatly presented. The authors provide clear explanations and intuitive interpretations of their findings.\n\n4. **Relevance to the Field**:\n   - The phenomenon of double descent is a topic of significant interest in the deep learning community. Understanding the dynamics of feature learning and generalization is crucial for developing better models and training strategies.\n\n#### Weaknesses of the Paper\n\n1. **Technical Novelty**:\n   - While the paper provides a novel analysis of epoch-wise double descent, some reviewers noted that similar insights have appeared in related works. The authors have addressed this by providing a detailed comparison with related works and clarifying the unique contributions of their approach.\n\n2. **Empirical Depth**:\n   - The empirical validation is solid but limited to a specific setup. The authors could have explored more diverse settings to further validate their findings.\n\n3. **Typos and Notational Issues**:\n   - The paper contains several typos and notational inconsistencies, which can be distracting and make the paper harder to follow. The authors have addressed many of these issues in their response, but a thorough review of the revised manuscript is necessary to ensure all issues are resolved.\n\n4. **Interpretation and Connections**:\n   - Some reviewers raised questions about the interpretation of the epoch-wise double descent peak and the connection to other forms of double descent. The authors have provided additional discussions and experiments to address these points, but further clarification might be beneficial.\n\n### Final Decision\n\nGiven the significant theoretical contributions, clear empirical validation, and the relevance of the topic to the deep learning community, the paper should be accepted for publication. The authors have addressed most of the concerns raised by the reviewers, and the remaining issues can be resolved through a thorough review of the revised manuscript.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper provides a novel theoretical framework for neural network approximation using tropical geometry. The main theoretical result, Theorem 2, establishes a connection between the approximation error of neural networks and the Hausdorff distance of tropical zonotopes. This is a significant contribution to the field of neural network compression and approximation.\n2. **Novel Algorithm**: The authors propose a new algorithm (Zonotope K-means and Neural Path K-means) for compressing fully connected layers of neural networks. The algorithm is theoretically grounded and shows promise in initial experiments.\n3. **Clarity and Accessibility**: The paper is well-written and accessible to non-domain experts, with a clear introduction to tropical geometry and its application to neural networks.\n4. **Empirical Validation**: The authors provide empirical results on several datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and compare their method to existing techniques, including ThiNet and L1 structured pruning. The results are promising and support the theoretical claims.\n\n#### Weaknesses:\n1. **Limited Scope of Experiments**: The experiments are primarily proof-of-concept and focus on relatively small architectures (LeNet, VGG, AlexNet). The paper does not evaluate the method on modern architectures like ResNets or Vision Transformers, which are more relevant in current research.\n2. **Comparison with State-of-the-Art**: The paper compares its method to ThiNet, which is a structured pruning method, but does not compare it to more recent and advanced pruning techniques. This limits the practical significance of the results.\n3. **Computational and Memory Complexity**: The paper does not provide a detailed analysis of the computational and memory complexity of the proposed algorithm. This information is crucial for understanding the practical feasibility of the method.\n4. **Overclaiming Results**: The abstract and some parts of the paper overclaim the performance of the proposed method, suggesting it is competitive with state-of-the-art techniques. This is not fully supported by the experimental results.\n5. **Bounds on Approximation Error**: The bounds provided in Propositions 4 and 5 are not practically useful, as they decrease as the percentage of remaining weights decreases. This suggests that the bounds may be loose and need further refinement.\n\n#### Addressing Reviewer Comments:\n- **Reviewer 1**: The reviewer points out the need for more modern architectures and a comparison of computational and memory complexity. The authors address these points by explaining the limitations of their current work and promising future extensions.\n- **Reviewer 2**: The reviewer suggests that the paper is outside their domain expertise but finds the theoretical contribution novel. The authors provide a summary and address the need for more large-scale datasets.\n- **Reviewer 3**: The reviewer raises several specific questions and points out the limitations of the experimental results. The authors provide detailed responses and clarify the scope of their work.\n- **Reviewer 4**: The reviewer is generally positive but suggests improvements in reproducibility and clarity. The authors address these points by providing code and additional explanations.\n\n### Final Decision\nWhile the paper has significant theoretical contributions and a novel algorithm, the experimental section is limited in scope and does not fully support the claims made in the abstract. The paper would benefit from more extensive experiments on modern architectures and a more detailed comparison with state-of-the-art techniques. However, the theoretical contributions are strong and the paper is well-written.\n\nFinal Decision: Accept\n\nThe paper should be accepted with the condition that the authors address the following points in the final version:\n1. Provide more extensive experiments on modern architectures like ResNets and Vision Transformers.\n2. Include a detailed analysis of the computational and memory complexity of the proposed algorithm.\n3. Refine the claims in the abstract and main text to accurately reflect the current scope and limitations of the work.\n4. Provide a more comprehensive comparison with state-of-the-art pruning techniques."
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n\n1. **Well-Written and Clear:**\n   - The paper is well-written and easy to follow, with sufficient technical details provided for reproducibility. The authors have addressed reviewer comments and made improvements, such as changing the title to better reflect the scope and adding more details about failure modes and dataset statistics.\n\n2. **Technical Soundness:**\n   - The method (MILAN) is well-motivated and technically sound. It introduces a novel approach to generating natural language descriptions of neurons in deep learning models, which is a significant contribution to the field of model interpretability.\n\n3. **Empirical Validation:**\n   - The authors provide extensive empirical validation, demonstrating the effectiveness of MILAN across different model architectures, datasets, and tasks. The results show that MILAN outperforms baseline methods in generating descriptions that align with human annotations.\n\n4. **Practical Applications:**\n   - The paper highlights three practical applications of MILAN: analysis, auditing, and editing. These applications demonstrate the potential of MILAN to improve model understanding and robustness, which is highly valuable in real-world scenarios.\n\n5. **Open-Source Contribution:**\n   - The authors plan to open-source the data, code, and trained models, which is a significant contribution to the research community.\n\n#### Weaknesses and Concerns\n\n1. **Inter-Annotator Agreement:**\n   - The inter-annotator agreement in the MILANNOTATIONS dataset is relatively low, which could affect the quality of the training data. The authors have addressed this by explaining how they handle inter-annotator inconsistency, but further validation or quality control measures could strengthen the dataset.\n\n2. **Generalizability:**\n   - While the paper demonstrates the effectiveness of MILAN on vision models, the scope is limited to visual features. The authors acknowledge this and suggest that extending MILAN to other domains (e.g., NLP) is a promising area for future research. However, the current results are specific to vision models and may not generalize to other tasks without additional work.\n\n3. **Baseline Comparison:**\n   - The authors have compared MILAN to baseline methods, but the comparison with off-the-shelf image captioning models (e.g., SAT trained on COCO) is still in progress. Including these results would provide a more comprehensive evaluation of MILAN's performance.\n\n4. **Ethical Considerations:**\n   - The paper touches on ethical considerations, such as the potential for bias in the dataset. However, a more detailed discussion of how to mitigate these biases and ensure fairness in the generated descriptions would be beneficial.\n\n5. **Failure Modes:**\n   - The paper includes a discussion of failure modes, but more detailed analysis and examples would help readers understand the limitations of MILAN and guide future improvements.\n\n### Final Decision\n\nDespite the identified weaknesses, the paper presents a significant and novel contribution to the field of model interpretability. The method is well-motivated, technically sound, and empirically validated. The practical applications and open-source contributions further enhance its value. The authors have addressed reviewer comments and made improvements to the paper.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Strengths of the Paper\n1. **Novelty and Motivation**:\n   - The paper introduces a novel framework, PEARL, for synthesizing data using deep generative models with differential privacy guarantees.\n   - The approach is well-motivated and addresses a significant problem in the field of data synthesis and privacy.\n\n2. **Theoretical Contributions**:\n   - The paper provides a clear mathematical formulation and theoretical guarantees for the performance of the proposed method.\n   - The use of characteristic functions and adversarial re-weighting objectives is innovative and of independent interest.\n\n3. **Empirical Evaluation**:\n   - The empirical results show that PEARL outperforms other methods at reasonable levels of privacy on multiple datasets.\n   - The authors have added additional baselines (DPCGAN) and datasets (Credit) during the rebuttal period, addressing some of the reviewers' concerns.\n\n4. **Clarity and Writing**:\n   - The paper is well-written and easy to follow, with clear explanations of the methodology and results.\n\n#### Weaknesses of the Paper\n1. **Benchmarking**:\n   - While the authors have added more baselines and datasets, the benchmarking is still somewhat limited. For tabular data, it would be beneficial to include more advanced baselines like PATE-GAN and CTGAN.\n   - For image data, the authors did not evaluate on more complex datasets like CIFAR-10, which is a standard benchmark in the field.\n\n2. **Privacy Budget and Sensitivity Calculation**:\n   - The sensitivity calculation and the auxiliary information release are not fully clear. The authors have provided some clarifications, but further explanation and justification are needed.\n   - The performance saturation at moderate values of epsilon needs more detailed analysis and discussion.\n\n3. **Complexity Analysis**:\n   - The computational complexity analysis is provided in the rebuttal, but it would be better to include this in the main paper to provide a complete picture of the method's efficiency.\n\n4. **Related Work**:\n   - The paper lacks a comprehensive review of related work. The authors have added some references in the rebuttal, but a more thorough discussion of existing methods and their limitations would strengthen the paper.\n\n5. **Editorial Quality**:\n   - The paper contains some inconsistent and non-precise descriptions, which need to be addressed to improve the overall quality.\n\n### Final Decision\nDespite the strengths of the paper, the weaknesses in benchmarking, clarity of privacy guarantees, and computational complexity analysis are significant. The authors have made efforts to address some of these issues in the rebuttal, but the paper still falls short of the standards expected for a top-tier conference.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **Technical Novelty and Significance**\n- **Novelty**: The paper introduces the Variational Neural Cellular Automata (VNCA), which is a novel approach that combines the principles of Neural Cellular Automata (NCA) with a Variational Auto-Encoder (VAE). While the idea of interpreting deep convolutional networks as NCAs is not entirely new, the specific application of NCAs in a generative model context is novel. The paper's main contribution is showing that NCAs can model complex data distributions through morphogenesis and exhibit resilience to damage, which is a significant and interesting property.\n- **Significance**: The VNCA is a first step towards understanding how NCAs can be used in generative modeling. The paper's focus on the self-organizing nature of NCAs and their ability to recover from damage is a valuable contribution to the field. The authors have also addressed the limitations of their model and provided a fair evaluation, which is important for scientific progress.\n\n#### **Empirical Novelty and Significance**\n- **Results**: The experimental results are mixed. The VNCA performs well on MNIST but less so on more complex datasets like CelebA. The likelihood results are not state-of-the-art, but the paper provides a clear and honest evaluation of its performance. The authors have added new experiments on damage recovery and latent space analysis, which are valuable and provide additional insights into the model's capabilities.\n- **Robustness**: The VNCA's ability to recover from damage is a significant empirical contribution. The new experiments in Section 3.3 show that the VNCA can learn stable attractors around the data, which is a unique and interesting property. This robustness is not a common feature in other generative models and adds to the paper's significance.\n\n#### **Writing and Clarity**\n- **Clarity**: The paper is well-written and easy to follow. The authors have addressed the reviewers' comments and improved the clarity of the paper. The new sections and figures are well-explained, and the paper provides a good balance between theoretical insights and empirical results.\n- **Reproducibility**: The authors have committed to making the code publicly available, which is a strong point in favor of the paper. The paper is also transparent about its limitations and the choices made during the experimental setup.\n\n#### **Addressing Reviewer Concerns**\n- **Reviewer 1**: The reviewer's main concern is the underwhelming experimental results. While the results are not state-of-the-art, the paper provides a valuable contribution to the understanding of NCAs in generative modeling. The new experiments on damage recovery and latent space analysis address some of the reviewer's concerns.\n- **Reviewer 2**: The reviewer points out that many claims are not surprising given the existing literature on resnets and neural ODEs. The authors have addressed this by clarifying the unique aspects of their work and the differences between NCAs and resnets.\n- **Reviewer 3**: The reviewer was already in favor of accepting the paper and has raised their score after the rebuttal. The new experiments and clarifications have improved the paper.\n- **Reviewer 4**: The reviewer is leaning towards rejection due to the underwhelming results. However, the new experiments and the paper's honesty about its limitations are valuable contributions.\n- **Reviewer 5**: The reviewer has raised their score after the rebuttal and is now in favor of accepting the paper. The new experiments and clarifications have addressed the reviewer's concerns.\n- **Reviewer 6**: The reviewer is concerned about the novelty of the paper. The authors have addressed this by clarifying the unique aspects of their work and the differences between NCAs and resnets.\n- **Reviewer 7**: The reviewer has raised their score after the rebuttal and is now in favor of accepting the paper. The new experiments and clarifications have improved the paper.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper introduces a novel and interesting approach to generative modeling using NCAs. While the experimental results are not state-of-the-art, the paper provides valuable insights into the capabilities of NCAs, particularly their robustness to damage. The authors have addressed the reviewers' concerns and improved the paper significantly. The paper is well-written, clear, and reproducible, making it a valuable contribution to the field."
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **1. Technical Novelty and Significance:**\n- **Novelty:** The paper introduces the concept of offline monotonic policy improvement and proposes Behavior Proximal Policy Optimization (BPPO). While the idea of using on-policy methods for offline RL is not entirely new, the paper provides a novel theoretical framework and practical implementation that is distinct from existing methods like Onestep RL. The theoretical analysis, particularly the derivation of the policy improvement bounds and the advantage replacement, is a significant contribution.\n- **Significance:** The paper addresses a significant problem in offline RL, where existing methods often struggle with overestimation of out-of-distribution actions. BPPO's approach of using on-policy methods and constraining the learned policy close to the behavior policy is a promising direction. The empirical results on the D4RL benchmark, especially on the Adroit, Kitchen, and Antmaze tasks, demonstrate the method's effectiveness and robustness.\n\n#### **2. Empirical Novelty and Significance:**\n- **Empirical Results:** The paper provides extensive empirical evaluations on the D4RL benchmark, showing that BPPO outperforms state-of-the-art methods on several challenging tasks. The performance gains on Adroit, Kitchen, and Antmaze are particularly notable, with significant improvements over existing baselines. The ablation studies and additional experiments, such as the analysis of hyperparameters and the importance weight, further strengthen the empirical results.\n- **Comparisons:** The paper includes comparisons with a wide range of baselines, including recent state-of-the-art methods like CQL, IQL, and RORL. The results show that BPPO is competitive and often superior, especially on more difficult tasks.\n\n#### **3. Clarity and Quality:**\n- **Clarity:** The paper is generally well-written and easy to follow. The theoretical derivations are presented clearly, and the experimental setup is well-documented. However, some sections, particularly the initial parts of the theoretical analysis, could benefit from further clarification and polishing.\n- **Quality:** The theoretical results are sound, and the empirical evaluations are thorough. The authors have addressed many of the reviewers' concerns in their responses, including the strong assumption 1, the soundness of the advantage replacement, and the importance of hyperparameters. The paper has been significantly improved through the review process.\n\n#### **4. Correctness:**\n- **Theoretical Correctness:** The theoretical results are generally correct, and the authors have addressed the concerns about the soundness of the method. The new proof techniques and the use of total variational divergence to measure the distance between the estimated behavior policy and the offline dataset are reasonable and well-justified.\n- **Empirical Correctness:** The empirical results are well-supported by the experiments, and the authors have provided additional analyses to address the reviewers' concerns. The performance gains on the D4RL benchmark are significant and consistent across multiple tasks.\n\n#### **5. Reproducibility:**\n- **Reproducibility:** The paper is well-documented, and the authors have promised to make their code public. The experimental setup and hyperparameters are clearly described, making it easier for other researchers to reproduce the results.\n\n### Final Decision\nFinal Decision: Accept\n\n### Justification\nThe paper makes significant contributions to the field of offline reinforcement learning by introducing a novel theoretical framework and a practical algorithm, BPPO. The empirical results are strong, particularly on challenging tasks, and the authors have addressed the reviewers' concerns effectively. The paper is well-written and well-documented, and the contributions are both novel and significant. Therefore, the paper should be accepted for publication at a top-tier conference."
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Significance\n- **Technical Novelty**: The paper introduces a novel approach to online knowledge distillation by using a label prior shift to induce diversity among teachers. This is a significant contribution as it addresses the issue of homogenization in peer-based ensembles. The use of importance sampling and post-compensated softmax (PC-Softmax) to maintain and leverage this diversity is also innovative.\n- **Empirical Novelty**: The paper provides a thorough empirical analysis, demonstrating the effectiveness of the proposed method on multiple datasets (CIFAR-10, CIFAR-100, and ImageNet). The results show improvements in top-1 error rate, negative log-likelihood, and expected calibration error, which are important metrics in the field.\n\n#### Clarity and Quality\n- **Clarity**: The paper is well-written and structured, making it easy to follow. The figures and tables are clear and informative. The authors have addressed the reviewers' comments and provided additional experiments to strengthen their claims.\n- **Quality**: The quality of the research is high. The authors have conducted extensive experiments and provided detailed ablation studies to validate their method. The paper is well-referenced, and the authors have compared their method with relevant baselines.\n\n#### Strengths\n- **Motivation and Problem Formulation**: The motivation for the paper is clear, and the problem of homogenization in peer-based ensembles is well-articulated.\n- **Methodology**: The proposed method is well-founded and logically sound. The use of label prior shift, importance sampling, and PC-Softmax is well-justified and effectively explained.\n- **Empirical Analysis**: The empirical analysis is robust and comprehensive. The authors have provided results on multiple datasets and compared their method with several baselines, including recent works like PCL and MVCL.\n\n#### Weaknesses\n- **Specialization of Setup**: The setup is specialized to online distillation and peer networks, which limits the generalizability of the method. However, the authors have addressed this concern by showing that the method can be applied to non-peer networks and by providing additional experiments.\n- **Baseline Selection**: The choice of baselines could be better justified. The authors have addressed this by including additional comparisons with recent works like CGL, PCL, and L-MCL.\n- **Minor Issues**: The paper has a few typos and minor issues in the writing, which the authors have addressed in their response.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Suggested additional ablations and comparisons. The authors have addressed these concerns by providing additional experiments and clarifications.\n- **Reviewer 2**: Raised concerns about the consistency of results with MVCL. The authors have provided additional experiments and explanations to address these concerns.\n- **Reviewer 3**: Praised the clarity and empirical analysis but noted minor issues with typos. The authors have addressed these issues.\n- **Reviewer 4**: Suggested additional comparisons with recent works. The authors have included these comparisons and provided detailed results.\n- **Reviewer 5**: Raised concerns about the novelty and experimental results. The authors have addressed these concerns by providing additional experiments and clarifications.\n\n### Final Decision\nGiven the significant contributions in terms of novelty, the robust empirical analysis, and the thorough addressing of reviewer feedback, the paper should be accepted for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Strengths:**\n1. **Novel Idea:**\n   - The paper introduces a novel framework, MCTransformer, which combines Monte-Carlo Tree Search (MCTS) with Transformers. This combination is a promising direction in the field of reinforcement learning, especially for offline settings.\n   - The idea of using Transformers to guide MCTS in the expansion and rollout phases is innovative and could lead to more efficient and effective exploration.\n\n2. **Initial Results:**\n   - The initial results on the SameGame domain show that MCTransformer outperforms both Transformer-only and MCTS-only solutions. This is a positive indication of the potential of the proposed method.\n\n#### **Weaknesses:**\n1. **Limited Evaluation:**\n   - The evaluation is limited to a single, non-standard domain (SameGame). This makes it difficult to assess the generalizability and robustness of the proposed method.\n   - The lack of comparisons to well-established benchmarks and other relevant methods (e.g., Decision Transformers, AlphaZero, and other MCTS-based solutions) weakens the empirical support for the claims.\n\n2. **Clarity and Presentation:**\n   - The paper suffers from clarity issues. The exposition of the method is unclear, and the writing could be significantly improved. Key components of the method, such as the rollout phase and the MCTS baseline, are not well-explained.\n   - The figures and tables lack clarity and contain typographical errors, which can confuse readers.\n\n3. **Technical Novelty:**\n   - The method is a combination of existing techniques (MCTS and Transformers) and does not introduce significant new technical contributions. The structure is similar to AlphaGo/AlphaZero, with the main difference being the use of Transformers instead of ResNet.\n   - The novelty is marginal, and the paper does not provide a strong argument for why the combination of MCTS and Transformers is particularly innovative or necessary.\n\n4. **Empirical Rigor:**\n   - The experiments are not well-designed. The comparison with a random player and the lack of fair comparisons (e.g., using the same time limit for simulations) make the results less convincing.\n   - The paper does not provide a standardized test set or compare with other offline RL methods, which are critical for validating the effectiveness of the proposed approach.\n\n#### **Reviewer Feedback:**\n- **Reviewer 1:** Highlights the lack of clarity, limited evaluation, and the need for more standard benchmarks and comparisons.\n- **Reviewer 2:** Emphasizes the poor novelty and the need for fair and comprehensive comparisons.\n- **Reviewer 3:** Points out the limited novelty, the need for more experiments, and the importance of comparing with other methods and benchmarks.\n\n### Final Decision\nGiven the significant weaknesses in the evaluation, clarity, and technical novelty, the paper does not meet the standards of a top-tier conference. While the idea is promising, the current version of the paper lacks the necessary rigor and depth to support its claims.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### **Clarity and Quality**\n- **Clarity:** The paper is generally well-written and easy to follow, as noted by all reviewers. The idea is intuitive and the motivation is clear.\n- **Quality:** The quality of the paper is somewhat mixed. While the method is well-explained and the experiments are conducted, the overall quality is hampered by several issues.\n\n#### **Novelty**\n- **Technical Novelty:** The novelty of the proposed method is a significant point of contention. Reviewer 1 and Reviewer 3 both argue that the novelty is thin, with the core idea being an incremental improvement over unlikelihood training (UL). Reviewer 2 and Reviewer 4 also note that the contributions are only marginally significant or novel.\n- **Empirical Novelty:** The empirical contributions are also limited. The experiments are primarily conducted on a small model (GPT-2 small) and a limited dataset (Wikitext-103). Reviewer 2 and Reviewer 4 both suggest that the experimental setup is weak and that the metrics used (perplexity, dist-1) are not the most appropriate for evaluating text generation quality.\n\n#### **Correctness and Robustness**\n- **Correctness:** Several reviewers (Reviewer 1, Reviewer 2, and Reviewer 4) point out that some of the paper's claims have minor issues. For example, the claim that unlikelihood training only considers the probabilities of negative tokens is incorrect, as UL jointly considers positive and negative tokens.\n- **Robustness:** The robustness of the method is questionable. Reviewer 2 and Reviewer 4 both suggest that the method should be tested on larger models and more diverse datasets to demonstrate its effectiveness. The human evaluation results are also not statistically significant, which further weakens the claims of the paper.\n\n#### **Reproducibility**\n- **Reproducibility:** The paper is reproducible, with the authors providing code and a Google Colab notebook. This is a positive aspect, but it does not outweigh the other issues.\n\n#### **Impact and Significance**\n- **Impact:** The impact of the proposed method is limited. While the method shows some improvements in reducing repetition, the overall significance is marginal. The method does not demonstrate a clear advantage over existing techniques, especially when applied to larger models and more complex tasks.\n- **Significance:** The significance of the contributions is also limited. The method is a small step forward in the field of text generation, but it does not address the broader issues of text degeneration in a comprehensive manner.\n\n### Final Decision\nGiven the thin novelty, limited experimental setup, and the lack of robustness in the method, the paper does not meet the standards of a top-tier conference. While the idea is interesting and the paper is well-written, the contributions are not significant enough to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Novel Approach**: The paper introduces a novel approach to OOD detection by leveraging graph structures and topological properties. This is a promising direction that could enhance the interpretability and robustness of OOD detection methods.\n2. **Competitive Performance**: The paper reports competitive performance on the LSUN dataset, achieving 97.95% AUROC on far-OOD and 98.79% AUROC on near-OOD detection tasks. These results are comparable to state-of-the-art techniques.\n3. **Human-Interpretable Concepts**: The use of human-interpretable concepts in the graph representation could facilitate human-in-the-loop machine learning, which is a valuable contribution.\n\n#### Weaknesses\n1. **Clarity and Writing**: The paper suffers from poor writing and clarity issues. There are numerous grammatical mistakes and unclear explanations, which make it difficult to follow the methodology and results.\n2. **Lack of Rigorous Validation**: The paper lacks a rigorous comparison with state-of-the-art OOD detection methods. The authors compare their method to graph embedding algorithms but do not validate their claims by reproducing state-of-the-art techniques for controlled comparisons.\n3. **Assumptions and Robustness**: The method relies heavily on the accuracy of the pre-trained object detection model. The paper does not provide an analysis of how the object detector's accuracy impacts the OOD detection performance, which is a critical detail.\n4. **Limited Experiments**: The empirical evaluation is limited to the LSUN dataset and lacks extensive experiments on other datasets (e.g., CIFAR-10/100, ImageNet). This limits the generalizability of the results.\n5. **Ablation Studies and Interpretability**: The paper lacks ablation studies and a detailed analysis of the interpretability of the OOD detection by semantic graph, which are important for understanding the method's strengths and weaknesses.\n6. **Related Work**: The related work section is inadequate and does not cover a significant portion of the OOD detection literature, which is crucial for placing the work in context.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Suggests improvements in clarity, detailed descriptions, and more rigorous experimental validation.\n- **Reviewer 2**: Strongly recommends rejection due to poor writing, lack of novelty, and insufficient empirical evaluation.\n- **Reviewer 3**: Points out the lack of novelty and missing details, but acknowledges the interesting idea and valuable results.\n- **Reviewer 4**: Highlights the novel idea but criticizes the unrealistic assumption of an oracle object detector, poor presentation, and limited experiments.\n\n### Final Decision\nGiven the significant weaknesses in clarity, rigorous validation, and experimental breadth, the paper does not meet the standards of a top-tier conference. While the idea is novel and has potential, the current state of the paper is not ready for publication without substantial revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### **1. Novelty and Significance**\n- **Novelty in Theorem 3.3**: Theorem 3.3, which shows that the trajectory of the product of the weight matrices in deep linear networks closely follows the trajectory of the corresponding convex problem, is a novel and significant contribution. This insight provides a deeper understanding of why overparameterized deep linear networks can converge efficiently, despite the non-convexity of the problem.\n- **Generalization to Strongly Convex Loss**: The extension of the analysis to general strongly convex and smooth loss functions, rather than just the $L_2$ loss, is a valuable contribution. This generalization is not trivial and requires handling low-rank data matrices, which introduces additional technical challenges.\n- **Initialization Schemes**: The paper considers multiple initialization schemes, including Gaussian, orthogonal, and balanced initialization. While some of these schemes have been studied before, the paper provides a unified analysis that covers a broader range of initializations, which is a significant improvement over previous work.\n\n#### **2. Technical Contributions**\n- **Handling Low-Rank Data**: The paper introduces the use of seminorms and restricted operators to handle low-rank data matrices, which is a novel technical contribution. This allows the analysis to be more general and applicable to a wider range of practical scenarios.\n- **Concentration Inequalities**: The use of concentration inequalities for beta distributions to analyze the one-peak random projections and embeddings initialization is a technical advancement that adds to the paper's novelty.\n- **Eigenvalue Estimation**: The estimation of the eigenvalues of the linear operator $P(t)$ is a key technical contribution that enables the paper to derive the sharp convergence rates.\n\n#### **3. Clarity and Quality**\n- **Clarity**: The paper is well-written and easy to follow. The new Section 4, which provides insights into the main results, is particularly valuable and enhances the clarity of the paper.\n- **Quality**: The technical content is sound, and the proofs are rigorous. The paper addresses the concerns raised by the reviewers and provides detailed responses to their questions.\n\n#### **4. Addressing Reviewer Concerns**\n- **Novelty Concerns**: The authors have addressed the concerns about the novelty of Theorems B.1 and B.2 by explaining the technical challenges and extensions compared to previous work. The introduction of seminorms and the handling of low-rank data are significant technical contributions.\n- **Insights and Discussion**: The new Section 4 provides valuable insights into the results, addressing the lack of discussion pointed out by Reviewer 6. The authors have also made the necessary changes to the title and abstract to avoid overclaiming.\n- **Minor Issues**: The authors have addressed the minor issues raised by the reviewers, such as the font size in the figures and the typo in the special balanced initialization.\n\n### Final Decision\nGiven the significant technical contributions, the novel insights provided, and the thorough addressing of reviewer concerns, the paper should be accepted for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Interesting Observation**: The paper presents a novel and interesting observation that the order of batch normalization (BN) and activation functions can significantly affect the performance of neural networks, especially when using bounded activation functions like Tanh.\n2. **Clear Hypothesis and Analysis**: The authors provide a clear hypothesis and conduct a series of experiments to support their claims. They introduce metrics to measure saturation, skewness, and sparsity, which are well-defined and useful for their analysis.\n3. **Empirical Validation**: The paper includes extensive empirical validation using various benchmarks and architectures, which supports the main findings.\n\n#### Weaknesses:\n1. **Clarity and Rigor**: Several claims and arguments are not clearly defined or rigorously supported. For example, the concept of \"center of the function\" and the relationship between saturation and sparsity need more precise definitions and explanations.\n2. **Generalizability**: The analysis is limited to VGG-like architectures and does not cover more modern architectures like ResNets. This limits the practical applicability of the findings.\n3. **Contradictory Observations**: Some observations, such as the relationship between saturation and sparsity, are contradictory and require further clarification. For instance, the claim that higher saturation decreases sparsity but the Swap model has both high saturation and high sparsity is not well-explained.\n4. **Comparison with ReLU**: The paper does not convincingly demonstrate that the Swap model with bounded activation functions outperforms the Convention model with ReLU, which is a widely used activation function. This limits the practical significance of the findings.\n5. **Experimental Setup**: The experimental setup and analysis have some issues. For example, the use of VGG16 for CIFAR-100 is overparameterized, and the authors' solution to remove the last five layers is not well-justified.\n6. **Reviewer Feedback**: The authors have addressed some of the reviewer feedback, but several concerns remain unaddressed or inadequately addressed. For example, the relationship between asymmetric saturation and generalization performance is still not clearly established.\n\n#### Additional Considerations:\n1. **Reproducibility**: The paper provides code and detailed training details, which is a positive aspect. However, the code for calculating key metrics like saturation, skewness, and sparsity was not clearly documented initially.\n2. **Novelty**: The paper introduces a new perspective on the interaction between BN and activation functions, which is a significant contribution. However, the practical impact of the findings is limited due to the issues mentioned above.\n\n### Final Decision\nGiven the interesting and novel observation, the clear hypothesis and analysis, and the extensive empirical validation, the paper has several strengths. However, the lack of clarity and rigor in some claims, the limited generalizability, and the contradictory observations are significant weaknesses that need to be addressed. The paper also does not convincingly demonstrate practical significance compared to widely used activation functions like ReLU.\n\nFinal Decision: Reject\n\nThe paper has the potential to be a valuable contribution, but it requires substantial revisions to address the identified weaknesses and improve the clarity and rigor of the arguments."
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Theoretical Contributions\n1. **Theorem 1 and Its Validity**:\n   - Theorem 1 is a key theoretical contribution of the paper. However, there are significant concerns about its novelty and tightness.\n   - For binary classification, Theorem 1 is essentially equivalent to the bound in TRADES, which raises questions about its novelty.\n   - For multi-class classification, the bound is loose, and the inequality $\\leq$ is necessary but not particularly insightful. The term $1 \\{ p_{\\theta}(Y | z(X)) < 1/2 \\}$ is used to bound the original equation, but this bound is not tight and is considered artificial by some reviewers.\n   - The authors argue that the bound is tighter than that in TRADES, but the practical significance of this tightness is not clear.\n\n2. **Clarity and Rigor**:\n   - The definition of $z(x)$ is not rigorously defined in the initial manuscript, leading to confusion among reviewers. The authors have provided a more rigorous definition in the rebuttal, but this should have been clear from the beginning.\n   - The theoretical part is not well-supported by the method, and the connection between the theoretical bound and the practical method is not strong.\n\n#### Empirical Contributions\n1. **Performance on Standard Datasets**:\n   - The paper shows consistent improvements over existing methods on standard datasets like CIFAR-10, F-MNIST, and SVHN. The improvements are marginal but consistent.\n   - The authors have provided additional results on CIFAR-100, which further supports the empirical claims.\n\n2. **Comparison with Baselines**:\n   - The results for baselines, especially with extra data, are not consistent with those reported in the original papers. The authors have provided explanations for these discrepancies, but the differences in experimental settings (batch size, learning rate scheduler, etc.) make the comparisons less convincing.\n   - The improvements over HAT and TRADES are small, and the authors have acknowledged this. However, they argue that the improvements in standard accuracy are significant and that the method can be further improved by tuning hyperparameters.\n\n3. **Additional Experiments**:\n   - The authors have provided additional experiments, including varying $\\lambda$ and comparing with AWP-TRADES, which strengthen the empirical claims.\n   - The method also shows improvements in fairness, which is an important aspect of adversarial robustness.\n\n#### Novelty and Significance\n1. **Novelty**:\n   - The method is similar to existing methods like MART, but the authors argue that the theoretical motivation and empirical performance make it a significant contribution.\n   - The improvements over existing methods are marginal, but the consistency across different datasets and architectures is noteworthy.\n\n2. **Significance**:\n   - The paper addresses an important problem in adversarial robustness and provides a theoretically motivated method.\n   - The empirical results, while not groundbreaking, are consistent and show improvements over existing methods.\n\n### Final Decision\nGiven the detailed analysis, the paper has several strengths, including a well-motivated method, consistent empirical improvements, and additional experiments. However, the theoretical contributions are not as strong as initially claimed, and the improvements over existing methods are marginal. Despite these limitations, the paper provides valuable insights and contributions to the field of adversarial robustness.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Interesting Observation**: The paper presents a novel and interesting observation that the order of batch normalization (BN) and activation functions can significantly impact the performance of neural networks, especially when using bounded activation functions like Tanh. This is a valuable contribution to the understanding of neural network architecture design.\n2. **Clear Hypothesis and Analysis**: The authors provide a clear hypothesis and a logical chain of reasoning to explain why the \"Swap\" order (BN after activation) performs better than the conventional order (BN before activation) for bounded activation functions. They introduce metrics to measure saturation, skewness, and sparsity, which are well-defined and useful for their analysis.\n3. **Empirical Validation**: The paper includes extensive experiments on various benchmarks and architectures, providing empirical evidence to support their claims. The results are consistent and show a significant improvement in performance for the Swap model.\n\n#### Weaknesses:\n1. **Clarity and Rigor**: Several claims and arguments in the paper are not clearly explained or rigorously supported. For example:\n   - The claim that \"asymmetric saturation enhances generalization\" is not convincingly supported. The authors need to provide more evidence to rule out other factors, such as accelerated training.\n   - The explanation of why the Swap model has higher sparsity despite higher saturation is not intuitive and requires further clarification.\n   - The term \"center of the function\" is not precisely defined, leading to confusion.\n2. **Generalizability**: The analysis is limited to VGG-like architectures and does not cover more modern architectures like ResNets. The authors' claim that Tanh cannot be used in residual connections is not well-supported and contradicts their own experiments.\n3. **Comparative Analysis**: The paper does not provide a comprehensive comparison with ReLU-based models, which are widely used and often outperform bounded activation functions. This limits the practical significance of the findings.\n4. **Experimental Setup**: The experimental setup and analysis have some issues:\n   - The use of VGG16 for CIFAR-100 is overparameterized, leading to inconsistent results in deeper layers. The authors address this by using VGG16_11, but this should have been done in the initial experiments.\n   - The impact of weight decay on sparsity and accuracy is not thoroughly explored, and the results are not consistent across different weight decay values.\n\n#### Addressing Reviewer Comments:\n1. **Reviewer 1**: The author response addresses the need for additional figures and clarifies the use of sigmoid. However, the response does not fully address the need for more quantitative information to support the argument.\n2. **Reviewer 2**: The author response provides additional experiments and clarifications, but the arguments about sparsity and generalization are still not fully convincing.\n3. **Reviewer 3**: The author response addresses the concerns about the saturation metric and the impact of deep blocks, but the overall argument about the benefits of asymmetric saturation remains weak.\n4. **Reviewer 4**: The author response provides additional experiments to address the concerns about accelerated training and the impact of different hyperparameters, but the paper still lacks a clear and rigorous explanation of the mechanisms.\n5. **Reviewer 5**: The author response addresses the concerns about the experimental setup and the use of Tanh in residual connections, but the paper still lacks a clear and rigorous explanation of the mechanisms.\n\n### Final Decision\nWhile the paper presents an interesting and novel observation, the arguments and explanations are not sufficiently rigorous and clear. The paper also lacks a comprehensive comparison with ReLU-based models and does not fully address the concerns raised by the reviewers. Therefore, the paper should be rejected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Novel Concept**: The idea of collaborative adversarial training (CAT) is innovative and addresses a gap in the literature by leveraging the complementary strengths of different adversarial training methods.\n2. **Empirical Results**: The paper demonstrates state-of-the-art robustness on CIFAR-10 and CIFAR-100 under the Auto-Attack benchmark, which is a strong empirical validation of the method's effectiveness.\n3. **Clarity and Readability**: The paper is well-written and easy to follow, making it accessible to a broad audience.\n\n#### Weaknesses\n1. **Computational Cost**: The proposed method involves training two models simultaneously, which doubles the computational cost. This is a significant drawback, especially given the already high computational requirements of adversarial training.\n2. **Lack of Theoretical Analysis**: The paper does not provide a theoretical analysis to explain why the proposed method works. This lack of theoretical grounding makes it difficult to understand the underlying mechanisms and potential limitations.\n3. **Limited Baselines and Comparisons**: The paper does not compare the proposed method with more recent and stronger baselines, such as ensemble adversarial training methods. This limits the robustness of the empirical results.\n4. **Typos and Missing Citations**: The paper contains several typos and fails to cite relevant prior work, which is a significant oversight in a top-tier conference submission.\n5. **Efficiency Concerns**: The method's efficiency is a major concern, especially in practical applications where computational resources are limited. The authors do not address how to mitigate this issue or provide any efficiency improvements.\n6. **Ablation Studies**: The paper lacks detailed ablation studies to understand the impact of different components of the proposed method. This makes it difficult to assess the contributions of each part of the framework.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Points out the lack of theoretical analysis, the increased computational cost, and the need for more detailed experimental analysis and comparisons with recent baselines.\n- **Reviewer 2**: Criticizes the lack of novelty, the trivial experimental results, and the efficiency concerns. Also, suggests comparing with a single model trained with multiple attacks.\n- **Reviewer 3**: Highlights the need for more extensive experiments on larger datasets and different network architectures, and the importance of comparing with data augmentation approaches.\n- **Reviewer 4**: Suggests further elaboration on why CAT improves performance and the need for more ablation studies.\n\n### Final Decision\nGiven the significant weaknesses in the paper, particularly the lack of theoretical analysis, the increased computational cost, and the limited baselines and comparisons, the paper does not meet the standards of a top-tier conference. While the empirical results are promising, the method's practicality and theoretical grounding are insufficiently addressed.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n- **Novelty**: The paper introduces a bi-objective optimization model to maintain a population of agents with diverse playing styles and high skill levels. While the idea of maintaining diversity in self-play is not entirely new, the specific approach of using a meta bi-objective model to ensure that high-level agents with diverse playing styles are more likely to be incomparable is a novel contribution. However, the novelty is somewhat limited by the fact that similar ideas have been explored in the quality-diversity literature, which the paper does not adequately address.\n- **Significance**: The significance of the contribution is moderate. The proposed method shows promise in improving the generalization of policies in self-play, which is a crucial aspect of reinforcement learning in complex environments. However, the impact is somewhat diminished by the lack of comparison to quality-diversity algorithms and the limited number of domains tested.\n\n#### Empirical Novelty and Significance\n- **Empirical Setup**: The experiments are conducted in two games, Pong and Justice Online. While these are reasonable choices, the inclusion of only two domains limits the robustness of the empirical results. The paper would benefit from additional experiments in more diverse and complex environments to better validate the generalizability of the proposed method.\n- **Experimental Design**: The experimental design has some issues. For example, the sampling of opponents in the pool for the experiments in Table 3 is problematic, as it gives an unfair advantage to the proposed method. This needs to be addressed to ensure the validity of the results.\n- **Results**: The results are promising, showing that the proposed method can learn a well-generalized policy and maintain a diverse set of high-level agents. However, the lack of confidence intervals and detailed statistical analysis makes it difficult to fully assess the significance of the results.\n\n#### Clarity, Quality, and Reproducibility\n- **Clarity**: The paper is generally well-written and easy to follow. However, there are some areas that require more clarification, such as the definition of \"playing style\" and the notation used in the equations. The paper would benefit from more detailed explanations and examples.\n- **Quality**: The quality of the paper is good, but it has some technical issues and missing details that need to be addressed. The lack of comparison to quality-diversity algorithms and the problematic experimental design are significant concerns.\n- **Reproducibility**: The appendix provides sufficient details to reproduce the results, but the lack of source code is a minor issue. The paper would be more reproducible if the source code were made available.\n\n#### Correctness\n- **Claims and Statements**: Most of the claims and statements are well-supported, but there are some issues. For example, the paper does not adequately discuss the work of Lanctot et al. (2017) and other quality-diversity algorithms, which are relevant to the proposed method. Additionally, the experimental design has some flaws that need to be addressed.\n\n### Final Decision\nGiven the technical and empirical issues, the limited number of domains tested, and the problematic experimental design, the paper does not meet the standards of a top-tier conference. However, the proposed method shows promise and could be a valuable contribution with further validation and improvements.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### **1. Technical Novelty and Significance**\n- **Novelty**: The paper introduces a novel mechanism for label differential privacy (DP) in regression tasks. The proposed \"randomized response on bins\" (RR-on-Bins) mechanism is designed to be optimal under a given regression loss function. This is a significant contribution, as it extends the concept of label DP from classification to regression, which is a more complex and less explored area.\n- **Significance**: The paper addresses a practical problem in machine learning where label privacy is crucial. The proposed method is theoretically sound and has been proven to be optimal under the given conditions. The empirical results demonstrate the effectiveness of the method, showing that it outperforms existing DP mechanisms.\n\n#### **2. Empirical Evaluation**\n- **Quality of Experiments**: The paper includes a thorough empirical evaluation on several datasets, demonstrating the efficacy of the proposed method. The experiments are well-designed and the results are convincing.\n- **Comparison with Baselines**: The authors have addressed the reviewers' concerns by including a comparison with the method from Ghazi et al. (2021) in the appendix. This comparison shows that RR-on-Bins performs better, which strengthens the empirical validation of the proposed method.\n\n#### **3. Clarity and Writing**\n- **Clarity**: The paper is well-written and easy to follow. The authors have clearly explained the problem, the proposed solution, and the experimental results. The technical details are presented in a clear and organized manner.\n- **Reproducibility**: The paper provides enough details to reproduce the results. The authors have also included additional experiments and discussions in the appendix, which enhances the reproducibility of the work.\n\n#### **4. Addressing Reviewer Comments**\n- **Reviewer Responses**: The authors have effectively addressed the reviewers' comments and concerns. They have provided additional experiments, clarified the differences between regression and classification, and discussed the limitations of their approach. The responses are thorough and demonstrate a deep understanding of the topic.\n\n#### **5. Potential Impact**\n- **Impact on the Field**: The paper has the potential to make a positive impact on the field of differential privacy, particularly in the context of regression tasks. The proposed method is practical and can be applied in various real-world scenarios where label privacy is a concern.\n\n### Final Decision\nFinal Decision: Accept\n\n### Summary\nThe paper \"Regression with Label Differential Privacy\" introduces a novel and significant contribution to the field of differential privacy. The proposed method, RR-on-Bins, is theoretically sound, empirically validated, and addresses a practical problem in regression tasks. The authors have effectively addressed the reviewers' comments and provided a well-written and well-structured paper. Given the technical novelty, empirical significance, and potential impact, the paper should be accepted for publication at a top-tier conference."
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Analysis\n\n#### Novelty and Significance\nThe paper introduces a novel framework, Functional Risk Minimization (FRM), which challenges the traditional assumption of data coming from a single function followed by noise. Instead, it models each data point as coming from its own function. This is a significant departure from the standard Empirical Risk Minimization (ERM) approach and has the potential to address more realistic noise processes in real-world datasets. The idea of associating each data point with its own function is innovative and could have a substantial impact on the field of machine learning.\n\n#### Technical Contributions\n1. **Theoretical Foundations**: The paper provides a theoretical framework for FRM, showing that it subsumes ERM for common loss functions. This is a strong theoretical contribution.\n2. **Algorithmic Development**: The authors derive an approximate algorithm (Equation 9) for models with many parameters, which is crucial for practical implementation. They also discuss the computational aspects, including the use of Hessian-vector products to avoid instantiating the full Hessian, which is a practical solution to scalability issues.\n3. **Empirical Evaluation**: The paper includes experiments on regression, reinforcement learning, and unsupervised learning using CNNs. These experiments demonstrate that FRM can outperform ERM in scenarios with structured variations, which is a promising result.\n\n#### Clarity and Presentation\n1. **Clarity**: The paper has been criticized for its clarity, particularly in the connection between the theoretical framework and the implementable algorithm. The authors have addressed some of these issues in their response, but the paper could still benefit from further clarification.\n2. **Writing**: The paper is generally well-written, but there are some confusing points, such as the use of \"ERM\" and the distinction between the FGM modeling assumption and the FRM learning objective. The authors have promised to improve the writing in the final version.\n\n#### Scalability\n1. **Scalability Concerns**: Reviewers have raised concerns about the scalability of FRM, particularly the computational cost of the Hessian-based approximation. The authors have addressed these concerns by showing that the approximation is both accurate and scalable, and they have provided experimental evidence of FRM's scalability to real datasets and neural networks.\n2. **Future Work**: The authors acknowledge that the current implementation is slower than ERM but argue that this is a first step, and they plan to work on speeding up the computations. This is a reasonable approach, given the novelty of the framework.\n\n#### Addressing Reviewer Concerns\n1. **Reviewer 1**: The author response addresses the concerns about the connection between the theoretical framework and the implementable algorithm, the use of Hessian information, and the lack of formal arguments. They have also clarified the use of \"ERM\" and the distinction between FGM and FRM.\n2. **Reviewer 2**: The author response clarifies the relationship between FRM and hierarchical Bayesian models and addresses the concerns about the consistency of the training and test objectives. They have also added new experiments on unsupervised learning.\n3. **Reviewer 3**: The author response addresses the concerns about computational cost and the importance of FRM. They have provided additional experiments and clarified the potential impact of FRM.\n\n### Final Decision\nThe paper presents a novel and significant contribution to the field of machine learning. While there are some issues with clarity and scalability, the authors have addressed these concerns in their response and have provided additional experiments to support their claims. The potential impact of FRM is high, and the paper opens up new avenues for research in understanding and improving generalization in over-parameterized models.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Problem and Motivation\nThe paper addresses a significant and practical problem in machine learning: the trade-off between using more data for training and having a reliable validation set for model evaluation. This issue is particularly relevant in real-world applications where data is often limited. The motivation is clear and well-articulated, and the problem is indeed underexplored in the literature.\n\n#### Novelty and Contribution\nThe proposed Proximal Validation Protocol (PVP) is a novel approach that leverages data augmentation and distribution-preserving sampling to construct a validation set without splitting the training data. This idea is innovative and has the potential to improve the efficiency and reliability of model evaluation in practical scenarios. The method is straightforward and easy to understand, which is a strength.\n\n#### Empirical Evaluation\nThe empirical evaluation is a mixed bag. The paper demonstrates the effectiveness of PVP on three data modalities (images, text, and tabular data), which is a strong point. However, there are several issues with the experiments:\n1. **Dataset Choice**: The datasets used are relatively small, particularly in the image domain (CIFAR10-LT and CIFAR100-LT). Larger and more diverse datasets, such as the original CIFAR-10, CIFAR-100, or ImageNet, would provide stronger empirical support.\n2. **Model Selection**: The choice of models (e.g., Distill-BERT instead of BERT) and the lack of experiments with different models and optimizers limit the generalizability of the results.\n3. **Ablation Studies**: The paper lacks thorough ablation studies to understand the impact of different hyperparameters, such as the size of the proximal validation set and the choice of data augmentation techniques.\n4. **Statistical Significance**: The results in Tables 1 and 2 lack statistical significance tests, which are crucial for validating the claims.\n\n#### Clarity and Presentation\nThe paper is generally well-written and easy to follow. However, there are some issues with clarity and presentation:\n1. **Definition of Metrics**: The F1 score and other metrics are not clearly defined, leading to confusion.\n2. **Language and Terminology**: Some language choices (e.g., \"plague,\" \"entertained,\" \"frustratingly\") are unconventional for a scientific paper and should be revised.\n3. **Tables and Figures**: Tables 3 and 4 could be improved for better readability, and the use of colored arrows (up-red, down-blue) is not explained.\n\n#### Theoretical Justification\nThe paper lacks a strong theoretical foundation. The authors do not provide a rigorous justification for why the proximal validation set is representative of the data-generating distribution. This is a significant weakness, as the effectiveness of the validation set is crucial for reliable model evaluation.\n\n### Final Decision\nGiven the mixed feedback from the reviewers and the issues identified, the paper has several strengths but also significant weaknesses that need to be addressed. The problem is important, and the proposed solution is novel, but the empirical evaluation and theoretical justification are lacking. Therefore, I recommend rejecting the paper with the possibility of resubmission after addressing the major concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Motivation and Relevance\nThe paper addresses the important problem of predicting protein-protein interactions (PPIs), which is crucial for understanding cellular behavior and functionality. The motivation for using structural embeddings is well-founded, as it leverages the physical binding properties of proteins, which are often ignored in sequence and network-based methods.\n\n#### Methodology\nThe authors propose a method that combines pretrained structural embeddings from OmegaFold with graph neural networks (GNNs) to predict PPIs. While the idea of using structural embeddings is promising, the methodology has several limitations:\n1. **Limited Novelty**: The main contribution is the use of pretrained structural embeddings from OmegaFold, which is a well-established model. The authors do not propose any novel pretraining methods or significant modifications to the GNN architecture. This limits the novelty of the work.\n2. **Model Design**: The use of mean pooling to obtain fixed-length representations of proteins may fail to capture local structures that are crucial for interactions. This is a significant limitation in the model design.\n3. **Baseline Choices**: The baselines used for comparison (GraphSAGE and GAT) are not the state-of-the-art (SOTA) methods for PPI prediction. The paper does not compare with more advanced methods like dMaSIF or AlphaFold-Multimer, which are known to perform well in PPI prediction tasks. This makes it difficult to assess the true performance of the proposed method.\n4. **Empirical Results**: The empirical results show only marginal improvements over the baselines, and the lack of confidence intervals and detailed comparisons with SOTA methods further weakens the claims.\n\n#### Clarity and Presentation\nThe paper has several issues in clarity and presentation:\n1. **AUC Interpretation**: The reviewers point out that the AUC metric and the binary classification setup are not clearly explained, which makes it difficult to interpret the results.\n2. **Tables and Figures**: The tables are not in a digestible format, and the lack of confidence intervals makes it hard to assess the statistical significance of the results.\n3. **Typos and Grammatical Errors**: The paper contains several typos and grammatical mistakes, which affect the overall readability.\n\n#### Technical and Empirical Contributions\n1. **Technical Novelty**: The technical contributions are limited. The use of pretrained structural embeddings from OmegaFold and the application of GNNs are not novel in themselves. The paper does not introduce any significant innovations in either the pretraining or the GNN architecture.\n2. **Empirical Significance**: The empirical results are not robust or comprehensive. The lack of comparison with SOTA methods and the marginal improvements over the baselines suggest that the method does not provide a significant advancement in the field.\n\n### Final Decision\nGiven the limited novelty, weak empirical results, and issues in clarity and presentation, the paper does not meet the standards of a top-tier conference like ICLR. While the motivation is strong, the technical and empirical contributions are not sufficient to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novel Approach**: The paper introduces a novel method for protein sequence and structure co-design using a trigonometry-aware encoder and a roto-translation equivariant decoder. This approach is innovative and addresses a significant challenge in the field of protein engineering.\n2. **Strong Empirical Results**: The method outperforms previous state-of-the-art baselines on multiple tasks, demonstrating its effectiveness and potential impact.\n3. **Iterative Refinement**: The iterative translation mechanism, which updates both sequence and structure in one shot, is a significant improvement over autoregressive and diffusion models, reducing inference costs.\n4. **Clear Motivation and Problem Formulation**: The paper clearly articulates the problem of protein sequence and structure co-design and provides a well-motivated solution.\n5. **Reproducibility**: The authors have promised to release the code, datasets, and experimental environments, which is crucial for reproducibility and further research.\n\n#### Weaknesses:\n1. **Clarity and Detail**: Some technical details, such as the SeqIPA calculation and the context feature update, were initially unclear. However, the authors have addressed these concerns in their responses and have promised to include more detailed explanations in the final version.\n2. **Comparisons and Baselines**: The paper could benefit from more comprehensive comparisons with recent models like ProtMPNN and larger datasets. The authors have conducted additional experiments and comparisons, which should be included in the final version.\n3. **Evaluation Metrics**: The current evaluation metrics, while standard in the field, may not fully capture the functional performance of the designed proteins. The authors have acknowledged this and plan to include more functional-aware metrics in future work.\n4. **Claims and Novelty**: Some claims about the novelty and generalizability of the method were initially overreaching. The authors have revised these claims and provided more accurate and nuanced statements.\n5. **Training Cost and Complexity**: The training cost and complexity of the model are significant, which the authors have addressed by providing detailed training times and comparing them with other models.\n\n#### Reviewer Feedback:\n- **Positive Feedback**: Reviewers 1, 3, 4, 6, and 8 generally found the paper to be well-written, novel, and with strong empirical results. They were satisfied with the authors' responses to their concerns.\n- **Constructive Criticism**: Reviewers 2, 5, and 7 provided constructive feedback on the clarity, comparisons, and evaluation metrics. The authors have addressed these concerns and made the necessary revisions.\n\n### Conclusion\nThe paper presents a novel and effective method for protein sequence and structure co-design, with strong empirical results and significant potential impact. The authors have addressed the majority of the reviewers' concerns and have made substantial improvements to the manuscript. While there are still some areas for improvement, the overall quality and contributions of the paper are strong enough to warrant acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### **1. Novelty and Significance**\n- **Novelty**: The paper addresses a novel and important problemâ€”data leakage in federated learning (FL) for tabular data. While FL has been extensively studied in image and NLP domains, the unique challenges of tabular data (mixed discrete-continuous features, structured data) have not been thoroughly explored. The authors identify these challenges and propose a method, TabLeak, to address them. The use of softmax relaxation, pooled ensembling, and entropy-based uncertainty estimation are novel in this context.\n- **Significance**: The problem of data leakage in FL for tabular data is highly relevant, especially in high-stakes applications like finance and healthcare. The paper's findings highlight the vulnerability of FL systems in these domains, which is a significant contribution to the field.\n\n#### **2. Technical Contributions**\n- **Methodology**: The proposed method, TabLeak, is built on well-known techniques (softmax relaxation, ensembling, entropy) but is adapted and combined in a novel way to address the specific challenges of tabular data. The authors provide a clear and logical explanation of how these techniques work together to improve reconstruction accuracy.\n- **Experiments**: The experimental evaluation is thorough and well-designed. The authors compare their method against strong baselines (random guessing, cosine similarity) and demonstrate significant improvements. They also conduct ablation studies and sensitivity analyses to validate the effectiveness of each component of TabLeak.\n\n#### **3. Clarity and Quality**\n- **Writing**: The paper is generally well-written and easy to follow, as noted by Reviewer 3. However, Reviewer 1 points out some issues with clarity and grammar. The authors have addressed these issues in their rebuttal, and the paper has been updated to improve readability.\n- **Reproducibility**: The authors provide detailed experimental setups and source code, making the results reproducible. This is a strong point in favor of the paper.\n\n#### **4. Addressing Reviewer Concerns**\n- **Reviewer 1**: The reviewer's main concerns were about the technical novelty and writing quality. The authors have addressed these concerns by providing additional experiments and clarifying the text.\n- **Reviewer 2**: The reviewer questioned the technical novelty and suggested more sophisticated relaxations (e.g., Gumbel-Softmax). The authors have conducted experiments showing that the Gumbel-Softmax is less effective than the softmax relaxation in this context.\n- **Reviewer 3**: The reviewer provided detailed feedback on the experimental setup and the claims made in the paper. The authors have addressed these concerns by adding new experiments and clarifying the text.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper addresses a novel and significant problem in federated learning for tabular data, proposes a well-designed and effective method, and provides thorough experimental validation. The authors have addressed the concerns raised by the reviewers, improving the clarity and robustness of the paper. The contributions are significant and the paper is well-suited for publication in a top-tier conference."
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Problem Relevance and Motivation\nThe paper addresses a highly relevant and challenging problem in machine learning: adapting models to distribution shifts, particularly in the context of label shift and conditional shift. This is a critical issue in real-world applications where models are deployed in environments that differ from the training data. The motivation is strong, and the problem is well-articulated.\n\n#### Novelty and Contribution\nThe paper proposes several heuristics to improve the performance of existing label shift adaptation methods in the presence of conditional shift. These heuristics include:\n1. Using an out-of-distribution (OOD) validation set to select hyper-parameters.\n2. Adding scaling hyper-parameters to the loss function.\n3. Handling non-invertible confusion matrices by approximating them with an identity matrix.\n\nWhile these heuristics are not entirely novel, they are novel in the context of online label shift adaptation and the specific problem setting. The paper also extends the label shift adaptation methods to regression problems, which is a valuable contribution.\n\n#### Empirical Evaluation\nThe empirical evaluation is a significant strength of the paper. The authors conduct experiments on both synthetic and realistic datasets, including classification and regression tasks. However, the scope of the experiments is limited, and the results are not always conclusive. The improvements over baseline methods are modest, and in some cases, the proposed methods perform worse. This is a major weakness, as it limits the generalizability and practical utility of the proposed methods.\n\n#### Clarity and Reproducibility\nThe paper is generally well-written and well-structured, but there are several issues with clarity and reproducibility:\n1. **Notation and Definitions**: Several key terms and notations are not clearly defined, making it difficult to understand the methods and results. For example, the definition of \"conditional shift\" and the details of the heuristics are not clearly explained.\n2. **Experimental Details**: The experimental setup and protocols are not fully detailed, which makes it challenging to reproduce the results. For example, the details of the grid search for hyper-parameters and the collection of the validation set are not clearly described.\n3. **Theoretical Justification**: The paper lacks theoretical justification for the proposed heuristics, which is a significant weakness. The Bayesian interpretation is not well-explained, and the connection to the problem at hand is not clear.\n\n#### Addressing Reviewer Comments\nThe authors have made several revisions in response to the reviewer comments, including:\n1. **Clarifying Notations and Definitions**: The authors have added more details to the background and methods sections to clarify notations and definitions.\n2. **Adding Baselines and Results**: The authors have added results for the Optimal Fixed Classifier (OFC) and a variant OFC-H, and have included additional results comparing hyper-parameter selection with in-distribution (IID), out-of-distribution (OOD), and oracle validation.\n3. **Simplifying Heuristics**: The authors have simplified Heuristic 3 to use the identity matrix instead of a tunable scalar, which makes the method more straightforward and easier to understand.\n\nHowever, the authors have not fully addressed the need for a more comprehensive empirical evaluation and theoretical justification. The scope of the experiments remains limited, and the improvements over baseline methods are still modest.\n\n### Final Decision\nGiven the relevance of the problem, the novel contributions, and the improvements made in response to reviewer comments, the paper has merit. However, the limited empirical evaluation and the lack of theoretical justification are significant weaknesses that need to be addressed. Therefore, I recommend that the paper be **rejected** for publication in its current form, but with the suggestion that the authors revise and resubmit the paper with a more comprehensive empirical evaluation and theoretical justification.\n\nFinal Decision: **Reject**"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Novel Idea**: The paper introduces a novel concept of using Binary Labels as an auxiliary task in multi-task learning. This idea is interesting and has the potential to improve the performance of classification models.\n2. **Empirical Improvements**: The authors show that their approach can lead to improvements in accuracy across multiple datasets, including Cifar-100, Caltech101, Sun397, FGVC-Aircraft, and Stanford Cars. This is a positive aspect, especially for more challenging datasets.\n3. **Data Efficiency**: The paper demonstrates that the proposed method can improve performance when training data is limited, which is a valuable contribution in scenarios where labeled data is scarce.\n4. **Clarity**: The paper is generally well-written and easy to follow, making it accessible to a broad audience.\n\n#### Weaknesses:\n1. **Lack of Thorough Analysis**: The paper lacks a detailed analysis of why the Binary Labels improve performance. This is a significant gap, as understanding the underlying mechanisms is crucial for further development and application of the method.\n2. **Insufficient Baselines and Comparisons**: The experimental section is weak. The authors should compare their method against more baselines, including state-of-the-art methods and related techniques like label smoothing and dense labels. This would provide a more comprehensive evaluation of the proposed approach.\n3. **Clarity Issues**: There are several clarity issues in the paper, such as the use of \"dense labels\" without explanation, the lack of details on the Metabalance method, and the unclear procedure for generating Binary Labels. These issues make it difficult to fully understand and reproduce the results.\n4. **Limited Scope**: The paper is limited to multiclass image classification and does not explore other types of classification tasks, such as multilabel classification. Additionally, the method is not evaluated on larger benchmarks like ImageNet, which would provide a more robust validation of its effectiveness.\n5. **Technical Contributions**: The technical contributions of the paper are somewhat limited. The multi-task learning architecture is basic, and there is no in-depth discussion of the weighting between the two tasks or the impact of different hyperparameters.\n6. **Reproducibility**: The paper lacks detailed implementation details, making it challenging to reproduce the results. The authors should provide more information on the optimizer, learning rate, data preprocessing, number of epochs, batch size, and other hyperparameters.\n\n#### Summary of Reviews:\n- **Reviewer 1**: Highlights the novelty and potential of the idea but points out the need for more thorough analysis, additional baselines, and better clarity.\n- **Reviewer 2**: Criticizes the paper for poor notation, lack of related work, and insufficient empirical results. The reviewer also points out several technical issues.\n- **Reviewer 3**: Acknowledges the simplicity and effectiveness of the method but notes the limited empirical improvements and lack of further analysis.\n- **Reviewer 4**: Emphasizes the need for a clearer explanation of the Binary Labels, more detailed experimental results, and a broader evaluation on larger benchmarks.\n\n### Final Decision\nGiven the novel idea and empirical improvements, the paper has some merit. However, the significant weaknesses in the experimental section, lack of thorough analysis, and clarity issues make it difficult to justify acceptance at a top-tier conference. The paper would benefit from substantial revisions to address these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Technical Contributions and Novelty\n1. **Phase Transition Analysis**: The paper provides a detailed analysis of phase transitions in permuted linear regression, which is a significant and novel contribution. The authors identify the precise phase transition thresholds for both the oracle and non-oracle cases, which is a valuable addition to the literature.\n2. **Message Passing Framework**: The use of message passing (MP) to study phase transitions in this context is innovative. While MP is a well-known technique, its application to permuted linear regression is novel and provides new insights.\n3. **Approximation Techniques**: The authors introduce approximation methods to handle the non-oracle case, which is a practical and significant contribution. The approximation framework is well-motivated and appears to be effective.\n\n#### Clarity and Quality\n1. **Writing and Notation**: The paper is poorly written and notationally heavy, which makes it difficult to follow. The notation is inconsistent, and many terms are introduced without proper explanation. This is a major issue that significantly reduces the paper's readability and accessibility.\n2. **Organization**: The paper is poorly organized. It heavily relies on prior work and does not provide a self-contained exposition, which makes it challenging for readers unfamiliar with the background material. The introduction and background sections need significant improvement to make the paper more accessible.\n3. **Experiments and Results**: The empirical results are well-aligned with the theoretical predictions, which is a positive aspect. However, the presentation of the results is unclear, and the code is not publicly available, which affects reproducibility.\n\n#### Significance and Impact\n1. **Relevance**: The problem of permuted linear regression is relevant and has potential applications in various fields, including databases, privacy, and security. The phase transition analysis provides valuable insights into the problem's behavior.\n2. **Impact**: The paper's contributions, if clearly presented, could have a significant impact on the field. However, the current state of the paper limits its impact due to the issues with clarity and organization.\n\n#### Reviewer Feedback\n1. **Positive Feedback**: Reviewers 3 and 4 recognize the paper's technical contributions and novelty. They appreciate the precise identification of phase transition thresholds and the use of message passing.\n2. **Negative Feedback**: Reviewers 1, 2, and 5 highlight significant issues with the paper's clarity, organization, and writing. They also point out that some claims are not well-supported and that the paper is difficult to follow.\n\n### Final Decision\nGiven the significant technical contributions and the novel application of message passing to permuted linear regression, the paper has the potential to be a valuable addition to the literature. However, the current state of the paper, with its poor writing, notational inconsistencies, and organizational issues, makes it unsuitable for publication in its current form. The authors need to address these issues to make the paper more accessible and clear.\n\nFinal Decision: Reject\n\nThe authors should consider a major revision to improve the clarity, organization, and readability of the paper before resubmitting it to a top-tier conference."
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n- **Incremental Contribution**: The paper builds on existing ideas, particularly split learning, but introduces a novel loss function (Center Loss) to enhance privacy. However, the novelty is limited, as the core idea of splitting the model and using an additional loss function is not groundbreaking.\n- **Technical Flaws**: Reviewer 2 points out significant technical flaws, particularly in the threat model and the feasibility of the approach. The server can easily recreate the encoder, and the privacy guarantees during the training phase are questionable. These issues undermine the core claims of the paper.\n\n#### Empirical Novelty and Significance\n- **Limited Evaluation**: The experimental evaluation is limited to two datasets (MNIST and CelebA) and two models (LeNet-5 and VGG-16). The lack of comparison with more sophisticated models (e.g., ResNets, DenseNets, Vision Transformers) and more challenging tasks (e.g., identity inference) weakens the empirical significance.\n- **Benchmarking**: The paper compares the proposed method with only one baseline (Shredder) and does not consider other relevant approaches, such as fully homomorphic encryption. This limited benchmarking makes it difficult to assess the effectiveness of the proposed method.\n\n#### Clarity and Quality\n- **Clarity**: The paper is generally clear and well-written, but it contains several grammatical errors and ambiguous statements. These issues affect the overall clarity and readability.\n- **Quality**: The quality of the work is compromised by the technical flaws and the limited empirical evaluation. The lack of a well-defined security model and the potential for information leakage through the encoded features further reduce the quality.\n\n#### Reproducibility\n- **Reproducibility**: The paper provides some details on hyperparameters and experimental settings, but it lacks crucial information such as optimizers and the number of training epochs. This makes the work less reproducible.\n\n### Final Decision\nGiven the significant technical flaws, limited empirical evaluation, and the incremental nature of the contribution, the paper does not meet the standards of a top-tier conference like ICLR. The issues raised by the reviewers, particularly the lack of a robust threat model and the potential for information leakage, are critical and need to be addressed before the paper can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Strengths of the Paper\n1. **Motivation and Problem Relevance**:\n   - The paper addresses a significant and challenging problem in the field of learning in games, specifically the convergence to good equilibria. This is a crucial issue that has not been extensively explored in the literature, making the problem highly relevant.\n\n2. **Theoretical Contributions**:\n   - The paper provides a solid theoretical foundation by proving pointwise convergence of q-replicator dynamics to Nash equilibria in a broader class of games compared to previous works.\n   - The introduction of average performance metrics and the analysis of the average price of anarchy (APoA) are novel and provide a new perspective on the quality of equilibria.\n\n3. **Empirical Analysis**:\n   - The empirical results for 2x2 symmetric coordination games are well-presented and provide valuable insights into the performance of different learning dynamics.\n   - The use of visual and experimental methods to support the theoretical findings is a strength.\n\n4. **Clarity and Writing**:\n   - The paper is well-written and easy to follow, with clear explanations of the algorithms, techniques, and results.\n   - The authors have addressed the reviewers' comments and made necessary improvements, such as clarifying the abstract and fixing notation issues.\n\n#### Weaknesses of the Paper\n1. **Limited Scope of Results**:\n   - The main theoretical results are limited to 2x2 symmetric coordination games, which is a relatively restricted setting. This limitation is a significant weakness, as the paper claims to initiate a broader analysis of average performance metrics.\n   - The lack of a more general treatment of the problem, especially for higher-dimensional games, is a notable gap.\n\n2. **Comparison with Prior Work**:\n   - The paper does not sufficiently acknowledge and compare its results with the prior work by PP16. While the authors have added a paragraph explaining the differences, the initial lack of detailed comparison could have led to misunderstandings about the novelty of the results.\n   - The contributions of the paper are somewhat incremental compared to PP16, which is a significant concern for a top-tier conference.\n\n3. **Formal Proofs and Generalization**:\n   - The paper provides empirical evidence for the monotonicity of regions of attraction but lacks formal proofs for this claim. This leaves the results somewhat incomplete and preliminary.\n   - The authors should provide more concrete arguments or evidence for the generalizability of their approach to more complex game settings.\n\n#### Final Considerations\n- **Novelty and Significance**:\n  - Despite the limitations, the paper introduces a new and important problem in the field of learning in games. The concept of average performance metrics and the analysis of the APoA are significant contributions.\n  - The paper's approach and results, while limited, provide a foundation for future research and could inspire further work in this area.\n\n- **Impact and Relevance**:\n  - The problem of convergence to good equilibria is highly relevant to the community, and the paper's contributions, even if preliminary, are valuable.\n  - The paper's interdisciplinary approach, combining tools from machine learning, game theory, and dynamical systems, is a strength.\n\n### Final Decision\nFinal Decision: Accept\n\nThe paper addresses a significant and challenging problem, introduces novel concepts, and provides a solid foundation for future research. While the results are limited to 2x2 games and the comparison with prior work could be more detailed, the overall contribution is valuable and relevant to a top-tier conference."
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Analysis\n\n#### Strengths\n1. **Identification of Period Drift**: The paper introduces the concept of \"period drift\" in federated learning, which is a novel and important observation. This concept addresses the heterogeneity in data distributions across different communication rounds, which is a significant issue in federated learning.\n2. **Meta-Learning Aggregation**: The proposed method, FedPA, uses a meta-learning approach to learn an adaptive aggregation strategy. This is a novel and potentially impactful approach to addressing the biases introduced by both client and period drift.\n3. **Experimental Results**: The paper provides experimental results on two datasets (EMNIST and MovieLens) that demonstrate the effectiveness of FedPA compared to baseline methods.\n\n#### Weaknesses\n1. **Lack of Theoretical Justification**: The paper does not provide a strong theoretical foundation for the concept of period drift or why the proposed meta-learning approach is particularly suited to address it. This lack of theoretical support weakens the paper's claims.\n2. **Comparison with Related Work**: The paper does not compare FedPA with other relevant methods, such as FedET and DS-FL, which also use proxy datasets. This comparison is crucial to demonstrate the unique advantages of FedPA.\n3. **Ablation Studies**: The paper lacks ablation studies to understand the impact of different components of the proposed method, such as the data distribution of the proxy dataset and the choice of neural network architectures.\n4. **Experimental Settings**: The experimental settings are limited. The datasets and models used are relatively simple, and the paper does not include results on more challenging datasets like CIFAR-100 or Stack Overflow. This limits the generalizability of the results.\n5. **Clarity and Quality**: The paper has several issues with clarity and quality, including grammatical mistakes, inconsistent citation formats, and small typos. The figures are also poorly formatted, making them difficult to read.\n6. **Reproducibility**: The paper does not provide code or detailed hyperparameter settings, which makes it challenging for other researchers to reproduce the results.\n\n#### Reviewer Feedback\n- **Reviewer 1**: Points out the lack of discrimination between client drift and period drift, the use of a proxy dataset, and the need for more comprehensive experiments and ablation studies.\n- **Reviewer 2**: Highlights the need for a more in-depth discussion on why meta-learning is suitable for addressing period drift and suggests comparing with related work.\n- **Reviewer 3**: Expresses concerns about the ad-hoc nature of the method, the strong assumption of a proxy dataset, and the experimental performance.\n- **Reviewer 4**: Suggests improvements in writing quality, additional experiments on other datasets, and quantifying the server overhead.\n\n### Final Decision\nGiven the significant weaknesses in theoretical justification, experimental settings, and clarity, and the lack of a strong comparison with related work, the paper does not meet the standards of a top-tier conference. While the concept of period drift and the meta-learning approach are novel, the paper needs substantial improvements to address the reviewers' concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\n#### Technical Novelty and Significance\n1. **Extension to Continuous Variables**: The paper introduces DeepSeaProbLog, which extends DeepProbLog to handle continuous and mixed discrete-continuous domains. This is a significant contribution as it addresses a major limitation of existing NPLP systems, which are restricted to discrete and finite probability distributions. The ability to handle continuous variables opens up new applications and makes the system more versatile.\n\n2. **Weighted Model Integration (WMI)**: The paper leverages WMI to handle the mixed discrete-continuous domain, which is a novel and significant approach. The integration of WMI with neural networks and logic programming is a key technical contribution. The authors also provide a practical implementation of WMI that supports differentiability, which is a first in the WMI literature.\n\n3. **Theoretical Contributions**: The paper includes theoretical contributions such as the proof of the correctness of the relation between the semantics of the hybrid probabilistic programming language and WMI inference (Proposition 4.1). Additionally, the proof of Proposition 4.2, which extends the result of Petersen et al. (2021) to the derivatives of relaxed functions, is a novel theoretical contribution.\n\n#### Empirical Evaluation\n1. **Toy Applications**: The paper evaluates DeepSeaProbLog on toy applications, which is a common practice in the early stages of research. However, the reviewers have pointed out the need for more extensive and diverse empirical evaluations. The authors have addressed some of these concerns by adding more generated examples and discussing the use of real meteorological data.\n\n2. **Comparison with Baselines**: The paper compares DeepSeaProbLog with neural networks and other NeSy baselines, showing improvements in data efficiency and generalization performance. However, the reviewers have suggested additional comparisons, particularly in the pure discrete setting and with more complex benchmarks.\n\n#### Clarity and Quality\n1. **Clarity**: The paper is generally well-written, and the authors have provided detailed responses to the reviewers' comments. However, some reviewers have noted that the paper could benefit from more clarity, particularly in explaining the handling of discrete random variables and the semantics of the language.\n\n2. **Quality**: The technical content is sound, and the claims are well-supported. The authors have addressed the reviewers' concerns about the correctness and reproducibility of the experiments.\n\n#### Incrementality\n1. **Incrementality Concerns**: Some reviewers have raised concerns about the incrementality of the work, suggesting that it is a small extension of DeepProbLog. However, the authors have argued that the extension to continuous variables and the integration of WMI are significant contributions that go beyond a simple incremental improvement.\n\n### Final Decision\nDespite the concerns raised by some reviewers, the paper makes significant contributions in extending NPLP to handle continuous variables, integrating WMI, and providing a practical implementation. The theoretical contributions and the potential impact on the field of neural-symbolic AI are substantial. While the empirical evaluation could be more extensive, the authors have addressed many of the reviewers' concerns and provided additional details and experiments.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\n#### Strengths:\n1. **Dataset Quality and Uniqueness**:\n   - The RuDar dataset is comprehensive and includes 3D radar echo observations, which is a significant improvement over existing datasets that often provide only single-level or aggregated data.\n   - The dataset covers multiple climate zones and a large temporal span, making it valuable for a wide range of precipitation nowcasting tasks.\n   - The inclusion of orography information adds another layer of complexity and realism to the dataset.\n\n2. **Baseline Evaluation**:\n   - The authors have evaluated several baselines, including optical flow and neural network models, which provides a solid foundation for comparing different approaches.\n   - The addition of Earthformer results, even if limited to intensity inputs, shows the authors' willingness to address the reviewers' concerns and enhance the paper.\n\n3. **Public Availability**:\n   - Both the dataset and the code for data processing and model preparation are publicly available, which is crucial for reproducibility and further research.\n\n4. **Clarity and Reproducibility**:\n   - The paper is well-written and easy to understand, with clear descriptions of the dataset and evaluation methods.\n   - The authors have provided detailed information about the data acquisition process and the distribution of the dataset.\n\n#### Weaknesses:\n1. **Methodological Novelty**:\n   - The paper primarily focuses on the dataset and baseline evaluations, with no significant methodological novelties.\n   - The baseline methods used are relatively simple, and the authors have not conducted extensive ablation studies or evaluations with more advanced models like transformers.\n\n2. **Evaluation Metrics**:\n   - The evaluation metrics used (MSE and F1) are standard but may not fully capture the nuances of precipitation nowcasting. Additional metrics like IoU (CSI) could provide a more comprehensive evaluation.\n\n3. **Cross-Dataset Evaluation**:\n   - The paper lacks cross-dataset evaluations, which would help in understanding the generalizability of the models and the dataset.\n\n4. **Future Work**:\n   - The authors mention potential applications in other ML tasks (e.g., data shift studying, anomaly detection) but do not provide experimental results to support these claims. This could be addressed in future work.\n\n#### Reviewer Feedback:\n- **Reviewer 1** and **Reviewer 2** both highlight the need for more advanced baseline models and cross-dataset evaluations.\n- **Reviewer 3** emphasizes the importance of methodological novelties and the potential limitations of the dataset's coarse resolution.\n\n### Final Decision\nWhile the paper has several strengths, particularly in the quality and uniqueness of the dataset, it lacks significant methodological novelties and comprehensive evaluations. The paper is well-written and the dataset is valuable, but it may not meet the high standards of a top-tier conference like ICLR, which typically expects more innovative contributions.\n\nFinal Decision: **Reject**\n\nHowever, the authors are encouraged to submit the paper to a more appropriate venue, such as a specialized journal or conference in the field of meteorology or geospatial data, where the dataset and its applications can have a significant impact."
    }
  ]
}