{
  "summary": {
    "model": "gpt-4o-mini",
    "total_papers": 100,
    "correct_predictions": 85,
    "accuracy": 85.0,
    "confusion_matrix": [
      [
        72,
        4
      ],
      [
        11,
        13
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 94.73684210526315,
        "precision": 86.74698795180723
      },
      "Accept": {
        "total": 24,
        "recall": 54.166666666666664,
        "precision": 76.47058823529412
      }
    }
  },
  "results": [
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Deep Interaction Processes for Time-Evolving Graphs\" presents a novel approach to modeling continuous time-evolving graphs using a deep neural network framework based on temporal point processes. The authors propose a mixture of temporal cascades and introduce mechanisms such as time gates and attention to enhance the model's performance in interaction prediction and classification tasks.\n\n#### Strengths:\n1. **Relevance of the Topic**: The modeling of time-evolving graphs is a significant area of research with practical applications in various domains, including finance and social networks. The paper addresses a relevant and timely problem.\n2. **Experimental Results**: The authors provide empirical evidence of the effectiveness of their approach through experiments on multiple datasets, demonstrating improved performance over existing methods.\n3. **Innovative Mechanisms**: The introduction of time gates and a selection mechanism to weight important nodes is a noteworthy contribution that adds complexity and potential to the model.\n\n#### Weaknesses:\n1. **Novelty Concerns**: Multiple reviewers raised concerns about the novelty of the contributions. The use of LSTMs and attention mechanisms has been explored in prior works, and the paper does not sufficiently differentiate its contributions from existing literature. The authors' claims of novelty are not convincingly substantiated, particularly regarding the stacked architecture and the specific combination of modules.\n2. **Insufficient Justification and Analysis**: Reviewers pointed out that the paper lacks a thorough analysis of the proposed components. For instance, the ablation studies do not provide insights into how different parameters (like K and alpha) affect performance. The authors need to demonstrate the significance of their architectural choices more clearly.\n3. **Comparison with Baselines**: The omission of comparisons with important recent works, such as JODIE, is a significant oversight. This limits the paper's ability to position itself within the current state of the art effectively.\n4. **Clarity and Presentation**: The paper is criticized for being difficult to understand, with unclear explanations of how the proposed methods work together. The authors have made some improvements in response to reviewer comments, but the overall clarity still needs enhancement. The introduction of an overview diagram is a positive step, but more clarity in the text is necessary.\n\n#### Reviewer Feedback:\n- Reviewers unanimously highlighted the need for a more detailed analysis of the model's components and their contributions to performance.\n- The lack of novelty and insufficient justification for the proposed methods were major concerns.\n- The experimental setup and results need to be more rigorously analyzed, especially in terms of parameter comparisons and the implications of the findings.\n\n#### Author Responses:\nThe authors have made efforts to address some of the reviewers' concerns, including adding comparisons with JODIE and providing additional analyses. However, the responses do not fully alleviate the concerns regarding novelty and the depth of analysis. The improvements made in the revised version are commendable, but they may not be sufficient to meet the standards of a top-tier conference.\n\n### Conclusion\nWhile the paper addresses an important problem and presents some innovative ideas, the concerns regarding novelty, insufficient analysis, and clarity are significant. The paper does not convincingly demonstrate that its contributions are sufficiently novel or impactful compared to existing literature. Therefore, it is recommended that the paper be rejected for publication at this time.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Score and Lyrics-Free Singing Voice Generation\" presents a novel approach to generating singing voices without relying on pre-assigned scores or lyrics. The authors explore three different schemes for generating singing voices: free singer, accompanied singer, and solo singer. The paper claims to be the first to tackle unconditional singing voice generation, which is a significant contribution to the field.\n\n#### Strengths:\n1. **Novelty**: The paper addresses a new problem in the domain of singing voice synthesis, which is a notable contribution. The exploration of unconditional singing voice generation is innovative and could pave the way for future research.\n2. **Effort in Data Preparation**: The authors created their own dataset and designed their tasks, which demonstrates a significant amount of work and commitment to the research.\n3. **Technical Implementation**: The use of GANs for audio generation is well-explained, and the authors provide a detailed description of their methods and architecture.\n4. **Open Source Code**: The authors have made their code available, which is a positive step towards reproducibility in research.\n\n#### Weaknesses:\n1. **Evaluation Metrics**: A major concern raised by multiple reviewers is the lack of robust evaluation metrics. The proposed metrics (vocalness, average pitch) do not adequately capture the quality of singing voice generation. The authors acknowledge this limitation but do not provide sufficient justification for their chosen metrics or how they relate to the quality of singing.\n2. **Lack of Baselines**: Initially, the paper did not compare its results with existing methods, which is critical for establishing the effectiveness of the proposed approach. Although the authors have added comparisons with Sinsy and Synthesizer V in their revisions, the initial lack of baselines weakened the paper's credibility.\n3. **Motivation and Clarity**: The motivation for exploring the three different schemes is not clearly articulated. Reviewers noted that the paper could benefit from a more thorough discussion of the implications and potential applications of the proposed methods.\n4. **Literature Review**: The literature review was deemed insufficient, particularly concerning existing neural network-based singing voice synthesis methods. This gap could mislead readers regarding the novelty and context of the work.\n5. **Confusion in Data Description**: The description of the training and testing datasets was initially unclear, leading to confusion about the scope and applicability of the results.\n\n#### Reviewer Feedback and Author Response:\nThe authors have made significant revisions in response to reviewer feedback, including:\n- Adding baseline comparisons with established singing voice synthesis systems.\n- Expanding the evaluation metrics and providing more context for the results.\n- Clarifying the training data and its relevance to the tasks.\n\nWhile these revisions address some of the concerns raised by the reviewers, the fundamental issues regarding the evaluation metrics and the clarity of motivation remain partially unresolved. The paper still lacks a comprehensive evaluation framework that convincingly demonstrates the superiority of the proposed methods over existing approaches.\n\n### Conclusion\nGiven the novelty of the research and the authors' efforts to address reviewer concerns, the paper has potential. However, the weaknesses in evaluation and clarity are significant enough to warrant a rejection at this time. The authors should focus on strengthening their evaluation framework and providing clearer motivations for their work before resubmitting to a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Domain Adaptive Multibranch Networks\" presents a novel approach to unsupervised domain adaptation by proposing a framework that allows different domains to undergo different sequences of operations, thereby enabling a more flexible and potentially more effective feature representation for recognition tasks. The reviewers generally recognize the novelty and potential of the proposed method, but they also raise significant concerns regarding the experimental validation and comparison with state-of-the-art methods.\n\n### Detailed Reasoning:\n\n1. **Novelty and Contribution**:\n   - The idea of using adaptive computation graphs for domain adaptation is indeed novel, and the reviewers appreciate the approach of allowing different domains to select their own computational paths. This flexibility could lead to better performance in domain adaptation tasks, which is a relevant and important area in computer vision.\n\n2. **Experimental Validation**:\n   - A major concern across all reviewers is the lack of comprehensive experimental validation. While the authors have conducted experiments with RPT and DANN, these are not considered state-of-the-art methods in the domain adaptation literature. The reviewers emphasize the necessity of comparing the proposed method against leading techniques like CDAN, which is crucial to establish the effectiveness of the proposed approach.\n   - The authors have acknowledged this gap and have indicated that they will include additional experiments in the revised version. However, the initial submission lacked sufficient experimental evidence to convincingly support the claims made about the method's effectiveness.\n\n3. **Ablation Studies and Hyperparameter Sensitivity**:\n   - Reviewers pointed out the absence of ablation studies and sensitivity analyses, which are essential for understanding the impact of various design choices in the proposed framework. The authors have committed to including these analyses in the revised version, which is a positive step, but the initial lack of such studies raises concerns about the robustness of the findings.\n\n4. **Clarity and Presentation**:\n   - The paper is generally well-written and easy to follow, which is a positive aspect. However, some terminological issues and the unusual referencing style were noted, and the authors have agreed to address these in the revised manuscript.\n\n5. **Response to Reviewer Comments**:\n   - The authors have provided thoughtful responses to the reviewers' comments and have indicated their willingness to improve the manuscript based on the feedback received. This is a good sign of their commitment to enhancing the quality of their work.\n\n### Conclusion:\nWhile the proposed method shows promise and the authors are willing to make necessary improvements, the initial submission lacks sufficient experimental validation and comparison with state-of-the-art methods, which are critical for acceptance at a top-tier conference. The authors' commitment to addressing these issues in a revised version is commendable, but the current state of the paper does not meet the high standards expected for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" presents a novel approach to training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD). The authors claim that their method is more efficient in terms of execution time and memory usage compared to traditional Expectation-Maximization (EM) algorithms, particularly in high-dimensional settings. They propose several innovations, including a new regularizer, a method for enforcing constraints during SGD, and a simplification of the GMM model based on local principal directions.\n\n### Strengths:\n1. **Relevance**: The topic of GMMs and their training in high-dimensional spaces is relevant to the field of machine learning, particularly in unsupervised learning contexts.\n2. **Novelty**: The introduction of a regularization technique to avoid pathological local minima and the use of SGD for GMM training are interesting contributions.\n3. **Implementation**: The authors provide a publicly available TensorFlow implementation, which is a positive aspect for reproducibility and practical application.\n\n### Weaknesses:\n1. **Incremental Contribution**: Several reviewers noted that the contributions appear incremental and do not significantly advance the state of the art. The authors' approach lacks novelty compared to existing methods, particularly in the context of online EM and other recent advancements in GMM training.\n2. **Experimental Evaluation**: The experimental results are not convincing. The datasets used (e.g., MNIST and SVHN) do not adequately demonstrate the method's effectiveness in truly high-dimensional settings. Additionally, there is a lack of comprehensive comparisons with existing methods, particularly regarding convergence guarantees and performance metrics.\n3. **Theoretical Justifications**: The paper lacks rigorous theoretical guarantees for convergence and performance, which is a significant drawback, especially for a method that aims to replace traditional approaches like EM.\n4. **Clarity and Justification**: Several sections of the paper, particularly those discussing the max-component approximation and the regularization technique, lack clear motivation and justification. Reviewers pointed out that the authors did not adequately address standard practices in the field, such as the log-sum-exp trick, which raises concerns about the authors' understanding of the existing literature.\n\n### Reviewer Concerns:\n- Reviewers expressed strong concerns about the clarity of the proposed methods and their justifications. For example, the motivation for the max-component approximation and the regularization technique was not sufficiently clear.\n- The experimental results were criticized for not being robust enough to support the claims made by the authors. There were also requests for additional details regarding the experimental setup and comparisons with EM.\n- The authors' responses to reviewer comments indicate an awareness of the issues raised but do not convincingly address the fundamental concerns about the paper's contributions and experimental rigor.\n\n### Conclusion:\nGiven the incremental nature of the contributions, the lack of robust experimental validation, and the insufficient theoretical backing, the paper does not meet the standards expected for publication at a top-tier conference. While the topic is relevant and the approach has potential, the execution and presentation fall short of what is required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to multi-agent coordination through a modular framework that emphasizes skill behavior diversification. The authors aim to address the challenge of coordinating multiple end-effectors in complex manipulation tasks, drawing inspiration from human learning strategies.\n\n#### Strengths:\n1. **Novelty and Relevance**: The paper tackles an important problem in the field of multi-agent reinforcement learning (MARL) by focusing on skill coordination, which is crucial for complex tasks. The approach of learning sub-skills and then coordinating them is innovative and relevant to current research trends.\n\n2. **Empirical Results**: The authors provide empirical evidence demonstrating that their method outperforms baseline approaches in various manipulation tasks. This is a strong point, as it shows the practical applicability of their framework.\n\n3. **Clarity of Presentation**: The paper is generally well-structured, and the authors have made efforts to clarify their methodology in response to reviewer comments. The inclusion of additional experiments and clarifications strengthens the paper.\n\n4. **Addressing Reviewer Concerns**: The authors have made substantial revisions based on reviewer feedback, including the addition of new baselines, clarification of notations, and improved presentation of results. This responsiveness indicates a commitment to improving the quality of their work.\n\n#### Weaknesses:\n1. **High Variance in Performance**: Reviewer 1 pointed out that the training curves exhibit high variance, which raises concerns about the robustness of the proposed method. While the authors have addressed this by running more seeds and applying moving averages, the initial high variance could indicate underlying issues that need further investigation.\n\n2. **Dependence on Predefined Subtasks**: The method relies on the specification of subtasks in advance, which may limit its applicability to more general scenarios. Reviewer 3 highlighted this concern, suggesting that the approach may not be as flexible as desired. The authors' response indicates that they believe this is a necessary trade-off, but it remains a limitation.\n\n3. **Lack of Strong Baselines**: Although the authors added new baselines, the absence of a strong baseline that uses DIAYN without predefined subtasks makes it difficult to fully evaluate the importance of expert knowledge in their approach. This could be a significant oversight in demonstrating the method's effectiveness.\n\n4. **Inconsistencies in Notation and Clarity**: While the authors have made efforts to improve clarity, some reviewers still found inconsistencies in notation and presentation. This could hinder the reader's understanding of the framework.\n\n5. **Limited Discussion of Related Work**: The authors could benefit from a more thorough discussion of alternative multi-agent methods, as suggested by Reviewer 1. This would provide a clearer context for their contributions and highlight the novelty of their approach.\n\n### Conclusion\nOverall, the paper presents a promising approach to a relevant problem in MARL, with empirical results supporting its effectiveness. However, the concerns regarding high variance in performance, reliance on predefined subtasks, and the need for stronger baselines are significant. While the authors have made commendable revisions, the limitations may affect the paper's impact and applicability in broader contexts.\n\nGiven these considerations, I recommend that the paper be accepted, but with the understanding that the authors should continue to address the highlighted weaknesses in future work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to multi-agent coordination through a modular framework that emphasizes the learning of sub-skills and their subsequent coordination. The authors draw parallels between human skill acquisition and robotic manipulation, which is a compelling motivation for the research.\n\n#### Strengths:\n1. **Novelty and Relevance**: The problem of coordinating multiple agents with diverse skills is highly relevant in the field of robotics and reinforcement learning. The approach of using skill behavior diversification to enhance coordination is innovative and addresses a significant gap in existing literature.\n\n2. **Empirical Validation**: The authors provide empirical results demonstrating the effectiveness of their method in complex manipulation tasks. The results indicate that their approach outperforms baseline methods, which is a strong point in favor of the paper.\n\n3. **Clarity of Presentation**: The paper is generally well-structured, and the authors have made efforts to clarify their methodology in response to reviewer comments. The inclusion of additional experiments and clarifications in the author response strengthens the paper.\n\n#### Weaknesses:\n1. **High Variance in Results**: Reviewer 1 pointed out the high variance in performance, which raises concerns about the robustness of the proposed method. While the authors have addressed this by running more seeds and applying moving averages, the initial variance could indicate underlying issues that need further exploration.\n\n2. **Dependence on Predefined Subtasks**: The reliance on predefined subtasks limits the generalizability of the approach. While the authors argue that this is necessary for complex tasks, it may hinder the applicability of their method in more dynamic or less structured environments.\n\n3. **Inconsistencies and Clarity Issues**: Despite improvements, some reviewers noted that notations were inconsistent and that certain figures could be clearer. While the authors have made revisions, the clarity of presentation is crucial for a top-tier conference, and lingering issues could detract from the paper's impact.\n\n4. **Limited Discussion of Alternatives**: The paper could benefit from a more in-depth discussion of alternative multi-agent methods. While the authors have addressed this concern, the lack of a comprehensive comparison with recent methods may weaken the paper's position in the field.\n\n5. **Scientific Insight**: Reviewer 2 expressed concerns about the scientific insight provided by the paper, suggesting that it does not contribute significantly to the theoretical understanding of multi-agent reinforcement learning. This is a critical aspect for acceptance in a top-tier conference.\n\n#### Conclusion:\nWhile the paper presents a novel and relevant approach to multi-agent coordination and has shown promising empirical results, it also has notable weaknesses, particularly regarding the robustness of results, reliance on predefined subtasks, and clarity of presentation. The authors have made commendable efforts to address reviewer concerns, but the lingering issues may prevent it from meeting the high standards expected at a top-tier conference.\n\nGiven these considerations, I recommend that the paper be rejected for publication, with the suggestion that the authors address the highlighted weaknesses and resubmit in the future.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents an interesting approach to modeling the development of the primate visual system using artificial neural networks (ANNs). The authors propose methods to significantly reduce the number of supervised synaptic updates required to achieve a high match to the adult primate ventral stream, which is a relevant and timely topic in computational neuroscience.\n\n### Strengths:\n1. **Novelty and Relevance**: The paper addresses a critical issue in the field of computational neuroscience—how to create models that are not only effective but also biologically plausible. The exploration of reducing supervised updates while maintaining performance is a significant contribution.\n2. **Methodological Rigor**: The authors present three complementary strategies that collectively reduce the number of supervised updates while achieving a substantial match to the ventral stream. This is a noteworthy achievement that could have implications for future research in both neuroscience and machine learning.\n3. **Clarity and Structure**: The paper is well-written and structured, making it accessible to readers. The context of the work is clearly described, and the experiments are thorough.\n\n### Weaknesses:\n1. **Biological Plausibility**: Several reviewers raised concerns about the biological relevance of the proposed methods, particularly the weight compression (WC) technique. The authors acknowledge that their approach is not a direct representation of biological processes but rather a hypothesis. This lack of direct biological grounding may limit the impact of the findings.\n2. **Interpretation of Results**: Reviewers pointed out that the use of BrainScore as a metric for evaluating model performance could be misleading. The authors need to clarify that achieving a high BrainScore does not necessarily equate to a high biological match. This is a critical point that could affect the perceived significance of their results.\n3. **Statistical Analysis**: There were concerns about the lack of statistical significance testing in the results. While the authors have conducted additional analyses in response to reviewer comments, the initial lack of rigor in this area raises questions about the robustness of their findings.\n\n### Reviewer Concerns:\n- Reviewers expressed doubts about the assumptions made in the paper, particularly regarding the analogy between ANN training and biological development. The authors have attempted to address these concerns in their responses, but the fundamental issue remains that the proposed methods may not fully capture the complexities of biological learning.\n- The paper's claims about the match to the brain need to be more carefully articulated to avoid overstating the implications of the findings. The authors have acknowledged this and indicated they will revise the text for clarity.\n\n### Conclusion:\nWhile the paper presents innovative ideas and methodologies, the concerns raised by the reviewers regarding biological plausibility, interpretation of results, and statistical rigor are significant. The authors have made efforts to address these issues, but the foundational concerns about the biological relevance of their approach and the interpretation of BrainScore metrics remain. Given the high standards of a top-tier conference, the paper may not yet meet the necessary criteria for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning to Transfer via Modelling Multi-level Task Dependency\" presents a novel approach to multi-task learning by modeling task dependencies at multiple levels using an attention mechanism. While the authors claim that their method improves upon existing multi-task learning frameworks by addressing the limitations of assuming strong correlations between tasks, the reviewers have raised several significant concerns that must be addressed.\n\n1. **Novelty and Contribution**: \n   - The authors assert that their work captures both general task dependency and data-specific task dependency, particularly in hierarchical data structures like text and graphs. However, several reviewers pointed out that the concept of modeling task relationships is not new and has been explored in prior works, notably the \"Taskonomy\" paper. The authors need to provide a clearer distinction between their approach and existing methods, which they have attempted to do in their rebuttal. However, the reviewers still found the distinctions insufficiently convincing.\n\n2. **Methodological Clarity**: \n   - Reviewers highlighted that the paper lacks clarity in its presentation, particularly regarding the definitions and extraction methods of key components like the general task dependency matrix and the attention mechanism. The authors have acknowledged these issues and claim to have added more details in their revisions, but the extent to which these changes address the reviewers' concerns remains unclear.\n\n3. **Experimental Validation**: \n   - The experimental section has been criticized for not including sufficient baselines and comparisons with relevant existing methods. The authors have agreed to add comparisons with methods that do not use attention, which is a positive step. However, the reviewers still expressed concerns about the evaluation setup, particularly regarding the potential overlap of training and testing data in multi-label settings. The authors clarified that this overlap does not occur, but the reviewers' concerns about the thoroughness of the evaluation remain.\n\n4. **Grammar and Presentation**: \n   - Multiple reviewers noted that the paper suffers from grammatical issues and poor presentation, making it difficult to follow. The authors have acknowledged these issues and stated that they have revised the manuscript for clarity. However, the effectiveness of these revisions is uncertain.\n\n5. **Reviewer Consensus**: \n   - The overall sentiment among the reviewers leans towards rejection, primarily due to the lack of clarity, insufficient novelty, and the need for more rigorous experimental validation. While the authors have made efforts to address some of these concerns, the reviewers still find the paper lacking in several critical areas.\n\n### Conclusion\nGiven the significant concerns raised by the reviewers regarding the novelty of the approach, clarity of the methodology, and thoroughness of the experimental validation, I believe that the paper does not meet the standards required for acceptance at a top-tier conference. The authors have made some improvements in their rebuttal, but these do not sufficiently address the fundamental issues identified by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A generalized probability kernel on discrete distributions and its application in two-sample test\" presents a novel approach to defining a generalized probability kernel (GPK) for discrete distributions, aiming to extend existing discrepancy statistics like the maximum mean discrepancy (MMD). The authors claim that their work connects discrete distribution-property estimation with kernel-based hypothesis testing, potentially opening new avenues for research.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - The idea of generalizing MMD to a kernel-based framework for discrete distributions is interesting and has potential implications for statistical testing. However, the reviewers have pointed out that the theoretical contributions are not sufficiently innovative and largely follow existing literature. The lack of a clear demonstration of the utility of the proposed estimators raises concerns about the paper's novelty.\n\n2. **Theoretical Rigor**:\n   - Multiple reviewers highlighted significant issues with the definitions and theoretical foundations presented in the paper. For instance, the confusion surrounding the terminology (e.g., mixing up \"kernel\" and \"distance\") and the lack of clarity in definitions (e.g., Definition 2 and Definition 6) suggest that the theoretical framework is not well-established. The absence of proofs for key theorems and the incomplete nature of several sections further undermine the paper's rigor.\n\n3. **Experimental Validation**:\n   - A critical aspect of any statistical method is its empirical validation. The reviewers unanimously noted the absence of experimental evaluations, which is a significant shortcoming. Without experiments to demonstrate the effectiveness of the proposed GPK and its application in two-sample tests, it is difficult to assess its practical relevance.\n\n4. **Presentation and Clarity**:\n   - The paper has been criticized for poor presentation, including numerous typos, grammatical errors, and unclear notation. While the authors have acknowledged these issues and made efforts to revise the manuscript, the extent of the problems suggests that the paper may not meet the standards expected at a top-tier conference.\n\n5. **Reviewer Feedback and Author Response**:\n   - The authors' responses indicate an awareness of the paper's shortcomings and a willingness to improve. However, the revisions made do not seem to adequately address the core concerns raised by the reviewers, particularly regarding the theoretical contributions and the lack of empirical validation.\n\n6. **Overall Impression**:\n   - While the topic is relevant and has potential, the execution of the paper is lacking in several critical areas. The theoretical framework is not sufficiently robust, the empirical validation is missing, and the presentation is not up to the standards expected for publication in a top-tier conference.\n\nGiven these considerations, the paper does not currently meet the necessary criteria for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Complex Query Answering with Neural Link Predictors\" presents a novel framework for answering complex queries over incomplete Knowledge Graphs (KGs) using neural link predictors. The authors propose a method called Continuous Query Decomposition (CQD), which translates complex queries into an end-to-end differentiable objective, allowing for efficient query answering without the need for extensive training on complex queries.\n\n### Strengths:\n1. **Novelty and Relevance**: The problem of answering complex queries on incomplete KGs is significant in the field of data mining and knowledge representation. The proposed method addresses this challenge effectively, which is a strong point in favor of the paper.\n  \n2. **Performance**: The experimental results demonstrate that CQD outperforms state-of-the-art methods while requiring significantly less training data. The reported improvements in Hits@3 (ranging from 8% to 40%) across various datasets are compelling and suggest that the method is both effective and efficient.\n\n3. **Explainability**: The authors provide insights into the explainability of their model, which is an important aspect of machine learning applications, particularly in knowledge graphs. This is a valuable contribution to the field.\n\n4. **Clarity and Structure**: The paper is generally well-structured and clear, making it accessible to readers. The authors have addressed many of the reviewers' concerns in their responses, indicating a willingness to improve the paper.\n\n### Weaknesses:\n1. **Mathematical Formalism**: Some reviewers noted that the paper contains excessive mathematical formalism, which could be simplified. While the authors argue that this is necessary for clarity and precision, it may hinder understanding for some readers.\n\n2. **Comparative Analysis**: There are concerns regarding the comparison with existing methods like GQE and Q2B. Some reviewers pointed out that the differences between CQD and these methods could be articulated more clearly, particularly regarding the optimization techniques used.\n\n3. **Timing and Complexity**: The authors have provided additional timing results, but there are still questions about the scalability of the combinatorial optimization method for longer chains. The exponential growth in complexity with respect to the number of hops could be a limitation in practical applications.\n\n4. **Examples and Context**: Some examples used in the paper were criticized for being artificial. The authors have acknowledged this and have made improvements, but the relevance of examples in illustrating the method's applicability remains crucial.\n\n5. **Ablation Studies**: While the authors have conducted some ablation studies, further exploration of the sensitivity of the method to different neural link predictors (e.g., DistMult vs. ComplEx) would strengthen the paper.\n\n### Conclusion:\nOverall, the paper presents a significant advancement in the field of complex query answering over knowledge graphs. The strengths, particularly in terms of performance and explainability, outweigh the weaknesses. The authors have shown responsiveness to reviewer feedback and have made substantial improvements to the paper. While there are areas for further clarification and enhancement, the contributions are valuable and relevant to the community.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Extreme Classification via Adversarial Softmax Approximation\" presents a novel approach to extreme classification by improving the negative sampling method used in softmax classifiers. The authors propose an adversarial sampling mechanism that enhances the gradient signal during training, which is crucial given the challenges posed by a large number of classes. The paper claims significant improvements in training time and convergence rates compared to existing methods.\n\n### Strengths:\n1. **Novel Contribution**: The introduction of an adversarial model for negative sampling is a fresh perspective that addresses the limitations of traditional uniform negative sampling. The theoretical insights provided, particularly the formalization of the signal-to-noise ratio, add depth to the contribution.\n2. **Clarity and Presentation**: The reviewers noted that the paper is well-structured and accessible, making it suitable for both experts and non-experts in the field.\n3. **Experimental Results**: The authors provide empirical evidence of their method's effectiveness on large-scale datasets, demonstrating a significant reduction in training time.\n\n### Weaknesses:\n1. **Literature Context**: Reviewer 2 and Reviewer 3 pointed out that the authors did not sufficiently contextualize their work within the existing literature on negative sampling and softmax approximations. Several relevant works were not discussed, which could mislead readers regarding the novelty and positioning of the proposed method.\n2. **Experimental Completeness**: The experimental evaluation lacks comparisons with some state-of-the-art methods, particularly those that do not rely on negative sampling. This omission limits the understanding of the proposed method's performance relative to the best available techniques.\n3. **Evaluation Metrics**: The reviewers suggested that the authors should evaluate their method on tail-label performance and other metrics beyond accuracy, which is critical in extreme classification scenarios where label distributions are often imbalanced.\n\n### Author Response:\nThe authors acknowledged the gaps in the literature review and committed to addressing these in the final version. They also agreed to include additional experiments on smaller datasets, which could provide valuable insights into the performance of their method compared to full softmax classification. However, they maintained that their focus on negative sampling justified the limited comparisons with non-negative sampling methods.\n\n### Conclusion:\nWhile the paper presents a novel approach with promising results, the lack of comprehensive literature context and experimental completeness raises concerns about its overall contribution to the field. The authors' commitment to addressing these issues in the final version is a positive sign, but the current state of the paper does not fully meet the standards expected at a top-tier conference. The reviewers' critiques highlight significant areas for improvement that are essential for a robust publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Differentiable Hebbian Consolidation for Continual Learning\" addresses a significant challenge in the field of machine learning, specifically in continual learning and catastrophic forgetting. The authors propose a novel framework that integrates Differentiable Hebbian Plasticity (DHP) into a softmax layer, aiming to enhance the retention of learned representations over time. The paper is well-motivated, drawing on cognitive principles and neurobiological substrates, and presents a method that is claimed to outperform existing baselines on various benchmarks.\n\n### Strengths:\n1. **Novelty and Relevance**: The paper tackles an important and timely issue in machine learning, which is the challenge of catastrophic forgetting in neural networks. The proposed method, which combines fast and slow weights, is a fresh perspective in the continual learning literature.\n   \n2. **Implementation and Accessibility**: The authors provide clear pseudocode for their method, indicating that it can be easily implemented in standard deep learning frameworks. This is a positive aspect as it enhances the reproducibility of the research.\n\n3. **Empirical Results**: The authors report improvements over simple baselines and demonstrate the method's effectiveness on various datasets, including their own class-imbalanced version of Permuted MNIST.\n\n4. **Response to Reviewers**: The authors have made a commendable effort to address the reviewers' comments and concerns, providing clarifications and additional experiments in their rebuttal.\n\n### Weaknesses:\n1. **Empirical Evaluation**: Several reviewers pointed out that the empirical evaluation could be stronger. While the authors have added experiments on CIFAR-10/100, the results still show that the proposed method does not consistently outperform established methods like Synaptic Intelligence (SI) across all splits. The performance of DHP Softmax alone is often not competitive, and it appears to work best in conjunction with other methods, which raises questions about its standalone efficacy.\n\n2. **Comparative Analysis**: The reviewers noted that the paper lacks a thorough comparison with other Hebbian-based approaches and task-specific methods. The authors' rebuttal attempts to clarify this but does not fully address the need for more comprehensive empirical comparisons.\n\n3. **Complexity and Clarity**: Some reviewers found the presentation of the method and its underlying principles to be somewhat convoluted. While the authors have added figures and clarified terminology, the complexity of the concepts may still hinder understanding for readers not deeply familiar with Hebbian learning.\n\n4. **Limited Scope of Experiments**: The reviewers suggested that the authors should explore more sophisticated datasets and tasks, such as CIFAR-100 or ILSVRC, to demonstrate the scalability and robustness of their method. The authors acknowledge this limitation but do not provide sufficient justification for not exploring these avenues more thoroughly.\n\n### Conclusion:\nWhile the paper presents a novel approach to a significant problem in machine learning and has made strides in addressing reviewer concerns, the empirical results are not sufficiently compelling to warrant acceptance at a top-tier conference. The method does not consistently outperform existing techniques, and the lack of comprehensive comparisons and evaluations on more challenging datasets limits its impact. Therefore, despite its strengths, the paper falls short of the high standards expected for publication in a prestigious venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" attempts to address the exploration-exploitation trade-off in Monte Carlo Tree Search (MCTS) within the context of reinforcement learning. However, the reviews provided by the three reviewers indicate significant issues that undermine the paper's suitability for publication in a top-tier conference.\n\n### Detailed Reasoning:\n\n1. **Clarity and Writing Quality**: \n   - All three reviewers highlighted severe issues with the writing quality of the paper. Reviewer 2 explicitly stated that they struggled to understand the abstract and the overall content due to grammatical issues and unclear phrasing. Reviewer 3 echoed this sentiment, noting that the paper was filled with incomprehensible passages and lacked clarity. The writing quality is a fundamental aspect of academic publishing, and the inability to convey ideas clearly is a critical flaw.\n\n2. **Lack of References**: \n   - Reviewer 1 pointed out the absence of citations, which is a significant oversight in any academic paper. The lack of references not only undermines the credibility of the research but also indicates a lack of engagement with existing literature. This is particularly concerning in a field as well-studied as reinforcement learning and MCTS.\n\n3. **Conceptual Clarity**: \n   - The reviewers expressed confusion regarding the core contributions of the paper. Reviewer 1 mentioned that the paper seems to be a \"less than half-baked\" effort, while Reviewer 3 described it as \"merely a set of notes.\" The lack of a clear and coherent presentation of the research question, methodology, and results makes it difficult to assess the validity and significance of the proposed approach.\n\n4. **Response from Authors**: \n   - The authors' response indicates an awareness of their shortcomings, particularly in writing and referencing. However, the acknowledgment of these issues without a clear plan for addressing them in the current submission suggests that the paper is not ready for publication. The authors express a desire to improve but do not provide any concrete steps taken to rectify the issues raised by the reviewers.\n\n5. **Overall Impression**: \n   - The cumulative feedback from the reviewers suggests that the paper is not at a stage suitable for publication. The combination of poor writing, lack of references, and unclear contributions indicates that significant revisions are necessary before it could be considered for acceptance.\n\n### Conclusion:\nGiven the substantial issues identified in the reviews, including clarity, writing quality, and lack of references, it is clear that the paper does not meet the standards expected for publication in a top-tier conference. The authors would benefit from addressing these concerns thoroughly before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning hierarchical representations using a progressive learning strategy in the context of variational autoencoders (VAEs). The authors claim that their method, pro-VLAE, improves disentanglement by progressively growing the network architecture to learn independent hierarchical representations.\n\n#### Strengths:\n1. **Novelty and Relevance**: The idea of combining progressive learning with hierarchical representation learning is fresh and relevant to the field of generative models. The authors position their work as a significant contribution to the literature on disentangled representations, which is a hot topic in machine learning.\n\n2. **Experimental Validation**: The authors provide quantitative and qualitative results demonstrating the effectiveness of their approach on benchmark datasets. The rebuttal includes additional experiments that address previous concerns raised by reviewers, showing improvements over the baseline model VLAE.\n\n3. **Clear Motivation**: The paper is well-motivated, with a clear rationale for why progressive learning can enhance disentanglement. The authors articulate the benefits of their approach and how it differs from existing methods.\n\n4. **Addressing Reviewer Concerns**: The authors have made substantial efforts to address the concerns raised by reviewers in their rebuttal. They conducted additional experiments, clarified the purpose of their work, and provided more detailed comparisons with existing models.\n\n#### Weaknesses:\n1. **Clarity of Purpose**: While the authors have clarified their purpose in the rebuttal, some reviewers still found the distinction between hierarchical representation and disentangled representation unclear. This could lead to confusion about the primary contribution of the paper.\n\n2. **Experimental Comparisons**: Although the authors conducted additional experiments, some reviewers noted that the qualitative results were not as convincing as expected. The paper could benefit from more thorough comparisons with the baseline models, particularly in terms of reconstruction quality and disentanglement metrics.\n\n3. **Metric Limitations**: The proposed disentanglement metric, MIG-sup, while innovative, has limitations as it requires ground truth for generative factors. This could restrict its applicability in real-world scenarios where such ground truth is not available.\n\n4. **Ablation Studies**: While the authors conducted some ablation studies, there is still a call for more comprehensive studies to isolate the effects of different components of their approach. This would strengthen the claims made about the effectiveness of their method.\n\n5. **Hyperparameter Sensitivity**: The paper acknowledges that the method is sensitive to hyperparameters, particularly the beta parameter. While this is a valid point, it raises concerns about the robustness of the proposed method across different settings.\n\n#### Conclusion:\nOverall, the paper presents a promising approach to disentangled representation learning through progressive learning strategies. The authors have made significant improvements in response to reviewer feedback, and the additional experiments bolster the validity of their claims. However, some concerns regarding clarity, experimental rigor, and metric limitations remain.\n\nGiven the novelty of the approach, the improvements made in the rebuttal, and the potential impact on the field, I believe the paper is suitable for publication with minor revisions to address the remaining concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training based on task-specific updates. The authors claim that their method outperforms existing multitask learning approaches, particularly in the context of computer vision and natural language understanding tasks.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - The proposed method, $\\alpha$VIL, aims to improve multitask learning by leveraging auxiliary tasks more effectively through dynamic task weighting. While the authors assert that their approach is distinct from existing methods like Discriminative Importance Weighting (DIW), reviewers have raised concerns about the novelty of the method. They argue that dynamic task weighting is not a new concept and that the proposed algorithm does not significantly differ from existing approaches, particularly in its reliance on meta-learning principles.\n   - The authors acknowledge the need for a clearer distinction between their method and existing ones, which is a valid point that undermines the perceived novelty of the work.\n\n2. **Theoretical Justification**:\n   - Reviewers have pointed out the lack of theoretical justification for the proposed method. The absence of a formal analysis or theoretical grounding weakens the paper's contribution and raises questions about the robustness of the proposed approach. The authors have recognized this shortcoming and plan to address it in a future version, but this does not alleviate the current concerns.\n\n3. **Empirical Evaluation**:\n   - The empirical results presented in the paper show mixed performance across different tasks. While the authors claim improvements over baseline methods, the reviewers note that these improvements are often marginal and fall within the range of statistical insignificance. The lack of consistent superiority over established methods raises doubts about the effectiveness of the proposed approach.\n   - Reviewers have also requested additional experiments and ablation studies to better understand the efficacy of the method. The authors' acknowledgment of this need indicates that the current empirical evaluation is insufficient to fully support their claims.\n\n4. **Clarity and Presentation**:\n   - The paper is generally well-written and easy to follow, which is a positive aspect. However, the reviewers have pointed out several areas for improvement in clarity, particularly regarding the algorithm's details and the experimental setup. The authors have committed to addressing these issues in a revised version, but the current submission does not meet the clarity expected at a top-tier conference.\n\n5. **Reviewer Consensus**:\n   - The consensus among reviewers leans towards rejection, primarily due to the lack of theoretical justification, insufficient empirical results, and concerns about the novelty of the approach. While the authors have provided thoughtful responses to reviewer comments, the fundamental issues raised remain significant.\n\n### Conclusion\nGiven the concerns regarding the theoretical foundation, empirical validation, and novelty of the proposed method, the paper does not meet the high standards expected for acceptance at a top-tier conference. The authors have identified areas for improvement, but the current version does not sufficiently address the critical feedback provided by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents a novel approach to integrating linguistic and visual information for the task of image segmentation based on referring expressions. The authors argue that their method, which modulates both top-down and bottom-up visual processing with language, leads to improved performance in comparison to existing models.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - The paper introduces a dual modulation approach that integrates language throughout the visual processing pipeline. While this is an interesting concept, several reviewers noted that the novelty appears limited. Reviewer 1 pointed out that many modeling techniques for language were not explored, and Reviewer 4 expressed concerns about the conceptual motivation behind the model. The authors' rebuttal did not sufficiently address these concerns, particularly regarding the originality of their approach compared to existing models like Step-ConvRNN.\n\n2. **Empirical Results**: \n   - The empirical results show that the proposed model achieves state-of-the-art or competitive results on some datasets. However, Reviewer 2 highlighted that the model struggles on certain metrics and may be over-tuned to the validation set. Reviewer 4 echoed this concern, stating that the quantitative improvements are not substantial enough to warrant a significant conceptual contribution. The lack of statistical significance tests further undermines the robustness of the claims made by the authors.\n\n3. **Clarity and Presentation**: \n   - The paper is generally well-written and easy to follow, as noted by multiple reviewers. However, there are suggestions for improving clarity, such as adding figures to explain concepts and providing more detailed explanations of the ablation studies. The authors have indicated they will implement some of these suggestions, which could enhance the paper's clarity.\n\n4. **Response to Reviewer Comments**: \n   - The authors provided responses to reviewer comments, but many of the concerns raised were not adequately addressed. For instance, the rationale for certain design choices was not convincingly justified, and the authors did not provide sufficient evidence to support their claims about the model's performance and its conceptual contributions.\n\n5. **Future Work and Limitations**: \n   - The authors acknowledged limitations in their approach and mentioned ongoing work to analyze the interaction between language and visual processing. However, the lack of concrete results or analyses in the current submission leaves questions about the model's effectiveness and the validity of its claims.\n\n### Conclusion\nGiven the concerns regarding the novelty and significance of the contributions, the empirical results' robustness, and the adequacy of the authors' responses to reviewer comments, the paper does not meet the high standards expected for acceptance at a top-tier conference. The issues raised by the reviewers indicate that the paper requires substantial revisions and additional empirical validation before it can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" presents a novel approach to reduce training time for convolutional neural networks (CNNs) by employing a multi-precision strategy. The authors claim that their method can achieve significant speedups in training time while maintaining accuracy comparable to full-precision training.\n\n#### Strengths:\n1. **Novelty and Relevance**: The topic of low-precision training is highly relevant in the current landscape of deep learning, especially with the increasing size of models and the need for efficient training methods. The proposed MuPPET strategy introduces a new dimension by dynamically switching precision based on gradient diversity, which is an interesting approach.\n\n2. **Empirical Results**: The authors provide empirical results demonstrating that MuPPET achieves an average training-time speedup of 1.28× across several networks while maintaining accuracy. This is a positive aspect, as it shows practical applicability.\n\n3. **Generalizability**: The authors argue that their method is agnostic to the specific architecture and dataset, which could make it broadly applicable across various scenarios.\n\n#### Weaknesses:\n1. **Lack of Clarity and Justification**: Several reviewers pointed out that the rationale behind the precision-switching mechanism is not well-explained. Reviewer 1 specifically noted that the heuristic nature of the switching mechanism feels under-justified. The authors need to provide a clearer theoretical foundation for why their approach works, particularly regarding the implications of gradient co-alignment.\n\n2. **Confusing Presentation**: Reviewers highlighted issues with the clarity of the presentation, including unclear notations and confusing descriptions of the quantization scheme. This lack of clarity could hinder reproducibility, which is a critical aspect of scientific research.\n\n3. **Comparative Performance**: Reviewer 2 noted that the results are not significantly superior to existing state-of-the-art methods. The paper needs to convincingly demonstrate that MuPPET offers substantial advantages over existing techniques, not just marginal improvements.\n\n4. **Minor Issues**: There are several minor issues, including typos and unclear figures, which detract from the overall professionalism of the manuscript.\n\n5. **Response to Reviewers**: While the authors have made efforts to address reviewer comments, some responses do not fully resolve the concerns raised. For instance, the justification for the switching mechanism remains somewhat heuristic, and the empirical results do not convincingly demonstrate a clear advantage over existing methods.\n\n#### Conclusion:\nGiven the strengths and weaknesses of the paper, it is clear that while the topic is relevant and the approach has potential, the execution lacks the necessary clarity and justification to meet the standards of a top-tier conference. The concerns raised by the reviewers regarding the heuristic nature of the switching mechanism, the clarity of the presentation, and the comparative performance need to be addressed more thoroughly.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" presents a novel approach to learning Mahalanobis metric spaces by formulating the problem as an optimization task aimed at minimizing the number of violated similarity/dissimilarity constraints. The authors claim to provide the first fully polynomial time approximation scheme (FPTAS) with nearly-linear running time for this problem, leveraging tools from linear programming.\n\n#### Strengths:\n1. **Novelty and Contribution**: The authors assert that their algorithm is the first to provide a provable guarantee on the number of violated constraints for arbitrary inputs, which is a significant contribution to the field of metric learning. This claim, if substantiated, could represent a meaningful advancement over existing methods like ITML and LMNN.\n   \n2. **Theoretical Framework**: The paper attempts to bridge the gap between linear programming techniques and metric learning, which is an interesting direction. The use of LP-type problems in this context is a valuable contribution that could inspire further research.\n\n3. **Experimental Results**: The authors provide experimental results that demonstrate the performance of their algorithm against existing methods, suggesting practical applicability.\n\n#### Weaknesses:\n1. **Clarity and Novelty**: Several reviewers expressed confusion regarding the novelty of the approach. While the authors claim that their method is distinct from existing methods, the reviewers found it challenging to identify the key differences and advantages. The authors need to clarify how their approach differs fundamentally from previous works and why it is advantageous.\n\n2. **State-of-the-Art Comparison**: The paper lacks a comprehensive discussion of the state-of-the-art in the field. Reviewer comments indicate that recent relevant works were not adequately addressed, which is crucial for situating the contribution within the existing literature.\n\n3. **Theoretical Guarantees**: There is a lack of clarity regarding the theoretical guarantees of the proposed approximation. Reviewers raised questions about the computational hardness of the problem and whether the approximation strategy is novel or has been applied in similar contexts before.\n\n4. **Proof Clarity**: The proof of Lemma 2.1 and other theoretical aspects were criticized for being unclear. The authors acknowledged this and promised to clarify in the final version, but the initial lack of clarity raises concerns about the rigor of the theoretical contributions.\n\n5. **Conclusion and Structure**: The absence of a conclusion in the initial submission is a significant oversight. A well-structured conclusion is essential for summarizing the contributions and implications of the research.\n\n6. **Experimental Validation**: While the authors present experimental results, some reviewers noted that the improvements in accuracy were not clearly demonstrated. This raises questions about the practical significance of the proposed method.\n\n### Conclusion\nThe paper presents an interesting approach to a relevant problem in metric learning, but it suffers from several weaknesses that undermine its clarity and impact. The novelty of the contribution is not sufficiently articulated, and the theoretical and experimental aspects require further refinement. Given the feedback from multiple reviewers and the need for substantial improvements, I recommend rejecting the paper in its current form. However, with significant revisions addressing the concerns raised, it could potentially be a strong candidate for future submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper titled \"Cortico-cerebellar networks as decoupled neural interfaces\" presents a novel hypothesis linking the cerebellum's function to decoupled neural interfaces (DNI) in deep learning. The authors propose that the cerebellum plays a crucial role in solving the credit assignment problem in the brain, akin to how DNIs address similar issues in artificial neural networks. The paper explores this hypothesis through a series of experiments involving recurrent neural networks and presents findings that suggest the cerebellum's architecture can enhance learning in both motor and cognitive tasks.\n\n### Evaluation of Reviewer Comments\n\n**Reviewer 1** appreciates the novelty of the hypothesis and the potential impact on both neuroscience and deep learning. However, they raise concerns about clarity and the need for a more focused presentation of the model, particularly regarding the distinction between feedforward and recurrent settings. They also suggest including comparisons with existing DNI architectures to strengthen the claims.\n\n**Reviewer 2** is less enthusiastic, arguing that the paper does not provide significant new insights into either the cerebellum or DNIs. They feel that the findings merely juxtapose existing knowledge without offering substantial contributions to the field.\n\n**Reviewer 3** acknowledges the clarity of the writing and the novelty of linking the cerebellum to DNI but criticizes the lack of empirical comparisons and concrete predictions. They emphasize the need for a stronger connection between the model and experimental findings related to cerebellar function.\n\n### Author Responses\n\nThe authors have made substantial revisions in response to the reviewers' comments. They have added new figures and sections to clarify their predictions and the relationship between their model and existing experimental data. They have also addressed concerns about the clarity of their presentation and the need for comparisons with existing models.\n\n1. **Predictions and Experimental Comparisons**: The authors have introduced new figures that illustrate predictions about cerebellar function and its relevance to learning tasks. They have also conducted ablation studies to demonstrate the importance of the cerebellum in learning, which addresses Reviewer 3's concerns about empirical validation.\n\n2. **Clarification of Concepts**: The authors have made efforts to clarify the distinctions between feedforward and recurrent settings and have emphasized the role of bootstrapping in their model. This addresses Reviewer 1's concerns about clarity and focus.\n\n3. **Comparison with Existing Models**: The authors have clarified how their model differs from existing DNI architectures, which addresses Reviewer 1's request for a clearer comparison.\n\n### Overall Assessment\n\nThe paper presents a compelling hypothesis that bridges neuroscience and deep learning, which is a timely and relevant topic. The revisions made by the authors have significantly improved the clarity and depth of the manuscript. They have addressed the major concerns raised by the reviewers, particularly regarding the lack of empirical validation and the need for clearer predictions.\n\nHowever, there are still some lingering concerns about the novelty of the insights provided. Reviewer 2's critique about the paper not offering substantial new insights into either the cerebellum or DNIs remains a point of contention. While the authors have made strides in addressing this, the paper may still be perceived as lacking in groundbreaking contributions.\n\nIn conclusion, the paper has improved significantly through revisions, and the authors have made a strong case for the relevance of their work. The potential implications for both neuroscience and deep learning warrant its acceptance, especially given the novel framework it proposes.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Structure and randomness in planning and reinforcement learning\" introduces a novel algorithm called Shoot Tree Search (STS), which aims to improve planning in large state spaces by modifying the expansion phase of Monte Carlo Tree Search (MCTS) through multi-step expansion. The authors claim that STS allows for better control over the bias-variance trade-off and provides empirical evidence of its effectiveness in challenging domains such as Sokoban and Google Research Football.\n\n#### Strengths:\n1. **Novelty and Contribution**: The idea of multi-step expansion in tree search is interesting and could potentially lead to improvements in planning algorithms. The authors have attempted to position STS as a method that balances depth and breadth in search, which is a relevant topic in reinforcement learning.\n2. **Empirical Evaluation**: The paper includes experiments that demonstrate STS's performance against various baselines, showing improvements in specific environments. The authors have also provided additional experimental results in their response to reviewers, which strengthens their claims.\n3. **Clarity and Presentation**: The paper is generally well-written, and the authors have indicated a willingness to improve clarity based on reviewer feedback.\n\n#### Weaknesses:\n1. **Technical Novelty**: Several reviewers raised concerns about the novelty of STS, pointing out that multi-step expansion has been explored in previous works. The authors need to clarify how STS differs fundamentally from existing methods and why it is a significant advancement.\n2. **Experimental Rigor**: While the authors conducted experiments, some reviewers noted that more thorough ablation studies are needed to isolate the effects of multi-step expansion from other factors, such as the bias introduced by the value function approximator. The lack of comprehensive comparisons with existing methods, particularly in terms of the number of simulations, raises questions about the robustness of the findings.\n3. **Intuition and Explanation**: Reviewers expressed that the paper lacks intuitive explanations for why STS should outperform MCTS in certain scenarios. The authors need to provide clearer insights into the algorithm's advantages and how it addresses the challenges posed by sparse rewards and exploration.\n4. **Presentation Issues**: Some reviewers pointed out unclear sections and the need for better organization of the content. The authors have acknowledged these issues and expressed intent to revise the manuscript, but the current state may not meet the standards of a top-tier conference.\n\n#### Reviewer Feedback:\nThe feedback from the reviewers highlights a mix of appreciation for the idea and concerns regarding its novelty, experimental validation, and clarity. While some reviewers were more favorable, others were skeptical about the contributions and the paper's readiness for publication.\n\n#### Author Responses:\nThe authors have provided responses to the reviewers' comments and have conducted additional experiments, which is a positive sign. However, the responses do not fully address the concerns about novelty and the need for more rigorous comparisons with existing methods. The authors' explanations regarding the differences between STS and previous works need to be more compelling to convince the community of its significance.\n\n### Conclusion\nGiven the mixed feedback from reviewers, the concerns regarding the novelty and experimental rigor, and the need for clearer explanations, I believe that while the paper has potential, it requires significant revisions to meet the standards of a top-tier conference. The authors should focus on addressing the technical novelty, conducting more thorough experiments, and improving the clarity of their explanations.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Uncertainty for deep image classifiers on out of distribution data\" addresses a significant problem in machine learning: the calibration of predictive uncertainty for deep learning models when faced with out-of-distribution (OOD) data. The authors propose a post hoc calibration method that utilizes outlier exposure to improve the calibration of model probabilities, particularly when the model is tested on corrupted data.\n\n### Strengths:\n1. **Relevance of the Problem**: The issue of overconfidence in model predictions, especially in OOD scenarios, is critical in many real-world applications. The authors tackle an important and timely topic.\n   \n2. **Novelty of Approach**: The proposed methods (Single Image and Multiple Image Method) are presented as new contributions to the field of model calibration. The idea of using a calibration set derived from corrupted images is an interesting twist on traditional calibration methods that typically rely on clean data.\n\n3. **Empirical Results**: The authors provide empirical evidence showing that their method improves calibration metrics (like Expected Calibration Error) across various datasets, which is a strong point in favor of their approach.\n\n4. **Simplicity and Speed**: The authors emphasize that their method is simple and fast to compute, which is a practical advantage for deployment in real-world applications.\n\n### Weaknesses:\n1. **Assumptions on Corruption**: A significant concern raised by reviewers is that the method relies on the assumption that the possible corruptions are known and limited to a specific set. This assumption may not hold in many practical scenarios where OOD data can come from entirely unanticipated distributions. This could limit the applicability of the proposed method.\n\n2. **Incomplete Baselines**: Reviewers noted that the paper does not sufficiently compare against a comprehensive set of baselines, particularly those that do not rely on corrupted data for calibration. This lack of comparison makes it difficult to assess the true effectiveness of the proposed methods.\n\n3. **Clarity and Presentation**: Several reviewers pointed out that the terminology and presentation of the methods could be clearer. For instance, the definition of terms like \\( p_{max} \\) and the rationale behind choosing specific calibration sets were not adequately explained. This lack of clarity could hinder the reader's understanding of the method.\n\n4. **Limited Generalization**: While the authors have shown that their method works on certain types of corruptions, there is skepticism about its performance on completely different types of OOD data. The reviewers expressed a desire for more extensive testing across a broader range of corruptions.\n\n5. **Response to Reviewer Concerns**: Although the authors provided a detailed response to reviewer comments and made revisions, some concerns remained unaddressed, particularly regarding the generalizability of their method to unseen corruptions.\n\n### Conclusion:\nThe paper presents a relevant and interesting approach to a significant problem in machine learning. However, the reliance on known corruptions, the lack of comprehensive baseline comparisons, and issues with clarity and generalization raise substantial concerns about the robustness and applicability of the proposed methods. While the authors have made efforts to address reviewer comments, the fundamental issues regarding assumptions and clarity remain.\n\nGiven these considerations, I recommend rejecting the paper for publication at this time. The authors may benefit from further refining their approach, expanding their empirical evaluations, and clarifying their presentation before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel approach to understanding the representational power of deep neural networks (DNNs) by leveraging Sharkovsky's Theorem from dynamical systems. The authors aim to characterize the depth-width trade-offs of ReLU networks in representing functions with periodic points, addressing an open question posed by Telgarsky regarding the limitations of shallow networks.\n\n### Strengths:\n1. **Novel Contribution**: The paper provides a new perspective on the depth-width trade-offs in DNNs, connecting it to chaos theory and dynamical systems. This interdisciplinary approach is innovative and adds depth to the existing literature.\n2. **Clear Exposition**: Reviewers noted the clarity of the exposition and the clean presentation of the results. The authors have made efforts to restructure the paper based on reviewer feedback, which enhances its readability.\n3. **Theoretical Insights**: The results presented in the paper offer a theoretical foundation for understanding why certain functions are difficult to approximate with shallow networks, which is a significant contribution to the field.\n4. **Empirical Validation**: The authors have conducted experiments on synthetic datasets to illustrate their theoretical findings, addressing concerns about the practical applicability of their results.\n\n### Weaknesses:\n1. **Practical Utility**: While the theoretical results are strong, there are concerns about the practical utility of the findings. Reviewer 3 pointed out that the period-dependent depth lower bound may not be easily applicable in real-world scenarios, as assessing the period for a given classification task remains a challenge.\n2. **Structure and Clarity**: Although the authors have restructured the paper, there are still suggestions for improvement regarding the organization of theoretical content and the introduction of key results. Some reviewers felt that the theoretical background could be overwhelming before presenting the main contributions.\n3. **Intuition for Definitions**: There were requests for more intuitive explanations of certain definitions and concepts, which could help readers better grasp the implications of the theoretical results.\n\n### Reviewer Feedback:\nThe feedback from the reviewers is overwhelmingly positive, with two reviewers strongly recommending acceptance and one providing constructive criticism that has been addressed in the revised version. The authors have shown responsiveness to reviewer comments, enhancing the paper's clarity and empirical validation.\n\n### Conclusion:\nThe paper makes a significant contribution to the understanding of DNNs and their structural properties, with a novel application of chaos theory. While there are valid concerns regarding practical applicability and clarity, the strengths of the paper, including its innovative approach and positive reviewer feedback, outweigh these weaknesses. The authors have demonstrated a willingness to improve the paper based on feedback, which is a positive indicator of their commitment to quality research.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Deep Ecological Inference\" presents a novel approach to the ecological inference problem using deep learning techniques. The authors claim to improve the accuracy of ecological inference by leveraging deep neural networks and Bayesian models, particularly in the context of predicting voter behavior from aggregate data in the Maryland 2018 midterm elections. \n\n### Strengths:\n1. **Relevance and Importance**: The ecological inference problem is significant in political science, especially in contexts like gerrymandering and understanding voting behavior. The application of machine learning to this problem is timely and relevant.\n2. **Real-World Data**: The use of actual election data enhances the practical implications of the research and provides a valuable dataset for the machine learning community.\n3. **Innovative Approach**: The integration of deep learning with ecological inference is an interesting direction that could lead to advancements in both fields.\n\n### Weaknesses:\n1. **Clarity and Structure**: Multiple reviewers pointed out that the paper is difficult to follow due to poor organization and unclear explanations. Key concepts, such as the loss function and input data, are not introduced early enough, making it challenging for readers unfamiliar with the domain to grasp the contributions.\n2. **Experimental Rigor**: The experiments lack depth and clarity. Reviewer 2 highlighted the absence of comparisons to standard models, which is crucial for validating the proposed methods. The paper does not provide sufficient details on experimental setups, making reproducibility difficult.\n3. **Insufficient Analysis**: Claims made in the paper regarding the performance of the proposed models are not well-supported by thorough analysis. For instance, the paper does not adequately explain why the deep multi-level model outperforms simpler models in specific contexts, nor does it address the surprising finding that a standard linear regression model performed best in some cases.\n4. **Typos and Writing Quality**: Numerous typos and minor writing issues were noted by multiple reviewers, indicating a lack of attention to detail that detracts from the overall quality of the paper.\n5. **Lack of Novelty**: Reviewer 4 expressed concerns that the approach may not present new technical contributions and that related work is inadequately cited. This raises questions about the originality of the research.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes a potentially valuable approach, the significant weaknesses in clarity, experimental rigor, and analysis overshadow its strengths. The lack of a robust comparison to existing methods, insufficient detail for reproducibility, and the presence of numerous typos suggest that the paper is not yet ready for publication in a top-tier conference. The authors need to improve the structure, provide clearer explanations, enhance the experimental analysis, and ensure that the writing is polished.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"SGD Learns One-Layer Networks in WGANs\" presents a theoretical analysis of the convergence properties of stochastic gradient descent-ascent (SGD) in the context of Wasserstein GANs (WGANs) when using a one-layer generator and specific types of discriminators (linear and quadratic). The authors claim to provide the first result demonstrating global convergence for non-linear generators in this setting.\n\n#### Strengths:\n1. **Novelty of Contribution**: The authors claim to address a gap in the literature regarding the global convergence of SGD in the GAN framework, particularly for non-linear generators. This is a relevant and important topic in the field of generative models.\n2. **Theoretical Analysis**: The paper provides a theoretical framework that shows convergence in polynomial time and sample complexity, which is a significant contribution to understanding the dynamics of WGANs.\n3. **Clarity of Writing**: The paper is well-structured and the theoretical results are presented clearly.\n\n#### Weaknesses:\n1. **Simplistic Discriminator Choices**: The primary criticism from reviewers is the choice of discriminators (linear and quadratic). These are considered too simplistic and not representative of the typical discriminators used in practice (which are often more complex neural networks). This raises concerns about the generalizability of the results to real-world applications of WGANs.\n2. **Misleading Claims**: The title and claims regarding \"one-layer networks\" may be misleading. The reviewers pointed out that the term \"one-layer\" could be interpreted in various ways, and the authors did not sufficiently clarify this in their responses. This ambiguity could lead to misunderstandings about the contributions of the paper.\n3. **Limited Scope**: The analysis does not extend to more complex discriminators, which could provide a more comprehensive understanding of the WGAN training dynamics. The authors' justification for not considering more complex discriminators (to avoid increased sample complexity) does not fully address the reviewers' concerns about the relevance of their findings to practical scenarios.\n4. **Lack of Empirical Validation**: While the theoretical results are interesting, the paper lacks empirical validation to support the claims made. The reviewers noted that the theoretical results might not hold in more complex settings, which is a significant limitation.\n\n#### Reviewer Consensus:\nThe reviewers expressed a mix of opinions, with one reviewer acknowledging the importance of the theoretical results but still considering the choice of discriminators a weak point. The other reviewers were more critical, emphasizing that the simplifications made in the analysis limit its applicability and relevance to the broader field of WGANs.\n\n#### Author Responses:\nThe authors attempted to clarify their contributions and justify their choices, but they did not adequately address the core concerns raised by the reviewers regarding the applicability of their results to more complex scenarios. Their insistence on the sufficiency of quadratic discriminators does not convincingly counter the argument that more complex discriminators are necessary for a comprehensive understanding of WGAN dynamics.\n\n### Conclusion\nGiven the significant concerns regarding the simplicity of the discriminators used, the potential for misleading claims, and the limited scope of the analysis, the paper does not meet the standards expected for publication at a top-tier conference. While the theoretical contributions are noteworthy, the lack of generalizability and empirical validation undermines the overall impact of the work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"GG-GAN: A Geometric Graph Generative Adversarial Network\" presents a novel approach to graph generation using a Wasserstein GAN framework. The authors claim to address several key challenges in graph generation, including modeling complex relations, isomorphic graphs, and latent distribution exploitation. While the paper has some merits, it faces significant criticisms from the reviewers that raise concerns about its contributions, experimental validation, and theoretical foundations.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - Reviewers have pointed out that the problem of graph generation using GANs has been previously explored, and the paper does not sufficiently differentiate itself from existing methods like graphRNN and NetGAN. The novelty of GG-GAN is questioned, particularly regarding its claims of isomorphism consistency and scalability. The lack of a comprehensive literature review further weakens the argument for its contributions.\n\n2. **Experimental Validation**:\n   - The experimental results are criticized for being based on small graphs (with a maximum of 20 vertices) and for not providing a convincing comparison with other state-of-the-art methods. Reviewer 1 specifically mentions that the datasets used may contain a large number of isomorphic graphs, which could skew the results. Additionally, the absence of comparisons with other relevant graph generation models (e.g., NetGAN, TagGen) is a significant oversight that undermines the paper's claims of superiority.\n\n3. **Theoretical Foundations**:\n   - Several reviewers raised concerns about the theoretical aspects of the paper. For instance, the existence of certain distributions and the sampling mechanisms used in GG-GAN are questioned. The clarity and rigor of the theoretical claims, such as Proposition 1 and the necessity of the mechanism to avoid collisions, need improvement. The reviewers suggest that the theoretical contributions do not convincingly support the proposed method.\n\n4. **Mode Collapse and Generation Diversity**:\n   - Reviewer 2 highlights a critical issue regarding mode collapse, which is a common problem in GANs. The authors do not adequately address this concern or provide evidence that GG-GAN can avoid this pitfall, which is essential for a generative model.\n\n5. **Clarity and Presentation**:\n   - While some reviewers appreciated the clarity of the writing, others pointed out that certain sections, such as the introduction of the parameter φ and the mechanism to avoid collisions, are confusing and lack sufficient justification. This indicates that the paper may not be accessible to all readers, which is a concern for publication in a top-tier conference.\n\n6. **Lack of Author Response**:\n   - The absence of an author response to address the reviewers' concerns is a significant drawback. It suggests that the authors may not be willing or able to clarify or improve upon the issues raised, which is critical for a paper seeking publication.\n\n### Conclusion\nGiven the significant concerns raised by multiple reviewers regarding the novelty, experimental validation, theoretical foundations, and clarity of the paper, it does not meet the standards expected for acceptance at a top-tier conference. The paper requires substantial revisions and additional experiments to address the criticisms effectively.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training. The authors claim that their method, $\\alpha$-Variable Importance Learning (VIL), utilizes task-specific updates of model parameters to optimize task weights, which they argue is a significant advancement over existing methods.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - The paper introduces a method that aims to improve multitask learning by dynamically adjusting task weights based on model updates. While the concept of dynamic task weighting is not new, the authors argue that their approach is distinct because it employs direct meta-optimization for task weights. However, reviewers have raised concerns about the novelty of the method, suggesting that it resembles existing techniques like Discriminative Importance Weighting (DIW) and meta-learning approaches such as MAML. The authors' response attempts to clarify the differences, but the reviewers remain unconvinced about the uniqueness of the contribution.\n\n2. **Theoretical Justification**: \n   - A significant criticism from all reviewers is the lack of theoretical justification for the proposed method. While the authors acknowledge this shortcoming and express intent to improve this aspect in a future version, the absence of a solid theoretical foundation in the current submission weakens the paper's credibility and scientific rigor.\n\n3. **Empirical Evaluation**: \n   - The empirical results presented in the paper show that $\\alpha$VIL does not consistently outperform the baselines across various tasks. Reviewers noted that the improvements are often marginal and fall within the range of statistical insignificance. The authors' response highlights that multitask learning improvements can be small, particularly in natural language understanding tasks, but this does not sufficiently address the reviewers' concerns about the robustness and effectiveness of their method.\n\n4. **Experimental Design**: \n   - Reviewers pointed out limitations in the experimental setup, such as the choice of datasets and the lack of comprehensive comparisons with a wider range of baselines. The authors' justification for using MultiMNIST as a sanity check is noted, but the reviewers argue that the experimental design could be more convincing by including more diverse tasks and additional ablation studies.\n\n5. **Clarity and Presentation**: \n   - The paper is generally well-written and easy to follow, which is a positive aspect. However, several reviewers pointed out areas for improvement in clarity, particularly regarding the algorithm's details and the rationale behind certain design choices.\n\n6. **Response to Reviewers**: \n   - The authors provided a thoughtful response to the reviewers' comments, acknowledging the need for further theoretical justification and additional experiments. However, the commitment to address these issues in a future version does not mitigate the current paper's shortcomings.\n\n### Conclusion\nGiven the concerns regarding the theoretical foundation, the empirical results not convincingly demonstrating the superiority of the proposed method, and the need for more comprehensive experimental validation, the paper does not meet the standards expected for acceptance at a top-tier conference. The authors have identified areas for improvement, but the current submission lacks sufficient rigor and clarity to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Automatic Music Production Using Generative Adversarial Networks\" presents a novel framework for automatic music accompaniment using Mel-spectrograms and CycleGAN. The authors claim to address a gap in the literature regarding automatic music arrangement in the audio domain, specifically focusing on generating accompaniments for bass and vocal lines.\n\n### Strengths:\n1. **Novelty**: The application of CycleGAN to music accompaniment generation is an interesting approach, and the authors have attempted to tackle a less-explored area in music generation.\n2. **Experimental Setup**: The authors have conducted experiments on two relevant tasks, which is a positive aspect of the paper.\n3. **Combination of Evaluation Methods**: The use of both subjective (human listener testing) and objective (automatic scoring) evaluation methods is commendable and adds depth to the analysis.\n\n### Weaknesses:\n1. **Title and Abstract Issues**: The title is misleading as it suggests a broader scope of \"Automatic Music Production,\" while the paper focuses specifically on automatic accompaniment. This could confuse readers and misrepresent the work's contributions. The abstract also lacks clarity and motivation for the research.\n   \n2. **Literature Review**: The paper does not adequately cite existing literature on music generation in the symbolic domain, which is relevant to the work. The authors also make claims about the novelty of treating music audio as images without acknowledging the existing body of work that has already explored this approach.\n\n3. **Methodological Details**: There are significant gaps in the methodological description, particularly regarding the Demucs algorithm for source separation and the CycleGAN architecture. The lack of details on model training, hyperparameters, and the phase retrieval process raises concerns about reproducibility and the validity of the results.\n\n4. **Evaluation Metrics**: The proposed evaluation metrics and the rationale behind the chosen attributes for evaluation are not well justified. The authors need to provide a stronger theoretical basis for their choices and how they relate to the quality of music production.\n\n5. **Experimental Design**: The experiments lack variety in terms of the combinations of sources used for generating accompaniments. The authors could have included more diverse experimental settings to better assess the model's capabilities.\n\n6. **Author Responses**: While the authors have made efforts to address reviewer comments, many of the responses do not sufficiently resolve the concerns raised. For example, the justification for the choice of evaluation metrics and the limitations of the source separation method remain vague.\n\n### Conclusion:\nThe paper presents an interesting approach to automatic music accompaniment generation, but it suffers from significant issues related to clarity, methodological rigor, and literature engagement. The concerns raised by the reviewers highlight the need for substantial revisions to improve the paper's quality and ensure that it meets the standards of a top-tier conference. Given the current state of the paper, it is not ready for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" presents a generative model called ROOTS for unsupervised object-wise 3D scene decomposition and rendering. The authors claim that their model provides a hierarchical object-oriented representation and is capable of disentanglement, compositionality, and generalization, which they compare against the Generative Query Networks (GQN).\n\n#### Strengths:\n1. **Novelty and Relevance**: The topic of unsupervised learning for 3D scene representation is highly relevant and important in the field of computer vision. The idea of object-oriented representation is intriguing and could have significant implications for various applications.\n2. **Qualitative Results**: The qualitative results presented in the paper are impressive, showcasing the model's ability to swap objects in scenes and render them from different viewpoints.\n3. **Technical Contributions**: The paper introduces a new framework that builds on existing models, which could be a valuable contribution to the literature.\n\n#### Weaknesses:\n1. **Writing Quality**: Multiple reviewers pointed out that the paper is poorly written, with numerous grammatical errors and unclear explanations. This significantly detracts from the paper's readability and overall impact.\n2. **Claims and Comparisons**: Reviewer 1 highlighted that the claim of being the \"first unsupervised model that can identify objects in a 3D scene\" is inaccurate, as there are existing models like MONet and Iodine that achieve similar results. This undermines the novelty of the work.\n3. **Experimental Rigor**: The experimental results lack robustness. Reviewer 2 noted that the claims made about ROOTS outperforming GQN are not substantiated by the provided figures. Additionally, there is a lack of ablation studies to demonstrate the contributions of different components of the model.\n4. **Related Work**: The paper fails to adequately position itself within the existing literature. Reviewer 3 pointed out that the authors did not cite significant prior work in 3D representation learning, which is crucial for contextualizing their contributions.\n5. **Complexity and Clarity**: The method is described as complex, and several reviewers expressed that the explanations are convoluted. This complexity should be matched with clarity in presentation, which the paper currently lacks.\n\n#### Reviewer Feedback and Author Response:\nThe authors have acknowledged the reviewers' concerns and have made revisions to improve clarity and address some of the criticisms. However, the revisions seem to have only partially addressed the major issues, particularly regarding experimental validation and the positioning of their work in relation to existing literature.\n\n#### Conclusion:\nWhile the paper presents an interesting idea and has potential contributions, the significant issues regarding clarity, experimental validation, and the accuracy of claims are substantial enough to warrant rejection. The authors need to conduct more rigorous experiments, provide clearer explanations, and better position their work within the existing body of literature before it can be considered for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\" presents a novel exploration strategy for continuous reinforcement learning (RL) environments. The proposed method, Max-Q Entropy Search (MQES), aims to improve exploration by leveraging both epistemic and aleatoric uncertainties in the context of continuous action spaces. The authors claim that their method outperforms existing state-of-the-art algorithms in empirical evaluations on Mujoco environments.\n\n#### Strengths:\n1. **Novelty**: The introduction of an information-theoretic approach to exploration in continuous RL is a significant contribution. The focus on both types of uncertainty (epistemic and aleatoric) is a relevant and timely topic in the field.\n2. **Theoretical Framework**: The paper attempts to provide a theoretical foundation for the proposed method, which is essential for understanding its implications and potential applications.\n3. **Empirical Results**: The authors report that MQES outperforms existing methods in certain Mujoco tasks, which suggests that the proposed approach has practical utility.\n\n#### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers noted that the paper is poorly written and difficult to follow. The presentation of key concepts, equations, and algorithms lacks clarity, making it challenging for readers to grasp the main contributions and methodologies. This is a critical issue for a top-tier conference.\n2. **Experimental Rigor**: The empirical evaluation is criticized for being preliminary and lacking sufficient detail. Key hyperparameters are not adequately discussed or ablated, and comparisons with other relevant methods (like OAC) are missing. The significance of the results is questioned due to the limited number of seeds used in evaluations.\n3. **Theoretical Justifications**: Several reviewers pointed out that the theoretical aspects of the paper are vague and not well justified. The derivations and definitions are often unclear, leading to confusion about the proposed method's validity and applicability.\n4. **Limited Baselines**: The choice of baselines for comparison is limited, and the authors do not adequately justify their selections. The lack of comparison with other relevant exploration methods diminishes the impact of the results.\n5. **Response to Reviewer Comments**: While the authors provided responses to reviewer comments, many of the issues raised were not fully addressed in a satisfactory manner. For instance, the need for clearer explanations and additional ablation studies was acknowledged but not sufficiently implemented in the revisions.\n\n#### Conclusion:\nGiven the significant weaknesses in clarity, experimental rigor, and theoretical justification, the paper does not meet the standards expected for publication in a top-tier conference. While the idea is promising and the novelty is acknowledged, the execution falls short, making it difficult for the community to fully appreciate the contributions of the work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Training independent subnetworks for robust prediction\" presents a novel approach to ensemble learning by utilizing a multi-input multi-output (MIMO) configuration to train independent subnetworks within a single neural network. The authors claim that this method allows for improved robustness and uncertainty estimation without a significant increase in computational cost, as it can achieve the benefits of multiple predictions in a single forward pass.\n\n#### Strengths:\n1. **Innovative Approach**: The idea of training independent subnetworks within a single model using MIMO is a clever and potentially impactful contribution to the field of ensemble learning. The authors demonstrate that these subnetworks can behave independently, which is a significant finding.\n2. **Thorough Experimental Evaluation**: The experimental results are comprehensive, showing improvements in various metrics (negative log-likelihood, accuracy, calibration error) across multiple datasets (CIFAR10, CIFAR100, ImageNet). The empirical evidence supporting the independence of subnetworks is well-presented.\n3. **Clarity and Presentation**: The paper is well-written and easy to follow, with clear explanations of the methodology and results.\n\n#### Weaknesses:\n1. **Parameter Increase Clarification**: While the authors address the 1% increase in parameters, the initial claim of achieving benefits \"for free\" could be misleading. The reviewers pointed out that the increase, albeit small, should be more transparently discussed.\n2. **Theoretical Insights**: Several reviewers noted the lack of theoretical analysis regarding the emergence of independence among subnetworks. While the authors provide empirical evidence, a theoretical underpinning would strengthen the paper significantly.\n3. **Comparison with Related Work**: The originality of the approach is questioned due to similarities with existing methods like BatchEnsemble. The authors have made efforts to clarify these differences, but further elaboration is needed to fully establish the novelty of their contribution.\n4. **Computational Cost Claims**: The authors assert that MIMO's computational cost is equivalent to that of a standard deep neural network. However, concerns were raised about the implications of using identical inputs for testing and the potential impact on batch processing. The authors have provided clarifications, but the reviewers remain skeptical about the practical advantages of MIMO over traditional ensembles.\n\n#### Reviewer Feedback:\nThe feedback from the reviewers is mixed but leans towards positive. Reviewers appreciate the innovative nature of the work and the thorough experimental validation. However, they also highlight significant concerns regarding theoretical grounding, clarity in the presentation of computational costs, and the originality of the approach in light of existing literature.\n\n#### Author Responses:\nThe authors have made commendable efforts to address the reviewers' concerns, including clarifying the source of the parameter increase, discussing the relationship with related work, and providing additional empirical analysis. However, some concerns, particularly regarding theoretical insights and the practical implications of their claims, remain partially addressed.\n\n### Conclusion\nGiven the innovative nature of the proposed method, the thorough experimental validation, and the authors' responsiveness to reviewer feedback, the paper presents a valuable contribution to the field. However, the concerns regarding theoretical grounding and clarity in computational cost claims are significant enough to warrant caution. \n\nIn light of these considerations, I recommend acceptance with the expectation that the authors will continue to refine their theoretical insights and clarify their claims in future iterations of the work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Deep Coherent Exploration For Continuous Control\" presents a novel exploration framework for deep reinforcement learning (RL) that aims to unify step-based and trajectory-based exploration strategies. The authors claim that their method, Deep Coherent Exploration, improves the speed and stability of learning in several continuous control tasks using popular RL algorithms such as A2C, PPO, and SAC.\n\n#### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new method that generalizes previous work on exploration strategies for deep RL, specifically addressing the limitations of existing methods when applied to deep neural networks. The authors provide a recursive inference step for handling latent variables, which is a significant contribution to the field.\n\n2. **Empirical Results**: The empirical results presented for on-policy methods (A2C and PPO) are strong, demonstrating clear improvements over baseline methods. The ablation studies are insightful and help clarify the contributions of different components of the proposed method.\n\n3. **Clarity and Presentation**: The paper is generally well-written, with a clear introduction and related work sections. The detailed appendix enhances reproducibility, which is crucial for the community.\n\n4. **Addressing Reviewer Concerns**: The authors have made efforts to address the reviewers' concerns in their responses, indicating a willingness to improve the paper based on feedback.\n\n#### Weaknesses:\n1. **Off-Policy Performance**: The performance of the proposed method on off-policy methods (SAC) is less convincing. Reviewers noted that the method does not significantly improve SAC's performance and may even degrade it in some cases. This raises questions about the practical significance of the method when applied to state-of-the-art algorithms.\n\n2. **Dependence on Prior Work**: Several reviewers pointed out that the method heavily relies on the framework established by van Hoof et al. (2017). While the authors acknowledge this, they need to clarify how their contributions extend beyond this prior work.\n\n3. **Limited Experimental Scope**: The experiments are primarily conducted in the MuJoCo environment, which, while standard, may not fully capture the robustness of the proposed method across diverse tasks. Reviewers suggested that additional experiments in more complex environments would strengthen the paper.\n\n4. **Clarity of Certain Sections**: Some sections, particularly those detailing the mathematical formulations and the integration with SAC, could benefit from clearer explanations. Reviewers noted that certain technical details were dense and could be challenging for readers unfamiliar with the concepts.\n\n5. **Hyperparameter Sensitivity**: Concerns were raised regarding the sensitivity of the proposed method to hyperparameter choices, particularly in the context of SAC. The authors need to provide more clarity on this aspect to assure readers of the method's robustness.\n\n#### Conclusion:\nWhile the paper presents a promising approach to exploration in deep RL and shows strong results for on-policy methods, the concerns regarding off-policy performance, reliance on prior work, and limited experimental validation are significant. The authors have made efforts to address these issues in their responses, but the lack of strong empirical support for off-policy methods and the need for clearer presentation suggest that the paper may not yet meet the high standards expected for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" presents a novel algorithm for pruning neurons in deep ReLU networks. The authors claim that their method can significantly reduce the number of neurons while maintaining the accuracy of the learned function. However, the reviewers have raised several substantial concerns regarding the paper's clarity, experimental validation, and theoretical grounding.\n\n### Detailed Reasoning\n\n1. **Clarity and Presentation**: Multiple reviewers noted that the paper is poorly written and difficult to follow. Important concepts are buried in footnotes, and the overall organization lacks coherence. The authors acknowledge this issue in their response and express a willingness to improve the presentation. However, the current state of the manuscript does not meet the standards expected at a top-tier conference.\n\n2. **Theoretical Contributions**: The authors reference an unpublished work to support their theoretical claims, which raises concerns about the validity and verifiability of their results. Reviewer 3 specifically points out that the reliance on unpublished work makes it challenging to assess the correctness of the proposed algorithm. The authors' response indicates that they recognize the need to clarify their theoretical contributions, but the lack of accessible references undermines the paper's credibility.\n\n3. **Experimental Validation**: The experiments conducted in the paper are limited to simple toy datasets and MNIST, which are not sufficiently complex to demonstrate the effectiveness of the proposed pruning technique. Reviewers have called for experiments on more challenging datasets (e.g., CIFAR) and comparisons with state-of-the-art pruning methods. The authors acknowledge the need for additional experiments but have not yet provided them. The lack of comprehensive experimental validation is a significant drawback.\n\n4. **Comparison with Existing Methods**: The reviewers emphasize that the paper does not adequately compare the proposed method with existing neuron pruning techniques. This omission is critical, as it prevents the reader from understanding the relative performance and advantages of the authors' approach. The authors' response indicates an intention to include such comparisons, but this has not been realized in the current version of the paper.\n\n5. **Motivation and Novelty**: While the authors claim that their method leverages the properties of ReLU neurons for pruning, reviewers have pointed out that some of the ideas presented (e.g., removing inactive neurons) are not particularly novel. The authors need to better articulate the unique contributions of their work and how it advances the field of neural network pruning.\n\n6. **Technical Issues**: Reviewers have identified several technical issues, including unclear definitions, typos, and inconsistencies in notation. The authors have acknowledged these problems and expressed a commitment to address them, but the current state of the manuscript is still lacking in rigor.\n\n### Conclusion\n\nGiven the significant concerns raised by the reviewers regarding clarity, theoretical grounding, experimental validation, and comparison with existing methods, the paper is not currently suitable for publication at a top-tier conference. While the authors have shown a willingness to improve their work, the extent of the revisions needed suggests that the paper requires substantial reworking before it can be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Improved Training of Certifiably Robust Models\" presents a novel approach to enhancing the robustness of neural networks against adversarial attacks through the introduction of two regularizers aimed at tightening convex relaxations. The authors argue that their method leads to improved certified robustness compared to existing techniques. \n\n#### Strengths:\n1. **Novel Contribution**: The paper addresses a significant problem in the field of adversarial robustness, specifically the gap between certifiable and empirical robustness. The introduction of two regularizers is a meaningful contribution that could advance the state of the art.\n2. **Empirical Results**: The authors provide empirical evidence demonstrating that their proposed regularizers yield tighter certification bounds than non-regularized baselines. They claim to achieve state-of-the-art certified accuracies on both MNIST and CIFAR10 datasets, which is a positive aspect.\n3. **Revisions and Clarifications**: The authors have made substantial revisions in response to reviewer comments, including clarifying the theoretical underpinnings of their approach and improving the presentation of the paper.\n\n#### Weaknesses:\n1. **Theoretical Clarity**: Multiple reviewers pointed out that the theoretical justification for the proposed regularizers is convoluted and lacks clarity. There are concerns about the assumptions made in the analysis, particularly regarding the conditions under which the proposed regularizers yield tight bounds. The authors' responses indicate they have attempted to clarify these points, but the complexity remains a concern.\n2. **Experimental Validation**: While the authors have added new experimental results, the reliance on only MNIST and CIFAR10 datasets is a limitation. The reviewers noted that these datasets may not be sufficient for a comprehensive evaluation of the proposed methods. The authors acknowledged this limitation but did not commit to expanding their experiments in the future.\n3. **Comparison with Existing Methods**: There are concerns regarding the comparisons made with existing methods. Some reviewers noted that the paper does not adequately contextualize its results against stronger baselines, which could undermine the perceived effectiveness of the proposed approach. The authors have added some comparisons but may not have fully addressed this concern.\n\n#### Reviewer Opinions:\n- Reviewer 1 expressed significant concerns about the clarity of the analysis and the experimental results, leaning towards rejection.\n- Reviewer 2 highlighted the unclear theoretical justification and the need for better experimental validation, leaning towards weak rejection.\n- Reviewer 3 acknowledged the strengths of the paper but pointed out presentation issues and the limited dataset, suggesting a weak accept.\n\n### Conclusion\nThe paper presents a potentially valuable contribution to the field of adversarial robustness, but it suffers from significant theoretical and experimental weaknesses. The clarity of the theoretical framework is crucial for acceptance at a top-tier conference, and the concerns raised by the reviewers indicate that the paper may not yet meet the high standards expected. The authors have made efforts to address the reviewers' comments, but the fundamental issues regarding clarity and experimental validation remain.\n\nGiven these considerations, I recommend rejecting the paper at this time, with the hope that the authors can further refine their theoretical framework and expand their experimental validation in future submissions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Target Propagation via Regularized Inversion\" presents a novel approach to target propagation (TP) as an alternative to backpropagation (BP) for training recurrent neural networks (RNNs). The authors propose a method based on regularized inversion of network layers and provide experimental results demonstrating its application to sequence modeling problems.\n\n### Strengths:\n1. **Novelty**: The paper introduces a new variant of target propagation that is theoretically grounded and presents a clear implementation strategy within differentiable programming frameworks.\n2. **Clarity**: The writing is generally clear and structured, making it accessible to readers.\n3. **Theoretical Insights**: The authors attempt to provide theoretical guarantees regarding the relationship between TP and BP, which is a valuable contribution to the understanding of optimization methods in neural networks.\n\n### Weaknesses:\n1. **Limited Empirical Evidence**: The experimental results are not convincing enough to demonstrate the superiority of the proposed method over BP. The accuracy reported in experiments is low, and the authors do not adequately compare their method against state-of-the-art RNN models. This raises concerns about the practical applicability of the proposed approach.\n2. **Theoretical Gaps**: Reviewers noted that the theoretical analysis is not sufficiently robust. For instance, the bounds provided in the paper are considered loose, and the relationship between TP and Gauss-Newton methods is not fully explored. This lack of depth in theoretical discussion undermines the claims made about the advantages of TP.\n3. **Scope of Contribution**: Several reviewers pointed out that the proposed algorithm loses many of the attractive features of TP, such as the ability to optimize without weight transport. This limits its applicability and interest in broader contexts, such as decentralized optimization or neuromorphic hardware design.\n4. **Lack of Author Response**: The absence of an author response to address the reviewers' concerns is a significant drawback. This could indicate a lack of engagement with the feedback provided, which is critical for addressing weaknesses in the paper.\n\n### Conclusion:\nGiven the mixed reviews, the paper does present some interesting ideas and a novel approach to target propagation. However, the weaknesses in empirical validation, theoretical rigor, and the lack of a response to reviewer comments significantly detract from its overall contribution. The reviewers' consensus leans towards skepticism regarding the method's effectiveness compared to existing techniques, and the paper does not sufficiently demonstrate its advantages.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical exploration of the Feedback Alignment (FA) algorithm, focusing on its convergence properties and implicit regularization phenomena in deep linear networks. The authors provide convergence guarantees for both continuous and discrete dynamics, and they introduce concepts such as implicit anti-regularization and initialization schemes that can enhance learning.\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper makes a significant contribution by providing a rigorous theoretical analysis of the FA algorithm, which has been less understood compared to backpropagation. The exploration of convergence rates and the identification of implicit biases are valuable additions to the literature.\n  \n2. **Clarity and Structure**: Reviewers noted that the paper is well-structured and clearly written, which is essential for conveying complex theoretical concepts.\n\n3. **Novel Observations**: The identification of implicit anti-regularization and the conditions under which it occurs is an interesting finding that could have implications for understanding learning dynamics in neural networks.\n\n4. **Numerical Experiments**: The numerical comparisons between FA and gradient descent (GD) demonstrate the practical relevance of the theoretical findings, even if the settings are somewhat simplified.\n\n#### Weaknesses:\n1. **Assumptions and Limitations**: Several reviewers pointed out that the analysis relies heavily on specific assumptions, such as the diagonalizability of matrices and the use of spectral initialization. These assumptions may limit the generalizability of the results to more practical scenarios where random initialization is common.\n\n2. **Significance of Results**: Some reviewers expressed concerns about the significance of the contributions, suggesting that the results may not be sufficiently novel or impactful given the restrictive assumptions. The gap between the theoretical findings and empirical successes in non-linear networks remains a concern.\n\n3. **Lack of Empirical Evidence**: While the authors conducted numerical experiments, there is a call for more extensive testing under various initialization schemes, particularly those that reflect common practices in training neural networks.\n\n4. **Response to Reviewer Concerns**: The authors provided thoughtful responses to reviewer comments, addressing concerns about clarity and the significance of their findings. However, some responses did not fully alleviate the concerns regarding the applicability of their results to non-spectral initializations.\n\n#### Conclusion:\nThe paper presents a solid theoretical foundation for understanding the FA algorithm, which is a timely and relevant topic in the field of neural networks. However, the reliance on specific assumptions and the limited empirical validation raise questions about the broader applicability of the findings. While the theoretical contributions are noteworthy, the concerns regarding the significance and generalizability of the results suggest that the paper may not meet the high standards expected at a top-tier conference.\n\nGiven these considerations, I recommend that the paper be rejected for publication, with the suggestion that the authors address the highlighted concerns and consider expanding their empirical investigations in future submissions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents a novel approach to 3D snapshot microscopy using Fourier convolutional networks for end-to-end optimization of optical encoders and deep learning decoders. The authors claim that their method can efficiently decode information from multiple depths in a volume, outperforming existing UNet-based architectures.\n\n#### Strengths:\n1. **Novelty and Application**: The integration of Fourier convolutional networks into the context of 3D snapshot microscopy is an interesting application of deep learning techniques. The potential implications for biological imaging, particularly in neuroscience, are significant.\n2. **Experimental Results**: The authors provide promising results in simulations, demonstrating that their proposed architecture can outperform existing methods in both 3D snapshot microscopy and lensless imaging tasks.\n3. **Clarity and Structure**: The paper is well-written, with a clear structure and thorough explanations of the methodology and results. The limitations section is also appreciated, as it acknowledges the challenges faced in real-world applications.\n\n#### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the technical novelty of the paper is marginal, as the concept of Fourier-domain convolutions has been previously explored (e.g., Rippel et al. 2015). The authors have acknowledged this in their response and provided additional citations, but the paper still lacks a clear demonstration of how their approach significantly advances the state of the art beyond existing work.\n2. **Evaluation Concerns**: The evaluation is primarily based on simulated data, which raises questions about the generalizability of the results to real-world scenarios. While the authors have collected a dataset of zebrafish images, the lack of experimental validation in real settings is a significant limitation.\n3. **Comparative Analysis**: Reviewers expressed concerns about the comparisons made with UNet architectures. The paper does not sufficiently clarify why UNet performs poorly in this context, and the results presented in Table 3 regarding speed and performance could be misleading without proper context.\n4. **Niche Application**: Some reviewers noted that while the work is interesting, it may be more suited for application-specific conferences or journals rather than a top-tier machine learning conference like ICLR. This raises questions about the broader impact and relevance of the work within the machine learning community.\n\n#### Reviewer Consensus:\nThe reviews indicate a split opinion, with some reviewers leaning towards acceptance due to the quality of the work and its potential impact, while others are more critical of the novelty and evaluation aspects. The authors' responses have addressed some concerns but have not fully mitigated the issues raised regarding the novelty and the need for real-world validation.\n\n### Conclusion\nGiven the strengths of the paper in terms of clarity, potential application, and the quality of the experimental results, it shows promise. However, the significant concerns regarding the novelty of the contributions, the reliance on simulated data, and the need for more robust comparative analysis weigh heavily against acceptance at a top-tier conference. The paper would benefit from further validation and a clearer demonstration of its contributions relative to existing work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Global Node Attentions via Adaptive Spectral Filters\" presents a novel approach to graph neural networks (GNNs) that aims to address the limitations of existing models when applied to disassortative graphs. The authors propose a GNN model that incorporates a global self-attention mechanism using learnable spectral filters, allowing the model to attend to nodes regardless of their distance. The experimental results indicate that the proposed model outperforms state-of-the-art baselines on disassortative graphs and performs comparably on assortative graphs.\n\n#### Strengths:\n1. **Novelty and Motivation**: The paper tackles an important problem in GNNs, specifically the challenge of applying these models to disassortative graphs. The motivation for using global attention to capture distant node features is well-articulated, and the authors provide a clear rationale for their approach.\n\n2. **Empirical Evaluation**: The authors conduct thorough experiments on multiple benchmark datasets, demonstrating the effectiveness of their model. The inclusion of comparisons with various state-of-the-art methods strengthens the paper's contributions.\n\n3. **Clarity of Presentation**: The paper is well-written and accessible, making it easier for readers from different backgrounds to understand the proposed methodology and results.\n\n#### Weaknesses:\n1. **Complexity Concerns**: Several reviewers raised concerns about the computational complexity of the proposed method, particularly regarding the computation of the attention scores and the eigen decomposition involved in Eq. (3). The authors' rebuttal attempts to clarify this point, but the complexity remains a contentious issue. The reviewers argue that the claimed complexity may not hold in practice, which could limit the scalability of the proposed model.\n\n2. **Limited Novelty**: Some reviewers expressed skepticism about the novelty of the approach, suggesting that the use of attention mechanisms to capture distant nodes is not a new idea. They argue that existing methods, such as ChevNet, can also achieve similar results, albeit with different performance characteristics. The authors' rebuttal attempts to differentiate their work by emphasizing the learnability of their spectral filters, but this distinction may not be sufficiently compelling to overcome the reviewers' concerns.\n\n3. **Dataset Limitations**: The reviewers noted that the experiments were conducted on relatively small disassortative graphs. While the authors have added results for a larger dataset (Chameleon), there is still a call for more extensive experimentation on larger and more diverse datasets to validate the generalizability of their approach.\n\n4. **Lack of Theoretical Results**: One reviewer pointed out the absence of theoretical results to support the claims made in the paper. While empirical results are valuable, theoretical backing can significantly enhance the credibility of the proposed method.\n\n5. **Evaluation Tasks**: The focus on node classification as the sole evaluation task may limit the understanding of the model's performance across different graph-related tasks. The authors acknowledge this limitation and express intent to explore additional tasks in future work.\n\n### Conclusion\nWhile the paper presents a promising approach to addressing the limitations of GNNs on disassortative graphs, the concerns regarding computational complexity, novelty, and the scope of experimental validation are significant. The reviewers' critiques highlight that the paper may not meet the high standards expected at a top-tier conference, particularly in terms of theoretical contributions and comprehensive empirical validation.\n\nGiven these considerations, I recommend rejecting the paper at this time. The authors may benefit from addressing the raised concerns and expanding their experimental framework before resubmitting to a future conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\" presents a novel approach to improving the efficiency of diffusion models through the introduction of Differentiable Diffusion Sampler Search (DDSS) and Generalized Gaussian Diffusion Models (GGDM). The authors aim to reduce the number of inference steps required to generate high-quality samples while maintaining or improving sample quality.\n\n#### Strengths:\n1. **Novelty and Relevance**: The problem addressed is significant in the field of generative models, particularly given the computational demands of standard diffusion models. The proposed method aims to optimize the sampling process, which is a relevant and timely contribution.\n  \n2. **Empirical Results**: The authors provide compelling empirical results demonstrating that their method achieves better sample quality (as measured by FID scores) with fewer inference steps compared to existing methods like DDPM and DDIM. This is a strong point in favor of the paper.\n\n3. **Theoretical Improvements**: The authors have made efforts to clarify the theoretical underpinnings of their approach, particularly in response to reviewer concerns. The introduction of the Kernel Inception Distance (KID) as a perceptual loss function provides a clearer justification for their optimization objective.\n\n4. **Clarity Improvements**: The authors have addressed several clarity issues raised by reviewers, making the paper more accessible and easier to understand.\n\n#### Weaknesses:\n1. **Theoretical Guarantees**: Despite improvements, some reviewers still express concerns about the lack of theoretical guarantees regarding the distribution of samples generated by the proposed method. The optimization of perceptual loss instead of maximizing ELBO raises questions about the underlying statistical properties of the generated samples.\n\n2. **Generalization**: The empirical results are primarily based on smaller datasets (CIFAR-10 and ImageNet 64x64). While the authors plan to include results on more challenging datasets, the current scope may limit the generalizability of their findings.\n\n3. **Clarity and Presentation**: Although the authors have made improvements, some reviewers still find the presentation lacking in clarity, particularly regarding the mathematical formulations and the implications of the proposed methods.\n\n4. **Dependence on Pre-trained Models**: The reliance on pre-trained diffusion models without fine-tuning may limit the applicability of the proposed method in scenarios where such models are not available.\n\n5. **Potential Overfitting**: Concerns about overfitting to evaluation metrics like FID and IS were raised, suggesting that while the empirical results are strong, they may not fully reflect the robustness of the method.\n\n### Conclusion\nOverall, the paper presents a significant contribution to the field of generative models, addressing an important challenge with promising empirical results. The authors have made substantial efforts to clarify their methodology and address reviewer concerns. However, the lack of strong theoretical guarantees and the need for further validation on larger datasets remain points of concern.\n\nGiven the strengths of the paper, particularly in terms of novelty and empirical results, and considering the improvements made in response to reviewer feedback, I believe the paper is suitable for acceptance with the expectation that the authors will continue to refine their theoretical justification and expand their empirical validation in future work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a significant theoretical advancement in understanding the optimization landscape of two-layer ReLU neural networks. The authors claim to provide a complete characterization of the global minima of the non-convex training problem through a convex optimization framework, which is a notable contribution to the field.\n\n### Strengths:\n1. **Novelty and Theoretical Contribution**: The paper extends previous work by Pilanci and Ergen (2020) and provides a comprehensive characterization of optimal solutions without relying on duality, which is a fresh perspective in the literature. The results regarding the absence of spurious local minima for sufficiently wide networks are particularly impactful.\n  \n2. **Clear Framework**: The authors introduce the concepts of minimal and nearly minimal neural networks, which help structure their analysis and provide clarity in understanding the relationship between the convex optimization problem and the neural network training landscape.\n\n3. **Practical Implications**: The polynomial-time algorithm for checking global optimality is a practical contribution that could be beneficial for practitioners in the field.\n\n4. **Well-Organized Presentation**: The paper is mostly well-written and organized, making it accessible for readers who may not be deeply familiar with the technical details of convex optimization.\n\n### Weaknesses:\n1. **Limited Scope**: The analysis is confined to two-layer networks with ReLU activation. While the authors acknowledge this limitation, the potential for extending these results to deeper networks or other activation functions remains a critical question that is not fully addressed. Reviewers have pointed out the need for further exploration in this area.\n\n2. **Minor Corrections Needed**: Several reviewers noted minor issues, such as typos and unclear definitions, which the authors have acknowledged and promised to correct. While these do not fundamentally undermine the paper's contributions, they do indicate a need for careful proofreading.\n\n3. **Comparative Novelty**: Some reviewers expressed concerns about the novelty of the contributions relative to existing literature. While the authors have made efforts to clarify how their work differs from prior studies, the paper could benefit from a more explicit discussion of how it builds upon or diverges from previous findings.\n\n### Reviewer Feedback:\nThe feedback from the reviewers is generally positive, with two reviewers recommending acceptance and highlighting the paper's strengths in theoretical contributions and clarity. However, there are also calls for further exploration of the results' applicability to deeper networks and other activation functions, which the authors have acknowledged in their responses.\n\n### Author Responses:\nThe authors have provided thoughtful responses to the reviewers' comments, indicating a willingness to improve the paper based on feedback. They have addressed concerns about the scope of their results and have committed to including additional references and clarifications in the final version.\n\n### Conclusion:\nGiven the significant theoretical contributions, the clarity of presentation, and the authors' responsiveness to reviewer feedback, the paper is a strong candidate for acceptance. While there are limitations regarding the scope and some minor issues to address, these do not outweigh the overall contributions of the work. The potential implications for understanding the optimization landscape of neural networks make this paper a valuable addition to the conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" addresses an important and timely topic in the field of optimization, particularly in the context of machine learning where non-iid sampling is prevalent. The authors investigate the behavior of Stochastic Gradient Descent with momentum (SGDm) under covariate shift, identifying a resonance phenomenon that can lead to divergence. \n\n#### Strengths:\n1. **Relevance and Importance**: The topic is highly relevant, especially for applications in continual learning and reinforcement learning where data is often non-iid. The identification of resonance as a failure mode of SGDm is a significant contribution to the understanding of optimization dynamics in these contexts.\n\n2. **Theoretical Framework**: The authors provide a theoretical framework that connects concepts from nonlinear resonance theory to machine learning optimization. This interdisciplinary approach is commendable and could pave the way for further research in this area.\n\n3. **Empirical Validation**: The paper includes empirical experiments that support the theoretical findings, demonstrating that the resonance phenomenon persists even in more complex settings, such as neural networks and other optimizers like Adam.\n\n4. **Clarity and Structure**: The paper is well-structured, following a clear scientific format that outlines the phenomenon, hypothesis, analysis, and empirical validation.\n\n#### Weaknesses:\n1. **Novelty of Technical Contributions**: Several reviewers raised concerns about the novelty of the mathematical techniques used. While the authors claim a novel connection between resonance theory and machine learning, the reliance on existing results from numerical analysis and the lack of new proof techniques weaken the perceived novelty of the contributions. This is a significant concern for a top-tier conference.\n\n2. **Presentation and Intuition**: Reviewers noted that the presentation of key results, particularly Theorem 1, could be improved. The connection between the theoretical results and the observed resonance behavior was not sufficiently intuitive, which could hinder the paper's impact.\n\n3. **Practical Implications**: While the theoretical findings are interesting, the practical implications of the resonance phenomenon in real-world scenarios remain somewhat unclear. Reviewers pointed out that the conditions under which resonance occurs may be too restrictive to be widely applicable in practice.\n\n4. **Stochasticity in SGD**: The analysis primarily focuses on expected gradients, which raises questions about the applicability of the results to the stochastic nature of SGDm. Although the authors conducted additional experiments to address this concern, the initial reliance on expected gradients may limit the generalizability of their findings.\n\n5. **Limited Scope of Empirical Results**: While the empirical results are valuable, they primarily focus on synthetic data. The lack of real-world datasets to demonstrate the resonance phenomenon limits the paper's applicability and relevance to practitioners.\n\n#### Reviewer Consensus:\nThe reviewers presented a mixed consensus. While some reviewers appreciated the theoretical contributions and empirical results, others were critical of the novelty and practical implications. The concerns regarding the technical contributions and the clarity of the presentation are particularly noteworthy, as they are crucial for acceptance in a top-tier conference.\n\n### Conclusion\nGiven the strengths of the paper in addressing a relevant problem and providing a theoretical framework, alongside empirical validation, it does have merit. However, the significant concerns regarding the novelty of the contributions, clarity of presentation, and practical implications suggest that the paper may not meet the high standards expected for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning DNA folding patterns with Recurrent Neural Networks\" presents a machine learning approach to predict DNA folding patterns using various models, including a bidirectional LSTM (Long Short-Term Memory) network. The authors claim that their model outperforms traditional methods in predicting Topologically Associating Domains (TADs) from epigenetic data.\n\n#### Strengths:\n1. **Relevance and Novelty**: The application of machine learning to molecular biology, particularly in understanding chromatin structure, is timely and relevant. The use of LSTM networks to capture sequential dependencies in DNA data is a promising approach.\n2. **Performance**: The authors report that their bidirectional LSTM model outperforms other models, which is a significant contribution to the field.\n3. **Biological Insight**: The identification of informative epigenetic features and their biological significance adds value to the research.\n\n#### Weaknesses:\n1. **Methodological Concerns**: Several reviewers raised concerns about the lack of clarity in the methodology. Specifically, the definition and application of the weighted mean squared error (wMSE) were questioned, and it was suggested that the authors should compare their model against a vanilla LSTM and other baseline models using the same loss function.\n2. **Generalizability**: The authors did not test their model on other datasets or types of epigenetic data (e.g., ATAC-seq), which raises questions about the generalizability of their findings. The authors acknowledged this limitation but did not provide a strong justification for their choice.\n3. **Clarity and Presentation**: Reviewers noted that the paper lacked clarity in several sections, including the description of the data, methods, and results. The presence of only one equation and unclear references detracts from the overall quality of the paper.\n4. **Literature Review**: The authors did not adequately cite relevant previous works, which could provide context and support for their claims. This oversight was pointed out by multiple reviewers.\n\n#### Author Responses:\nThe authors provided responses to the reviewers' comments and made revisions to the paper. They expanded the literature review, clarified the methodology, and added additional evaluation metrics. However, some concerns, particularly regarding the generalizability of the model and the clarity of the presentation, remain inadequately addressed.\n\n### Conclusion\nWhile the paper presents an interesting application of machine learning in computational biology and shows promising results, the methodological concerns, lack of clarity, and insufficient generalizability significantly weaken its contribution. The paper does not meet the high standards expected for acceptance at a top-tier conference, particularly in terms of methodological rigor and clarity of presentation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\" presents a novel approach to improve the efficiency of diffusion models in generating high-quality samples. The authors introduce the Differentiable Diffusion Sampler Search (DDSS) and the Generalized Gaussian Diffusion Models (GGDM), which aim to reduce the number of inference steps required while maintaining or improving sample quality.\n\n#### Strengths:\n1. **Novelty and Relevance**: The problem addressed is significant in the field of generative models, particularly as diffusion models have gained popularity. The proposed methods aim to optimize the sampling process, which is a critical bottleneck in practical applications.\n  \n2. **Empirical Results**: The authors provide compelling empirical results demonstrating that their method achieves better sample quality (as measured by FID scores) with fewer sampling steps compared to existing methods like DDPM and DDIM. This is a strong contribution that could have practical implications.\n\n3. **Theoretical Improvements**: The authors have made efforts to clarify the theoretical underpinnings of their approach, particularly in relation to the optimization of perceptual loss and its connection to Kernel Inception Distance (KID). This addresses some of the concerns raised by reviewers regarding the theoretical justification of their method.\n\n4. **Response to Reviewer Feedback**: The authors have actively engaged with reviewer comments and made substantial revisions to clarify their methodology, improve the exposition, and provide additional empirical evidence. This responsiveness is commendable and indicates a willingness to improve the work based on constructive criticism.\n\n#### Weaknesses:\n1. **Clarity and Presentation**: Despite improvements, several reviewers noted that the paper's clarity could still be enhanced. Some technical details remain complex and may hinder understanding for readers not deeply familiar with the topic.\n\n2. **Theoretical Guarantees**: While the authors have made strides in justifying their choice of perceptual loss, some reviewers still expressed concerns about the lack of theoretical guarantees regarding the distribution of generated samples. The optimization of perceptual loss instead of likelihood raises questions about the statistical properties of the generated samples.\n\n3. **Limited Dataset Evaluation**: The empirical evaluation primarily focuses on smaller datasets (CIFAR-10 and ImageNet 64x64). Reviewers suggested that results on larger and more challenging datasets would strengthen the claims of the paper. The authors have indicated plans to include additional datasets in the camera-ready version, which is a positive step.\n\n4. **Potential Overfitting**: There are concerns about the possibility of overfitting to evaluation metrics (FID/IS scores) due to the nature of the perceptual loss used. While the authors argue that the limited degrees of freedom mitigate this risk, further empirical validation would be beneficial.\n\n#### Conclusion:\nOverall, the paper presents a significant advancement in the field of diffusion models, with promising empirical results and a clear motivation for the proposed methods. While there are still areas for improvement, particularly in clarity and theoretical grounding, the authors have made substantial efforts to address reviewer concerns. The potential impact of the work, combined with the authors' responsiveness to feedback, suggests that the paper is suitable for publication.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"The weighted mean trick – optimization strategies for robustness\" presents a novel approach to weighted empirical risk minimization (ERM) that aims to improve robustness against outliers by optimizing higher-order moments of the loss distribution. The authors claim that their method, referred to as the \"weighted mean trick,\" can enhance the performance of existing robust loss functions while providing a stronger theoretical foundation.\n\n#### Strengths:\n1. **Interesting Problem**: The topic of optimizing higher-order moments in loss functions is relevant and has implications for robust learning algorithms. The potential to improve robustness against outliers is a significant contribution to the field.\n2. **Experimental Validation**: The authors provide experimental results demonstrating that their method performs comparably to existing robust loss functions on noisy datasets, which adds empirical support to their theoretical claims.\n3. **Theoretical Insights**: The paper attempts to establish a connection between weighted means and higher-order moments, which could be valuable for future research.\n\n#### Weaknesses:\n1. **Theoretical Clarity**: Multiple reviewers pointed out that the theoretical contributions are not sufficiently rigorous. For instance, the choice of weights and their implications for convexity are not clearly justified. The lack of clarity regarding the conditions under which the proposed method operates raises concerns about its applicability.\n2. **Algorithmic Issues**: Reviewers raised significant concerns about the proposed algorithm, particularly regarding its gradient descent formulation. The authors' response did not adequately address the confusion about whether the algorithm is genuinely minimizing the intended objective. This lack of clarity could lead to misunderstandings about the method's effectiveness.\n3. **Inconsistencies and Ambiguities**: Several reviewers noted inconsistencies in notation and terminology, which hinder the paper's readability. The authors' responses indicate that they have made some revisions, but the fundamental issues regarding clarity and coherence remain a concern.\n4. **Limited Novelty**: While the paper presents an interesting approach, several reviewers felt that the contributions were marginally significant or novel. The theoretical results were described as straightforward, and the paper does not sufficiently differentiate itself from existing literature.\n\n#### Reviewer Consensus:\nThe feedback from the reviewers is mixed, with some expressing interest in the problem and potential applications, while others highlight significant theoretical and algorithmic shortcomings. The concerns about the clarity of the theoretical framework, the validity of the algorithm, and the overall novelty of the contributions weigh heavily against acceptance.\n\n#### Author Responses:\nThe authors provided detailed responses to reviewer comments, indicating a willingness to address concerns. However, many of the responses did not fully resolve the issues raised, particularly regarding the theoretical foundations and algorithmic clarity. The authors acknowledged limitations and proposed future work, but this does not compensate for the current paper's shortcomings.\n\n### Conclusion\nGiven the significant theoretical and algorithmic concerns raised by multiple reviewers, along with the mixed feedback regarding the novelty and clarity of the contributions, the paper does not meet the standards expected for acceptance at a top-tier conference. The authors have made efforts to address feedback, but the fundamental issues remain unresolved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" presents a novel approach to defending against adversarial attacks using a modified autoencoder framework, AE-GAN+sr. The authors claim that their method reduces computational costs while maintaining or improving performance compared to existing methods like Defense-GAN.\n\n#### Strengths:\n1. **Novelty and Relevance**: The topic of adversarial defense is highly relevant in the current landscape of machine learning, particularly in deep learning. The proposed method introduces a new approach by leveraging an autoencoder to assist in the search for a natural reconstruction of adversarial examples.\n  \n2. **Empirical Results**: The authors provide empirical results demonstrating that their method outperforms Defense-GAN in terms of computational efficiency while achieving comparable performance on MNIST and Fashion-MNIST datasets.\n\n3. **Response to Reviewers**: The authors have made efforts to address the reviewers' comments, including adding comparisons with PGD adversarial training and additional results on the CelebA dataset, which strengthens their claims.\n\n#### Weaknesses:\n1. **Incremental Contribution**: Several reviewers noted that the contribution appears incremental rather than groundbreaking. While the authors have improved upon Defense-GAN, the overall advancement in the field may not be significant enough for a top-tier conference.\n\n2. **Experimental Evaluation**: The experimental results primarily focus on MNIST and Fashion-MNIST, which are relatively simple datasets. The reviewers pointed out the need for evaluations on more complex datasets like CIFAR-10 to better demonstrate the robustness and generalizability of the proposed method.\n\n3. **Clarity and Presentation**: Multiple reviewers criticized the writing quality and clarity of the paper. Issues such as typos, unclear figures, and convoluted explanations detract from the overall presentation and make it difficult for readers to follow the methodology and results.\n\n4. **Concerns about Robustness**: Reviewer 3 raised concerns about the reliance on the autoencoder's ability to detect adversarial samples, which could be susceptible to white-box attacks. The authors' responses indicate some understanding of these limitations, but they do not fully address the potential vulnerabilities in their approach.\n\n5. **Lack of Comprehensive Comparisons**: Although the authors added comparisons with PGD adversarial training, they did not include results for TRADES or other state-of-the-art defenses, which would provide a more comprehensive evaluation of their method's performance.\n\n#### Conclusion\nWhile the paper presents a relevant and interesting approach to adversarial defense, the concerns regarding its incremental nature, the need for more robust experimental validation, and the clarity of presentation weigh heavily against it. The authors have made some improvements in response to reviewer comments, but these may not be sufficient to elevate the paper to the standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" addresses an important issue in multi-modal learning, specifically the tendency of deep neural networks (DNNs) to rely heavily on one modality while under-utilizing others. The authors propose a new metric, the conditional utilization rate, and a training algorithm, balanced multi-modal learning, to mitigate this issue. \n\n#### Strengths:\n1. **Relevance of Topic**: The problem of modality imbalance in multi-modal learning is significant and timely, given the increasing use of multi-modal data in various applications.\n2. **Proposed Metrics and Algorithm**: The introduction of conditional utilization rate and conditional learning speed as metrics to quantify modality usage is a novel contribution. The proposed balanced multi-modal learning algorithm is a practical approach to address the identified issue.\n3. **Empirical Validation**: The authors provide empirical results across multiple datasets, demonstrating that their proposed method can improve generalization performance.\n\n#### Weaknesses:\n1. **Lack of Theoretical Support**: Several reviewers pointed out that the theoretical underpinnings of the greedy learner hypothesis are not well-supported. While empirical results are provided, the lack of a solid theoretical foundation weakens the claims made in the paper.\n2. **Experimental Design Concerns**: Reviewers noted inconsistencies in experimental results, particularly in Table 1, where results for different datasets were not clearly explained. The lack of statistical significance in some comparisons raises questions about the robustness of the findings.\n3. **Limited Dataset Variety**: The experiments primarily focus on visual modalities, which may limit the generalizability of the findings. Reviewers suggested including datasets with more diverse modalities (e.g., audio-visual) to strengthen the claims.\n4. **Insufficient Comparison with Existing Methods**: The authors did not adequately compare their method with existing state-of-the-art techniques that address similar issues, which is critical for establishing the novelty and effectiveness of their approach.\n\n#### Reviewer Feedback:\n- Reviewers expressed mixed opinions, with some acknowledging the potential of the work while others highlighted significant gaps in validation and theoretical grounding. The responses from the authors addressed some concerns but did not fully resolve the issues raised, particularly regarding the need for more rigorous theoretical analysis and comprehensive experimental validation.\n\n#### Conclusion:\nWhile the paper presents an interesting and relevant topic with some novel contributions, the lack of strong theoretical support, inconsistencies in experimental results, and insufficient comparisons with existing methods are significant drawbacks. The reviewers' concerns indicate that the paper does not yet meet the high standards expected for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" presents a new algorithm for pruning neurons in deep ReLU networks. The authors claim that their method can significantly reduce the number of neurons without a substantial loss in accuracy, leveraging theoretical insights from their previous work.\n\n#### Strengths:\n1. **Novelty of Approach**: The paper attempts to introduce a new algorithm for neuron pruning based on theoretical foundations, which is a relevant and important area in deep learning.\n2. **Potential Practical Applications**: If successful, the proposed method could lead to more efficient neural networks, which is a significant concern in the field.\n\n#### Weaknesses:\n1. **Lack of Clarity and Rigor**: Multiple reviewers pointed out that the paper is poorly written and difficult to follow. Important concepts are not well-defined, and the presentation lacks a logical flow. This significantly hampers the reader's ability to understand the contributions and the methodology.\n   \n2. **Insufficient Experimental Validation**: The experiments conducted are limited to simple toy datasets and MNIST. There is a lack of comparison with state-of-the-art pruning techniques, which is crucial for validating the effectiveness of the proposed method. Reviewer comments consistently highlight the need for more comprehensive experiments, including more complex datasets like CIFAR.\n\n3. **Reliance on Unpublished Work**: The paper heavily references an unpublished work by the authors, which raises concerns about the reproducibility and verifiability of the claims made. This reliance on unpublished results makes it difficult for reviewers to assess the validity of the theoretical foundations upon which the algorithm is built.\n\n4. **Vague Contributions**: Reviewers noted that the contributions of the paper are not clearly articulated. The authors' responses indicate an awareness of this issue, but the original manuscript fails to communicate the significance of the contributions effectively.\n\n5. **Theoretical Justifications**: While the authors claim to provide a theoretical basis for their pruning method, the reviewers found the explanations lacking. The connection between the theory and the proposed algorithm is not sufficiently established, and the mathematical rigor is questioned.\n\n6. **Typos and Grammatical Errors**: The manuscript contains numerous typos and grammatical errors, which detract from its professionalism and readability.\n\n#### Reviewer Consensus:\nThe majority of reviewers recommend rejection, citing the aforementioned issues. They express concerns about the clarity, experimental validation, and theoretical grounding of the work. The authors acknowledge these weaknesses in their responses but do not provide a clear plan for addressing them in the current version of the paper.\n\n### Conclusion\nGiven the significant weaknesses in clarity, experimental validation, reliance on unpublished work, and vague contributions, the paper does not meet the standards expected for publication at a top-tier conference. The authors have potential insights and a novel approach, but the current presentation and validation are insufficient.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" attempts to address the exploration-exploitation trade-off in Monte Carlo Tree Search (MCTS) methods within the context of reinforcement learning. However, the reviews provided by the three reviewers indicate significant issues that undermine the paper's suitability for publication in a top-tier conference.\n\n### Detailed Reasoning:\n\n1. **Clarity and Writing Quality**: \n   - All three reviewers have pointed out severe issues with the writing quality of the paper. Reviewer 2 explicitly states that the paper is \"badly and unprofessionally written,\" making it difficult to understand the core contributions. Reviewer 3 echoes this sentiment, noting that the writing is incomprehensible and filled with grammatical errors. The lack of clarity in the abstract and throughout the paper is a critical flaw, as it prevents readers from grasping the research's intent and significance.\n\n2. **Lack of References**:\n   - Reviewer 1 highlights the absence of citations, which is a fundamental requirement for academic writing. The lack of references not only diminishes the credibility of the work but also suggests a lack of engagement with existing literature. This is particularly concerning in a field as well-studied as reinforcement learning and MCTS.\n\n3. **Conceptual Understanding**:\n   - The reviewers express confusion regarding the proposed method and its implementation. Reviewer 1 describes the paper as \"less than half-baked,\" indicating that the ideas presented are not fully developed or articulated. Reviewer 3 mentions that the paper appears to be a set of notes rather than a coherent academic contribution. This lack of clarity and development raises questions about the validity and robustness of the proposed method.\n\n4. **Response from Authors**:\n   - The authors' response indicates an awareness of their shortcomings, particularly in writing and referencing. However, the response does not address the core issues raised by the reviewers regarding the clarity of the proposed method or the lack of a solid foundation in existing research. The authors express a desire to improve but do not provide a concrete plan for addressing the feedback in the current submission.\n\n5. **Overall Contribution**:\n   - While the topic of improving MCTS methods is relevant and potentially valuable, the execution in this paper is lacking. The reviewers unanimously agree that significant revisions are necessary before this work could be considered for publication. The paper does not meet the standards expected at a top-tier conference, where clarity, rigor, and engagement with existing literature are paramount.\n\n### Conclusion:\nGiven the substantial issues with writing quality, lack of references, and unclear presentation of ideas, the paper does not meet the necessary standards for acceptance. The authors would benefit from revising their work significantly before resubmitting to ensure that their contributions are clearly articulated and grounded in existing research.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning\" presents a novel approach to combining meta-reinforcement learning (meta-RL) and imitation learning (IL) to improve task adaptation in environments with sparse rewards. The authors propose a method that utilizes expert demonstrations to guide the learning process, aiming to enhance the agent's ability to adapt to new tasks quickly.\n\n### Detailed Reasoning\n\n**Strengths:**\n1. **Novelty and Relevance:** The integration of meta-RL and IL is a timely and relevant topic in the field of reinforcement learning, addressing the challenges of sample efficiency and adaptability in sparse reward settings.\n2. **Empirical Results:** The paper reports promising empirical results, demonstrating that PERIL outperforms existing baselines such as PEARL and MetaIL, which is a significant contribution to the field.\n3. **Visualizations and Organization:** The paper is well-organized, and the visualizations provided help clarify the results and the learning process.\n\n**Weaknesses:**\n1. **Complexity and Clarity:** Multiple reviewers pointed out that the method is overly complex, with numerous components and loss terms that are not clearly explained. This complexity makes it difficult for readers to understand the contributions and the necessity of each component. The notation is inconsistent, and there are several instances where the mathematical formulations are not clearly defined or explained.\n2. **Insufficient Comparison with Prior Work:** The paper lacks a thorough comparison with existing methods that also combine meta-RL and IL. Reviewers noted that the absence of these comparisons limits the ability to assess the true value of the proposed method. Additionally, the treatment of related work is limited, and several relevant citations are missing.\n3. **Assumptions and Limitations:** The assumption that an expert distribution over trajectories conditioned on the learned latent variable is available is seen as a strong and potentially unrealistic assumption. This raises questions about the practical applicability of the method in real-world scenarios.\n4. **Experimental Rigor:** The experiments lack statistical significance tests and multiple seeds, which are necessary for validating the results. The reviewers expressed concerns about the credibility of the experimental setup and the need for more rigorous evaluation, especially in terms of zero-shot learning claims.\n5. **Lack of Ablation Studies:** There is a lack of ablation studies to determine which components of the proposed method contribute to its performance. This makes it challenging to understand the impact of each design choice.\n\n### Conclusion\nWhile the paper presents a novel approach and shows promising results, the significant issues related to clarity, complexity, insufficient comparisons with prior work, and the lack of rigorous experimental validation raise concerns about its readiness for publication. The reviewers' feedback indicates that substantial revisions are needed to address these weaknesses, particularly in clarifying the method, improving the experimental setup, and providing a more comprehensive treatment of related work.\n\nGiven these considerations, I recommend rejecting the paper in its current form, with the hope that the authors can address the highlighted issues in a future submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" presents a novel approach to layout representation learning using a large-scale dataset of PowerPoint slides. The authors propose a pre-training method for a Transformer-based model, CanvasEmb, which they claim achieves state-of-the-art performance on two downstream tasks: element role labeling and image captioning. \n\n### Strengths:\n1. **Large-scale Dataset**: The construction of a dataset with over one million slides is a significant contribution, addressing the scarcity of labeled layout data in previous works. This dataset can serve as a valuable resource for future research in graphic design intelligence.\n2. **Innovative Approach**: The application of Transformer-based architectures to layout representation is a fresh perspective that has not been extensively explored in this domain.\n3. **Potential Applications**: The paper discusses various applications of the proposed model, such as layout auto-completion and retrieval, which could have practical implications in graphic design.\n\n### Weaknesses:\n1. **Insufficient Evaluation**: The evaluation of the proposed method is lacking in rigor. The authors primarily compare their model against a decision-tree baseline, which does not adequately demonstrate the advantages of their approach. The reviewers pointed out the need for comparisons with more sophisticated baselines, including existing neural architectures and other relevant works in the field.\n2. **Technical Details**: The paper lacks sufficient technical details about the Transformer architecture and the pre-training process. This omission makes it difficult for readers to fully understand the model's workings and its advantages over previous methods.\n3. **Dataset Construction Details**: The reviewers noted that the paper does not provide enough information about how the dataset was constructed, including the parsing process and the completeness of the properties. This information is crucial for assessing the dataset's quality and the model's applicability.\n4. **Related Work**: The related work section is underdeveloped, failing to adequately position the proposed method within the existing literature. The authors need to clarify how their work differs from and builds upon previous studies, particularly those that have also explored layout representation.\n5. **Clarity and Presentation**: While the writing is generally clear, there are areas where clarity could be improved, particularly in the description of the model and its evaluation. The authors should address the reviewers' suggestions for improving the clarity of their explanations.\n\n### Reviewer Feedback:\nThe reviewers collectively raised concerns about the evaluation methodology, the need for stronger baselines, and the lack of clarity in the presentation of the model and its contributions. The authors acknowledged these issues in their response and expressed a willingness to improve the paper in these areas. However, the extent of the revisions needed to address these concerns is significant.\n\n### Conclusion:\nWhile the paper presents an interesting approach and a valuable dataset, the current evaluation and presentation are insufficient for a top-tier conference. The lack of rigorous comparisons, technical details, and clarity in the related work section are critical shortcomings that need to be addressed before the paper can be considered for acceptance. The authors' willingness to revise the paper is commendable, but the fundamental issues identified by the reviewers suggest that the paper is not yet ready for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A Boosting Approach to Reinforcement Learning\" presents a novel approach to reinforcement learning (RL) by leveraging boosting techniques from supervised learning. The authors claim to develop an efficient algorithm for RL in Markov decision processes (MDPs) that does not depend on the number of states, which is a significant contribution given the computational challenges associated with large state spaces.\n\n### Strengths:\n1. **Novelty and Relevance**: The idea of applying boosting techniques to RL is intriguing and has the potential to contribute significantly to the field. The authors argue that their approach does not require the convexity of the cost function, which distinguishes it from previous work, particularly that of Hazan and Singh (2021).\n2. **Theoretical Contributions**: The paper provides a rigorous theoretical analysis, including sample complexity and running time bounds that are polynomial in various parameters, which is a valuable contribution to the RL literature.\n3. **Potential Impact**: If the proposed method is validated, it could have implications for solving large-scale RL problems, making it relevant to both the RL and optimization communities.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers highlighted significant issues with clarity, notation overload, and the overall presentation of the paper. The dense mathematical notation and lack of intuitive explanations make it difficult for readers to grasp the main ideas and contributions. This is a critical issue for a top-tier conference, where clarity is essential for disseminating complex ideas.\n2. **Incremental Novelty**: While the authors claim novelty, several reviewers pointed out that the contributions appear to be incremental compared to existing work, particularly regarding the similarities with Hazan and Singh (2021). The authors need to more clearly articulate how their work advances the field beyond existing literature.\n3. **Lack of Empirical Validation**: The paper is primarily theoretical, and while the authors acknowledge this, several reviewers suggested that initial experiments would strengthen the paper. Empirical validation is often crucial for acceptance in top-tier conferences, especially in applied fields like RL.\n4. **Response to Reviewer Comments**: The authors' responses indicate an awareness of the issues raised but do not fully address the concerns regarding clarity and the need for empirical results. The promise to clarify notations and provide more intuition is positive, but it may not be sufficient to overcome the existing presentation issues.\n\n### Conclusion:\nGiven the strengths of the paper in terms of novelty and theoretical contributions, it has potential value for the community. However, the significant weaknesses in clarity, presentation, and the lack of empirical validation raise concerns about its readiness for publication. The reviewers' consensus indicates that while the paper has interesting ideas, it requires substantial revisions to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" presents a novel framework for synthesizing data in a differentially private manner using deep generative models. The authors claim that their approach allows for the generation of synthetic data without reusing the original data, thus maintaining rigorous privacy guarantees. The reviewers generally recognize the significance of the problem addressed and appreciate the novelty of the proposed method, but they also highlight several weaknesses that need to be addressed.\n\n#### Strengths:\n1. **Novelty and Motivation**: The approach is well-motivated and addresses an important issue in the field of data synthesis and privacy. The use of a one-shot approach to provide privacy is a significant contribution.\n2. **Theoretical Guarantees**: The authors provide theoretical guarantees for their method, which adds credibility to their claims.\n3. **Empirical Performance**: Initial empirical evaluations show that PEARL outperforms existing methods like DP-MERF and DP-GAN, which is a positive indicator of its effectiveness.\n\n#### Weaknesses:\n1. **Benchmarking and Comparisons**: Multiple reviewers pointed out the need for more thorough benchmarking against a wider range of datasets and methods. While the authors added a new dataset (Credit) and a new baseline (DPCGAN), the absence of comparisons with other significant methods like PATE-GAN and CTGAN for tabular data remains a concern. The reviewers also suggested using more complex datasets (e.g., CIFAR-10) to better evaluate the method's performance.\n2. **Clarity and Justification**: Several reviewers noted that certain aspects of the paper, such as the sensitivity calculation and the privacy budget allocation, were not clearly explained. The authors provided clarifications in their response, but the initial lack of clarity may have hindered the understanding of their contributions.\n3. **Related Work**: Reviewer 3 pointed out that important references were missing, which could have provided context and support for the claims made in the paper. The authors have added some of these references in their response, but the initial oversight raises questions about the thoroughness of the literature review.\n4. **Complexity Analysis**: The lack of a detailed complexity analysis was noted as a weakness. The authors have since provided some analysis, but the initial omission may affect the perceived robustness of the method.\n\n#### Reviewer Scores:\n- Reviewer 1: Correctness (4), Technical Novelty (3), Empirical Novelty (3)\n- Reviewer 2: Correctness (3), Technical Novelty (3), Empirical Novelty (3)\n- Reviewer 3: Correctness (3), Technical Novelty (3), Empirical Novelty (3)\n\nThe scores indicate that while the reviewers see potential in the paper, they also recognize that it falls short in several areas, particularly in empirical validation and clarity.\n\n### Conclusion\nThe paper presents a promising approach to a significant problem in data synthesis and privacy. However, the concerns raised by the reviewers regarding benchmarking, clarity, and related work are substantial. The authors have made efforts to address some of these issues in their response, but the paper still lacks comprehensive empirical validation against a wider array of methods and datasets. Given the standards of a top-tier conference, where thoroughness and clarity are paramount, I believe that the paper requires further refinement before it can be accepted.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" presents a novel architecture for generating synthetic time-series data using Variational Auto-Encoders (VAEs). The authors claim that their approach offers advantages in interpretability, the ability to incorporate domain knowledge, and reduced training times compared to existing methods, particularly Generative Adversarial Networks (GANs).\n\n### Strengths:\n1. **Clarity and Writing**: The paper is well-written and clearly conveys the main ideas and methodologies.\n2. **Interesting Approach**: The integration of classical time-series modeling techniques (like trend and seasonality) into a VAE framework is an interesting contribution that could enhance interpretability and usability in practical applications.\n\n### Weaknesses:\n1. **Lack of Novelty**: Multiple reviewers pointed out that the proposed model does not significantly deviate from standard VAE architectures. The claim that it better captures temporal aspects of data lacks substantial empirical support, especially when compared to existing models like TimeGAN.\n2. **Insufficient Empirical Evaluation**: The empirical evaluation is limited in scope. While the authors tested their model on four datasets, the datasets are not well-known, making it difficult to compare results with existing literature. Additionally, the experiments do not adequately explore the hyper-parameters introduced by the new decoder blocks, nor do they provide ablation studies to support claims of interpretability.\n3. **Interpretability Claims**: Despite claiming that interpretability is a key contribution, there is no discussion or empirical evaluation of how the proposed model achieves this interpretability. This is a significant oversight, as it undermines one of the main selling points of the paper.\n4. **Methodological Concerns**: Reviewers raised concerns about the clarity of the methodology, particularly regarding the VAE loss function and the architecture's design choices. The lack of a thorough discussion on these aspects makes it difficult to assess the technical soundness of the proposed model.\n5. **Limited Contribution to the Field**: Several reviewers expressed skepticism about the practical contributions of the proposed synthetic data generator, questioning its relevance to real-world challenges in time-series analysis.\n\n### Conclusion:\nGiven the feedback from multiple reviewers, it is clear that while the paper presents an interesting idea, it falls short in several critical areas, including novelty, empirical validation, and clarity of methodology. The lack of a robust evaluation of the proposed model's interpretability and the limited scope of the experiments further weaken the paper's case for acceptance. \n\nOverall, the paper requires significant revisions to address these concerns before it can be considered suitable for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical insights in meta-learning and their practical applications, particularly in few-shot learning (FSL). The authors propose two regularization terms based on recent theoretical advancements to improve the generalization capacity of popular meta-learning algorithms.\n\n#### Strengths:\n1. **Motivation and Relevance**: The motivation to connect theoretical insights with practical applications in meta-learning is timely and relevant, especially given the increasing interest in few-shot learning.\n2. **Clarity and Organization**: The paper is well-organized and clearly written, making it accessible to readers.\n3. **Experimental Results**: The authors provide empirical results demonstrating that their proposed regularization terms can improve the performance of existing meta-learning algorithms on standard benchmarks.\n\n#### Weaknesses:\n1. **Novelty of Contributions**: Multiple reviewers raised concerns about the novelty of the proposed regularization methods, suggesting that they are not fundamentally new and are similar to existing techniques like weight decay and spectral normalization. The authors argue that their approach is distinct because it combines two regularization strategies to satisfy specific theoretical assumptions. However, the reviewers remain skeptical about the significance of this contribution.\n2. **Theoretical Consistency**: Reviewer 2 pointed out inconsistencies between the theoretical framework and the practical few-shot learning setting, particularly regarding the absence of a validation set and the assumptions about task distributions. The authors have attempted to address these concerns in their response, but the clarity and applicability of their theoretical framework remain questionable.\n3. **Statistical Significance of Results**: Reviewer 3 noted that the improvements reported in the experimental results are often within the range of standard deviation, casting doubt on the practical significance of the proposed methods. The authors claim that their results are statistically significant, but this needs to be clearly demonstrated in the paper.\n4. **Limited Comparisons**: Several reviewers mentioned that the paper lacks sufficient comparisons with recent state-of-the-art methods in meta-learning. While the authors have added some additional comparisons in their response, it is unclear if these are comprehensive enough to validate their claims.\n\n#### Reviewer Concerns:\n- The reviewers collectively expressed skepticism about the novelty and significance of the proposed regularization methods.\n- There are concerns about the theoretical assumptions and their applicability to practical scenarios.\n- The experimental results, while showing some improvements, do not convincingly demonstrate a substantial advantage over existing methods.\n\n#### Author Responses:\nThe authors have made efforts to address the reviewers' concerns by clarifying their methodology, adding more experimental results, and providing examples to justify their theoretical assumptions. However, the responses do not fully alleviate the concerns regarding the novelty and significance of their contributions.\n\n### Conclusion\nGiven the mixed feedback from reviewers, particularly regarding the novelty and practical significance of the proposed methods, as well as the theoretical concerns raised, I believe that the paper does not meet the high standards expected for acceptance at a top-tier conference. The contributions, while interesting, do not sufficiently advance the field in a way that justifies publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents a theoretical exploration of the convergence properties of three classical reinforcement learning algorithms (Temporal Difference Learning, Fitted Value Iteration, and Residual Minimization) in the context of overparameterized linear function approximation. The authors aim to provide insights into how these algorithms behave differently in an overparameterized regime compared to traditional underparameterized settings.\n\n#### Strengths:\n1. **Novelty of Perspective**: The paper addresses an important and timely topic in reinforcement learning, particularly as deep learning models become increasingly overparameterized. The exploration of fixed points in this context is a valuable contribution to the literature.\n2. **Theoretical Contributions**: The authors provide a unified interpretation of the convergence properties of the algorithms under study, which is a significant theoretical advancement. They also propose new regularizers that improve stability and performance in empirical settings.\n3. **Empirical Validation**: The paper includes empirical results that demonstrate the effectiveness of the proposed regularizers, which adds practical relevance to the theoretical findings.\n\n#### Weaknesses:\n1. **Lack of Novelty in Some Results**: Several reviewers pointed out that some of the results presented in the paper are not sufficiently novel and have been previously established in the literature. For instance, the fixed point solutions and certain optimization results were noted to be similar to existing work without adequate citation or acknowledgment.\n2. **Theoretical Rigor**: There are concerns regarding the correctness of some theoretical claims, with reviewers highlighting potential errors in equations and proofs. This raises questions about the overall rigor of the theoretical contributions.\n3. **Empirical Evaluation**: The empirical experiments were criticized for being conducted in overly simplistic environments, which may not adequately demonstrate the practical applicability of the proposed methods. Additionally, the experiments did not convincingly address the complexities of real-world scenarios where the algorithms would be applied.\n4. **Clarity and Presentation**: Several reviewers noted issues with the clarity of the writing and the presentation of mathematical results. This could hinder the paper's accessibility to a broader audience.\n\n#### Reviewer Consensus:\nThe reviews reflect a mix of enthusiasm for the theoretical contributions and skepticism regarding the novelty and rigor of the results. While some reviewers leaned towards acceptance, citing the importance of the topic and the potential for future work, others expressed significant concerns about the correctness and clarity of the paper.\n\n#### Author Responses:\nThe authors provided responses to reviewer comments, indicating a willingness to address concerns and clarify points of confusion. However, the responses did not fully alleviate the doubts raised by reviewers regarding the novelty of certain results and the empirical validation of their claims.\n\n### Conclusion\nGiven the mixed reviews, the paper presents a valuable exploration of an important topic but suffers from significant issues related to novelty, theoretical rigor, and empirical validation. The concerns raised by multiple reviewers about the correctness of results and the clarity of presentation suggest that the paper may not yet meet the high standards expected for publication at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration\" proposes a new training objective aimed at addressing the text degeneration issues commonly faced by autoregressive language models (LMs) trained with cross-entropy loss. The authors introduce a contrastive token learning (CT) objective that aims to improve the generation quality by distinguishing between positive (target) tokens and negative (repetitive) tokens.\n\n### Strengths:\n1. **Clarity and Motivation**: The paper is well-written and presents its ideas clearly. The motivation behind the proposed method is sound, as it addresses a known issue in language generation.\n2. **Experimental Results**: The authors report improvements in generation quality and reduced repetition in their experiments, which is a positive aspect of the work.\n3. **Reproducibility**: The authors have provided code and resources for reproducibility, which is a valuable contribution.\n\n### Weaknesses:\n1. **Novelty**: Multiple reviewers have pointed out that the novelty of the proposed method is limited. The core idea appears to be an incremental improvement over existing unlikelihood training (UL) methods. Reviewer 1 explicitly states that the contributions are marginally significant or novel, and this sentiment is echoed by others.\n2. **Experimental Setup**: The experimental setup is criticized for being insufficient. The use of GPT-2 small and the limited range of tasks do not convincingly demonstrate the effectiveness of the proposed method. Reviewers suggest that experiments should be conducted on larger models and a broader range of tasks to validate the claims.\n3. **Metrics and Evaluation**: The choice of metrics for evaluation has been questioned. Perplexity is not considered a reliable measure of generation quality, and the lack of more comprehensive metrics (like dist-2,3,4) raises concerns about the robustness of the results. Additionally, the human evaluation results do not show statistical significance, which undermines the claims of improved fluency and reduced repetition.\n4. **Comparison with Baselines**: The paper lacks comparisons with crucial baselines, such as ancestral sampling and typical sampling, which could provide a more comprehensive understanding of the proposed method's performance.\n\n### Conclusion:\nWhile the paper presents an interesting idea and is well-written, the concerns regarding novelty, experimental rigor, and evaluation metrics are significant. The lack of a robust experimental setup and the limited novelty suggest that the paper does not meet the standards expected for publication in a top-tier conference. The reviewers' consensus leans towards rejection, primarily due to the thin novelty and insufficient experimental validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"On the Implicit Biases of Architecture & Gradient Descent\" investigates the interplay between neural network architecture and the optimization process (gradient descent) in determining generalization performance. The authors propose new theoretical tools to analyze the average test error of neural networks under the Gaussian process (NNGP) framework and explore how gradient descent can enhance generalization by selecting functions with larger margins.\n\n#### Strengths:\n1. **Theoretical Contributions**: The paper presents a novel PAC-Bayes bound that is non-vacuous and analytically tractable, which is a significant contribution to the field. The authors also provide insights into the relationship between architecture, gradient descent, and generalization, which is a critical area of research in deep learning.\n\n2. **Clarity and Presentation**: The paper is well-written and presents its findings clearly. The authors effectively communicate complex ideas, making the paper accessible to a broader audience.\n\n3. **Novel Insights**: The exploration of the implicit biases introduced by both architecture and gradient descent is an interesting angle that adds depth to the existing literature. The authors' findings regarding the relationship between margin and generalization are thought-provoking.\n\n#### Weaknesses:\n1. **Experimental Validation**: A significant concern raised by multiple reviewers is the limited experimental validation of the theoretical claims. The reliance on a specific optimizer (\"Nero\") without comparison to other optimization methods (like SGD) raises questions about the generalizability of the results. The experiments do not sufficiently cover a range of architectures or datasets, which limits the robustness of the conclusions.\n\n2. **Incremental Contributions**: While the theoretical results are interesting, some reviewers argue that they are not sufficiently novel compared to existing literature, particularly the work by Valle-Pérez et al. (2019). The paper does not convincingly demonstrate how its contributions advance the field beyond what is already known.\n\n3. **Confusion in Terminology**: The use of terms like \"implicit bias of gradient descent\" has been criticized for being misleading. The authors acknowledge this and plan to clarify their language, but the initial confusion may detract from the paper's impact.\n\n4. **Lack of Comprehensive Comparisons**: The paper does not adequately compare its findings with existing literature, particularly regarding the performance of NNGP versus GD-trained networks. This lack of context makes it difficult to assess the significance of the authors' claims.\n\n5. **Concerns About Margin Analysis**: Reviewers expressed skepticism about the claims regarding margin and its implications for generalization. The authors' arguments need to be more rigorously supported, especially in light of existing literature that may contradict their findings.\n\n#### Reviewer Consensus:\nThe reviewers are divided, with some expressing strong concerns about the paper's experimental rigor and clarity, while others appreciate the theoretical contributions. The overall sentiment leans towards the paper being premature for publication in a top-tier conference, primarily due to the lack of comprehensive experimental validation and the need for clearer articulation of its contributions.\n\n### Conclusion\nGiven the strengths and weaknesses outlined, the paper presents interesting theoretical insights but lacks sufficient empirical validation and clarity in its claims. The concerns raised by multiple reviewers about the experimental setup and the novelty of the contributions suggest that the paper is not yet ready for publication in a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"A Frequency Perspective of Adversarial Robustness\" presents a novel approach to understanding adversarial examples in deep learning through frequency analysis. The authors argue that adversarial examples are not strictly high-frequency noise, as commonly believed, but are instead dataset-dependent. They also propose methods for adversarial training that incorporate frequency constraints.\n\n#### Strengths:\n1. **Novel Perspective**: The paper challenges a widely held misconception about adversarial examples being high-frequency phenomena. This is a significant contribution as it opens up new avenues for research and understanding in the field of adversarial robustness.\n2. **Empirical Analysis**: The authors provide extensive empirical results that support their claims, including experiments across different datasets and architectures. This strengthens the validity of their findings.\n3. **Practical Implications**: The proposed frequency-based adversarial training method could have practical implications for improving adversarial robustness, which is a critical area of research in machine learning.\n\n#### Weaknesses:\n1. **Lack of Clarity in Contributions**: Several reviewers pointed out that the unique contributions of the paper were not clearly articulated. While the authors have attempted to clarify this in their response, the initial confusion may have affected the paper's reception.\n2. **Limited Novelty**: Some reviewers noted that the conclusion that adversarial examples are dataset-dependent is not new and has been previously established in the literature. This raises questions about the novelty of the findings.\n3. **Insufficient Exploration of Other Attacks**: The paper primarily focuses on PGD attacks, which limits the generalizability of the findings. The authors did mention additional experiments with other attacks in their response, but these were not included in the main paper.\n4. **Presentation Issues**: Minor grammatical errors and the presentation of figures were noted as areas needing improvement. While these are not critical to the paper's scientific contribution, they do affect readability and professionalism.\n\n#### Reviewer Consensus:\nThe reviewers were divided in their opinions. Some found the insights valuable and recommended acceptance, while others raised significant concerns about clarity, novelty, and the scope of the experiments. The authors' responses addressed many of these concerns, but the initial lack of clarity regarding contributions and the limited exploration of different attacks remain points of contention.\n\n#### Conclusion:\nGiven the strengths of the paper, particularly its novel perspective and empirical analysis, it has the potential to contribute meaningfully to the field of adversarial robustness. However, the concerns regarding clarity of contributions and the limited scope of attacks are significant. The authors have committed to addressing these issues in a revised version, which could enhance the paper's impact.\n\nConsidering the potential for improvement and the overall contributions, I recommend acceptance with the expectation that the authors will refine their presentation and clarify their contributions in future revisions.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" presents a novel approach to out-of-distribution (OOD) detection by leveraging graph structures and topological properties. The authors claim to achieve competitive performance on the LSUN dataset, with high AUROC scores for both far-OOD and near-OOD detection tasks. However, the reviews from multiple reviewers highlight several significant issues that raise concerns about the paper's readiness for publication in a top-tier conference.\n\n### Strengths:\n1. **Interesting Idea**: The concept of using graph structures to represent relationships among features in images is intriguing and could lead to advancements in OOD detection.\n2. **High Performance**: The reported performance metrics (97.95% AUROC for far-OOD and 98.79% AUROC for near-OOD) suggest that the method has potential.\n\n### Weaknesses:\n1. **Lack of Clarity and Rigor**: Multiple reviewers pointed out that the paper is poorly written, with grammatical errors and unclear terminology. The use of vague terms like \"common sense\" and \"experience\" without rigorous definitions detracts from the paper's credibility.\n2. **Insufficient Experimental Validation**: The empirical results are not convincing. The paper does not adequately compare its method against state-of-the-art techniques in OOD detection, which is crucial for establishing the significance of the contributions. The reliance on a single dataset (LSUN) limits the generalizability of the findings.\n3. **Methodological Concerns**: The method's reliance on a pre-trained object detection network raises questions about its applicability to diverse datasets, particularly those where the object detector may not perform well. This could lead to incomplete or erroneous semantic graphs, undermining the entire approach.\n4. **Lack of Detail**: Reviewers noted that the paper lacks sufficient detail regarding the methodology, making it difficult for others to reproduce the results. Key aspects such as the architecture of the detector, the feature extraction process, and the experimental setup are inadequately described.\n5. **Novelty Issues**: While the idea of using graph structures is interesting, reviewers expressed concerns about the novelty of the approach, suggesting that it combines existing methods without introducing significant new concepts.\n\n### Conclusion:\nGiven the significant weaknesses identified by the reviewers, particularly regarding clarity, methodological rigor, and empirical validation, the paper does not meet the standards expected for publication in a top-tier conference. The lack of a robust comparison with existing methods and the insufficient detail provided for reproducibility further support the decision to reject the paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes\" presents a novel approach to neural network compression through the lens of tropical geometry. The authors claim to provide theoretical contributions that lead to a new understanding of neural network approximation, specifically focusing on the Hausdorff distance of tropical zonotopes. They also propose a compression algorithm based on K-means clustering and evaluate its performance empirically.\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper introduces a new theoretical framework for understanding neural network approximation using tropical geometry, which is a fresh perspective in the field. The authors have made an effort to connect this theory to practical applications in neural network compression.\n2. **Mathematical Rigor**: The analysis of approximation error and the bounds provided are mathematically sound, which adds credibility to the theoretical claims.\n3. **Novelty**: The approach of using tropical geometry for structured pruning is relatively unexplored, which could inspire further research in this area.\n\n#### Weaknesses:\n1. **Empirical Evaluation**: The experiments are primarily conducted on small-scale datasets (MNIST and Fashion-MNIST) and older architectures (LeNet). This raises concerns about the generalizability and practical applicability of the proposed method. The reviewers pointed out the need for experiments on more modern architectures and larger datasets (e.g., CIFAR-10/100, ImageNet).\n2. **Comparative Analysis**: The paper lacks a comprehensive comparison with state-of-the-art methods. While the authors claim to compare their method with ThiNet, which is outdated, they do not provide sufficient evidence to support their claims of improvement over modern techniques. The reviewers highlighted the absence of comparisons with more recent methods, which is crucial for establishing the significance of the proposed approach.\n3. **Clarity and Scope**: The authors' responses indicate that they have made efforts to clarify the scope of their work, emphasizing that it is a theoretical contribution rather than a breakthrough in pruning techniques. However, the initial presentation of the paper may have led to misunderstandings regarding its aims and contributions.\n4. **Implementation Details**: The reviewers noted the lack of detailed information regarding the computational complexity and runtime of the proposed algorithm. While the authors provided some insights in their response, a more thorough discussion in the paper itself would enhance its clarity and reproducibility.\n\n#### Reviewer Feedback:\nThe reviewers provided a mix of positive and negative feedback. While they acknowledged the theoretical contributions and the novelty of the approach, they also raised valid concerns regarding the empirical validation and the need for clearer comparisons with state-of-the-art methods. The authors' responses addressed some of these concerns, but the overall empirical contribution remains limited.\n\n### Conclusion\nGiven the strengths of the theoretical contributions and the novelty of the approach, the paper has potential value for the community, particularly for researchers interested in tropical geometry and neural network approximation. However, the empirical results are not sufficiently robust, and the lack of comprehensive comparisons with modern methods limits the paper's impact. The authors have made efforts to clarify their contributions and scope, but the experimental validation still requires significant improvement.\n\nConsidering these factors, I recommend that the paper be rejected at this time, with the suggestion that the authors conduct more extensive empirical evaluations and comparisons with state-of-the-art methods before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" presents a novel framework that integrates Monte-Carlo Tree Search (MCTS) with Transformers in the context of offline reinforcement learning, specifically applied to the game SameGame. While the idea of combining these two methodologies is intriguing and has potential implications for improving exploration and evaluation in reinforcement learning, the reviewers have raised several significant concerns that must be addressed.\n\n### Strengths:\n1. **Novelty of Concept**: The integration of Transformers with MCTS is a novel approach that could pave the way for future research in this area. The idea of using Transformers to enhance the exploration capabilities of MCTS is a promising direction.\n2. **Performance Claims**: The authors claim that their method outperforms both Transformer-only and MCTS-only solutions, which is a positive indication of the framework's effectiveness.\n\n### Weaknesses:\n1. **Limited Empirical Evaluation**: All reviewers pointed out that the empirical evaluation is limited to a single domain (SameGame), which is considered relatively toy-like. The lack of comparisons with more established benchmarks (e.g., MiniGo, Atari) and other relevant algorithms (e.g., AlphaZero) raises concerns about the generalizability and robustness of the results.\n2. **Clarity and Presentation Issues**: The writing quality and clarity of the paper have been criticized. Reviewers noted that the exposition is unclear, making it difficult to understand the methodology and results. This is a significant issue for a top-tier conference where clarity is paramount.\n3. **Comparative Analysis**: The reviewers expressed concerns about the fairness of the comparisons made in the experiments. For instance, the lack of a learned prior policy in the MCTS comparisons and the absence of comparisons to other MCTS-based methods with neural networks weaken the empirical claims of the paper.\n4. **Technical Novelty**: Several reviewers questioned the originality of the approach, suggesting that it closely resembles existing frameworks like AlphaGo/AlphaZero, with only superficial modifications. This raises doubts about the contribution's significance.\n\n### Conclusion:\nGiven the substantial concerns raised by the reviewers regarding the empirical evaluation, clarity of presentation, and the novelty of the contributions, it is clear that the paper is not ready for acceptance at this time. While the idea has potential, the execution and supporting evidence are lacking. The authors would benefit from addressing the reviewers' comments, expanding their empirical evaluation, and improving the clarity of their writing before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Enriching Online Knowledge Distillation with Specialist Ensemble\" presents a novel approach to online knowledge distillation (KD) by introducing a label prior shift to diversify teacher models and employing a post-compensated Softmax aggregation method. The authors claim that their method enhances the diversity of teacher models, which is crucial for effective knowledge distillation.\n\n#### Strengths:\n1. **Clarity and Structure**: The paper is well-written and structured, making it easy to follow. The authors provide a clear motivation for their work and articulate the problem they aim to solve.\n2. **Empirical Analysis**: The authors conducted extensive experiments, demonstrating improvements over selected baselines in terms of accuracy and calibration error. The ablation studies included are a positive aspect, showing the authors' commitment to understanding the contributions of their method.\n3. **Novelty**: While some components of the proposed method are not entirely new, the integration of these elements into a cohesive framework for online KD is presented as a novel contribution.\n\n#### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the contributions of the paper are marginally significant or novel. The methods employed (label prior shift, importance sampling, and PC-Softmax) have been discussed in prior works, and the paper does not sufficiently differentiate its contributions from existing literature.\n2. **Experimental Comparisons**: The paper lacks comparisons with more recent and relevant methods in the field, such as CGL and L-MCL, which have shown better performance. Although the authors conducted additional experiments in response to reviewer comments, the initial lack of comprehensive comparisons raises concerns about the robustness of their claims.\n3. **Specificity of Setup**: The reviewers noted that the proposed method seems overly specialized for online distillation and peer networks. The authors did not convincingly argue why their approach could not be applied to offline distillation or non-peer networks, which limits the generalizability of their findings.\n4. **Minor Issues**: Several reviewers pointed out minor issues with clarity and correctness, including typos and unclear statements. While these do not fundamentally undermine the paper, they detract from its overall quality.\n\n#### Reviewer Feedback:\n- Reviewer 1 highlighted the need for more ablation studies and clearer explanations for the choice of baselines.\n- Reviewer 2 raised concerns about the novelty and the consistency of results with existing methods.\n- Reviewer 3 acknowledged the empirical analysis but noted that many component ideas have been previously introduced.\n- Reviewer 4 pointed out the lack of comparisons with recent works and suggested that the performance improvements were not significant enough.\n- Reviewer 5 echoed concerns about the novelty and the need for more convincing experimental results.\n\n#### Author Responses:\nThe authors provided detailed responses to reviewer comments, indicating a willingness to improve the paper. They conducted additional experiments and clarified some points in their writing. However, the responses did not fully address the concerns regarding the novelty and the need for broader comparisons with recent methods.\n\n### Conclusion\nWhile the paper presents an interesting approach to online knowledge distillation and demonstrates some empirical improvements, the concerns regarding the limited novelty, insufficient experimental comparisons, and specificity of the setup are significant. The paper does not convincingly establish its contributions as being sufficiently novel or impactful for a top-tier conference. \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" presents a novel adversarial training algorithm aimed at enhancing the robustness of deep neural networks against adversarial attacks. The authors claim that their method, ARoW, is theoretically motivated and empirically superior to existing algorithms, particularly in the context of multi-class classification problems.\n\n#### Strengths:\n1. **Theoretical Contribution**: The authors provide a new theoretical framework (Theorem 1) that extends existing results from TRADES to multi-class classification. This is a notable contribution, as it addresses a gap in the literature regarding the robustness of multi-class classifiers.\n2. **Empirical Results**: The paper includes extensive experiments demonstrating that ARoW outperforms existing methods in terms of both clean and robust accuracy across several datasets (CIFAR10, F-MNIST, SVHN). The authors also conduct ablation studies to analyze the effects of different components of their method.\n3. **Clarity and Structure**: The paper is well-written and clearly structured, making it accessible to readers. The authors effectively communicate their motivation, methodology, and results.\n\n#### Weaknesses:\n1. **Theoretical Claims**: Several reviewers raised concerns about the validity and significance of Theorem 1. They argue that the theorem does not provide a substantial improvement over existing results and that the bounds presented are either loose or trivial in certain cases (e.g., binary classification). The authors acknowledged these concerns in their rebuttal but did not sufficiently address the fundamental issues raised regarding the novelty and rigor of their theoretical contributions.\n2. **Marginal Improvements**: While the empirical results show that ARoW outperforms existing methods, many reviewers noted that the improvements are marginal, particularly when compared to methods like HAT. This raises questions about the practical significance of the proposed method.\n3. **Experimental Setup**: Some reviewers pointed out discrepancies in the reported results for baseline methods, suggesting that the authors may not have optimally tuned their baselines. This could undermine the validity of the claims regarding ARoW's superiority.\n4. **Limited Dataset Diversity**: The experiments are primarily conducted on smaller datasets. Reviewers expressed concerns about the generalizability of the results to larger datasets, which is crucial for establishing the robustness of the proposed method in real-world applications.\n\n#### Reviewer Consensus:\nThe reviewers are divided in their opinions. Some express strong support for the paper, citing its theoretical and empirical contributions, while others are critical of its theoretical claims and the marginal nature of its empirical improvements. The authors' responses to reviewer comments indicate a willingness to clarify and improve the manuscript, but they do not fully resolve the concerns regarding the theoretical contributions and the significance of the empirical results.\n\n### Conclusion\nGiven the mixed feedback from reviewers, particularly regarding the theoretical contributions and the marginal improvements over existing methods, the paper does not meet the high standards typically expected for acceptance at a top-tier conference. The concerns about the theoretical rigor and the practical significance of the results are substantial enough to warrant rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Sample and Computation Redistribution for Efficient Face Detection\" presents a novel approach to face detection that addresses the challenges of high computation costs and the detection of small faces in low-resolution images. The authors propose two methods: Computation Redistribution (CR) and Sample Redistribution (SR), which aim to optimize the network architecture and training samples for improved performance.\n\n#### Strengths:\n1. **Performance Improvement**: The proposed methods demonstrate significant improvements in detection accuracy, particularly for small faces, as evidenced by the results on the WIDER FACE dataset. The SCRFD-34GF model outperforms existing state-of-the-art methods like TinaFace by a notable margin while being computationally efficient.\n2. **Ablation Studies**: The authors provide a detailed ablation study that supports the effectiveness of both CR and SR methods, showing that they contribute positively to the overall performance.\n3. **Practical Relevance**: The methods are practical and relevant for real-world applications, especially in scenarios where computational resources are limited, such as mobile devices.\n\n#### Weaknesses:\n1. **Technical Novelty**: Several reviewers noted that the methods, while effective, are somewhat straightforward and lack significant novelty. The search strategies employed in CR were described as not particularly interesting, and there is a call for more innovative approaches.\n2. **Comparative Analysis**: The paper lacks comprehensive comparisons with other state-of-the-art network search methods. While the authors have added some comparisons in their responses, the initial lack of such comparisons raised concerns about the robustness of their claims regarding the superiority of their methods.\n3. **Clarity and Detail**: Some reviewers pointed out that the paper could benefit from clearer explanations and additional figures to illustrate the proposed methods. The authors have addressed some of these concerns in their responses, but the initial clarity of the paper was questioned.\n\n#### Reviewer Feedback:\n- Reviewers generally acknowledged the effectiveness of the proposed methods but expressed concerns about the novelty and the need for more rigorous comparisons with existing methods.\n- The responses from the authors indicate a willingness to improve the paper based on reviewer feedback, which is a positive sign.\n\n### Conclusion\nWhile the paper presents a valuable contribution to the field of face detection, the concerns regarding its novelty and the adequacy of comparative analysis with other methods are significant. The improvements made in response to reviewer comments are commendable, but they may not fully address the initial weaknesses highlighted by multiple reviewers. Given the standards of a top-tier conference, where both novelty and rigorous evaluation against existing methods are critical, the paper may not meet the threshold for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform aimed at optimizing multi-objective problems using Bayesian optimization techniques. The authors claim that their platform, AutoOED, is user-friendly and modular, allowing researchers to design and evaluate their own multi-objective Bayesian optimization (MOBO) algorithms efficiently.\n\n#### Strengths:\n1. **Motivation and Relevance**: The problem of optimal experimental design (OED) is well-motivated, particularly in fields where multiple conflicting objectives must be balanced. The authors highlight the need for accessible tools in this area, which is a valid and important concern.\n\n2. **User-Friendliness**: The emphasis on a graphical user interface (GUI) for users with little coding experience is a significant contribution. This could potentially broaden the user base for MOBO techniques, making them more accessible to practitioners in various fields.\n\n3. **Modular Design**: The modular framework allows for flexibility in designing and testing different algorithms, which is a practical advantage for researchers looking to experiment with various optimization strategies.\n\n4. **Asynchronous Optimization**: The introduction of the Believer-Penalizer (BP) strategy for asynchronous optimization is a novel aspect of the platform, which could provide benefits in terms of efficiency.\n\n#### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the contributions of the paper are primarily engineering-oriented, with limited methodological advancements. The novelty of the BP strategy and the overall platform design is questioned, as many aspects appear to be incremental improvements over existing systems.\n\n2. **Performance Claims**: The empirical results presented in the paper are mixed. Reviewers noted that the performance of AutoOED is not consistently superior to existing platforms, and some claims regarding its effectiveness were seen as overstated. The authors' response to these concerns did not sufficiently address the discrepancies highlighted by the reviewers.\n\n3. **Theoretical Justification**: The lack of theoretical backing for the BP strategy raises concerns about its robustness and generalizability. While empirical results are important, a solid theoretical foundation is often expected in top-tier conferences.\n\n4. **Scope of Application**: The initial claim that AutoOED can handle any number of objectives was later clarified to be limited to 2 or 3 objectives, which could mislead potential users. Although the authors have since updated the platform to support more objectives, this limitation was a significant concern during the review process.\n\n5. **Comparison with Existing Tools**: The paper's comparisons with existing Bayesian optimization tools were criticized for being insufficient. While the authors have made efforts to include more comparisons in their revisions, the initial lack of comprehensive benchmarking undermined the paper's credibility.\n\n#### Conclusion:\nWhile AutoOED presents a potentially useful tool for the community, the concerns raised by the reviewers regarding its novelty, empirical validation, and theoretical grounding are significant. The paper appears to be more of a software engineering contribution rather than a groundbreaking methodological advancement, which may not align with the expectations of a top-tier conference. The authors have made efforts to address some of the reviewers' concerns, but the fundamental issues regarding the novelty and robustness of the contributions remain.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"CAT: Collaborative Adversarial Training\" proposes a novel framework for adversarial training that aims to improve the robustness of neural networks by leveraging the strengths of different adversarial training methods. The authors observe that different models trained with distinct adversarial strategies exhibit varying performance on adversarial examples, leading to the idea of collaborative training where two models are trained simultaneously and share adversarial examples to guide each other's learning.\n\n### Strengths:\n1. **Clear Presentation**: The paper is well-written and easy to follow, which is essential for conveying complex ideas in a research context.\n2. **Promising Results**: The authors report state-of-the-art robustness on CIFAR-10 under the Auto-Attack benchmark, which is a significant achievement.\n3. **Innovative Approach**: The idea of combining different adversarial training methods is interesting and has the potential to contribute to the field of adversarial machine learning.\n\n### Weaknesses:\n1. **Lack of Novelty**: Several reviewers pointed out that the concept of combining adversarial training methods is not entirely new and has been explored in previous works. The paper does not sufficiently differentiate itself from existing ensemble methods or provide a thorough comparison with them.\n2. **Theoretical Justification**: There is a notable absence of theoretical analysis explaining why the proposed collaborative approach would yield better results than traditional single-model training. This lack of theoretical grounding raises questions about the robustness of the claims made.\n3. **Experimental Limitations**: The experiments are primarily conducted on small datasets (CIFAR-10 and CIFAR-100), which may not be representative of the method's effectiveness on larger, more complex datasets. Additionally, the paper lacks comparisons with more recent state-of-the-art methods and does not explore the impact of using more than two models or different architectures.\n4. **Computational Cost**: The proposed method doubles the computational cost, which is a significant drawback in practical applications. The reviewers raised concerns about the efficiency of training two models from scratch, suggesting that a single model trained with multiple adversarial examples might be a more efficient alternative.\n5. **Insufficient Ablation Studies**: The reviewers noted the need for more ablation studies to understand the contributions of various components of the proposed method. This would help clarify the effectiveness of the collaborative approach compared to other strategies.\n\n### Conclusion:\nWhile the paper presents an interesting approach to adversarial training and achieves promising results, the concerns regarding novelty, theoretical justification, and experimental rigor are significant. The lack of a robust comparison with existing methods and the high computational cost further weaken the case for acceptance. Given the standards of a top-tier conference, the paper does not sufficiently address these critical issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to numerical representation in deep learning, specifically through the introduction of Block Minifloat (BM) formats. The authors claim that their method allows for efficient training of deep neural networks (DNNs) using low-precision representations (4-8 bits) while maintaining accuracy comparable to standard floating-point formats.\n\n#### Strengths:\n1. **Novelty and Contribution**: The introduction of the Block Minifloat representation is a significant contribution to the field of DNN training, particularly in the context of hardware efficiency. The paper builds on existing concepts but extends them in a way that could lead to practical improvements in DNN training efficiency.\n   \n2. **Hardware Evaluation**: The authors provide a thorough hardware evaluation, demonstrating that their BM formats can lead to substantial reductions in area and power consumption compared to FP8 formats. This is a critical aspect, as hardware implementation is often a bottleneck in deploying DNNs in resource-constrained environments.\n\n3. **Experimental Validation**: The paper includes experiments across various models and datasets, showing that the proposed representation can achieve competitive accuracy with significantly reduced resource requirements.\n\n4. **Revisions and Clarifications**: The authors have made substantial revisions in response to reviewer comments, addressing concerns about clarity, methodology, and the implications of their findings. This responsiveness indicates a commitment to improving the quality of their work.\n\n#### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the concept of shared exponent bias is not entirely new and that the novelty of the BM representation may be limited. While the authors argue that their approach offers distinct advantages, the foundational ideas are built on existing work, which may dilute the perceived innovation.\n\n2. **Complexity in Implementation**: Reviewer 2 raised concerns about the overhead associated with using multiple BM formats during training. The authors assert that this has been accounted for in their hardware design, but the complexity of managing different formats could pose practical challenges in real-world applications.\n\n3. **Clarity and Presentation**: Despite improvements, some reviewers noted that the presentation of results and methodology could still be confusing. The paper would benefit from clearer definitions and explanations, particularly regarding the mathematical formulations and experimental setups.\n\n4. **Lack of Larger Network Testing**: While the authors focused on smaller networks, the absence of experiments on larger architectures like ResNet50 limits the generalizability of their findings. The authors acknowledge this limitation but do not provide a strong justification for not including larger networks.\n\n5. **Denormal Handling**: The handling of denormal numbers and the associated hardware complexity was questioned. While the authors provided some clarification, the implications of this handling on performance and accuracy remain somewhat ambiguous.\n\n### Conclusion\nOverall, the paper presents a promising approach to improving the efficiency of DNN training through innovative numerical representations. The hardware evaluation and experimental results are compelling, but concerns about the novelty of the contribution, complexity of implementation, and clarity of presentation remain significant. The authors have made commendable efforts to address reviewer feedback, but the paper still requires further refinement to fully articulate its contributions and implications.\n\nGiven the strengths and weaknesses outlined, I believe the paper has potential but may not meet the high standards expected for acceptance at a top-tier conference without further improvements.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform aimed at optimizing multi-objective problems in experimental design using Bayesian optimization techniques. The authors claim that their platform, AutoOED, introduces a novel strategy called Believer-Penalizer (BP) for asynchronous optimization and provides a user-friendly graphical interface for non-expert users.\n\n#### Strengths:\n1. **Motivation and Relevance**: The problem of optimal experimental design is well-motivated, and the need for user-friendly tools in this domain is clear. The authors highlight the gap in existing software that caters to non-expert users, which is a significant contribution.\n2. **User-Friendliness**: The emphasis on a graphical user interface (GUI) and modular design is a strong point, as it lowers the barrier for entry for users who may not be familiar with coding or complex optimization techniques.\n3. **Empirical Evaluation**: The authors conducted extensive empirical evaluations across multiple benchmarks, which is a positive aspect of the paper. They also provide a real-world application example, which adds practical relevance.\n\n#### Weaknesses:\n1. **Novelty and Scientific Contribution**: Multiple reviewers expressed concerns regarding the novelty of the contributions. The main methodological innovation, the BP strategy, is described as straightforward and lacking rigorous theoretical backing. The reviewers noted that the contributions are largely incremental and do not significantly advance the state of the art in multi-objective optimization.\n2. **Performance Claims**: Reviewers pointed out inconsistencies in the performance claims made in the paper. Figures presented in the paper do not convincingly support the assertion that AutoOED consistently outperforms existing methods. The empirical results appear to be overstated, with some benchmarks showing AutoOED underperforming compared to other methods.\n3. **Limited Scope**: Initially, the platform was claimed to handle multi-objective problems with more than two objectives, but reviewers noted that it was primarily effective for two or three objectives. Although the authors have since claimed to have addressed this limitation, the initial presentation may mislead readers about the platform's capabilities.\n4. **Theoretical Insights**: The lack of theoretical insights into the BP strategy and its combination of existing methods (Kriging Believer and Local Penalization) raises questions about the robustness and generalizability of the proposed approach.\n\n#### Reviewer Feedback:\nThe feedback from the reviewers was mixed, with some appreciating the engineering contributions and user-friendly aspects of the platform, while others were critical of the scientific rigor and novelty. The consensus leaned towards the view that while the platform is well-engineered, it does not provide sufficient novel methodological contributions to warrant acceptance at a top-tier conference.\n\n#### Author Responses:\nThe authors provided clarifications and updates in response to reviewer comments, emphasizing the platform's usability and modular design. They acknowledged the limitations and made some revisions to the paper. However, the core concerns regarding novelty and empirical validation remain largely unaddressed.\n\n### Conclusion\nGiven the mixed reviews, the lack of significant methodological contributions, and the concerns regarding the empirical results, I believe that the paper does not meet the standards expected for acceptance at a top-tier conference. While the engineering aspects are commendable, the scientific contributions are insufficiently novel and robust.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Functional Risk Minimization\" presents a novel framework that challenges the traditional assumptions of supervised learning by proposing that each data point is generated from its own function rather than a single function with noise. This approach aims to provide a more realistic representation of noise in data and introduces Functional Risk Minimization (FRM) as a new training objective.\n\n#### Strengths:\n1. **Novelty**: The concept of modeling each data point with its own function is innovative and has the potential to significantly impact the field of machine learning. The authors argue that FRM can lead to better performance in scenarios where data variations are structured, which is a relevant consideration in real-world applications.\n2. **Empirical Results**: The empirical results presented in the paper show that FRM outperforms Empirical Risk Minimization (ERM) in certain scenarios, particularly with non-uniform noise in regression tasks. The authors have also added new experiments to demonstrate the scalability of FRM to neural networks, which addresses a key concern raised by reviewers.\n3. **Potential Impact**: The authors articulate a compelling vision for how FRM could disrupt the current understanding of risk minimization in machine learning, especially in the context of over-parameterized models.\n\n#### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers noted that the presentation of the paper is confusing. The terminology used (e.g., the distinction between ERM and FRM) is not standard and could mislead readers. The authors have acknowledged this and committed to improving clarity in the final version, which is a positive step.\n2. **Theoretical Foundations**: There are significant concerns regarding the theoretical underpinnings of FRM. Reviewers pointed out that the paper lacks formal arguments to support claims about the performance of FRM compared to ERM. The authors have attempted to address these concerns in their response, but the clarity and rigor of these arguments need to be improved.\n3. **Scalability Issues**: While the authors claim that FRM can scale to larger models, the reviewers highlighted that the current implementation is computationally expensive and may not be practical for large datasets. The authors have provided additional experiments to support their claims, but the scalability of FRM remains a concern that needs further exploration.\n\n#### Reviewer Consensus:\nThe reviewers are divided in their opinions. While there is a general agreement on the novelty and potential significance of the work, concerns about clarity, theoretical justification, and scalability are prominent. Reviewer 1 is particularly critical, emphasizing the lack of connection between the theoretical framework and the proposed algorithm, while Reviewer 3 is more favorable, suggesting that the paper is marginally above the acceptance line.\n\n#### Author Responses:\nThe authors have made a concerted effort to address the reviewers' concerns, providing clarifications and additional experiments. They have acknowledged the need for clearer presentation and have committed to improving the theoretical justification for their claims. However, the effectiveness of these changes will depend on the final revisions and whether they adequately address the reviewers' critiques.\n\n### Conclusion:\nGiven the paper's innovative approach and the potential for significant contributions to the field, it is worth considering for acceptance. However, the concerns regarding clarity, theoretical rigor, and scalability are substantial and need to be addressed in the final version. The authors' willingness to revise and improve the paper is a positive sign, but the current state of the paper still leaves room for improvement.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Proximal Validation Protocol\" addresses a significant issue in machine learning regarding the trade-off between training data and validation data. The authors propose a novel method to construct a validation set using data augmentation and a distribution-preserving sampling strategy, which they claim can improve model evaluation without sacrificing training data.\n\n### Strengths:\n1. **Relevance of the Problem**: The issue of validation set construction is indeed a critical one in machine learning, especially in real-world applications where data is often limited. The paper identifies a gap in existing literature and proposes a solution, which is commendable.\n2. **Novelty**: The approach of using data augmentation to create a proximal validation set is a novel contribution that could potentially benefit the field.\n3. **Empirical Results**: The authors provide empirical evidence suggesting that their method outperforms traditional validation protocols across multiple data modalities.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers noted that the paper lacks clarity, particularly in the experimental section. The definitions of metrics and the presentation of results are confusing, which undermines the paper's contributions.\n2. **Empirical Evaluation**: Several reviewers pointed out that the empirical evaluation is insufficient. The datasets used are relatively small, and there is a lack of thorough ablation studies to understand the robustness of the proposed method. Additionally, the paper does not convincingly demonstrate how the proposed validation set leads to better model selection and hyperparameter tuning.\n3. **Theoretical Justification**: The lack of theoretical backing for the claims made in the paper is a significant concern. Reviewer 4 explicitly states that there is no reason to believe that the constructed proximal validation set is representative of the data generating distribution, which is a fundamental assumption in machine learning.\n4. **Generalizability**: The method appears to be tailored for classification tasks, and there is little discussion on its applicability to other types of tasks, such as regression or unsupervised learning.\n\n### Reviewer Consensus:\nThe reviewers are divided, with some recognizing the potential of the proposed method while others express significant concerns regarding its empirical and theoretical foundations. Reviewer 1 recommends a weak reject, citing unclear experiments and a lack of thoroughness. Reviewer 2 also highlights the need for more robust empirical support. Reviewer 3 sees merit in the paper but calls for more extensive experiments, while Reviewer 4 strongly recommends rejection due to the lack of theoretical justification.\n\n### Conclusion:\nGiven the significant concerns raised about the clarity of the presentation, the robustness of the empirical evaluation, and the lack of theoretical support for the claims made, it is clear that the paper does not meet the standards expected for publication at a top-tier conference. While the problem addressed is important and the proposed solution has potential, the current version of the paper does not provide sufficient evidence to support its claims or demonstrate its effectiveness convincingly.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Out-of-Distribution Image Detection Using the Normalized Compression Distance\" presents a novel approach to out-of-distribution (OOD) detection that does not require retraining the model or using OOD samples for validation. The proposed method, MALCOM, leverages a similarity metric based on the normalized compression distance to extract informative patterns from feature maps of convolutional neural networks (CNNs). \n\n#### Strengths:\n1. **Novelty and Relevance**: The problem of OOD detection is highly relevant, especially in safety-critical applications. The proposed method addresses practical constraints that are often overlooked in existing literature, making it a valuable contribution.\n2. **Experimental Evaluation**: The authors conducted experiments on several benchmarks and provided a thorough evaluation against state-of-the-art methods. The results indicate that MALCOM performs well under the specified constraints.\n3. **Clear Motivation**: The motivation for the proposed method is well-articulated, emphasizing the need for practical solutions in real-world applications.\n\n#### Weaknesses:\n1. **Comparison Methodology**: Reviewers raised concerns about the fairness of the comparisons made with existing methods. Specifically, the paper did not adequately justify why comparisons were made with methods that require OOD samples for validation, which could skew the results. The authors did attempt to address this in their response by conducting additional experiments, but the initial lack of clarity remains a significant issue.\n2. **Effectiveness of the Proposed Method**: There is skepticism regarding whether the improvements observed are due to the proposed method itself or merely minor tweaks. The reviewers suggested conducting more ablation studies to clarify the contributions of different components of the method.\n3. **Clarity and Presentation**: Several reviewers noted issues with the clarity of the writing and the presentation of results. The abstract, in particular, was criticized for its grammatical structure. While the authors have made efforts to improve the writing, the initial presentation may have affected the reviewers' perceptions of the work.\n4. **Theoretical Guarantees**: The lack of theoretical guarantees regarding the exhaustiveness of the proposed method in detecting all possible OOD examples raises concerns about its robustness and applicability in critical scenarios.\n\n#### Reviewer Feedback:\n- Reviewer 1 and Reviewer 2 both expressed that while the work is clever and novel, it still requires further refinement and justification, particularly regarding the constraints imposed on the method.\n- Reviewer 3 acknowledged the novelty but questioned the significance and efficiency of the approach, suggesting that the experiments could be more complex and comprehensive.\n\n#### Author Responses:\nThe authors provided a detailed response to the reviewers' comments, addressing concerns about the justification of constraints and the effectiveness of their method. They conducted additional experiments and clarified their methodology, which is commendable. However, the initial shortcomings in the paper may have impacted its reception.\n\n### Conclusion\nWhile the paper presents a novel approach to a relevant problem and has made efforts to address reviewer concerns, the initial weaknesses in comparison methodology, clarity, and theoretical grounding are significant. The reviewers' feedback indicates that the paper is not yet ready for publication in a top-tier conference, as it requires further refinement and validation of its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Protein Sequence and Structure Co-Design with Equivariant Translation\" presents a novel approach to protein design that aims to address the limitations of existing methods by proposing a framework that can jointly model protein sequences and structures. The authors claim that their method, ProtSeed, significantly improves inference speed and performance compared to state-of-the-art baselines.\n\n#### Strengths:\n1. **Novelty and Relevance**: The topic of protein design is highly relevant and important in bioengineering and computational biology. The proposed method attempts to tackle the challenge of co-designing protein sequences and structures, which is a significant advancement in the field.\n2. **Empirical Results**: The authors provide experimental results demonstrating that ProtSeed outperforms existing methods across multiple tasks. This empirical validation is crucial for establishing the effectiveness of their approach.\n3. **Clear Methodology**: The paper is generally well-written, and the methodology is presented clearly. The authors have made efforts to address reviewer concerns and have provided additional details in their responses.\n\n#### Weaknesses:\n1. **Claims of Novelty**: Several reviewers raised concerns about the novelty of the work, suggesting that aspects of the proposed method are similar to existing approaches, particularly AlphaFold2 and other co-design methods. The authors have acknowledged these concerns and agreed to revise their claims, but the initial over-claiming could mislead readers.\n2. **Lack of Details**: Reviewers pointed out the absence of specific hyperparameter settings, training details, and the need for a reproducibility statement. While the authors have committed to releasing their code and models upon acceptance, the lack of these details in the current submission is a significant drawback.\n3. **Discrepancies with Prior Work**: There are concerns regarding the comparison with previous models, particularly regarding the RMSD scores and the effectiveness of the proposed method relative to existing techniques. The authors have attempted to address these discrepancies, but the clarity and justification of their comparisons remain insufficient.\n4. **Ablation Studies**: While the authors conducted additional ablation studies, the initial lack of such studies raised questions about the contributions of different components of their model. The reviewers noted that the absence of ablation studies made it difficult to assess the significance of the proposed modifications.\n\n#### Reviewer Consensus:\nThe reviewers are divided in their opinions. Some reviewers express strong support for acceptance based on the empirical results and the potential impact of the work, while others highlight significant concerns regarding the novelty, clarity, and reproducibility of the claims made in the paper. The authors have made efforts to address these concerns in their responses, but the initial shortcomings in the manuscript may undermine the overall quality.\n\n### Conclusion\nGiven the strengths of the paper, particularly in terms of its relevance and empirical results, it shows promise for acceptance. However, the concerns regarding over-claiming, lack of methodological details, and the need for clearer comparisons with existing work are substantial. The authors have indicated a willingness to revise their claims and improve the manuscript based on reviewer feedback, which is a positive sign.\n\nConsidering the potential for improvement and the authors' responsiveness, I recommend acceptance with the expectation that the authors will address the highlighted weaknesses in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" presents a theoretical analysis of the convergence rates of gradient descent (GD) applied to deep linear neural networks. The authors claim to provide sharp rates of convergence that are independent of initialization types and show that the depth of the network does not affect the optimal rate of convergence, given sufficiently wide layers.\n\n#### Strengths:\n1. **Clarity and Structure**: The paper is well-written and structured, making it easy to follow the arguments and results presented. The authors have made efforts to clarify their contributions in the revised version, including a new section that discusses insights related to their main theorems.\n  \n2. **Novel Insights**: The observation that the trajectory of GD for deep linear networks closely follows that of the corresponding convex problem is intriguing and could provide valuable insights into the behavior of optimization in non-convex settings.\n\n3. **Technical Rigor**: The authors have provided a rigorous analysis of the convergence behavior of GD, and the results are well-supported by mathematical proofs.\n\n4. **Addressing Reviewer Concerns**: The authors have made significant revisions in response to reviewer comments, including clarifying the novelty of their results and addressing concerns about overclaims in the title and abstract.\n\n#### Weaknesses:\n1. **Novelty Concerns**: Several reviewers expressed concerns regarding the novelty of the contributions. While the authors claim that their main results extend existing work, particularly that of Du and Hu (2019), the reviewers noted that many of the results appear to be technical extensions rather than fundamentally new insights. The authors have attempted to clarify the technical challenges they overcame, but some reviewers remain skeptical about the significance of these contributions.\n\n2. **Dependence on Prior Work**: The paper builds heavily on existing results, and while it does extend them, the extent of this extension is debated. The reviewers noted that the convergence rates achieved were already established in prior work, which raises questions about the originality of the findings.\n\n3. **Lack of Intuitive Understanding**: Some reviewers pointed out that the paper lacks intuitive explanations for the results, particularly regarding the convergence region and the implications of overparameterization. The authors have added a new section to address this, but it remains to be seen if it sufficiently clarifies these points.\n\n4. **Minor Issues**: There are several minor issues related to clarity and correctness that were pointed out by reviewers, such as typographical errors and the need for clearer definitions. While these do not undermine the overall contributions, they indicate that the paper could benefit from further refinement.\n\n#### Conclusion:\nThe paper presents a solid theoretical contribution to the understanding of gradient descent in deep linear neural networks. However, the concerns regarding novelty and dependence on prior work are significant. The authors have made efforts to address these concerns, but the lingering doubts from multiple reviewers suggest that the contributions may not be sufficiently groundbreaking for a top-tier conference.\n\nGiven the mixed feedback from reviewers, particularly regarding the novelty and significance of the contributions, I recommend rejection. However, the authors have shown potential for improvement, and with further refinement and clearer articulation of their novel contributions, they could resubmit to a future conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning Sample Reweighting for Adversarial Robustness\" presents a novel approach to adversarial training by introducing a bilevel optimization framework that learns to reweight training samples based on a class-conditioned margin. The authors claim that their method improves both clean and robust accuracy compared to existing techniques.\n\n#### Strengths:\n1. **Novelty of Approach**: The paper proposes a learning-based reweighting strategy for adversarial robustness, which is a significant contribution to the field. The use of bilevel optimization and the integration of meta-learning (MAML) into adversarial training is an interesting angle that has not been extensively explored.\n2. **Clarity and Structure**: The paper is well-written and organized, making it accessible to readers. The methodology is sound, and the authors provide a clear explanation of their approach.\n3. **Empirical Results**: The experiments demonstrate improvements in both clean and robust accuracy on benchmark datasets, which supports the claims made in the paper.\n\n#### Major Concerns:\n1. **Limited Novelty**: Several reviewers pointed out that the approach is a direct adaptation of existing methods (like MAML) to adversarial training, which raises questions about its novelty. The contributions may not meet the high standards expected at a top-tier conference.\n2. **Evaluation of Adaptive Attacks**: A significant concern raised by multiple reviewers is the lack of evaluation against adaptive attacks. Given that the proposed method introduces a learned reweighting mechanism, it is crucial to assess how this affects the model's robustness against adaptive adversaries. The authors' response that this is \"beyond the scope\" of the paper is unsatisfactory, as it is a critical aspect of evaluating the effectiveness of their method.\n3. **Performance Margins**: The improvements reported in the paper are described as marginal, particularly under strong adversarial attacks like AutoAttack. The reliance on combining their method with TRADES to achieve good results raises questions about the standalone effectiveness of BiLAW.\n4. **Comparison with State-of-the-Art**: The paper lacks comparisons with more recent state-of-the-art methods, which diminishes the strength of the claims regarding improvements in robust accuracy. Reviewers noted that the authors should clarify how their method fits within the current landscape of adversarial training techniques.\n\n#### Minor Concerns:\n- The paper contains some minor typographical and notational inconsistencies that need to be addressed.\n- The discussion on the multi-class margin and its superiority over other input encodings lacks depth and theoretical justification.\n\n#### Reviewer Consensus:\nThe majority of reviewers expressed concerns about the novelty and completeness of the evaluation. While some reviewers were inclined to accept the paper based on its empirical results, the prevailing sentiment is that the paper does not sufficiently address critical aspects of its methodology and evaluation.\n\n### Conclusion\nGiven the concerns regarding novelty, the lack of evaluation against adaptive attacks, and the marginal improvements reported, the paper does not meet the standards expected for acceptance at a top-tier conference. The authors have made a commendable effort, but significant revisions and additional experiments are necessary to strengthen their claims and address the reviewers' concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" presents a method for predicting protein-protein interactions (PPIs) by leveraging pretrained structural embeddings from OmegaFold and utilizing graph neural networks (GNNs). The reviewers provided a range of comments, highlighting both strengths and weaknesses of the work.\n\n### Strengths:\n1. **Motivation**: The motivation for using structural embeddings in PPI prediction is well-founded. The authors recognize that similar proteins do not necessarily interact, which justifies the need for structural information.\n2. **Experimental Validation**: The authors conducted experiments on multiple datasets, which is a positive aspect of the study.\n\n### Weaknesses:\n1. **Limited Novelty**: Multiple reviewers pointed out that the contributions of the paper are marginally significant. The method primarily adopts existing techniques (OmegaFold embeddings and GNNs) without introducing substantial modifications or novel approaches. This lack of originality is a critical concern for a top-tier conference.\n2. **Weak Baselines**: Reviewers criticized the choice of baselines, suggesting that the comparisons made are not against state-of-the-art methods. This raises questions about the validity of the claimed improvements.\n3. **Clarity and Presentation**: Several reviewers noted issues with clarity, including unclear definitions of metrics (e.g., AUC) and the overall presentation of results. The tables were also criticized for not being easily digestible and lacking confidence intervals.\n4. **Empirical Results**: The improvements reported in the empirical results were described as marginal, and the lack of confidence intervals further undermines the robustness of the findings. Additionally, the absence of comparisons to other relevant methods (e.g., AlphaFold-Multimer) limits the paper's impact.\n5. **Technical Issues**: Reviewers raised concerns about the model design, particularly the necessity of using a GNN for a task that could potentially be addressed with simpler pairwise comparisons. The methodology for negative sampling was also unclear.\n\n### Conclusion:\nGiven the overall assessment, the paper suffers from significant limitations in novelty, clarity, and empirical validation. The reviewers collectively expressed concerns about the contributions being marginal and the methodology lacking rigor. While the motivation is strong, the execution does not meet the standards expected for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Behavior Proximal Policy Optimization\" (BPPO) presents a novel approach to offline reinforcement learning (RL) by leveraging the conservatism of online on-policy algorithms. The authors claim that their method can effectively address the challenges associated with offline RL without introducing additional constraints or regularization, which is a significant contribution to the field.\n\n#### Strengths:\n1. **Theoretical Contribution**: The paper provides a theoretical foundation for monotonic policy improvement in offline RL, which is a valuable addition to the literature. The authors derive important results that extend existing policy improvement theorems to the offline setting.\n2. **Empirical Results**: The experiments conducted on the D4RL benchmark demonstrate that BPPO outperforms several state-of-the-art offline RL algorithms in various environments, particularly in more challenging tasks like Adroit and Antmaze.\n3. **Clarity and Presentation**: The paper is generally well-written, and the presentation of the proposed algorithm is clear. The authors have made efforts to address reviewer concerns in their rebuttal, which indicates a willingness to improve the work.\n\n#### Weaknesses:\n1. **Soundness of Theoretical Claims**: Several reviewers raised concerns about the soundness of the theoretical claims, particularly regarding the assumptions made (e.g., Assumption 1) and the implications of replacing the advantage estimates. The authors have attempted to address these concerns by modifying Assumption 1, but the underlying issues regarding the approximation of advantage estimates remain a point of contention.\n2. **Novelty and Overlap with Prior Work**: Reviewers noted that the theoretical results presented in the paper overlap with existing works, such as Generalized Proximal Policy Optimization (GePPO). While the authors argue that their approach is distinct, the perceived lack of novelty could undermine the paper's impact.\n3. **Experimental Rigor**: Although the authors have supplemented their experiments in response to reviewer feedback, there are still concerns about the comprehensiveness of the experimental analysis. For instance, the lack of results in sparse reward settings and comparisons with additional state-of-the-art methods (e.g., EDAC, LAPO, RORL) could weaken the empirical contributions of the paper.\n\n#### Reviewer Consensus:\nThe reviewers are divided in their opinions. While some see merit in the theoretical insights and empirical results, others express skepticism regarding the soundness of the method and its novelty. The rebuttal from the authors has addressed some concerns but has not fully alleviated doubts about the theoretical and empirical contributions.\n\n### Conclusion\nGiven the significant theoretical contributions and the promising empirical results, the paper has potential value for the community. However, the concerns regarding the soundness of the theoretical framework and the novelty of the contributions are substantial. The authors have made efforts to address these issues, but the lingering doubts from multiple reviewers suggest that the paper may not yet meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Identifying Phase Transition Thresholds of Permuted Linear Regression via Message Passing\" presents an interesting exploration of phase transitions in permuted linear regression, utilizing a message-passing framework. However, the reviews indicate significant concerns regarding clarity, novelty, and the overall rigor of the work.\n\n### Detailed Reasoning:\n\n1. **Clarity and Writing Quality**: \n   - Multiple reviewers highlighted that the paper suffers from poor writing quality, making it difficult to follow. Reviewer 1 specifically noted that the notation-heavy nature of the paper requires familiarity with prior works, which is a significant barrier for readers. This lack of clarity is a critical issue for a top-tier conference, where the expectation is that papers should be accessible to a broad audience.\n   - Reviewer 5 pointed out that the mathematical writing is suboptimal, with numerous inaccuracies and confusing definitions. This suggests that the authors have not adequately communicated their findings, which is essential for publication.\n\n2. **Novelty and Contribution**:\n   - While the paper claims to identify precise phase transition thresholds, several reviewers (notably Reviewer 2 and Reviewer 5) argue that the contributions are not sufficiently novel. They point out that the results for the oracle case seem to be corollaries of established theorems in prior works, and the non-oracle case relies on approximations that may not be rigorously justified.\n   - The claim of introducing a \"framework\" is criticized for being vague, and the proposed partial permutation estimator lacks sufficient justification for its significance compared to existing methods.\n\n3. **Technical Rigor**:\n   - Reviewers raised concerns about the assumptions made in the paper, particularly regarding the practicality of the oracle case and the justification of certain assumptions (e.g., Assumption 1). The lack of rigorous support for key claims undermines the paper's credibility.\n   - The empirical results presented in the paper are also questioned, with concerns about the alignment of theoretical predictions with empirical observations. This gap between theory and practice is a significant issue that needs to be addressed for the paper to be considered robust.\n\n4. **Reproducibility**:\n   - The absence of code or supplementary materials for reproducing the results is a notable weakness. Reproducibility is a critical aspect of research, especially in computational fields, and the lack of such resources diminishes the paper's impact.\n\n5. **Overall Impression**:\n   - The combination of poor clarity, questionable novelty, and insufficient technical rigor leads to a consensus among reviewers that the paper is not ready for publication in its current form. While the topic is of interest, the execution does not meet the standards expected at a top-tier conference.\n\n### Conclusion:\nGiven the significant concerns raised by multiple reviewers regarding clarity, novelty, technical rigor, and reproducibility, it is clear that the paper requires substantial revisions before it can be considered for publication. The authors should focus on improving the writing quality, providing clearer justifications for their assumptions and contributions, and ensuring that their results are reproducible.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Online black-box adaptation to label-shift in the presence of conditional-shift\" addresses a relevant and timely problem in machine learning: the adaptation of predictive models to distribution shifts that occur in real-world applications. The authors build on previous work by Wu et al. (2021) and propose heuristics to improve performance in scenarios where both label shift and conditional shift are present. \n\n### Strengths:\n1. **Relevance of the Problem**: The issue of distribution shifts is critical in machine learning, especially for models deployed in dynamic environments. The paper's focus on adapting to these shifts is well-motivated and addresses a significant gap in the literature.\n2. **Empirical Investigation**: The authors conduct empirical studies on various datasets, which is essential for validating their proposed methods. They provide a clear presentation of their results, making it possible for others to reproduce their findings.\n3. **Novelty**: The exploration of label shift adaptation in regression settings and the introduction of heuristics for non-invertible confusion matrices are noteworthy contributions.\n\n### Weaknesses:\n1. **Lack of Theoretical Justification**: The proposed heuristics lack a solid theoretical foundation. While the authors acknowledge this, the absence of theoretical support makes it difficult to assess the robustness and generalizability of their methods.\n2. **Limited Empirical Scope**: The empirical results, while suggestive, do not provide strong evidence of significant improvements over existing methods. The reviewers noted that in many cases, the proposed methods did not outperform the baseline methods, which raises questions about their practical utility.\n3. **Clarity and Completeness**: Several reviewers pointed out that the paper is not self-contained and lacks clarity in key areas, such as the definitions of terms and the explanations of methods. This lack of clarity could hinder understanding and reproducibility.\n4. **Dependence on Additional Validation Sets**: The reliance on out-of-distribution (OOD) validation sets is a potential limitation, as obtaining such sets may not always be feasible in practice. The authors need to address the implications of this requirement more thoroughly.\n\n### Reviewer Concerns:\n- Reviewers consistently highlighted the need for a more systematic approach to the types of distribution shifts considered. The framing of the problem as \"label shift in the presence of conditional shift\" may mislead readers into thinking that the conditional shift is merely an extension of label shift, rather than a more complex issue.\n- The empirical studies were criticized for not including a broader range of baselines and for not providing sufficient context for the results obtained. This limits the ability to draw generalizable conclusions from the findings.\n- The authors' responses to reviewer comments indicate an awareness of these issues, but they do not fully resolve the concerns raised, particularly regarding the theoretical underpinnings of their heuristics.\n\n### Conclusion:\nWhile the paper presents a novel approach to a relevant problem and includes empirical studies, the lack of theoretical justification, limited empirical scope, and clarity issues significantly undermine its contribution. The reviewers' feedback suggests that the paper is not yet ready for publication in a top-tier conference, as it requires substantial revisions to address the identified weaknesses.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" presents a new dataset aimed at improving precipitation nowcasting by incorporating 3D radar echo observations from multiple weather radars across various climate zones in Russia. The authors claim that this dataset will serve as a benchmark for evaluating precipitation nowcasting models and can be utilized in other machine learning tasks.\n\n#### Strengths:\n1. **Dataset Contribution**: The dataset is a significant contribution to the field of weather forecasting, as it includes multiple dimensions of data (e.g., reflectivity at various altitudes) that are not commonly found in existing datasets. This could potentially lead to improved models for precipitation nowcasting.\n2. **Comprehensive Evaluation**: The authors have evaluated several baseline models, including traditional methods and a recent transformer-based model (Earthformer), which adds value to the paper by demonstrating the dataset's applicability.\n3. **Public Availability**: The dataset and code are publicly available, which is a positive aspect for reproducibility and further research.\n\n#### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the methodological contributions are marginally significant. The paper primarily focuses on the dataset rather than introducing novel methodologies or advanced modeling techniques.\n2. **Evaluation Metrics**: The use of MSE as the sole evaluation metric for model performance has been criticized. While the authors mention that they also provide F1 scores, the reviewers felt that a more comprehensive set of metrics would be necessary to fully assess model performance in the context of precipitation nowcasting.\n3. **Lack of Extensive Experiments**: Reviewers noted that the evaluation of baseline models could be expanded, particularly with more complex models and additional ablation studies. The authors' response indicates that they plan to include more results in the camera-ready version, but this raises concerns about the completeness of the current submission.\n4. **Comparative Analysis**: There is a lack of comparative analysis with other datasets, which could help to highlight the unique advantages of the RuDar dataset.\n\n#### Reviewer Feedback:\nThe feedback from the reviewers is mixed but leans towards skepticism regarding the paper's suitability for a top-tier conference. While the dataset is valuable, the lack of methodological novelty and the limited scope of experiments diminish its impact. Reviewers suggested that the paper might be more appropriate for a journal rather than a high-impact conference.\n\n#### Author Responses:\nThe authors have acknowledged the reviewers' comments and indicated plans to enhance the paper with additional results and clarifications. However, the responses do not fully address the concerns regarding the novelty and depth of the methodological contributions.\n\n### Conclusion\nGiven the strengths of the dataset and its potential impact on the field, it is clear that the paper has merit. However, the concerns regarding the limited novelty, insufficient evaluation of advanced models, and the overall depth of the research suggest that it does not meet the high standards typically expected for acceptance at a top-tier conference. The paper would benefit from further development and a more comprehensive exploration of its implications.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" addresses an important issue in the context of Machine Learning as a Service (MLaaS) — the privacy of data exchanged between clients and servers. The authors propose a method that involves splitting a neural network into two parts, with the client processing raw data through the first part and sending encoded representations to the server for inference. \n\n#### Strengths:\n1. **Relevance of Topic**: The topic of privacy in MLaaS is highly relevant and timely, given the increasing concerns about data privacy in cloud computing.\n2. **Clarity**: The paper is generally well-written and easy to follow, which is a positive aspect for readers.\n3. **Experimental Results**: The paper presents experimental results that suggest the proposed method maintains accuracy while attempting to enhance privacy.\n\n#### Weaknesses:\n1. **Incremental Contribution**: Multiple reviewers noted that the idea of splitting a neural network is not novel and has been explored in prior works, particularly in the context of split learning. The paper does not sufficiently differentiate its contributions from existing literature.\n2. **Technical Flaws**: Reviewer 2 pointed out significant flaws in the proposed method, including issues with the threat model and the assumption that the server would implement the center loss honestly. This raises serious concerns about the effectiveness of the proposed approach in real-world scenarios.\n3. **Limited Experimental Evaluation**: The experimental analysis is criticized for being simplistic, using only two datasets and lacking comparisons with a broader range of baselines. This limits the robustness of the findings and their applicability to more complex scenarios.\n4. **Ambiguities and Errors**: Several reviewers highlighted ambiguities in the paper, such as unclear definitions and grammatical errors, which detract from the overall quality and clarity of the work.\n5. **Lack of Security Model**: Reviewer 3 noted that the paper does not define a security model, which is crucial for understanding the privacy guarantees of the proposed method.\n\n#### Novelty and Impact:\nThe novelty of the work is considered marginal by multiple reviewers, with suggestions that the contributions do not meet the standards expected at a top-tier conference like ICLR. The empirical results, while indicating some effectiveness, do not convincingly demonstrate a significant advancement over existing methods.\n\n### Conclusion\nGiven the significant concerns raised by the reviewers regarding the novelty, technical flaws, limited experimental evaluation, and clarity of the paper, it does not meet the high standards required for acceptance at a top-tier conference. The paper appears to be an incremental contribution with serious issues that undermine its validity and impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" presents a novel approach to neural-symbolic AI by introducing DeepSeaProbLog, which extends existing neural probabilistic logic programming frameworks to handle both discrete and continuous random variables. The authors claim that their work addresses significant limitations in current systems, particularly in terms of flexibility and expressiveness in modeling real-world problems.\n\n#### Strengths:\n1. **Novelty and Contribution**: The introduction of DeepSeaProbLog is a notable advancement in the field of neural-symbolic AI. The authors provide a theoretical foundation linking the semantics of their language to weighted model integration (WMI), which is a significant contribution. They also claim to be the first to propose a practical implementation of differentiable WMI, which could have broader implications for the field.\n\n2. **Implementation and Experiments**: The authors have implemented DeepSeaProbLog and conducted experiments demonstrating its capabilities in various tasks, including a neural-symbolic object detection task and a variational auto-encoder. The empirical results suggest that DeepSeaProbLog can outperform traditional neural networks and other neural-symbolic baselines in specific scenarios.\n\n3. **Clarity and Structure**: The paper is generally well-written, and the authors have made efforts to clarify their contributions in response to reviewer comments. They have also committed to releasing their implementation, which enhances reproducibility.\n\n#### Weaknesses:\n1. **Incremental Nature**: Several reviewers expressed concerns that the contributions of DeepSeaProbLog may be incremental rather than groundbreaking. While the authors argue against this characterization, the perceived similarity to DeepProbLog and the lack of extensive empirical evaluation across diverse benchmarks weaken the case for significant novelty.\n\n2. **Empirical Evaluation**: The empirical evaluation is limited, with some reviewers noting that the experiments primarily focus on toy applications. While the authors have provided some justification for their choices, a broader range of applications and benchmarks would strengthen the paper's claims about the effectiveness of DeepSeaProbLog.\n\n3. **Technical Clarity**: Some reviewers pointed out that certain technical aspects, such as the handling of discrete random variables and the semantics of the language, were not sufficiently detailed in the main text. This lack of clarity could hinder understanding and reproducibility.\n\n4. **Performance-Efficiency Tradeoff**: There is a notable lack of discussion regarding the performance-efficiency tradeoff of DeepSeaProbLog compared to existing methods. Reviewers highlighted the need for a more thorough analysis of how the proposed method performs relative to traditional approaches, particularly in terms of computational efficiency.\n\n5. **Response to Reviewers**: While the authors provided a comprehensive response to reviewer comments, some concerns regarding clarity and the incremental nature of the work remain unaddressed. The authors' insistence on the novelty of their approach may not fully convince all reviewers.\n\n### Conclusion\nOverall, while the paper presents a promising advancement in the field of neural-symbolic AI and introduces a novel framework, the concerns regarding the incremental nature of the contributions, limited empirical evaluation, and clarity of technical details weigh heavily against acceptance. The paper would benefit from further empirical validation and a more robust discussion of its contributions relative to existing work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" presents a bi-objective optimization model aimed at enhancing the generalization of reinforcement learning agents through the maintenance of a diverse population of agents with varying playing styles. The authors claim that their approach leads to improved performance in complex games, specifically Pong and Justice Online.\n\n### Strengths:\n1. **Challenging Problem**: The paper addresses a significant challenge in reinforcement learning, particularly in the context of self-play and agent diversity, which is a relevant and timely topic in the field.\n2. **Empirical Results**: The experiments conducted in two different gaming environments provide some evidence that the proposed method can yield competitive results compared to existing approaches.\n3. **Clarity**: The paper is generally well-written and easy to follow, which is important for dissemination in the academic community.\n\n### Weaknesses:\n1. **Literature Gaps**: Several reviewers pointed out the omission of key literature, particularly the work by Lanctot et al. (2017) on multi-agent reinforcement learning and self-play algorithms. This oversight raises concerns about the completeness of the literature review and the robustness of the proposed method against established baselines.\n2. **Experimental Design Flaws**: The experimental setup has been criticized for potentially favoring certain algorithms due to the imbalanced representation of agents in the opponent pool. This could skew the results and undermine the validity of the claims made by the authors.\n3. **Lack of Baselines**: The absence of comparisons to other relevant algorithms, particularly from the quality-diversity literature, limits the ability to assess the true novelty and effectiveness of the proposed approach. Reviewers suggested that including such comparisons would strengthen the paper significantly.\n4. **Simplification of Playing Style**: The reduction of playing style to a single scalar is seen as a significant limitation. This simplification may overlook the complexity of agent behaviors and could be a point of contention in the broader context of multi-agent systems.\n5. **Insufficient Detail in Experiments**: Reviewers noted that the paper lacks detailed information about the experimental setup, such as the number of runs and confidence intervals for the reported results. This lack of rigor in the empirical evaluation diminishes the reliability of the findings.\n\n### Overall Assessment:\nWhile the paper tackles an important problem and presents some promising results, the identified weaknesses significantly undermine its contributions. The lack of engagement with relevant literature, potential biases in the experimental design, and insufficient empirical rigor suggest that the paper does not meet the standards expected for publication at a top-tier conference. The reviewers' concerns about the novelty and significance of the contributions further reinforce this conclusion.\n\nGiven these considerations, the paper should be rejected for publication. The authors may benefit from addressing the highlighted weaknesses and conducting more comprehensive experiments before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"New Bounds For Distributed Mean Estimation and Variance Reduction\" addresses a significant problem in distributed machine learning, specifically focusing on distributed mean estimation (DME) and variance reduction. The authors propose a novel quantization method leveraging lattice theory, which allows for improved communication efficiency while estimating the mean of distributed vectors. The paper presents theoretical contributions, including new bounds that do not depend on input norms, and provides experimental results demonstrating practical improvements over existing methods.\n\n### Detailed Reasoning\n\n1. **Significance of the Problem**: The problem of distributed mean estimation is crucial in various applications, especially in distributed machine learning. The authors correctly identify the limitations of previous approaches that rely on input norms, which can lead to poor performance in practical scenarios where inputs are concentrated around the mean.\n\n2. **Novelty of Contributions**: The use of lattice-based quantization is an interesting and innovative approach. The authors provide a new connection between DME and lattice theory, which is a valuable contribution to the field. The paper also includes lower bounds that demonstrate the asymptotic optimality of their communication to error trade-off, which adds depth to their theoretical contributions.\n\n3. **Experimental Validation**: The authors present experimental results that support their theoretical claims, showing practical improvements in communication efficiency. However, the experiments are somewhat limited, primarily focusing on two machines. While the authors mention additional experiments in the appendix, expanding the experimental setup to include more machines in the main body would strengthen the paper.\n\n4. **Reviewer Feedback**: The reviewers generally provide positive feedback, highlighting the clarity of the problem definition, the natural motivation, and the thoroughness of the work. However, they also raise valid concerns regarding the practical implementation of the algorithms, the need for additional discussion on certain aspects, and the clarity of some definitions. The authors have acknowledged these points and committed to addressing them in the revision.\n\n5. **Addressing Concerns**: The authors' responses to the reviewers' comments are thoughtful and indicate a willingness to improve the paper. They clarify the differences between theoretical and practical algorithms, acknowledge the need for clearer definitions, and express intent to enhance the experimental section. This responsiveness is a positive indicator of the authors' commitment to quality.\n\n6. **Minor Issues**: While there are several minor issues and suggestions for improvement, they do not fundamentally undermine the paper's contributions. The authors have indicated they will address these in the revision.\n\n### Conclusion\nOverall, the paper presents a significant advancement in the field of distributed mean estimation, with a novel approach that has both theoretical and practical implications. The authors have shown a willingness to improve the paper based on reviewer feedback, and the contributions are substantial enough to warrant acceptance, provided that the suggested revisions are made.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics\" addresses a significant and relevant problem in the field of game theory and learning dynamics. The authors aim to quantify the average-case performance of q-replicator dynamics (QRD) in games, which is a valuable contribution given the existing focus on convergence without sufficient attention to the quality of equilibria reached.\n\n#### Strengths:\n1. **Relevance of the Problem**: The study of average-case performance in no-regret learning dynamics is timely and important, especially as the community seeks to understand not just convergence but also the quality of equilibria.\n2. **Clarity and Structure**: The paper is generally well-written, with a clear motivation and logical flow. The authors have made efforts to clarify their contributions in response to reviewer comments.\n3. **Empirical Evidence**: The authors provide empirical results that support their theoretical claims, which is a strong point in favor of the paper.\n\n#### Weaknesses:\n1. **Incremental Contribution**: Several reviewers noted that the contributions of the paper appear to be incremental, particularly in relation to prior work (PP16). While the authors argue that their work expands upon PP16, the reviewers felt that the novelty was not sufficiently highlighted or discussed.\n2. **Limited Scope**: The focus on symmetric 2x2 coordination games is seen as a limitation. While the authors acknowledge the challenge of extending their results to more general games, the lack of formal results beyond this restricted case raises concerns about the paper's overall impact.\n3. **Technical Clarity**: Some reviewers pointed out issues with the clarity of definitions and the rigor of certain statements, which could hinder reproducibility and understanding. The authors have made efforts to address these concerns, but the initial lack of clarity may have affected the reviewers' perceptions of the paper's quality.\n\n#### Reviewer Feedback:\n- Reviewer 1 and Reviewer 2 expressed concerns about the novelty and significance of the results, suggesting that the paper does not sufficiently differentiate itself from prior work.\n- Reviewer 3 acknowledged the interesting questions posed but felt that the paper made limited progress in answering them, particularly in terms of generalizing results beyond the 2x2 case.\n- Reviewer 4 recognized the importance of the problem but also noted the restricted case study and the need for clearer expression of contributions.\n\nThe authors' responses to reviewer comments indicate a willingness to improve the paper and clarify their contributions. They have made revisions to the abstract and addressed specific technical concerns raised by the reviewers. However, the fundamental issues regarding the perceived incremental nature of the contributions and the limited scope of the results remain.\n\n### Conclusion\nWhile the paper addresses an important topic and has made some contributions, the concerns regarding its novelty, the limited scope of results, and the clarity of presentation suggest that it may not meet the high standards expected for acceptance at a top-tier conference. The reviewers' feedback indicates that while the paper is a step in the right direction, it may require further development and a more substantial contribution to the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Regression with Label Differential Privacy\" presents a novel approach to ensuring label differential privacy in regression tasks. The authors propose a mechanism based on a \"randomized response on bins\" and provide theoretical proofs of its optimality under specific regression loss functions. The empirical evaluation demonstrates that their method outperforms existing differential privacy methods, such as Laplace and Exponential mechanisms, across various datasets.\n\n#### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new mechanism for label differential privacy tailored for regression tasks, which is a significant contribution to the field. The theoretical results regarding the optimality of the proposed mechanism are well-articulated and provide a solid foundation for the claims made.\n   \n2. **Clarity and Presentation**: The paper is well-written and easy to follow, which is crucial for a top-tier conference. The authors have addressed reviewer comments effectively, enhancing the clarity of their arguments and results.\n\n3. **Empirical Validation**: The experimental results are convincing, showing that the proposed method performs better than existing methods. The authors have also responded positively to suggestions for additional experiments, indicating a willingness to improve their work based on feedback.\n\n#### Weaknesses:\n1. **Comparison with Prior Work**: Several reviewers noted that while the contributions are significant, aspects of the work overlap with prior research, particularly the work by Ghazi et al. (2021). Reviewer 5 specifically pointed out that the proposed method is not technically novel compared to this prior work. The authors have attempted to clarify the differences, but the perceived novelty remains a concern.\n\n2. **Minor Issues in Claims**: Some reviewers pointed out minor issues in the claims made in the paper, suggesting that certain statements require additional support or clarification. While these do not undermine the overall contribution, they indicate that the paper could benefit from further refinement.\n\n3. **Generalization Error Considerations**: There were concerns raised about the generalization error and how it relates to the proposed mechanism. While the authors acknowledged this and suggested it as a future research direction, it indicates that the current work may not fully address all practical implications of their approach.\n\n4. **Optimal Bins Discussion**: The question regarding the number of optimal bins being smaller than the number of different labels was raised, and while the authors provided an empirical observation, a formal proof is still lacking. This could be a limitation in understanding the full implications of their method.\n\n### Conclusion\nOverall, the paper presents a significant advancement in the field of label differential privacy for regression tasks. Despite some concerns regarding novelty and minor issues in claims, the strengths of the paper, including its clear presentation, theoretical contributions, and empirical validation, outweigh the weaknesses. The authors have shown responsiveness to reviewer feedback and have made efforts to clarify and enhance their work.\n\nGiven the potential impact of this research on the differential privacy community and the rigorous approach taken by the authors, I recommend acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Data Leakage in Tabular Federated Learning\" addresses a significant and timely issue in the realm of federated learning (FL), particularly focusing on the vulnerabilities associated with tabular data. The authors propose a novel attack method, TabLeak, which aims to reconstruct private data from gradients in a federated learning setting. The paper presents a comprehensive experimental evaluation demonstrating the effectiveness of the proposed method.\n\n#### Strengths:\n1. **Relevance and Importance**: The topic of data leakage in federated learning is highly relevant, especially given the increasing use of FL in sensitive domains like finance and healthcare. The focus on tabular data is particularly noteworthy, as most existing literature has concentrated on image and text data.\n   \n2. **Novelty**: The paper introduces a new method (TabLeak) that combines softmax structural priors, pooled ensembling, and an entropy measure to tackle the unique challenges posed by tabular data. This approach is a step forward in understanding and mitigating privacy risks in FL.\n\n3. **Empirical Evaluation**: The authors provide a thorough empirical evaluation across multiple datasets, demonstrating significant improvements over baseline methods. The results are compelling and suggest that the proposed method is effective in reconstructing private data.\n\n4. **Clarity and Reproducibility**: The paper is generally well-written, and the authors have made efforts to ensure reproducibility by providing source code and detailed experimental setups.\n\n#### Weaknesses:\n1. **Standard Techniques**: Several reviewers pointed out that some of the techniques employed (e.g., softmax relaxation and ensemble methods) are not particularly novel and have been used in other contexts. This raises questions about the overall novelty of the contributions.\n\n2. **Presentation Issues**: Reviewer 1 highlighted issues with the clarity of writing, including grammatical errors and obscure sentences. While Reviewer 3 found the paper easy to follow, the mixed feedback on clarity suggests that the presentation could be improved.\n\n3. **Claims and Justifications**: Some claims made in the paper regarding the uniqueness of challenges posed by tabular data were questioned. For instance, the assertion that the mix of categorical and continuous features leads to higher variance lacked sufficient empirical support initially, although the authors addressed this in their response.\n\n4. **Baseline Comparisons**: The choice of baselines was critiqued, with suggestions that stronger or more relevant baselines could have been used for a fairer evaluation of TabLeak's performance.\n\n5. **Minor Issues**: Several minor issues were raised regarding notation consistency and the need for clearer explanations of certain methodologies.\n\n#### Reviewer Feedback Summary:\n- Reviewer 1 expressed concerns about the lack of novelty and poor presentation.\n- Reviewer 2 emphasized that the contributions are not significant or novel, despite acknowledging the identification of unique aspects of tabular data.\n- Reviewer 3 provided a more balanced view, recognizing the empirical novelty and effectiveness of the proposed method while suggesting improvements in justifying claims and addressing minor issues.\n\n#### Author Responses:\nThe authors provided thoughtful responses to the reviewers' comments, addressing concerns about novelty, the justification of their methods, and the empirical evidence supporting their claims. They also made revisions to the paper to clarify certain points and improve the overall presentation.\n\n### Conclusion\nWhile the paper has notable strengths, particularly in its relevance and empirical contributions, it also faces significant criticisms regarding the novelty of its methods and clarity of presentation. The authors have made efforts to address these concerns, but the mixed feedback from reviewers suggests that the paper may not meet the high standards expected at a top-tier conference.\n\nGiven the importance of clarity and novelty in academic publishing, and considering the critiques regarding the standard nature of some techniques used, I recommend rejecting the paper. The authors may benefit from further refining their contributions and addressing the clarity issues before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Batch Normalization and Bounded Activation Functions\" presents an interesting investigation into the interaction between batch normalization (BN) and bounded activation functions, particularly focusing on the order of these components in neural network architectures. The authors claim that placing BN after bounded activation functions (the \"Swap model\") leads to improved performance compared to the conventional order. They attribute this improvement to the phenomenon of asymmetric saturation and increased sparsity.\n\n#### Strengths:\n1. **Novel Observation**: The paper introduces a novel perspective on the order of BN and activation functions, particularly for bounded activations like Tanh. The finding that the Swap model can outperform the conventional model under certain conditions is noteworthy.\n2. **Empirical Analysis**: The authors provide extensive empirical evidence supporting their claims, including various experiments across different architectures and activation functions.\n3. **Clear Logic Chain**: The paper follows a logical progression from observation to analysis, making it relatively easy to follow the authors' reasoning.\n\n#### Weaknesses:\n1. **Lack of Rigorous Support**: Several claims made in the paper are not sufficiently supported by rigorous analysis or empirical evidence. For instance, the relationship between asymmetric saturation and generalization performance is not convincingly established. Reviewers have pointed out that the authors need to clarify how they exclude other factors that could contribute to performance improvements.\n2. **Limited Generalizability**: The analysis is primarily focused on bounded activation functions and does not adequately address how these findings might apply to more commonly used activation functions like ReLU. The paper does not convincingly demonstrate that the Swap model is beneficial across a broader range of architectures.\n3. **Confusion in Metrics**: Reviewers expressed concerns about the saturation and sparsity metrics used in the analysis. The authors need to clarify how these metrics are defined and their implications for performance.\n4. **Inconsistencies in Results**: There are inconsistencies in the results presented, particularly regarding the performance of the Swap model compared to the conventional model with ReLU. The authors need to address these discrepancies more thoroughly.\n5. **Clarity and Presentation**: While the paper is generally well-written, there are areas where clarity could be improved. Some terms and concepts are not defined clearly, leading to confusion among reviewers.\n\n#### Reviewer Feedback:\nThe feedback from multiple reviewers indicates a consensus that while the paper presents an interesting observation, the arguments connecting the observations to the conclusions are weak. Reviewers have highlighted the need for more rigorous analysis and clearer presentation of results. The authors' responses to reviewer comments show an effort to clarify some points, but many concerns remain unaddressed.\n\n### Conclusion\nGiven the interesting nature of the findings, the paper has potential. However, the weaknesses in the rigor of the analysis, the clarity of the arguments, and the generalizability of the results are significant enough to warrant rejection at this stage. The authors should consider addressing these issues and resubmitting to a future conference once they have strengthened their claims and provided more comprehensive evidence.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Meta-Referential Games to Learn Compositional Learning Behaviours\" presents a novel benchmark for investigating compositional learning behaviors in artificial agents through the lens of meta-reinforcement learning. While the topic is indeed relevant and interesting, the reviews indicate several significant shortcomings that hinder the paper's acceptance at a top-tier conference.\n\n### Detailed Reasoning:\n\n1. **Clarity and Communication**: Multiple reviewers noted that the paper is difficult to follow. The introduction lacks a clear presentation of the problem and its significance, and the details surrounding the proposed methods and experiments are insufficiently explained. This lack of clarity is a critical issue, as it prevents readers from fully grasping the contributions and implications of the work.\n\n2. **Novelty and Significance**: The reviewers expressed concerns regarding the novelty and significance of the contributions. While the idea of using a symbolic continuous stimulus (SCS) representation is interesting, its applicability beyond the specific experiments conducted in the paper is questioned. The reviewers suggest that the contributions are marginally significant or novel, which is not sufficient for a top-tier conference.\n\n3. **Experimental Rigor**: The experimental results presented in the paper are deemed insufficient. Reviewers pointed out the lack of comprehensive comparisons with existing methods, such as one-hot encodings, and the absence of ablation studies to understand the impact of different components of the proposed method. The performance metrics reported are also not compelling, with results being only slightly above chance levels. This raises concerns about the robustness of the findings.\n\n4. **Engagement with Existing Literature**: The paper does not adequately situate itself within the broader literature on compositional learning and generalization. Reviewers highlighted the need for a more thorough discussion of how the proposed methods relate to existing work, which would help clarify the contributions and significance of the research.\n\n5. **Reviewer Feedback and Author Response**: The authors acknowledge the reviewers' comments and express a willingness to improve the paper. However, the extent of the revisions needed to address the fundamental issues raised is substantial. The reviewers' critiques suggest that the paper requires significant reworking to meet the standards expected at a top-tier conference.\n\n### Conclusion:\nGiven the multiple critical issues identified by the reviewers, including clarity, novelty, experimental rigor, and engagement with existing literature, the paper does not currently meet the standards for acceptance. While the topic is promising, the paper requires substantial revisions and improvements to be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Multi Task Learning of Different Class Label Representations for Stronger Models\" presents a novel approach to image classification by introducing a new label representation called Binary Labels and employing a multi-task learning paradigm. The authors claim that this method enhances model performance, particularly in scenarios with limited training data.\n\n### Strengths:\n1. **Novelty**: The concept of using Binary Labels as an auxiliary task in multi-task learning is an interesting and relatively unexplored area. The idea of varying label representations to improve model performance is a fresh perspective that could lead to further research.\n2. **Potential Benefits**: The authors report improvements in accuracy across various datasets and architectures, which suggests that their approach may have practical implications for real-world applications, especially in challenging classification tasks.\n\n### Weaknesses:\n1. **Experimental Rigor**: The experimental section is notably weak. Reviewers pointed out the lack of comprehensive comparisons against state-of-the-art methods and additional baselines. The results presented do not convincingly demonstrate the superiority of the proposed method, as improvements are often marginal.\n2. **Clarity and Reproducibility**: There are significant clarity issues throughout the paper. Key concepts, such as the generation of Binary Labels and the use of Metabalance, are not adequately explained. This lack of clarity raises concerns about reproducibility, as reviewers expressed uncertainty about how to replicate the results based on the provided information.\n3. **Theoretical Justification**: The paper lacks a strong theoretical foundation or intuition behind why Binary Labels improve performance. Reviewers noted that the authors did not sufficiently discuss the properties of Binary Labels that differentiate them from traditional one-hot encodings.\n4. **Related Work**: The authors did not adequately address existing literature on label representation and multi-task learning. This oversight diminishes the perceived novelty and significance of their contributions.\n\n### Reviewer Consensus:\nThe reviewers generally agree that while the idea is interesting, the execution falls short of the standards expected for a top-tier conference. The empirical results are not robust enough, and the clarity issues hinder the understanding of the proposed method. The lack of an author response further complicates the situation, as it leaves the reviewers' concerns unaddressed.\n\n### Conclusion:\nGiven the paper's interesting premise but significant shortcomings in experimental validation, clarity, and theoretical justification, it does not meet the high standards required for acceptance at a top-tier conference. The authors need to conduct more thorough experiments, provide clearer explanations, and engage more deeply with related work to strengthen their submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning\" presents a novel approach to address the challenges of client drift and period drift in federated learning (FL) through a method called FedPA. The reviewers provided a range of comments that highlight both strengths and weaknesses of the paper.\n\n#### Strengths:\n1. **Identification of Period Drift**: The paper introduces the concept of period drift, which is a valuable addition to the understanding of challenges in federated learning. This concept could pave the way for further research in the area.\n2. **Novel Aggregation Strategy**: The proposed FedPA method, which utilizes a parameterized aggregator within a meta-learning framework, is an interesting approach that attempts to improve model aggregation in FL.\n3. **Experimental Results**: Some reviewers noted that the experimental results show competitive performance compared to existing methods, which is a positive aspect of the paper.\n\n#### Weaknesses:\n1. **Lack of Theoretical and Experimental Justification**: Multiple reviewers pointed out that the paper does not adequately differentiate between client drift and period drift, nor does it provide sufficient theoretical backing for the proposed method. The lack of clear definitions and distinctions weakens the overall contribution.\n2. **Use of Proxy Dataset**: The reliance on a proxy dataset for training the aggregator is a significant limitation. Reviewers expressed concerns about the generalizability of the method and suggested that comparisons with similar methods (e.g., FedET and DS-FL) are necessary to validate its effectiveness.\n3. **Insufficient Experimental Settings**: The experimental settings were criticized for being too simplistic, with suggestions to include more complex datasets (e.g., CIFAR-100) to better demonstrate the method's capabilities.\n4. **Writing Quality**: The paper suffers from grammatical errors and clarity issues, which detract from the overall quality and readability. Reviewers noted that the writing needs significant improvement.\n5. **Reproducibility Concerns**: The absence of code and detailed hyperparameter tuning information raises concerns about the reproducibility of the results.\n\n#### Reviewer Consensus:\nThe reviewers generally leaned towards rejection, citing insufficient novelty, lack of rigorous experimental validation, and clarity issues. While some reviewers acknowledged the potential of the proposed method, they emphasized that the paper does not sufficiently demonstrate its significance or superiority over existing approaches.\n\n### Conclusion\nGiven the identified weaknesses, particularly the lack of theoretical grounding, insufficient experimental validation, and clarity issues, the paper does not meet the standards expected for publication at a top-tier conference. The concerns raised by the reviewers indicate that substantial revisions are necessary before the work can be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Learning DNA folding patterns with Recurrent Neural Networks\" presents a study on predicting DNA folding patterns using various machine learning methods, particularly focusing on a bidirectional LSTM model. The authors claim that their model outperforms traditional methods in predicting Topologically Associating Domains (TADs) from epigenetic data.\n\n#### Strengths:\n1. **Relevance and Novelty**: The application of machine learning to molecular biology, particularly in understanding DNA folding patterns, is a relevant and timely topic. The use of a bidirectional LSTM model is an interesting approach that leverages the sequential nature of DNA data.\n2. **Performance Claims**: The authors report that their model outperforms other traditional machine learning methods, which is a significant contribution to the field.\n3. **Biological Significance**: The identification of informative epigenetic features and their biological implications adds value to the research.\n\n#### Weaknesses:\n1. **Methodological Concerns**: Multiple reviewers raised concerns about the lack of methodological novelty and clarity. Reviewer 3 specifically noted that the paper is similar to previously published works and lacks sufficient novelty. The authors did not adequately address how their approach differs from existing literature.\n2. **Evaluation Metrics**: The evaluation of the model's performance is questioned. Reviewers pointed out that the weighted mean squared error (wMSE) used as a loss function may not have been consistently applied across all models, making comparisons difficult. Additionally, the lack of a broader set of evaluation metrics (e.g., R², MAE) in the initial submission raised concerns about the robustness of the findings.\n3. **Generalizability**: There are concerns regarding the generalizability of the model. The authors did not test their model on other Hi-C datasets or different epigenetic features, which limits the applicability of their findings.\n4. **Clarity and Presentation**: Several reviewers noted issues with clarity in the presentation of the methods and results. The authors' responses indicate that they have made some revisions, but the initial lack of clarity may have hindered the understanding of their work.\n\n#### Author Responses:\nThe authors provided detailed responses to reviewer comments and made revisions to the manuscript. They expanded the literature review, clarified the methodology, and added additional evaluation metrics. However, the responses did not fully address the concerns about the novelty of the work and the generalizability of the findings.\n\n### Conclusion\nWhile the paper presents an interesting application of machine learning in computational biology, the concerns raised by the reviewers regarding methodological novelty, clarity, and evaluation metrics are significant. The authors' revisions and responses, while helpful, do not sufficiently mitigate these concerns. The lack of clear differentiation from existing works and the limited generalizability of the findings suggest that the paper may not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of how different activation functions influence the training dynamics of overparametrized neural networks. The authors focus on the minimum eigenvalue of a Gram matrix associated with the training process, providing insights into the convergence rates for various activation functions, both smooth and non-smooth.\n\n### Strengths:\n1. **Theoretical Contribution**: The paper addresses a significant gap in the literature by providing a theoretical framework for understanding the impact of activation functions on training dynamics. This is particularly relevant given the increasing use of various activation functions in practice.\n   \n2. **Comprehensive Analysis**: The authors cover a wide range of activation functions and provide a clear distinction between smooth and non-smooth functions. This classification is crucial for understanding the implications of their results.\n\n3. **Empirical Validation**: The inclusion of empirical experiments, particularly on synthetic data and CIFAR10, adds credibility to their theoretical claims. The authors acknowledge the gaps between theory and practice, which demonstrates a level of rigor and honesty in their findings.\n\n4. **Well-Structured Appendix**: Despite concerns about length, the appendix contains detailed proofs and extensions, which are essential for a thorough understanding of the results. The authors have made efforts to improve navigation through the appendix, which is a positive step.\n\n5. **Reviewer Support**: Both reviewers expressed a favorable view of the paper, highlighting its thoroughness and relevance. Reviewer 2 explicitly recommends acceptance, citing the paper's contributions to the understanding of neural network training.\n\n### Weaknesses:\n1. **Length and Complexity**: The paper is lengthy, and some reviewers noted that it may be challenging for a general audience to parse. While the authors have made efforts to improve organization, the complexity of the material may still pose a barrier to understanding.\n\n2. **Appendix Concerns**: While the appendix is comprehensive, it has been criticized for being too long and potentially overwhelming. The authors have attempted to address this by adding a table of contents and reorganizing sections, but further simplification could enhance readability.\n\n3. **Minor Errors**: There were some minor errors and unclear statements noted by the reviewers, which the authors have addressed in their revisions. However, the presence of these issues initially raises concerns about the paper's polish.\n\n4. **Empirical Gaps**: The gap between theoretical predictions and empirical results, particularly on CIFAR10, could be a point of concern. While the authors acknowledge this, further exploration of these discrepancies could strengthen the paper.\n\n### Conclusion:\nOverall, the paper makes a significant contribution to the understanding of activation functions in the context of overparametrized neural networks. The theoretical insights are valuable, and the empirical validation, despite some gaps, supports the findings. The authors have shown responsiveness to reviewer feedback and have made efforts to improve the clarity and organization of the paper.\n\nGiven the strengths of the theoretical contributions, the positive feedback from reviewers, and the authors' willingness to address concerns, I believe the paper meets the standards for acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training based on task-specific updates. The authors claim that their method, $\\alpha$-Variable Importance Learning (VIL), outperforms existing multitask learning approaches in various settings. However, the reviewers raised several significant concerns regarding the theoretical justification, empirical results, and overall novelty of the proposed method.\n\n### Detailed Reasoning:\n\n1. **Theoretical Justification**: \n   - Reviewer 1 and Reviewer 3 both pointed out the lack of theoretical grounding for the proposed method. While the authors acknowledge this shortcoming and express intent to improve it in a future version, the absence of a solid theoretical foundation is a critical issue for a top-tier conference. Theoretical justification is essential for understanding the underlying principles of the proposed method and its expected behavior.\n\n2. **Empirical Results**:\n   - The empirical results presented in the paper do not convincingly demonstrate the superiority of $\\alpha$VIL over the baselines. Reviewers noted that the improvements are often marginal and fall within the range of statistical insignificance. Reviewer 2 highlighted that the results on the MultiMNIST dataset, while showing some improvement, are not robust enough to justify the proposed method's claims. The mixed results on the NLU tasks further weaken the case for acceptance.\n\n3. **Novelty and Comparison with Existing Methods**:\n   - Reviewers expressed skepticism about the novelty of the proposed method, particularly in light of existing dynamic task weighting approaches like Discriminative Importance Weighting (DIW). The authors argue that their method is distinct because it employs direct meta-optimization for task weights, but this distinction is not sufficiently clear or compelling. The reviewers' concerns about the similarity to existing methods suggest that the contribution may not be significant enough for a top-tier conference.\n\n4. **Experimental Design**:\n   - The choice of datasets and experimental setups has been criticized. For instance, Reviewer 3 noted that the experimental setup for MultiMNIST only includes one auxiliary task, which does not fully explore the potential of multitask learning. Additionally, the reviewers requested more comprehensive experiments and ablation studies to validate the proposed method's effectiveness, which the authors have acknowledged but have not yet provided.\n\n5. **Response to Reviewer Comments**:\n   - The authors' responses indicate a willingness to improve the paper based on feedback, which is commendable. However, the issues raised are fundamental and would require substantial revisions and additional experiments to address adequately. The authors' acknowledgment of the need for more theoretical justification and empirical validation suggests that the current version of the paper is not ready for publication.\n\n### Conclusion:\nGiven the significant concerns regarding theoretical justification, empirical validation, and the overall novelty of the proposed method, the paper does not meet the standards expected for acceptance at a top-tier conference. The authors have identified areas for improvement, but these would require considerable effort and additional research to address.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Variational Neural Cellular Automata\" presents a novel approach to generative modeling by integrating Neural Cellular Automata (NCA) into a variational framework. The authors claim that their model, the Variational Neural Cellular Automata (VNCA), can generate diverse outputs while maintaining robustness against perturbations, inspired by biological processes of cellular growth and differentiation.\n\n#### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new generative model that leverages the principles of NCAs, which is a relatively unexplored area in generative modeling. The authors argue that their approach can model complex data distributions through morphogenesis, which is a significant contribution to the field.\n\n2. **Clarity and Presentation**: The paper is well-written and structured, making it easy to follow the proposed methodology and experimental setup. The authors have addressed reviewer comments and improved the paper significantly in the rebuttal phase, adding new experiments and clarifications.\n\n3. **Experimental Results**: While the results on MNIST are promising, the authors have also shown that the VNCA can generate images on more complex datasets like CelebA, albeit with some limitations. The addition of experiments demonstrating the model's robustness to perturbations is a valuable contribution.\n\n4. **Engagement with Reviewers**: The authors have actively engaged with the reviewers' comments, providing clarifications and additional experiments that enhance the paper's quality.\n\n#### Weaknesses:\n1. **Experimental Performance**: Several reviewers pointed out that the experimental results, particularly on CelebA, are underwhelming compared to state-of-the-art models. The likelihood scores are significantly lower than existing methods, which raises concerns about the practical applicability of the VNCA.\n\n2. **Technical Novelty**: Some reviewers expressed skepticism about the novelty of the approach, suggesting that the insights presented could be seen as extensions of existing work in deep learning and NCAs. The relationship between NCAs and deep convolutional networks has been previously established, which may undermine the perceived novelty of the VNCA.\n\n3. **Motivation and Clarity of Contribution**: There are concerns regarding the clarity of the problem the VNCA is addressing. Some reviewers felt that the motivation for the work was not sufficiently articulated, particularly in terms of what specific limitations of prior work the VNCA is overcoming.\n\n4. **Limited Scope of Experiments**: The experiments primarily focus on a few datasets, and while the authors have added new analyses, the overall scope remains limited. More diverse datasets and tasks could strengthen the claims about the model's capabilities.\n\n5. **Concerns about Claims**: Some claims, such as those regarding super-resolution and the model's robustness, were questioned by reviewers. The authors have since removed unsupported claims, but this raises concerns about the initial rigor of the claims made in the paper.\n\n### Conclusion\nWhile the paper presents an interesting approach and has made significant improvements in response to reviewer feedback, the concerns regarding experimental performance, clarity of contribution, and the novelty of the approach weigh heavily against acceptance. The paper does not sufficiently demonstrate that the VNCA outperforms or offers substantial advantages over existing methods, particularly in terms of generative modeling performance.\n\nGiven these considerations, I recommend rejecting the paper for publication at this time. The authors may benefit from further refining their experimental results and clarifying the unique contributions of their work before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Multi-scale Feature Learning Dynamics: Insights for Double Descent\" presents a theoretical investigation into the phenomenon of double descent in neural networks, particularly focusing on epoch-wise double descent. The authors utilize a linear teacher-student model to derive closed-form expressions for generalization error over training time, providing insights into the dynamics of feature learning at different scales.\n\n#### Strengths:\n1. **Novelty and Relevance**: The exploration of epoch-wise double descent is a timely and relevant topic in the field of deep learning. The authors contribute to the understanding of how different scales of feature learning affect generalization error, which is a significant aspect of neural network training dynamics.\n   \n2. **Analytical Framework**: The use of a linear teacher-student model allows for a clear analytical treatment of the problem, making complex dynamics more tractable. The closed-form expressions derived are valuable for theoretical insights.\n\n3. **Empirical Validation**: The authors validate their theoretical findings with numerical experiments, showing a qualitative match between their model and a ResNet18 trained on CIFAR10. This connection to real-world applications enhances the paper's impact.\n\n4. **Response to Reviewers**: The authors have made substantial revisions in response to reviewer comments, addressing concerns about clarity, connections to existing literature, and the implications of their findings. They have added discussions on implicit regularization, connections to prior work, and provided additional experiments.\n\n#### Weaknesses:\n1. **Technical Novelty**: While the contributions are significant, some reviewers noted that aspects of the work overlap with prior literature. The authors need to clearly delineate how their work advances the field beyond existing studies.\n\n2. **Clarity and Presentation**: Several reviewers pointed out issues with typos and clarity in the presentation of the results. Although the authors have made efforts to address these, the initial presentation detracted from the overall readability and understanding of the work.\n\n3. **Depth of Discussion**: Some reviewers expressed a desire for deeper discussions on certain aspects, such as the implications of their findings in more complex settings or the potential for triple descent. While the authors have added some discussions, further elaboration could strengthen the paper.\n\n4. **Connections to Real-World Scenarios**: Although the authors have made efforts to connect their findings to deep learning practices, the linear model's simplicity may limit its applicability to more complex neural networks. The authors should continue to explore these connections in future work.\n\n#### Conclusion:\nOverall, the paper presents a valuable contribution to the understanding of double descent in neural networks, with a solid theoretical foundation and empirical validation. The authors have shown responsiveness to reviewer feedback and have made significant improvements to the manuscript. While there are areas for further clarification and depth, the core contributions are strong enough to warrant acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" addresses a significant problem in the field of robot learning and manipulation: the extraction of reusable skills from human demonstrations using weak supervision. The authors propose a method inspired by multiple instance learning (MIL) to segment trajectories into skill primitives without requiring detailed annotations. \n\n#### Strengths:\n1. **Relevance of the Problem**: The problem of learning reusable skills from demonstrations is indeed important and relevant in the context of robot learning. The approach of using weak supervision is a valuable contribution to the field, as it can potentially reduce the burden of manual annotation.\n2. **Innovative Approach**: The use of weakly-supervised learning to segment trajectories is an interesting application of MIL techniques, which could have implications for various domains beyond robotics.\n\n#### Weaknesses:\n1. **Lack of Empirical Support**: The reviewers consistently pointed out that the empirical results presented in the paper are underwhelming. The reported accuracy rates (~35-60%) are low, and there is a lack of comparison with fully-supervised baselines. This raises questions about the effectiveness of the proposed method.\n2. **Insufficient Experimental Design**: The experimental setup lacks clarity and detail. Reviewers noted that the paper does not adequately describe the number of demonstrations required, the architectural choices, or the hyperparameters used. This makes it difficult to assess the validity of the results.\n3. **Limited Scope and Novelty**: The novelty of the approach is questioned, particularly regarding the pooling techniques used. The reviewers noted that the log-sum-exp pooling method has been previously established in other domains, which diminishes the perceived contribution of the paper.\n4. **Inconsistencies and Ambiguities**: There are concerns about the consistency of skill predictions over time, and the potential for the model to switch predictions at each time step without maintaining coherence. This could undermine the utility of the segmentation for downstream tasks.\n5. **Theoretical Limitations**: Reviewer 3 highlighted the need for a theoretical understanding of the dataset requirements for the proposed method to be effective. The authors did not adequately address this concern, which is critical for establishing the feasibility of their approach.\n\n#### Author Responses:\nThe authors provided responses to the reviewers' comments, attempting to clarify their methodology and the rationale behind their experimental design. However, many of the concerns raised by the reviewers remain unaddressed or inadequately addressed. For instance, while the authors argue that their method is more challenging due to the lack of permutation information, they do not provide sufficient evidence that their approach can effectively learn from the weak supervision they propose.\n\n### Conclusion\nGiven the significant concerns raised by the reviewers regarding the empirical results, experimental design, and overall clarity of the contributions, the paper does not meet the standards expected for acceptance at a top-tier conference. The lack of strong empirical evidence and the need for clearer theoretical foundations suggest that the paper requires substantial revisions and additional experiments before it can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Natural Language Descriptions of Deep Visual Features\" presents a novel approach (MILAN) for generating natural language descriptions of neuron activations in deep learning models for computer vision. The authors claim that their method can provide fine-grained descriptions that capture various levels of abstraction in visual features, which can be beneficial for understanding and controlling deep learning models.\n\n#### Strengths:\n1. **Novelty and Relevance**: The paper addresses a significant gap in the interpretability of deep learning models by providing a method to generate natural language descriptions for neuron activations. This is a relevant and timely topic in the field of AI, particularly as models become more complex and opaque.\n   \n2. **Technical Soundness**: The methodology is well-articulated, and the authors provide sufficient technical details for reproducibility. The use of mutual information to guide the generation of descriptions is a clever approach that aligns well with the goals of interpretability.\n\n3. **Empirical Validation**: The authors conduct extensive experiments across different model architectures and datasets, demonstrating the generalizability of their approach. The results indicate that MILAN-generated descriptions achieve high agreement with human annotations, which is a strong point in favor of the method.\n\n4. **Potential Applications**: The paper outlines several practical applications of the MILAN method, including analysis, auditing, and editing of models, which adds to its significance.\n\n5. **Response to Reviewers**: The authors have shown a willingness to engage with reviewer feedback, making several improvements to the paper based on the comments received. This responsiveness is a positive indicator of the authors' commitment to the quality of their work.\n\n#### Weaknesses:\n1. **Inter-Annotator Agreement**: Reviewer 2 raised concerns about the inter-annotator agreement in the MILANNOTATIONS dataset, which could affect the reliability of the training data. The authors' response did not sufficiently address how they handled this inconsistency during training, which is a critical aspect of the methodology.\n\n2. **Generalizability Beyond Vision Models**: While the authors acknowledge that their experiments focus on vision models, they should provide more clarity on the potential limitations of applying MILAN to other domains, such as NLP. The reviewers pointed out that the title and scope could be misleading, and the authors have agreed to refine this.\n\n3. **Bias in Descriptions**: The authors admit that their dataset may inherit biases from the underlying datasets used for training. While they plan to address this in future work, the current lack of bias control is a significant concern, especially given the implications of deploying such models in real-world applications.\n\n4. **Failure Modes**: Although the authors included a discussion of failure modes in their response, the initial paper did not adequately highlight these issues. Understanding the limitations and potential pitfalls of the MILAN method is crucial for its acceptance in the community.\n\n5. **Minor Issues**: Several minor issues were noted by reviewers, such as typos and unclear references to figures, which could detract from the overall clarity of the paper.\n\n#### Conclusion:\nOverall, the paper presents a compelling and innovative approach to interpreting deep learning models through natural language descriptions. The strengths of the paper, particularly its novelty, technical soundness, and empirical validation, outweigh the weaknesses. However, the authors need to address the concerns regarding inter-annotator agreement, bias, and the generalizability of their findings more thoroughly in the final version.\n\nGiven the positive feedback from reviewers, the authors' responsiveness to comments, and the potential impact of the work, I recommend acceptance with the expectation that the authors will address the highlighted concerns in their final submission.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Disentangling Representations of Text by Masking Transformers\" presents a novel approach to learning disentangled representations from pre-trained transformer models, specifically BERT, by employing binary masks over weights or activations. The authors claim that their method can effectively disentangle features such as syntax and semantics, as well as sentiment and genre, without the need for fine-tuning the model parameters.\n\n#### Strengths:\n1. **Novelty**: The approach of using binary masks to uncover subnetworks in pre-trained models is innovative and adds a new perspective to the field of representation learning.\n2. **Robustness**: The authors emphasize the importance of robustness against spurious correlations, which is a significant concern in NLP. Their method aims to address this issue, making it relevant and timely.\n3. **Clarity**: The paper is well-written, and the methodology is presented in a clear and understandable manner. The authors have responded to reviewer comments effectively, clarifying their approach and addressing concerns.\n\n#### Weaknesses:\n1. **Generalizability**: Several reviewers raised concerns about the generalizability of the proposed method, as the experiments were primarily conducted on a limited dataset (movie reviews) with only two genres. While the authors argue that their approach is applicable to various domains, the lack of diverse datasets in the experiments limits the strength of this claim.\n2. **Performance Comparison**: The paper does not convincingly demonstrate that the proposed method outperforms existing state-of-the-art methods across standard benchmarks. While the authors claim improvements over certain baselines, the results are not compelling enough to establish superiority, particularly in semantic tasks.\n3. **Experimental Setup**: The experimental design has been criticized for not including a wider variety of genres and tasks. The reviewers noted that the results could be misleading, as the finetuned baseline performed comparably well in some cases.\n4. **Lack of Robustness Metrics**: While the authors discuss robustness, they do not provide sufficient quantitative metrics to measure the robustness of their model against spurious correlations, which is a critical aspect of their claims.\n\n#### Reviewer Concerns:\n- Reviewers expressed doubts about the clarity of the results presented in figures and tables, particularly regarding the performance of the proposed method compared to finetuned models.\n- The authors' responses to reviewer comments indicate a willingness to improve the paper, but they also highlight that some concerns remain unaddressed, particularly regarding the generalizability and robustness of the method.\n\n### Conclusion\nWhile the paper presents a novel approach and addresses an important problem in NLP, the concerns regarding generalizability, performance, and robustness metrics are significant. The lack of compelling evidence that the proposed method consistently outperforms existing techniques across diverse datasets and tasks raises doubts about its impact and applicability. Given these considerations, the paper does not meet the high standards expected for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Batch Normalization and Bounded Activation Functions\" presents an interesting exploration of the interaction between batch normalization (BN) and bounded activation functions, particularly focusing on the performance implications of the order in which these components are applied in neural network architectures. The authors claim that placing BN after bounded activation functions leads to better performance due to the effects of asymmetric saturation and increased sparsity.\n\n#### Strengths:\n1. **Novel Observation**: The paper identifies a counterintuitive result that the order of BN and activation functions can significantly affect performance, particularly with bounded activations like Tanh. This is a relevant and timely topic in deep learning research.\n2. **Empirical Analysis**: The authors conduct extensive experiments across various architectures and activation functions, providing a solid empirical foundation for their claims.\n3. **Clear Logic Chain**: The paper presents a logical progression from observation to hypothesis to experimental validation, making it relatively easy to follow the authors' reasoning.\n\n#### Weaknesses:\n1. **Lack of Clarity and Rigor**: Several claims made in the paper are not rigorously supported. For instance, the relationship between asymmetric saturation and generalization performance is not convincingly established. Reviewers noted that the authors did not adequately address the possibility that low saturation in deeper layers could also contribute to better generalization.\n2. **Limited Generalizability**: The analysis is primarily focused on bounded activation functions and does not adequately address how these findings might apply to more commonly used activation functions like ReLU. The paper's conclusions may not be broadly applicable to a wide range of architectures.\n3. **Confusing Metrics**: The saturation and sparsity metrics used in the paper are not clearly defined, leading to confusion about their implications. Reviewers pointed out that the saturation metric might not be comprehensive enough to support the claims made.\n4. **Inconsistencies in Results**: There are inconsistencies in the experimental results, particularly regarding the performance of the Swap model compared to the Convention model. The authors did not sufficiently explore or explain these discrepancies, which raises concerns about the robustness of their findings.\n5. **Rebuttal Limitations**: While the authors provided responses to reviewer comments, many of the rebuttals did not fully address the concerns raised. For example, the authors did not convincingly demonstrate that the observed performance improvements were not simply due to accelerated training.\n\n#### Conclusion:\nWhile the paper presents an interesting observation and conducts a thorough empirical analysis, the lack of clarity, rigor, and generalizability significantly undermines its contributions. The concerns raised by the reviewers regarding the validity of the claims and the robustness of the findings are substantial. The paper does not meet the high standards expected for acceptance at a top-tier conference, where clarity, rigor, and generalizability are paramount.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "### Detailed Reasoning\n\nThe paper titled \"Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs\" presents a novel approach to recurrent neural networks (RNNs) inspired by biological mechanisms, specifically the thalamocortical architecture, to improve the learning and chaining of motor motifs. The authors claim that their proposed architecture enhances the accuracy and robustness of RNNs in generating complex behaviors through the efficient chaining of learned motifs.\n\n#### Strengths:\n1. **Relevance of Topic**: The paper addresses a significant challenge in machine learning—continual learning and the chaining of learned behaviors—which is relevant to both neuroscience and artificial intelligence.\n2. **Biological Inspiration**: The incorporation of insights from neuroscience, particularly the thalamocortical circuit, adds a unique perspective to the design of RNNs, potentially bridging the gap between biological learning mechanisms and artificial systems.\n3. **Experimental Validation**: The authors provide empirical results demonstrating the effectiveness of their proposed architecture in improving performance during motif generation and chaining.\n\n#### Weaknesses:\n1. **Concerns About Novelty and Significance**: Multiple reviewers expressed doubts regarding the novelty and significance of the contributions. Reviewer 2 and Reviewer 3 noted that the proposed resetting mechanism may not be the most reasonable or innovative approach, suggesting that simpler alternatives could suffice. This raises questions about the necessity of the proposed architecture.\n2. **Limited Scope of Experiments**: The experiments conducted are on a relatively small scale (300 units, 10 motifs), which may not generalize well to larger, more complex systems. Reviewer 3 specifically pointed out that the relevance of such a small toy system to broader machine learning applications is unclear.\n3. **Lack of Evaluation of Alternatives**: Reviewers highlighted that the authors did not sufficiently explore or test alternative hypotheses or methods that could address the same problem without the need for the proposed inductive bias. This lack of thorough exploration diminishes the perceived robustness of the claims made.\n4. **Writing and Clarity Issues**: Several reviewers noted that the writing is confusing and that the presentation of the results could be improved. This is critical for a top-tier conference, where clarity and communication of ideas are paramount.\n\n#### Reviewer Feedback:\n- Reviewer 1 appreciated the authors' efforts to address concerns but maintained their original scores, indicating that the revisions did not sufficiently resolve the issues.\n- Reviewer 2 and Reviewer 3 raised significant concerns about the novelty and the experimental design, suggesting that the contributions are marginal.\n- Reviewer 4 acknowledged some strengths but also pointed out the lack of clarity in the presentation and the need for deeper exploration of the results.\n- Reviewer 5 recognized the relevance of continual learning but criticized the clarity of the writing and the need for more robust experimental details.\n\n#### Author Response:\nThe authors provided a detailed response to the reviewers' comments, clarifying their methodology and addressing some of the concerns raised. However, the responses did not fully alleviate the doubts regarding the novelty and significance of the work, nor did they convincingly demonstrate the necessity of their approach over simpler alternatives.\n\n### Conclusion\nGiven the mixed feedback from reviewers, particularly the concerns regarding the novelty, significance, and clarity of the work, it appears that while the paper presents an interesting approach, it does not meet the high standards expected for acceptance at a top-tier conference. The lack of robust experimental validation and exploration of alternatives further weakens the case for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"NORML: Nodal Optimization for Recurrent Meta-Learning\" presents a novel approach to meta-learning, specifically targeting the scalability issues of existing frameworks. While the topic is relevant and timely, the reviewers have raised several significant concerns that need to be addressed.\n\n### Detailed Reasoning\n\n1. **Novelty and Contribution**: \n   - Reviewer 1 and Reviewer 3 both highlight that the contribution of NORML appears to be incremental compared to existing methods, particularly the Meta-Learner LSTM by Ravi & Larochelle. The authors do not provide a direct comparison with this foundational work, which is critical for establishing the novelty of their approach. The reviewers suggest that the paper should clarify how NORML improves upon or differs from existing methods, particularly in terms of parameter efficiency and performance.\n\n2. **Experimental Validation**:\n   - The experimental results presented in the paper are not convincing. Reviewer 1 points out that the authors do not include results from Sun et al., which could provide a more comprehensive context for evaluating their method. Additionally, the choice of using fully connected networks for the Omniglot experiment limits the applicability of the findings, as most few-shot learning methods utilize convolutional architectures. The lack of a fair comparison with the Meta-Learner LSTM and other relevant baselines (like MAML++) further weakens the experimental validation.\n\n3. **Theoretical Claims**:\n   - The paper makes several claims regarding the advantages of using an LSTM-based meta-learner, particularly in terms of gradient flow through multiple inner-loop updates. However, the reviewers note that these claims are not substantiated by the experimental results, which involve very few inner-loop steps. This lack of validation raises questions about the theoretical contributions of the paper.\n\n4. **Writing Quality**:\n   - Reviewer 2 emphasizes that the writing quality is subpar, with numerous grammatical errors and unclear descriptions. This is a significant issue for a top-tier conference, as clarity and precision in scientific communication are paramount. The reviewers have pointed out specific instances where the writing could be improved, indicating that a substantial overhaul is necessary.\n\n5. **Inaccuracies in Related Work**:\n   - Several inaccuracies in the descriptions of prior work were noted by the reviewers. These inaccuracies could mislead readers and undermine the credibility of the paper. It is essential for the authors to accurately represent existing literature to establish a solid foundation for their contributions.\n\n### Conclusion\nGiven the significant concerns raised by the reviewers regarding novelty, experimental validation, theoretical claims, writing quality, and inaccuracies in related work, it is clear that the paper does not meet the standards expected for publication at a top-tier conference. The authors need to address these issues comprehensively before the work can be considered for acceptance.\n\nFinal Decision: Reject"
    }
  ]
}