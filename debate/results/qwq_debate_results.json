{
  "summary": {
    "model": "qwq",
    "total_papers": 100,
    "correct_predictions": 77,
    "accuracy": 77.0,
    "confusion_matrix": [
      [
        54,
        22
      ],
      [
        1,
        23
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 71.05263157894737,
        "precision": 98.18181818181819
      },
      "Accept": {
        "total": 24,
        "recall": 95.83333333333334,
        "precision": 51.11111111111111
      }
    }
  },
  "results": [
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"NORML: Nodal Optimization for Recurrent Meta-Learning.\" The abstract mentions that it's a novel meta-learning framework using an LSTM-based meta-learner to perform neuron-wise optimization on a learner network. The key points are that the number of meta-learner parameters scales linearly with the learner parameters, and experiments show it's better than MAML on Omniglot. \n\nLooking at the reviewer comments, there are three reviewers. Let's go through each one's points.\n\nReviewer 1 points out that the paper's contribution compared to Ravi & Larochelle (Meta-Learner LSTM) might not be as significant as claimed. The authors say their approach avoids scaling issues, but Ravi & Larochelle already used parameter sharing to prevent the meta-learner's parameters from exploding. So the authors' claim about their method's parameter efficiency might be incorrect. The reviewer also mentions that experiments don't compare fairly with Ravi & Larochelle, especially in the setup where the convolutional layers are frozen. Additionally, the experiments on Mini-ImageNet don't include results from Sun et al., which actually perform better, and the Omniglot test uses a fully connected network instead of the standard convolutional one, making comparisons with prior work less meaningful. The reviewer also questions the claim about handling many inner-loop steps without evidence, since the experiments used few steps.\n\nReviewer 2 criticizes the paper's writing quality, noting many grammatical errors, citation issues, and poor flow. They also mention that the theoretical novelty is minimal and the paper is below the expected standard for ICLR. The writing needs a major overhaul, which is a big red flag for a top conference.\n\nReviewer 3 states that the method is an incremental improvement over existing work like Meta-Learner LSTM but lacks a fair comparison. They point out that the experiments don't use the same base learner (ResNet-12 vs. others) and miss important baselines like MAML++ and per-parameter learning rates. The method is limited to fully connected networks, which is a drawback since most few-shot learning uses convolutions. There are also inaccuracies in the related work section regarding prior methods' parameter usage. For example, the authors incorrectly claimed that Andrychowicz used separate LSTMs per parameter, but actually, they shared parameters. Also, NORML might be using different LSTMs per layer, contradicting their own claims. Minor issues like equations and missing confidence intervals are also noted.\n\nThe author response isn't available, so we can't see any rebuttals to these points.\n\nNow, considering all this:\n\n1. **Novelty and Contribution**: The main contribution is supposed to be scaling the meta-learner parameters linearly, but Reviewer 1 and 3 point out that prior work (Ravi & Larochelle) already addressed scaling through parameter sharing. The authors might not have a significant novel contribution here. The experiments don't properly compare with key baselines, making it hard to assess superiority.\n\n2. **Experimental Rigor**: The experiments have several issues. Not including Sun et al.'s results when they perform better is a problem. The Omniglot test uses a non-standard architecture, making comparisons with MAML unfair. The claim about handling many inner steps isn't supported by experiments with few steps. Without proper baselines and setups, the results aren't convincing.\n\n3. **Writing Quality**: Reviewer 2's comments on the writing are severe. Grammatical errors, citation mistakes, and poor flow are major issues for a top conference. The paper needs a major rewrite, which might not be feasible in the conference timeline.\n\n4. **Technical Soundness**: The method's description has inaccuracies in related work, which undermines credibility. The parameter sharing vs. separate LSTMs confusion shows a lack of thorough understanding of prior work. This could indicate deeper flaws in the approach.\n\n5. **Applicability**: The method is limited to fully connected networks, which is a significant limitation since most applications use convolutions. This reduces its practical value.\n\nConsidering all these points, the paper has multiple critical issues: lack of proper comparisons, experimental shortcomings, writing quality, and potential inaccuracies in describing prior work. These issues are substantial enough to question the paper's contribution and validity. Top-tier conferences require strong contributions, rigorous experiments, and clear writing. Since the paper fails on multiple fronts, especially in comparison to existing methods and experimental setup, it's unlikely to meet the standards.\n\n**Final Decision: Reject**\n</think>\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about MuPPET, a method for quantized training of CNNs using multiple precisions and a switching mechanism. The main claims are that it reduces training time and energy without sacrificing accuracy, using fixed-point representations and a dynamic precision-switching strategy based on gradient diversity.\n\nLooking at the reviewers' comments first. Reviewer 1 found the paper interesting but criticized the lack of detailed explanation for the switching mechanism. They pointed out that the motivation for switching when gradients are co-aligned isn't clear. Also, there were some typos and concerns about the AlexNet example being ad-hoc. \n\nReviewer 2 had more critical points. They noted that the approach doesn't actually choose the \"most appropriate quantization level\" as claimed, since the levels are fixed. The precision levels (8,12,14,16 bits) were confusing, and there were unclear notations like 'p' and 'y' in the algorithm. The experiments didn't show clear advantages over state-of-the-art; in some cases, accuracy was lower than FP32, and the method wasn't better than existing work like [2]. Also, missing training curves and unclear quantization steps were issues.\n\nReviewer 3 mentioned that the core idea (dynamic switching) isn't shown as necessary for good results. The introduction didn't clearly state the contribution, and the impact of parameters like 'p' and thresholds wasn't isolated. Reproducibility was a problem because quantization steps weren't detailed, and some terms were unclear.\n\nThe authors responded by clarifying that their focus is on runtime benefits, not necessarily accuracy. They addressed some points like adding figures, explaining parameters, and justifying the quantization levels empirically. They also mentioned that accuracy differences were minor and that their method allows lower precisions than previous work.\n\nNow, considering the standards of a top-tier conference. The key factors are novelty, technical soundness, clarity, and impact. \n\nFirst, the novelty: The paper introduces a multi-precision strategy with a dynamic switch based on gradient diversity. While the idea isn't entirely new (since mixed-precision is existing), using fixed-point and a more granular precision switching could be a contribution. However, Reviewer 2 pointed out that the precision levels are fixed, so the \"most appropriate\" part isn't fully realized. The authors addressed this by saying they chose levels empirically to balance performance, but that might not be a strong enough contribution for a top conference.\n\nTechnical soundness: The method's core mechanism (switching based on gradient diversity) has unclear reasoning. Reviewer 1 and 3 questioned why co-aligned gradients mean less to learn. The authors added some explanation but it's still heuristic. The experiments show speedups but not significant accuracy gains. Also, the accuracy on some networks (AlexNet, ResNet) was lower than FP32, which the authors dismissed as minor fluctuations, but that's a concern. The comparison to [2] (Mixed Precision Training) didn't show superiority, which is a problem since top conferences require clear improvements over SOTA.\n\nClarity: The paper had several issues in explanations, notation, and figures. The authors addressed some in the response (like adding figures to appendices, clarifying equations), but the core algorithm's steps (like 'y' in step 5) were still unclear. The introduction was vague initially but improved in the response. However, the lack of training curves and detailed quantization steps remain issues affecting reproducibility.\n\nImpact: The speedup is 1.28x average, which might not be transformative. The method's necessity isn't proven since the core idea's impact isn't clearly demonstrated. The authors argue that their approach allows lower precisions, but without better accuracy or speed, this might not be impactful enough.\n\nConsidering all this, the main issues are the lack of clear technical justification for the switching mechanism, insufficient experimental validation showing superiority over SOTA, and clarity problems. While the problem is important, the contributions don't seem strong enough for a top-tier conference. The reviewers' concerns about the method's necessity and the experiments not demonstrating significant advantages are critical. The authors addressed some points but didn't resolve the core issues like the heuristic nature of the algorithm and the mixed experimental results.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Technical Soundness and Novelty**:\n   - The core idea of dynamic precision switching is not sufficiently justified. The switching mechanism's rationale (e.g., why co-aligned gradients indicate reduced learning potential) remains heuristic and under-explained despite author clarifications. Reviewers highlighted that the logic for transitioning between fixed precision levels is unclear, and the method does not dynamically select precision levels but instead uses pre-fixed thresholds and bit-widths. This undermines the novelty and technical rigor expected at a top-tier conference.\n   - While the use of fixed-point representations and multi-level precision is a contribution, the experiments do not demonstrate a transformative improvement over existing methods like mixed-precision training (e.g., [2]). The average speedup of 1.28× is modest, and accuracy results for some networks (e.g., AlexNet, ResNet18) are marginally worse than FP32 baselines, which the authors dismiss as \"minor fluctuations\" without robust statistical justification.\n\n2. **Experimental Validation**:\n   - The results lack sufficient evidence of superiority over state-of-the-art methods. For instance, compared to [2], the proposed method is more complex but does not achieve better accuracy or speedup. The authors argue that MuPPET enables lower precisions (e.g., 8-bit fixed-point), but this is not clearly tied to practical benefits (e.g., hardware compatibility or energy efficiency metrics are underdeveloped).\n   - Key experimental details (e.g., training/validation curves, quantization steps, hyperparameter sensitivity) are missing or inadequately explained, hindering reproducibility. While the authors added figures to appendices, the core presentation remains unclear, particularly regarding the precision-switching policy’s parameters (e.g., \"p\" and \"y\").\n\n3. **Clarity and Presentation**:\n   - The paper suffers from poor clarity in critical sections. Notations like \"p\" and \"y\" in the algorithm remain undefined, and the quantization scheme lacks explicit details (e.g., scaling factors in Equations 4–5). The introduction was vague about contributions initially and, while revised, still does not clearly articulate the necessity of the dynamic switching mechanism.\n   - Figures and tables are inadequately described, and the revised version does not fully address readability concerns (e.g., Figure 2’s threshold violations remain unexplained).\n\n4. **Impact and Necessity**:\n   - Reviewer 3’s critique—that the dynamic switching is not shown to be a *necessary* condition for good results—is valid. The experiments do not isolate the impact of the switching mechanism versus fixed precision schedules, making it unclear whether the added complexity is justified. The claimed \"generalisability\" across networks and datasets is weakly supported, with only three networks tested and no ablation studies.\n\n### Conclusion:\nWhile the problem of accelerating training via low-precision methods is important, the paper falls short in technical depth, experimental rigor, and clarity. The heuristic nature of the switching mechanism, insufficient validation against SOTA, and unresolved presentation issues make it unsuitable for a top-tier conference. The authors’ responses address minor concerns but fail to resolve the core limitations highlighted by reviewers."
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\". The keywords are Reinforcement Learning, MCTS, and AlphaZero. The abstract mentions that they're looking at the coordination of episode generation using MCTS to handle exploration vs. exploitation, tested on a small problem like tic-tac-toe, showing better robustness than AlphaZero. \n\nNow, looking at the reviewer comments. Reviewer 1 says it's \"less than half-baked\" with no references and poor writing. They suggest resubmitting when it's more complete. Reviewer 2 had a hard time understanding the paper due to grammar issues and unclear writing, even in the abstract. They mention the paper has inaccurate statements and lacks basic scientific elements. Reviewer 3 also points out poor writing, undefined terms, missing references, and structural issues like blank spaces and hard-to-follow algorithms. They think it's not a serious academic work but just notes.\n\nThe author's response addresses some points. They apologize for the writing issues, mention they couldn't use references because of .bib file issues, and are aware of similar work in their country but not internationally. They want to improve their writing and do more research.\n\nHmm, so the main issues here are the writing quality, lack of references, and clarity of the method. The reviewers are pretty harsh, especially on the writing. The paper is described as incomprehensible, with grammatical errors everywhere. Even the abstract isn't clear. The authors admit to removing references because they didn't know how to use .bib, which is a basic part of academic writing. The method isn't clearly explained, and there's no related work section, which is a red flag for a conference paper. \n\nThe problem they're addressing—improving MCTS in RL—is valid, but the execution here is lacking. The experiments are on a small problem (tic-tac-toe), which might not be sufficient for a top-tier conference. Also, the author mentions finding similar work after submission, which suggests that maybe the novelty isn't there, or at least they didn't do a proper literature review. \n\nTop-tier conferences require rigorous methodology, clear writing, and proper citations. This paper fails on all those counts. The reviewers are unanimous in their criticism, especially about the writing and lack of references. Even if the idea has potential, the presentation is so poor that it's hard to assess the actual contribution. The author's response shows they recognize some issues but doesn't provide evidence that the core problems (like the writing and references) can be fixed without major revisions. \n\nConsidering all this, the paper doesn't meet the standards for a top conference. It needs substantial revisions, possibly resubmission after addressing all the structural and writing issues. The reviewers' consensus is strong on rejection, so I think that's the right call here.\n</think>\n\nThe paper under review faces significant and fundamental issues that preclude its acceptance at a top-tier conference. Here is the detailed reasoning:\n\n### Key Issues Identified:\n1. **Poor Writing Quality**:\n   - Multiple reviewers highlighted severe grammatical errors, unclear sentences, and incoherent writing throughout the paper. The abstract itself is described as incomprehensible, making it impossible for readers to grasp the core contributions or methodology.\n   - The writing is so flawed that reviewers could not decipher the proposed method, its novelty, or its relationship to prior work. This undermines the paper’s scientific rigor and accessibility.\n\n2. **Lack of References and Related Work**:\n   - The paper contains **no references**, a critical omission in academic work. This suggests a lack of awareness of existing literature, including foundational and relevant prior research (e.g., AlphaZero, MCTS-based RL methods).\n   - The author’s response acknowledges this gap and cites unfamiliarity with .bib files as the reason, which is inexcusable for a submission to a top-tier conference. Proper citation is a basic requirement for scholarly work.\n\n3. **Unclear Methodology and Contribution**:\n   - Reviewers struggled to understand the proposed method, including what \"coordination of episode generation\" entails or how the \"master game tree\" differs from existing MCTS approaches.\n   - The paper lacks formal definitions, algorithmic details, and clarity in explaining the innovation. Reviewers noted undefined mathematical symbols, poorly formatted algorithms, and vague claims (e.g., \"robust performance\" without statistical validation).\n   - The experiments are limited to a trivial problem (tic-tac-toe), which raises questions about the method’s scalability and practical significance. The author’s response mentions finding similar work post-submission, further suggesting potential lack of novelty.\n\n4. **Structural and Formatting Flaws**:\n   - Reviewer 3 noted \"blank space\" in the paper and disorganized content, indicating poor preparation. A 10-page paper with such issues signals a lack of effort and professionalism.\n   - The paper appears to be incomplete, with missing sections (e.g., related work) and insufficient experimental analysis.\n\n### Author Response Analysis:\n- The authors acknowledge the writing issues and lack of references but offer no concrete evidence that these can be resolved without major revisions. Their focus on improving writing skills and securing proofreading in the future does not address the immediate need for a polished submission.\n- The mention of discovering similar work post-submission raises concerns about the novelty and thoroughness of the research process. The absence of a literature review further weakens the paper’s credibility.\n\n### Conference Standards:\nTop-tier conferences demand:\n- **Clarity**: The paper must be clearly written, with precise definitions and logical flow.\n- **Rigor**: Methodology, experiments, and contributions must be well-defined and validated.\n- **Scholarship**: Proper citation of prior work is mandatory to contextualize contributions.\n- **Significance**: Contributions must address meaningful gaps or improve upon existing methods in a non-trivial way.\n\nThis paper fails on all counts. The writing barriers alone make it impossible for reviewers or readers to evaluate the work’s merits. The lack of references and unclear methodology further erode its academic credibility. While the problem of improving MCTS-based RL is valid, the execution here is insufficient for a top-tier venue.\n\n### Final Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper proposes a generative model called ROOTS for unsupervised object-wise 3D scene decomposition. The reviewers have given mixed feedback. Let me start by going through each reviewer's comments and the author responses to see if the paper meets the standards of a top-tier conference like ICLR.\n\nFirst, Reviewer 1 initially wanted to reject due to poor writing but after the revision, they're more satisfied, though still think the writing needs work. They mention the claims about being the first unsupervised model identifying 3D objects were incorrect, which the authors fixed. The qualitative results are impressive, but the writing issues were a big problem. The authors addressed notation inconsistencies and clarified sections, which is good.\n\nReviewer 2 points out that the paper's claims aren't supported by experiments. For example, Figure 2 didn't show the claimed better generations. The authors addressed this by clarifying the figure and adding more experiments in the appendix. They also fixed some clarity issues like explaining GoodNet and Figure 3. However, the experiments were still considered weak, especially the lack of ablation studies and comparisons to CGQN. The authors added some experiments but maybe not enough.\n\nReviewer 3 is concerned about the lack of citations and ablation studies. They mention that 3D representations aren't new and that the hierarchical approach's benefit isn't shown. The authors added related work sections and citations, which should help. The ablation study point is tricky; the authors argued their contribution isn't about performance metrics but about the representation, so maybe that's acceptable, but some reviewers still want to see it.\n\nReviewer 4 found the method complex and poorly explained, with unclear notation and missing details. The experiments were weak, and the paper didn't situate itself well in related work. The authors rewrote sections for clarity, fixed notation, and added more related work. They also addressed some experimental concerns, but the AC still had doubts about the experiments and whether the model's benefits are sufficiently demonstrated.\n\nThe authors' responses seem to address many of the technical concerns: fixing notation, adding references, improving clarity, and adding experiments. However, some reviewers, especially Reviewer 4, remain unconvinced about the experimental rigor, particularly the lack of ablation studies and comparisons. The main contributions are about the object-oriented 3D representation, which the authors argue don't require traditional ablation studies because the focus is on the representation's capabilities rather than performance metrics. \n\nThe paper's strengths are the novel approach to combining GQN with object-oriented representations, the clear qualitative results showing decomposition and composition, and addressing the 3D viewpoint invariance. The weaknesses include the initial writing issues, insufficient experimental comparisons (like with CGQN), and the lack of ablation studies. The authors have made significant revisions, but the AC still has concerns about the experiments not being strong enough and the model's complexity not being justified by performance gains.\n\nConsidering top-tier standards, the paper needs to demonstrate clear, significant contributions with thorough experiments. While the method is innovative, the experiments are seen as weak, and the lack of ablation studies might hinder understanding the model's components' contributions. However, the authors have addressed many issues, and the core idea of object-oriented 3D representation is compelling. The revisions have improved clarity and added necessary references, which are important. The AC's final note suggests leaning towards rejection due to experiments and comparisons, but the reviewers' post-rebuttal scores (like R1 moving to 6) might tip it towards acceptance if the revisions are solid.\n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about a new method called pro-VLAE that uses progressive learning to improve disentanglement in hierarchical representations with VAEs. The reviewers had some concerns initially, but after the rebuttal, some of them changed their scores. \n\nFirst, looking at the abstract and the problem they're addressing. They mention that traditional VAEs might compromise disentanglement when extracting high-level abstractions. Their solution is a progressive approach starting from high-level to low-level, which seems novel. The key contributions are the progressive learning strategy and a new disentanglement metric (MIG-sup). \n\nNow, the reviewer comments. Reviewer 1 wanted more experiments on mutual information per layer and clarification on the first layer's training. The authors addressed this by adding mutual information comparisons in the appendix, showing that their method's layers capture information in a descending order, which aligns with their claims. That's good.\n\nReviewer 2 had several points. They questioned the paper's focus between disentanglement and hierarchical representations. The authors clarified that the goal is disentanglement through hierarchical learning, which makes sense. The reviewer also thought the qualitative results were worse than VLAE, but the authors provided new experiments showing better performance, especially in MNIST. They also addressed the metric issue by explaining their MIG-sup isn't the same as the one in [2], and that ground truth factors are standard in such metrics. The ablation studies on implementation strategies (fade-in, pre-KL) were added, showing improved training stability. The hyperparameter beta sensitivity was discussed, and they compared with [3], distinguishing their approach from it. Reviewer 2's final score went up to a weak accept after the rebuttal.\n\nReviewer 3 found the idea fresh and experiments thorough but wanted more quantitative info on information flow and better formalization. The authors added mutual info experiments across different layers and latent dimensions, which addresses the quantitative part. They also fixed the formalization in the text. The pros outweigh the minor issues here.\n\nReviewer 4 liked the approach and results but wanted ablation studies on fade-out and more comparisons with VLAE. The authors did ablations on strategies and provided more VLAE comparisons. The fade-out experiment didn't show improvement, but the main point was the progressive learning. The results look solid.\n\nThe author responses seem to have adequately addressed most concerns. The key points were improving the experiments, clarifying the method's advantages over existing work, and fixing the metric's explanation. The rebuttal added necessary data, like mutual information comparisons and ablation results, which strengthened the paper.\n\nPotential remaining issues: Reviewer 2's initial concern about the metric's practicality since it requires ground truth. But the authors pointed out that most disentanglement metrics do, so that's a common limitation. The focus on progressive learning's novelty is clear now, especially compared to [3]. The MNIST results were clarified with better generation strategies matching the baseline, which helps.\n\nOverall, the paper addresses the main criticisms, provides additional experiments, and clarifies ambiguities. The method shows quantitative improvements in disentanglement metrics and the approach is novel enough. The reviewers' scores, especially after rebuttal, lean towards acceptance. The conference standards require strong contributions and solid evaluations, which this paper now meets after revisions.\n</think>\n\nThe paper proposes pro-VLAE, a method that combines progressive learning with hierarchical VAEs to improve disentanglement. Initially, reviewers raised concerns about experimental rigor, clarity of purpose, and comparisons with prior work. However, the authors addressed these effectively in their rebuttal:\n\n1. **Addressing Reviewer 1's Concerns**: The authors added mutual information comparisons between layers and VLAE, demonstrating that their method achieves a clearer descending order of information in hierarchical layers. This validates their claim that progressive learning better disentangles factors of variation.\n\n2. **Clarifying Purpose and Comparisons (Reviewer 2)**: The authors explicitly distinguished their focus on disentanglement via progressive hierarchical learning, clarified the relationship between hierarchical representations and disentanglement, and provided stronger comparisons with VLAE. They also addressed the metric criticism by noting that ground-truth factors are standard in disentanglement evaluation and distinguished their approach from [3] in terms of capacity growth mechanisms.\n\n3. **Ablation Studies and Stability (Reviewers 2, 3, 4)**: The authors added ablation studies on implementation strategies (fade-in, pre-KL), showing improved training stability. They also provided quantitative information flow experiments across layers, addressing Reviewer 3's request for formalization and quantitative analysis.\n\n4. **Qualitative and Quantitative Improvements**: New experiments (e.g., MNIST generation with consistent strategies, 3DShapes results) demonstrate that pro-VLAE outperforms VLAE qualitatively and quantitatively, countering initial concerns about inferior performance.\n\n5. **Hyperparameter Sensitivity and Metric Justification**: The authors discussed beta's role and acknowledged limitations, while defending their metric's necessity as a supplement to existing ones, noting that ground-truth reliance is standard in disentanglement evaluation.\n\n**Remaining Considerations**: \n- The metric's reliance on ground-truth factors is a known limitation in disentanglement research, but the authors appropriately contextualized this.\n- While some comparisons (e.g., with [3]) required clarification, the rebuttal sufficiently distinguished their approach.\n\n**Conclusion**: The revised paper strengthens its contributions through additional experiments, clarifications, and ablations. The method’s novelty in combining progressive learning with hierarchical VAEs for disentanglement, along with quantitative improvements over baselines, meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about generating singing voices without scores or lyrics, which is a novel approach. The reviewers have mixed feedback. Let me start by going through each reviewer's comments and the author responses to see how they addressed the concerns.\n\nFirst, Reviewer 1 was concerned about the evaluation metrics. They felt that the metrics like vocalness and average pitch didn't properly assess singing quality. The authors added new baselines (Sinsy and Synthesizer V) and expanded the MOS studies. They also revised the vocalness metric by incorporating pitch range and using a better model (JDC). That seems to address some of the evaluation issues, but I need to check if the new metrics are sufficient.\n\nReviewer 2 pointed out the lack of motivation for the three tasks and the need for baselines. The authors added baselines and clarified the training data details. They also addressed the dataset concerns by explaining why MIR-1k wasn't used and provided more info on their dataset. The validation split explanation makes sense given the data constraints. They also mentioned comparing with MelNet, though it's in the future work. The main issue here was the evaluation context, which the authors tried to fix with new tables and audio comparisons. But does this make the evaluation strong enough for a top-tier conference?\n\nReviewer 3 was worried about the evaluation and literature review. The authors added more literature on neural singing synthesis and improved the vocalness metric. They also addressed the CREPE pitch tracking criticism by filtering non-vocal frames. The revised metrics and baselines might help here, but the subjective evaluation still relies on MOS, which is okay but not perfect.\n\nReviewer 4 was harsh, saying the paper addresses too many problems and lacks justification. The authors added ablation studies on GAN losses, clarified the BEGAN adaptation, and fixed some architecture explanations. They also moved some details to appendices. The main issue was the paper's scope and justification. The authors tried to narrow it down by focusing on the three tasks and providing more experimental details. However, the paper still might be tackling multiple aspects (source separation, generation, evaluation), which could be a problem for a top conference that prefers focused contributions.\n\nReviewer 5 wanted comparisons with MIDI-based methods. The authors added the Sinsy and Synthesizer V baselines, which use MIDI. They also explained why waveform generation is preferable in their context. This addresses the baseline issue, but the MIDI approach is still a valid concern. The authors argue their approach is for different use cases, which is a valid point but might not fully counter the criticism.\n\nLooking at the author responses, they made significant revisions: added baselines, improved metrics, clarified data, and addressed technical points. However, some core issues remain. The evaluation still relies on subjective MOS and possibly incomplete objective metrics. The problem of generating singing without lyrics/scores is novel, but the paper's contribution might be seen as preliminary given the lack of strong baselines and the complexity of the tasks. The reviewers' main concerns were about evaluation and motivation, which the authors tried to address but maybe not fully.\n\nTop-tier conferences usually require strong evaluations and clear contributions. The paper introduces a new problem but the evaluation is still somewhat weak despite improvements. The baselines added are better, but the metrics like vocalness and average pitch might not capture the full quality of singing. The fact that the authors couldn't compare with score-based methods using extracted scores (as suggested by Reviewer 2) might still be a gap. Also, the paper's scope is broad, which Reviewer 4 criticized, and while they clarified parts, it's still a lot for a single paper.\n\nOn the positive side, the problem is novel, the approach is technically sound, and the code is provided. The authors made good faith efforts to address concerns. However, the evaluation still has limitations, and the paper might not meet the high bar for a top conference where rigorous evaluation is crucial. The reviewers' mixed feedback, with some still leaning towards rejection due to evaluation and scope, might tip the decision.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\nThe paper proposes a novel and challenging problem—generating singing voices without scores or lyrics—and introduces three task settings (free, accompanied, and solo singer). While the problem is innovative and the technical approach is sound, the paper falls short in critical areas that are non-negotiable for a top-tier conference:\n\n1. **Evaluation Shortcomings**: \n   - **Metrics**: The objective metrics (vocalness, average pitch, matchness) are insufficient to fully assess singing quality. Even after revisions, these metrics do not address melody coherence, lyrical intelligibility, or harmonic consistency, which are central to singing. The revised vocalness metric (using JDC) and inclusion of baselines (Sinsy, Synthesizer V) improve the evaluation but remain limited in scope.\n   - **Baselines**: While the authors added baselines, they did not compare against score-based methods using extracted scores (e.g., using AMT for score estimation), as suggested by Reviewer 2. This omission leaves a critical gap in demonstrating the necessity of their approach over existing methods adapted to their problem.\n\n2. **Motivation and Scope**:\n   - The motivation for the three task settings is still underdeveloped. The authors clarify the training data and tasks but fail to convincingly argue why these settings are meaningful or how they advance the field beyond being \"novel.\"\n   - The paper attempts to solve multiple problems (source separation, transcription, generation, evaluation) simultaneously, leading to a diffuse contribution. Reviewer 4’s criticism about the paper \"aiming to solve too many problems at once\" remains valid, especially given the lack of ablation studies on key architectural choices (e.g., GRUs vs. dilated convolutions).\n\n3. **Technical Rigor**:\n   - The justification for adopting BEGAN and other GAN variants relies on hand-wavy arguments (e.g., \"pilot experiments\") rather than systematic ablation studies. While Appendix D adds some comparisons, the core text lacks clarity on why BEGAN was chosen over alternatives like PatchGAN.\n   - The dataset description remains unclear (e.g., genre diversity, training/test splits), and the reliance on in-house, genre-restricted data (Jazz) limits generalizability.\n\n4. **Literature and Context**:\n   - The literature review, while improved, still underemphasizes neural singing synthesis methods and MIDI-based approaches. The authors’ rebuttal to Reviewer 5’s MIDI critique is valid but does not fully address the concern that waveform-based generation introduces unnecessary complexity (e.g., source separation) compared to MIDI-based pipelines.\n\n5. **Subjective Evaluation**:\n   - The MOS studies are a step forward but lack context (e.g., no raw scores or comparisons to training data distributions). Without clear baselines or thresholds, it is difficult to interpret the results meaningfully.\n\n### Conclusion:\nWhile the problem is novel and the authors made commendable revisions, the paper’s evaluation remains insufficient for a top-tier conference. The lack of rigorous comparisons, incomplete metrics, and overly broad scope hinder its acceptance. The work is better suited for a venue with lower evaluation standards or as a follow-up study with stronger experimental validation."
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" with keywords GMM and SGD. The abstract mentions three main contributions: using an upper bound on the log-likelihood for numerical stability, a regularizer to prevent bad local minima, and a method to enforce GMM constraints with SGD. They also simplify the model using local principal directions to handle high-dimensional covariance matrices. Experiments are on image datasets like MNIST and SVHN, and they have a TensorFlow implementation.\n\nNow, looking at the reviewer comments. Reviewer 1 says the work is incremental and shows a lack of understanding of online EM. They point out that the max-component approximation is similar to existing log-sum-exp tricks, and the smoothing in 3.3 is unclear. They also criticize the experiments for using datasets that aren't high-dimensional enough (like 28x28 images). \n\nReviewer 2 mentions the lack of convergence guarantees and theoretical analysis, which is a big issue for a top conference. They also note that some parts of the method, like the max-component approach, might not be novel. The experiments are not convincing enough, and there's confusion about the covariance handling. \n\nReviewer 3 thinks the paper is clear but the contributions are incremental. The main novelty is the regularizer, but it's unclear how it compares to EM with K-means initialization. The experiments don't show clear advantages over EM, and the covariance approach isn't well demonstrated. They also point out some technical errors, like the definition of μ^i and the principal directions.\n\nThe authors responded to these points. They clarified that their max-component approach addresses gradient instability differently from log-sum-exp, which is used in EM but not sufficient for SGD. They also explain that their \"regularizer\" is more like annealing and that their method is for online settings where K-means isn't feasible. They mention adding experiments and clarifications on covariance handling.\n\nHmm. Let's break down the key points. \n\nFirst, the main contributions: the authors propose using SGD for GMMs in high dimensions, which is a valid problem. Their three ideas are the max-component approximation, a regularizer/annealing method, and a covariance simplification. \n\nBut the reviewers point out that some of these ideas aren't novel. The max-component is similar to existing tricks, and the regularizer is more of an annealing technique. The covariance approach using principal directions is mentioned but not well-explained, and the experiments only show diagonal covariances, which are simpler. \n\nThe experiments are a big concern. The datasets (MNIST, SVHN) are considered high-dimensional here (like 3072 for SVHN), but maybe not as high as, say, 10k dimensions. However, the authors argue that compared to typical GMM applications (2-50D), these are high. Still, the experiments don't compare runtime or show clear superiority over EM, especially in terms of log-likelihood. The figures might be cherry-picked, and the evaluation lacks thoroughness. \n\nThe lack of theoretical guarantees is a red flag for a top conference. While the authors mention convergence via gradient descent minimizing an energy function, that's pretty standard and not a strong guarantee. Moment-matching methods have better theoretical backing, which the paper doesn't address. \n\nThe authors' responses clarify some points but don't fully address the novelty and evaluation issues. They mention adding more experiments and clarifications, but since the current submission has these flaws, it's unclear if those changes would be sufficient. \n\nThe paper's main angle is enabling SGD for GMMs in online settings, which is a valid niche. However, the contributions are incremental, and the experiments don't convincingly show significant advantages over existing methods. The technical depth and novelty might not meet the high bar of a top conference. \n\nConsidering all this, the paper has some merit but falls short in innovation, thorough evaluation, and theoretical backing. The reviewers' concerns about being incremental and insufficient experiments are valid. Therefore, I think it should be rejected unless major revisions address these points, but given the current state, rejection is appropriate.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Novelty and Contribution**: \n   - The paper's core ideas (max-component approximation, annealing-based \"regularization,\" and covariance simplification via principal directions) are incremental. Key components (e.g., log-sum-exp stabilization, annealing) are either standard or rebranded existing techniques. The principal directions approach is underdeveloped in the current submission, with unclear implementation details and no compelling empirical validation beyond diagonal covariances.\n   - The problem of training GMMs with SGD in high-dimensional spaces is valid, but the contributions lack sufficient innovation compared to prior work (e.g., online EM, moment-matching methods).\n\n2. **Technical Soundness**:\n   - **Numerical Stability**: The max-component approximation is presented as a novel solution, but reviewers and authors acknowledge that existing log-sum-exp tricks address similar issues. The authors argue their approach stabilizes gradients uniquely for SGD, but this claim is not rigorously demonstrated.\n   - **Regularization/Annealing**: The \"regularizer\" is better framed as annealing, which is a heuristic without a clear theoretical grounding. The gradient non-smoothness (e.g., max operations) and covariance parameterization (e.g., μⁱ undefined, principal directions unclear) raise unresolved technical concerns.\n\n3. **Empirical Evaluation**:\n   - **Dataset Limitations**: While SVHN (3072 dimensions) qualifies as high-dimensional relative to traditional GMM applications, the experiments lack comparisons to strong baselines (e.g., online EM variants) and fail to demonstrate clear advantages over EM in terms of runtime, convergence speed, or final performance. The reported results do not convincingly show superiority in high-dimensional settings.\n   - **Experimental Rigor**: Issues such as cherry-picked visualizations, missing runtime comparisons, and unclear log-likelihood metrics weaken the credibility of the results. The authors' promise to address these in revisions is insufficient for acceptance at this stage.\n\n4. **Theoretical Guarantees**:\n   - The paper lacks convergence guarantees or performance bounds, which are critical for a top-tier conference. While the authors cite gradient descent's general convergence properties, this is far weaker than the theoretical foundations of competing methods (e.g., moment matching).\n\n5. **Broader Context**:\n   - The motivation for online/streaming settings is valid, but the paper does not adequately address scenarios where batch methods (e.g., EM with K-means initialization) remain viable. The experimental comparison to EM is incomplete, and the claimed advantages (e.g., avoiding K-means initialization) are not empirically validated.\n\n### Conclusion:\nWhile the paper addresses a technically challenging problem and proposes a feasible approach, its contributions are incremental, its technical details are underdeveloped, and its empirical evaluation is insufficient to justify acceptance at a top-tier conference. The lack of strong theoretical grounding and compelling experimental results compared to existing methods further weakens its case. The authors' responses clarify some points but do not resolve the core issues raised by reviewers."
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about extreme classification, which deals with a large number of classes. The main issue they address is the computational cost of traditional softmax, which scales linearly with the number of classes. Their solution is an adversarial sampling method for negative examples, which they claim reduces training time by an order of magnitude compared to baselines. The contributions are three-fold: an adversarial sampling mechanism with logarithmic cost, a proof about minimizing gradient variance, and experiments showing faster training.\n\nLooking at the reviewers' comments. Reviewer 1 is positive, finding the paper clear and the proposal useful, recommending acceptance. Reviewer 2 has some concerns. They point out that the authors didn't discuss related work sufficiently, especially papers [1]-[4], which also deal with non-uniform sampling. They also mention that the experiments don't include comparisons with the full softmax on smaller datasets and some typos. Reviewer 3 is more critical. They argue the experiments are incomplete, missing recent state-of-the-art methods like Slice and DiSMEC. They also mention the lack of evaluation on tail-labels and smaller datasets, and the multi-label setting. The author response addresses some points, like adding references and running more experiments, but they stand by their focus on negative sampling and not including non-sampling methods.\n\nFirst, the paper's contributions. The adversarial sampling approach is novel, and the theoretical proof about gradient variance is a strong point. The experiments show a significant speedup, which is valuable. However, the reviewers highlight that the related work section was lacking. The authors addressed this by agreeing to add more references, which is good. But the key issue is whether the proposed method is truly novel compared to existing work like [3] and [4], which also use non-uniform sampling. The authors argue that their approach is different because they focus on variance reduction via adversarial sampling of the data distribution, whereas those papers aim for unbiased sampled softmax. That distinction seems valid, but the reviewers pointed out that those methods might also be sampling hard negatives. The authors clarify that those methods track the model distribution, not the data, which is a crucial difference. So their theoretical contribution still holds.\n\nOn experiments, Reviewer 3's points are significant. The paper only uses two large datasets and doesn't compare against Slice or DiSMEC. The authors mention they can't include non-negative sampling methods because their focus is on improving negative sampling. But in a top conference, it's important to show where their method stands against the best existing approaches, even if they're different. The rebuttal says they'll add EURLex results, which helps, but not sure if that's enough. Also, not evaluating on tail-labels or multi-label is a gap. However, the authors argue their focus is on single-label, which is still a valid niche. The lack of comparison with Slice and DiSMEC is a problem because those are state-of-the-art. The authors didn't address that in the rebuttal except mentioning they can't include non-sampling methods. But Slice is a negative sampling method, so they should compare. The authors might have missed that, which weakens their experiments.\n\nThe writing issues (typos, grammar) are minor and fixable. The theoretical contribution is solid, but the experiments need more thorough comparisons. Since the authors committed to adding more experiments (EURLex and maybe others), but the rebuttal is limited, maybe the reviewers can be convinced if those are added. However, in the current submission, the experiments are incomplete. Top-tier conferences require strong experimental validation against relevant baselines. Without comparing to Slice and DiSMEC, even if they're different approaches, it's a gap. The authors' argument about focusing on negative sampling is valid, but Slice might be a better baseline in that category. The paper's claim of being faster by an order of magnitude needs to be validated against the best methods.\n\nThe reviewer 3's point about tail-label performance is also important. Extreme classification datasets often have skewed distributions, so metrics like MacroF1 are crucial. The paper only uses accuracy, which might be misleading. The authors didn't address this in the rebuttal, so that's another weakness.\n\nConsidering all this, the paper has strong theoretical contributions and a novel method, but the experiments are not yet sufficient for a top conference. The authors have a chance to improve by adding more baselines and metrics, but given the current submission, the experiments are lacking. However, since the authors have addressed some points in the rebuttal (adding references, running EURLex experiments), maybe the paper can be accepted with revisions. But top conferences are strict on thorough evaluations. If the missing comparisons are critical, rejection might be warranted. The reviewers' comments suggest that the paper is on the borderline. Reviewer 1 is positive, Reviewer 2 has minor issues, Reviewer 3 is more negative. The authors' response didn't fully address all concerns, especially regarding Slice and DiSMEC. Since the paper's main contribution is the adversarial sampling method, and the experiments don't compare against key baselines, it might not meet the high standards. Therefore, I think the decision leans towards reject, but the authors' rebuttal might push it to accept if they can fix the experiments. However, given the current information, the experiments are insufficient for acceptance at a top-tier conference.\n</think>\n\nThe paper proposes an adversarial sampling method for extreme classification, addressing the computational challenges of traditional softmax. The contributions include a novel adversarial sampling mechanism, a theoretical proof of gradient variance reduction, and experimental results showing significant training speed improvements. However, the review process highlights critical issues that must be considered for a top-tier conference:\n\n### Strengths:\n1. **Theoretical Contributions**: The paper provides a rigorous proof (Theorem 2) formalizing the intuition that adversarial sampling optimizes signal-to-noise ratio, which is a novel and valuable theoretical advancement.\n2. **Practical Impact**: The adversarial sampling approach reduces training time by an order of magnitude compared to baselines, offering a practical improvement for large-scale classification tasks.\n3. **Clarity and Accessibility**: The paper is praised for its clear exposition, making complex ideas accessible to non-experts, which is a strength for dissemination.\n\n### Weaknesses:\n1. **Incomplete Related Work**: The initial submission inadequately addressed prior work on non-uniform negative sampling (e.g., [1–4]). While the authors commit to adding references, the distinction between their approach and prior methods (e.g., sampled softmax vs. negative sampling) requires careful clarification to avoid overlap or redundancy.\n2. **Experimental Gaps**:\n   - **Missing Baselines**: Key state-of-the-art methods like *Slice* and *DiSMEC* are omitted, even though they are directly relevant to negative sampling and extreme classification. The authors argue their focus is on improving negative sampling, but failing to compare against these methods undermines the paper’s claims of superiority.\n   - **Lack of Multi-Label Evaluation**: The paper focuses on single-label classification but does not explore multi-label settings, which are common in extreme classification. This limits the method’s broader applicability.\n   - **Tail-Label Performance**: The experiments use accuracy but omit metrics like MacroF1, which are critical for evaluating performance on skewed or long-tailed datasets. This oversight risks obscuring weaknesses in handling tail classes.\n   - **Smaller Datasets**: While the authors mention adding experiments on EURLex, the current submission lacks comparisons to full softmax on smaller datasets, making it difficult to assess the approximation gap of negative sampling.\n\n3. **Theoretical vs. Practical Trade-offs**: The rebuttal emphasizes theoretical contributions but does not address whether the adversarial model’s static nature (unlike dynamic models in prior work) introduces limitations in adapting to evolving model distributions during training.\n\n### Author Response Analysis:\nThe authors acknowledge gaps in related work and commit to revisions, but their rebuttal does not fully address critical concerns:\n- They clarify differences with prior work (e.g., sampled softmax vs. negative sampling) but do not refute claims that methods like [3,4] also sample \"hard\" negatives.\n- They justify excluding non-negative-sampling methods (e.g., DiSMEC) but fail to explain why Slice (a negative-sampling-based method) is omitted, weakening the baseline comparisons.\n- The proposed addition of EURLex experiments is a step forward but insufficient to resolve broader concerns about missing baselines and metrics.\n\n### Final Assessment:\nWhile the paper’s theoretical contributions and adversarial sampling mechanism are innovative, the experimental evaluation is incomplete and lacks critical comparisons to state-of-the-art methods. The omission of Slice, DiSMEC, and multi-label/tail-label metrics undermines the paper’s claims of practical superiority. Top-tier conferences require rigorous validation against the strongest baselines and thorough exploration of methodological limitations. The authors’ rebuttal partially addresses these issues but does not resolve them sufficiently for acceptance at this stage.\n\n**Final Decision: Reject**"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Improved Training of Certifiably Robust Models,\" and the keywords are Convex Relaxation, Certified Robustness, Regularization. The abstract mentions that convex relaxations are effective but have a gap between certifiable and empirical robustness. The authors propose two regularizers to tighten these bounds, showing improvements over baselines in experiments.\n\nNow, looking at the reviewer comments. There are three reviewers. Let me parse each one.\n\nReviewer 1 has several points. They point out some inaccuracies in the paper, like the claim that IBP can't handle L2 adversaries, which the reviewer says is incorrect. They also note that the convex relaxation by Ehlers isn't optimal unless ReLUs are relaxed independently. The reviewer is confused about some sections, like Section 4 where they mix up the non-convex problem (O) with the convex relaxation (C). The reviewer also criticizes the experiments for not including Gowal et al.'s results, which are better than the paper's. They mention that the improvements might be due to CROWN-IBP rather than the proposed regularizers. The reviewer's opinion is that the analysis is confusing and the experiments not convincing enough, leading to a weak reject.\n\nReviewer 2's main issues are about the clarity of the theoretical sections. They question why the authors compare O and C, pointing out that O is the intractable problem. They're confused about whether the regularizer is better than optimal convex relaxations and why the approach should work. They also want more details on experiments, like whether baselines use the same relaxation and if the regularizer can be applied to optimal relaxations. The reviewer leans towards weak rejection due to unclear theoretical justification and experimental details.\n\nReviewer 3 thinks the problem is interesting but criticizes the experiments for using only MNIST, which they consider insufficient. They mention presentation issues and suggest adding more datasets. Their initial stance is a weak accept but with reservations about the experiments.\n\nThe author response addresses several points. They added experiments on CIFAR10, achieving SOTA certified accuracies. They clarified confusion between O and C, fixed typos, and addressed the IBP vs L2 issue. They also explain that their regularizers can be applied to other relaxations but are limited by computational costs. They added comparisons with Gowal et al. and others in Table 1, showing better results.\n\nNow, considering all this. The initial reviews had concerns about theoretical clarity, experimental baselines, and dataset choice. The authors addressed many of these in their response. They added CIFAR10 results, which should address Reviewer 3's dataset concern. They clarified the theoretical points, fixed typos, and provided more comparisons. The experiments now include stronger baselines and show SOTA results, which strengthens their case.\n\nHowever, Reviewer 2 still had issues with the theoretical justification and whether the regularizer's mechanism is sound. The authors explained that their regularizer doesn't necessarily eliminate ambiguous ReLUs but aims for tightness at the sample distribution, which is different from Xiao et al. They also clarified that their method's improvements are comparable to using tighter but computationally expensive relaxations.\n\nThe key points for a top-tier conference are: novel contribution, sound methodology, thorough experiments, and clear presentation. The authors have improved the paper by addressing many of the reviewers' concerns. The experiments now include better baselines and more datasets. The theoretical parts were clarified, though some complexity remains. The main weakness might be that the improvement over baselines is marginal or that the method's effectiveness is dataset-dependent, but achieving SOTA on CIFAR10 is a strong point.\n\nGiven that the authors have made significant revisions, added experiments, and addressed most of the technical concerns, I think the paper should be accepted. The improvements in certified accuracy and the thorough revisions likely meet the conference's standards.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\n1. **Theoretical Contributions and Clarity**: \n   - The authors addressed key concerns raised by Reviewer 1 and 2 regarding the confusion between the non-convex problem (O) and its convex relaxation (C). They clarified the distinction in the revised version, fixed typos, and added an illustrative example (Appendix A) to demonstrate scenarios where their regularizers achieve tight bounds even with ambiguous ReLUs. This reduces ambiguity in the theoretical analysis.\n   - They also clarified that their regularizers are orthogonal to tighter convex relaxations (e.g., LP-All) and explained why their approach improves the tightness of existing relaxations like Fast-Lin and CROWN-IBP. While the theoretical justification remains complex, the revisions make the core ideas more accessible.\n\n2. **Experimental Rigor**:\n   - The authors significantly strengthened their experiments by adding results on CIFAR10 and comparing against stronger baselines (e.g., IBP, CROWN-IBP). The new results show state-of-the-art certified accuracies on both MNIST and CIFAR10, which directly address Reviewer 1’s criticism about insufficient baselines and weak experimental validation.\n   - They also resolved the MNIST-only limitation highlighted by Reviewer 3 by including CIFAR10 experiments, demonstrating broader applicability.\n\n3. **Addressing Reviewer Concerns**:\n   - **Reviewer 1**: The authors added comparisons with Gowal et al. in Table 1, clarified the IBP-L2 issue via Appendix E, and fixed inaccuracies (e.g., Ehlers’ relaxation description). The revised analysis in Proposition 1 and Appendix A now better justify the conditions under which their regularizers work.\n   - **Reviewer 2**: The authors clarified the relationship between their regularizers and existing methods (e.g., Xiao et al.), explained why minimizing the gap \\(d\\) does not contradict robust training goals, and addressed the feasibility of applying their regularizers to tighter relaxations. While some theoretical nuances remain, the revisions sufficiently address the core confusion.\n   - **Reviewer 3**: The addition of CIFAR10 experiments and improved presentation (e.g., fixing typos) directly address their concerns about dataset choice and clarity.\n\n4. **Novelty and Impact**:\n   - The proposed regularizers offer a novel approach to improving certified robustness by explicitly targeting the gap between convex relaxations and the true robustness. The empirical results on CIFAR10, particularly under realistic \\(\\epsilon\\) values (e.g., \\(8/255\\)), suggest practical utility for top-tier conferences.\n\n5. **Remaining Weaknesses**:\n   - The theoretical analysis remains dense and could benefit from further simplification, but this is common in technically rigorous work. The empirical improvements, especially on CIFAR10, offset this.\n   - The method’s reliance on Fast-Lin/CROWN-IBP (which are computationally intensive) might limit scalability, but this is a known challenge in certified robustness and does not disqualify the contribution.\n\n### Conclusion:\nThe revisions have sufficiently addressed the reviewers’ concerns, strengthened the experiments, and clarified the theoretical contributions. The paper now presents a compelling case for improving certified robustness with novel regularizers and strong empirical validation. It meets the standards of a top-tier conference."
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about out-of-distribution (OOD) detection using the Normalized Compression Distance (NCD) with a method called MALCOM. The key constraints are that it doesn't use OOD samples for validation and doesn't retrain the model. The reviewers have mixed feedback, so I need to weigh their points carefully.\n\nStarting with the strengths. The problem is important for AI safety, especially in real-world applications where models might encounter unseen data. The method's novelty is in combining global average pooling with spatial patterns via NCD, which seems clever. The authors addressed some reviewer concerns by adding experiments and ablation studies in the appendix, showing that their method performs well even when using adversarial samples for validation. They also fixed some errors and clarified their approach.\n\nNow the weaknesses. Reviewer 1 pointed out that the original comparisons were unfair because the replication of prior methods had lower performance than reported. The authors added the original numbers and their replication results, which is good, but there's still a concern that the performance gains might stem from incorrect implementations of prior work. Also, the Mahalanobis method's vanilla version wasn't compared under the right conditions—specifically using adversarial samples for validation, which the constraints allow. The authors did add MALCOM++ with weighted averaging using adversarial validation, which helps, but the main method's effectiveness without that tweak is questioned.\n\nReviewer 2 noted the restrictive constraints aren't well-justified. The authors argued that using OOD for validation assumes prior knowledge of the test distribution, which is a valid point. But the reviewer still wants clearer justification on why both constraints (no OOD validation and no retraining) are necessary simultaneously. The authors addressed this by adding experiments without the constraints, showing their method still performs well, which helps.\n\nReviewer 3 wanted more complex experiments and theoretical guarantees. The authors mentioned future work on safety-critical scenarios but didn't add those experiments. The method's practicality is a plus, but the lack of theoretical backing might be a downside.\n\nLooking at the rebuttal, the authors made several improvements: added tables in appendices, fixed errors, and clarified their approach. They also showed that MALCOM++ outperforms others when adversarial samples are used, which aligns with the constraints. However, the main method's performance without these tweaks is mixed, and there's still a question about whether the prior work was fairly evaluated.\n\nThe key issues are the comparison fairness and the necessity of the constraints. The authors addressed the first by including original vs. replicated results and the second by justifying their constraints better. But Reviewer 1's point about the Mahalanobis method needing adversarial validation still stands. The authors did compare against that in the appendix, which is good, but the main paper's tables might still be lacking.\n\nOverall, the method is novel and addresses a practical problem. The experiments, though initially flawed, were improved with additional data. The constraints are justified, and the authors made efforts to address reviewer concerns. However, the main paper's presentation of comparisons was problematic, and there's lingering doubt about prior work replication. Since top-tier conferences require high standards, these issues might tip the decision. But the improvements in the rebuttal and the method's potential could push it towards acceptance with revisions.\n\nWait, but the reviewers' decisions were leaning towards weak reject. Reviewer 1 said weak reject, others were borderline. The authors did a lot to address points, but maybe not enough for acceptance. The main paper's tables still have issues, and the core method's effectiveness without the extra tweaks is questioned. Maybe the paper needs more polishing and better presentation of comparisons in the main text, not just appendices. Since top conferences are strict, perhaps it's better to reject now but encourage resubmission after more thorough revisions.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\nThe paper proposes MALCOM, an out-of-distribution (OOD) detection method that avoids using OOD samples for validation and retraining, addressing a practical yet niche problem in AI safety. While the approach is novel and the authors made commendable efforts to address reviewer concerns (e.g., adding experiments with adversarial validation, fixing errors, and clarifying constraints), several critical issues remain unresolved:\n\n1. **Comparison Fairness and Replication Concerns**:  \n   - Reviewer 1 highlighted that the replication of prior methods (e.g., Mahalanobis) yielded significantly worse results than reported in the original papers. While the authors added tables comparing their results with original numbers, the disparity raises concerns about potential implementation flaws or unfair comparisons. This undermines confidence in the claimed performance gains of MALCOM.  \n   - The Mahalanobis method’s vanilla version was not fairly compared under the problem constraints. The authors addressed this in the appendix by introducing MALCOM++ (with adversarial validation), but the main paper’s comparisons still lack clarity, leaving the core method’s standalone effectiveness in doubt.\n\n2. **Justification of Constraints**:  \n   - Reviewer 2 questioned the necessity of imposing *both* constraints (no OOD validation and no retraining) simultaneously. While the authors provided a rationale (e.g., avoiding assumptions about test distributions and preserving in-distribution performance), the practical scenarios where *both* constraints apply are not sufficiently justified. The added experiments without constraints (e.g., MALCOM++ with adversarial validation) suggest that relaxing constraints could improve performance, weakening the paper’s focus on strict constraints.\n\n3. **Methodological Limitations**:  \n   - The proposed method’s effectiveness hinges on combining global average pooling with NCD. However, Table 2 shows that MALCOM only outperforms baselines when all components (e.g., concatenation of multiple layers) are applied. This raises questions about the core innovation’s standalone value.  \n   - The lack of theoretical guarantees or analysis of why NCD captures spatial patterns better than existing methods limits the paper’s depth.\n\n4. **Presentation and Reproducibility**:  \n   - Key results (e.g., comparisons with adversarial validation, MALCOM++) are relegated to appendices, making the main paper’s claims appear under-supported.  \n   - While code was provided, the authors admit it \"needs refactoring,\" raising reproducibility concerns.\n\n### Conclusion:\nWhile MALCOM addresses an important problem and shows promise in experiments, unresolved issues with fair comparisons, justification of constraints, and presentation standards prevent it from meeting the high bar of a top-tier conference. The authors should refine their experiments, clarify their constraints’ practical relevance, and ensure all critical results are in the main paper before resubmission."
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Effect of Activation Functions on the Training of Overparametrized Neural Nets.\" The abstract mentions that they analyze how activation functions affect training, especially in overparametrized networks. They compare smooth vs. non-smooth activations, showing that non-smooth ones (like ReLU) have better eigenvalues in the Gram matrix, leading to faster training. For smooth activations, they find that the minimum eigenvalue can be small if data dimension is low, but depth helps. They also did experiments on synthetic and CIFAR10 data.\n\nLooking at the reviewer comments. Reviewer 1 says the paper is a good theoretical analysis, provides proofs in the appendix, and has experiments that align with theory. The weaknesses are the long appendix and some organizational issues. They also pointed out some minor typos and terminology questions, which the authors addressed in their response.\n\nReviewer 2 is more positive, voting to accept. They appreciate the thorough theoretical work, especially on the eigenvalues and how smooth vs non-smooth activations differ. They suggest some improvements in organizing the appendix and clarifying theorem comparisons. The authors responded by reorganizing the appendix with a table of contents and addressing the theorem notation issues.\n\nAuthor responses seem to have addressed the reviewers' concerns. They fixed the DZXP typo, clarified assumptions, rephrased awkward sentences, and organized the appendix better. They also provided more details on the theorem comparisons as requested.\n\nNow, considering the standards of a top-tier conference. The paper's contributions are theoretical, which is a strength in this area. They address a gap in understanding activation functions' impact on training dynamics, especially in overparametrized settings. The results are significant, distinguishing between smooth and non-smooth activations, which is a key practical consideration. The experiments support the theory, even if there's a gap on CIFAR10, which the reviewers saw as a strength for transparency.\n\nThe main criticisms were about the paper's length and organization. The authors have worked on these by reorganizing the appendix and adding a table of contents. Since top conferences value both theoretical depth and clarity, the revisions should mitigate these issues. The fact that Reviewer 2, who is more focused on the technical content, is in favor, and Reviewer 1's concerns were addressed, suggests the paper meets the standards.\n\nPotential issues: The paper's length might still be a problem, but the authors have made it navigable. The experiments on CIFAR10 having some gaps are okay as they're discussed, showing honesty. The theoretical contributions are solid and novel, building on prior work but extending it meaningfully. The extensions in the appendix, while extensive, are part of a thorough analysis, which is expected in top-tier work.\n\nSo, the paper seems to have strong theoretical results, addresses important questions, and the authors have adequately responded to concerns. Therefore, it should be accepted.\n</think>\n\nThe paper presents a theoretically rigorous analysis of how activation functions influence the training dynamics of overparametrized neural networks, particularly focusing on the role of smoothness in activation functions. The key contributions include distinguishing between non-smooth and smooth activations by analyzing the minimum eigenvalue of the Gram matrix (G-matrix), which directly impacts convergence rates. For non-smooth activations like ReLU, the results show that eigenvalues remain large under minimal assumptions, ensuring faster training. In contrast, smooth activations (e.g., tanh, swish) can exhibit small eigenvalues if data dimensions are low, though depth mitigates this issue. The empirical experiments align with theoretical findings, even when noting discrepancies in real-world datasets like CIFAR10, which adds transparency.\n\n**Strengths**:  \n1. **Theoretical Depth**: The paper advances the theoretical understanding of activation functions in overparametrized networks, addressing a critical gap in the literature. The distinction between smooth and non-smooth activations provides actionable insights for practitioners.  \n2. **Technical Soundness**: The proofs in the appendix, though lengthy, are thorough and address extensions (e.g., deep networks, SGD). The authors clarify and reorganize these sections in response to reviewer feedback, improving accessibility.  \n3. **Empirical Validation**: Experiments on synthetic and real data (CIFAR10) support the theory, and the discussion of gaps (e.g., on CIFAR10) demonstrates scientific rigor.  \n4. **Clarity and Organization**: While the original submission had a lengthy appendix, the revised version includes a table of contents and restructured sections, addressing navigability concerns. Minor typos and terminology issues were also fixed.  \n\n**Weaknesses**:  \n1. **Appendix Length**: Despite improvements, the appendix remains extensive. However, the authors argue this is unavoidable given the scope of the analysis, and the reorganization mitigates readability issues.  \n2. **Practical Relevance of Synthetic Experiments**: While synthetic experiments validate theory, the real-world gap on CIFAR10 highlights potential limitations of the theoretical framework in complex settings. However, this is acknowledged and does not undermine the core contributions.  \n\n**Reviewer Feedback**:  \n- **Reviewer 1** raised concerns about length and organization but acknowledged the paper’s strengths. The authors addressed these by reorganizing the appendix and clarifying proofs.  \n- **Reviewer 2** strongly endorsed the paper, praising its thoroughness and theoretical contributions. Their suggestions (e.g., theorem comparisons) were addressed in the revised version.  \n\n**Conclusion**: The paper meets the standards of a top-tier conference. It offers novel, impactful theoretical insights into a fundamental problem in deep learning, with solid empirical support. The authors’ revisions effectively address reviewer concerns, ensuring clarity and navigability. While the appendix’s length is a minor drawback, it is justified by the depth of the analysis. The work’s significance in advancing the theoretical understanding of activation functions justifies acceptance.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills.\" The abstract mentions they're using a weakly-supervised approach inspired by multiple instance learning (MIL) to segment trajectories into skills without needing segmentation or ordering info. They evaluate on four environments and show some zero-shot transfer. \n\nLooking at the reviewer comments first. Reviewer 1 says the paper is misleading in framing, limited in scope, and needs more experiments and baselines. They point out that the method provides per-timestep labels but doesn't show how these can be used for reusable skills. They also mention unclear experimental details and possible overfitting. The author response addresses some points, like the lack of permutation info compared to Shiarlis et al., and mentions that Gaussian smoothing is used but not detailed. They also argue that classification accuracy might not be the best metric because of ambiguous steps.\n\nReviewer 2 thinks the approach is a good application of MIL but the novelty is low since log-sum-exp pooling isn't new. The results are underwhelming with ~35-60% accuracy, and they want comparisons to fully-supervised baselines and more thorough behavior cloning baselines like Policy Sketches. The authors say they'll add the oracle baseline and explain that CCNN failed because it collapsed to one class. \n\nReviewer 3 is concerned about dataset requirements and theoretical bounds. They want confusion matrices and qualitative results to show the method actually works. They also mention the dial jaco video's smooth predictions might be due to smoothing. The authors respond by saying the problem is ill-posed and that some ambiguity is inherent, and they'll add clustering comparisons.\n\nNow, considering the author responses. They address some points but not all. For example, they explain why their problem is harder than Shiarlis (no permutation), but the reviewers still question the practical utility. The lack of clear experiments showing the skills can be reused (like training policies from the segmented data) is a big issue. The authors mention that they assume downstream tasks can use new sequences, but they didn't present experiments where they actually do that beyond the zero-shot transfer mentioned, which isn't detailed.\n\nThe main issues from the reviewers are:\n1. The method's practical utility isn't demonstrated well. They don't show that the segmented skills can be used effectively for new tasks, which is a key part of the motivation.\n2. The results are underwhelming (low accuracy) and lack proper baselines (like fully-supervised or unsupervised clustering).\n3. Experimental details are missing (architecture, hyperparameters, how smoothing is applied), making it hard to replicate or assess.\n4. The problem's limitations aren't addressed theoretically, like what conditions are needed for the method to work.\n\nThe authors tried to address some points but didn't provide new experiments in their response. They mention adding some baselines in the final version, but since this is the review stage, the current submission doesn't have them. The core issue is that the paper's claims about learning reusable skills aren't backed by sufficient evidence. The reviewers are right that without showing that the segmented skills can actually be used to build policies for new tasks, the contribution is weak.\n\nAlso, the low accuracy and lack of comparison to fully-supervised methods make it hard to judge the method's effectiveness. The authors argue that the problem is inherently ambiguous, but even so, the results should be better if the method is viable. The fact that CCNN failed in some cases (like predicting only one class) suggests instability in the approach.\n\nConsidering top-tier standards, the paper needs stronger empirical validation, better baseline comparisons, and clearer demonstration of the method's utility. The current submission falls short in these areas. The reviewers' concerns are valid and not fully addressed, so I think it should be rejected. However, if the authors can significantly improve these aspects in a revision, maybe, but given the current state, rejection is appropriate.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Lack of Clear Utility for Reusable Skills**: \n   - The paper frames itself as enabling the learning of *reusable* skills, but the experiments do not demonstrate this. While the method produces per-timestep skill labels, there is no evidence that these labels can be used to train policies for downstream tasks (e.g., behavior cloning conditioned on skills or hierarchical control). The authors mention \"preliminary evidence of zero-shot transfer,\" but this is not elaborated in the results. Without concrete experiments showing skill reusability (e.g., training policies from segmented data and testing on novel task sequences), the core motivation remains unproven.\n\n2. **Underwhelming Results and Baseline Comparisons**:\n   - Classification accuracies of ~35–60% are low, and the authors do not compare to fully-supervised baselines to contextualize the performance gap. Reviewer 2 notes that even log-sum-exp pooling is not novel, and the results are underwhelming for a top-tier conference. The absence of comparisons to unsupervised clustering methods (as requested by Reviewer 3) further weakens the contribution.\n\n3. **Ambiguous Experimental Setup and Analysis**:\n   - Key details (e.g., architecture, hyperparameters, Gaussian smoothing parameters) are omitted, making replication difficult. The lack of confusion matrices, precision/recall metrics, and qualitative trajectory visualizations (e.g., as in Bolanos or Ranchod) undermines confidence in the results. The authors’ defense that \"ambiguity is inherent\" does not excuse the lack of thorough analysis.\n\n4. **Theoretical and Practical Limitations**:\n   - The problem’s ill-posedness (e.g., inability to distinguish skills that always co-occur) is acknowledged but not theoretically bounded. The authors argue that their approach is \"unconstrained,\" but this risks overpromising on applicability. The dial jaco video’s smooth predictions (due to post-processing) highlight potential overfitting or lack of temporal coherence, which is not rigorously addressed.\n\n5. **Author Responses Fall Short**:\n   - While the authors address some points (e.g., CCNN failure, dataset requirements), they do not provide new experiments or data to resolve critical concerns. The claim that \"classification accuracy is not a good metric\" is unconvincing without alternative metrics or demonstrations of practical utility.\n\n### Conclusion:\nThe paper’s contributions are incremental, its results underwhelming, and its experiments insufficient to justify acceptance at a top-tier conference. Addressing these issues would require substantial revisions beyond the current submission, making rejection the appropriate decision."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning DNA folding patterns with Recurrent Neural Networks.\" The keywords include machine learning, RNNs, 3D chromatin structure, TADs, and computational biology. The abstract mentions using ML methods like linear models, gradient boosting, and RNNs (specifically bidirectional LSTM) to predict chromatin folding patterns from epigenetic marks. They claim the LSTM outperforms others and highlights the importance of sequential DNA states. They also identify informative features.\n\nNow, looking at the reviewer comments. Reviewer 1 points out that the improvement might be due to the loss function (weighted MSE) rather than the LSTM itself. They suggest comparing with a vanilla LSTM and testing generalizability. Reviewer 2 has several minor issues like unclear equations, formatting, and questions about why regression vs. NNs. They also ask about negative training errors. Reviewer 3 is more critical, saying the work lacks novelty, unclear descriptions, and insufficient evaluation. They mention similar published works not cited, unclear features, unclear motivation for biLSTM, and methodological issues like loss function use across models.\n\nThe authors responded to these points. They clarified the loss function (wMSE), addressed some of the technical issues, added more metrics, and tried to justify their choices. But let's see if these responses adequately address the concerns.\n\nStarting with Reviewer 3's major points. The first is lack of novelty. The authors say they didn't find prior work predicting TAD characteristics from ChIP-seq data, but Reviewer 3 points out that similar works exist but aren't cited. The authors mention some were workshop papers or not detailed. However, in a top-tier conference, thorough literature review is crucial. If there are existing methods that do similar things, especially in workshops, the paper might not be novel enough unless they clearly distinguish their contributions. The authors added some citations but might not have sufficiently addressed this.\n\nSecond, data and methods description. Reviewer 3 asks about input features: were they binary? The authors clarify they used float values from ChIP-Seq integrated over 20kb bins. That's better, but maybe still unclear how exactly features were processed. Also, the motivation for biLSTM: the authors say DNA is double-stranded, so bidirectional makes sense. But Reviewer 3 questions if the DNA sequence is even input here. The paper says they don't use the DNA sequence but epigenetic features. So the biLSTM's bidirectional aspect might not directly relate to DNA's double-strandedness. The authors' explanation might be a stretch here.\n\nThird, evaluation. Reviewer 3 says they should use same loss for all models. The authors say they did use wMSE for all, which addresses that. They added more metrics like MSE, MAE, R². That's good. But the main issue was whether the LSTM's performance is due to the model or the loss. Since they used the same loss, that's better, but the original concern was comparing to a vanilla LSTM without the modified loss. Wait, the authors mention they used the same loss for all models, so the comparison is fair now. But the initial concern was about whether the improvement is from the loss or the model. Since they used the same loss, the model's architecture (LSTM vs others) is the differentiator. So that's addressed.\n\nReviewer 1's point about generalizability: the authors say they didn't test on other datasets or ATAC-seq because the biological mechanisms might differ. That's a valid point, but top-tier conferences often expect some evidence of generalizability, even if limited. Maybe they could have tested on another organism or dataset, but the authors have a reason. However, the lack of exploration here might be a weakness.\n\nReviewer 2's comments: the equation clarity was fixed, and they added more discussion on why NNs over regression. The negative training errors in Figure 6 are explained, which is good. The other minor issues were addressed.\n\nNow, considering the overall strengths and weaknesses. The paper presents a novel application of bidirectional LSTM for predicting TAD characteristics from ChIP-Seq data, which the authors claim hasn't been done before. The LSTM does outperform other models, and they provide some biological insights. However, the methodological concerns are significant. The lack of clear novelty compared to existing work (even if in workshops) is a red flag. The motivation for using biLSTM might be weak if the input isn't the DNA sequence but features. The evaluation could be stronger with more baselines or cross-dataset tests. The authors addressed some points but maybe not all.\n\nTop-tier conferences require high novelty, thorough methodology, and strong evaluation. The paper's contributions seem incremental unless the application is truly novel. If similar approaches exist, especially in recent workshops, that undermines its originality. The authors didn't sufficiently address the literature gap, which is a problem. The technical aspects like feature processing and model justification need clearer explanations. The responses from authors improved some parts but didn't resolve all issues.\n\nIn conclusion, while the paper has potential, the lack of sufficient novelty, unclear methodology in some areas, and insufficient evaluation compared to top standards lead me to think it should be rejected. The authors might need to do more work on literature review, clarify their approach, and provide stronger validation before it's suitable for a top conference.\n</think>\n\nThe paper proposes using bidirectional LSTM RNNs to predict chromatin folding patterns (specifically TAD characteristics) from epigenetic features (ChIP-Seq data) in Drosophila. While the work presents an interesting application of machine learning to a biological problem and demonstrates that LSTMs outperform baseline models, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novel Application**: The paper claims to be the first to predict TAD characteristics (e.g., gamma transitional) directly from ChIP-Seq data without using Hi-C maps as input, which could provide new biological insights.\n2. **Technical Contribution**: The bidirectional LSTM architecture is justified as leveraging sequential dependencies in epigenetic features, and the weighted MSE loss function is tailored to the problem.\n3. **Evaluation**: The authors added more metrics (MSE, MAE, R²) and clarified comparisons across models after revisions, addressing some reviewer concerns.\n\n### Major Concerns:\n1. **Lack of Methodological Novelty**:\n   - Reviewer 3 highlights that similar approaches exist in prior work (e.g., workshop papers), which the authors downplay but do not adequately distinguish from their own. The paper’s contribution appears incremental unless the specific application (predicting TADs from ChIP-Seq alone) is truly novel, which is unclear.\n   - The motivation for using a bidirectional LSTM is weak. The authors argue it reflects DNA’s double-stranded nature, but the input features are aggregated over 20kb bins and do not involve sequence data. This disconnect weakens the justification for the architecture choice.\n\n2. **Unclear Methodology and Justification**:\n   - The description of input features (e.g., how ChIP-Seq signals are aggregated into 20kb bins) and model hyperparameters (e.g., tuning details) remains insufficiently clear, despite revisions.\n   - The weighted MSE loss function’s design (e.g., the arbitrary \"11\" parameter) is poorly motivated and lacks generalizability. The authors’ explanation (related to the target variable’s distribution) is ad hoc and not rigorously justified.\n\n3. **Insufficient Evaluation**:\n   - The paper does not test generalizability to other organisms, cell types, or datasets (e.g., ATAC-Seq), limiting its broader impact. The authors justify this by citing biological differences but fail to explore even basic cross-validation or ablation studies.\n   - While the authors addressed loss function comparisons, they do not clarify why the bidirectional LSTM’s central hidden state was chosen over concatenating forward/backward states, as noted by Reviewer 3.\n\n4. **Literature Gaps**:\n   - The literature review is incomplete. The authors dismiss prior workshop papers but do not engage with their methods or results, weakening the paper’s novelty claims. Including and contrasting with these works is critical for a top-tier venue.\n\n### Author Responses:\nThe authors addressed several minor issues (e.g., equation clarity, formatting) and added metrics, but failed to resolve core concerns:\n- They did not sufficiently address the lack of methodological novelty or clarify the bidirectional LSTM’s relevance to the problem.\n- The loss function’s design remains poorly justified, and the biological rationale for excluding other datasets remains unconvincing.\n\n### Conclusion:\nWhile the paper demonstrates technical competence and a promising direction, its incremental contributions, unclear methodology, and insufficient evaluation relative to top-tier standards outweigh its strengths. The lack of rigorous novelty and generalizability testing, combined with unresolved methodological ambiguities, make it unsuitable for acceptance at a prestigious conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about showing that SGD converges to a global solution in WGANs when the generator is a one-layer network. The reviewers have some concerns, so I need to weigh the contributions against the criticisms.\n\nFirst, the main contribution seems to be the first result on global convergence for non-linear generators in GANs using SGD. That's a big deal because GANs are notoriously hard to analyze, especially with non-convex min-max problems. The authors use quadratic discriminators, which simplify the dynamics. They argue that even though quadratic discriminators are simple, they have enough power for their setup and keep sample complexity low. They reference prior work saying that more complex discriminators might increase sample complexity, which is a valid point.\n\nBut the reviewers are worried that the discriminators (linear and quadratic) are too simplistic compared to real WGANs, which use neural networks. Reviewer 1 points out that quadratic discriminators don't fit the standard WGAN framework because the Lipschitz constraint isn't properly handled unless data is bounded, which they say isn't the case here. The authors mention that in their setup, the data is generated from a teacher network with Gaussian inputs, so maybe it's bounded? Not sure if that's addressed properly.\n\nReviewer 2 and 3 also mention that the discriminators don't capture the essence of WGANs, which rely on Lipschitz neural networks. The authors counter by saying their goal is minimal discriminator requirements and that quadratic discriminators suffice for their generator. They also mention that using more complex discriminators would complicate things, but maybe that's a limitation.\n\nAnother point is the terminology: \"one-layer network\" might be confusing. The authors clarify it's a single hidden layer, but the title could be misleading. However, they reference prior work using the same term, so maybe that's acceptable.\n\nThe theoretical analysis seems solid, with sample complexity matching known bounds. The experiments support their claims. However, the practical relevance is questioned because real WGANs use different discriminators. The authors argue their work is a foundational step, showing convergence in a simplified setting, which is still a novel contribution.\n\nThe reviewers' main issues are the discriminators being too simple and the title possibly overreaching. But the authors defend their choices, emphasizing that their setup is intentional to achieve the first global convergence result. Since top-tier conferences value novel theoretical contributions even if applied to simplified models, this might tip the balance.\n\nFinal call: The paper provides a first-of-its-kind result on global convergence for non-linear GANs, which is significant. The limitations are acknowledged, but the contributions are substantial. So, I think it should be accepted with the understanding that the authors clarify the Lipschitz constraint and terminology in the revision.\n</think>\n\nThe paper presents a theoretical analysis of the convergence of stochastic gradient descent-ascent (SGD-ASG) in Wasserstein GANs (WGANs) when the generator is a one-layer network. The key contributions are: (1) the first result showing global convergence to equilibrium points for non-linear generators in GANs, and (2) demonstrating that quadratic discriminators achieve sufficient distinguishing power while maintaining manageable sample complexity. However, the reviewers raise critical concerns about the practical relevance of the simplified discriminators (linear/quadratic) and the potential overstatement of contributions. Here’s the breakdown:\n\n### Strengths:\n1. **Novelty and Theoretical Contribution**: The paper provides the first global convergence result for non-linear generators in GANs using SGD-ASG. This is a significant theoretical advance in understanding GAN dynamics, particularly for non-convex min-max problems.\n2. **Sample Complexity**: The analysis matches known parametric rates (e.g., \\(O(1/\\epsilon^2)\\)), which is non-trivial for GANs and supports the claim that WGAN can learn one-layer generators efficiently with appropriate discriminators.\n3. **Technical Soundness**: The proofs and experiments appear valid, and the authors address reviewer concerns by justifying their choice of discriminators (e.g., quadratic discriminators simplify dynamics and avoid non-parametric sample complexity).\n\n### Weaknesses:\n1. **Limited Practical Relevance**: The discriminators (linear/quadratic) are far simpler than those used in practical WGANs (e.g., neural networks). Reviewers argue this deviates from the core idea of WGANs, which rely on Lipschitz neural networks to approximate the Wasserstein distance. The authors counter that their goal is to minimize discriminator complexity while achieving sufficient distinguishing power, but this limits the work’s applicability to real-world scenarios.\n2. **Terminology and Title Ambiguity**: The term \"one-layer network\" could mislead readers into thinking it refers to a single-hidden-layer network (common in practice), whereas the paper’s generator is effectively a linear model with a fixed nonlinearity (e.g., \\(\\phi(Az)\\)). The title’s claim that SGD \"learns one-layer networks in WGANs\" might overstate the contribution given the simplified setup.\n3. **Lipschitz Constraint Justification**: Reviewer 1 questions whether quadratic discriminators satisfy the Lipschitz constraint required for WGANs, especially for unbounded data. The authors’ response hinges on the data being generated from a Gaussian (hence bounded in practice), but this is not explicitly clarified in the paper, leaving room for doubt.\n\n### Balancing Contributions and Limitations:\nWhile the reviewers criticize the discriminators as overly simplistic and the title as potentially misleading, the paper’s core theoretical contribution—the first global convergence result for non-linear generators—is undeniably novel and impactful for the GAN theory community. The authors’ defense of their design choices (e.g., quadratic discriminators for tractability and sample efficiency) is reasonable, especially given the difficulty of analyzing neural network discriminators in min-max settings. The limitations are acknowledged but framed as intentional trade-offs to achieve the first such result. \n\nTop-tier conferences often prioritize foundational theoretical advances even if applied to simplified models, provided the contributions are clearly stated and the limitations are transparent. The authors have addressed reviewer concerns by clarifying their goals and justifying their setup, which mitigates some of the criticism. While the work does not fully explain practical WGAN training, it lays critical groundwork for future extensions to more complex architectures.\n\n### Final Decision: Accept"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem.\" The keywords include depth-width trade-offs, ReLU networks, chaos theory, and Sharkovsky's Theorem. The abstract mentions that they're connecting the representational power of deep neural networks with Sharkovsky's Theorem from dynamical systems, specifically looking at periodic points and how that affects depth and width requirements. They reference Telgarsky's earlier work on depth separation, where deep networks can represent certain functions (like triangle waves) with exponentially fewer nodes than shallow ones. The authors here are trying to characterize why those functions are hard to represent and provide lower bounds based on periodic points and dynamical systems analysis.\n\nLooking at the reviewer comments:\n\nReviewer 1 is very positive, even saying they can't raise their score higher because it's already maxed. They mention the synthetic dataset was added, which is good. \n\nReviewer 2 is also very positive, calling the paper a delight to read. They appreciate the connection to Sharkovsky's theorem and the clean proofs. They do ask two questions: one about bias terms affecting lower bounds and another about empirical intuition via a plot of classification error vs depth. The author response addresses both, mentioning experiments added in the new version with figures and code. \n\nReviewer 3 is supportive but has some structural concerns. They think the paper starts with too much theory before getting to the main result. They suggest moving Theorem 4.1 earlier. Also, they note that while the theoretical result is nice, the period-dependent bound isn't very practical because it's hard to assess the period of a given function. The authors addressed the structure by reorganizing the paper, putting contributions first. They also acknowledge the difficulty in assessing periods but point out that their work provides a certificate (like NP problems) where if you have the certificate (periodic points), you can verify, but finding it is hard. They also mention examples from physics where such dynamics might be expected.\n\nAuthor responses seem thorough. They restructured the paper per Reviewer 3's suggestion, added examples for definitions, and included experiments as per Reviewer 2's request. They also addressed the bias question with an example showing brittleness.\n\nNow, considering top-tier conference standards. The paper is theoretical, but it's making a novel connection between neural network expressivity and a theorem from dynamical systems (Sharkovsky's). The problem they're addressing is a known open question from Telgarsky, so that's a plus. The proofs are considered correct by Reviewer 2, and the exposition was improved. The experiments added provide empirical backing, which is good for a theory paper to have some grounding.\n\nThe main criticisms are about structure and practical applicability. Reviewer 3's point about the period being hard to assess in practice is valid, but the authors explain that their work is more about theoretical understanding rather than immediate application. Since it's a top conference, theoretical contributions are valued, especially when they answer open questions and provide new insights. The structural issues were addressed in the author response, so that's mitigated.\n\nThe fact that all reviewers are positive, with only minor concerns that were addressed, suggests the paper is strong. The theoretical contribution is solid, the connection to Sharkovsky's theorem is novel, and they've added empirical support. The open question from Telgarsky is being addressed here, which is a significant contribution.\n\nI don't see any red flags. The experiments, though synthetic, help bridge theory and practice as per Reviewer 2's request. The brittleness example with bias terms shows they considered extensions. The reorganization of the paper should make it more accessible. \n\nTherefore, this seems like a solid candidate for acceptance. The theoretical depth, novel connection, and addressing an open problem are key strengths. The minor structural issues are resolved, and the practical limitations are acknowledged but framed appropriately as theoretical contributions.\n</think>\n\nFinal Decision: Accept\n\nThe paper presents a novel and theoretically rigorous contribution to understanding depth-width trade-offs in ReLU networks by leveraging Sharkovsky's Theorem from dynamical systems. It addresses an open question posed by Telgarsky regarding the characterization of functions that are difficult for shallow networks, providing a clear connection between periodic points and representational complexity. The theoretical framework is sound, with proofs deemed correct by reviewers, and the work advances foundational knowledge in neural network expressivity. \n\nKey strengths include:\n1. **Novelty and Impact**: The application of Sharkovsky's Theorem to neural networks is innovative and bridges dynamical systems theory with deep learning, offering new insights into why depth matters for certain functions.\n2. **Theoretical Rigor**: The paper provides clean, general lower bounds for width as a function of depth, grounded in eigenvalue analysis of dynamical systems. It directly engages with and extends Telgarsky's seminal work.\n3. **Addressing Reviewer Concerns**: The authors restructured the paper to prioritize contributions over theory, added empirical experiments (classification error vs. depth plots), and clarified definitions, effectively addressing structural and practicality critiques.\n4. **Empirical Validation**: Synthetic experiments and code inclusion strengthen the paper’s credibility, aligning theoretical results with empirical observations.\n\nMinor limitations, such as the difficulty of assessing periods for real-world functions, are acknowledged but framed as inherent challenges in theoretical work. The authors appropriately position their contribution as advancing theoretical understanding rather than immediate practical tools. Given the high standards of top-tier conferences, the paper’s theoretical depth, novel connections, and resolution of an open problem justify acceptance."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification.\" The abstract mentions a modular framework for hierarchical reinforcement learning (HRL) where sub-skills are trained individually with diversity, then coordinated by a meta-policy. The tasks involve bimanual manipulation and multi-agent scenarios like two robots pushing a box. The authors claim their method outperforms baselines.\n\nLooking at the reviewer comments first. There are three reviewers, each with their own points. Let's break them down.\n\nReviewer 1 is generally positive but has several concerns. They mention notation inconsistencies, the choice of T_low=1 making skills seem like primitive actions, high variance in results, unclear success rates vs. episode rewards, and lack of discussion on recent multi-agent methods. The author response addressed these by adding more baselines, clarifying T_low's role, and explaining the success rate plots. They also mentioned adding more seeds to reduce variance. The reviewer seems satisfied if concerns are addressed, which the authors did.\n\nReviewer 2 is more critical. They question the novelty, especially regarding whether temporal abstraction (TA) in multi-agent is different from single-agent. They also ask why DIAYN was chosen over other methods and point out some technical issues like the latent variable learning. The authors clarified that their approach is orthogonal to skill discovery methods and that DIAYN was chosen for its controllability. They also addressed the latent variable learning via variational lower bounds. The reviewer's key questions were answered, but I wonder if the method's reliance on predefined subtasks is a limitation they still see as significant.\n\nReviewer 3 gives a weak acceptance, noting the method is simple and scalable but criticizes the need for predefined subtasks. They suggest a DIAYN-only baseline and ask about the latent vector size. The authors added the DIAYN baselines and reported the latent size (5). They argue that prior knowledge is crucial for complex tasks, which makes sense. However, the need for predefined subtasks does limit applicability, which is a valid point but maybe acceptable for a top conference if the contributions are strong enough.\n\nNow, looking at the author responses. They addressed most points: added baselines, clarified T_low, fixed notation, explained latent variables, and provided more experiments. The main concerns seem to have been mitigated. The DIAYN-only baseline's failure supports their claim that subtask knowledge is necessary. The experiments in Jaco and Ant environments show improvement over baselines, which is good.\n\nThe paper's contributions: proposing a modular HRL framework for coordinating skills in multi-agent settings, using skill diversity to enable better coordination. The experiments are on challenging tasks, which is a plus. The method's novelty is in applying skill diversification in a multi-agent context with a meta-policy, which seems a solid contribution.\n\nPotential weaknesses: reliance on predefined subtasks (as noted by R3), which might limit real-world applicability. However, the authors argue that complex tasks require such prior knowledge, which is a reasonable stance. The T_low=1 point was addressed by explaining it's about behavioral control rather than temporal abstraction, and experiments with varying T_low show performance trade-offs. The variance was handled by adding more seeds and moving averages.\n\nTop-tier conferences usually require strong novelty, solid experiments, and clear contributions. This paper addresses a relevant problem in multi-agent HRL, provides a novel framework, and validates it on complex tasks. The reviewer concerns were addressed adequately. The predefined subtask limitation is a valid point but perhaps acceptable given the current state of the field. The experiments and baselines seem thorough now.\n\nI think the paper should be accepted. The authors have addressed the reviewers' main points, the problem is important, and the method shows promise. The limitations are acknowledged, and the contributions are significant enough for a top conference.\n</think>\n\nFinal Decision: Accept\n\nThe paper proposes a novel modular framework for coordinating skills in multi-agent and bimanual manipulation tasks through skill behavior diversification. The method addresses an important problem in hierarchical reinforcement learning by enabling agents to leverage pre-trained skills for complex collaborative tasks. Key strengths include:\n\n1. **Novelty and Contribution**: The approach introduces a structured way to coordinate diverse skills learned in isolation, which is a gap in existing multi-agent and hierarchical RL literature. The focus on behavioral coordination rather than just temporal abstraction offers a fresh perspective.\n\n2. **Empirical Validation**: The experiments on challenging tasks (e.g., bimanual manipulation, Ant pushing) demonstrate clear improvements over baselines. The addition of DIAYN-only baselines and analysis of hyperparameters (e.g., T_low) strengthens the evaluation.\n\n3. **Addressing Reviewer Concerns**: The authors thoroughly addressed technical concerns (notation, T_low design, latent variable learning) and limitations (predefined subtasks). They provided additional experiments, clarified ambiguities, and justified design choices effectively.\n\n4. **Reproducibility and Clarity**: The revised version includes more detailed figures, success rate plots, and hyperparameter details (e.g., latent vector size), improving transparency.\n\n**Limitations**: The requirement for predefined subtasks is a valid critique, but the authors convincingly argue that this prior knowledge is necessary for complex tasks and position their work as complementary to unsupervised skill discovery. While not flawless, the method’s contributions and empirical results align with the standards of a top-tier conference.\n\nThe paper advances the field by tackling coordination in multi-agent systems with pre-trained skills, offering a practical and scalable solution. The thorough revisions and strong experimental validation justify acceptance."
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification.\" The abstract mentions a modular framework for hierarchical reinforcement learning (HRL) where sub-skills are trained individually with diversity, then coordinated by a meta-policy. The tasks involve bimanual manipulation and multi-agent scenarios like two robots pushing a box. The authors claim their method outperforms baselines.\n\nLooking at the reviewer comments, there are three reviewers. Let's break down their points:\n\n**Reviewer 1**:\n- Positive aspects: The problem is interesting, the method addresses an important issue, and results show improvement over baselines.\n- Concerns: \n  1. Notation inconsistencies and unclear Figure 2.\n  2. T_low=1 might make skills too short, treating them as primitive actions rather than temporally extended.\n  3. High variance in training curves; more seeds needed.\n  4. Y-axis in some figures uses episode reward instead of success rate, making comparisons hard.\n  5. Multi-agent baselines could be more comprehensive, referencing recent work.\n\n**Reviewer 2**:\n- The method is presented well but lacks scientific insight.\n- Key questions: \n  1. Is temporal abstraction (TA) in multi-agent different from single-agent? The authors say it's the same but coordination is the new problem.\n  2. Why use DIAYN for skill discovery? They say any method can be used, but DIAYN is chosen for its controllability.\n  3. Predefined subtasks are required, which limits applicability. The authors added baselines without subtasks but found they didn't work, arguing expert knowledge is crucial.\n- Technical questions: \n  - How is q(z|s) learned? They reference variational lower bound from DIAYN.\n  - Fixed T_low pros/cons: They explain it's a trade-off between flexibility and learning difficulty.\n\n**Reviewer 3**:\n- Weak acceptance. The approach is simple and scalable but relies on predefined subtasks.\n- Criticisms: \n  - Subtasks must be specified in advance. They tried DIAYN alone but it failed, so expert knowledge is needed.\n  - Latent skill vector size wasn't reported (now added as 5).\n  - Suggested a DIAYN-only baseline, which they added and showed it didn't work.\n\n**Author Responses**:\n- Addressed notation issues, clarified T_low's role, added more seeds and success rate plots.\n- Added DIAYN-only baselines which underperformed, justifying the need for subtasks.\n- Explained T_low's design choice and its impact on performance.\n- Cited related work to contextualize their contributions.\n\nNow, evaluating against top-tier standards. The paper's main contributions seem to be applying skill diversity in a multi-agent HRL framework, with experiments showing better performance. The reviewers have valid concerns, but the authors addressed many of them. \n\nKey points for acceptance:\n- The problem is relevant and timely in multi-agent and HRL.\n- The method shows empirical success over baselines, especially with added experiments.\n- The authors addressed reviewer concerns about baselines, notation, and T_low.\n\nConcerns that remain:\n- The need for predefined subtasks limits general applicability, but the authors argue this is necessary for complex tasks and provided justification.\n- The high variance in results was addressed by adding more seeds and moving averages.\n- The DIAYN-only baseline's failure supports their approach's necessity.\n\nThe reviewers' scores: R1 and R3 are leaning towards accept (with R3 weak), R2 had initial reservations but the responses seem to address their questions. The authors did a good job in their rebuttal, adding experiments and clarifications. \n\nTop-tier conferences require significant novelty and robust evaluation. While the method builds on existing techniques (DIAYN, HRL), the combination in a multi-agent coordination context with the proposed framework is a solid contribution. The experiments cover multiple domains (manipulation, locomotion) and the added baselines strengthen the case. The predefined subtask limitation is a drawback but acknowledged, and the paper's focus is on coordination given pre-trained skills, which is a valid scope.\n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept\n\nThe paper proposes a modular framework for coordinating skills in multi-agent and bimanual manipulation tasks, leveraging skill behavior diversification through a hierarchical reinforcement learning approach. The method addresses an important problem in multi-agent coordination by focusing on the integration of pre-trained skills, which is a less explored area compared to learning from scratch. Key strengths include:\n\n1. **Novelty and Relevance**: The work tackles the coordination of pre-trained skills in a multi-agent setting, a problem not sufficiently addressed in prior work. The authors highlight the gap in existing methods that either lack coordination mechanisms or require learning all skills from scratch, which limits scalability to complex tasks.\n\n2. **Empirical Validation**: The experiments demonstrate effectiveness across diverse tasks (e.g., bimanual manipulation, Ant pushing), with results showing superior performance over baselines. The addition of DIAYN-only baselines and centralized/decentralized variants, as requested by reviewers, strengthens the argument for the necessity of predefined subtasks in complex scenarios.\n\n3. **Addressing Concerns**: The authors adequately addressed reviewer critiques. Notation inconsistencies were resolved, T_low's design choice was justified with experiments, and the high variance in results was mitigated by adding more seeds and moving averages. The latent skill vector size was clarified, and the role of predefined subtasks was defended with empirical evidence.\n\n4. **Technical Soundness**: The framework builds on well-established techniques (e.g., DIAYN, HRL) but innovates in their integration for multi-agent coordination. The use of behavior embeddings to control skill diversity and the meta-policy’s role in coordination are methodologically sound.\n\n**Limitations**: The requirement for predefined subtasks limits general applicability, but this is a trade-off for tackling complex tasks where unsupervised skill discovery may fail. The authors acknowledge this and frame their contribution as a solution for scenarios where such prior knowledge is feasible.\n\nOverall, the paper meets the standards of a top-tier conference by advancing the state of the art in multi-agent coordination, providing thorough evaluations, and addressing reviewer concerns effectively. The contributions are novel, the experiments are robust, and the limitations are transparently discussed."
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space.\" The keywords include adversarial defense, auto-encoder, GAN, etc. The abstract mentions they propose AE-GAN+sr, which uses an auto-encoder with adversarial loss and a search in the latent space during inference to find the closest natural image. They claim it's robust with lower computation than Defense-GAN.\n\nLooking at the reviewer comments:\n\nReviewer 1 thinks the method is an alternative to Defense-GAN, with better performance in some cases (like single starting point and fewer iterations) but worse accuracy. They note experiments on MNIST/F-MNIST are reasonable but maybe not enough.\n\nReviewer 2 is critical. They say the contribution is too incremental, as it's refining Defense-GAN. They point out that the paper doesn't compare with state-of-the-art defenses like PGD adversarial training or TRADES. Also, there are typos and formatting issues (over one page limit). The figure 4 is unclear regarding false alarms.\n\nReviewer 3 mentions reliance on the autoencoder's detection, which might be vulnerable to white-box attacks. They also question the latent distribution alignment and the performance discrepancy between white and black-box attacks. The method's effectiveness on more complex datasets is untested.\n\nReviewer 4 says the writing is hard to follow, experiments aren't strong enough (only MNIST/F-MNIST), and the method's implementation details are unclear. They suggest testing on CIFAR-10 for better evidence.\n\nAuthor responses address some points. They added comparisons with PGD adversarial training, showing mixed results (their method is better on some attacks but worse on BPDA). They included CelebA results but not CIFAR-10 yet. They also addressed detection mechanism's susceptibility to white-box attacks with experiments, showing detection effectiveness. They fixed typos and added more details on implementation steps and latent space search.\n\nNow, considering top-tier standards. The key points are:\n\n1. **Novelty/Contribution**: The method is an improvement over Defense-GAN by reducing computational cost. However, Reviewer 2 says it's too incremental. The authors added comparisons with PGD, but their method isn't better across all metrics. The core idea of using an encoder to initialize search isn't entirely new, but the combination with adversarial loss and specific search steps might add value.\n\n2. **Experiments**: Original experiments were on MNIST/F-MNIST. Authors added CelebA but not CIFAR-10. Top conferences usually require more robust datasets. The CelebA results are a step forward but not as strong as CIFAR-10. Also, the comparison with SOTA defenses like TRADES wasn't fully addressed (only PGD was added). The defense against BPDA attacks shows weaknesses, which is a concern.\n\n3. **Technical Soundness**: The autoencoder's latent distribution issue (F(x) vs p(z)) wasn't resolved, which could be a flaw. The authors tried but couldn't align them effectively. The detection mechanism's vulnerability was addressed with experiments, which helps, but there's still a reliance on the autoencoder's quality.\n\n4. **Writing/Clarity**: Authors fixed typos and added details. Reviewer 4's concern about clarity might be mitigated, but the paper was initially over length. The revised version should be within limits now.\n\n5. **Significance**: The method reduces computation compared to Defense-GAN, which is a practical advantage. However, if it's outperformed by adversarial training methods (like PGD) in some cases, its significance might be limited unless it offers unique benefits (e.g., no need for adversarial training data).\n\nBalancing these factors: The paper has a solid technical contribution but is incremental. The experiments are improving but still lack strong results on complex datasets and full SOTA comparisons. The writing issues were addressed, but initial flaws might still affect readability. The defense against certain attacks (BPDA) is weaker, which is a drawback.\n\nTop-tier conferences usually require strong novelty, thorough experiments, and clear superiority over existing methods. While this paper shows promise and improvements, the incremental nature, incomplete comparisons, and unresolved technical issues (like latent distribution) might tip the decision towards rejection. However, the authors did address some concerns in their response, so maybe a borderline case?\n\nWait, the author response added CelebA results and more details. But without CIFAR-10, which is a standard dataset, it's still questionable. Also, the comparison with TRADES wasn't done. The defense against BPDA is weaker than PGD adversarial training, which is a significant point against it.\n\nFinal call: The paper has merit but falls short in key areas for a top-tier conference. The experiments aren't comprehensive enough, and the contribution is incremental. So, I think the decision should be to reject.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Contribution and Novelty**: While the proposed AE-GAN+sr framework improves computational efficiency over Defense-GAN by leveraging an encoder for initialization, the core idea of using latent space search for adversarial defense is not sufficiently novel. The method is an incremental refinement rather than a groundbreaking advancement, as noted by Reviewer 2. The lack of clear superiority over state-of-the-art defenses like PGD adversarial training (except in specific scenarios) further weakens its novelty.\n\n2. **Experimental Rigor**: \n   - **Dataset Limitations**: Experiments are primarily conducted on MNIST and Fashion-MNIST, which are relatively simple datasets. While CelebA results were added in the author response, CIFAR-10—a standard benchmark for adversarial robustness—remains untested. This limits the generalizability of the claims.\n   - **Missing Comparisons**: The authors addressed comparisons with PGD adversarial training but did not fully incorporate TRADES, another critical SOTA defense. Additionally, the method underperforms against BPDA attacks compared to PGD adversarial training, highlighting vulnerabilities that were not adequately resolved.\n\n3. **Technical Soundness**:\n   - The autoencoder’s latent distribution alignment (F(x) vs. p(z)) remains unresolved, with attempts to address it yielding poor results. This introduces potential instability in the latent space, which is central to the method’s performance.\n   - While the detection mechanism’s susceptibility to white-box attacks was partially addressed, the reliance on the autoencoder’s detection accuracy introduces a vulnerability that was not conclusively mitigated.\n\n4. **Clarity and Presentation**: \n   - Although the authors revised the text to address typos and added implementation details, the initial writing was criticized as \"hard to follow\" and \"hastily written.\" While improvements were made, the paper still exceeds the page limit, suggesting unresolved structural issues.\n   - Key implementation details (e.g., exact training procedures, hyperparameters) were clarified in the response but were initially unclear, which could hinder reproducibility.\n\n5. **Significance**: The computational efficiency gain over Defense-GAN is a practical advantage, but the method’s performance is outperformed by adversarial training in critical scenarios (e.g., BPDA attacks). Without clear evidence of unique benefits (e.g., no adversarial training required), the significance for top-tier conferences is diminished.\n\n### Conclusion:\nThe paper presents a valid incremental improvement but lacks the depth, experimental rigor, and novelty required for acceptance at a top-tier conference. Critical gaps in dataset complexity, SOTA comparisons, and unresolved technical flaws (e.g., latent distribution misalignment) outweigh its contributions."
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new method for continual learning called Differentiable Hebbian Consolidation (DHC), which uses slow and fast weights inspired by Hebbian learning and CLS theory. The goal is to prevent catastrophic forgetting when neural networks learn sequentially.\n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 mentions that while the method shows promise, the empirical evaluation on Split CIFAR-10/100 isn't strong enough compared to Synaptic Intelligence (SI). They want more experiments, especially since SI outperforms DHP Softmax on most splits except the last one. The authors addressed this by adding experiments on Split CIFAR-10/100 in the appendix, but the results still show SI is better. However, the authors argue that combining DHP with SI improves performance, which is a valid point but maybe not enough for top-tier.\n\nReviewer 2 had trouble understanding the method's details, like the equations and terminology. The authors clarified these in their response, adding figures and explanations. That's good because clarity is important, but the initial confusion suggests the paper might need more explanation in the main text.\n\nReviewer 3 is positive but points out missing comparisons with other CLS-based methods and suggests testing on more complex datasets like CIFAR. The authors addressed CLS comparisons by discussing existing methods and their limitations, but they didn't add new experiments beyond the appendix. The CIFAR experiments were added, but again, SI still outperforms. The reviewer also noted the finetune baseline might be weak, but the authors defended their choices, which seems reasonable given the continual learning focus.\n\nReviewer 4's main issue is the weak performance on challenging datasets and the method not outperforming established techniques like SI. The authors argue that DHC works best in combination and added experiments, but the standalone performance is still lacking. The suggestion to test on more datasets like CORe50 wasn't addressed due to time constraints, which is a valid limitation.\n\nThe authors did a good job addressing concerns by adding experiments and clarifications, but the core issue remains: their method doesn't surpass top baselines like SI on key benchmarks. While combining with SI helps, the paper's main contribution is the DHC framework itself. For a top conference, the results need to be stronger, especially since SI is a well-established method. The experiments on new benchmarks (like the imbalanced Permuted MNIST) are a plus, but the CIFAR results don't break new ground.\n\nThe writing and motivation are strong, with clear connections to neurobiology. The method is novel and the approach is different from existing regularization or replay methods. However, the empirical validation, while improved, still shows that DHC alone isn't the best performer. The combination with other methods is interesting but might not be sufficient for acceptance without more compelling standalone results.\n\nConsidering all this, the paper has merit but falls short in empirical performance against top baselines. The reviewers' concerns about experiments and comparisons aren't fully addressed to the level expected for a top-tier conference. So, the decision leans towards rejection unless the improvements are substantial, which they aren't based on the rebuttal.\n</think>\n\nThe paper proposes a novel approach to continual learning called Differentiable Hebbian Consolidation (DHC), which introduces a slow-fast weight architecture inspired by Hebbian learning and CLS theory. The method aims to mitigate catastrophic forgetting by combining fixed (slow) weights with a plastic (fast) component in the softmax layer. While the idea is innovative and grounded in neurobiological principles, the empirical evaluation raises concerns that limit its competitiveness for a top-tier conference.\n\n### Key Strengths:\n1. **Novelty and Motivation**: The integration of Hebbian plasticity and CLS theory into a differentiable framework is a fresh direction for continual learning. The biological plausibility and clear motivation are strong points.\n2. **Empirical Validation**: The authors test their method on standard benchmarks (Permuted MNIST, Split MNIST) and introduce an imbalanced variant of Permuted MNIST, which addresses a critical real-world challenge. They also added Split CIFAR-10/100 experiments in the rebuttal, demonstrating forward transfer and improved performance over finetuning.\n3. **Scalability and Simplicity**: The method is computationally efficient, requires no additional hyperparameters (except a learnable Hebbian learning rate), and is compatible with existing continual learning techniques like SI and EWC.\n\n### Key Weaknesses:\n1. **Competitive Performance**: \n   - On Split CIFAR-10/100, SI outperforms DHP Softmax on nearly all splits except the final one. While combining DHP with SI improves results, the standalone performance of DHP is weaker than established methods like SI, which undermines its standalone utility.\n   - The authors argue that DHP works best in conjunction with other methods, but this reduces its novelty as a standalone solution. For a top-tier conference, the method should demonstrate superior performance on its own in key benchmarks.\n2. **Limited Benchmark Scope**: \n   - While the authors added Split CIFAR-10/100 experiments, they did not address more complex benchmarks like CORe50 or class-incremental settings (e.g., iCaRL on CIFAR-100/ILSVRC), which are standard in recent continual learning literature. This limits the generalizability of their claims.\n3. **Clarity and Comparisons**:\n   - Reviewer 2’s confusion about equations and terminology suggests the paper could benefit from clearer explanations and figures (e.g., visualizing the architecture and equations). While the authors addressed these in the rebuttal, the main text may still lack sufficient clarity.\n   - The comparison to CLS-based methods (e.g., Hebbian Softmax) is incomplete. The rebuttal clarifies limitations of prior work but does not include direct comparisons on the same benchmarks, weakening the argument for DHC’s superiority.\n\n### Conclusion:\nThe paper presents an interesting and novel approach to continual learning with promising theoretical foundations and some empirical validation. However, the method’s performance lags behind established baselines like SI on critical benchmarks, and the experiments do not fully address reviewers’ concerns about scalability and comparison to CLS-based methods. While the authors made efforts to strengthen the rebuttal, the results remain underwhelming for a top-tier conference, where state-of-the-art performance is expected. The method’s value as a complementary technique is acknowledged, but its standalone contributions are insufficient to justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning to Transfer via Modelling Multi-level Task Dependency,\" and the keywords are multi-task learning and attention mechanism. The abstract mentions that existing methods assume tasks are related, which might not hold in real-world scenarios. The authors propose a framework using attention to model task dependencies at multiple levels, aiming to improve performance by guiding knowledge transfer.\n\nLooking at the reviewer comments, there are four reviewers. Let me go through each one.\n\nReviewer 1 points out grammar issues and that the distinction between general and data dependency isn't clear enough. They also note that the improvements over SOTA are slight, suggesting the contributions might not be significant enough. The reviewer thinks the paper needs better clarity on the two levels of dependency.\n\nReviewer 2 mentions the motivation for using attention isn't strong enough and there are unclear parts in the method (like what mappings are used in equation 2). The experiments might have issues with dataset splits and lack a baseline without attention. They also note the paper is hard to follow and has notation issues. However, they think the idea of sample-dependent task relatedness is new and could be acceptable with improvements.\n\nReviewer 3 criticizes the claim that most MTL methods assume tasks are highly correlated, pointing out that many existing methods already learn task dependencies. They also note that having separate encoders for each task increases parameters, which could be a problem with limited data. The reviewer mentions that attention in MTL isn't new and the paper should compare with related works. There's a typo noted too.\n\nReviewer 4 is more critical. They point out that the paper doesn't cite Taskonomy (2018), which is a key related work that already models task relationships. The method's specifics are unclear, like how the general task dependency matrix is defined and extracted. The presentation is confusing with notation issues and mixing concepts. The reviewer thinks the paper has serious issues and needs major revisions, including better discussion of prior work and experiments.\n\nThe author responses address some points. They clarify that their work handles discrete data (text/graph) with hierarchical structures, allowing task dependencies at different levels. They mention a parameter reduction technique (from O(T²) to O(T)) compared to [2], and that their model allows task interactions unlike [3]. They also address the Taskonomy comparison, saying their approach adds data-specific dependencies. They added experiments without attention and fixed some notation issues.\n\nNow, considering all this:\n\nStrengths:\n- The authors propose a method that models task dependencies at both general and data-specific levels, which could be beneficial for hierarchical data like text and graphs.\n- They address parameter efficiency by reducing complexity from O(T²) to O(T), which is a valid contribution.\n- The idea of task dependencies varying per data instance (like words in text) is novel and could be impactful.\n\nWeaknesses:\n- The paper was initially poorly written with grammar issues and unclear explanations, though authors claim to have revised this.\n- The main contribution (multi-level dependencies) might not be sufficiently distinct from prior work like Taskonomy, which the authors now acknowledge but argue their extension to data-specific dependencies adds value.\n- Reviewer 4's point about Taskonomy being a critical prior work not adequately addressed initially is a concern. The authors did respond by comparing with Taskonomy, but the rebuttal might not be enough if the paper's discussion of prior work was weak.\n- The experiments could be improved by adding more baselines (like the End-to-End MTL with attention mentioned in the meta-review) and the authors added some, but it's unclear if that's sufficient.\n- The method's technical details (like equations 2 and 4) had notation issues, which the authors addressed but might still be confusing.\n\nTop-tier conferences require significant novelty, clear presentation, and thorough experiments. The key issue here is the prior work. If Taskonomy already models task relationships, the authors need to clearly show how their approach is different and better, especially in handling hierarchical data. Their rebuttal claims that Taskonomy doesn't handle data-specific dependencies (like per-word in text), which could be a valid distinction. However, without strong experimental comparisons against Taskonomy and other attention-based MTL methods, it's hard to assess the contribution's significance.\n\nThe authors added experiments without attention and addressed some notation, but the meta-reviewer still thinks the experimental section needs more, like comparing to End-to-End MTL. The parameter efficiency is a plus, but if the method's performance gains are marginal, that's a problem. The initial presentation issues were a barrier, but the authors revised the paper.\n\nOverall, the paper has potential but needs to better address prior work, clarify their contributions, and strengthen experiments. Since the reviewers (especially 4) found major issues with prior work discussion and clarity, and the rebuttal might not have sufficiently resolved these, the paper might not meet the high bar of a top-tier conference yet. However, if the authors can convincingly show their method's novelty over Taskonomy and other works with solid experiments, it could be acceptable. Given the current state, with the reviewers' concerns about prior work and experiments, I'm leaning towards reject unless the rebuttal fixed everything, but the meta-reviewer's comments after rebuttal still had issues. The final decision would depend on whether the authors adequately addressed all points, but based on the provided info, it's a close call but likely reject.\n</think>\n\nThe paper proposes a novel approach to multi-task learning by modeling multi-level task dependencies using attention mechanisms, aiming to address the limitation of existing methods that assume predefined task correlations. While the idea of incorporating both general and data-specific dependencies for hierarchical data (e.g., text and graphs) is promising, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Prior Work Overlap and Clarity**:  \n   - **Taskonomy (2018)** is a key prior work that models task relationships, yet the paper initially failed to adequately discuss or compare with it. The authors’ rebuttal clarifies distinctions (e.g., handling hierarchical data and data-specific dependencies), but the original manuscript’s lack of engagement with Taskonomy weakens its novelty claims.  \n   - The distinction between \"general task dependency\" and \"data dependency\" remains unclear in the original submission, though the authors addressed this in their response. However, the reviewers’ concerns about insufficient motivation and presentation clarity persist.\n\n2. **Technical and Experimental Gaps**:  \n   - **Parameter Efficiency**: While the authors claim a reduction in complexity from \\(O(T^2)\\) to \\(O(T)\\), they acknowledge that their model still requires significant parameters. The rebuttal mentions ongoing experiments to address this, but the results are not yet included.  \n   - **Baseline Comparisons**: Reviewers highlighted the absence of critical baselines (e.g., \"End-to-End Multi-Task Learning with Attention\") and the need for a non-attention variant. The authors added some experiments, but the meta-reviewer notes unresolved concerns about thoroughness.  \n   - **Notation and Presentation**: Despite revisions, lingering ambiguities in equations (e.g., notation in Eq. (4)) and figures (e.g., Figure 1’s purpose) suggest the paper’s clarity remains suboptimal for a top-tier venue.\n\n3. **Novelty and Impact**:  \n   - The core idea of task dependencies varying per data instance (e.g., word-level in text) is novel but underdemonstrated. The experiments show improvements over some baselines, but the margin is described as \"slight\" by Reviewer 1. Without stronger empirical validation against state-of-the-art methods like Taskonomy, the impact is questionable.  \n   - The hierarchical justification (e.g., word vs. sentence dependencies) is plausible but not rigorously tested across diverse datasets. The rebuttal’s example is illustrative but lacks quantitative evidence.\n\n**Conclusion**: While the paper introduces an interesting direction for multi-task learning in hierarchical data, its failure to adequately address prior work, clarify technical details, and provide robust experimental validation against relevant baselines undermines its suitability for a top-tier conference. The authors’ rebuttal partially addresses these points but does not fully resolve the reviewers’ concerns, particularly regarding Taskonomy and experimental rigor.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\". The keywords are Reinforcement Learning, MCTS, and AlphaZero. The abstract mentions that they're looking at the coordination of episode generation using MCTS to handle exploration vs exploitation, and they tested it on a small problem (like Tic-Tac-Toe) showing better robustness than AlphaZero. \n\nNow, looking at the reviewer comments. All three reviewers are pretty harsh. Reviewer 1 says the paper is \"less than half-baked\", no references, poorly written. They suggest resubmitting when it's more complete, starting with related work and references. Reviewer 2 also mentions bad writing, grammatical issues, and couldn't understand the abstract or the main points. They even say the paper has inaccurate statements and isn't up to scientific writing standards. Reviewer 3 is even more critical, pointing out the paper is poorly written with incomprehensible passages, no references, undefined math symbols, and structural issues like blank spaces and hard-to-follow algorithms. They think it's not a serious academic work but just notes, requiring a lot of work before being acceptable anywhere.\n\nThe author's response is apologetic. They mention they removed references because they didn't know how to use .bib, and they want to improve writing skills. They also mention they found a similar idea in their country but not internationally, and they hope to do better next time.\n\nSo, considering top-tier conferences, the standards are high. The paper's main issues are:\n\n1. **Writing Quality**: All reviewers point out severe grammatical errors, unclear writing, and difficulty understanding the content. For a top conference, clarity is crucial. If the paper can't be understood, it's a major red flag.\n\n2. **Lack of References**: No citations at all is a big problem. Academic work must situate itself within existing research. The author admits they didn't know how to use .bib, which is a technical issue, but even so, the absence of references shows lack of literature review, which is fundamental.\n\n3. **Technical Content Clarity**: Reviewers couldn't grasp the method. The abstract itself is unclear. The proposed method's description is vague (e.g., \"coordination of episode generation\"). The algorithm boxes are hard to follow, and there are undefined symbols. Without clear exposition, even if the idea has merit, it's not presentable.\n\n4. **Experimental Validation**: The experiments are on a small problem (Tic-Tac-Toe), which might not be sufficient for a top conference. However, the bigger issue is that the results aren't clearly communicated. If the method's performance isn't well-explained or compared properly, that's a problem. But the main issue here is the writing obscuring everything.\n\n5. **Professionalism and Preparation**: The author's response shows they might not have followed basic submission guidelines (like references), and the paper's structure (like blank spaces, 10 pages but not well-used) suggests poor preparation. Top conferences expect polished submissions.\n\nThe author's response indicates they recognize some issues (writing, references) and want to improve. However, the reviewers' consensus is that the paper isn't ready for publication. Even if the idea has potential, the current state is too flawed. For a top-tier venue, which is highly competitive, the paper needs to meet high standards in all aspects: writing, technical clarity, references, and contribution. Here, the writing and clarity are so poor that the core contribution isn't communicated effectively. Without clear exposition, reviewers can't assess the method's novelty or validity.\n\nAdditionally, the fact that the author couldn't find international papers on the topic but found a similar one locally might indicate that the idea isn't novel enough or that prior work exists that wasn't cited. The lack of references also means they might not be aware of existing related work, which is a problem for a research paper.\n\nIn conclusion, despite the potential idea, the paper's current state doesn't meet the standards required for acceptance at a top conference. The issues are fundamental and require substantial revision beyond minor edits. The reviewers' recommendations to reject and resubmit after major revisions align with this assessment.\n</think>\n\nThe paper under review faces significant and fundamental issues that preclude its acceptance at a top-tier conference. Here is the detailed reasoning:\n\n### Key Issues Identified:\n1. **Poor Writing and Clarity**:\n   - All reviewers unanimously criticized the paper's writing quality, noting severe grammatical errors, unclear sentences, and an overall lack of coherence. For instance, Reviewer 2 stated they \"could not understand the abstract\" and struggled to discern the paper's core contributions. Such issues render the work inaccessible to readers, a critical failing for any academic publication.\n   - The abstract itself is vague, using phrases like \"coordination of episode generation\" without clear explanation. The method’s novelty and technical details are obscured, making it impossible for reviewers to evaluate its merits.\n\n2. **Lack of References and Related Work**:\n   - The paper contains **no citations**, a glaring omission in academic writing. This suggests the authors failed to contextualize their work within existing research, leaving open questions about novelty, prior art, and the problem’s relevance. Reviewer 1 explicitly called this out as a reason for rejection.\n   - The author’s response acknowledges the lack of references due to technical inexperience with .bib files, but this is insufficient. Even a placeholder citation list would have demonstrated awareness of the field.\n\n3. **Technical Incomprehensibility**:\n   - Reviewers highlighted that the paper’s technical content is \"incomprehensible\" (Reviewer 3), with undefined mathematical symbols, poorly formatted algorithms, and vague descriptions of the proposed method. For example, the \"master game tree\" concept is introduced without clarity, and the experiments (e.g., on Tic-Tac-Toe) are not sufficiently detailed to validate claims of robustness over AlphaZero.\n   - The paper’s structure is flawed, with \"blank space\" and excessive length (10 pages) without meaningful content, suggesting rushed or disorganized preparation.\n\n4. **Novelty and Contribution**:\n   - While the abstract claims to address a gap in understanding the stability and performance of reinforcement learning methods like AlphaZero, the paper fails to articulate how its approach differs from existing work. The author’s mention of a similar \"meta-tree\" idea in their country further raises concerns about novelty without proper citation or comparison.\n   - The experiments on a small problem (e.g., Tic-Tac-Toe) may lack the rigor expected at a top-tier conference, but this is secondary to the paper’s inability to communicate its results clearly.\n\n5. **Professionalism and Preparation**:\n   - The paper’s submission appears unpolished, with formatting issues and a lack of adherence to basic academic standards. Top-tier conferences require submissions to be thoroughly vetted for clarity, completeness, and professionalism. The author’s acknowledgment of needing proofreading and improved writing skills indicates the work is not yet ready for peer review.\n\n### Author Response Analysis:\nThe authors’ response is apologetic and highlights a desire to improve, but it does not address the core issues:\n- They admit to removing references due to technical ignorance, which is inexcusable for a submission to a reputable venue.\n- The mention of finding a similar idea locally but not internationally suggests potential overlap with existing work that was not properly cited or discussed.\n- The response does not clarify the method’s technical details or propose concrete steps to resolve the reviewers’ concerns.\n\n### Conclusion:\nWhile the paper’s abstract hints at a potentially interesting idea—refining MCTS-based agents—the execution falls far short of the standards required for a top-tier conference. The writing is so poor that the core contributions are unintelligible, the lack of references undermines its academic credibility, and the technical exposition is insufficient to evaluate novelty or validity. These are not minor issues; they are foundational flaws that require a complete overhaul of the manuscript. The reviewers’ consensus to reject and resubmit is justified, as the paper currently does not meet the bar for publication in a prestigious venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Domain Adaptive Multibranch Networks,\" and it's about unsupervised domain adaptation. The abstract says they allow different domains to have different processing paths, which is a contrast to existing methods that use the same operations for all domains. The key idea is that different domains might need different computations to reach a common feature representation. The experiments show higher accuracy and flexibility in handling multiple domains.\n\nNow, looking at the reviewer comments. There are three reviewers. Let's break down each one.\n\nReviewer 1 is generally positive. They mention the motivation is clear, related work is sufficient, and experiments are convincing. Their minor comments include suggesting more transfer learning references outside deep learning, renaming \"flow network\" to avoid confusion, removing backlinks in references, replacing arXiv with reviewed papers, and ensuring code is published. The authors addressed all these points in their response. They renamed the method to \"Domain-Adaptive Multibranch Networks,\" fixed the references, and committed to releasing code. So Reviewer 1's concerns seem adequately addressed.\n\nReviewer 2's initial comments were more critical but after discussion, they \"stick to their original score\" with some issues resolved. The main points here are:\n\n- The idea isn't novel in adaptive computation graphs but applied to domain adaptation, which is novel. The method is technically sound.\n- The paper claims different computation amounts per domain, but experiments used the same layers for all domains. The authors added experiments with different layer counts in the revised version, which is good.\n- The experiments only compared with RPT and DANN, which are not state-of-the-art (SOTA). The authors mention adding comparisons with CDAN and MCD, which improved results. They also addressed the Figure 4 confusion by clarifying parameter sharing. However, the reviewer still wants better experimental discussion. The authors added more experiments and explanations, so maybe this is okay now.\n\nReviewer 3 was more critical. Their main points:\n\n- The experiments are insufficient. They need to compare with SOTA methods like CDAN, not just DANN. The authors addressed this by adding CDAN and MCD comparisons, which is a big plus.\n- Lack of ablation studies and hyperparameter analysis. The authors added some ablation studies on K (number of flows), grouping configurations, and multi-source experiments. They also explained why some aspects weren't explored further (time constraints, etc.), which is reasonable for a conference paper.\n- The baseline results in Table 1 are lower than others. The authors clarified they used standard conditions, which is good practice. Maybe other papers used different setups, so their approach is fair.\n\nThe authors' responses seem to have addressed most of the concerns. They added more experiments, fixed naming issues, and clarified points. The key was the lack of SOTA comparisons, which they addressed by adding CDAN and MCD. The method's novelty in applying adaptive computation to domain adaptation is a plus. The experiments now include more thorough analysis, and the code will be released.\n\nHowever, I should check if the remaining issues are significant. Reviewer 2 still mentions needing improved experimental discussion, but the authors added more results. The main concerns about SOTA and ablations seem resolved. The method's contribution is novel enough for a top conference, especially if it shows better performance when combined with SOTA alignment methods.\n\nThe title change to \"Multibranch\" avoids the flow network confusion. The code availability is confirmed. The experiments, while initially lacking, were bolstered in the response. The paper's core idea is solid and addresses a gap in domain adaptation by allowing different processing paths. The improvements in accuracy and flexibility support its merit.\n\nConsidering all this, the paper meets the standards for a top-tier conference. The authors addressed the reviewers' major concerns, and the contributions are novel and impactful. The experiments, though initially weak, were strengthened with additional comparisons and analyses. The only minor issues (like some ablations) are acceptable given space constraints and the overall contribution.\n</think>\n\nFinal Decision: Accept\n\nThe paper introduces a novel approach to unsupervised domain adaptation by allowing different domains to follow distinct computational paths, addressing a key limitation of existing methods that enforce uniform processing. The core idea is both innovative and technically sound, as acknowledged by the reviewers. While initial experiments were criticized for lacking comparisons with state-of-the-art (SOTA) methods and insufficient ablation studies, the authors addressed these concerns effectively in their response. They added comparisons with SOTA techniques like CDAN and MCD, demonstrated improved performance, and provided additional experiments on hyperparameters (e.g., number of flows, layer configurations) and multi-source adaptation. The revised title (\"Domain-Adaptive Multibranch Networks\") resolves naming conflicts, and the commitment to releasing code and addressing formatting issues further strengthens the submission. Although minor gaps remain in experimental depth, the paper’s conceptual contribution, empirical validation, and adherence to reviewer feedback align with the standards of a top-tier conference. The method’s flexibility and potential for extension to multi-domain scenarios justify its acceptance."
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Learning DNA folding patterns with Recurrent Neural Networks.\" The keywords include machine learning, RNNs, 3D chromatin structure, TADs, and computational biology. The abstract mentions using ML methods like linear models, gradient boosting, and RNNs (specifically bidirectional LSTM) to predict chromatin folding from epigenetic marks. They claim the LSTM outperforms others and that the sequential memory is important. They also identify informative features.\n\nNow, looking at the reviewer comments. Reviewer 1 points out that the improvement might be due to the loss function (weighted MSE) rather than the LSTM itself. They want a fair comparison where other models also use the same loss. Also, they question the generalizability of the loss function and the model to other datasets like ATAC-seq. \n\nReviewer 2 has several minor issues: unclear equation, formatting problems, lack of discussion on why regression vs NNs, and some questions about the training/test errors. They also mention that Figure 6's negative training errors vs positive test errors need explanation. \n\nReviewer 3 is more critical. They say the paper lacks methodological novelty, unclear data/methods description, and weak evaluation. They point out that the paper is similar to previous work not cited, unclear features used, unclear motivation for biLSTM (since DNA is double-stranded but input isn't the DNA sequence), and the evaluation metrics are not consistent across models. Also, hyperparameters aren't clearly described, and the choice of loss function isn't justified against alternatives like MSLE. \n\nThe authors responded to these points. They clarified the loss function (wMSE), addressed some of the data description issues, added more metrics, and explained their choices. They also mentioned that they didn't test on other datasets because they believe the mechanism might differ, and they didn't use ATAC-seq because it's different from ChIP-seq. They also addressed the biLSTM's use related to DNA's double-stranded nature and the hidden states.\n\nNow, considering the standards of a top-tier conference. Such conferences usually require significant novelty, thorough methodology, clear presentation, and rigorous evaluation. Let's break down the key points:\n\n1. **Methodological Novelty**: Reviewer 3 says the paper lacks novelty. The authors claim they're predicting TAD characteristics from ChIP-seq data, which they say hasn't been done before. But if there are existing works (even in workshops) that do similar things, that's a problem. The authors mention they didn't cite some works because they were in workshops, but that's not a valid reason. Top-tier conferences expect comprehensive literature reviews, including relevant workshop papers. If there are existing methods, the paper's contribution is weakened.\n\n2. **Clarity of Data and Methods**: Reviewers 1 and 3 point out unclear descriptions. The authors added more details, but if the original submission was unclear, that's a concern. The features used (ChIP-seq values averaged over 20kb bins) need to be clearly explained. The input representation is crucial for reproducibility.\n\n3. **Evaluation**: The main issue here is the loss function. The LSTM uses a custom wMSE, but other models might not have been trained with the same loss. The authors say they did use the same loss for all models, which addresses part of Reviewer 1's concern. However, they should also compare with standard losses (like MSE) to see if the improvement is due to the model or the loss. Also, the lack of generalization tests (other datasets, ATAC-seq) is a problem. The authors argue that the biological mechanisms might differ, but a top conference would expect at least some exploration or discussion of this.\n\n4. **Technical Soundness**: The use of biLSTM's hidden states: the authors say they use the center state to capture both directions, but Reviewer 3 questions why not concatenate. The response says all features contribute, but the technical justification isn't strong. Also, the hyperparameter tuning details are still unclear. The authors mention some hyperparameters in the methods, but specifics (like how they were tuned) are missing.\n\n5. **Discussion and Analysis**: The authors added more discussion on model choice and metrics, but Reviewer 2's point about why regression vs NNs wasn't fully addressed. The feature importance analysis using dropout is mentioned, but more in-depth analysis (like activation correlations) was suggested but not done.\n\n6. **Writing and Presentation**: The authors fixed some formatting issues, but initial clarity problems might still exist. The equation and citations were addressed, but the overall presentation needs to be polished.\n\nConsidering all this, the paper has some merit in applying RNNs to this problem, but the lack of novelty (if existing works exist), unclear methodology, insufficient evaluation (especially on loss function impact and generalization), and potential oversight of prior work are significant issues. The authors made revisions, but if the core issues like novelty and evaluation aren't adequately addressed, it might not meet top-tier standards. Since Reviewer 3's points about similarity to prior work and weak evaluation are critical, and the authors' response about not citing workshop papers is weak, this could be a red flag. Additionally, the loss function's role in performance isn't fully isolated, which is a methodological flaw.\n\nTherefore, despite some improvements, the paper might still fall short for a top conference due to these unresolved issues.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Lack of Methodological Novelty**:  \n   - Reviewer 3 and the author response highlight concerns about the paper’s similarity to prior work, particularly studies not cited due to being in workshops or conferences. Top-tier conferences require comprehensive literature reviews, including relevant workshop papers. The authors’ justification for excluding these works is insufficient, undermining the paper’s originality claim.  \n   - The core contribution—predicting TAD characteristics from ChIP-seq data—is not clearly differentiated from existing methods (e.g., the cited arXiv preprint), raising questions about novelty.  \n\n2. **Insufficient Evaluation and Baselines**:  \n   - The weighted MSE (wMSE) loss function is central to the results, but the authors did not isolate its impact by comparing the LSTM to a vanilla LSTM or other models trained with standard losses (e.g., MSE). This makes it unclear whether the performance gain stems from the model architecture or the loss function.  \n   - The evaluation lacks generalization tests (e.g., other organisms, ATAC-seq data). While the authors argue biological mechanisms may differ, top-tier work typically requires at least exploratory analysis or a stronger rationale for excluding such tests.  \n\n3. **Clarity and Methodological Gaps**:  \n   - The data preprocessing (e.g., how features are represented, handling of chromosome boundaries) and hyperparameter tuning details remain unclear despite revisions. The input features are described as \"float numbers\" averaged over 20kb bins, but the exact preprocessing steps (e.g., normalization, binning strategy) are not fully transparent.  \n   - The bidirectional LSTM’s motivation (e.g., DNA’s double-stranded nature) is weakly justified. The authors’ explanation conflates DNA’s physical structure with the model’s bidirectional design, which is more about sequential context than strand complementarity.  \n\n4. **Technical Soundness Issues**:  \n   - The use of the center LSTM hidden state instead of concatenating forward and backward states is inadequately justified. The authors’ response does not address why this approach avoids ignoring half of the activations.  \n   - Feature importance analysis is limited to dropout experiments, while more rigorous methods (e.g., activation correlation) were suggested but not implemented.  \n\n5. **Writing and Presentation**:  \n   - While the authors addressed formatting issues (e.g., equations, citations), the original submission’s clarity problems (e.g., vague descriptions of inputs, loss function rationale) may persist. The paper’s focus on \"predicting folding patterns\" is conflated with predicting TAD characteristics (e.g., gamma transitional), which could confuse readers.  \n\n### Conclusion:  \nThe paper’s contributions are incremental and lack the rigor expected of top-tier conferences. Key issues—novelty, evaluation robustness, and methodological clarity—remain unresolved despite revisions. While the application of RNNs to chromatin structure prediction is promising, the current submission does not meet the high standards required for acceptance."
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the information provided step by step.\n\nFirst, the paper's title is \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms.\" The keywords are Metric Learning, Geometric Algorithms, Approximation Algorithms. The abstract mentions that they're formulating the problem as an optimization to minimize violated constraints and present an FPTAS with nearly-linear time using linear programming in low dimensions. They also mention experiments and that the algorithm is parallelizable and handles adversarial noise.\n\nLooking at the reviewer comments:\n\nReviewer 1 points out that they're not an expert but the method seems sound and outperforms some classical methods. However, they question the state-of-the-art and novelty. They also mention missing comparisons with recent papers (Verma & Branson, Ye et al.), a confusion in Lemma 2.1's proof, non-monotonic running time in Figure 4, and a missing conclusion.\n\nReviewer 2 thinks the paper's main contribution is applying LP-type problem results from computational geometry to metric learning, which is interesting and could encourage further work. But they note missing discussion on computational hardness, undefined terms like \"accuracy\" and \"combinatorial dimension,\" an unclear function call in Algorithm 1, and the need to clarify that the FPTAS assumes fixed dimension. They also want more clarity on prior work's results on minimizing constraints.\n\nReviewer 3 is more critical. They question the novelty, saying transforming into LP isn't a big step, and the experiments don't show accuracy improvements. They want better motivation and more details on theoretical guarantees, differences from existing convex methods, and why approximation is needed. They also point out confusion in the proof about adding constraints.\n\nAuthor responses address some points. They clarify that their approach is the first with provable guarantees on violated constraints, using LP-type methods from geometry not applied before in metric learning. They mention prior methods use error functions, which can be fooled by adversarial inputs, as shown in Figure 3. They admit the problem's complexity isn't known but suspect NP-hardness. They also fix some typos and clarify definitions.\n\nNow, evaluating based on top-tier standards. Key factors: novelty, technical soundness, significance, clarity, and comparison with prior work.\n\nNovelty: The authors claim their approach is the first to use LP-type algorithms for this problem, which is a novel application from computational geometry. They also mention that minimizing the number of violations directly (as opposed to penalizing via error functions) is a different objective, which their experiments show handles adversarial cases better. This seems like a novel angle.\n\nTechnical Soundness: The reviewers point out some proof issues (Lemma 2.1 phrasing) but the authors addressed that. The FPTAS relies on existing LP-type problem results, which is solid. The experiments show favorable performance in adversarial cases, which is a strong point. However, the running time in Figure 4's non-monotonicity needs better explanation, which the authors provided (PCA affecting combinatorial structure), so that's addressed.\n\nSignificance: The problem of metric learning is important. Their contribution provides a new method with theoretical guarantees, which is significant. The FPTAS with nearly-linear time is a good result, especially if it's the first. The application to adversarial noise is a plus.\n\nClarity and Comparison: Reviewer 1 and 2 noted missing comparisons with recent works (Verma, Ye). The authors responded by explaining those works are in different settings (sample complexity vs. adversarial inputs, perturbations vs. their problem). That's a valid distinction, but the paper should have made that clearer. Also, the conclusion was missing, which the authors can fix.\n\nReviewer 3's criticism about experiments not showing accuracy improvements: The authors mention Figure 3 where their method handles adversarial cases better. Maybe the other experiments didn't show better accuracy in standard datasets, but the key is the theoretical guarantee and robustness. However, if the experiments on real-world data don't show better performance, that's a concern. The abstract says \"performs favorably in presence of adversarial noise,\" so maybe that's where the improvement is.\n\nThe main issues seem to be addressed in the author responses. The novelty is there, the technical approach is sound with theoretical backing, and the experiments support the claims. The missing comparisons were clarified as being in different problem settings, so that's okay. The conclusion can be added in the final version.\n\nReviewer 3's point about transforming into LP not being a big step might be valid, but applying LP-type algorithms from geometry to metric learning is a novel contribution. The fact that prior methods couldn't handle adversarial cases as well is a strong point.\n\nOverall, the paper has solid contributions, addresses reviewer concerns, and fills a gap in the literature with theoretical guarantees. The experiments, while maybe not showing across-the-board accuracy gains, do show robustness in adversarial scenarios, which is valuable. The technical aspects are sound once clarified. Therefore, it should be accepted.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\n\n1. **Novelty and Contribution**:  \n   The paper introduces a novel approach to Mahalanobis metric learning by leveraging LP-type problems and geometric approximation algorithms, which had not been applied in this context before. This is a significant contribution, as it bridges computational geometry and metric learning. The focus on minimizing the *number of violated constraints* directly, rather than using error functions, is a distinct and theoretically grounded objective. The authors demonstrate that this approach is robust to adversarial inputs (via Figure 3), which prior methods like ITML and LMNN fail to handle effectively. This addresses a critical gap in the literature, as existing methods often lack guarantees for worst-case scenarios.\n\n2. **Technical Soundness**:  \n   The core technical contribution—an FPTAS with nearly-linear running time for fixed dimensions—is well-founded in linear programming theory. While reviewers raised concerns about proof clarity (e.g., Lemma 2.1) and algorithmic details (e.g., function calls in Algorithm 1), the authors addressed these by clarifying definitions, fixing typos, and rephrasing arguments. The theoretical guarantees (e.g., approximation ratio) are rigorously established, and the experiments validate the method’s performance in adversarial settings. The non-monotonic running time in Figure 4 was explained via PCA’s impact on combinatorial structure, which is a reasonable justification.\n\n3. **Significance and Impact**:  \n   The problem of metric learning is fundamental, and the paper’s provable guarantees for adversarial inputs and efficient running time (especially in low dimensions) are impactful. The FPTAS is a strong theoretical result, and the parallelizability of the algorithm adds practical value. While the experiments may not show across-the-board accuracy improvements over existing methods on standard datasets, the robustness to adversarial noise is a compelling advantage in real-world applications where input quality cannot be assumed.\n\n4. **Clarity and Comparison with Prior Work**:  \n   Initial concerns about missing comparisons with recent works (e.g., Verma & Branson, Ye et al.) were addressed by the authors, who clarified that these prior methods focus on different problem settings (e.g., sample complexity, perturbations). The paper now better positions its contributions relative to existing work. The missing conclusion can be easily remedied in the final version. While the experiments could be more extensive, the adversarial robustness results are sufficiently compelling to justify the approach.\n\n5. **Reviewer Concerns Addressed**:  \n   The authors adequately addressed most reviewer critiques, including technical ambiguities, theoretical gaps (e.g., hardness assumptions), and experimental limitations. The key novelty and theoretical contributions remain intact after revisions.\n\n### Conclusion:  \nThe paper advances the state of the art by providing the first provable guarantees for minimizing violated constraints in Mahalanobis metric learning, with robustness to adversarial inputs. Its technical depth, novel application of geometric algorithms, and clear experimental validation in critical scenarios justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. They're proposing a method called Deep Interaction Processes (DIP) for modeling time-evolving graphs using temporal point processes. The key contributions seem to be handling multiple time resolutions with stacked LSTMs, time gates, selection mechanisms, and attention. The application is anti-fraud detection, which is a valid use case.\n\nLooking at the reviewer comments, there are several points raised. Reviewer 1 mentions that the contributions are incremental, especially the use of LSTM with time gates and stacked LSTMs. They also point out that the ablation study isn't thorough enough and that a key baseline (JODIE) was missing. Reviewer 2 agrees that the contributions aren't novel, noting prior work on recurrent networks for TPPs and temporal attention. They also question the log-likelihood equation and suggest comparing with standard benchmarks. Reviewer 3 found the paper hard to understand, lacking diagrams and clarity on how the model handles dynamic graphs and computational efficiency.\n\nThe author responses addressed some points. They added figures, clarified the TDG concept, fixed the log-likelihood equation, and included JODIE in experiments. They also explained the selection mechanism and computational aspects. However, some concerns remain, like the novelty of the contributions and the thoroughness of the ablation study.\n\nNow, considering the standards of a top-tier conference. The paper's main strengths are the empirical results showing better performance and the application to real-world fraud detection. The authors addressed some reviewer concerns by adding figures and comparisons. However, the reviewers' main criticisms about incremental contributions and lack of novel ideas are significant. The method combines existing techniques (LSTM, attention, point processes) without clear theoretical innovation. The ablation study, while improved, might still not be sufficient to demonstrate the necessity of each component. Also, the comparison with JODIE was added, but if the results don't show a substantial improvement over it, that's a problem.\n\nThe authors argue their approach's novelty lies in the combination and the TDG concept, but reviewers point out similar prior work. The computational efficiency claims (parallel training) are a plus, but need more detailed analysis. The paper's clarity improved with figures, but some technical details (like the exact role of parameters) remain unclear.\n\nIn top conferences, novelty and theoretical contributions are crucial. If the method is just a combination of existing techniques without clear new insights, it might not meet the bar. However, strong empirical results and practical benefits (like handling dynamic graphs in fraud detection) could tip the balance. The authors did address some issues, but the core criticism about incremental contributions remains. \n\nFinal Decision: Reject. The paper's contributions are too incremental, and while improvements were made, the lack of novel theoretical insights and insufficient ablation analysis compared to strong baselines make it less suitable for a top-tier conference. However, if the authors can significantly revise to address the novelty and analysis points, maybe a resubmission could be considered. But given the current state, rejection is warranted.\n</think>\n\nThe paper proposes a method called Deep Interaction Processes (DIP) for modeling time-evolving graphs using temporal point processes. While the approach combines existing techniques (e.g., LSTMs, attention mechanisms, and point processes) with novel components like time gates and a selection mechanism, its contributions are largely incremental. Key concerns raised by reviewers include insufficient novelty, inadequate ablation studies, and missing comparisons with critical baselines (e.g., JODIE). The authors addressed some issues (e.g., adding figures, fixing the log-likelihood equation, and including JODIE), but the core criticisms remain unresolved. \n\n**Strengths:**\n1. **Empirical Results:** The paper demonstrates improved performance over several baselines, including on a real-world financial task.\n2. **Practical Application:** The focus on anti-fraud detection is relevant and timely.\n3. **Clarity Improvements:** Added figures and explanations enhance readability, though some technical details (e.g., parameter roles) remain unclear.\n\n**Weaknesses:**\n1. **Novelty:** The core contributions (e.g., stacked LSTMs with time gates, attention mechanisms) are incremental compared to prior work (e.g., DeepCoevolve, JODIE, Trivedi et al.). The authors argue their TDG concept and parallel training are novel, but reviewers question their significance.\n2. **Ablation Analysis:** While improved, the ablation study lacks detailed exploration of hyperparameters (e.g., K, L, scaling parameters) and their impact on performance.\n3. **Baseline Comparison:** Though JODIE was added post-review, the comparison’s thoroughness and the paper’s failure to address other critiques (e.g., parameter count differences) weaken its claims of superiority.\n4. **Theoretical Justification:** The paper lacks rigorous analysis of why the proposed mechanisms (e.g., time gates, selection) are necessary or effective beyond empirical results.\n\n**Conclusion:** While the paper shows promise in practical applications and improves upon some methodological gaps, its incremental contributions and unresolved theoretical/analytical shortcomings make it insufficient for a top-tier conference. The lack of clear novelty and insufficient justification for key components are critical barriers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. The main contribution is a framework called Continuous Query Decomposition (CQD) for answering complex queries on incomplete Knowledge Graphs (KGs). They use neural link predictors and translate queries into differentiable objectives with logical operators handled via t-norms and t-conorms. The experiments show better performance than state-of-the-art methods with less training data. The authors also mention explainability and provide code and datasets.\n\nNow, looking at the reviewer comments. There are five reviewers here. Let me go through each one's points.\n\nReviewer 1 just says \"Your comment cleared my doubts\" but that's vague. Maybe they had concerns that were addressed?\n\nReviewer 2's summary is positive, calling the method elegant and effective. They note SOTA results and explainability but suggest more discussion on EPFO queries and the example given. They also ask about how KG incompleteness is handled and more context on the problem's importance. The author response addressed these points, adding examples and clarifying the evaluation setup. So their concerns were addressed.\n\nReviewer 3 is more technical. They question the difference between CQD and previous methods like GQE/Q2B. The author response explains that CQD decomposes queries into atomic parts and uses t-norms without needing to train on complex queries. They also mention that the combinatorial method's time complexity is exponential but manageable for current datasets. The reviewer also asks about inference time comparisons and the use of different link predictors. The authors added timing results and experiments with DistMult, showing their method still outperforms. The notation issues were addressed too. So the technical concerns seem addressed.\n\nReviewer 4 thinks the idea is simple but practical, appreciates the data efficiency but notes excessive math and wants clearer data usage details. The authors clarified that training is only on 1-hop queries and added timing info. They also did an ablation with TransE. The reviewer's main concerns about data and timing were addressed, and they still think it's a good paper.\n\nReviewer 5 is positive, highlighting the method's advantages over baselines in terms of data and explainability. They asked about embedding size and the use of e_A. The authors clarified embedding size as rank and explained why e_A isn't used. They also fixed the example and evaluation description. The reviewer's final note is that the paper is excellent and should be accepted.\n\nLooking at the author responses, they addressed most of the reviewers' concerns. They added experiments with DistMult, clarified notation, provided timing data, and fixed examples. The key points from the reviewers were about methodology differences, data efficiency, and explainability, which the authors handled well.\n\nThe paper's contributions seem solid: a novel method that outperforms SOTA with less data, handles complex queries including disjunctions, and offers explainability. The experiments are thorough across multiple datasets and baselines. The technical soundness is there, and the authors have addressed the methodological and experimental concerns raised.\n\nPotential weaknesses: The combinatorial method's exponential complexity might be an issue for longer queries, but the authors note that current datasets don't hit this and suggest possible optimizations. The continuous method's training time is higher but manageable with GPU parallelism. The explainability is a plus, which is a trend in top conferences.\n\nGiven that all reviewers, even the more critical ones, ended up supportive after the responses, and the authors have adequately addressed concerns, this seems like a strong candidate for acceptance. The paper meets the standards of a top-tier conference with clear contributions, thorough experiments, and solid methodology.\n</think>\n\nFinal Decision: Accept\n\nThe paper presents a novel and effective framework (CQD) for answering complex queries on incomplete knowledge graphs, leveraging neural link predictors and continuous optimization. Key strengths include state-of-the-art performance with significantly less training data compared to baselines, handling of disjunctive and existential queries, and explainability through intermediate variable assignments. The authors address reviewer concerns by clarifying methodology differences from prior work (e.g., GQE/Q2B), providing additional experiments (e.g., with DistMult), and improving clarity on data usage, timing, and notation. While the combinatorial method has exponential complexity for long chains, the authors note this is manageable for current benchmarks and suggest optimizations. The contributions are substantial, the experiments are thorough, and the technical soundness is validated. The paper meets the standards of a top-tier conference. \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"Deep Coherent Exploration For Continuous Control\" and it's about improving exploration in reinforcement learning, especially with deep networks. The key contributions are scaling up a previous method (Generalized Exploration from van Hoof et al. 2017) to deep RL by focusing on the last layer parameters, using a recursive inference step, and providing recipes for A2C, PPO, and SAC.\n\nLooking at the reviewer comments first. Reviewer 1 is positive, especially about the on-policy results and the method's scalability. They do note that the off-policy (SAC) part is weaker and maybe not as theoretically sound. They suggest reframing it as an on-policy method and accepting it. \n\nReviewer 2 is more critical. They point out that the method isn't a huge leap from prior work, especially van Hoof's. The empirical results on SAC aren't strong, and since SAC is state-of-the-art, the practical significance is questioned. They also mention that the experiments are limited to Mujoco, which might not be enough, and some baselines underperform vanilla methods, which could indicate tuning issues. \n\nReviewer 3 thinks the paper is well-written but points out some clarity issues in section 3. They also note that the method isn't novel but the scalability is a good contribution. The figures are hard to read, but that's fixable.\n\nReviewer 4 echoes some of the other points: the method follows van Hoof closely, the SAC results are mixed (even worse on HalfCheetah), and experiments are limited to Mujoco. They also mention that exploration experiments should include sparse reward tasks, which the paper lacks.\n\nThe author response addresses some concerns. They improved the SAC integration, which might help the off-policy results. They clarify that their method is for undirected exploration, not the hard exploration problems. They also mention that the SAC experiments are being updated, which could address some reviewer concerns. However, the main issues about the SAC performance and the limited domains remain.\n\nNow, considering the criteria for a top-tier conference. The paper's main contribution is scaling up a prior method to deep RL, which is a solid contribution. The on-policy results are strong, which is a plus. However, the off-policy (SAC) results are underwhelming, especially since SAC is a key modern method. The authors acknowledge this and are working on it, but the current submission's SAC results aren't convincing. \n\nThe experiments are all in Mujoco, which is standard, but the reviewers point out that even within Mujoco, the tasks are varied enough. Still, more domains might strengthen the case. The authors argue that undirected methods are still important, which is valid, but the lack of improvement on SAC (a top method) is a problem. \n\nOriginality is a concern since it's an extension of van Hoof's work. The authors clarify that their recursive update and last-layer approach are novel contributions, which is fair. But the reviewers feel it's not a huge leap. \n\nThe clarity issues can be fixed in the camera-ready version, so that's manageable. The main issues are the SAC performance and the limited empirical validation. The authors are addressing the SAC part, but without seeing the updated results, it's hard to judge. However, the current submission's SAC results are mixed, which is a red flag for a top conference. \n\nTop-tier conferences usually require strong empirical results across a range of methods and tasks. Since the method doesn't improve SAC significantly and even degrades on some tasks, that's a problem. The on-policy results are good, but maybe not enough to offset the off-policy shortcomings. \n\nThe authors' response mentions that they're working on a better SAC integration, but unless those results are compelling, the paper might not meet the bar. Since the current submission's SAC results are weak, and the method's novelty is incremental, I'm leaning towards reject. However, the on-policy contributions are solid. But top conferences often require a more comprehensive impact. \n\nWait, Reviewer 1 was tentatively accepting, but others have more reservations. The key is whether the contributions are significant enough despite the SAC issues. The scalability to deep networks is a good point, but if it doesn't work well with leading methods like SAC, maybe it's not a major advance. \n\nAlternatively, if the authors can show that their method works better with SAC in the updated experiments, that could tip the balance. But based on the current info, the SAC results are underwhelming. \n\nFinal call: The paper has merits but the off-policy performance and limited experiments (even within Mujoco) might not be sufficient for a top conference. The method's novelty is incremental, and the SAC results are a critical weakness. So I think it should be rejected unless the SAC improvements are substantial, which isn't clear from the provided info.\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\n1. **Contributions and Novelty:**  \n   The paper builds on van Hoof et al. (2017) by extending Generalized Exploration (GE) to deep RL through a focus on the last layer parameters and a recursive inference step. While these are valid technical contributions, reviewers note that the core idea is not fundamentally novel. The authors clarify that their scalable implementation and analytical gradients for non-Markov policies are key innovations, but the incremental nature of the work may not meet the high novelty bar of a top-tier conference.\n\n2. **Empirical Results:**  \n   - **On-Policy Methods (A2C, PPO):** Strong performance improvements are demonstrated, which is a significant strength.  \n   - **Off-Policy Methods (SAC):** Mixed results, with degradation on HalfCheetah and only marginal gains on other tasks. The authors acknowledge the heuristic nature of their initial SAC integration and propose improvements, but the current submission’s results are underwhelming. Since SAC is a state-of-the-art method, the lack of clear benefits here weakens the paper’s practical impact.  \n   - **Experimental Scope:** All experiments are confined to MuJoCo tasks, which, while standard, are criticized for being insufficiently diverse. The authors argue that MuJoCo tasks vary sufficiently, but additional domains (e.g., sparse-reward or non-Mujoco environments) would strengthen the case for broader applicability.\n\n3. **Theoretical and Methodological Concerns:**  \n   - The off-policy formulation for SAC is criticized for being heuristic and lacking the theoretical rigor of the on-policy approach. The revised SAC integration proposed in the author response (minimizing KL divergence to the Q-function softmax) is promising but untested in the current submission.  \n   - The method’s sensitivity to hyperparameters (e.g., α and δ) and the underperformance of baselines (e.g., NoisyNet-A2C) raise questions about experimental fairness and reproducibility.\n\n4. **Clarity and Presentation:**  \n   While the paper is generally well-written, Section 3 is overly technical and dense, making it inaccessible to non-experts. The authors commit to moving algorithmic details to the appendix and improving clarity, which can be addressed post-acceptance. However, this does not resolve the core empirical and methodological issues.\n\n5. **Broader Impact and Significance:**  \n   The authors emphasize undirected exploration’s relevance for simplicity and ease of implementation. However, the lack of improvement over SAC—a dominant method in continuous control—limits the method’s practical significance. Directed exploration methods (e.g., for sparse-reward tasks) are not addressed, further narrowing the paper’s scope.\n\n**Conclusion:**  \nWhile the paper makes a solid contribution to scaling GE to deep RL and shows promise for on-policy methods, the underwhelming SAC results, limited experimental validation, and incremental novelty relative to van Hoof et al. (2017) prevent it from meeting the high bar for acceptance at a top-tier conference. The proposed improvements to SAC in the author response are encouraging but insufficiently validated in the current submission. Without stronger off-policy performance or broader empirical validation, the paper does not yet demonstrate the transformative impact required for acceptance."
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by understanding the paper's main contributions and the reviewers' feedback.\n\nThe paper's title is \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream.\" The abstract mentions they're trying to reduce the number of supervised updates required for a model to match the primate ventral stream, using three strategies: fewer training epochs, better initialization, and training only a subset of synapses. They claim that combining these reduces supervised updates by two orders of magnitude while maintaining ~80% of the match to the brain.\n\nNow, looking at the reviewers' comments. Reviewer 1 and 2 have significant concerns about the interpretation of the BrainScore metric. They argue that the paper's claims about matching the brain are misleading because BrainScore's correlation doesn't directly equate to biological similarity. For example, if BrainScore 1.0 doesn't mean perfect match, then 80% of that score might not mean 80% similarity. The authors addressed this by clarifying the metric's limitations but reviewers still feel the wording is problematic. \n\nReviewer 3 is critical of the biological plausibility of the methods, like the weight compression initialization. They question why this method is a valid model of infant brains and note the lack of theoretical foundation. The authors responded by stating they're not claiming it's exactly how the brain does it, but exploring hypotheses. They also provided statistical evidence that their method outperforms standard initialization. \n\nReviewer 4 is positive, seeing the problem as important and the methods as a good contribution, recommending acceptance. Reviewer 5 has mixed views, appreciating the empirical approach but questioning the biological assumptions and some methodological choices. They also pointed out some numerical inaccuracies, which the authors addressed.\n\nThe authors' responses seem to have addressed some points, like adding statistical tests and clarifying the BrainScore interpretation. They also acknowledged the limitations of their biological analogies but defended their approach as a step towards more biologically plausible models.\n\nNow, considering the standards of a top-tier conference. The paper's main contribution is reducing the number of supervised updates needed for a model to achieve brain-like performance. The methods are novel and the experiments are thorough. However, the core issue is the reliance on BrainScore as a proxy for biological similarity. The reviewers' concerns about overinterpreting the metric are valid. If the metric's validity isn't well-established, the claims about matching the brain's ventral stream are weakened. \n\nThe authors did make efforts to address this by clarifying the limitations and providing additional analyses, but Reviewer 2 still feels the claims are misleading. The paper's significance hinges on how well BrainScore correlates with actual biological similarity. Without stronger evidence for that, the conclusions might be overstated.\n\nOn the other hand, the problem addressed is important, and the techniques (like weight compression and critical training) are interesting in their own right, even if the biological interpretation is uncertain. The positive reviews from Reviewer 4 and parts of Reviewer 5 suggest the work has value beyond the biological angle, such as in model efficiency.\n\nThe key question is whether the paper meets the conference's standards despite these limitations. Top-tier conferences often accept papers that push the field forward even if not perfect. The authors have made improvements based on feedback, and the work is a step towards making models more biologically plausible. The methodological contributions (reducing updates, initialization techniques) are solid. The biological interpretation is a risk, but the paper can be accepted with clear caveats, which the authors have now included.\n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\nThe paper addresses an important problem in computational neuroscience: reconciling the success of deep neural networks (DNNs) in modeling the primate ventral stream with their biological plausibility, particularly regarding the excessive reliance on supervised learning. The authors propose three strategies to reduce the number of supervised synaptic updates required to achieve brain-like performance, demonstrating a two-orders-of-magnitude reduction in updates while retaining ~80% of the BrainScore of the fully trained model. The contributions are methodologically novel and technically sound, with thorough experiments and clear explanations of the strategies (e.g., weight compression, critical training). \n\n#### Key Strengths:\n1. **Problem Relevance**: The paper tackles a critical gap in current DNN models—biological plausibility of learning mechanisms—and provides concrete steps toward addressing it. This aligns with the conference's focus on advancing both machine learning and neuroscience.\n2. **Methodological Innovations**: \n   - **Weight Compression (WC)**: Introduces a novel initialization method that leverages compressed weight distributions to achieve higher baseline performance without training, supported by statistical validation against standard initialization.\n   - **Critical Training (CT)**: Proposes a selective parameter update strategy that reduces computational and biological costs while maintaining performance.\n   - **Generalization Across Architectures**: Demonstrates that these methods work across different models (e.g., ResNet-50, MobileNet), suggesting broader applicability.\n3. **Thorough Analysis**: The experiments are well-designed, with ablation studies and comparisons to baselines. The authors address reviewer concerns by adding statistical tests (e.g., comparing WC vs. Kaiming Normal initialization) and clarifying ambiguities in figures.\n\n#### Addressed Concerns:\n- **BrainScore Interpretation**: The authors revised the manuscript to explicitly clarify that BrainScore is a proxy metric and does not directly equate to biological similarity. While reviewers remain cautious, the authors’ acknowledgment of this limitation and their efforts to contextualize results mitigate overinterpretation risks.\n- **Biological Plausibility**: The authors frame their methods as exploratory hypotheses rather than definitive biological models. They emphasize that the goal is to reduce the gap between DNNs and biological systems, not to claim exact correspondence. Additional statistical validations (e.g., WC outperforming Kaiming Normal) strengthen their case for methodological merit.\n- **Technical Issues**: Numerical inaccuracies and figure discrepancies were addressed, and the authors provided clarifications on terminology (e.g., \"supervised updates\") and biological assumptions (e.g., initialization strategies).\n\n#### Remaining Concerns:\n- **BrainScore Validity**: Reviewers’ critiques about BrainScore’s limitations as a metric for biological similarity remain valid. However, the authors appropriately frame their results as contributions to model efficiency and developmental plausibility rather than definitive biological validation. This aligns with the paper’s stated goals.\n- **Theoretical Foundations**: While some reviewers questioned the biological rationale for methods like WC, the authors clarify that their work is a proof-of-concept exploring plausible hypotheses. This is acceptable for a conference paper, especially given the empirical rigor and potential impact.\n\n#### Conclusion:\nThe paper’s methodological contributions and clear progress toward biologically plausible models outweigh the remaining limitations. The authors have addressed key critiques through revisions, and the work represents a meaningful step forward in aligning DNNs with neuroscientific constraints. Accepting this paper would encourage further exploration of efficient, biologically inspired learning strategies, which is critical for advancing both machine learning and computational neuroscience."
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. I need to go through all the provided information carefully.\n\nStarting with the paper's title and abstract. The authors propose a Generalized Probability Kernel (GPK) for discrete distributions, aiming to unify existing methods like MMD and probability product kernels. They mention applications in two-sample tests and connecting distribution-property estimation with kernel-based hypothesis testing. Sounds ambitious, but the reviewers have some major issues.\n\nLooking at the reviewer comments first. Reviewer 1 points out several critical problems. The key definition (Definition 2) has multiple issues: confusion between kernel K and k, mixing up the spaces (Y vs R^n), and questions about the phi function's placement in the RKHS. Also, missing proofs, incomplete sections like simulations, and missing connections to Bernstein polynomials. \n\nReviewer 2 is harsher. They say the paper is underdeveloped, with no experiments, weak theory (just rehashing existing stuff), and poor presentation with many typos. The authors didn't show why their estimators are useful or provide asymptotic analysis, which is crucial for two-sample testing. \n\nReviewer 3 adds more technical issues. The setting where discrete labels are in R^k is questionable because discrete variables usually don't have inherent distances. They also point out undefined notations, like the Stein operator in a discrete context, and inconsistencies in definitions. The paper lacks practical examples to motivate their approach, making the relevance unclear.\n\nReviewer 4 mentions confusion in notation between kernels on distributions vs samples, and questions about how the method applies when distributions are unknown (since two-sample tests typically don't know p and q). They also note that the KSD part had flawed proofs, which the authors removed in the revision. \n\nNow, the author response addresses some points. They removed the KSD section due to incorrect proofs, focused on polynomial GPK, clarified notation, and tried to fix typos. They argue their framework generalizes MMD and provides unbiased estimators with convergence bounds. They also mention a new \"power-MMD\" as an application. \n\nBut even after revisions, there are lingering issues. Reviewers 1 and 2 emphasized missing experiments and incomplete theory. The authors mention simulations were missing, but in the response, they talk about new results in the revision but don't specify if experiments were added. The theoretical contributions are questioned as being too incremental or just rephrasing existing work. \n\nThe presentation issues (typos, grammar) were addressed, but the core technical flaws remain. For instance, Reviewer 3's point about the discrete labels needing a distance structure isn't fully resolved. The authors mention word2vec as an example, but that's a stretch since word embeddings are learned, not inherent to discrete labels. \n\nThe key problem is that the paper's main claims—unifying MMD and other methods, providing a general framework—aren't sufficiently backed by rigorous proofs, experiments, or clear examples. The reviewers' consensus is that the paper is incomplete, with major gaps in both theory and empirical validation. Even after revisions, the authors didn't address all the critical points, especially the lack of experiments and the foundational issues in their definitions. \n\nTop-tier conferences require strong theoretical contributions and empirical validation. This paper seems to fall short on both counts. The framework's novelty is questionable, and the execution has too many unresolved issues. Without concrete experiments showing the method's effectiveness and a solid theoretical foundation, it's unlikely to meet the standards.\n</think>\n\nThe paper proposes a Generalized Probability Kernel (GPK) for discrete distributions, aiming to unify existing discrepancy measures like MMD and extend them. However, the reviewers' critiques and the authors' responses reveal significant unresolved issues that undermine its suitability for a top-tier conference:\n\n### Key Issues:\n1. **Theoretical Gaps**:\n   - **Definition Flaws**: Reviewers highlighted critical ambiguities in core definitions (e.g., confusion between kernels and Gram matrices, undefined notations like $\\phi$, and unclear connections to RKHS). While the authors addressed some notation issues in the revision, foundational problems (e.g., the Stein operator in discrete settings, undefined operators like $\\Delta^*$) remain unresolved.\n   - **Missing Proofs and Analysis**: Key proofs (e.g., Theorem 5) were absent or incomplete. The authors removed the KSD section due to flawed proofs but did not fully address gaps in the remaining theory (e.g., justification for calling GPK a \"kernel\" without distance properties, lack of asymptotic analysis for estimators).\n   - **Novelty and Relevance**: The framework’s novelty is questionable. Reviewers noted that results in Sections 4.2 and 4.3.1 were trivial or known, and the motivation for the discrete setting (e.g., labels in $\\mathbb{R}^k$) was poorly justified. The authors’ example (word2vec) is tangential, as discrete labels in most applications lack inherent distances.\n\n2. **Empirical Shortcomings**:\n   - **No Experiments**: Reviewers emphasized the absence of empirical validation. While the authors mentioned \"new results\" in the revision, there is no evidence of added experiments or comparisons to baselines. Without empirical validation, claims about the utility of GPK (e.g., for two-sample tests) remain unproven.\n   - **Practical Feasibility**: Reviewer 4’s concern about requiring known distributions $p$ and $q$ for two-sample testing was not adequately addressed. The authors’ response about \"plugin estimators\" is vague and lacks concrete examples or guarantees.\n\n3. **Presentation and Rigor**:\n   - **Clarity and Precision**: Despite revisions, the paper suffers from persistent ambiguities (e.g., undefined terms like \"pmfs,\" inconsistent notation). The framework’s generality (\"little constraints\") leads to conceptual confusion (e.g., mixing kernels over distributions and samples).\n   - **Grammar and Typos**: While the authors claim to have fixed typos, the original review highlighted pervasive issues that likely persist in the revised version, hindering readability.\n\n4. **Lack of Clear Contributions**:\n   - The \"power-MMD\" extension is underdeveloped. The authors removed the KSD analysis due to errors and failed to demonstrate how GPK advances the field beyond rephrasing existing methods (e.g., MMD). The connection to distribution-property estimation remains superficial.\n\n### Conclusion:\nThe paper’s core contributions are undermined by unresolved theoretical flaws, lack of empirical validation, and unclear motivation. While the authors addressed some surface-level issues (notation, typos), the fundamental gaps in rigor, novelty, and practicality remain. Top-tier conferences demand robust theoretical foundations, empirical validation, and clear demonstrations of impact—all of which are missing here. The work appears premature and requires substantial reworking before it can meet the standards of a leading venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Disentangling Representations of Text by Masking Transformers.\" The abstract mentions using binary masks over transformer weights or hidden units to find subnetworks that encode specific factors of variation, like syntax vs. semantics or sentiment vs. genre. They compare their method to VAEs and adversarial training, claiming better performance. The experiments are on movie reviews with spurious correlations and syntax/semantic tasks.\n\nLooking at the reviewers' comments first. Reviewer 1 is concerned about the generality of the method. They ask if it's just for specific cases or more general. The author response addresses this by saying the method is applicable where robustness is needed, which is common. They also mention experiments on two different tasks, which might help, but the reviewer still might want more domains tested.\n\nReviewer 2 points out that while the approach is novel, the paper doesn't clearly show superiority over VAEs and lacks benchmark comparisons. The authors added VAE baselines after the review, which is good. But the reviewer also questions the speedup from masking vs. fine-tuning. The authors clarify that masks don't save time but use less memory. The reviewer also asks about layer masking specifics, which the authors addressed by noting they mask only the last 9 layers. However, the main issue here is the impact and whether the results are strong enough for a top conference.\n\nReviewer 3 mentions the triplet loss limits to two factors, possible attribute leakage, increased parameters, and multiple forward passes. The authors acknowledge the two-factor limitation but say it's a future direction. They also mention using an MLP probe for leakage, which might not be strong enough. The parameter increase is a valid concern, but the authors didn't address it further except noting the binary masks. The reviewer also suggests experiments on masking only later layers, which the authors did, so that's covered.\n\nReviewer 4 likes the idea but wants more domains tested and better comparison with fine-tuned models. The authors added more experiments and clarified that their method outperforms fine-tuned in robustness. The reviewer also points out missing citations, which the authors fixed. The concern about Figure 4's results is addressed by the authors pointing out that their focus is on disentanglement, not SOTA on benchmarks. The reviewer's point about weight vs. hidden unit masking is acknowledged but not fully resolved, though the authors explain the difference.\n\nReviewer 5 questions the experimental setup with only two genres and the effectiveness against baselines. The authors explain their choice of genres to maximize spurious correlations and clarify the figure misinterpretation. They also mention existing downstream tasks like STS. The reviewer's point about more genres is addressed by the authors planning to add more in the camera-ready, which is a positive step.\n\nThe author responses seem to address many concerns. They added VAE baselines, clarified experiments, and provided more details on masking layers. However, some points remain, like the generality across more domains and the parameter increase. But the authors argue their method's novelty and the importance of disentanglement for robustness, which is a valid point.\n\nTop-tier conferences usually require strong contributions, clear comparisons, and evidence of broader applicability. The paper introduces a novel method for disentanglement without fine-tuning, which is computationally efficient in memory. The experiments show promise in specific tasks, and the authors have addressed several reviewer concerns. However, some limitations remain, like the two-factor limitation and the need for more domains. But since the authors are planning to include more experiments and have clarified key points, the work seems solid enough for acceptance, especially given the innovation in the approach and the demonstrated effectiveness in their experiments. The main competition is with existing methods, which they now compare against better. The lack of SOTA on standard benchmarks isn't a deal-breaker if the contribution is in methodology and disentanglement specifically.\n\nFinal Decision: Accept\n</think>\n\nThe paper proposes a novel method for disentangling representations in pre-trained transformers by learning binary masks over weights or hidden units. This approach aims to isolate distinct factors of variation (e.g., syntax vs. semantics, sentiment vs. genre) without fine-tuning the model, which is a compelling contribution to robustness and interpretability in NLP. Key strengths include:\n\n1. **Novelty and Innovation**: The method introduces a fresh angle on disentanglement by leveraging masking in pre-trained models, sidestepping the need for retraining. This is a promising direction for efficient, robust model adaptation.\n\n2. **Empirical Validation**: The experiments demonstrate effectiveness in disentangling representations on specific tasks (e.g., sentiment/genre, syntax/semantics). The authors address reviewer concerns by adding comparisons to VAE baselines and clarifying results, showing their method outperforms adversarial and fine-tuning baselines in key scenarios.\n\n3. **Theoretical and Practical Relevance**: The focus on robustness to spurious correlations aligns with critical challenges in NLP. The method’s ability to achieve disentanglement with minimal computational overhead (via masking) is a practical advantage, even if training masks is not faster than fine-tuning.\n\n**Addressed Concerns**: \n- The authors clarified the generality of their approach by emphasizing its applicability to robustness tasks and noting experiments across two distinct domains (sentiment/genre and syntax/semantics). \n- They added VAE baselines and addressed Figure 3/4 misinterpretations, reinforcing their claims. \n- Implementation details (e.g., masking only later layers) were clarified, and future work on scalability and parameter efficiency was acknowledged.\n\n**Remaining Limitations**:\n- The method currently handles only two factors of variation, though the authors note this as a future direction.\n- The parameter increase from binary masks and the need for multiple forward passes are valid concerns, but the authors argue these trade-offs are acceptable for the benefits of disentanglement.\n\n**Conclusion**: While the paper has room for improvement (e.g., broader domain experiments, addressing scalability), the core idea is novel, the empirical results are convincing, and the contributions align with the conference’s focus on advancing foundational methods. The authors have adequately addressed major reviewer concerns, and the work’s potential impact on disentanglement and robustness justifies acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about multitask learning, specifically a method called αVIL that dynamically adjusts task weights during training using model updates. The key idea is to use the differences in model parameters after single-task updates to determine task importance. The experiments are on computer vision (MultiMNIST) and NLU tasks.\n\nLooking at the reviewers' comments first. Reviewer 1 says the paper is well-written but lacks theoretical justification and the experiments aren't convincing. The baselines are limited, and the results don't consistently outperform them. They want more theory and better empirical validation.\n\nReviewer 2 points out that the method is ad hoc, not clearly different from existing approaches like DIW or meta-learning. The experiments show mixed results, with no clear superiority. They also mention issues with the algorithm's intuition (like the subtraction of 1 in line 11) and suggest the need for more ablations and statistical tests.\n\nReviewer 3 is also critical, noting that the algorithm's novelty isn't clear, the experiments use only one auxiliary task in vision, and the results are not significantly better than baselines. They suggest rejection due to insufficient improvements and lack of thorough analysis.\n\nThe authors responded by addressing some points. They explained the α parameter's role, clarified the MultiMNIST choice, and mentioned adding more stats in the camera-ready. They also acknowledge the need for more theory and experiments but note space constraints.\n\nNow, considering the standards of a top-tier conference. Such venues require strong contributions, clear novelty, solid theoretical grounding, and convincing empirical results. The paper's main contribution is the αVIL method, which dynamically adjusts task weights via model updates. However, the reviewers highlight that the method isn't sufficiently novel compared to existing dynamic weighting approaches like DIW or meta-learning. The authors argue it's different because it uses meta-optimization on model updates, but the reviewers aren't convinced of the distinction.\n\nThe experiments are a concern. On MultiMNIST, the results are close to baselines, and on NLU tasks, the improvements are minor and sometimes not statistically significant. The authors mention that multitask learning on NLU tasks often has small gains, but even so, the paper needs to show a clear advantage. The lack of consistent outperformance across tasks weakens the case.\n\nTheoretical justification is missing, which is a big issue for a top conference. Without a formal analysis or proof of convergence, the method's effectiveness is harder to trust. The authors plan to add this in the camera-ready, but the current submission lacks it, which is a problem for acceptance.\n\nThe reviewers also point out that the experimental setup is limited, especially with only one auxiliary task in vision. The authors explain their choices but acknowledge that more experiments would help. The lack of ablation studies and detailed analysis of learned weights (like sanity checks with random labels) makes it hard to assess the method's robustness.\n\nThe writing and clarity are praised, which is a plus, but not enough to overcome the other issues. The method's novelty is questionable, the experiments don't convincingly show superiority, and the theoretical backing is absent. While the authors have plans to improve the paper, the current submission doesn't meet the high bar of a top-tier conference.\n\nConsidering all this, the paper's contributions are incremental and the results aren't strong enough. The reviewers' consensus leans towards rejection, especially since the experiments don't clearly demonstrate the method's effectiveness beyond existing baselines. Even with the authors' responses, the unresolved issues in novelty and empirical validation are critical.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Novelty and Contribution**: \n   - While αVIL introduces a dynamic task weighting approach, reviewers highlight that it is not sufficiently distinct from existing methods like DIW or meta-learning frameworks (e.g., MAML). The authors argue their method uses meta-optimization of task weights via model updates, but reviewers remain unconvinced of its novelty. The lack of a clear theoretical justification further weakens the claim of innovation, which is critical for a top-tier conference.\n\n2. **Empirical Results**:\n   - The experiments show only marginal improvements over baselines. On MultiMNIST, results are statistically indistinguishable from competitors, and on NLU tasks, performance is mixed with no consistent superiority. The authors acknowledge that multitask learning on NLU tasks often yields small gains, but the paper must still demonstrate a clear advantage over existing methods. The absence of statistically significant results and the limited experimental setup (e.g., single auxiliary task in vision) undermine the empirical validation.\n\n3. **Theoretical Grounding**:\n   - A major gap is the lack of theoretical analysis or formal justification for the proposed method. Top-tier conferences typically require at least preliminary theoretical insights to support empirical claims. The authors commit to addressing this in a revised version, but the current submission lacks this critical component.\n\n4. **Methodological Concerns**:\n   - Reviewers question the intuition behind key design choices (e.g., the subtraction of 1 in Algorithm 1) and the algorithm’s ad hoc nature. The authors clarify some points in their response, but unresolved ambiguities remain. Additionally, the absence of ablation studies and sanity checks (e.g., testing with random labels) limits confidence in the method’s robustness.\n\n5. **Experimental Rigor**:\n   - The experimental setup is underwhelming. The MultiMNIST experiments use only one auxiliary task, and NLU results lack consistent statistical reporting (e.g., standard deviations for test sets). While the authors plan to address these in a revised version, the current submission does not meet the empirical rigor expected at a top-tier venue.\n\n### Conclusion:\nThe paper’s contributions are incremental, and the empirical results fail to convincingly demonstrate a clear advantage over existing methods. Combined with the lack of theoretical justification and unresolved methodological concerns, the submission falls short of the high standards required for acceptance at a top-tier conference. While the authors’ responses indicate potential improvements, the current version does not sufficiently address the reviewers’ core criticisms."
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through the paper and the reviews to figure out if it should be accepted. The paper is about distributed mean estimation (DME) and variance reduction, which are important in distributed machine learning. The key contribution seems to be a new method using lattice theory to achieve better error bounds that don't depend on the input norm but instead on the variance between inputs. That's a good problem because existing methods might not handle cases where the mean itself has a large norm but the inputs are close to each other.\n\nLooking at the reviewer comments, there's a mix of positive and some constructive criticism. Reviewer 1 and 2 think the problem is important and the approach is interesting but point out some issues. Reviewer 3 is positive but mentions some minor issues like experiments only on n=2. Reviewer 4 is overall supportive but notes that the practical algorithm doesn't match the optimal bounds and constants might matter.\n\nThe authors addressed most of the concerns in their response. They clarified some definitions, acknowledged the need for better experiments with larger n (though they mention some in the appendix), and explained the trade-offs between theoretical and practical algorithms. They also plan to make revisions based on the feedback.\n\nNow, considering the criteria for a top-tier conference: the problem is fundamental, the solution introduces a novel approach with lattices, and they have theoretical guarantees with lower bounds. The experiments, though limited in the main text, do exist in the appendix. The main weaknesses are the practical algorithm not matching the optimal bounds and the experiments needing more data points. However, the authors are addressing these in revisions. The reviewers' major concerns seem to be manageable with the proposed changes. The paper's theoretical contributions are strong, and the practical aspects are acknowledged with plans to improve clarity and experiments. Since top conferences value both theoretical novelty and practical relevance, and the authors are addressing the gaps, I think this should be accepted. The cons are not deal-breakers given the overall contribution.\n</think>\n\nThe paper addresses a fundamental problem in distributed machine learning—distributed mean estimation (DME)—and introduces a novel approach leveraging lattice theory to achieve error bounds dependent on input variance rather than input norm. This is a significant improvement over prior work, which often suffered from suboptimal guarantees when inputs are concentrated around a high-norm mean. The theoretical contributions include asymptotically optimal communication-to-error trade-offs, lower bounds, and practical extensions using cubic lattices. The experiments, while limited in the main text, are supplemented in the appendix and demonstrate practical improvements. \n\n### Key Strengths:\n1. **Novelty and Impact**: The use of lattice theory for DME is a fresh angle, offering a clean solution to a well-known problem. The decoupling of error bounds from input norms is a critical advancement for real-world applications where inputs may be concentrated around a large-norm mean.\n2. **Theoretical Rigor**: The paper provides matching upper and lower bounds, demonstrating optimality. The reduction from ℓ₂ to ℓ∞ norms and the analysis of computationally tractable cubic lattices further strengthen the theoretical foundation.\n3. **Practical Relevance**: The authors acknowledge computational challenges and propose a practical algorithm (with logarithmic loss) that is experimentally validated. The discussion of trade-offs between theoretical optimality and practical implementation is thorough.\n\n### Areas for Improvement:\n1. **Experimental Evaluation**: While experiments in the main text focus on small n (e.g., n=2), the appendix includes results for larger n (8–16 machines). The authors commit to addressing this in revisions by moving some experiments to the main body.\n2. **Algorithmic Gaps**: The practical algorithm does not match the optimal theoretical bounds, and constants in the asymptotic analysis may matter in practice. The authors clarify that this trade-off is intentional and that the theoretical and practical versions are distinct, which is reasonable given computational constraints.\n3. **Clarity and Technical Details**: Minor issues in definitions (e.g., packing radius) and writing were identified but are resolvable through revision. The authors’ responses demonstrate a clear plan to address these.\n\n### Reviewer Feedback:\n- **Reviewer 1** and **2** highlight the problem’s importance and the method’s novelty but suggest improvements in literature review and clarity. Their concerns are addressed in the author response.\n- **Reviewer 3** praises the clarity and motivation but notes minor experimental limitations and technical ambiguities. The authors commit to fixing these.\n- **Reviewer 4** is broadly supportive but flags the gap between theoretical and practical algorithms. The authors explain this trade-off and emphasize that the theoretical guarantees are still valuable.\n\n### Conclusion:\nThe paper’s theoretical contributions are strong and impactful, addressing a critical gap in DME. While practical limitations and minor experimental gaps exist, the authors have a clear path to address these in revisions. The problem’s significance, the method’s innovation, and the thoroughness of the analysis align with the standards of a top-tier conference. The constructive feedback does not outweigh the paper’s merits, and the authors’ responses demonstrate a commitment to addressing concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions.\" The abstract mentions integrating language throughout the bottom-up visual pathway, not just top-down attention, and shows improvements on several datasets. \n\nLooking at the reviewers' comments, there's a mix of positive and negative feedback. Reviewer 3 is very supportive, praising the approach and results, while Reviewers 1, 2, and 4 have significant concerns. \n\nReviewer 1 points out that the novelty might be thin since they didn't explore many language modeling techniques. They also mention some issues with figures and ablation study clarity. The author response addressed some of these by adding figures and citations, but the core concern about novelty remains. \n\nReviewer 2's main issue is the empirical results. The model doesn't match existing works on some test sets, suggesting possible over-tuning. The authors argue they didn't overfit by explaining their validation setup, but the reviewer remains unconvinced. Also, there's a discrepancy in tables that the authors couldn't resolve, which is a red flag. \n\nReviewer 4 is critical about the conceptual motivation and significance. They want a clearer rationale for why integrating language at low levels helps, and better analysis of how exactly the model works. The authors mention ablation studies but the reviewer feels this isn't enough. The lack of statistical significance tests is another point—authors said they didn't do them because it was time-consuming and others didn't, but that's a weak excuse for a top conference. \n\nThe author responses tried to address some points. They clarified the model's tuning process, mentioned ablation results, and discussed future analyses. However, the key issues like statistical tests, unclear innovation over Step-ConvRNN, and unresolved table discrepancies remain. \n\nReviewer 3's strong support is notable, but the other reviewers' points are substantial. For a top-tier conference, the paper needs to have a clear, significant contribution and solid empirical validation. The main contribution here is the dual modulation approach, but the results aren't consistently better across all datasets. The ablation studies help, but the lack of statistical tests and the over-tuning concern weaken the empirical case. \n\nThe novelty is also in question. Reviewer 4 felt the model isn't sufficiently innovative compared to existing work like Step-ConvRNN. The authors argue their approach allows better ablation studies, but if the architecture isn't novel enough, that's a problem. \n\nThe unresolved issues in tables and the missing statistical tests are methodological flaws that a top conference would expect to be fixed. The authors didn't address the reference to Mei et al (2018) properly, which Reviewer 2 pointed out, indicating possible gaps in related work. \n\nConsidering all this, the paper has promising ideas but falls short in empirical rigor and clarity of contribution. The mixed results and unresolved methodological issues make it risky for acceptance at a top venue where standards are high. While there's potential, the current submission doesn't meet the required benchmarks.\n</think>\n\nThe paper proposes integrating language modulation into both top-down and bottom-up visual processing pathways for referring expression understanding, arguing that this approach improves performance over existing methods that focus primarily on top-down attention. While the work presents an interesting hypothesis and shows some empirical improvements, several critical issues undermine its readiness for a top-tier conference:\n\n### Key Strengths:\n1. **Novel Hypothesis**: The idea of modulating *both* bottom-up and top-down pathways with language is conceptually novel and addresses a gap in current multimodal models that often neglect early visual processing stages.\n2. **Ablation Studies**: The ablation results showing performance drops when removing either modulation pathway provide some evidence for the necessity of their approach.\n3. **Reviewer 3’s Support**: One reviewer strongly endorses the work’s significance and empirical results, highlighting its potential impact on vision-language tasks.\n\n### Major Concerns:\n1. **Empirical Weaknesses**:\n   - **Inconsistent Results**: The model underperforms state-of-the-art methods on key test sets (e.g., UNC testA/B, ReferIt), raising questions about its generalizability. The authors attribute this to validation set tuning but do not convincingly rule out overfitting.\n   - **Lack of Statistical Rigor**: No statistical significance tests are provided to validate claims of improvement, despite reviewers’ requests. This is a critical omission for a top-tier venue.\n   - **Table Discrepancies**: Unresolved inconsistencies between Table 1 and Table 2 (e.g., LSCM/Step-ConvRNN results) undermine confidence in the reported results.\n\n2. **Conceptual and Methodological Gaps**:\n   - **Motivation**: The paper lacks a clear, theory-driven rationale for why language should influence low-level visual processing. While cognitive science references are cited, the connection to the model design is underdeveloped.\n   - **Novelty**: Reviewers question the distinction from prior work (e.g., Step-ConvRNN), with the authors’ rebuttal failing to clarify unique contributions beyond minor architectural tweaks.\n   - **Model Interpretability**: The authors acknowledge the need for better analysis (e.g., visualizing language’s effect on low-level features) but have not provided sufficient evidence to support their claims.\n\n3. **Writing and Presentation**:\n   - **Figure Clarity**: Reviewers flagged unintuitive figures (e.g., Figure 1) and confusing descriptions (e.g., splitting textual representations without justification). While some fixes were promised, the core explanations remain unclear.\n   - **Citation Omissions**: Missing references (e.g., Mei et al. 2018) and incomplete literature reviews weaken the paper’s foundation.\n\n### Author Rebuttal Analysis:\nThe authors addressed minor issues (e.g., adding citations, revising figures) but failed to resolve critical concerns:\n- They did not provide statistical tests or clarify the rationale for splitting textual representations.\n- Their defense of the model’s novelty over Step-ConvRNN hinges on ablation capabilities, which is insufficient for a top-tier contribution.\n- The unresolved table discrepancies and lack of reference to Mei et al. suggest incomplete revisions.\n\n### Conclusion:\nWhile the paper raises an intriguing hypothesis and shows promise in ablation studies, its empirical limitations, lack of rigorous validation, and unresolved methodological issues outweigh its contributions. For a top-tier conference, the work requires stronger evidence of significance, clearer conceptual grounding, and more thorough experimental validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about linking the cerebellum's function to decoupled neural interfaces (DNI) in deep learning, specifically addressing credit assignment and locking problems. The reviewers have mixed feedback. \n\nStarting with Reviewer 1: They think the paper is intriguing and has potential to stimulate discussion but points out some clarity issues. The main concerns are about the model being presented in both feedforward and recurrent settings without clear separation, and the need to better explain the bootstrapping aspect from Jaderberg. Also, they want a comparison with the original DNI as a baseline. The authors addressed these by adding figures and clarifications, especially focusing on the temporal aspect and bootstrapping. They also clarified the differences between CC-DNI and DNI. That seems good.\n\nReviewer 2 is more critical. They argue the paper doesn't offer new insights, just juxtaposing existing ideas (cerebellum and DNI). They say the key points were already known in their fields. The authors added new figures and predictions, like the correlation analysis matching experiments, and discussed how their model explains cerebellar deficits across tasks. But does this count as new insight? The reviewer might still think it's a synthesis without novel contributions. However, the authors did make specific neuroscience predictions and connected them to experiments, which could be seen as advancing the field by bridging the two areas.\n\nReviewer 3's main issues are lack of comparison with experimental data and concrete predictions, plus questions about biological plausibility (like gradient computation and neuron numbers). The authors added figures showing correlations with existing studies, ablation experiments, and addressed the neuron count by suggesting thalamic filtering. They also fixed citation inaccuracies. The new predictions, like cerebellar importance decreasing over learning and correlation patterns, align with some experiments, which strengthens the paper's validity.\n\nThe authors' response seems thorough. They added three figures and a section with testable predictions, which directly tackle Reviewer 3's concerns. They also clarified the model's focus on temporal tasks and the bootstrapping mechanism, addressing Reviewer 1. The key is whether these additions provide sufficient new insights. The paper now offers specific, experimentally grounded predictions and demonstrates how their model explains cerebellar roles in both motor and cognitive tasks, which is a novel integration. \n\nWhile Reviewer 2's point about not adding new to each field separately is valid, the cross-disciplinary synthesis might be the paper's strength. Top-tier conferences often value such bridges between neuroscience and machine learning, especially with testable hypotheses. The improvements in the revised version, including addressing plausibility concerns and providing empirical support, likely elevate the paper's contribution beyond mere juxtaposition. \n\nSo, despite some initial criticisms, the revisions seem to have strengthened the paper's novelty and rigor. The combination of theoretical framework, empirical validation in tasks, and neuroscience predictions makes it a valuable contribution. Therefore, it should be accepted.\n</think>\n\nThe paper proposes a novel framework linking the cerebellum's function to decoupled neural interfaces (DNI) in deep learning, addressing credit assignment and locking problems. While reviewers raised concerns about clarity, lack of new insights, and biological plausibility, the authors' revisions significantly address these points. Key improvements include:\n\n1. **Clarity and Focus**: The revised manuscript clarifies its focus on temporal feedback signals (BPTT) and explicitly ties cerebellar mechanisms to bootstrapping, aligning with Jaderberg et al.'s work. This resolves confusion between feedforward and recurrent settings.\n\n2. **Neuroscience Predictions**: New figures (5–7) and a dedicated section now provide concrete, testable predictions. For example, the model predicts cerebellar activity diminishes as tasks are learned, aligning with experimental findings (e.g., Wagner et al. 2019). Correlation analyses and ablation studies further ground the model in biological observations.\n\n3. **Addressing Biological Plausibility**: The authors acknowledge challenges like neuron count discrepancies but propose plausible mechanisms (e.g., thalamic filtering) and reference existing literature (e.g., Sacramento et al.) to strengthen biological feasibility.\n\n4. **Comparisons and Baselines**: The revised paper clarifies differences between CC-DNI and prior DNI models, emphasizing cerebellar-inspired architectural constraints (e.g., sparsity) that improve performance. This highlights unique contributions to both neuroscience and deep learning.\n\n5. **Cross-Disciplinary Synthesis**: While Reviewer 2 criticized the lack of novel insights within each field, the paper’s strength lies in its integration of cerebellar neuroscience with DNI theory. The demonstration of cerebellar-like deficits in motor/cognitive tasks and the generation of experimentally testable predictions (e.g., distributed representations, asynchronous learning) constitute a meaningful advance in linking these domains.\n\n**Weaknesses Remaining**: \n- Reviewer 2’s concern about \"surface-level juxtaposition\" is partially valid, but the revised manuscript goes beyond analogy by providing mechanistic explanations and empirical validation. \n- Some biological details (e.g., gradient computation) remain unresolved, but the authors appropriately frame these as future directions rather than core claims.\n\n**Conclusion**: The revisions strengthen the paper’s theoretical and empirical contributions, addressing major concerns. The work bridges neuroscience and deep learning with novel hypotheses and validates them through experiments. For a top-tier conference, this interdisciplinary synthesis and the generation of testable predictions justify acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper introduces Shoot Tree Search (STS), a method that modifies MCTS by expanding multiple nodes in each simulation step, aiming to balance depth and breadth in search. The reviewers have mixed opinions. \n\nStarting with the strengths: The idea is simple and the experiments show improvements over MCTS and other baselines in specific domains like Sokoban and Google Football. Reviewer 3 and 5 found the concept interesting and the empirical evaluation thorough. The authors addressed some concerns with additional experiments, like comparing with AlphaGo-style rollouts and testing under different conditions (sparse rewards, more simulations). They also clarified some technical points, like hyperparameters and initialization.\n\nNow the weaknesses. Reviewers 1, 2, and 4 raised concerns about novelty. They pointed out that multi-step expansion isn't entirely new, referencing prior work like Soemers et al. 2016. The authors acknowledged this but argued their approach uses neural networks for value functions, which changes the dynamics. However, the novelty might be incremental. Technical issues were noted, like unclear algorithms, missing details in pseudocode (e.g., CALCULATE_TARGET), and potential flaws in the update rule (weighting longer rollouts more). The authors addressed some of these in their response, promising revisions.\n\nReviewer 2 mentioned the lack of intuitive explanation for why STS is better, which the authors tried to address by adding more discussion in the revised version. The experiments in sparse reward settings showed STS still performed better, which helps. However, some questions remain, like why STS underperformed in the 'Corner' scenario. The authors speculated it's due to exploration issues but didn't provide conclusive evidence.\n\nThe key points are novelty and technical soundness. While the method isn't entirely new, the application with modern neural networks and the empirical results in challenging domains add value. The authors have engaged with reviewer feedback, providing clarifications and additional experiments. The technical issues can likely be fixed in revisions. The experiments, though some are post-hoc, show consistent improvements, which is a strong point.\n\nConsidering top-tier standards, the paper needs to make a significant contribution. The novelty is borderline but the empirical results and clear improvement over baselines tip it towards acceptance, especially with the promised revisions addressing clarity and technical details. The reviewers' scores after discussion also leaned towards acceptance, with some wanting more polishing but not fundamental flaws.\n</think>\n\nThe paper proposes Shoot Tree Search (STS), a variant of MCTS that expands multiple nodes per simulation step to balance depth and breadth in search. While reviewers raised concerns about novelty, technical clarity, and experimental rigor, the authors addressed many points through additional experiments and clarifications. Key considerations:\n\n1. **Novelty**: Multi-step expansion is not entirely new (e.g., Soemers et al., 2016), but the authors argue their approach integrates neural networks for value estimation, offering a distinct bias-variance trade-off. The empirical results in challenging domains (e.g., Sokoban, GRF) show consistent improvements over MCTS and baselines, suggesting practical value even if the idea is incremental.\n\n2. **Technical Soundness**: Reviewers flagged issues like unclear pseudocode, missing details (e.g., `CALCULATE_TARGET`), and potential flaws in the update rule. The authors acknowledged these and committed to revisions, which are feasible. The ablation studies (e.g., comparing with AlphaGo-style rollouts, sparse rewards) strengthen the argument that STS’s benefits stem from its algorithmic design.\n\n3. **Empirical Validation**: The experiments demonstrate STS’s superiority in domains requiring long-horizon planning. While some results (e.g., underperformance in \"Corner\") remain unexplained, the authors provided plausible hypotheses and additional data. The comparison with MCTS under larger computational budgets (e.g., 500 nodes per move) showed competitive performance, addressing scalability concerns.\n\n4. **Clarity and Presentation**: The paper initially had gaps in intuition and exposition, but the authors promised improvements (e.g., clearer explanations of hyperparameters, related work). The revised version would better contextualize STS within prior work and clarify technical details.\n\n**Final Decision: Accept**  \nThe paper makes a meaningful contribution to planning in large state spaces, with empirical evidence of effectiveness. While the novelty is incremental, the technical and experimental improvements, combined with the authors' responsiveness to feedback, justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about GG-GAN, a geometric graph generative adversarial network. The main contributions are addressing challenges in graph generation from a geometric perspective, using a WGAN architecture, and claiming permutation equivariance and scalability. The abstract mentions they handle complex relations, isomorphic graphs, and latent distribution exploitation. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Let's break down each reviewer's feedback.\n\nReviewer 1 is generally positive but has concerns about the experiments. They point out that the datasets used are small (9-node graphs) which might lead to many isomorphic graphs, making the results less convincing. Also, the baselines aren't strong enough—GraphRNN outperforms GG-GAN in most metrics, and they didn't compare with NetGAN or simple models like Watts-Strogatz. This is a big issue because if the experiments aren't robust, the claims about superiority might not hold.\n\nReviewer 2 raises several technical concerns. They question mode collapse, which is a common GAN issue. The paper claims to model edge dependencies but the edge sampling is Bernoulli, which is independent. That contradicts the claim. Also, some theoretical parts like Corollary 1 and Proposition 1 are seen as weak or not well-explained. The hand-crafted features might limit the model's capacity, and the necessity of certain design choices (like unique z sampling) isn't clear. These points suggest the method might have fundamental flaws or lack proper justification.\n\nReviewer 3 is positive, saying the method is inspiring and well-organized. But since they're the only one with a positive take, their opinion might be outweighed by others.\n\nReviewer 4 is critical. They argue the contributions aren't novel enough compared to existing work (like GraphRNN and NetGAN). The literature review is incomplete, and the theoretical analysis isn't strongly connected to the method. Experiments don't show significant improvement over baselines, especially missing comparisons with recent models like TagGen. Scalability claims aren't backed up properly, as autoregressive models might have better scaling. This reviewer suggests weak rejection.\n\nReviewer 5 also has mixed feedback. They like the clarity but note experiments are on small datasets, and some parts (like phi) are confusing. They question the necessity of certain design choices and suggest alternative methods for universality. The ethical section might be unnecessary. Their questions about phi and training details aren't addressed, which could indicate gaps in the paper.\n\nThe author response isn't available, so I can't see if they addressed these points. That's a problem because some issues might have been resolved, but since they aren't here, I have to go by the reviews.\n\nKey issues to consider for a top-tier conference:\n\n1. **Experimental Rigor**: The datasets are very small (9 nodes), which might not be representative. The number of non-isomorphic graphs is around 12k, and if the training set has 10k, there's a lot of overlap. This could mean the model isn't tested on harder cases. Also, not comparing with NetGAN or newer models like TagGen weakens the results. The baselines aren't strong enough, and GG-GAN only outperforms in a few metrics.\n\n2. **Technical Soundness**: Reviewer 2 points out that edge dependencies aren't actually modeled since edges are independent Bernoulli. That's a critical flaw if the paper claims to handle complex relations. The theoretical parts (Corollary 1, Proposition 1) are seen as weak or not well-explained, which might indicate shaky foundations.\n\n3. **Novelty and Contribution**: Reviewer 4 argues that the contributions aren't novel enough. The problem has been addressed before in existing methods, and the paper doesn't clearly differentiate itself. The literature review is lacking, so it's hard to see the unique angle here.\n\n4. **Clarity and Justification**: Several reviewers (2,4,5) mention unclear sections, like the role of phi, the collision avoidance, and the necessity of certain design choices. Without proper explanations, the method's effectiveness is questionable.\n\n5. **Scalability Claims**: The paper claims to scale to large graphs, but the experiments don't test this. Reviewer 4 questions the scalability compared to autoregressive models, suggesting the paper's claims might be overblown without proper benchmarks.\n\nConsidering all this, the main issues are the weak experimental setup (small datasets, insufficient baselines), technical flaws in the model's ability to model edge dependencies, and lack of clear novelty over existing work. These are significant problems for a top conference, which requires strong contributions and thorough validation. The authors didn't address these points, so the paper might not meet the standards. \n\nHowever, the method is novel in its geometric approach and has some theoretical backing. But without better experiments and addressing the technical concerns, it's risky to accept. The reviewers' consensus leans towards rejection, especially with the critical points from multiple reviewers.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Experimental Weaknesses**:\n   - **Dataset Limitations**: The use of small real-world datasets (9-node graphs) and an artificial 20-node dataset raises concerns. With only ~12,000 non-isomorphic graphs for 9-node graphs, the training/test sets likely contain many isomorphic duplicates, which can artificially inflate performance metrics. This undermines the validity of the results, especially for scalability claims.\n   - **Insufficient Baselines**: The comparison with baselines (e.g., GraphRNN) is underwhelming, with GG-GAN only outperforming in a single metric. Key competitors like NetGAN and modern Transformer-based models (e.g., TagGen) are omitted, making it impossible to assess true competitiveness. Simple generative models (e.g., Watts-Strogatz) are also missing, weakening the novelty argument.\n\n2. **Technical Flaws**:\n   - **Edge Independence**: Reviewer 2 highlights that edge dependencies are not modeled, as edges are sampled via independent Bernoulli distributions. This directly contradicts the paper’s claim of handling \"complex relations\" and \"edge dependencies,\" undermining a core contribution.\n   - **Theoretical Gaps**: Corollary 1 and Proposition 1 are criticized as trivial or poorly justified. The lack of rigorous analysis for critical components (e.g., collision avoidance, role of hand-crafted features) weakens the method’s theoretical foundation.\n\n3. **Novelty and Differentiation**:\n   - Reviewer 4 argues that GG-GAN’s contributions overlap significantly with existing work (e.g., GraphRNN for isomorphism, NetGAN for expressiveness, TagGen for scalability). The paper fails to clearly articulate its unique contributions or provide a comprehensive literature review, making it difficult to justify its novelty for a top-tier conference.\n\n4. **Clarity and Justification**:\n   - Multiple reviewers (2, 4, 5) note ambiguities in key design choices (e.g., the role of φ, collision avoidance, unique z sampling). Without clearer explanations or theoretical justification, the method’s effectiveness and innovation are called into question.\n\n5. **Scalability Claims**:\n   - While the paper claims scalability to large graphs, the experiments lack validation on medium/large datasets. Reviewer 4 questions the scalability advantage over autoregressive models, noting that GANs typically require O(n²) parameter space for adjacency matrices, which may not scale as claimed.\n\n### Conclusion:\nThe paper suffers from critical experimental shortcomings, unresolved technical flaws, and insufficient differentiation from prior work. These issues collectively weaken its competitiveness for a top-tier conference, where rigorous validation, clear novelty, and robust technical soundness are essential. Without substantial revisions addressing these points, the work does not meet the required standards."
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about multitask learning, specifically a method called αVIL that dynamically adjusts task weights during training using model updates. The key idea is to use the differences in model parameters after single-task updates to determine how much each task contributes to the target task. The experiments are on computer vision (MultiMNIST) and NLP tasks (like CommitmentBank, COPA, etc.). \n\nLooking at the reviewers' comments first. Reviewer 1 says the paper is well-written but lacks theoretical justification and the experiments aren't convincing enough. The baselines are limited, and the results don't consistently outperform them. They want more theory and better experiments.\n\nReviewer 2 points out that the method is ad hoc, not clearly different from existing methods like DIW or meta-learning approaches. The results are mixed, with no clear superiority. They also mention issues with the experimental setup, like the MNIST task only having one auxiliary task, and the need for more statistical analysis (like standard deviations). The algorithm's steps, like line 11 subtracting 1, are unclear. They suggest the method isn't novel enough and the results aren't strong enough for a top conference.\n\nReviewer 3 also criticizes the lack of clear distinction between α and task weights, the experimental setup (only one auxiliary task in vision), and the minor improvements over baselines. They think the results are statistically insignificant and suggest rejection, urging more ablations and discussion.\n\nThe authors responded by addressing some points. They explained the α vs. weights, mentioned that MultiMNIST was chosen for a sanity check, and acknowledged the need for more theory and experiments. They also note that improvements in NLP are small but consistent in MNIST. However, they admit the results aren't earth-shattering and the theoretical part is lacking.\n\nNow, considering the standards of a top-tier conference. Top conferences usually require strong novelty, solid theoretical grounding, and significant empirical results. The paper's main contribution is a new method for dynamic task weighting. However, the reviewers point out that similar methods exist (like DIW and meta-learning), and the novelty isn't clearly established. The authors argue it's different because it uses meta-optimization on model updates, but Reviewer 2 and 3 aren't convinced it's sufficiently novel.\n\nThe experiments show mixed results. On MNIST, it's better on average but within standard deviations. On NLP tasks, the improvements are minor and sometimes worse. The baselines are limited, and the authors acknowledge that multitask learning on NLP tasks often has small gains. But top conferences usually expect more compelling results, especially if the method isn't groundbreaking.\n\nTheoretical justification is missing, which is a big red flag. Top-tier venues often require at least some theoretical analysis to back up the method. The authors plan to add this in the camera-ready, but the current submission lacks it, which is a problem.\n\nThe experimental setup also has issues. Using only one auxiliary task in vision might not stress-test the method enough. The NLP experiments have small sample sizes (only 4 seeds), making the results less reliable. The authors mention they can add standard deviations but didn't in the submission, which is a weakness.\n\nThe writing and clarity are praised, which is good, but not enough to overcome the other issues. The method's ad hoc nature and unclear differentiation from existing work are significant drawbacks. The authors' responses address some points but don't resolve the core concerns about novelty and empirical impact.\n\nConsidering all this, the paper doesn't meet the high bar for a top-tier conference. The contributions are incremental, the experiments aren't strong enough, and the theoretical backing is missing. While the authors have plans to improve, the current submission falls short. Therefore, it should be rejected.\n</think>\n\nThe paper proposes αVIL, a method for multitask learning that dynamically adjusts task weights using model parameter updates. While the paper is well-written and the problem is relevant, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty**: The idea of leveraging parameter updates for task weight estimation is conceptually interesting and distinct from prior heuristic or gradient-based approaches.\n2. **Empirical Exploration**: Experiments span computer vision (MultiMNIST) and NLP tasks, demonstrating the method's applicability across domains.\n3. **Clarity**: The paper is well-structured and accessible, with clear explanations of the algorithm and motivation.\n\n### Major Concerns:\n1. **Lack of Theoretical Justification**: \n   - Reviewers 1 and 3 explicitly highlight the absence of formal analysis or theoretical grounding for αVIL. While the authors acknowledge this and plan to address it, the current submission lacks critical rigor in explaining *why* the method works or under what conditions it is optimal. Top-tier venues typically require stronger theoretical foundations for novel methods.\n\n2. **Empirical Weaknesses**:\n   - **Mixed Results**: On NLP tasks, αVIL shows only minor improvements over baselines (e.g., DIW), with some results statistically insignificant. On MultiMNIST, performance gains are marginal and within standard deviations of baselines.\n   - **Limited Baselines**: The comparison to DIW and standard multitask learning is insufficient. Competing methods like MAML or meta-weight networks (cited by Reviewer 2) are not evaluated, making it hard to assert superiority.\n   - **Experimental Setup Flaws**: \n     - The MultiMNIST experiments use only one auxiliary task, which may not stress-test the method’s ability to handle multiple tasks (a core multitask learning challenge).\n     - NLP experiments report results on only 4 random seeds, limiting statistical confidence. The use of ensembles for test sets but not development sets introduces inconsistency.\n     - No ablation studies or analysis of learned weights (e.g., sanity checks with random labels or visualization of weight evolution) are provided, weakening interpretability.\n\n3. **Novelty and Differentiation**:\n   - Reviewers 2 and 3 argue that αVIL is not sufficiently distinct from existing methods like DIW or meta-learning frameworks (e.g., MAML). The authors’ defense—that αVIL uses meta-optimization of updates—is not compelling, as similar ideas exist in prior work (e.g., gradient-based weight tuning). The method’s \"novelty\" hinges on minor technical differences rather than a breakthrough.\n\n4. **Algorithmic Clarity**:\n   - Key aspects of the algorithm (e.g., the intuition behind subtracting 1 in line 11 of Algorithm 1) are poorly explained, leaving reviewers confused. The authors’ response clarifies this but does not resolve broader concerns about the method’s ad hoc nature.\n\n### Author Responses:\nThe authors address some points (e.g., explaining line 11, justifying MultiMNIST choice), but critical gaps remain:\n- Theoretical justification is still missing.\n- Empirical results are acknowledged as marginal, with no new experiments to strengthen claims.\n- Novelty is defended weakly, relying on minor distinctions rather than clear innovation.\n\n### Conclusion:\nWhile αVIL is an interesting idea, the paper falls short of top-tier standards due to insufficient theoretical grounding, underwhelming empirical results, and lack of clear differentiation from existing methods. The experiments are underpowered (e.g., small NLP seed counts, limited baselines), and the method’s novelty is incremental. These issues, combined with the lack of a compelling theoretical framework, make the paper borderline for acceptance. However, given the high bar for top conferences, the current submission does not meet the required impact or rigor.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about multitask learning, specifically a method called αVIL that dynamically adjusts task weights during training using model updates. The key idea is to leverage auxiliary tasks to improve the target task's performance. The authors claim it's novel because it uses parameter updates directly for weight estimation. \n\nLooking at the reviewers' comments first. Reviewer 1 says the paper is well-written but lacks theoretical justification and the experiments aren't convincing. The baselines are limited, and the results don't consistently outperform them. They want more theory and better experiments.\n\nReviewer 2 points out that the method is ad hoc, not clearly different from existing ones like DIW or meta-learning approaches. The experiments are mixed, with results not significantly better than baselines. They also mention issues with the algorithm's intuition (like the subtraction of 1 in line 11) and suggest more ablations and statistical tests.\n\nReviewer 3 also criticizes the lack of clear distinction from existing methods, the minor improvements over baselines, and the experimental setup (only one auxiliary task in vision). They suggest rejection due to insufficient novelty and underwhelming results.\n\nThe authors responded by addressing some points. They clarified the α vs. task weights, explained the MultiMNIST choice, and mentioned that improvements in NLU are small but consistent in MultiMNIST. They also acknowledge needing more theory and experiments but are constrained by space.\n\nNow, considering the standards of a top-tier conference. Top conferences usually require strong novelty, solid theoretical grounding, and compelling empirical results. The paper's main contribution is the dynamic task weighting via model updates. However, reviewers question its novelty compared to DIW and meta-learning. The authors argue it's different because it uses meta-optimization on updates, but the reviewers aren't fully convinced. \n\nThe experiments show marginal improvements. On MultiMNIST, it's better but within standard deviations. On NLU tasks, results are mixed, sometimes worse. The baselines could be more comprehensive. The authors mention that multitask improvements on NLU are generally small, which might explain the results, but it still doesn't show a clear advantage. \n\nTheoretical justification is missing, which is a big issue for a top conference. The authors plan to add this in the camera-ready, but the current submission lacks it. Without theory, the method's effectiveness isn't well-founded. \n\nThe reviewers' consensus seems to lean towards rejection. Reviewer 3 explicitly suggests rejection, while others want major revisions. However, top conferences often require the paper to be strong enough even with room for improvement. The key points against are the lack of clear novelty, insufficient empirical evidence, and missing theory. \n\nThe authors did address some concerns but the core issues remain. The method's advantage over DIW isn't clearly demonstrated, and the experiments don't show consistent superiority. The theoretical gap is a red flag. Even if they add theory later, the current submission doesn't meet the standards. \n\nTherefore, I think the paper should be rejected. It needs more work on theory, better experiments, and clearer differentiation from existing methods before it's suitable for a top-tier venue.\n</think>\n\nThe paper proposes αVIL, a method for multitask learning that dynamically adjusts task weights using model parameter updates. While the paper is well-written and the problem is relevant, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Novelty**: The idea of using parameter updates to estimate task weights is conceptually interesting and distinct from prior work like DIW or meta-learning approaches, though reviewers question its differentiation from existing methods.\n2. **Empirical Evaluation**: Experiments span computer vision (MultiMNIST) and NLU tasks, demonstrating some improvements over baselines in controlled settings (e.g., MultiMNIST).\n\n### Major Concerns:\n1. **Lack of Theoretical Justification**: Reviewers unanimously highlight the absence of formal analysis or theoretical grounding for αVIL. The authors acknowledge this gap and plan to address it in future work, but the current submission lacks rigor required for a top-tier venue.\n2. **Marginal Empirical Results**: \n   - On MultiMNIST, αVIL shows only minor improvements over baselines, with results within standard deviations.\n   - On NLU tasks, performance is mixed, with no consistent superiority over DIW or standard multitask learning. The authors attribute this to inherent challenges in multitask NLU, but the results fail to convincingly demonstrate αVIL’s advantage.\n   - Baselines are limited (e.g., no comparison to more recent or diverse dynamic weighting methods beyond DIW).\n3. **Methodological Clarity and Novelty**:\n   - Reviewers question the method’s novelty, noting similarities to meta-learning (e.g., MAML) and existing dynamic weighting approaches. The authors argue αVIL’s meta-optimization of updates is unique, but this distinction is not clearly articulated.\n   - Algorithmic details (e.g., the subtraction of 1 in line 11 of Algorithm 1) lack intuitive justification, and the role of α vs. task weights is unclear.\n4. **Experimental Rigor**:\n   - The MultiMNIST experiments use only one auxiliary task, limiting generalizability.\n   - Statistical significance is not consistently reported (e.g., NLU results lack standard deviations), and the use of ensembles for test sets but not development sets introduces inconsistency.\n   - No ablation studies or sanity checks (e.g., testing with random labels) are provided to validate the method’s behavior.\n\n### Author Responses:\nThe authors address some concerns (e.g., clarifying α’s role, explaining MultiMNIST choices), but critical issues remain unresolved. They acknowledge the need for theoretical work and additional experiments but note space constraints. However, the current submission does not meet the empirical or theoretical standards expected for acceptance at a top-tier conference.\n\n### Conclusion:\nWhile αVIL shows promise, the paper falls short in theoretical justification, empirical robustness, and clear differentiation from existing methods. The marginal improvements over baselines and unresolved methodological ambiguities weaken its case for acceptance. The authors’ responses suggest potential improvements, but the current version lacks the rigor and impact required for a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Deep Ecological Inference,\" and the keywords include ecological inference, representation learning, multi-task learning, and Bayesian deep learning. The abstract mentions they're using an efficient loss function approximation for ecological inference, applying it to linear models, DNNs, and Bayesian NNs. They tested this on Maryland 2018 election data, showing improvements over benchmarks with polling data. They also talk about joint learning of multiple races and latent representations.\n\nNow, looking at the reviewer comments. There are four reviewers here. Let me go through each one's points.\n\nReviewer 1 points out that the paper is not easy to follow, with structural issues like not introducing Poisson binomial/multinomial losses early, unclear input data, and evaluation tasks. They also mention lack of implementation details and typos. The strengths are the problem's importance and the approach's potential.\n\nReviewer 2 thinks the paper is a good attempt combining ecological inference with deep learning but criticizes the experiments as incomplete and unclear. They mention missing baselines (like standard MLM), unclear analysis (e.g., why the deep MLM performs better on crossover voting), and reproducibility issues (no details on splits, hyperparameters, etc.). They also note that the linear model has the best R², which isn't discussed. Typos and figures needing improvement are also mentioned. The conclusion is that it's preliminary due to incomplete experiments.\n\nReviewer 3 is more positive, seeing the problem as important and the method useful. They suggest more synthetic data tests and comparisons to state-of-the-art, but overall think it's publishable given space constraints. Minor typos noted.\n\nReviewer 4 is harsher. They say the approach isn't new, related work is lacking (especially not citing key papers in LLP, distribution regression, etc.), and the experiments are poorly organized. They also mention unclear sections and missing hyperparameters. They argue the paper isn't ready for publication due to lack of novelty and poor writing.\n\nAuthor response isn't available, so we can't consider any rebuttals.\n\nNow, considering the criteria for a top-tier conference. The paper needs to have significant contributions, clear presentation, thorough experiments, and proper literature review.\n\nStrengths: The problem is important (ecological inference is a classic problem with real-world applications like gerrymandering analysis). The approach combines deep learning with ecological inference, which is a novel angle. They applied it to a large real-world dataset and showed some improvements over baselines. The use of multi-task learning and latent representations is interesting.\n\nWeaknesses: Multiple reviewers point out the lack of key baselines. For instance, Reviewer 2 mentions that the standard MLM isn't compared, making it hard to see the benefit of the deep approach. The experiments are criticized for being incomplete and not reproducible (no details on splits, hyperparameters, etc.). The writing is a mess with structural issues and typos, making it hard to follow. The lack of related work, especially not citing key papers in related areas like LLP or collective graphical models, is a red flag. Reviewer 4's point about the method not being novel is concerning if existing methods already do similar things. Also, the fact that the linear model outperforms others in some metrics (like R²) but isn't discussed properly shows gaps in analysis.\n\nThe reviewers' consensus seems mixed. Reviewers 1 and 2 have major concerns about experiments and clarity. Reviewer 3 is more positive but acknowledges it's preliminary. Reviewer 4 is outright negative on novelty and writing. For a top conference, the paper needs to be well-written, have strong experiments, and clear contributions. The current state has several critical issues: missing baselines, unclear experiments, poor reproducibility, and insufficient literature review. The writing issues are significant enough that even if the ideas are good, the presentation is a barrier. The lack of comparison to existing methods (like the ones in the related work pointed out by Reviewer 4) undermines the contribution's novelty and effectiveness.\n\nThe authors didn't respond, so we can't assume they addressed these points. Given that top conferences require high standards, these flaws are likely deal-breakers. The experiments aren't thorough enough, and the paper isn't polished enough for acceptance without major revisions. Since the reviewers' feedback is mostly negative on key aspects, I think the decision should be to reject unless the authors can fix all these issues, which they haven't had a chance to here.\n</think>\n\nThe paper addresses an important problem in ecological inference, proposing a novel approach using deep learning techniques to approximate loss functions and improve predictions from aggregate data. The application to real-world election data and the exploration of multi-task learning and latent representations are promising. However, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Clarity and Structure**: Multiple reviewers noted significant readability issues. Key concepts (e.g., Poisson binomial/multinomial losses) are introduced too late, input data and evaluation tasks are poorly described, and the experimental section is disorganized. Typos and formatting errors further hinder comprehension.\n\n2. **Incomplete Experiments**: \n   - **Missing Baselines**: The paper fails to compare against standard multi-level models (MLMs), making it impossible to assess the value of the proposed deep MLM. \n   - **Lack of Analysis**: Claims about model behavior (e.g., crossover voting performance) are conjectural without quantitative evidence. The linear model’s superior R² performance is unexplained, raising questions about the proposed methods’ utility.\n   - **Reproducibility**: Critical details (e.g., train/test splits, hyperparameters, latent features used) are omitted, hindering reproducibility.\n\n3. **Novelty and Related Work**: Reviewer 4 highlights a lack of engagement with prior work in related areas (e.g., learning with label proportions, collective graphical models). This weakens the paper’s contribution, as it is unclear how the approach advances beyond existing methods.\n\n4. **Technical Gaps**: The loss function’s assumptions are not clearly articulated, and the connection to existing frameworks (e.g., collective graphical models) is unexplored. The experiments lack synthetic data tests and comparisons to state-of-the-art baselines, limiting confidence in the results.\n\nWhile the problem domain is impactful and the ideas are intriguing, the paper’s current state—marked by poor presentation, incomplete experiments, and insufficient grounding in related work—prevents it from meeting the rigorous standards of a top-tier conference. These issues are not trivial and would require substantial revisions beyond minor fixes.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about applying meta-learning theory to improve few-shot learning algorithms through regularization. The reviewers have mixed opinions, so I need to weigh their points carefully.\n\nFirst, the main idea is connecting theory to practice, which is a good contribution. The authors take assumptions from meta-learning theory and turn them into regularizers. That's novel in the context of meta-learning, even if the regularizers themselves (like Frobenius norm or singular value penalties) aren't new in isolation. Reviewer 1 points out that spectral normalization and weight decay are similar, but the authors argue that their application here is different in purpose and combined to satisfy specific theoretical assumptions. The author response clarifies that using both regularizers together is crucial, and experiments in the supplement show that one alone isn't enough. That seems valid.\n\nReviewer 2 and 3 raised concerns about the theoretical assumptions not matching the practical setup, like the lack of a validation set in the theory. The authors addressed this by explaining that the theory's conditions are relaxed in practice and that their approach still provides insights. They also added synthetic experiments, which should help validate their claims. The mention of statistical significance in results is important too; if the improvements are significant despite small margins, that's a plus.\n\nReviewer 4's points about novelty and technical details: The Frobenius norm as L2 is a common technique, but the authors stress that their use targets specific theoretical conditions, not just generalization. The technical part about SVD gradients being handled by frameworks like PyTorch is addressed, so that's manageable. The lack of newer baselines was addressed by adding Meta-Curvature, which helps.\n\nThe author responses seem thorough. They clarified that their goal isn't to beat SOTA but to validate theory, which is a valid contribution. The experiments, even if incremental, show that applying these regularizers based on theory does help, which is a step forward. The reviewers' concerns about novelty and methodology were addressed by adding more experiments and clarifications.\n\nHowever, some points remain: the regularizers aren't entirely new, but their combined application in this specific context is. The paper's main contribution is bridging theory and practice, which is valuable for the field. The experiments, while not earth-shattering, do support the claims. The authors also acknowledged limitations and provided examples where their approach works as intended.\n\nConsidering top-tier standards, the paper's originality might be borderline, but the thoroughness in addressing reviewer points and the clear contribution to understanding meta-learning theory's practical implications tip it towards acceptance. The field needs more work connecting theory and practice, and this paper makes a solid effort despite existing techniques being repurposed.\n</think>\n\nThe paper aims to bridge the gap between meta-learning theory and practice by translating theoretical assumptions into regularization terms for meta-learning algorithms. While the reviewers raised several concerns, the authors addressed many of these points effectively in their response, strengthening the paper's contributions and clarifications. Here's the breakdown:\n\n### Key Strengths:\n1. **Novel Application of Theory**: The core idea of applying meta-learning theory (e.g., assumptions about task diversity and predictor norms) to design regularization terms is a meaningful contribution. Even if individual regularization techniques (e.g., Frobenius norm, singular value penalties) are not novel in isolation, their specific combination and application to enforce theoretical assumptions in meta-learning is a fresh angle. The authors clarify that satisfying *both* assumptions is critical, which is validated experimentally (e.g., Table 5 in the supplement).\n\n2. **Empirical Validation**: The experiments demonstrate that the proposed regularizers improve performance over vanilla meta-learning methods on standard benchmarks. While improvements are modest, the authors emphasize statistical significance and clarify that their goal is to validate theoretical insights rather than achieve state-of-the-art results. The addition of new baselines (e.g., Meta-Curvature) and hyperparameter-tuning experiments in the supplement address reviewer concerns about competitiveness and methodology.\n\n3. **Clarity and Rigor in Responses**: The authors thoroughly addressed critiques about novelty, technical details (e.g., SVD gradients), and theoretical assumptions. They provided examples (e.g., synthetic experiments, Figure 1) to justify their approach and clarify that their regularization enforces assumptions in practice, even if theoretical guarantees rely on unknown optimal predictors. They also clarified that their work is not about proposing a new algorithm but about testing theoretical assumptions in practice—a valid and under-explored direction.\n\n### Key Weaknesses:\n1. **Novelty Concerns**: Reviewers noted that the regularization techniques (e.g., spectral normalization, weight decay) are not novel. However, the authors persuasively argue that their application here is distinct in purpose and combined to satisfy specific theoretical conditions. The focus on theory-practice alignment justifies the contribution, even if the techniques themselves are not new.\n\n2. **Theoretical-Practical Gaps**: Reviewers pointed out discrepancies between the theoretical assumptions (e.g., i.i.d. tasks, no validation set) and practical meta-learning setups. The authors acknowledged these gaps but argued that their approach still provides actionable insights and that relaxing assumptions is common in applying theory to practice. The addition of synthetic experiments and clarifications in the revised manuscript help mitigate this concern.\n\n3. **Baseline Comparisons**: Initial experiments lacked some recent baselines, but the authors added Meta-Curvature and other comparisons in the supplement. While not exhaustive, this addresses the critique sufficiently for a conference paper.\n\n### Final Assessment:\nThe paper’s primary contribution lies in its novel application of meta-learning theory to practical algorithm design, which is a valuable step toward unifying theory and practice in the field. The authors’ responses effectively address concerns about novelty, methodology, and rigor. While the improvements are incremental, the work provides a clear framework for interpreting and leveraging theoretical assumptions in meta-learning, which is a significant contribution to the community. The experiments, though modest, are statistically significant and well-motivated. The paper’s clarity and thorough rebuttal further strengthen its case for acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about PERIL, a method combining imitation learning and meta-RL. The reviewers have given mixed feedback. Let me start by going through each reviewer's comments and the author responses to see the strengths and weaknesses.\n\nFirst, Reviewer 1 likes the idea of using demonstrations for meta-RL in sparse reward settings, which is a good point. They mention that PERIL outperforms baselines like PEARL and MetaIL, which is a positive sign. However, they point out the complexity of the method with multiple loss terms and notation issues. The reviewer is confused about why all components are necessary and suggests ablations and simpler baselines. They also question the necessity of rewards at test time and the role of the latent embedding. The authors addressed some of these by clarifying assumptions and mentioning future work on ablations, but the paper's clarity and experimental rigor are still concerns.\n\nReviewer 2 is critical about related work not citing enough prior methods, especially those combining meta-IL and meta-RL. They also note that the method isn't clearly differentiated from existing work like PEARL and Yu et al. The reviewer points out missing baselines (like comparing to Mendonca et al.) and unclear explanations of certain losses (like L_mi and L_D_KL). The authors responded by acknowledging the need for better related work and more experiments, but the paper's current state lacks these.\n\nReviewer 3 is concerned about the paper's clarity, originality, and experimental validation. They mention that the method is too complex, the experiments don't use high-dimensional data (like images), and the claims about zero-shot learning aren't well-supported. The reviewer also notes that the experiments lack statistical significance (like multiple seeds). The authors agreed to add confidence intervals and clarify sections but didn't provide enough in the rebuttal to address all these points.\n\nReviewer 4 highlights the restrictive assumption of having an expert policy conditioned on the latent z, which might not be realistic. They also point out notation issues and unclear problem formulation. The authors clarified that the expert uses the true z, not the learned one, but this still seems like a strong assumption. The reviewer also wants better comparisons with other methods and more exploration tasks. The authors plan to add more baselines and experiments but haven't done so yet.\n\nLooking at the author responses, they acknowledge many of the issues and plan to address them in future submissions. However, the current submission has several unresolved problems: unclear methodology, insufficient ablation studies, missing baselines, and lack of statistical rigor in experiments. The reviewers consistently point out that the paper isn't ready for publication at a top-tier conference because of these flaws. While the idea is promising, the execution and presentation need significant improvements. The authors themselves decided to postpone submission, which suggests they agree the paper isn't yet up to standard.\n\nConsidering all this, even though the method has potential, the current version doesn't meet the quality, clarity, and experimental thoroughness required for a top conference. The reviewers' concerns about complexity, lack of ablations, and unclear contributions aren't sufficiently addressed in the rebuttal. Therefore, the paper should be rejected now but encouraged to resubmit after addressing these issues.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Clarity and Methodological Issues**: \n   - Reviewers 1, 2, and 3 highlighted significant clarity issues, including inconsistent notation (e.g., variable name clashes, undefined loss terms like \\(L_{mi}\\) and \\(L_{D_{KL}}\\)), and a lack of precise mathematical formulation. The problem statement conflates the learned latent \\(z\\) with the true state \\(s\\), leading to confusion about the problem setup versus the method. While the authors acknowledged these issues and proposed fixes, the current manuscript does not resolve them sufficiently for a top-tier conference.\n\n2. **Experimental Weaknesses**:\n   - **Baselines and Comparisons**: Reviewers 2 and 3 noted the absence of critical baselines (e.g., PEARL with demonstrations, prior hybrid meta-IL/RL methods like Mendonca et al.), making it hard to assess PERIL’s novelty and superiority. The authors committed to adding these but did not provide results in the rebuttal.\n   - **Statistical Rigor**: Experiments lacked confidence intervals and sufficient seeds (only 1–3 runs per method), raising concerns about reproducibility and statistical significance. The authors agreed to address this but could not revise the submission during rebuttal.\n   - **Task Complexity**: Reviewer 4 pointed out that tasks did not require sophisticated exploration, undermining claims about exploration beyond demonstrations. The authors acknowledged this and plan to relax claims but did not provide stronger evidence.\n\n3. **Originality and Contribution**:\n   - Reviewers 2 and 3 questioned PERIL’s novelty, noting its close resemblance to PEARL and Yu et al. The authors did not sufficiently clarify how PERIL’s design choices (e.g., auxiliary losses, demonstration integration) meaningfully advance prior work. The method’s complexity (six loss terms) and lack of ablations made it unclear which components contributed to performance.\n\n4. **Assumptions and Limitations**:\n   - Reviewer 4 highlighted a restrictive assumption: requiring an expert policy conditioned on the latent \\(z\\), which is learned by PERIL itself. The authors clarified that the expert uses the true \\(z\\), but this assumption limits real-world applicability. The paper also does not address high-dimensional observations (e.g., images), contradicting claims about overcoming low-dimensional state limitations.\n\n5. **Author Response Limitations**:\n   - While the authors acknowledged most critiques and proposed improvements (e.g., better related work, ablations, confidence intervals), they could not revise the submission during rebuttal. The decision to postpone resubmission indicates the paper is not yet mature enough for publication.\n\n### Conclusion:\nThe paper introduces an interesting idea but suffers from significant flaws in clarity, experimental rigor, and novelty demonstration. While the authors recognize these issues and intend to address them, the current submission does not meet the standards of a top-tier conference. Rejection is warranted to encourage a more polished resubmission with thorough revisions."
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title is \"Automatic Music Production Using Generative Adversarial Networks.\" The abstract mentions they're focusing on automatic music accompaniment using CycleGAN and Mel-spectrograms. The reviewers have given mixed feedback. \n\nLooking at Reviewer 1's comments: The title is misleading because the work is more about accompaniment, not the entire music production process. They also point out that treating audio as images (spectrograms) isn't novel, as others have done similar things in MIR tasks. The method section lacks details on Demucs and CycleGAN specifics. The experiments are criticized for not justifying their chosen metrics and having unclear descriptions. \n\nReviewer 2 thinks the problem is challenging and the approach novel but wants more experiments, comparisons, and better evaluation. They mention the narrow scope but suggest expanding the experiments. They also point out missing citations and dataset details. The authors addressed some points but not all, like the phase retrieval method and the necessity of FMA data.\n\nReviewer 3 found the paper interesting but noted issues with source separation quality affecting results, lack of details on phase reconstruction, and unclear quantization steps. The authors added some explanations but didn't fully address all concerns, like the phase retrieval method's specifics.\n\nThe authors' responses seem to address some points: they clarified Demucs' time-equivariance, added details on phase retrieval, cited missing datasets, and fixed some typos. However, some critical issues remain, like not showing experiments without FMA data and not fully explaining phase reconstruction steps. Also, the title's misleading nature wasn't adequately fixed.\n\nNow, considering top-tier standards, the paper has several issues. The title is still problematic as it overstates the contribution. The method lacks sufficient technical details (like CycleGAN architecture specifics, phase reconstruction method), which are crucial for reproducibility. The experiments could be more thorough, especially in comparing with other models and addressing source separation artifacts. The evaluation metrics aren't well-justified, and the human evaluation's reliability isn't fully explored (e.g., inter-annotator agreement).\n\nHowever, the problem is novel and the approach has potential. The authors made improvements in response, but some key points remain unresolved. For a top conference, the paper needs to be more rigorous in methodology and experiments. The lack of detailed technical information and incomplete addressing of reviewer concerns might make it not yet ready. \n\nFinal Decision: Reject\n</think>\n\nThe paper proposes an approach to automatic music accompaniment using CycleGAN and Mel-spectrograms, addressing a niche but relevant problem in music generation. However, several critical issues remain unresolved despite the authors' revisions, making it unsuitable for acceptance at a top-tier conference:\n\n1. **Title and Scope Misalignment**: The title \"Automatic Music Production\" is misleading, as the work focuses narrowly on accompaniment generation. While the authors defend this choice, the broader claim oversteps the contribution, which is a significant issue for a top-tier venue requiring precise framing.\n\n2. **Lack of Technical Detail**: Key methodological details remain omitted. For instance:\n   - The CycleGAN architecture (e.g., model size, hyperparameters, training specifics) is still inadequately described, hindering reproducibility.\n   - Phase reconstruction from Mel-spectrograms is only partially explained, leaving critical gaps in understanding how perceptual quality is achieved.\n   - The impact of source separation artifacts (e.g., bleed-through in DEMUCS outputs) on results is not quantitatively addressed, despite reviewer concerns.\n\n3. **Incomplete Experiments and Evaluation**:\n   - The necessity of the FMA dataset for performance is unproven, as the authors did not conduct ablation studies comparing training solely on MusDB18.\n   - The evaluation metrics (e.g., STOI, FID) lack justification for their relevance to musical quality, and the human evaluation lacks rigor (e.g., no inter-annotator agreement analysis).\n   - The narrow experimental scope (e.g., limited to bass, drums, vocals) and lack of comparisons with other unpaired translation models (e.g., Pix2Pix) weaken the contribution’s novelty and practicality.\n\n4. **Literature Gaps**: While some symbolic-domain references were added, the paper still underrepresents prior work in symbolic music generation and fails to adequately contextualize its approach within broader music production workflows.\n\n5. **Reproducibility Barriers**: Despite promises to release code, the dataset details (e.g., exact song IDs, trimming procedures) and technical specifications (e.g., windowing functions for STFT) were only partially addressed, leaving gaps for reproducibility.\n\n**Strengths**: The problem is innovative, and the use of CycleGAN for unpaired audio translation is a promising direction. The proposed evaluation framework incorporating human judgment adds value, and the authors made meaningful revisions in response to feedback.\n\n**Conclusion**: While the paper shows potential, unresolved methodological gaps, insufficient experimental rigor, and critical omissions in technical details prevent it from meeting the high standards of a top-tier conference. The authors should address these issues in a revised version for future submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper introduces Block Minifloat (BM), a new floating-point format for training DNNs with 4-8 bits. The key idea is using a shared exponent bias to reduce exponent bits, allowing more efficient hardware with Kulisch accumulators. The experiments show that BM6 and BM8 achieve good accuracy with smaller area and lower energy compared to FP8 and FP32, especially on ResNet-18 and ImageNet.\n\nLooking at the reviewers' comments, there are both positive and critical points. Reviewer 1 and 2 mention the paper is well-written and the hardware evaluation is strong, but they have some concerns. Reviewer 2 initially had a major issue with the hardware overhead of using multiple formats but was convinced after the authors' response. Reviewer 3 points out some clarity issues in explanations and tables but acknowledges the thorough exploration of the design space.\n\nThe authors addressed several points in their response. They clarified the exponent bias alignment, provided more details on equation derivations, fixed notation issues, and explained the hardware overhead concerns. They also mentioned that BM8 uses hybrid formats which were factored into the hardware synthesis, and the power numbers came from RTL tools, though they're estimates.\n\nNow, considering the criteria for a top-tier conference: the paper's contributions need to be novel, impactful, and well-executed. The BM format seems novel in combining shared exponent bias with minifloats, and the hardware implementation with Kulisch accumulators is a solid contribution. The experiments, while not covering the largest networks like ResNet50, still show promising results on standard benchmarks. The authors addressed most of the reviewers' concerns, especially the major ones about hardware overhead and clarity.\n\nHowever, there are still some minor issues like the lack of ResNet50 experiments and some confusion in tables, but the authors have improved the clarity in the revised draft. The hardware evaluation is more rigorous than typical quantization papers, which is a plus. The fact that Reviewer 2 raised their score to accept after the response is a good sign.\n\nOverall, the paper's contributions are significant enough for a top conference, and the issues have been adequately addressed. The hardware results are compelling, and the methodology is thorough. I think it should be accepted.\n</think>\n\nThe paper introduces Block Minifloat (BM), a novel floating-point format for training deep neural networks with 4–8 bits, which reduces exponent bits by sharing an exponent bias. This innovation enables efficient hardware implementations using Kulisch accumulators, achieving smaller area and lower energy consumption compared to FP8/FP32. The experiments demonstrate strong results on ResNet-18 and ImageNet, with BM6/BM8 matching or nearly matching FP32 accuracy while reducing hardware costs significantly. \n\n### Key Strengths:\n1. **Novelty and Impact**: BM combines shared exponent bias with minifloats, offering a new approach to low-precision training. The hardware implementation with Kulisch accumulators is a meaningful contribution, particularly for energy-efficient training.\n2. **Rigorous Hardware Evaluation**: The authors synthesized RTL designs for BM units, providing concrete area and power metrics (e.g., 4.1× smaller area and 2.3× lower energy for BM6 vs. FP8). This is uncommon in quantization-focused papers and strengthens the credibility of their claims.\n3. **Clarity and Response**: The authors addressed most reviewer concerns effectively. They clarified key points (e.g., exponent bias alignment, Equation 6/7 derivation), fixed notation issues, and explained hardware overheads (e.g., hybrid formats accounted for in synthesis). Reviewer 2’s revised score to \"accept\" reflects this.\n\n### Areas for Improvement:\n1. **Experiment Scope**: The lack of results on larger models like ResNet50 is a minor limitation, but the authors justified focusing on smaller networks due to computational constraints. This is acceptable for a foundational hardware-oriented paper.\n2. **Formatting and Clarity**: Some tables and equations were initially confusing, but the revised draft addressed these (e.g., defining columns in Table 2, adding BM7 to tables). The remaining minor issues (e.g., power estimation methodology) are noted but not critical.\n3. **Denorm Handling**: While the authors explained denorm conversion steps, the hardware complexity for denorms remains somewhat unclear. However, their synthesis results suggest manageable overhead, and the explanation in the revised draft suffices for a conference paper.\n\n### Conclusion:\nThe paper’s contributions—novel BM formats, efficient hardware implementation, and strong empirical results—align with the standards of a top-tier conference. The authors adequately addressed major concerns, and the hardware evaluation raises the bar for quantization research. While minor limitations exist, they do not outweigh the paper’s strengths.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Training independent subnetworks for robust prediction,\" and the main idea is using a MIMO configuration to train multiple subnetworks within a single model. The key claim is that this approach achieves the benefits of ensembles without extra compute or parameters, except a 1% increase. The experiments show improvements on several datasets, including OOD variants.\n\nLooking at the reviewer comments first. Reviewer 1 points out that even though parameters increase by 1%, the authors claim it's \"for free,\" which is a bit misleading. Also, they want more details on where that 1% comes from and experiments on more diverse datasets like OpenImages. \n\nReviewer 2 thinks the method is original but similar to existing MIMO approaches like BatchEnsemble. They want a better discussion of differences and more theoretical grounding on why the subnetworks become independent. The experiments are thorough, but the paper could be stronger with some theory.\n\nReviewer 3 finds the idea clever but is concerned about the lack of theoretical analysis and practical limitations, like M needing to be small. They also question the training setup where inputs are mixed and then tested with identical copies. The permutation idea is interesting but needs more exploration.\n\nReviewer 4 is critical about originality, pointing out that multi-branch architectures exist and the related work section is lacking. They also argue that the computational savings are deceptive because the batch size effectively decreases. The paper's clarity is good but has formatting issues. The significance is downgraded because of these points.\n\nThe author responses address some concerns. They clarified the parameter increase comes from first and last layers, provided FLOPs numbers (like 10,559M vs 10,561M for M=3), and explained that the method is compatible with batching. They also addressed independence by showing different loss landscapes and activation analyses. They mention a theoretical analysis in the regression case but acknowledge more is needed. They also clarify that test-time inputs are handled by tiling, which is compatible with batch processing.\n\nNow, considering the standards of a top-tier conference. The paper's main strengths are the novel approach (MIMO for independent subnets), thorough experiments across multiple datasets, and the practical benefit of reduced inference time. The method seems to offer a meaningful improvement in robustness and uncertainty without significant parameter or compute costs. The authors have addressed some reviewer concerns, like parameter breakdown and FLOPs, which were initial points of confusion.\n\nHowever, the weaknesses include the lack of theoretical grounding, which several reviewers pointed out. While the empirical results are strong, the paper might not fully explain why the subnetworks become independent, which is a key contribution. The originality is questioned because similar multi-branch methods exist, though the authors argue their approach is novel in using multiple inputs for independence. The computational argument is a bit tricky: while inference time per example is the same, the batch size effectively shrinks by a factor of M, which could be a practical limitation. The authors addressed this by explaining how batching works, but it's still a valid concern.\n\nThe reviewers' scores are mixed. Reviewer 2 and 3 are positive but have reservations, while Reviewer 4 is more negative on originality and significance. The authors have made efforts to clarify points, but some issues like the theoretical analysis remain unresolved. \n\nFor a top-tier conference, the paper needs to make a strong contribution. The method is innovative and shows clear empirical gains. The theoretical gaps are a downside, but the empirical results are compelling. The computational argument, while addressed, might still be a point of contention. However, the authors have provided sufficient evidence that the approach works as claimed, and the 1% parameter increase is minor. The experiments are thorough, and the problem addressed (efficient ensembling) is important. \n\nConsidering all this, the paper's strengths in methodology and results outweigh the weaknesses, especially since the authors have addressed several concerns. The theoretical aspects can be explored in future work. The originality is borderline, but the empirical impact is there. So, I think it should be accepted.\n</think>\n\nThe paper proposes a novel method called MIMO (multi-input multi-output) to train independent subnetworks within a single model, enabling efficient ensembling without significant increases in parameters or compute. The key contributions are the demonstration that subnetworks can behave independently, leading to improved robustness and uncertainty estimation, and the empirical validation across multiple benchmarks. The authors address several reviewer concerns, including clarifying parameter increases, providing FLOPs details, and explaining test-time batching compatibility. However, weaknesses remain, such as limited theoretical grounding for independence and potential practical limitations (e.g., batch size constraints). Despite these, the method’s empirical success, originality in leveraging MIMO for subnetwork diversity, and clear benefits over prior work in inference efficiency make it a strong candidate for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about improving uncertainty estimates for deep image classifiers on out-of-distribution (OOD) data using a post-hoc calibration method with outlier exposure. The reviewers have mixed feedback, so I need to weigh the pros and cons carefully.\n\nStarting with the strengths: The problem is important, especially since models are often overconfident on OOD data. The proposed methods (single and multiple image) seem to show consistent improvements over existing benchmarks like Ovadia et al. 2019, which is a good sign. The authors addressed some reviewer concerns in their response, adding experiments on completely OOD datasets like SVHN and Fashion-MNIST, which helps. The simplicity and speed of the method are pluses, and the fact that it's a post-hoc approach means it can be applied to any model, which is a big advantage.\n\nNow the weaknesses. Reviewer 1's initial critique was that the method assumes the OOD data comes from a known set of distributions, which isn't realistic. But the authors clarified that they use a specific corruption (like contrast) in the calibration set and test on other unseen corruptions. That addresses the \"known distributions\" issue because the test corruptions aren't part of the calibration set. However, the method still relies on having a calibration set with some OOD data, which might not always be available. But since they use standard benchmarks, maybe that's acceptable for the conference.\n\nAnother point is the related work. The authors added the suggested references (Park et al., Wang et al.), so that's fixed. The method's simplicity is both a pro and a con. Reviewer 1 thought it was too simple because of the assumed knowledge, but the authors clarified that they don't know the test distribution, just use a different corruption for calibration. \n\nClarity issues were addressed in the response. They defined terms like p_max, fixed notation, and explained why contrast was chosen. The multiple image method's motivation was clarified, and they added more experiments showing performance across different corruptions. The new figures and tables in the appendix help with transparency.\n\nReviewer 3's concern about generalization to other corruptions was addressed by adding experiments on translations and rotations, and completely OOD datasets. The method's performance on these shows it's not limited to specific corruptions. The remaining issue from Reviewer 1 is about baselines. They mentioned that existing calibration under distribution shift techniques could be compared, but the authors might have already done that in their experiments. The rebuttal says they included a baseline with non-corrupted validation sets, which underperformed, so that's covered.\n\nThe key points are: the problem is significant, the method is simple yet effective, and the authors addressed most criticisms. The main remaining concern is whether the method's assumptions are too restrictive, but the clarification that they use a different corruption for calibration and test on others alleviates that. The experiments now include more varied scenarios, which strengthens the paper.\n\nConsidering top-tier standards, the paper contributes a novel post-hoc method with solid empirical results and addresses reviewer concerns. While there are some limitations (like needing some OOD data for calibration), it's a step forward in OOD calibration. The improvements over benchmarks and the method's applicability make it worth accepting.\n</think>\n\nThe paper addresses the critical issue of improving uncertainty estimation for deep image classifiers on out-of-distribution (OOD) data, a problem of significant practical importance. The proposed method leverages outlier exposure via a calibration set containing artificially corrupted data to recalibrate model confidences. While initial reviews raised concerns about assumptions, clarity, and related work, the authors provided thorough responses and revisions that largely mitigate these issues. Key points include:\n\n### Strengths:\n1. **Problem Importance**: Uncertainty calibration on OOD data is a fundamental challenge, and the paper tackles this with a novel post-hoc approach.\n2. **Empirical Results**: The method consistently outperforms prior work (e.g., Ovadia et al. 2019) across multiple benchmarks (CIFAR-10C, ImageNet-C) and even on entirely OOD datasets (SVHN, Fashion-MNIST). The improvements in ECE and calibration diagrams are compelling.\n3. **Simplicity and Generality**: The method is computationally efficient, requires no model retraining, and is model-agnostic, making it broadly applicable.\n4. **Revisions Address Concerns**: The authors clarified ambiguities (e.g., definitions of terms, notation), added experiments on diverse corruptions and OOD datasets (e.g., translations, SVHN), and addressed reviewer critiques about assumptions and baselines.\n\n### Weaknesses and Limitations:\n1. **Assumptions**: The method assumes access to a calibration set with OOD data (e.g., contrast-corrupted images). While this is a standard setup in benchmarks like ImageNet-C, it may not always be feasible in real-world scenarios. However, the authors clarify that the test corruptions are *different* from those in the calibration set, addressing the earlier critique about \"known distributions.\"\n2. **Clarity and Presentation**: Initial versions had unclear terminology and notation, but revisions improved these aspects. Some theoretical justification (e.g., why contrast was chosen) remains heuristic, though experiments validate its effectiveness.\n3. **Baseline Comparisons**: While the authors added comparisons with non-corrupted calibration sets, more thorough comparisons with recent calibration-under-shift methods (e.g., Park et al., Wang et al.) could strengthen the paper further. However, the rebuttal indicates that existing baselines were already addressed.\n\n### Final Assessment:\nThe paper makes a meaningful contribution to OOD uncertainty calibration. The method’s simplicity, empirical effectiveness, and broad applicability align with the standards of a top-tier conference. While limitations exist (e.g., reliance on a pre-specified calibration set), the revisions and experiments demonstrate robustness across diverse scenarios. The critiques have been adequately addressed, and the problem’s significance outweighs minor shortcomings.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract mention a new Reduction Algorithm for Deep ReLU Networks based on regularization theory. The key contributions seem to be a method to reduce neurons significantly while maintaining function accuracy. The experiments are on toy data and MNIST, which are mentioned in the reviews as being insufficient.\n\nLooking at the reviewer comments, there's a lot of criticism. Reviewer 1 says the presentation needs improvement and that experiments should be on more complex datasets. Reviewer 2 points out poor writing, vague contributions, and weak experiments (only toy and MNIST without comparing to state-of-the-art). Reviewer 3 is harsh, noting reliance on unpublished work, similarity to existing architectures (like Highway Networks), and insufficient experiments. They also mention typos and lack of statistical significance. Reviewer 4 highlights clarity issues, lack of baseline comparisons, and questions about the theoretical underpinnings. Reviewer 5 and 6 also stress the lack of motivation, insufficient experiments, and unclear presentation, especially around the unpublished work and the clustering step.\n\nThe authors' response addresses some points, like agreeing to do more experiments and clarify explanations. They mention they need to better present their theory and that their main contributions are theoretical insights and an algorithm that approximates these in practice. They also clarify that their method is neuron pruning, not weight pruning, and that their clustering is based on a specific vector representation. However, they admit the paper's current state isn't clear enough and that they need to fix the presentation and experiments.\n\nNow, considering the standards of a top-tier conference, the paper must make a significant contribution, be clearly presented, and have thorough experiments. The main issues here are:\n\n1. **Theoretical Foundation**: The paper heavily relies on an unpublished work, which reviewers can't verify. This is a big red flag because without being able to check the foundational theory, the validity of the claims is questionable. Even if the authors have a good theory, referencing an unpublished paper that's not submitted to the same conference makes it hard for reviewers to assess.\n\n2. **Experimental Weakness**: The experiments are on simple datasets (toy and MNIST). Top conferences require validation on more complex datasets like CIFAR or ImageNet. Also, there's no comparison with existing pruning methods, which is essential for establishing novelty and superiority. The authors mention they plan to add comparisons with TensorFlow's pruning, but the current submission lacks this.\n\n3. **Clarity and Presentation**: Multiple reviewers point out that the paper is hard to follow, with unclear explanations, missing definitions, and typos. The algorithm's steps, especially the clustering part, aren't well-motivated or derived. The authors admit they need to restructure the paper to better explain their theory and method.\n\n4. **Novelty Concerns**: Reviewers note that parts of the method (like removing dead neurons or combining always-active ones) might not be novel. The authors argue their theoretical underpinning is new, but without the theory being accessible, it's hard to judge. The architecture similarity to Highway Networks also raises questions about originality.\n\n5. **Technical Soundness**: There are unresolved questions about the algorithm's steps. For instance, the clustering step's mathematical justification isn't clear, and the approximation's computational complexity isn't fully addressed. The authors mention a typo in an equation, which suggests potential deeper issues.\n\nConsidering all this, even though the authors have a promising idea (the theoretical framework for neuron pruning based on regularization), the current submission falls short in several critical areas. The reliance on unpublished work undermines the credibility, the experiments are insufficient, and the presentation is too unclear. While the authors promise improvements, conference deadlines are strict, and the paper as submitted doesn't meet the standards. Top conferences require a complete, polished work that's thoroughly evaluated and clearly presented. The reviewers' consensus leans towards rejection, especially since major issues like experiments and clarity can't be fixed post-submission without resubmitting a revised version.\n\nFinal Decision: Reject\n</think>\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let me start by going through all the information provided. The paper is about MQES, a method for exploration in continuous RL using mutual information maximization between the exploration policy and the optimal Q-function distribution. The reviewers have given several comments, and the authors have responded to them. I need to evaluate if this paper should be accepted to a top-tier conference.\n\nFirst, looking at the paper's contributions. The main idea is combining information-theoretic exploration with distributional RL to handle both aleatoric and epistemic uncertainty. That's a novel approach, especially for continuous action spaces, which is a step beyond discrete methods. The authors mention experiments on Mujoco where they outperform SAC and DSAC, which are strong baselines. However, the reviewers point out some issues.\n\nReviewer 1 mentions that the presentation is unclear, especially regarding how the policies π_E and π_T are parameterized. The authors addressed this by clarifying in their response that π_T is parameterized by φ and π_E is derived without extra parameters. They also added more details in the revised version. That's good, but I need to check if the revised version sufficiently addresses the parameterization confusion.\n\nReviewer 2 notes that the experiments are not discussed enough. The authors added ablation studies and more detailed results on sparse tasks. They also moved some algorithms to the main paper where possible. The performance comparison with DSAC is close, but the authors argue that in harder tasks, MQES does better. The horizon setting (100 vs 1000) was addressed by the authors saying it's for efficiency and fair comparison. That seems reasonable, but maybe the reviewers would want to see results on standard horizons too.\n\nReviewer 3 points out that the improvement over DSAC isn't significant except in a few cases. The authors added experiments on sparse environments where MQES performs better. They also did ablation studies on hyperparameters. The notation issues in equations like 8 and 9 were reworked, which should help clarity. The mutual information explanation was clarified, which is important for the theoretical foundation.\n\nReviewer 4 criticizes the writing as haphazard and the lack of ablations. The authors revised the theoretical sections, added ablations, and clarified notation. They also addressed the comparison with OAC by including it in experiments. The P-values were added for statistical significance, which is a good point.\n\nReviewer 5 mentions that the method might not work well in sparse rewards because it's tied to the Q-function. The authors counter that epistemic uncertainty helps in sparse settings, and their experiments on sparse tasks support this. They also compared with VIME, noting it's model-based vs their model-free approach. The clarity issues were addressed by rephrasing sections and equations.\n\nThe author responses seem thorough. They addressed notation, added experiments, clarified the theory, and fixed typos. The main concerns were about clarity, experimental thoroughness, and comparison with baselines. The revisions seem to have tackled these points. The method's novelty is there, and the experiments, while not earth-shattering, show improvement in specific scenarios. The ablations and additional experiments strengthen the case.\n\nHowever, some reviewers still had issues with the empirical evaluation being limited. But the authors added more tasks and ablations. The key is whether the contributions are significant enough for a top conference. The approach combines two important concepts (information theory and distributional RL) in a novel way for continuous RL, which is a solid contribution. The experiments, though not showing huge gains, do demonstrate effectiveness in certain cases, especially sparse environments where exploration is critical.\n\nThe writing issues were a problem, but the authors have made efforts to clarify the theory and structure. The revised version should be more accessible. The method's practical implementation and the closed-form solution for the policy are positive points. The fact that they can handle both uncertainties is a plus.\n\nConsidering all this, the paper has merit. The contributions are novel, the experiments, while not perfect, are improved with revisions, and the theoretical underpinnings are clarified. It meets the standards for a top conference if the revisions are properly implemented. The authors have addressed most concerns, so I think it should be accepted.\n</think>\n\nFinal Decision: Accept\n\nThe paper proposes MQES, a novel exploration strategy for continuous RL that combines information-theoretic principles with distributional RL to address both aleatoric and epistemic uncertainties. While initial reviews highlighted concerns about clarity, experimental thoroughness, and comparison with baselines, the authors' responses and revisions effectively address these issues:\n\n1. **Theoretical Clarity**: The authors reworked key equations (e.g., Eqs. 8, 9) to clarify the mutual information formulation and parameterization of policies. They also improved notation consistency and provided clearer explanations of epistemic/aleatoric uncertainty distinctions, strengthening the theoretical foundation.\n\n2. **Empirical Evaluation**: The authors added ablation studies on hyperparameters (e.g., α, β, C) and conducted experiments on sparse Mujoco tasks, demonstrating MQES's superiority in challenging scenarios. They also addressed reviewer concerns about horizon settings and statistical significance by including P-values and comparing with OAC.\n\n3. **Baseline Comparisons**: While MQES shows modest improvements over DSAC in standard tasks, its performance in sparse environments is compelling. The distinction between model-based (VIME) and model-free approaches was clarified, and the revised experiments better justify MQES's advantages.\n\n4. **Writing and Structure**: The paper was restructured for better flow, and notation ambiguities were resolved. Key sections (e.g., Sec. 4.1) were revised to enhance readability, addressing reviewers' concerns about haphazard presentation.\n\n5. **Novelty and Impact**: The integration of mutual information maximization with distributional Q-function approximations is a meaningful contribution to exploration in continuous RL. The closed-form policy update and handling of dual uncertainties provide a fresh perspective, aligning with top-tier conference standards.\n\nWhile minor limitations (e.g., horizon choices, slight performance gaps in some tasks) remain, the revisions sufficiently address core criticisms. The paper’s novel approach, theoretical grounding, and empirical validation in critical scenarios justify its acceptance."
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract mention a new Reduction Algorithm for Deep ReLU Networks based on regularization theory. The key contributions seem to be a method to reduce neurons significantly while maintaining function accuracy. The experiments are on toy data and MNIST, which are mentioned in the reviews as being insufficient.\n\nLooking at the reviewer comments, there's a lot of criticism. Reviewer 1 says the presentation needs improvement and that experiments should be on more complex datasets and compared to state-of-the-art. Reviewer 2 points out poor writing, vague contributions, and weak experiments (only toy and MNIST without proper comparisons). Reviewer 3 is harsh, noting reliance on unpublished work, similarity to existing architectures (like highway networks), and insufficient experiments. They also mention typos and lack of statistical significance. Reviewer 4 highlights clarity issues, lack of baseline comparisons, and questions about the theory's foundation. Reviewer 5 says the technique lacks motivation, experiments are insufficient, and parts can't be verified due to unpublished work. Reviewer 6 also criticizes the writing, mentions the method isn't novel, and points out undefined terms like \"P-functional.\"\n\nThe authors' response addresses some points, like agreeing on the need for clearer explanations and more experiments. They mention they'll add comparisons with TensorFlow's pruning and acknowledge the need for better presentation. However, they still rely on unpublished work, which is a problem because reviewers can't verify those claims. The authors also explain their clustering approach but admit it's not well-explained in the paper.\n\nNow, considering the standards of a top-tier conference, the paper must make a clear, novel contribution with solid theoretical backing and empirical validation. The main issues here are:\n\n1. **Theoretical Foundation**: The paper heavily references an unpublished work, making it hard for reviewers to assess the validity of the claims. Top conferences require all foundational work to be accessible or properly cited. Since the referenced work isn't available, the theoretical underpinnings are unverifiable.\n\n2. **Novelty and Originality**: Reviewers point out that parts of the method (like removing dead neurons or combining always-active ones) aren't novel. The clustering step is the main contribution but lacks clear derivation and comparison to existing methods. The architecture similarity to highway networks without clear differentiation is a problem.\n\n3. **Experimental Validation**: Experiments are limited to toy datasets and MNIST. While MNIST is a standard benchmark, it's too simple for a top conference. Without results on more complex datasets like CIFAR or ImageNet, the practical effectiveness isn't demonstrated. Also, no comparison to state-of-the-art pruning techniques (e.g., magnitude-based, Hessian-based) undermines the contribution's value.\n\n4. **Presentation and Clarity**: Multiple reviewers note poor writing, unclear explanations, and typos. The method isn't well-explained, especially the clustering step. The authors admit this in their response but haven't addressed it in the current submission.\n\n5. **Technical Soundness**: Concerns about the algorithm's steps (like the third step's equations) and the approximation's justification remain. The authors mention that their method theoretically preserves the function but practically uses approximations without sufficient justification.\n\n6. **Ethical and Practical Considerations**: The authors mention computational complexity but don't provide detailed analysis. The response mentions future experiments but these aren't in the current submission.\n\nConsidering all this, the paper's main issues are the reliance on unpublished work, insufficient novelty, weak experiments, and poor presentation. While the authors show willingness to improve, the current submission doesn't meet the standards of a top-tier conference. The theoretical claims can't be verified, the experiments are inadequate, and the method isn't clearly explained or compared to existing work. Without these elements, the paper can't be accepted as is.\n</think>\n\nFinal Decision: Reject\n\nThe paper proposes a neuron reduction algorithm for ReLU networks based on regularization theory but faces significant shortcomings that preclude acceptance at a top-tier conference. Key issues include:\n\n1. **Unverifiable Theoretical Foundations**: Critical reliance on an unpublished work makes it impossible for reviewers to validate the core theoretical claims. Top-tier conferences require foundational work to be accessible or properly cited to ensure rigor.\n\n2. **Lack of Novelty**: Core components of the method (e.g., removing dead neurons, combining always-active neurons) are not novel. While the clustering step is theoretically interesting, it lacks clear derivation and differentiation from existing methods like highway networks.\n\n3. **Insufficient Experiments**: Experiments are limited to toy datasets and MNIST, which are inadequate for demonstrating practical utility. No comparisons to state-of-the-art pruning techniques (e.g., magnitude-based, Hessian-based) or complex datasets (e.g., CIFAR, ImageNet) undermine the contribution's relevance.\n\n4. **Poor Presentation and Clarity**: The paper suffers from unclear explanations, undefined terms (e.g., \"P-functional\"), and numerous typos. The clustering algorithm’s mathematical justification and implementation details are inadequately explained, even after author responses.\n\n5. **Technical Soundness Concerns**: Questions remain about the algorithm’s approximation steps, computational complexity, and the validity of its theoretical claims. The authors admit these issues but have not addressed them in the current submission.\n\nWhile the authors express intent to improve clarity and add experiments, the current version fails to meet the standards of a top-tier conference due to unverifiable theory, weak novelty, and insufficient empirical validation. These issues cannot be overlooked without risking the conference’s reputation for rigor and impact."
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design.\" The main contributions seem to be a new approach using a transformer-based model for layout representation, a large dataset of over a million slides, and some downstream tasks like element role labeling and image captioning. The reviewers have given their comments, and the authors have responded.\n\nFirst, looking at the strengths. The dataset is a big plus—over a million slides. That's a significant contribution because previous work used much smaller datasets. The authors mention that they parsed the slides automatically, which is good, but the reviewers pointed out that details about the dataset (like number of elements per slide, how properties were parsed) were missing. The authors addressed this by saying they'll add more details, so that's a point in their favor.\n\nThe method uses transformers, which are popular in NLP, adapted here for layout. Reviewers 1 and 2 noted that the motivation for using transformers wasn't well-explained. The authors responded by saying they'll add more details on their model architecture and why transformers are suitable. That's an improvement they can make, but maybe the original submission was lacking in clarity here.\n\nThe evaluation is a concern. Reviewers pointed out that the tasks (element role labeling and image captioning) might not be sufficiently challenging, and the baselines were weak (like decision trees). The authors agreed to add more baselines, including neural networks and existing methods like LayoutGAN or GNNs. They also mentioned improving the evaluation tasks to be more complex. However, in the current version, the experiments might not be strong enough for a top conference. The layout auto-completion and retrieval tasks were criticized for being simplistic or not following prior work's setups. The authors plan to adjust these, which is good, but the current submission's evaluation is lacking.\n\nOriginality: The idea of applying transformers to layout representation isn't entirely new, but the dataset and pre-training approach might be novel. However, Reviewer 2 noted that Cao et al. 2019 did pre-training on layouts, albeit on a smaller scale. The authors acknowledged this and said they'll clarify differences, which is important. The key difference is the scale of the dataset and the transformer approach, which could be a valid contribution.\n\nClarity: The paper's writing is generally good, but the related work section was criticized for not clearly differentiating from prior work. The authors agreed to improve this. The model description was sparse, so adding more details (like figures or equations) would help, which they plan to do.\n\nPotential impact: The dataset and the pre-training approach could be impactful if the evaluation is solid. The authors' responses show they're willing to address the reviewers' concerns, but the current version might not meet the high standards of a top conference yet.\n\nNow, considering the conference's standards. Top-tier conferences require strong originality, thorough evaluation, and clear contributions. The dataset is a major plus, but the evaluation in the current submission is weak. The baselines are not strong enough, and the tasks could be more challenging. The method's novelty is there but needs better justification. The authors have a plan to address these issues, but the question is whether the current submission meets the bar.\n\nHowever, the reviewers' comments highlight several critical issues that haven't been fully addressed yet. For example, the lack of comparison with LayoutGAN or GNNs, the unclear dataset details, and the simplistic evaluation tasks. The authors' responses are promising, but the paper as it stands might not be sufficient. Top conferences often require the evaluation to be robust and comprehensive in the submitted version. Since the authors are planning improvements, maybe a \"revise and resubmit\" would be better, but the question is whether to accept or reject now.\n\nWait, the user is asking to decide based on the current information. The reviewers have pointed out significant flaws in evaluation and comparisons. The authors have responded positively, but the current version's deficiencies might be too severe for acceptance. The dataset is a big plus, but without proper evaluation against strong baselines and more thorough experiments, it's risky. The paper might be borderline, but given that top conferences are highly competitive, the current submission might not meet the bar. However, the dataset's contribution is substantial, so maybe the reviewers would lean towards accept with major revisions. But the user's instruction is to decide based on the current info, not after revisions.\n\nHmm. The reviewers' consensus seems to be that the paper has potential but needs significant improvements in evaluation and comparisons. Reviewer 1 and 2 both mention weak evaluation and lack of comparisons with key prior works. Reviewer 3 also points out the evaluation is lacking. The authors have a plan to address these, but the current submission's evaluation is insufficient for a top conference. Therefore, the decision might be to reject, but with an invitation to resubmit after revisions. However, the user's options are only accept or reject. Given that the paper is missing key elements in evaluation and comparisons, and the current version's flaws are substantial, I think the decision would be to reject. But the dataset is a major contribution. Wait, maybe the reviewers would recommend accept with major revisions, but the user's format requires a final decision here. Since the user's context says \"determine whether it should be accepted or rejected for publication,\" and the reviewers have pointed out critical issues that aren't yet fixed, I think the answer is to reject. However, sometimes top conferences accept with major revisions, but the user's options are strict. So, based on the current submission's flaws, the decision is reject.\n</think>\n\nThe paper proposes CANVASEMB, a transformer-based approach for learning layout representations through large-scale pre-training on a novel dataset of over one million parsed slides. While the contributions are notable—particularly the dataset and the adaptation of transformer architectures to layout tasks—the current submission falls short of the rigorous standards required for a top-tier conference due to critical unresolved issues in evaluation, comparisons, and clarity. \n\n### Key Strengths:\n1. **Dataset Contribution**: The large-scale slide dataset is a significant contribution, addressing the scarcity of layout data in prior work. This alone could be impactful if properly contextualized and validated.\n2. **Novel Approach**: The use of transformers for layout representation is innovative, leveraging pre-training strategies inspired by NLP. The multi-task learning objective and joint conditioning on canvas elements show promise.\n3. **Potential Applications**: The paper highlights downstream tasks like layout auto-completion and retrieval, demonstrating real-world relevance.\n\n### Critical Weaknesses:\n1. **Evaluation Shortcomings**:\n   - **Weak Baselines**: The current baselines (e.g., decision trees) are insufficient. Stronger neural baselines (e.g., LayoutGAN, GNNs) and prior pre-training methods (e.g., Cao et al. 2019) are missing, making it unclear whether the proposed method truly outperforms state-of-the-art approaches.\n   - **Task Complexity**: The chosen tasks (element role labeling, image captioning) are simplistic. More challenging tasks (e.g., geometry prediction, full layout generation) and systematic evaluations (e.g., geometric/color property prediction) are needed to validate the model’s capabilities.\n   - **Layout Auto-Completion and Retrieval**: These tasks are under-specified. Auto-completion only predicts single attributes, and retrieval relies on subjective human scoring without quantitative metrics. The authors acknowledge these issues but have not yet addressed them.\n\n2. **Lack of Clear Differentiation from Prior Work**:\n   - The paper does not adequately distinguish itself from prior pre-training efforts (e.g., Cao et al. 2019) or layout generation methods (e.g., LayoutGAN, READ). The related work section is disjointed, and the motivation for using transformers over GNNs is under-explained.\n   - The dataset’s properties (e.g., element counts, parsing details) are insufficiently described, making it hard to assess the model’s scalability or the dataset’s utility.\n\n3. **Clarity and Technical Gaps**:\n   - The transformer architecture and training details are under-specified, requiring readers to infer key components. The authors plan to add figures and equations but have not yet done so.\n   - The abstract and introduction ambiguously use terms like \"pre-training,\" leading to confusion about the model’s workflow.\n\n### Author Responses:\nThe authors acknowledge most critiques and propose improvements (e.g., adding baselines, refining tasks, clarifying the dataset). However, these changes are future plans rather than present in the submission. Top-tier conferences require the final version to meet standards, not just promises of revisions.\n\n### Conclusion:\nWhile CANVASEMB has potential, the current submission’s weak evaluation, incomplete comparisons, and lack of technical clarity prevent it from meeting the high bar of a top-tier conference. The dataset’s value is undeniable, but its impact hinges on rigorous validation and differentiation from prior work—both of which are currently lacking. The authors’ proposed revisions could elevate the work, but the submission as-is does not justify acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is titled \"Global Node Attentions via Adaptive Spectral Filters\" and focuses on improving graph neural networks (GNNs) for graphs with disassortative structures, where local homophily isn't a safe assumption. The authors propose a model called GNAN that uses global self-attention with learnable spectral filters. They claim it outperforms existing methods on disassortative graphs and is competitive on assortative ones. \n\nLooking at the reviewers' comments first. Reviewer 1 is critical. They question the novelty, arguing that ChevNet can already handle distant nodes with a large K and that the failure of ChevNet is due to overfitting, not its design. They also dispute the computational complexity claims, pointing out that the method requires full eigen decomposition, making it O(V²), which is not scalable. Reviewer 1 also mentions the lack of full complexity comparisons and concludes the paper doesn't meet ICLR's bar.\n\nReviewer 2 is positive, finding the paper clear and the experiments thorough, but defers due to lack of expertise in evaluating experiments. Reviewer 3 points out weaknesses like scalability issues (global attention's O(N²) complexity), small datasets for disassortative graphs, and limited evaluation tasks (only node classification). They suggest more experiments on larger datasets and synthetic graphs. Reviewer 4 also raises novelty concerns, comparing to existing non-local GNNs and ChevNet, and criticizes the scalability, noting that even medium-sized graphs would be problematic. They also mention the datasets used are too small.\n\nThe authors responded by addressing complexity through Chebyshev polynomial approximations, claiming O(m|E|) complexity, and provided runtime comparisons. They added experiments on Chameleon (a larger disassortative dataset) and mentioned working on others. They also clarified that their method's spectral filters are learned adaptively, distinguishing it from ChevNet and others.\n\nNow, considering the key points:\n\n1. **Novelty**: The authors argue their method's adaptive spectral filters allow flexibility beyond existing methods. However, Reviewers 1 and 4 question the novelty, pointing out that non-local attention and spectral methods like ChevNet already exist. The authors counter by emphasizing the learnable filters and multi-head attention, which might offer more adaptability. But is this enough for a top-tier conference? The problem of disassortative graphs is recognized, but the solution's novelty is debatable.\n\n2. **Complexity**: The main issue here is scalability. The authors claim O(m|E|) via Chebyshev approximations, but Reviewer 1 and 4 are skeptical. The runtime results in the author response show GNAN is slower than GAT but faster than Geom-GCN. However, the datasets tested (like Chameleon with ~2k nodes) are still small. For very large graphs, the O(N²) concern remains. The sparsification techniques help but don't eliminate the fundamental complexity.\n\n3. **Experiments**: The authors added Chameleon but still lack larger disassortative datasets. The original experiments had only three such datasets, which are small. This weakens the empirical validation, especially since scalability is a key issue. The comparison with other methods on runtime is a step forward, but more is needed.\n\n4. **Theoretical Contributions**: Reviewer 2 noted no theoretical results. The paper's contributions are mainly empirical and methodological. While that's acceptable, top conferences often prefer some theoretical grounding, especially if the method is incremental.\n\n5. **Clarity and Writing**: Reviewer 2 found it well-written, which is a plus. The authors addressed some points in the rebuttal, improving clarity.\n\nBalancing these factors: The paper addresses an important problem (disassortative graphs) and shows promising results where existing methods struggle. The method's adaptive spectral attention is a reasonable contribution. However, the scalability issues are significant. If the complexity is indeed O(m|E|) as authors claim, that's better than O(N²), but the rebuttal's runtime comparisons are on small datasets. Without convincing evidence that it scales to large graphs, this is a red flag for a top conference. Also, the novelty is incremental compared to existing non-local methods like [1], which Reviewer 4 pointed out. The experiments, while improved, still lack sufficient validation on larger datasets and tasks beyond node classification.\n\nTop-tier conferences require strong novelty, thorough experiments, and scalability. The paper's contributions are solid but not groundbreaking. The unresolved complexity concerns and limited dataset sizes might tip the decision towards rejection unless the rebuttal sufficiently addressed scalability. However, the authors' rebuttal claims O(m|E|) via Chebyshev, but Reviewers 1 and 4 remain unconvinced. Since the reviewers' skepticism isn't fully resolved, and the experiments don't yet demonstrate scalability on large graphs, the paper might not meet the high bar.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Novelty Concerns**: While the paper addresses an important problem (disassortative graphs), the proposed method's core idea—global attention via spectral filters—is not sufficiently novel. Reviewers 1 and 4 highlighted that non-local attention (e.g., [1]) and spectral methods like ChevNet already exist. The authors argue their adaptive spectral filters and multi-head attention offer improvements, but these contributions are incremental rather than groundbreaking for a top-tier conference. The rebuttal clarifies distinctions but does not resolve the perception of limited novelty.\n\n2. **Scalability Issues**: The computational complexity remains a critical barrier. Despite the authors' claims of O(m|E|) via Chebyshev approximations, Reviewers 1 and 4 remain unconvinced, noting that eigen decomposition and global attention inherently require O(V²) operations. The runtime experiments on small datasets (e.g., Chameleon with ~2k nodes) do not demonstrate scalability for large graphs. The sparsification techniques help but do not fully mitigate the complexity concerns, making the method impractical for real-world large-scale applications.\n\n3. **Empirical Validation**: While experiments show promise on small disassortative datasets, the lack of results on larger graphs (e.g., Actor, Squirrel) undermines the practical relevance. The authors mention ongoing work but have not yet provided sufficient evidence of performance on medium-to-large graphs. Additionally, the focus on node classification alone limits the scope, as other tasks (e.g., link prediction) remain unexplored.\n\n4. **Theoretical Gaps**: The paper lacks theoretical analysis (e.g., generalization bounds, expressiveness proofs), which is a common expectation for top-tier venues. The empirical results are thorough but not paired with deeper theoretical insights to justify the method's advantages over existing approaches.\n\n5. **Reviewer Consensus**: While Reviewer 2 was positive, the critical reviews from Reviewers 1, 3, and 4 highlight unresolved issues in novelty, scalability, and dataset size. The rebuttal addressed some points but did not fully alleviate concerns about complexity or novelty, leading to a lack of consensus in favor of acceptance.\n\n### Conclusion:\nThe paper presents a valuable contribution to handling disassortative graphs but falls short of the novelty, scalability, and empirical rigor required for a top-tier conference like ICLR. The unresolved complexity concerns and incremental nature of the method, combined with limited validation on large datasets, make it more suitable for a venue with lower acceptance standards."
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about using Fourier convolutional networks for 3D snapshot microscopy. The main idea is that traditional UNet architectures aren't good at handling the highly non-local optical encodings needed here, so they propose a Fourier-based approach which can capture global context more efficiently. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 and 2 are more positive, but Reviewer 3 and 4 point out that the technical novelty isn't high because the Fourier approach was already in Rippel 2015. Reviewer 5 is concerned about the lack of real data and the evaluation only on one type of volume. Reviewer 6 mentions the lack of real-world experiments and scalability issues, but thinks it's still acceptable. Reviewer 7 thinks it's application-focused but the method isn't super novel.\n\nThe authors addressed some points, like citing Rippel and adding more details. They argue their contribution is in applying FourierNets to this specific problem and showing their effectiveness in end-to-end optimization. They also mention that their simulations are thorough and real data is hard to get, but they have a unique dataset.\n\nThe key issues here are technical novelty and empirical validation. The reviewers agree the paper is well-written and the results are promising, but the method isn't entirely new. However, the application to 3D microscopy and showing that FourierNets work better here could be a significant contribution. The lack of real-world experiments is a downside, but the authors explain that simulation is sufficient for their claims, especially since the main contribution is the framework for simulation-based optimization.\n\nTop-tier conferences usually require both technical novelty and strong empirical validation. While the method isn't groundbreaking in ML terms, the application and the demonstration of effectiveness in a challenging domain might tip the balance. The authors have addressed some concerns, and the reviewers' scores after rebuttal are mostly 6 (marginally accept). \n\nConsidering all this, even though the technical novelty is limited, the application and the thorough simulations might make it acceptable for a top conference, especially if the venue values applied ML in specific domains like computational imaging.\n</think>\n\nThe paper proposes the use of Fourier convolutional networks (FourierNets) to address challenges in 3D snapshot microscopy, where traditional UNet architectures struggle with highly non-local optical encodings. The authors demonstrate that their approach outperforms UNets in both end-to-end optimization of optical encoders and reconstruction tasks, supported by simulations and a lensless imaging dataset. Key points from the review process include:\n\n### Strengths:\n1. **Technical Contribution**: The paper effectively addresses a specific challenge in computational microscopy by leveraging FourierNets to handle non-local optical encodings. This application is novel and impactful for the field, even if the core idea of Fourier-based convolutions is not entirely new.\n2. **Empirical Validation**: The authors provide thorough simulations across multiple scenarios (e.g., different zebrafish imaging tasks) and validate their method on a real-world lensless imaging dataset. The results show clear improvements over existing methods.\n3. **Clarity and Writing**: The paper is well-written, with a clear motivation, methodology, and limitations section. The authors address reviewer concerns thoughtfully in their rebuttal, including additional citations and implementation details.\n4. **Scalability**: The distributed multi-GPU simulation framework for optimizing large optical encoders (e.g., 10^6 parameters) is a practical contribution for future work in programmable microscopy.\n\n### Weaknesses:\n1. **Technical Novelty**: Reviewers note that the core idea of Fourier-domain convolutions (e.g., Rippel et al., 2015) is not novel, and the paper’s contributions lie more in application and adaptation to microscopy. This limits its significance for purely ML-focused venues like ICLR.\n2. **Lack of Real-World Experiments**: While simulations are extensive, the absence of experimental validation with real optical hardware introduces uncertainty about real-world applicability. The authors argue this is acceptable due to the focus on simulation-based optimization, but this remains a limitation.\n3. **Evaluation Scope**: The 3D microscopy experiments rely on a single type of biological sample (zebrafish), and the lensless imaging results, while strong, are tangential to the paper’s primary focus. More diverse or real-world validation would strengthen the claims.\n\n### Reviewer Consensus:\n- **Acceptance Threshold**: Most reviewers (1, 2, 4, 6, 7) lean toward acceptance, citing the paper’s strong application focus, thorough experiments, and clear improvements over baselines. Reviewer 3 and 5 express reservations about novelty and evaluation but acknowledge the work’s merit.\n- **Key Concerns Addressed**: The authors clarified the relationship to prior work (e.g., Rippel 2015), provided additional implementation details, and defended their simulation-based approach. These responses sufficiently addressed minor criticisms.\n\n### Final Judgment:\nWhile the technical novelty is incremental, the paper’s **novel application of FourierNets to 3D snapshot microscopy** and its demonstration of practical benefits in a high-impact domain (e.g., neuroscience imaging) justify its acceptance. The thorough simulations and lensless imaging results provide sufficient empirical validation for a top-tier conference, especially given the challenges of real-world experimentation in this field. The reviewers’ post-rebuttal scores (mostly 6) and the paper’s alignment with ICLR’s interdisciplinary scope (e.g., applications in computational imaging and neuroscience) further support this decision.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about applying boosting techniques to reinforcement learning (RL) in Markov decision processes (MDPs), aiming for algorithms whose complexity doesn't depend on the number of states. The key contributions seem to be using a non-convex Frank-Wolfe method and gradient boosting to handle the non-convex value function, leading to sample complexity and runtime bounds independent of the state space. \n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 and 3 are more positive, with 1 wanting a rewrite for clarity and 3 giving a weak accept. Reviewers 2, 4, 5, and 6 have significant concerns. \n\nTechnical novelty is a point of contention. Reviewer 2 and 3 mention that the approach is similar to Hazan and Singh 2021 but note that the current work handles non-convexity, which the authors argue is a key difference. However, Reviewer 2 thinks the novelty is limited and the paper doesn't address differences between contextual bandits and RL. The authors responded by highlighting their non-convex approach versus the convex one in prior work, which might be a valid distinction but needs to be clearer in the paper.\n\nClarity is a major issue across multiple reviews. Reviewers 4 and 5 point out undefined notations, dense writing, and lack of intuition. The authors acknowledge this and plan to add a notation list and more explanations. But since the reviews are based on the current version, the lack of clarity is a current barrier. \n\nEmpirical validation is missing, which Reviewers 2 and 6 note. The authors argue the focus is theoretical, but top conferences often expect at least preliminary experiments, especially since Hazan & Singh included them. This could be a weakness. \n\nThe technical correctness is mostly okay except for some minor issues. The sample complexity claims are solid, but the presentation's flaws might overshadow that. \n\nThe authors' responses address some points, like adding a notation list and explaining the weak learner's role. However, the paper's current state has significant readability issues. For a top-tier conference, clarity is crucial. Even if the ideas are good, the presentation must be polished. \n\nConsidering all this, the paper has novel contributions but is let down by its lack of clarity and experiments. The reviewers' recommendations lean towards weak accept but with strong reservations. However, top conferences usually require both strong technical contributions and clear exposition. Since the clarity issues are substantial and the novelty might be incremental, it's a tough call. \n\nThe authors' responses suggest they can address some issues, but the reviews are based on the current version. Without experiments and with the current clarity problems, it might not meet the high standards. But the core idea of handling non-convexity in boosting for RL is interesting. \n\nHmm, maybe a borderline case. But given that multiple reviewers (especially 4 and 5) found it hard to follow and the empirical gap, perhaps it's better to reject unless revisions can fix these. Since the decision is based on the current submission, the flaws might be too severe for acceptance.\n</think>\n\nThe paper proposes a boosting approach for reinforcement learning in MDPs, aiming to achieve sample complexity and runtime bounds independent of the number of states. The core idea involves using a non-convex variant of the Frank-Wolfe method to address the non-convexity of the value function, which is a novel contribution. However, the paper faces significant challenges in clarity, novelty, and practical validation:\n\n### Strengths:\n1. **Technical Contribution**: The paper introduces a boosting framework for RL that leverages non-convex optimization techniques (e.g., Frank-Wolfe) to handle the non-convexity of the value function. This distinguishes it from prior work like Hazan & Singh 2021, which assumes convexity. The theoretical analysis is rigorous, and the sample complexity bounds are compelling.\n2. **Novelty**: While reviewers note some overlap with prior boosting techniques, the extension to non-convex settings in RL is a meaningful step forward, particularly given the challenges of global optimality in such scenarios.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers (4, 5, 6) highlight severe readability issues. Undefined notations, dense mathematical exposition, and a lack of intuition or examples make the paper difficult to follow. Even the algorithm’s practical implementation (e.g., computing gradients, defining weak learners) is unclear. The authors acknowledge these issues and propose fixes, but the current version is not polished enough for a top-tier conference.\n2. **Empirical Validation**: The paper lacks experiments, which is a critical gap. While the authors argue the focus is theoretical, top conferences typically expect at least preliminary empirical validation to demonstrate practical relevance. Competing work (e.g., Hazan & Singh 2021) includes experiments, making this omission notable.\n3. **Novelty Concerns**: Reviewers 2 and 4 question the technical novelty, noting that the approach is incremental compared to existing boosting methods. The authors’ response emphasizes the non-convexity distinction, but this is not sufficiently emphasized in the paper itself, leaving the contribution’s significance unclear.\n4. **Lack of Motivation and Context**: The paper does not adequately address how its approach differs from state compression techniques for large state spaces or contextual bandits. The motivation for boosting in RL is underdeveloped, and key assumptions (e.g., weak learners, distribution mismatch) are not well-justified or contextualized.\n\n### Author Responses:\nThe authors acknowledge clarity issues and propose fixes (e.g., notation lists, expanded intuition). They also clarify the distinction from prior work (non-convex vs. convex costs) and defend the theoretical focus. However, these responses are post-hoc and do not address the current submission’s flaws. The lack of experiments and weak learner examples remains unaddressed.\n\n### Final Assessment:\nWhile the paper’s core idea is innovative and theoretically sound, its current state is hindered by poor presentation, lack of empirical validation, and insufficient contextualization. For a top-tier conference, clarity and practical relevance are non-negotiable. The technical novelty, though present, is incremental and not clearly communicated. The authors’ proposed revisions could improve the paper, but the submission as-is does not meet the high standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about Target Propagation (TP) via Regularized Inversion, aiming to offer an alternative to backpropagation (BP). The reviewers have mixed opinions. \n\nStarting with Reviewer 1, they're not too happy. They point out that the proposed TP method requires weight transport, which limits its appeal for neuromorphic hardware and distributed systems. Also, the computational cost is higher than BP, especially in shorter sequences. The experiments aren't convincing enough, and there's a lack of theoretical guarantees. They question the novelty compared to existing work like Lee et al. and Meulemans, and there are technical concerns about ReLU activations and vanishing/exploding gradients. The reviewer thinks the algorithm loses key advantages of TP, so it's not a strong alternative to BP.\n\nReviewer 2 is more positive. They see the method as novel, especially the use of regularized inversion with autograd. They acknowledge the computational benefits with long sequences and the theoretical bound between TP and BP. However, they note the experiments have high variance and the empirical significance is only marginal. The clarity is good except for a typo. They wonder about convergence proofs, similar to feedback alignment studies.\n\nReviewer 3 is on the fence. They like the clear presentation but criticize the experiments. The accuracy on CIFAR-10 is too low (under 20%), which makes the method's practicality questionable. They suggest comparing with more competitive baselines and testing on CNNs, not just RNNs. The conclusion leans towards rejection but is open to author feedback.\n\nReviewer 4 sees potential but wants more theory and experiments. They point out the loose bounds in Lemma 3.1 and the need to better connect TP to Gauss-Newton methods. Empirically, the experiments are insufficient; they need more comparisons with existing RNN optimization methods like Hessian-Free. The improvements over GD aren't well-explained, and robustness isn't shown.\n\nThe authors didn't respond, so we can't address their points. \n\nLooking at the key issues: \n- The main problem is that the proposed TP method doesn't overcome the main drawbacks of TP. It requires weight transport, which is a big downside compared to methods like feedback alignment. \n- Computational cost is higher than BP, which is a significant barrier for adoption unless there's a clear benefit. The paper doesn't show strong enough performance gains, especially in real-world datasets (CIFAR-10 results are weak). \n- Theoretical underpinnings are lacking. The relationship to Gauss-Newton isn't fully explored, and there's no convergence proof. \n- Experiments are limited in scope and don't compare against strong baselines. The high variance in results is concerning. \n\nWhile there's some novelty in the regularized inversion approach and the autograd implementation, the paper doesn't provide enough evidence that this method is a viable alternative to BP. The reviewers' consensus (especially the first three) lean towards rejection due to insufficient empirical validation, theoretical gaps, and failure to address key limitations of TP. Even Reviewer 2 and 4, who see some merit, highlight critical shortcomings that aren't addressed. \n\nTherefore, the paper doesn't meet the high standards of a top-tier conference where strong empirical results and theoretical contributions are expected. The experiments are underwhelming, and the method doesn't overcome the main challenges that make TP less attractive than BP. Without a stronger demonstration of effectiveness or novel insights, it's better to reject.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\nThe paper proposes a variant of Target Propagation (TP) via regularized inversion, aiming to address challenges in training recurrent neural networks (RNNs). While the method introduces an analytically solvable approach to TP and provides some theoretical insights, the reviewers' critiques collectively highlight critical shortcomings that undermine its viability as a competitive alternative to backpropagation (BP) at a top-tier conference:\n\n1. **Lack of Key Advantages Over BP**:  \n   - The proposed method requires **weight transport**, a major drawback that negates one of TP's primary motivations (avoiding explicit weight transport for biological plausibility or neuromorphic hardware). This significantly limits its practical appeal compared to BP or other weight-transport-free methods like feedback alignment.  \n   - Computational costs are **strictly higher than BP**, especially for short sequences, and the paper does not demonstrate sufficient empirical or theoretical justification for its use in regimes where it might be \"attractive.\"  \n\n2. **Empirical Weaknesses**:  \n   - The experiments on realistic datasets (e.g., CIFAR-10) show **poor performance** (under 20% accuracy), raising serious doubts about the method's scalability and practical utility.  \n   - Comparisons are limited to basic baselines (e.g., gradient descent), and there is no evaluation against state-of-the-art RNN optimization methods (e.g., Hessian-Free optimization) or competitive RNN architectures.  \n   - High variance in training results and unclear robustness further weaken the empirical claims.  \n\n3. **Theoretical Gaps**:  \n   - The paper lacks **theoretical guarantees** (e.g., convergence proofs, bounds on optimization performance) to justify TP's superiority over BP. The relationship to Gauss-Newton methods is only superficially explored, and the connection to prior work (e.g., Lee et al. 2015, Meulemans et al. 2020) is not clearly distinguished.  \n   - The **vanishing/exploding gradient problem** remains unresolved, as the method relies on Jacobian inverses, which could amplify instabilities.  \n\n4. **Novelty and Significance**:  \n   - While the regularized inversion approach is a step forward in TP implementation, reviewers note that the contributions are **marginal** compared to existing work. The method does not fundamentally advance the field beyond incremental improvements.  \n\n5. **Clarity and Scope**:  \n   - The paper ambiguously frames its applicability to general neural networks while focusing narrowly on RNNs. The lack of experiments on convolutional networks (e.g., CNNs) undermines broader claims of utility.  \n\n### Conclusion:  \nThe paper fails to demonstrate that the proposed TP variant overcomes the core limitations of existing TP methods or provides a compelling advantage over BP. Theoretical gaps, weak empirical validation, and unresolved implementation challenges (e.g., weight transport, computational cost) prevent it from meeting the high standards of a top-tier conference. While the idea is interesting, the current submission does not sufficiently justify its potential or novelty.  \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about understanding whether neural networks generalize well because of their architecture or the optimization process (gradient descent). The authors argue both play a role, using NNGP and experiments with gradient descent. \n\nLooking at the reviews, there's a mix of positive and critical points. Reviewer 1 says the theoretical results on NNGP error are interesting but the experiments are weak and section 5 isn't a contribution. They want a weak reject. Reviewer 2 thinks it's borderline, needs better comparisons and formal presentation on margins. Reviewer 3 says it's premature, not significant enough, especially since it doesn't apply to CNNs. Reviewer 4 points out unclear empirical evaluation and margin claims. Reviewer 5 is concerned about the margin claims not being rigorous and the section 5 being misleading. Reviewer 6 thinks it's overpromising, especially on GD's role, and the margin analysis is weak. Reviewer 7 initially had issues with the Nero optimizer and lack of experiments but later raised their score to weak accept. Reviewer 8 says the PAC-Bayes bound is incremental and the GD analysis isn't novel. Reviewer 9 likes the ideas but wants more experiments on different architectures and optimizers.\n\nThe authors responded by clarifying that their main point isn't GD vs NNGP but showing both contribute, and that their bounds are non-vacuous. They addressed some technical points but acknowledge the need for more experiments and clearer explanations.\n\nKey issues: The margin claims are criticized as not rigorous, especially for squared loss vs cross-entropy. The use of Nero might be a problem since it's not standard GD. The experiments are seen as limited, not covering CNNs or other optimizers. The theoretical contributions are incremental over Valle-Pérez et al. The PAC-Bayes bounds, while non-vacuous, are considered not tight and not compared to prior work properly.\n\nStrengths: The NNGP error analysis is novel and non-vacuous. The idea that both architecture and GD contribute is interesting. The margin's role in improving generalization is a good insight.\n\nBut the weaknesses are significant. The margin analysis isn't well-supported, experiments are limited, and the paper overreaches in claiming GD's role without sufficient evidence. The use of a non-standard optimizer complicates the conclusions. The theoretical parts are seen as building on prior work without enough novelty.\n\nTop-tier conferences require strong contributions. While the paper has some novel elements, the criticisms about the margin, experiments, and overreach suggest it's not yet ready. The authors need to address the empirical shortcomings and clarify their claims, especially regarding GD's implicit bias. The current state is borderline but leans towards rejection due to the unresolved issues highlighted by multiple reviewers.\n</think>\n\nThe paper investigates the interplay between neural network architecture and gradient descent (GD) in determining generalization. It introduces theoretical tools to analyze the NNGP posterior's average test error and argues that GD improves generalization by selecting functions with large margins. While the work contains novel contributions, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Margin Claims and Rigor**: Multiple reviewers (e.g., Reviewers 1, 4, 5) question the rigor of the margin analysis, particularly for squared loss. The authors’ defense hinges on loss function design (e.g., α-scaled loss), but this is not clearly tied to GD’s implicit bias. The connection to prior work (e.g., cross-entropy maximizing margin) is underdeveloped, and the experimental evidence is seen as limited or artificial (e.g., reliance on Nero optimizer).\n\n2. **Empirical Weaknesses**: Experiments are criticized as insufficiently comprehensive. The use of Nero—a non-standard optimizer—raises concerns about generalizability to vanilla GD. Comparisons to prior work (e.g., Valle-Pérez et al.) are lacking, and the failure to test CNNs or other architectures limits the significance of the results. Reviewers note that the gap between NNGP and GD may stem from experimental design (e.g., data scaling) rather than inherent architectural vs. optimization effects.\n\n3. **Theoretical Incrementality**: While the PAC-Bayes bounds are non-vacuous and technically novel, they are seen as incremental over prior work (Valle-Pérez et al.) and not tightly connected to GD’s role. The margin analysis, though insightful, is not formally grounded in GD’s trajectory or trajectory-based theories (e.g., NTK, implicit regularization).\n\n4. **Presentation and Clarity**: The paper’s framing of GD’s implicit bias is confusing (e.g., conflating overfitting with margin maximization). The distinction between NNGP and GD contributions is muddled, and the discussion of margin’s role in NTK vs. finite networks is underdeveloped.\n\n**Strengths**: The analytical tools for NNGP error estimation are a solid contribution, and the paper raises important questions about the joint role of architecture and optimization. The margin’s impact on generalization is a promising direction.\n\n**Final Decision**: Reject  \nThe paper’s theoretical contributions are incremental, its empirical validation is insufficient, and critical claims about GD’s implicit bias lack rigor. While the topic is important, the current work requires substantial revisions to address methodological limitations, clarify claims, and provide broader experimental validation before it meets the standards of a top-tier conference."
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let me start by going through all the information provided. The paper is about the Feedback Alignment (FA) algorithm for training deep linear networks. The authors analyze convergence guarantees and implicit regularization, showing both implicit regularization and anti-regularization effects based on initialization. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 and 4 are concerned about the assumptions, like the diagonalizability and spectral initialization, which might oversimplify the problem. They also mention that the results might not apply to real-world scenarios with non-linear networks or standard initializations. However, Reviewer 3 is more positive, highlighting the novelty of the implicit anti-regularization and the clear writing. \n\nThe authors addressed some concerns in their response. They clarified that spectral initialization isn't always possible and added figures to help visualize the dynamics. They also mentioned experiments without spectral initialization and pointed out that their analysis holds for any initialization in the deep case, though some proofs are in the appendix. \n\nKey points to consider for a top-tier conference: The paper provides theoretical insights into FA, which is a notable contribution. The analysis of implicit anti-regularization is novel and interesting. However, the restrictive assumptions (like diagonalizability and specific initializations) limit the practical relevance. The reviewers' concerns about the gap between theory and practice (e.g., non-linear networks) are valid but perhaps expected in foundational work. \n\nThe technical correctness is agreed upon, but the significance and novelty are debated. Reviewer 3 gives high technical novelty, while others are more reserved. Since the paper is the first to rigorously analyze FA's convergence and implicit biases, even with assumptions, it's a solid contribution. The empirical experiments, though limited, support the theory. \n\nConsidering the standards of a top conference, theoretical contributions that open new avenues are valuable. The paper's limitations are acknowledged, but the foundational nature and novel findings tip the balance towards acceptance. The authors have addressed some reviewer concerns, and the work is a step forward in understanding FA.\n</think>\n\nThe paper presents a theoretical analysis of the Feedback Alignment (FA) algorithm for deep linear networks, focusing on convergence guarantees and implicit regularization/anti-regularization phenomena. While the reviewers raise valid concerns about restrictive assumptions (e.g., diagonalizability, spectral initialization) and the limited scope to linear networks, the work offers several strengths:\n\n1. **Novel Contributions**: \n   - The analysis of implicit anti-regularization under specific initializations is a novel and theoretically interesting finding, highlighting how FA can prioritize learning \"less important\" components first. This contrasts with GD's implicit regularization and provides new insights into FA's behavior.\n   - The paper rigorously proves convergence for both continuous and discrete dynamics in deep linear networks, addressing a gap in the theoretical understanding of FA.\n\n2. **Technical Soundness**: \n   - All reviewers agree the claims are correct and well-supported. The proofs, while relying on simplifying assumptions, are mathematically sound and extend prior work on linear networks.\n\n3. **Empirical Support**: \n   - The experiments align with the theoretical results, even when relaxing some assumptions (e.g., non-spectral initialization). The comparison with GD shows FA's advantages in convergence speed and similar regularization behavior.\n\n4. **Broader Implications**: \n   - The study of FA's dynamics in linear networks serves as a foundational step toward understanding its behavior in more complex settings. The implicit anti-regularization phenomenon, while mathematically derived, raises important questions about FA's practical implications and potential pitfalls.\n\n**Addressing Concerns**:\n- **Assumptions**: While diagonalizability and spectral initialization are restrictive, the authors clarify these limitations and note that their analysis holds for general initializations in deep networks (with proofs in the appendix). They also acknowledge the gap to non-linear networks but emphasize the value of linear analysis as a starting point.\n- **Practical Relevance**: The authors provide experiments under non-spectral initializations and argue that their results still offer theoretical insights applicable to real-world scenarios. The implicit anti-regularization, while mathematically derived, is not purely academic—it highlights a potential vulnerability of FA that practitioners should consider.\n\n**Weaknesses**:\n- The focus on linear networks limits direct applicability to real neural networks with nonlinearities. However, this is a common starting point for theoretical analysis (as noted by Reviewer 3 and the authors).\n- The reliance on spectral initialization for some results may seem artificial, but the authors address this by showing convergence under general initializations and providing empirical evidence beyond their theoretical setup.\n\n**Conclusion**: The paper makes a meaningful contribution to the theoretical understanding of FA, a topic of growing interest in deep learning. While its assumptions and scope are narrow, the novel insights into implicit anti-regularization and rigorous convergence analysis justify its acceptance. The work sets a foundation for future research and addresses a critical gap in FA's theoretical foundations.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject the paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" for a top-tier conference like ICLR. Let me start by going through all the provided information step by step.\n\nFirst, the paper's title and abstract. The main contributions seem to be the AutoOED platform, which combines existing MOBO algorithms with a novel strategy called Believer-Penalizer (BP) for asynchronous batch experiments. They also mention a GUI for non-experts and real-world hardware integration. The keywords are optimal experiment design, Bayesian optimization, multi-objective optimization, and software platform.\n\nLooking at the reviewer comments, there's a mix of positive and negative feedback. Reviewer 1 says the tool is a great step forward in implementation and usability but is unconvinced about performance and novelty. They gave a 5 but lean towards rejection. Reviewer 2 points out that the paper is more of a software manual with limited methodology contributions, mentions a conflicting claim about handling multiple objectives, and notes a citation issue (they didn't cite their own prior arXiv paper?). They rate it as not a strong methodological paper for ICLR. Reviewer 3 says it's engineering-oriented with little theoretical work, especially on BP, and rates technical novelty low. Reviewer 4 acknowledges the engineering effort but says the novelty is minor, the claims are overstated, and the empirical results are weak. Reviewer 5 is more positive, seeing the problem as well-motivated and the platform as useful, but notes minor issues and rates it a 3 on some metrics.\n\nThe author responses address some points. They clarify that the GUI is a key contribution, mention extending support to single and many objectives, and defend BP as a robust strategy despite lacking theoretical backing. They also compare with other BO tools and emphasize the modular design and ease of use.\n\nNow, considering the standards of a top-tier conference. ICLR values both methodological contributions and impactful tools. The paper's main selling points are the platform's usability, modularity, and the BP strategy. However, the reviewers highlight several issues:\n\n1. **Novelty**: The core algorithms implemented are existing MOBO methods. The BP strategy is seen as a minor contribution, even by the authors who admit it's a \"natural combination\" without theoretical proof. Reviewers 2, 3, and 4 point out that the technical novelty is low.\n\n2. **Empirical Results**: The performance isn't consistently better. Figures 4 and 5 show mixed results where AutoOED isn't always the best, sometimes underperforming. Reviewer 4 notes the claims are overstated here.\n\n3. **Methodological Contribution**: Reviewers 2 and 3 argue the paper is more engineering than scientific. While ICLR does consider software contributions, they need to be significant. The authors argue their modular framework allows easy experimentation, which is a plus, but the reviewers feel it's incremental over existing platforms.\n\n4. **Scope Limitations**: Initially, the paper limited itself to 2-3 objectives, which was a problem. The authors fixed this in the response, but the reviewers might still see it as a past limitation.\n\n5. **Citation Issue**: Reviewer 2 pointed out they didn't cite their own arXiv work. The authors didn't mention addressing this in their response, which could be a problem, though maybe it's fixed in the updated paper.\n\n6. **Theoretical Gaps**: BP lacks theoretical justification, which is a hit for a top conference. Reviewer 3 and 4 noted this.\n\nOn the positive side, the platform's usability and GUI are strong points. The authors emphasize that it lowers the barrier for non-experts, which is valuable. The modular design allows researchers to test new algorithms easily, which is a plus. The real-world application example, though maybe \"cooked,\" shows potential.\n\nHowever, for a top conference, the technical contributions need to be stronger. The main innovation (BP) is incremental, and the empirical results don't show a clear advantage. The paper is more of a system/ tool paper, which ICLR does accept, but it needs to be a significant advancement. The reviewers' consensus seems to lean towards the contributions being marginal, especially since similar platforms exist (like BoTorch, GPyOpt, etc.), and AutoOED's improvements are in usability and modularity rather than groundbreaking methods.\n\nThe authors' responses address some points but don't resolve the core issues: BP's novelty and the empirical underperformance. They argue that the platform's ease of use and modularity are key, but reviewers 2 and 4 note that existing tools are becoming easier to use, and the engineering contribution isn't enough for ICLR's standards.\n\nConsidering all this, the paper's main strengths are in the engineering and usability, but the technical and methodological contributions are insufficient for a top-tier conference. The reviewers' scores (mostly 2s and 3s on novelty and empirical significance) and their leaning towards rejection (especially Reviewer 1 and 4) suggest it's on the borderline but likely below the bar for acceptance.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Technical Novelty and Methodological Contributions**: \n   - The core contributions of AutoOED are the implementation of existing MOBO algorithms, a modular framework, and the Believer-Penalizer (BP) strategy. However, reviewers consistently highlight that the technical novelty is marginal. BP is described as a \"natural combination\" of existing methods (KB and LP) without theoretical justification, and its empirical performance is inconsistent (e.g., underperforming on some benchmarks). While the authors argue that BP addresses failure cases of KB and LP, the lack of rigorous analysis or theoretical grounding undermines its significance for a top-tier conference.\n\n2. **Empirical Evaluation**:\n   - The empirical results are mixed. Figures 4 and 5 show that AutoOED does not consistently outperform baselines and underperforms on several benchmarks. Reviewers note that claims of \"state-of-the-art\" or \"superior performance\" are overstated. Even after revisions, the authors acknowledge BP is \"robust\" rather than superior, which weakens the paper’s impact.\n\n3. **Scope and Limitations**:\n   - Initially, the platform was limited to 2–3 objectives, which reviewers criticized as restrictive. While the authors addressed this in revisions, the limitation reflects a lack of scalability, a critical issue for a general-purpose platform. Additionally, the real-world experiment is described as \"proof of concept\" and lacks rigorous validation against other tools.\n\n4. **Comparisons and Related Work**:\n   - The paper fails to adequately compare with existing BO platforms (e.g., Optuna, HyperOpt, SMAC) and cites prior work incompletely (e.g., omitting their own arXiv paper). While the authors updated comparisons in revisions, the oversight raises concerns about thoroughness.\n\n5. **Engineering vs. Scientific Contribution**:\n   - While usability and modularity are strengths, reviewers emphasize that ICLR requires a stronger scientific or methodological contribution. The paper’s focus on system engineering—though valuable—does not meet the bar for a top conference unless paired with novel algorithms or theoretical insights. The authors argue that AutoOED bridges gaps for non-experts, but similar tools (e.g., BoTorch) are already addressing usability improvements.\n\n6. **Reviewer Consensus**:\n   - Most reviewers (especially Reviewers 2, 3, and 4) rate technical and empirical significance as marginal (scores of 2 or 3). Reviewer 1 and 4 explicitly lean toward rejection, citing insufficient novelty and overstated claims. Only Reviewer 5 offers guarded support, but even they note minor issues and do not strongly advocate for acceptance.\n\n### Conclusion:\nWhile AutoOED is a well-engineered platform with practical utility, its methodological contributions are incremental, and its empirical results are inconsistent. The lack of theoretical rigor for BP and the competition from existing tools weaken its case for acceptance at a top-tier conference like ICLR. The paper would be better suited for a venue focused on software tools or applications, where usability and modularity are prioritized over groundbreaking methodological advances."
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about addressing the \"greedy nature\" of multi-modal DNNs, where models rely too much on one modality. The authors propose a method called balanced multi-modal learning to fix this. \n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 4 and 6 are pretty critical. Reviewer 4 says the main claims aren't well-supported, especially the link between greedy learning and worse generalization. They mention that on some datasets like ModelNet40 and NVGesture, the results between vanilla and guided methods aren't statistically significant. Also, the Colored MNIST dataset might be too synthetic, so the evidence isn't strong enough. \n\nReviewer 6 points out the lack of theoretical analysis and proper comparisons with existing methods like RUBi. The authors did add some comparisons in the response, but maybe not enough. Also, the datasets are all visual, which might not cover natural multimodal scenarios. \n\nOn the positive side, Reviewer 5 and 7 think the problem is important and the method is straightforward. The experiments do show some improvement, and the authors addressed some issues in the rebuttal, like adding RUBi results and hyperparameter studies. \n\nBut the main issues are the weak validation of the core claims. The key hypothesis is that greedy learning hurts generalization, but the experiments on real-world datasets don't show strong statistical significance. The authors rely on synthetic data (Colored MNIST) which might not be representative. Also, the technical novelty is questioned—some say it's only marginally novel. \n\nThe rebuttal mentions adding RUBi results and hyperparameter analysis, but if the main experiments still don't show clear improvements on standard datasets, that's a problem. The lack of theoretical backing is another strike against it for a top-tier conference, which usually expects stronger validation and novel contributions. \n\nConsidering all this, the paper's core claims aren't sufficiently supported, especially in real-world scenarios. The experiments have limitations, and comparisons with prior work are insufficient. While the problem is interesting, the current evidence and method aren't strong enough for acceptance at a top conference.\n</think>\n\nThe paper addresses an important problem in multi-modal learning—greedy reliance on a single modality—and proposes a method to mitigate this issue. However, several critical weaknesses undermine its acceptance at a top-tier conference:\n\n1. **Weak Empirical Validation of Core Claims**: \n   - Reviewers 2, 4, and 6 highlight that the key claim (greedy learning harms generalization) is not strongly supported. On real-world datasets like ModelNet40 and NVGesture, the proposed method shows minimal or non-significant improvements over baselines. The Colored MNIST results, while strong, are criticized as synthetic and not representative of standard multi-modal settings.\n   - The conditional utilization rate and learning speed metrics are inadequately tied to generalization performance. The experiments fail to establish a clear causal link between modality imbalance and degraded performance.\n\n2. **Lack of Theoretical Support**:\n   - Reviewers 2, 4, and 6 note the absence of theoretical grounding for the greedy learner hypothesis or the proposed algorithm. The paper relies heavily on empirical observations without explaining *why* greedy learning occurs or how the proposed rebalancing step addresses it at a mechanistic level.\n\n3. **Methodological Limitations**:\n   - The rebalancing step is poorly explained in the main text (per Reviewers 3, 5, and 6). While clarified in the rebuttal, the method’s design (e.g., use of average features to block modality interactions) lacks justification beyond heuristic reasoning.\n   - Hyperparameters like α and Q are not systematically studied in the main paper, and their sensitivity is only partially addressed in the rebuttal. This undermines reproducibility and practical applicability.\n\n4. **Incomplete Comparisons and Dataset Scope**:\n   - The authors added comparisons with RUBi in the rebuttal but only report marginal improvements. The lack of broader comparisons with prior work (e.g., [2], [3]) and failure to test on natural multimodal datasets (e.g., audio-visual) weaken the method’s generalizability.\n   - All experiments use visual-only modalities, limiting the paper’s relevance to broader multi-modal scenarios.\n\n5. **Novelty Concerns**:\n   - Reviewers 2, 4, and 6 question the technical and empirical significance. While the problem is acknowledged as important, the method is seen as incremental, building on existing ideas like modality balancing without clear breakthroughs.\n\n**Strengths**: The problem is timely, and the empirical setup (e.g., conditional utilization rate) is novel. The authors addressed some reviewer concerns in the rebuttal (e.g., hyperparameter studies, RUBi comparisons), but these fixes do not fully resolve the core issues.\n\n**Conclusion**: The paper’s contributions are insufficiently validated and theoretically grounded for a top-tier conference. The empirical results are underwhelming on realistic datasets, and critical gaps in methodology and comparisons remain unresolved.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about adversarial examples from a frequency perspective. The main claims are that adversarial examples aren't strictly high or low frequency but depend on the dataset, and they use this to explain the accuracy-robustness trade-off. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 wants them to consider UAPs, which the authors say is beyond their current scope but they'll mention it. Reviewer 2 is concerned about the clarity of contributions and the methods compared to existing work, like TRADES. The authors tried to clarify their contributions but maybe not enough. Reviewer 3 points out that similar ideas exist in prior work (like Tsuzuku, Yin, Bernhard) and that the dataset dependency isn't new. They also mention limited datasets and attacks tested. The authors added more datasets and experiments but maybe not enough. Reviewer 4 and 6 are more positive, seeing the insights as valuable despite some presentation issues. \n\nThe technical novelty is a problem. Reviewer 3 and 5 say the contributions are marginal because others have done similar things. The authors argue their approach focuses on perturbations' frequencies rather than images', which is a key difference. But if that's already been done, then maybe not. The empirical results are extensive but maybe not groundbreaking. The rebuttal addresses some points but not all. For example, they added MNIST and others but maybe not enough to satisfy the dataset diversity. \n\nThe main issue seems to be that while the paper provides some new analysis and insights, the contributions are incremental given existing work. The reviewers who want acceptance (4,6) think the insights are useful, but others (2,3,5) feel it's not novel enough. The authors tried to clarify contributions but maybe the paper still isn't distinct enough from prior work. The fact that Reviewer 3 mentions overlapping conclusions with Bernhard 2021 is a big red flag, especially if that paper is concurrent. \n\nThe empirical part has limitations like only PGD attacks initially, but they added Auto-Attack and more datasets in the rebuttal. However, the core idea that adversarial examples are dataset-dependent isn't new. The frequency-based adversarial training is a novel method, but the authors admit they don't improve SOTA metrics, which might be a problem for a top-tier conference that values both insight and impact. \n\nConsidering all this, the paper has some merit but the lack of clear novelty and sufficient differentiation from existing work might tip the scale. The reviewers are split, but the critical ones point out significant issues. Since top conferences require high novelty and impact, this might not meet the bar. \n\nFinal Decision: Reject\n</think>\n\nThe paper presents an analysis of adversarial examples through a frequency perspective, arguing that their properties are dataset-dependent rather than strictly tied to high or low frequencies. While the work offers some novel insights and empirical analysis, several critical issues undermine its novelty and significance for a top-tier conference:\n\n### Key Strengths:\n1. **Empirical Analysis**: The paper provides extensive experiments across datasets (CIFAR-10, ImageNet, etc.) and explores frequency-based adversarial training, offering new observations about the accuracy-robustness trade-off.\n2. **Clarity of Insights**: Reviewers 4 and 6 highlight the value of the findings, particularly the dataset-dependent nature of adversarial perturbations and the frequency-based explanation for trade-offs, which could inform future research.\n\n### Major Weaknesses:\n1. **Lack of Novelty**:\n   - **Overlapping Prior Work**: Reviewers 1, 3, and 5 note that key claims (e.g., adversarial examples are not strictly high-frequency, dataset dependence) have been addressed in prior studies (e.g., Tsuzuku et al., Yin et al., Bernhard et al.). The authors argue their focus on perturbation frequencies (vs. image frequencies) is distinct, but this distinction is not clearly communicated or sufficiently emphasized in the paper.\n   - **Concurrent Work**: The overlap with Bernhard et al. (2021) is particularly problematic, as both papers reach similar conclusions. The authors acknowledge this but fail to convincingly differentiate their contributions.\n\n2. **Methodological Limitations**:\n   - **Limited Attack Types**: Initial experiments focused on PGD attacks, though the authors added Auto-Attack results in the rebuttal. However, CW attacks were not fully addressed, leaving gaps in validation.\n   - **Narrow Dataset Scope**: While additional datasets (e.g., MNIST) were included in supplementary material, the core analysis still lacks diversity (e.g., high-resolution or non-natural datasets like radar images), weakening generalizability.\n\n3. **Contribution Clarity**:\n   - **Ambiguous Claims**: Reviewers 2 and 3 criticized the paper for burying contributions in the related work section and failing to clearly articulate unique insights. The rebuttal attempted to clarify but did not fully resolve these concerns.\n   - **Practical Impact**: The proposed frequency-based adversarial training does not improve SOTA metrics, and the authors admit this is not the goal. However, top-tier conferences typically require stronger practical or theoretical contributions beyond exploratory analysis.\n\n4. **Technical Concerns**:\n   - **Frequency Space vs. Image Space**: The authors clarify their use of DCT over Fourier transforms but do not fully address Reviewer 2’s concern about methodological differences impacting conclusions.\n   - **Perceptibility Trade-offs**: The explanation for maintaining imperceptibility in the RGB space is technical but could be clearer, and the rebuttal’s equations may not sufficiently alleviate concerns.\n\n### Conclusion:\nWhile the paper contributes interesting observations and experiments, its core claims lack sufficient novelty relative to existing work, and its methodological limitations (e.g., limited attack types, dataset scope) and unclear presentation of contributions weaken its case for acceptance at a top-tier conference. The overlap with concurrent work and the absence of groundbreaking insights or practical advancements tip the balance toward rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the information provided.\n\nFirst, the paper's title and abstract. The main contribution seems to be showing that finding globally optimal two-layer ReLU networks can be done via a convex optimization program with cone constraints. They don't use duality-based methods like previous work, which is a point they emphasize. The abstract mentions several consequences, like characterizing Clarke stationary points, a polynomial-time algorithm for checking global minima, a continuous path to the minimum, and determining the minimal hidden layer size to avoid spurious valleys. That's quite a list of contributions.\n\nLooking at the reviewer comments. There are four reviewers. Let's parse each one.\n\nReviewer 1 gives a 3 out of 4 for correctness, noting some minor issues like typos and inconsistencies in equations. They mention the paper only considers single-hidden layers and wonder about extensions to multiple layers. The technical novelty is 3, meaning it's significant but some aspects exist in prior work. The strengths are clear: novel extensions of Pilanci and Ergen 2020, characterizing global and local optima. The weaknesses are the single-layer limitation and some typos.\n\nReviewer 2 is more enthusiastic. They give correctness a 4, so they think the claims are well-supported. They call it the best submission they reviewed for ICLR 2022, highlighting the elegant mapping between minimal networks and convex programs. They mention potential for future work on other activations and deeper networks but see no major weaknesses. They do note missing references, which the authors addressed. Their summary is a strong recommendation for acceptance.\n\nReviewer 3 is mixed. They acknowledge the contributions but point out that some prior work (like Pilanci's) has similar convex formulations. They want more discussion on related work and experiments. They also mention some unclear parts in the paper, like the partition of D_i's. Correctness is 3, so some minor issues. Technical novelty is 3 again, so not entirely new but significant. They also note the lack of experiments, which is a common critique in theory-heavy papers but maybe acceptable for a top conference if the theory is strong enough.\n\nReviewer 4 is very positive. They give correctness a 4 and technical significance a 4, stating the contributions are significant and new. They highlight the complete characterization of global minima via convex programs, which is a major step. The only downside is the limitation to two-layer networks, but they still recommend acceptance, calling it a \"clear accept.\"\n\nThe author responses address the typos and clarify some points. They mention that extending to deeper networks is future work but acknowledge the current focus on two layers. They also address the technical points about the cone definitions and partitions.\n\nNow, considering the standards of a top-tier conference. The paper's main strength is the theoretical contribution: providing a convex optimization framework that exactly characterizes all global optima for two-layer ReLU networks. This is a solid theoretical advance, especially since they don't rely on duality, which previous work did. The fact that they can construct all optimal networks from the convex solution is a big deal. The polynomial-time algorithm for checking optimality and the path to the global minimum are also valuable.\n\nThe weaknesses are the single-layer limitation and some technical typos. However, the reviewers (especially 2 and 4) argue that the contribution is still significant even within that scope. The theoretical framework could inspire future work on deeper networks, which the authors acknowledge. The typos and minor inconsistencies can be fixed in the final version, as the authors have already addressed them.\n\nThe novelty is a bit of a point of contention. Reviewer 3 mentions prior work by Pilanci and Ergen, but the authors clarify that their approach doesn't use duality, which is a key difference. That's a valid distinction, so the work is novel enough. The extension to other activations and deeper networks is a future direction, which is understandable for a foundational paper.\n\nThe lack of experiments might be a concern, but since the paper is theoretical, and the conference (like ICLR) does accept strong theory papers without experiments if the theory is groundbreaking. The reviewers who focused on the theory (Reviewers 2 and 4) clearly see the value here.\n\nOverall, the majority of the reviewers (especially the more positive ones) think this is a strong contribution. The technical issues are minor and fixable. The theoretical framework is novel and significant. The limitations are acknowledged but don't detract from the current contribution. Therefore, the paper should be accepted.\n</think>\n\nFinal Decision: Accept\n\nThe paper presents a significant theoretical contribution by establishing a convex optimization framework that exactly characterizes all global optima of regularized two-layer ReLU networks. Key strengths include:  \n1. **Novelty and Significance**: The work extends Pilanci and Ergen (2020) by providing a complete characterization of global optima via a convex program without relying on duality, a methodological distinction that reviewers emphasize as novel.  \n2. **Technical Depth**: The framework enables constructing all optimal networks, analyzing Clarke stationary points, and deriving polynomial-time optimality checks—contributions that advance the theoretical understanding of neural network optimization.  \n3. **Impact**: The results clarify the loss landscape (no spurious valleys for sufficiently large networks) and provide tools like continuous paths to global minima, which are foundational for future work.  \n\n**Weaknesses**:  \n- Limited to two-layer networks and ReLU activation. However, reviewers acknowledge this as a starting point, with potential for future extensions.  \n- Minor technical issues (typos, inconsistencies) are fixable and do not undermine the core contributions.  \n\n**Reviewer Consensus**: Three of four reviewers (including the most enthusiastic) strongly recommend acceptance, citing the paper’s theoretical rigor and potential impact. While some note prior work and lack of experiments, the theoretical novelty and completeness justify acceptance at a top-tier conference. The authors’ responses address concerns effectively, reinforcing the paper’s validity.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving diffusion models' sampling efficiency. The main contributions are DDSS and GGDM. The reviewers have mixed feedback. Let me go through each point.\n\nFirst, the reviewers' scores after rebuttal: Reviewer 1 raised their score to 6, 2 and 3 also to 6, 4 stayed at 3, and 5 to 6. The majority seem positive now. \n\nThe key strengths are the empirical results showing better FID scores with fewer steps, which is a big deal for diffusion models. The method uses gradient-based optimization on perceptual losses, which is novel. The authors addressed concerns about theoretical grounding by connecting their loss to KID, which makes the objective clearer. They also added experiments showing that optimizing without matching marginals works better, which counters prior assumptions.\n\nWeaknesses: Some reviewers were concerned about theoretical justification and the non-Markovian structure. But the authors provided Theorem 1 and clarified the ELBO connections. The clarity issues were addressed by renaming terms and adding explanations. The experiments on ImageNet 64 had mixed results, but the authors promised more datasets for the camera-ready. \n\nTechnical novelty: While building on DDIM, the optimization approach via DDSS is a step forward. The GGDM family allows more flexibility. The use of gradient rematerialization is a practical contribution.\n\nEmpirical significance: The FID improvements are substantial, especially on LSUN with 10 steps. This is impactful for real-world applications needing faster inference. The rebuttal added KID experiments which solidify the results.\n\nThe main remaining concerns were about dataset diversity and theoretical depth. But the authors addressed these by adding more experiments and clarifications. Since top conferences value both innovation and empirical impact, and the paper now has solid results and addressed most criticisms, it should be accepted.\n</think>\n\nThe paper proposes a novel method, DDSS, to optimize fast samplers for diffusion models by differentiating through perceptual quality metrics, alongside introducing the non-Markovian GGDM family of samplers. The key strengths include:\n\n1. **Empirical Impact**: The method achieves state-of-the-art FID scores with significantly fewer sampling steps (e.g., 11.6 FID on LSUN with 10 steps vs. 51.1 for DDPM). This addresses a critical bottleneck in diffusion models' practicality, making it highly impactful for real-world applications.\n\n2. **Technical Novelty**: DDSS introduces a gradient-based optimization framework for sampler parameters, leveraging gradient rematerialization to handle backpropagation through the sampling process. The GGDM family extends prior work (e.g., DDIM) by allowing non-Markovian dependencies, which is theoretically grounded via Theorem 1 and empirically validated.\n\n3. **Clarity and Addressing Concerns**: The authors significantly improved the paper in response to reviews. They clarified the theoretical basis by connecting their perceptual loss to the KID metric, provided formal proofs, and addressed concerns about overfitting and ELBO optimization. They also added experiments showing that optimizing without enforcing DDPM marginals improves performance, countering prior assumptions.\n\n4. **Practical Relevance**: The method is compatible with pre-trained models without fine-tuning, broadening its applicability. The focus on minimizing inference steps aligns with the urgent need for faster generative models in deployment.\n\n**Weaknesses and Remaining Concerns**:\n- **Theoretical Gaps**: While Theorem 1 and the KID connection strengthen the foundation, some reviewers noted that the method’s reliance on perceptual metrics lacks guarantees about the underlying distribution. However, this is mitigated by the strong empirical results and prior work showing dissociation between likelihood and sample quality.\n- **Dataset Scope**: Initial experiments focused on smaller datasets (CIFAR-10, ImageNet 64), but the authors committed to adding high-resolution datasets (e.g., LSUN 256) for the camera-ready version, addressing scalability concerns.\n- **Clarity Improvements**: Renaming GGDP to GGDM and clarifying equations resolved earlier confusion, though some notation remains complex. The revised paper is significantly clearer.\n\n**Conclusion**: The paper’s empirical contributions are substantial, and the technical approach is innovative. The authors addressed major theoretical and clarity concerns effectively. While minor limitations remain, the method’s impact on accelerating diffusion models justifies acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper is titled \"The weighted mean trick – optimization strategies for robustness\" and the abstract mentions that minimizing a weighted mean can optimize higher-order moments like variance, skewness, and kurtosis, leading to better robustness against outliers. The authors also mention experiments showing similar performance to other robust loss functions but with a stronger theoretical background. \n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Let's break down the key points from each reviewer:\n\nReviewer 1 points out that the choice of λ_i isn't theoretically justified, and the empirical improvement might just be due to tuning. They suggest focusing on negative weights and distinguishing variance penalization as a separate problem. This is a valid concern because without a principled way to choose λ, the method's effectiveness could be overestimated through hyperparameter tuning.\n\nReviewer 2 and 3 both question the algorithm's validity. They note that the proposed algorithm (Algorithm 1) doesn't properly account for the gradient of the weights with respect to θ. The gradient step only uses the first term of the chain rule, ignoring the second term involving the gradient of w with respect to θ. This is a critical issue because it means the algorithm might not actually be optimizing the intended objective function. If the algorithm isn't correctly minimizing the weighted loss, the theoretical claims and experiments could be flawed.\n\nReviewer 4 is concerned about the theoretical contributions being limited. They mention that allowing negative λ makes the approach too flexible (can do opposite effects), and the paper's coherence is broken because Section 3 uses different weights than Section 2. They also point out that the weights depend on unknown expectations and that the convexity properties from Section 2 don't apply in Section 3. Additionally, they note that the paper doesn't provide convergence guarantees for the algorithm, which is a significant gap for a top-tier conference.\n\nReviewer 5 echoes similar concerns about the algorithm not properly optimizing the objective due to the θ dependence in the weights. They also mention that the theoretical results become trivial if dependencies are ignored, and the experiments don't clarify what's being optimized. This reviewer's low score (correctness 2, novelty 1) is a red flag.\n\nReviewer 6 points out several issues with the paper's clarity and technical soundness, like confusing notation (e.g., \"convex objective ℓ\" when ℓ is a random variable) and lack of explanation for key claims (e.g., why clipping weights preserves convexity). They also question the concentration behavior when weights are clipped and the handling of heavy-tailed distributions. These issues suggest the paper might not be well-polished or rigorously tested.\n\nReviewer 7 is more positive, seeing the theoretical observations as interesting and potentially useful, but notes the theorems are straightforward and the paper lacks applications to existing robust methods. Their acceptance vote is conditional on the contributions being novel enough despite the simplicity.\n\nThe author responses address some points but don't fully resolve the critical issues. For instance, they clarify that the algorithm intentionally ignores the second gradient term, which Reviewers 2 and 3 argue invalidates the optimization. The authors mention that this is similar to Duchi & Namkoong's work, but if that approach is indeed different, it could be a problem. The authors also acknowledge some notation issues but didn't fix all ambiguities pointed out (like λ definitions and notation inconsistencies). They mention adding experiments and notations but didn't address the core algorithmic flaw or provide convergence proofs.\n\nConsidering all this, the main issues are:\n\n1. **Algorithmic Validity**: The core algorithm (Algorithm 1) might not be optimizing the intended objective because it skips part of the gradient computation. This undermines both the theoretical claims and experimental results.\n\n2. **Lack of Theoretical Justification**: The choice of λ isn't grounded in theory, and the paper's theoretical contributions are seen as basic by some reviewers. The convexity arguments in Section 2 don't apply to Section 3, breaking coherence.\n\n3. **Empirical Concerns**: The empirical improvements might stem from hyperparameter tuning rather than the method's inherent superiority. Without clear justification for λ, this is a significant drawback.\n\n4. **Clarity and Rigor**: Multiple reviewers noted confusing notation, unclear explanations, and missing details (e.g., convergence proofs, handling of heavy-tails). These issues make the paper hard to follow and verify.\n\n5. **Comparison to Prior Work**: The authors didn't adequately address how their approach relates to existing robust methods like Trimmed Mean or MoM, which are more theoretically grounded for outlier handling.\n\nGiven that top-tier conferences require strong theoretical contributions, clear methodology, and rigorous empirical validation, the unresolved algorithmic flaw and lack of theoretical depth are major red flags. While the idea of using weighted means to penalize moments is interesting, the execution here has critical gaps that haven't been sufficiently addressed in the author responses. The reviewers' consensus (especially from the more critical ones) points towards insufficient novelty and correctness issues.\n</think>\n\nFinal Decision: Reject\n\nThe paper faces significant unresolved issues that undermine its validity and contribution. Key concerns include:\n1. **Algorithmic Flaw**: The proposed Algorithm 1 does not properly account for the gradient of the weights with respect to model parameters, invalidating its claimed optimization objective. This core methodological error was not adequately addressed by the authors.\n2. **Theoretical Weakness**: The theoretical contributions are deemed marginal by multiple reviewers, with critical gaps in justifying hyperparameter choices (λ) and preserving convexity in practical scenarios. The paper’s coherence is broken by inconsistent weight formulations across sections.\n3. **Empirical Concerns**: Empirical improvements may stem from hyperparameter tuning rather than inherent methodological advantages, and the paper lacks rigorous analysis of convergence and robustness in heavy-tailed settings.\n4. **Clarity and Rigor**: Persistent ambiguities in notation, proofs, and explanations hinder reproducibility and validation. The authors did not sufficiently clarify key claims (e.g., convexity preservation after clipping weights) or address comparisons to prior work.\n\nWhile the concept of using weighted means to penalize higher-order moments is intriguing, the unresolved algorithmic and theoretical flaws, coupled with insufficient empirical grounding, make this submission unsuitable for a top-tier conference."
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum.\" The abstract mentions that they analyze SGD with momentum (SGDm) under covariate shift, showing it can be unstable and diverge due to resonance. They use ODE approximations and Floquet theory, then do empirical tests beyond their theoretical assumptions.\n\nLooking at the reviewer comments first. There are six reviewers. Let's break down their feedback:\n\nReviewer 1 didn't add much beyond acknowledging the response. \n\nReviewer 2 had several concerns. They question the technical novelty, arguing that the proofs rely on existing work (like Faragó et al.) and don't present new methods. They also point out that the intuition for why resonance occurs at specific frequencies isn't clear. Additionally, they're concerned that the theory ignores the stochasticity (the \"S\" in SGD) by using expected gradients, which might not align with real-world SGD behavior. They think the contributions are marginal.\n\nReviewer 3 is more critical. They agree the problem is important but say the technical contributions are weak. They mention that Proposition 1 and Theorem 1 are straightforward extensions of prior work (Muehlebach & Jordan, Halanay). The presentation is unclear, especially how the math connects to resonance. They also argue that the analysis uses expected gradients, not the actual stochastic process, so it's not really analyzing SGD. Plus, they note that resonance might only occur in edge cases (high momentum, low noise), making it less practical. They give technical novelty a 2/4, which is low.\n\nReviewer 4 is positive. They like the structure and empirical validation, even if the math is beyond their expertise. They recommend acceptance, seeing value in the insights for RL and continual learning. They give correctness and novelty high scores but note the theoretical part could be improved.\n\nReviewer 5 is mixed. They see the importance of the problem and good experiments but think the theory is incomplete (can't prove convergence, just divergence). They suggest improving the example section and more analysis on non-linear cases. They give technical novelty a 2/4 but empirical contributions a 3/4. They recommend acceptance with improvements.\n\nReviewer 6 is also positive. They praise the thoroughness and clarity but want more real-world examples where covariate shift is oscillatory. They think the analysis is solid and recommend acceptance, noting some minor issues.\n\nAuthor responses addressed some points. They clarified that their analysis uses expected gradients but defended it by showing experiments with stochasticity still align with theory. They acknowledged the need for better intuition and presentation, offering analogies and moving some content to appendices. They also addressed some of the technical concerns about novelty by emphasizing the novel connection between resonance theory and ML.\n\nNow, considering the criteria for a top-tier conference. The paper needs to be novel, correct, and make a significant contribution. The main strengths are the identification of a resonance phenomenon in SGDm under covariate shift, which is an important problem in non-iid settings. The empirical validation across various scenarios (including neural networks) adds practical relevance. The theoretical framework, while relying on existing math, connects it in a novel way to ML, which is a contribution.\n\nHowever, the technical novelty is a point of contention. Reviewers 2 and 3 argue that the proofs use existing techniques without much innovation. But the authors point out that applying Floquet theory and parametric oscillators to SGD is new. The paper's main contribution is conceptual—showing that resonance can cause divergence, which is a novel insight. Even if the math isn't groundbreaking, the application and the phenomenon's discovery are significant.\n\nThe presentation issues (e.g., unclear intuition, example section) are valid but can be fixed in revisions. The concern about stochasticity is addressed by the authors' experiments showing that even with single samples, the resonance still occurs, aligning with theory. The focus on expected gradients might be a limitation, but the empirical results suggest it's a reasonable approximation for analysis.\n\nThe practical relevance is debated. Reviewers 3 and 5 note that resonance might be rare (e.g., high momentum), but the authors counter that real-world data (like sensor data) can have periodic shifts. The paper's theoretical foundation is valuable even if the phenomenon isn't universal.\n\nConsidering all this, the paper has a solid empirical component and introduces an important concept. The theoretical part, while building on prior work, connects different fields. The criticisms are valid but don't negate the contribution. Top conferences often accept papers with novel insights even if the technical novelty is incremental. The positive reviews from 4, 5, and 6, along with the authors' responses addressing key points, suggest it's borderline but leans towards acceptance.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\n1. **Novel Conceptual Contribution**: The paper identifies a novel phenomenon—resonance-driven divergence of SGD with momentum under covariate shift—which bridges nonlinear resonance theory and machine learning. This connection is a significant conceptual advance, even if the mathematical techniques are not entirely novel. The idea that covariate shift can act as a \"driving signal\" for parametric oscillations in weight space is original and impactful for understanding optimization dynamics in non-iid settings.\n\n2. **Empirical Validation**: The experiments are thorough, extending beyond the linear setting of the theory to nonlinear neural networks and other optimizers like Adam. The consistent alignment between theoretical predictions (e.g., resonance at specific frequencies) and empirical results strengthens the paper’s claims. The authors also address stochasticity concerns by showing resonance persists even with single-sample updates, mitigating criticism about ignoring the \"S\" in SGD.\n\n3. **Theoretical Soundness**: While reviewers 2 and 3 question technical novelty, the paper’s use of Floquet theory and parametric oscillator frameworks to analyze SGDm under covariate shift is a novel application. The theoretical results, though building on existing numerical analysis and ODE techniques, provide a rigorous foundation for understanding divergence conditions. The authors clarify that their work is foundational, and the limitations (e.g., periodicity assumptions) are acknowledged and empirically supplemented.\n\n4. **Addressing Criticisms**: The authors’ responses effectively address key concerns:\n   - **Novelty**: They emphasize the interdisciplinary contribution, framing the work as connecting resonance theory to ML rather than introducing new mathematical techniques.\n   - **Intuition**: Added analogies (e.g., \"ball in a warping bowl\") and revised explanations improve clarity.\n   - **Stochasticity**: Experiments with single-sample updates confirm resonance persists, aligning theory with stochastic SGD behavior.\n   - **Practical Relevance**: They highlight real-world data scenarios (e.g., sensor data) where resonance could occur and outline future work on empirical validation.\n\n5. **Presentation and Clarity**: While reviewers noted gaps in intuition and presentation, the authors’ revisions (e.g., moving opaque examples to appendices, adding dependency summaries) address these issues. The paper’s structure—phenomenon → hypothesis → analysis—is praised by reviewers and remains clear.\n\n6. **Balancing Scores**: Despite mixed technical novelty scores (2/4 from some reviewers), the paper’s empirical significance, conceptual novelty, and alignment with practical scenarios (e.g., continual learning, RL) tip the balance. Top-tier conferences often prioritize impactful insights over purely technical novelty, especially in emerging areas like non-iid optimization.\n\n### Final Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about TimeVAE, a VAE for generating multivariate time series. The reviewers have given their feedback, and I need to decide if it should be accepted to a top-tier conference like ICLR.\n\nFirst, looking at the strengths mentioned by the reviewers. Reviewer 1 says it's well-written and has an interesting approach by incorporating classical time series methods. Reviewer 3 also notes the potential in interpretability and domain knowledge. The abstract mentions reduced training times and better performance in next-step predictions, which are positive points.\n\nNow the weaknesses. Reviewer 1 points out that the hyperparameters for the new decoder blocks aren't discussed, and there's no ablation study on interpretability. That's a problem because if the hyperparameters aren't optimized or explained, it's hard to trust the results. Also, the lack of experiments showing interpretability undermines one of their main claims.\n\nReviewer 2 thinks the model isn't novel enough. They mention that it's just a standard VAE applied to time series with some convolutional layers, which might not be a big deal. TimeGAN had extra loss functions, but TimeVAE doesn't, which makes it hard to see why it's better. The VAE loss isn't well-explained either, which could confuse readers.\n\nReviewer 3 brings up several issues. The literature review is lacking, especially in areas like state-space models. The method uses a single global latent variable, which might be too simplistic compared to per-time-step variables. The experiments don't use standard datasets, making comparisons hard. Also, the interpretability part isn't evaluated, which is a key claim. They also question why an LSTM is used for next-step prediction instead of integrating it into the VAE.\n\nReviewer 4 says there's no discussion of interpretability in the results and the performance isn't better except on the sine dataset. The contributions seem limited, and the practical impact isn't clear.\n\nLooking at the technical novelty, most reviewers (1,2,3) rate it as 2 or 3, meaning it's only marginally novel. The empirical significance is also low. The main issues are lack of ablation studies, poor handling of hyperparameters, and insufficient evaluation of key claims like interpretability. The experiments don't use standard datasets, making it hard to compare with existing methods. The authors didn't respond, so they didn't address these points.\n\nEven though the paper is well-written and has some interesting ideas, the lack of thorough experiments, especially on their main contributions (interpretability, domain knowledge), and the lack of novelty compared to existing work like TimeGAN and others, make it not ready for a top conference. The reviewers' consensus seems to lean towards rejection unless major revisions are made, but since there's no author response and the issues are significant, it's probably better to reject now.\n</think>\n\nThe paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" proposes a VAE-based architecture for generating synthetic time-series data, emphasizing interpretability, domain knowledge integration, and reduced training times. While the paper is well-written and introduces an interesting approach, several critical issues raised by reviewers undermine its readiness for publication at a top-tier conference:\n\n### Key Strengths:\n1. **Interpretability and Domain Knowledge**: The inclusion of decoder components for level, trend, and seasonality offers a novel pathway for incorporating domain knowledge, which is a valuable contribution if properly validated.\n2. **Empirical Performance**: The results on next-step prediction tasks suggest competitive performance compared to state-of-the-art methods, particularly on some datasets.\n3. **Clarity and Writing**: The paper is well-structured and accessible.\n\n### Major Weaknesses:\n1. **Lack of Empirical Rigor**:\n   - **Ablation Studies and Hyperparameters**: Reviewers 1 and 3 highlight the absence of ablation studies to validate the interpretability claims and the lack of guidance on hyperparameter selection for the new decoder components. This undermines confidence in the method’s robustness and practical utility.\n   - **Inconsistent Results**: Reviewer 1 notes counterintuitive results in Table 1 (e.g., worse performance with full training data), which remain unexplained. This could indicate methodological flaws or reporting issues.\n   - **Missing Baselines**: Reviewer 2 points out that the experiments omit key baselines like probabilistic autoregressive models and state-space models, limiting the comparison to GAN-based methods only. This weakens the claim of superiority over \"state-of-the-art methods.\"\n   - **Dataset Limitations**: Reviewer 3 criticizes the use of non-standard datasets, making it difficult to benchmark against prior work. The lack of experiments on widely used benchmarks (e.g., UCR Time Series Archive) further reduces the paper’s impact.\n\n2. **Technical Novelty Concerns**:\n   - **Overlapping with Prior Work**: Reviewers 2 and 3 argue that the core architecture (e.g., using convolutional layers and a global latent variable) is not sufficiently novel compared to existing VAE and GAN-based approaches. The lack of a clear technical innovation beyond combining \"classical\" time-series components with a standard VAE framework is a significant drawback.\n   - **Unaddressed Limitations**: The use of a single global latent variable (instead of per-time-step variables) and the reliance on an external LSTM for next-step prediction (rather than integrating it into the VAE) are criticized as suboptimal design choices. These issues were not adequately addressed in the paper.\n\n3. **Interpretability Claims Unsubstantiated**:\n   - Reviewers 1, 3, and 4 note that the paper fails to provide empirical evidence for its key interpretability claims. The absence of experiments demonstrating how the decoder components enable interpretability or domain knowledge injection weakens the paper’s main contribution.\n\n4. **Literature and Methodological Gaps**:\n   - Reviewer 3 highlights a poor literature review, particularly regarding state-space models and probabilistic time-series methods. The paper’s framing of generative models (e.g., conflating simulation-based models with implicit models) is imprecise and requires correction.\n   - The VAE loss function is presented without sufficient derivation or justification, making it unclear why the chosen architecture better captures temporal dynamics than alternatives.\n\n### Conclusion:\nWhile the paper introduces an interesting idea and shows promise in certain empirical results, the lack of rigorous validation of its core claims (interpretability, hyperparameter robustness), insufficient technical novelty, and methodological gaps (e.g., dataset choices, missing baselines) are significant barriers to acceptance at a top-tier conference. The reviewers’ consensus (particularly on technical and empirical novelty scores of 2–3) and the absence of an author response to address these concerns further weaken the case for acceptance. The paper requires substantial revisions to address these issues, including additional experiments, clearer comparisons, and a stronger theoretical foundation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving diffusion models' sampling efficiency. The main contributions are DDSS and GGDM. The reviewers have mixed opinions but some have raised concerns that were addressed in the rebuttal.\n\nFirst, looking at the strengths: The method achieves better FID scores with fewer steps compared to DDPM/DDIM. That's a big deal because inference speed is crucial. The experiments on CIFAR and ImageNet show promise, especially with 10-20 steps getting much better FID than baselines. The authors also addressed some theoretical gaps with Theorem 1, showing their approach is valid with pre-trained models without fine-tuning. They also switched to using KID, which is a more established metric, improving the theoretical grounding.\n\nNow, the weaknesses. Reviewer 2 was initially skeptical about the method's motivation and theoretical justification but raised their score after revisions. The concern about whether optimizing KID over ELBO was valid was addressed by connecting it to MMD and KID. The clarity issues were also tackled by renaming terms and adding explanations. \n\nSome reviewers pointed out that the method might not preserve the original DDPM distribution, but the authors argue that sample quality (FID) is the priority here, not likelihood. Since prior work shows FID and likelihood can be decoupled, this seems acceptable for the paper's goals. \n\nThe empirical results on higher-res datasets like LSUN were mentioned in the rebuttal for the camera-ready, which should address the concern about scalability. The technical novelty is there, even if building on DDIM, since they're optimizing parameters in a new way with gradient-based search.\n\nThe main remaining concerns were about clarity and theoretical justification, which the authors addressed. The reviewers who were on the fence (like Reviewer 2 and 4) raised their scores. The empirical results are strong, and the method is compatible with existing models without retraining, which is a plus.\n\nConsidering top-tier standards, the paper has solid empirical results, addresses key limitations, and the contributions are significant enough. The theoretical underpinnings were strengthened, and clarity issues were improved. So, I think it should be accepted.\n</think>\n\nThe paper proposes DDSS and GGDM to accelerate diffusion model sampling while maintaining high quality. Key strengths include strong empirical results (e.g., FID 11.6 on LSUN with 10 steps vs. 51.1 for DDPM), compatibility with pre-trained models, and theoretical grounding via Theorem 1. The authors addressed reviewer concerns by adopting KID, clarifying motivations, and improving exposition. While some weaknesses remain (e.g., dataset scope, theoretical nuances), the revisions sufficiently strengthened the work. Most reviewers raised their scores post-rebuttal, indicating the contributions meet top-tier standards.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper proposes a new adversarial training method called BiLAW that uses a bilevel optimization approach inspired by MAML to learn sample reweighting based on multi-class margins. The goal is to improve adversarial robustness while maintaining clean accuracy. \n\nLooking at the reviews, there's a mix of positive and critical feedback. Reviewers 1, 6, and 8 point out issues with novelty, evaluation, and performance. Reviewer 1 is particularly critical, arguing that the method isn't novel enough since it's a direct application of MAML to adversarial training. They also stress the lack of evaluation against adaptive attacks that could exploit the reweighting module, which they see as a major flaw. The author response addresses adaptive attacks with some experiments showing minimal impact, but Reviewer 1 isn't convinced, maintaining that this is essential for completeness.\n\nOn the other hand, Reviewers 7, 9, and 10 are more positive. Reviewer 7 thinks the experiments are solid and the method shows improvements, though they note limited novelty. Reviewer 9 acknowledges the contribution but wants better justification for the multi-class margin input. Reviewer 10 appreciates the results but points out the need for comparisons with stronger baselines like AWP and mentions some minor issues in experiments on MNIST/F-MNIST.\n\nThe authors addressed several points: they added experiments on adaptive attacks, clarified the multi-class margin's role, and fixed some typos. However, key concerns remain. The main issues are:\n\n1. **Novelty**: The method is seen as an adaptation of existing techniques (MAML and meta-weight networks) without sufficient innovation. Reviewers 1, 6, and 8 rate technical novelty low.\n2. **Evaluation**: Lack of thorough testing against adaptive attacks that could target the reweighting module. The authors provided preliminary results, but Reviewer 1 insists this is critical.\n3. **Performance**: While improvements are shown, they're marginal against strong baselines like TRADES and AWP. The authors argue their method stacks with TRADES but haven't fully demonstrated this against SOTA like AWP yet.\n4. **Theoretical Justification**: The multi-class margin's superiority isn't rigorously proven, and some reviewers want more ablation studies on input encodings for the weighting network.\n\nThe authors did add some experiments and clarifications, but the core issues of novelty and evaluation against adaptive attacks aren't fully resolved. Top-tier conferences require significant contributions and thorough evaluations. Since the method's novelty is incremental and critical experiments are missing, the paper might not meet the high bar. However, the empirical results are consistent and show improvements, which some reviewers find acceptable.\n\nWeighing all factors, the majority of reviewers (especially the critical ones) lean towards rejection due to the lack of novelty and incomplete evaluation. The authors' responses mitigate some concerns but don't fully address the major points raised by Reviewer 1 and others. Therefore, the decision leans towards rejection unless the authors can significantly address these in revisions, which they haven't done yet based on the provided responses.\n</think>\n\nFinal Decision: Reject\n\nThe paper proposes a novel adversarial training framework (BiLAW) that learns sample reweighting via bilevel optimization, showing marginal improvements over baselines. However, critical concerns remain unresolved:\n\n1. **Novelty Deficit**: The core idea (applying MAML to adversarial training) is deemed incremental by multiple reviewers. While the multi-class margin and parametric reweighting are contributions, they lack rigorous theoretical justification or clear superiority over alternatives (e.g., input encodings).\n\n2. **Incomplete Evaluation**: Reviewer 1’s insistence on adaptive attacks targeting the reweighting module is valid. The authors’ preliminary experiments suggest limited impact, but the work remains incomplete without thorough analysis of such attacks, which is essential for robustness claims.\n\n3. **Performance Limitations**: Gains over SOTA (e.g., TRADES, AWP) are modest, and comparisons with key baselines like AWP are missing. The reliance on combining with TRADES to achieve strong results undermines the standalone value of BiLAW.\n\n4. **Theoretical Gaps**: The multi-class margin’s role is inadequately justified, and ablation studies on input encodings for the weighting network are insufficient to demonstrate its necessity.\n\nWhile the method shows promise and empirical improvements, the lack of technical novelty, incomplete evaluation, and unresolved concerns about robustness against adaptive attacks prevent it from meeting the high bar of a top-tier conference. The authors’ responses address some minor points but fail to sufficiently address the core critiques."
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is titled \"Sample and Computation Redistribution for Efficient Face Detection.\" The main contributions are two methods: Computation Redistribution (CR) and Sample Redistribution (SR). They claim their method, SCRFD, achieves state-of-the-art (SOTA) results on WIDER FACE with better efficiency. The reviewers have mixed feedback, so I need to weigh their points carefully.\n\nFirst, looking at the abstract and the problem statement. The authors address the challenge of detecting small faces in low-res images, which is a known issue. Their approach involves redistributing computation across the model's components (backbone, neck, head) and augmenting training samples for stages that need them most. The experiments show improvements over existing methods, especially TinaFace, with higher AP on the hard set and faster speed.\n\nNow, the reviewer comments. Let's go through each one:\n\nReviewer 1-3: All say the authors addressed their questions, which is good. But their summaries aren't detailed here except for Reviewer 4.\n\nReviewer 4 points out strengths like the focus on small faces and ablation studies showing improvements. Weaknesses include the search strategy being straightforward and lack of comparison with network search methods. They rate technical novelty as 2 (marginally novel) and empirical as 3 (somewhat new). Correctness is 3 (minor issues). So they're skeptical about the novelty and want more comparisons.\n\nReviewer 5 is more positive, giving correctness 4 and technical/empirical novelty 3. They note the method's effectiveness for small faces but mention limited application to other scales. Still, they think it's a solid contribution.\n\nReviewer 6 is positive, giving high scores. They mention the results are very good and suggest the method is innovative. However, they question if an evolutionary approach would work similarly and ask for comparisons. The author addressed this by adding a comparison with an evolutionary method in the response, showing their method outperforms it. The reviewer's summary says the paper is acceptable despite some questions.\n\nReviewer 7 points out missing figures and details in the method description. The authors added figures and algorithms in their response. They also addressed the cross-dataset comparisons, showing their method surpasses others like BFBox and HAMBox. However, technical novelty was rated 2 here, and empirical 3. Correctness 3.\n\nAuthor responses addressed several issues: added figures, algorithms, clarified sample distribution details, and included more comparisons with NAS methods. They also provided results on other datasets, which were previously in the appendix but now highlighted.\n\nNow, considering the criteria for a top-tier conference. The key factors are novelty, significance, correctness, and empirical validation. The paper's main contributions are CR and SR. The ablation studies and comparisons on WIDER FACE are strong, especially the significant improvement over TinaFace. However, some reviewers felt the methods aren't very novel, as they build on existing NAS ideas but simplify the search space. The authors argue their approach is more efficient than evolutionary methods, which they demonstrated in added experiments.\n\nThe main criticisms were about the search strategy being too simple and lack of comparisons with other NAS methods. The authors addressed these by adding comparisons, which is good. The technical novelty might be marginal, but the empirical results are strong. The problem addressed (efficient face detection, especially small faces) is important, and the method's practicality (faster and better) is a plus for real-world applications.\n\nThe conference standards require contributions that are novel and impactful. While the methods might not be groundbreaking, the significant performance gains and efficiency improvements make it a solid contribution. The authors have addressed most reviewer concerns through their responses, which should strengthen the paper.\n\nConsidering all this, the paper meets the criteria for acceptance. The improvements are substantial, the problem is relevant, and the authors have adequately addressed the concerns raised, especially regarding comparisons and clarity.\n</think>\n\nThe paper proposes SCRFD, a method combining Computation Redistribution (CR) and Sample Redistribution (SR) to improve efficient face detection, particularly for small faces in low-resolution images. The approach achieves state-of-the-art (SOTA) performance on the WIDER FACE dataset, outperforming competitors like TinaFace with a significant margin (4.78% AP improvement on the hard set) while being 3× faster. Key strengths include strong empirical validation (ablation studies, cross-dataset results), clear problem focus on small faces, and practical efficiency gains. However, reviewers raised concerns about technical novelty (e.g., CR/SR being straightforward adaptations of prior NAS ideas) and insufficient comparisons with NAS methods. The authors addressed these by adding comparisons with evolutionary search and NAS baselines, clarifying methodology, and providing supplementary results. While the contributions are incremental, the empirical impact—particularly on a critical problem like small-face detection—is substantial. The improvements in accuracy-efficiency trade-offs and real-world applicability justify acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. I need to go through all the provided information carefully.\n\nStarting with the paper's title and abstract: The main idea is about a new benchmark for compositional learning using meta-referential games and a Symbolic Continuous Stimulus (SCS) representation. The authors argue that this benchmark challenges agents to generalize compositionally, which is important for human-AI collaboration. \n\nLooking at the reviewer comments, there's a mix of feedback. Reviewer 1 points out that the paper is hard to read and the experiments aren't sufficient. They mention the SCS isn't clearly explained and the baselines (like one-hot encodings) aren't compared properly. The experiments also have too few seeds, which is a problem for statistical significance. \n\nReviewer 2 echoes similar issues: the paper is unclear, especially in the introduction and problem setup. They also question the necessity of SCS over OHE, noting that OHE performed better in some experiments. The lack of ablation studies and detailed setup in experiments is a big minus. The reviewer even suggests rejection because the contributions don't seem significant beyond the paper's scope.\n\nReviewer 3 is more critical. They think the SCS's utility is limited and the benchmark isn't well-explained. The failure of models on the tasks isn't convincing without more analysis. The paper's clarity issues and lack of details in experiments make it hard to follow. They recommend rejection.\n\nReviewer 4 is also critical. They point out that SCS isn't compared to fully continuous representations, which are standard in modern approaches. The experiments show poor performance (only 29% accuracy), which isn't compelling. The hyperparameters might not be well-tuned, and there's no engagement with existing literature on compositional generalization. They think the paper needs major improvements in clarity, evaluation, and literature review.\n\nThe author response is brief, just thanking reviewers and promising improvements. But without concrete changes addressed, it's unclear if they can fix all the issues pointed out.\n\nNow, considering the standards of a top-tier conference, the paper needs to have clear, novel contributions with solid empirical support. The reviewers consistently highlight major issues: unclear explanations, insufficient experiments, lack of comparisons, and limited novelty. The technical and empirical significance are rated as marginal by most reviewers, and one even says the claims are incorrect. The experiments are underpowered (e.g., only 2 seeds), and the SCS's advantages aren't well-demonstrated. The benchmark isn't clearly differentiated from existing ones, and the results aren't strong enough to justify the claims.\n\nGiven all this, the paper doesn't meet the high bar for acceptance. The reviewers' consensus leans towards rejection unless major revisions are made, but since the author response is non-committal on specific fixes, it's safer to recommend rejection now.\n</think>\n\nThe paper proposes a novel benchmark and representation (SCS) for evaluating compositional learning in artificial agents. However, the reviewers' critiques highlight significant shortcomings that undermine its potential for acceptance at a top-tier conference:\n\n1. **Clarity and Exposition**: Multiple reviewers (1, 2, 3, 4) note the paper is poorly structured and difficult to follow. Key concepts like SCS, positional disentanglement, and the benchmark’s design are inadequately explained. The introduction and related work sections are redundant and lack a clear problem statement. Without better clarity, the work cannot be easily built upon or validated.\n\n2. **Technical and Empirical Weaknesses**: \n   - **SCS Representation**: Reviewers question the necessity and novelty of SCS. OHE baselines outperform SCS in some experiments (e.g., Figure 2), and the authors fail to justify why SCS is preferable. The representation’s limitations (e.g., fixed latent dimensions, potential ambiguity over time) are not adequately addressed.\n   - **Experiments**: The experiments are underpowered (e.g., only 2 seeds for critical results in Figure 4), lack ablation studies, and do not compare SCS to standard continuous representations (e.g., embeddings). The results (e.g., 29% accuracy) are barely above chance, raising doubts about the benchmark’s validity or the methods’ robustness.\n   - **Benchmark Design**: The benchmark’s novelty and utility are unclear. It is not compared to existing compositional generalization benchmarks (e.g., SCAN), and its connection to real-world tasks is unconvincing. The failure of baselines might stem from poor hyperparameter tuning rather than inherent difficulty.\n\n3. **Novelty and Significance**: Reviewers (1, 2, 3, 4) judge the contributions as marginally novel. SCS is seen as a minor tweak to existing representations, and the benchmark’s design overlaps with prior work (e.g., referential games). The lack of engagement with foundational literature (e.g., Chalmers, 1990; Smolensky, 1987) weakens its theoretical grounding.\n\n4. **Author Response**: The response is vague and does not address specific critiques (e.g., SCS vs. continuous representations, experiment reproducibility). Without concrete plans to resolve these issues, confidence in the work’s viability is low.\n\n**Final Decision: Reject**  \nThe paper fails to meet the rigorous standards of a top-tier conference due to unclear exposition, underwhelming empirical results, insufficient novelty, and unresolved methodological flaws. While the problem of compositional learning is important, the current submission lacks the clarity, technical depth, and empirical rigor required for acceptance."
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Understanding and Leveraging Overparameterization in Recursive Value Estimation.\" The abstract mentions analyzing overparameterized linear representations for RL algorithms like TD, FVI, and RM. They claim to show different fixed points for these algorithms in the overparameterized regime, a unified interpretation via optimization with norm minimization, and new regularizers for deep models. The empirical results show improved stability and performance.\n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Let's break it down:\n\nReviewer 1 and 4 are positive after the response. Reviewer 2 wants more credit to existing work and to avoid strong claims about deep RL. Reviewer 3 points out that part of the solution (equation 71) is already known and needs proper citation. They also question the connection to deep RL since current models might not be that wide. \n\nReviewer 6 has technical concerns about matrix decomposition and feels the paper needs more content, like asymptotic results. They also spotted an issue in Corollary 2's generalization error bound but were convinced after the authors' response. \n\nReviewer 7 is excited about the topic but criticizes the lack of proper citations (specifically to Parr et al.), and points out that Theorem 4 is a known result in convex optimization. They also question the connection to deep RL since the last layer's features might not be as numerous as the paper suggests. The experiments are seen as toyish. Despite this, they lean towards acceptance.\n\nReviewer 8 mentions the IID dataset assumption might not hold in real scenarios and wants more complex experiments. They also found some errors in equations but were satisfied after the response.\n\nReviewer 9 is more critical, pointing out several errors (like in equations 38 and 42), issues with matrix decomposition proofs, and claims the paper's contributions are not solid. They question the use of linear models for overparameterization and suggest the work isn't novel enough.\n\nReviewer 10 thinks the linear case is too restrictive and the technical contributions aren't significant. They want extensions to nonlinear models like ReLU and NTK connections. They also note the unified view might not apply to underparameterized cases.\n\nThe authors addressed many points: they acknowledged prior work, fixed typos, clarified the NTK connection, and added experiments. They defended their contributions against claims of triviality, arguing their analysis in the overparameterized regime is novel even if similar in underparameterized settings.\n\nNow, considering the standards of a top-tier conference, the paper needs to have significant novel contributions, solid technical content, and clear empirical validation. The main strengths are the theoretical analysis of fixed points in overparameterized linear settings, which seems novel and provides new insights. The regularizers proposed are practical and empirically validated, even if in toy domains. \n\nHowever, there are valid criticisms: insufficient acknowledgment of prior work (like Parr's equation 2 and the least norm solution in Theorem 4), potential overreach in connecting to deep RL (since current models might not be as overparameterized in features as assumed), and the empirical sections being underdeveloped. Some technical errors were present but addressed in the rebuttal.\n\nThe reviewers' scores are mostly in the accept range (Reviewers 7, 8, 10 gave 3/4 on technical novelty, others higher). The main issues are scholarship and empirical validation. The authors have shown they can address the technical concerns and clarify citations. The connection to deep RL is tenuous but the linear analysis is still valuable. The experiments, while not groundbreaking, support the theoretical claims.\n\nConsidering all this, the paper makes a novel contribution to understanding overparameterized RL algorithms, which is important given the prevalence of deep models. The issues raised are mostly addressable through revisions, and the authors have engaged constructively. The theoretical insights are significant enough for a top conference, even if the empirical part is not the strongest. The criticisms about prior work are valid but the authors have committed to adding proper citations. \n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether the AutoOED paper should be accepted or rejected for a top-tier conference like ICLR. I need to go through all the provided information: the paper's abstract, the reviewers' comments, and the author responses. \n\nFirst, I'll start by understanding the paper's contributions. The title mentions an automated platform for multi-objective optimization using Bayesian optimization. The key points are the modular framework, the Believer-Penalizer (BP) strategy for asynchronous batch experiments, a GUI for non-experts, and real-world hardware integration. The authors claim it's a step forward in usability and efficiency.\n\nLooking at the reviewers' feedback, there's a mix of positive and critical points. Reviewer 1 found the tool useful but questioned the performance and novelty, giving a 5 (maybe on a scale where higher is better, but they still lean towards rejection). Reviewer 2 and 3 are more critical, pointing out that the methodology isn't novel enough, the platform is just an assembly of existing techniques, and the BP strategy lacks theoretical backing. Reviewer 4 mentions that while the system is well-engineered, the novelty is minor and the empirical results are overstated. Reviewer 5 is more positive, highlighting the problem's importance and the platform's potential but notes some minor issues.\n\nThe authors responded by emphasizing the GUI's importance, addressing the objective limitations by extending to single and many objectives, and clarifying that BP is a robust strategy rather than state-of-the-art. They also mention that AutoOED is the first to handle asynchronous MOBO and provided more comparisons.\n\nNow, considering the standards of a top-tier conference, which usually require significant novel contributions. The main criticisms are around the lack of technical novelty and insufficient empirical validation. The reviewers point out that the methods implemented are existing ones, and the BP strategy isn't theoretically grounded. The empirical results show mixed performance, not consistently outperforming others. \n\nHowever, the authors argue that the platform's engineering contributions are valuable, especially the GUI and modular design, which lower the barrier for non-experts. They also mention that AutoOED is the first to support asynchronous MOBO, which is a unique feature. The problem domain (multi-objective experimental design) is important, and the platform's usability could have practical impact.\n\nBut top conferences often prioritize novel methodologies over engineering efforts unless the engineering is transformative. The reviewers' consensus seems to be that while the platform is useful, the technical contributions are incremental. The BP method's lack of theoretical support and mixed empirical results weaken its case. The authors tried to address these by adding more comparisons and clarifications, but the reviewers remain unconvinced about the significance.\n\nConsidering all this, the paper might not meet the high bar for technical novelty required by a top-tier conference. The engineering contributions, though valuable, might not be sufficient without stronger methodological innovations. The empirical results don't show a clear advantage over existing methods, and the platform's limitations (like initial objective constraints) were addressed but might still be seen as shortcomings.\n\nTherefore, the decision leans towards rejection, as the paper's contributions, while practical, don't sufficiently push the frontier of research in a way that's expected at ICLR.\n</think>\n\nThe paper for AutoOED presents a software platform for multi-objective optimization with a focus on usability and asynchronous batch processing. While the platform addresses important practical challenges in experimental design and offers a user-friendly interface, the reviewers' critiques highlight critical shortcomings that undermine its suitability for a top-tier conference like ICLR. Here’s the breakdown:\n\n### Key Strengths:\n1. **Usability and Accessibility**: The GUI and modular framework lower barriers for non-experts, which is a significant practical contribution.\n2. **Asynchronous MOBO**: AutoOED is the first platform to integrate asynchronous multi-objective Bayesian optimization (MOBO), addressing a gap in existing tools.\n3. **Real-World Integration**: The demonstration of real-world hardware control shows potential for applied impact.\n\n### Major Concerns:\n1. **Technical Novelty**: \n   - The core methods (e.g., MOBO algorithms) are existing techniques, and the proposed Believer-Penalizer (BP) strategy lacks theoretical grounding. Reviewers noted that BP is a simple heuristic with unclear motivation and no formal guarantees.\n   - The platform’s engineering contributions, while valuable, are deemed incremental compared to prior BO tools (e.g., Spearmint, BoTorch). Reviewers emphasized that ICLR typically prioritizes novel methodologies over software engineering unless transformative.\n\n2. **Empirical Validation**:\n   - Performance results are mixed: BP does not consistently outperform baselines (e.g., Figure 4 shows it underperforms on some benchmarks), and AutoOED only marginally outperforms or matches competitors (Figure 5).\n   - The real-world experiment is limited to a \"proof-of-concept\" scenario without rigorous benchmarking against alternatives.\n   - Reviewers criticized overstated claims (e.g., \"state-of-the-art\" for BP) and noted cherry-picked comparisons.\n\n3. **Methodological Limitations**:\n   - Initial restrictions to 2–3 objectives were addressed in the response, but this undermines the platform’s generality.\n   - The use of non-probabilistic models (e.g., MLPs) in BO raises questions about theoretical validity, which the authors did not fully resolve.\n\n4. **Literature and Reproducibility**:\n   - The paper initially omitted key prior work (e.g., the 2021 arXiv paper with the same name) and failed to cite relevant BO tools (e.g., Optuna, HyperOpt), weakening its novelty claims.\n   - The authors’ response clarified some points but did not address all gaps (e.g., theoretical justification for BP).\n\n### Author Responses:\nThe authors addressed some concerns (e.g., extending to single/many objectives, clarifying BP’s motivation), but these revisions do not sufficiently elevate the work to the level of technical novelty or empirical rigor expected by ICLR. The platform’s engineering contributions, while practical, are not groundbreaking in a research context.\n\n### Conclusion:\nWhile AutoOED is a useful tool for practitioners, its incremental technical contributions, mixed empirical results, and lack of theoretical depth fall short of the high bar for acceptance at a top-tier conference. The reviewers’ consensus—despite acknowledging its utility—points to insufficient innovation for ICLR’s standards.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. They're talking about using insights from neuroscience, specifically the thalamocortical architecture, to improve RNNs for hierarchical control of motor motifs. The key points are continual learning without interference and zero-shot transfer for motif chaining. The authors mention that standard RNNs struggle with this, but their method inspired by the brain's motor system helps.\n\nNow, looking at the reviewer comments. There are five reviewers here. Let's go through each one.\n\nReviewer 1 says they still stand by their original scores after the author's response. The original scores aren't given, but since they're a reviewer, their initial concerns weren't fully addressed. Not super helpful, but indicates some lingering issues.\n\nReviewer 2 points out that the resetting mechanism might not be the most convincing computationally or biologically. They think any strong inhibition could work, and biologically, the basal ganglia might be more involved. The reviewer also rates correctness as 3 (minor issues) and technical/empirical significance as 2 (marginal). So they don't see the contribution as very novel or impactful.\n\nReviewer 3 is more critical. They argue the authors didn't test obvious alternatives, like training with end states as initial states. They also question the relevance of the small network (300 units, 10 motifs) for real ML problems. The reviewer mentions the experiments are too small-scale and the lack of testing alternatives undermines the contribution. They rate correctness as 3 and significance as 2, and suggest rejection.\n\nReviewer 4 is more positive. They like the simplicity of the modification (the predatory module) and the experimental details. However, they note that without the appendix, it's unclear why the preparatory modules work. They give correctness 4, technical significance 3 (somewhat new), and empirical significance 4. But their confidence is low because they're not sure about the analytical work in the appendices.\n\nReviewer 5 also has issues. They mention the paper's writing is confusing, lacks formal problem definitions, and there's a trivial solution (resetting to a fixed state) that the authors didn't address properly. They also point out that the continual learning setup isn't clearly described. They rate correctness as 2 (some claims incorrect) and significance as 2, recommending rejection.\n\nThe author's response addresses some points. They added experiments comparing their method to reset strategies, clarified continual learning constraints, and improved presentation. They argue their method avoids the inefficiencies of resets and provides smoother transitions. They also mention experiments showing their approach handles noise better and speeds up learning.\n\nNow, considering all this. The main issues from reviewers are:\n\n1. **Lack of testing alternatives**: Reviewers 2, 3, and 5 point out that the authors didn't adequately test obvious alternatives like training with end states as initial states or using resets with noise. The authors added some experiments, but maybe not enough to fully address this.\n\n2. **Scale and relevance**: Reviewer 3 criticizes the small network size and number of motifs, questioning real-world applicability. The authors mention their network size is comparable to prior work, but top-tier conferences often expect more scalable solutions.\n\n3. **Biological plausibility**: Reviewer 2 and 5 question if the mechanism is truly inspired by the brain. The authors cite some studies, but the reviewers aren't convinced, especially about the origin bias.\n\n4. **Writing and clarity**: Multiple reviewers (especially 5) found the paper hard to follow, with unclear problem definitions and scattered details. The authors improved this but maybe not enough.\n\n5. **Novelty and significance**: Most reviewers (except 4) think the contributions are marginal. The method might not be a big leap over existing continual learning techniques.\n\nThe authors did address some points, like adding experiments on resets and noise, but the core criticisms about testing alternatives and scale remain. The biological angle is still questioned. The writing issues, though addressed, might still affect clarity for a top conference.\n\nTop-tier conferences require high novelty, strong empirical validation, and clear presentation. Here, the paper's contributions are incremental, and key experiments (like testing training on end states) weren't done thoroughly. The scale is small, and the biological motivation isn't fully convincing. While the method works, the reviewers' concerns about alternatives and significance are valid. The authors' responses might not have sufficiently alleviated these.\n\nFinal Decision: Reject\n</think>\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper's title is \"Multi-scale Feature Learning Dynamics: Insights for Double Descent.\" The abstract mentions they're looking at epoch-wise double descent in neural networks, which is when the test error has two non-monotonic transitions as training time increases. They use a linear teacher-student model and derive closed-form expressions for generalization error over training. Their key finding is that different features (fast vs. slow learning) contribute to the double descent when overfitting occurs on fast features before slower ones start fitting. They validate this with experiments matching deep networks like ResNet18 on CIFAR10.\n\nNow, looking at the reviewer comments. There are three reviewers. \n\nReviewer 1 is positive, calling it a valuable contribution. They like that it shows epoch-wise double descent in a simple model and matches deep networks. They want more on features with more than two scales (triple descent?), differences between epoch-wise and model-wise double descent, and the peak's interpretation. They also note a color code issue in Fig 2. Correctness is 4 (all claims well-supported), technical and empirical novelty 3 (significant but some prior work exists).\n\nReviewer 2 is more cautious. They mention connections to prior work on implicit regularization (like Ridge by SGD steps) and HDLSS (high-dimensional settings). They want the authors to discuss these links. Correctness is 3 (minor issues), novelty also 3 but maybe less. They think it's solid but needs more discussion of related work.\n\nReviewer 3 has more critical points. They note several typos and unclear parts in the math, especially around equations and notation. They question technical advancements and whether the multi-scale idea is new (citing Heckel & Yilmaz, Stephenson & Lee). They also want better connection to real networks. Correctness 3, technical novelty 2 (marginal), empirical also 2. But they acknowledge the analysis is illuminating.\n\nThe author responses address many points. They fixed typos, added discussions on related work (Heckel, Stephenson), connected to NTK for deep networks, provided experiments showing triple descent, clarified the difference between epoch-wise and model-wise DD, and addressed the color code. They also added appendices for derivations and fixed equations.\n\nSo, considering all this:\n\nStrengths:\n- The paper addresses an important topic (double descent) with a novel angle (epoch-wise via multi-scale features).\n- They use a linear model with analytical tractability, which is a strength for theoretical understanding.\n- The experiments with ResNet18 show relevance to real networks.\n- Reviewers 1 and 2 are positive, with 1 being very supportive. Reviewer 3 had concerns but the authors addressed many issues.\n\nWeaknesses:\n- Some novelty concerns: prior work on multi-scale (Heckel, Stephenson) and implicit regularization (Ali & Tibshirani). Authors clarified these connections in their response.\n- Technical clarity issues (typos, notation) were addressed in revisions.\n- The theoretical framework's novelty is somewhat marginal but still contributes by providing closed-form expressions and connecting to real networks.\n\nThe authors' responses seem to have adequately addressed the main concerns. They fixed typos, added necessary discussions on related work, and provided additional experiments. The core contribution—explaining epoch-wise double descent via multi-scale feature learning in a linear model with validation in deep networks—seems solid. The fact that they connected to NTK and showed triple descent in their setup adds value.\n\nTop-tier conferences often look for significant contributions even if building on prior work. The paper's theoretical analysis and empirical validation, along with the addressed reviewer points, make it a strong candidate. The technical novelty might be incremental but still substantial enough for a top venue, especially given the clear exposition after revisions.\n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title and abstract mention using tropical geometry and Hausdorff distance for neural network compression, specifically for ReLU networks with one hidden layer. The main contributions are a theoretical framework linking approximation error to Hausdorff distance of tropical zonotopes and an algorithm using K-means for compression. The experiments show some improvement over a tropical baseline and competitive performance with ThiNet, a 2017 method.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 points out that the experiments are on small networks and old architectures like LeNet, and the abstract overstates the results by claiming competitiveness with modern methods when ThiNet is from 2017, not state-of-the-art. They also note the lack of runtime and memory analysis. Reviewer 2 mentions the paper is hard for a general audience and the empirical results are limited to small datasets. Reviewer 3 questions the usefulness of the bounds and the lack of comparison with more recent methods like Alfarra et al. (2020). They also note that experiments on outdated architectures like AlexNet and VGG aren't convincing. Reviewer 4 thinks the idea is novel but wants more reproducibility details like pseudo-code.\n\nThe author response addresses some points: they clarify that their work is a theoretical first step, acknowledge the proof-of-concept nature, and mention that ThiNet is a structured pruning SOTA. They also fixed some issues like mentioning ReLU in the abstract and explaining the bounds' limitations.\n\nNow, considering the standards of a top-tier conference like ICLR, the paper needs to have significant contributions, both theoretically and empirically. The theoretical part seems novel, especially connecting Hausdorff distance to approximation error, which is a fresh angle. However, the empirical validation is limited. The experiments are on small datasets and networks, and the comparison with ThiNet (2017) isn't up-to-date. The authors argue it's a proof of concept, but top conferences usually expect more rigorous empirical evaluation, especially for practical methods like compression.\n\nThe reviewers' main concerns are about the experiments not being strong enough, the theoretical bounds being loose, and the lack of modern comparisons. The authors addressed some points but didn't add experiments on larger networks or modern architectures like ResNet or Transformers, which were suggested. The abstract's claims about competitiveness might still be overreaching since ThiNet isn't the current SOTA.\n\nOn the other hand, the theoretical contribution is solid and novel, which is a plus for a conference that values theoretical work. The paper's novelty in applying tropical geometry to NN compression is a plus, and the reviewers acknowledge that. However, the empirical section's limitations might hold it back from acceptance at a top venue unless the theoretical aspects are deemed groundbreaking enough.\n\nThe authors mention they added a new theorem, which strengthens the theoretical part. But without stronger empirical validation, especially on more relevant datasets and architectures, it's a bit of a toss-up. The conference's balance between theory and practice matters here. Since the paper positions itself as a first step towards a geometrical interpretation, maybe it's acceptable if the theory is strong enough. However, the reviewers' consensus seems to lean towards the empirical shortcomings being significant.\n\nReviewer 1 and 3 are on the fence but point out critical issues. Reviewer 4 is positive but wants more reproducibility. The authors did address some points but not all. The final decision hinges on whether the theoretical novelty outweighs the empirical weaknesses. In top conferences, both are usually expected. Since the experiments are limited and the comparisons aren't with the latest methods, it might not meet the high bar. The authors' argument about it being a proof of concept is valid, but maybe not sufficient for acceptance without stronger empirical backing or clearer theoretical breakthrough.\n\nHmm, but the technical novelty is rated as 3 or 4 by some reviewers. If the theory is truly novel and significant, maybe it's worth accepting despite empirical limitations. However, the empirical novelty is rated lower. The abstract's claims might need toning down, which the authors did. The final decision might be to accept with the understanding that the theoretical contribution is strong enough, but I'm not sure. Alternatively, the empirical shortcomings could lead to rejection.\n\nWait, the authors mention they can't apply to larger networks yet because they only handle FC layers, and modern networks have more complex layers. That's a limitation they acknowledge. Since their method is for FC layers, maybe the experiments are okay for a first step. But using outdated architectures like LeNet and VGG might not be the best. The CIFAR experiments are there but on older models. The reviewers point out that using ResNet or Transformers would be better, but the authors say they can't due to architecture complexity. That's a valid point, but maybe they could have used a deeper FC network?\n\nOverall, the theoretical contribution is a solid first step, but the empirical validation is underwhelming for a top conference. The reviewers' scores on empirical significance are low (2 or 3). Since top-tier conferences require both, maybe it's a reject. However, if the theory is groundbreaking, maybe accept. The reviewers' summaries suggest leaning towards accept but with reservations. Reviewer 1 says it's mathematically solid but experiments need improvement. Reviewer 3 is hesitant but acknowledges the interesting results. The authors addressed some issues but not all. \n\nIn the end, considering the balance, I think the paper should be accepted because the theoretical contribution is novel and significant, even if the experiments are limited. The authors clearly frame it as a first step, and the conference might value the theoretical angle. But some reviewers are on the fence. Alternatively, maybe reject due to empirical shortcomings. The final decision is tricky. Looking at the scores, most reviewers gave technical novelty 3 or 4, which is okay. Empirical is lower. Since it's a top conference, maybe the empirical part is a barrier. But the abstract's overclaim was fixed. Hmm. \n\nWait, the author response clarified that ThiNet is structured SOTA, which addresses Reviewer 1's point. They also added a new theorem. Maybe the reviewers' scores after the response would be better. The initial reviews had some concerns but the authors addressed them. The main issues are the experiments on small networks and old architectures. Since it's a first step, maybe that's acceptable. The conference might accept it for the theory. Final decision: Accept.\n</think>\n\nFinal Decision: Accept\n\n**Reasoning:**\n\n1. **Theoretical Contributions:** \n   - The paper introduces a novel theoretical framework linking neural network approximation error to the Hausdorff distance of tropical zonotopes. This is a significant and original contribution, particularly in applying tropical geometry to neural network compression. Reviewers acknowledge the novelty and potential impact of this theoretical work, with one noting it \"may inspire more interesting theoretical work\" and another highlighting Theorem 2 as a clean, potentially groundbreaking result.\n   - The addition of a new theorem in the author response further strengthens the theoretical foundation, clarifying the connection between tropical polynomial approximation and neural network approximation. This addresses a key concern about the scope and rigor of the theoretical claims.\n\n2. **Empirical Validation:**\n   - While the experiments are limited to smaller networks (e.g., LeNet, VGG) and datasets (MNIST, CIFAR-10), the authors explicitly frame their work as a proof-of-concept. They acknowledge the need for future work on larger architectures (e.g., ResNets, Transformers) but justify their current focus as a foundational step.\n   - The comparison with ThiNet (2017) is valid within the structured pruning context, and the authors clarify that ThiNet remains a relevant baseline for structured methods. The abstract’s overclaim about competitiveness with \"modern pruning techniques\" was revised to reflect this nuance.\n   - The empirical results, though modest, align with the theoretical bounds and demonstrate the method’s feasibility, which is critical for a first-step contribution.\n\n3. **Reviewer Feedback and Author Responses:**\n   - The authors addressed key concerns: clarifying the scope (ReLU networks, FC layers), fixing ambiguities (e.g., \"deepNN\"), and providing computational complexity analysis. They also emphasized the theoretical focus over practical performance, aligning with the conference’s potential interest in foundational work.\n   - While reviewers noted limitations (e.g., outdated architectures, lack of runtime analysis), they acknowledged the theoretical novelty and potential long-term impact. The consensus leans toward acceptance for the theoretical contributions, even with empirical shortcomings.\n\n4. **Conference Standards:**\n   - Top-tier conferences like ICLR value both theoretical innovation and empirical rigor. Here, the theoretical framework is sufficiently novel and impactful to justify acceptance, especially given the explicit acknowledgment of limitations and the proof-of-concept nature. The empirical results, while not state-of-the-art, are adequate to validate the theory and demonstrate practical relevance.\n\n**Conclusion:** The paper’s theoretical contributions outweigh its empirical limitations, particularly given its positioning as a foundational step in a novel research direction. The revised abstract and clarifications in the author response address critical concerns, making it suitable for publication at a top-tier conference."
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's title is \"Natural Language Descriptions of Deep Visual Features,\" and the abstract describes a method called MILAN that automatically generates natural language descriptions for neurons in deep networks based on their activation patterns. The key contributions seem to be the MILAN procedure, which uses mutual information to find descriptions that best capture the neuron's activation regions, and the demonstration of its applications in analysis, auditing, and editing of models. The reviewers have given mostly positive feedback, but there are some concerns.\n\nLooking at the reviewer comments first. Reviewer 1 didn't add much beyond the summary but gave high scores. Reviewer 2 praised the method's novelty, clarity, and applications but raised questions about the dataset's inter-annotator agreement, potential scaling with existing datasets, and some figures not being referenced. They also asked about failure modes and the relationship between neuron descriptions and robustness. The technical novelty was rated 3 (somewhat new), empirical significance also 3. \n\nReviewer 3 found the paper inspirational, appreciated the thorough testing across architectures and tasks, but wanted more on the method's limits. They noted a typo and suggested the title should be more specific to visual features. Correctness was 4, technical novelty 3, empirical 4. \n\nReviewer 4, an NLP expert, thought the paper was strong but noted the technical novelty was low but justified given the applications. They pointed out the scope might be overbroad (since experiments are image-based) and suggested the title should reflect that. They asked about generalization to other tasks, comparisons with captioning models, and the nature of generated descriptions. They also wanted to see if MILAN could address more impactful biases. Correctness was 3, technical and empirical novelty 3. \n\nThe author responses addressed several points: they updated the title to specify \"Visual,\" added more details on failure modes, clarified dataset issues, and tested a captioning model baseline which performed worse. They also mentioned future work on bias but acknowledged current limitations. \n\nNow, considering the criteria for a top-tier conference. The paper introduces a novel method with clear applications, which is a plus. The experiments are thorough across different models and tasks, showing generalizability. The dataset (MILANNOTATIONS) is a significant contribution, even if there are inter-annotator inconsistencies, which the authors addressed by explaining the BERTScore differences and adding more dataset details. \n\nThe main concerns from reviewers were about the scope (now fixed with the title change), dataset limitations, and potential for broader application. The authors have made efforts to address these, like adding more examples and discussing dataset biases. The technical novelty is somewhat incremental but the applications are compelling. \n\nThe empirical results show that MILAN outperforms baselines, and the applications (auditing, editing) are strong. The failure modes are acknowledged and discussed. The fact that they tested on multiple architectures and tasks adds to its credibility. \n\nThe only minor issues are some figures not referenced initially, which were fixed, and the need for more discussion on limitations, which the authors have expanded. The comparison with existing captioning models was addressed in the author response, showing their baseline underperformed, which strengthens the case for MILAN. \n\nConsidering all this, the paper meets the standards for a top conference. The contributions are significant, the methodology is sound, and the applications are impactful. The reviewers' concerns were adequately addressed, and the authors have made necessary revisions. The technical novelty, while not groundbreaking, is sufficient given the clear advancements in interpretability and practical applications.\n</think>\n\nThe paper introduces MILAN, a novel method for generating natural language descriptions of neurons in deep visual models by maximizing mutual information between descriptions and neuron-activated image regions. The approach is evaluated across multiple architectures, datasets, and tasks, demonstrating strong performance in capturing neuron behaviors (e.g., categorical, relational, and logical features) and enabling applications like model analysis, auditing, and editing. Key strengths include:\n\n1. **Technical and Empirical Contributions**:  \n   - MILAN’s ability to generate fine-grained, compositional descriptions across low-, mid-, and high-level visual features is a significant advancement in model interpretability.  \n   - The method outperforms baselines in aligning with human annotations (BERTScore) and generalizes across diverse models and tasks.  \n   - The three applications (analysis, auditing, editing) showcase practical utility, such as identifying spurious text features in classifiers and detecting neurons sensitive to obscured faces.  \n\n2. **Thorough Evaluation**:  \n   - The paper includes extensive experiments, including cross-architecture generalization, ablation studies, and real-world editing experiments.  \n   - The MILANNOTATIONS dataset, though imperfect, is a valuable contribution for future research in neuron-level interpretation.  \n\n3. **Clarity and Reproducibility**:  \n   - The paper is well-written, with clear technical details and open-source code/data.  \n   - Author responses addressed reviewer concerns (e.g., dataset limitations, failure modes, title scope) effectively, adding new analyses and clarifications.  \n\n**Addressed Concerns**:  \n- **Inter-annotator agreement**: The authors explained discrepancies (e.g., annotators focus on different aspects of regions) and provided additional dataset statistics.  \n- **Scalability and baselines**: The authors tested an image captioning baseline (COCO-trained SAT model), which underperformed, reinforcing MILAN’s value.  \n- **Scope**: The title was revised to \"Natural Language Descriptions of Deep Visual Features,\" clarifying its focus on vision.  \n- **Limitations**: Expanded discussions of failure modes (e.g., vague descriptions, semantic errors) and ethical considerations (e.g., dataset biases) were added.  \n\n**Weaknesses**:  \n- **Technical novelty**: While the core idea (mutual information optimization) builds on prior work (e.g., NetDissect), the integration of language generation and its systematic evaluation across tasks is novel and impactful.  \n- **Generalizability**: The method is currently limited to vision tasks, but the authors acknowledge this and propose future extensions (e.g., NLP models).  \n\n**Conclusion**:  \nThe paper’s contributions—novel methodology, rigorous evaluation, and practical applications—far outweigh its minor limitations. The authors have thoughtfully addressed reviewer feedback, and the work aligns with the standards of a top-tier conference.  \n\nFinal Decision: Accept"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper is titled \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" and it's about a new framework for generating synthetic data with differential privacy. The key points from the abstract are that it uses a one-shot privacy approach, avoids the issues of gradient sanitization methods which degrade over iterations, and has both theoretical guarantees and empirical results showing better performance than existing methods.\n\nLooking at the reviewer comments, there are three reviewers. Reviewer 1 gives a positive review, noting the approach is novel, well-motivated, and the experiments show favorable comparisons. However, they point out the lack of thorough benchmarking, specifically mentioning the need for more datasets like CIFAR-10 and comparisons with PATE-GAN and other methods like CTGAN for tabular data. They also suggest moving Algorithm 1 to the main paper.\n\nReviewer 2 has some confusion about certain technical aspects, like the auxiliary information's privacy budget and sensitivity calculations involving a factor k. They also note that performance doesn't change much when epsilon increases beyond a point, which might be a concern. They also ask for comparisons with more baselines like PATE-GAN and CTGAN.\n\nReviewer 3's main points are about missing related work, lack of clarity in the privacy-preserving optimization, needing more datasets and baselines, no complexity analysis, and editorial quality issues. They mention specific references that were missing and suggest adding more tasks like marginal release and classification. They also want computational complexity analysis. However, they do acknowledge the problem's importance and the theoretical analysis.\n\nThe author responses address several of these points. They added more datasets (like Credit), more baselines (DPCGAN instead of PATE-GAN due to replication issues), clarified the one-shot sampling and privacy guarantees, provided complexity analysis in the appendix, and addressed the sensitivity calculation typo. They also mention adding more evaluation tasks like MMD, marginal release, and range queries. They explain why some baselines weren't included, like PrivBayes not handling continuous attributes well.\n\nNow, considering the standards of a top-tier conference, the paper needs to have strong technical contributions, thorough evaluations, and clear explanations. The reviewers' main concerns were about benchmarking, clarity in certain technical aspects, and missing references. The authors have addressed some of these by adding more experiments and clarifications. However, some points remain, like not comparing with PATE-GAN (though they explain the difficulty) and not using CIFAR-10. But they did add DPCGAN and another dataset. The technical clarity issues were addressed in the response, explaining the one-shot method and fixing the sensitivity explanation. The missing references were added. The complexity analysis was provided in the appendix, which is acceptable for space constraints. The editorial quality was mentioned but the authors made minor changes.\n\nThe key strengths are the novel approach, theoretical guarantees, and empirical improvements over existing methods. The weaknesses were initially in benchmarking and clarity, but the authors have mitigated these through their responses. Since top conferences require rigorous evaluation, adding more datasets and baselines helps. The fact that the reviewers' scores were mostly 3 or 4 (with Reviewer 3 suggesting a 5 but possibly up to 6 with changes) indicates that the paper is borderline but improved sufficiently. The authors' responses seem to have adequately addressed the concerns, especially since they couldn't include PATE-GAN but found a suitable alternative. The CIFAR-10 point is noted, but the authors explain the difficulty, which is reasonable.\n\nTherefore, considering the improvements made and the overall contributions, the paper should be accepted, provided the revisions are properly incorporated. The reviewers' positive assessments of the method's novelty and the addressed concerns tip the balance towards acceptance.\n</think>\n\nFinal Decision: Accept\n\nThe paper presents a novel framework (PEARL) for differentially private data synthesis, addressing key limitations of existing gradient sanitization methods. The approach is theoretically grounded with privacy guarantees and shows empirical improvements over baselines. While initial reviews highlighted concerns about benchmarking, technical clarity, and missing references, the authors provided thorough responses addressing these points:\n\n1. **Benchmarking**: Added datasets (e.g., Credit) and baselines (DPCGAN), and incorporated additional evaluation tasks (marginal release, range query, MMD). Though PATE-GAN was not included due to replication challenges, the authors justified their choices and provided alternatives.\n\n2. **Technical Clarity**: Clarified the one-shot privacy mechanism, sensitivity calculations, and computational complexity in the appendix. Addressed reviewer confusion about auxiliary information and privacy budget allocation.\n\n3. **Related Work and Editorial Quality**: Added missing references (e.g., PrivSyn, DPT) and improved descriptions to enhance precision.\n\n4. **Empirical Rigor**: Demonstrated performance saturation at moderate epsilon values and explained limitations, showing robustness across datasets and tasks.\n\nWhile minor issues like CIFAR-10 evaluation remain unresolved, the authors reasonably cited the lack of existing DP methods for complex image synthesis. The revised experiments and clarifications sufficiently strengthen the paper’s contributions, aligning it with top-tier conference standards. The novel technical approach, theoretical guarantees, and empirical validation justify acceptance."
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper on Variational Neural Cellular Automata (VNCA) for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title and abstract. The VNCA is inspired by biological cellular processes, aiming to create a generative model with self-organization. The key points are that it's a probabilistic model, uses local communication, and shows some robustness to perturbations. The experiments show it can generate diverse outputs but lags behind state-of-the-art in performance.\n\nLooking at the reviewer comments, there's a mix of opinions. Reviewer 1 and 4 are leaning towards rejection due to underwhelming results, especially on datasets like CelebA. They mention the need to average samples to get decent results and the lack of significant improvement over existing methods. Reviewer 2 points out that some ideas, like weight-shared ResNets, aren't novel, but sees value in connecting NCA with prior work. Reviewer 3 and 5 were initially positive and raised their scores after the rebuttal, especially with added experiments on damage recovery and latent space analysis. Reviewer 6 is critical about the technical novelty, arguing that the architecture isn't new but just a different interpretation. Reviewer 7 thinks it's an incremental improvement but sees potential in specific areas like damage resilience.\n\nThe author responses addressed several points. They added new experiments on damage recovery, clarified the architecture's differences from ResNets, and provided more details on hyperparameters and latent space analysis. They also fixed some minor issues like citations and terminology.\n\nNow, considering the standards of a top-tier conference, the key factors are technical novelty, significance, empirical results, and clarity. The VNCA introduces a generative model based on NCA, which is a novel approach in the context of VAEs. The authors highlight the self-organizing aspect and robustness, which are interesting properties. However, the empirical results are indeed underwhelming compared to SOTA, especially in likelihoods and sample quality on complex datasets. \n\nBut the rebuttal added some important points. The damage recovery experiments show a unique capability that might not be achievable with standard VAEs. The latent space analysis also provides some evidence of structured representations. The authors clarified that their main contribution is showing that NCAs can model complex data distributions and exhibit resilience, which is a step forward in applying NCA principles to generative modeling. \n\nReviewers 3 and 5, who are positive, emphasize the value in exploring NCA for generative tasks, even if not SOTA. The paper's honesty about shortcomings and thorough evaluation are praised. The technical novelty is somewhat marginal according to some, but the application and demonstration of NCA's potential in generative models might be considered a meaningful contribution.\n\nOn the other hand, the lack of significant performance gains and the existing prior work (like ResNets with shared weights) could weaken the novelty. However, the authors argue that their work is about the generative process and biological inspiration, not just the architecture. The damage experiments, while added post-rebuttal, do show a unique angle that other models might not handle as well.\n\nConsidering all this, the paper's main contributions are in exploring a new direction with NCA-based generative models, providing a thorough evaluation (including weaknesses), and demonstrating specific properties like robustness. Even if it's not SOTA, the novelty and the insights into self-organizing systems might be sufficient for a top conference, especially given the positive feedback from some reviewers and the improvements made in the rebuttal. The negative reviews focus on empirical results and novelty, but the field's progress sometimes requires such exploratory work that sets a baseline for future research.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\n1. **Technical Novelty and Significance**: \n   - While some reviewers (e.g., Reviewer 2 and 6) argue that the core architecture (weight-shared residual blocks) is not novel, the authors clarify that their key contribution is demonstrating how NCAs can model complex data distributions through morphogenesis and exhibit inherent robustness to perturbations. This application of NCA principles to generative modeling represents a meaningful step forward in bridging biological inspiration with machine learning, even if the architecture itself is not entirely new. The damage recovery experiments (added post-rebuttal) further highlight unique capabilities of the VNCA that distinguish it from standard VAEs and ResNets.\n\n2. **Empirical Results and Evaluation**:\n   - The VNCA underperforms state-of-the-art methods in likelihood and sample quality on datasets like CelebA, but the authors transparently acknowledge these limitations. The model’s ability to reconstruct and generate diverse outputs from a latent vector, despite its simplicity and local communication constraints, is a non-trivial achievement. The added experiments on damage resilience (Section 3.3) and latent space analysis (e.g., improved clustering and interpolations) provide empirical evidence of its unique properties, which are valuable contributions to the field.\n\n3. **Clarity and Reproducibility**:\n   - The paper is well-written and reproducible, with clear explanations of the architecture and experiments. The authors addressed reviewer concerns by adding hyperparameter details, clarifying architectural choices, and releasing code (as stated in the rebuttal). These improvements strengthen the paper’s rigor and accessibility.\n\n4. **Broader Impact and Community Value**:\n   - The work addresses an under-explored area in generative modeling: self-organizing systems inspired by biological processes. Even if the VNCA is not immediately practical for high-fidelity generation, it opens avenues for future research into robust, scalable, and biologically plausible generative models. The honest evaluation and clear documentation of shortcomings reduce publication bias and provide a useful baseline for the community.\n\n5. **Reviewer Consensus**:\n   - While some reviewers (e.g., Reviewers 1, 4, and 6) lean toward rejection due to empirical limitations, others (Reviewers 3, 5, and 7) argue for acceptance, citing the paper’s novelty, clarity, and potential impact. The rebuttal effectively addressed key criticisms (e.g., damage experiments, latent space analysis), leading to score upgrades from previously neutral or negative reviewers. The balance of feedback, particularly the constructive improvements made by the authors, tips the decision toward acceptance.\n\n### Conclusion:\nThe VNCA paper makes a valuable contribution to the exploration of self-organizing generative models, even with its empirical shortcomings. Its novel application of NCA principles, transparent evaluation, and unique properties (e.g., damage resilience) justify acceptance at a top-tier conference, where such foundational work is essential for advancing the field."
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about Behavior Proximal Policy Optimization (BPPO) for offline reinforcement learning. The main idea is that online on-policy algorithms like PPO can naturally handle offline RL due to their inherent conservatism, which helps avoid overestimation issues common in off-policy methods. The authors propose BPPO, which modifies PPO by changing the importance sampling ratios and using a clipping decay, achieving better results on D4RL benchmarks.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 points out some clarity issues in the writing and questions the soundness of the method, especially regarding the theoretical guarantees and experiments. They also note that the comparison with Onestep BPPO isn't convincing and that some claims aren't empirically verified. However, they acknowledge the theoretical contributions and the empirical results.\n\nReviewer 2 mentions that parts of the theoretical results aren't novel, referencing prior work like GePPO and another TRPO method. They also question the soundness of replacing the advantage estimates and note issues with Assumption 1. They suggest adding more baselines and hyperparameter analysis. Despite these points, they think the paper is below acceptance but might be a 4 if possible.\n\nReviewer 3 is more positive, finding the theoretical analysis and experiments sound but asks about advantage estimation accuracy and some algorithm details. They give a 3 on correctness but see some minor issues.\n\nReviewer 4 is critical about the empirical results not being compelling enough, pointing out that improvements on D4RL are marginal and that simpler methods like TD3+BC perform similarly. They also question the practical significance and the novelty compared to existing work. After the rebuttal, they moved to a weak accept but with low confidence.\n\nReviewer 5 raises concerns about Assumption 1, the replacement of the advantage, and convergence guarantees. They also note that the empirical improvements are minor but acknowledge the simplicity of the method.\n\nThe authors addressed many points in their responses. They revised Assumption 1 to use total variational divergence, added experiments on Antmaze, and included more baselines. They clarified the advantage replacement rationale and provided more ablation studies. They also emphasized the superior performance on harder tasks like Adroit and Kitchen.\n\nNow, considering the criteria for a top-tier conference: the paper's contributions need to be novel, technically sound, and empirically significant. The theoretical analysis, while building on prior work (like GePPO), introduces a new perspective on offline monotonic policy improvement. The empirical results show significant improvements on certain challenging benchmarks (Adroit, Kitchen, Antmaze), which is a strong point. However, the D4RL Gym results are mixed, and some reviewers felt the novelty was marginal due to existing similar approaches.\n\nThe authors addressed many of the technical concerns, especially around assumptions and hyperparameters. The rebuttal provided additional experiments that strengthen the empirical case, particularly on sparse reward tasks. The theoretical contributions, while not entirely novel, do offer a new angle and practical improvements.\n\nThe main remaining concerns are about the novelty and the practical significance. Since the method is simpler and performs better on key tasks, and the authors have addressed many of the methodological issues, I think the paper meets the bar for acceptance. The improvements on harder benchmarks and the theoretical insights, even if building on prior work, are sufficient for a top conference.\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is about Online Knowledge Distillation (OKD) using a specialist ensemble. The main contributions seem to be introducing label prior shifts to create diverse teachers and a post-compensation aggregation method. The reviewers have mixed opinions, so I need to weigh their points.\n\nFirst, looking at the abstract and title, the key points are diversity in teachers via label shifts and a new aggregation method. The authors claim their approach achieves better diversity and calibration, which are important in KD.\n\nNow, the reviewer comments. Reviewer 1 points out that the setup is specialized (online and peer networks), and the baselines aren't well-explained. They also mention that the method's novelty is limited, combining existing techniques without clear justification. The ablation studies are good but need more on individual components like PC-Softmax and importance sampling. The suggestion to weight teachers based on their labels isn't addressed, which could be a weakness.\n\nReviewer 2 is critical, noting that the results for MVCL aren't consistent with the original and that the paper's novelty isn't strong enough. They also mention that the method is too simple compared to recent works, which is a concern for a top conference.\n\nReviewer 3 is more positive, appreciating the framework and thorough experiments but notes some typos. They agree on limited novelty but think the combination is novel for online KD. Their main issue is minor typos, which the authors addressed in the response.\n\nReviewer 4 wants more comparisons with recent methods like CGL, PCL, and L-MCL. The authors provided additional experiments in their response, showing improvements over these, but the original submission might have lacked these. The error rate vs accuracy reporting is a minor point but shows attention to detail.\n\nReviewer 5 is concerned about experimental results not being convincing, especially since some methods outperform the proposed one. They also question the novelty, pointing out that label prior shifts and importance sampling aren't new. The authors addressed this by adding comparisons and clarifying their approach's uniqueness in the context of online KD.\n\nLooking at the author responses, they addressed several points: added experiments with PCL and others, clarified the use of importance sampling, and fixed typos. They also explained that their method's novelty lies in applying these techniques specifically to online KD, which previous works didn't do. The results in the response show improvements over baselines, which is positive.\n\nHowever, some issues remain. Reviewer 2's point about MVCL's inconsistency and the method's simplicity is a red flag. Also, the technical novelty is rated low by most reviewers (scores 2 out of 4). The empirical results, while improved in the response, might not be strong enough if the baselines are not properly addressed in the original submission. The fact that the authors had to add experiments post-review suggests the initial submission was lacking in thorough comparisons.\n\nTop-tier conferences require significant novelty and strong empirical validation. The paper's main contribution is combining existing techniques in a novel way for online KD, but the reviewers' consensus on marginal novelty is concerning. The experiments, after revisions, show improvements, but if the original submission didn't have these, it might not meet the conference's standards without revisions.\n\nThe authors did address many points, but the key issues—novelty and thorough baseline comparisons—were only partially resolved. Since the conference requires high standards, and the technical novelty is deemed marginal, I'm leaning towards rejection unless the improvements are substantial enough. However, the revised experiments do show better performance, and the authors clarified their approach's uniqueness. But since the reviewers (especially 2 and 5) still have doubts about novelty and results, maybe the paper is on the borderline.\n\nWait, the final decision needs to be based on the current information. The author responses added experiments that improve the results, but the reviewers' scores for technical and empirical significance are mostly 2 (marginal). The main strength is the framework's combination for online KD, but if that's not seen as novel enough, it might not pass. The fact that the authors had to do extra experiments to show competitiveness suggests the original submission was weak in that area. Since top conferences require strong contributions, and the novelty is considered limited, I think the decision should be to reject unless there's a strong argument for acceptance. However, the revised experiments might tip it, but the reviewers' scores are still low. The final decision is tricky, but given the reviewers' scores and the marginal novelty, I think it's more likely to reject.\n</think>\n\nFinal Decision: Reject\n\n### Detailed Reasoning:\n1. **Novelty and Significance**: \n   - Multiple reviewers (1, 2, 4, 5) highlight that the technical and empirical contributions are only marginally novel. While the authors combine existing techniques (label prior shift, importance sampling, PC-Softmax) in the context of online knowledge distillation (OKD), the reviewers question whether this constitutes a significant advancement. The authors argue that their approach is novel for OKD, but the reviewers remain unconvinced, noting that similar ideas exist in prior work (e.g., MoE, importance sampling) and that the method could be applied to offline settings without clear justification for its OKD-specific benefits.\n\n2. **Empirical Validation**:\n   - The original submission lacked thorough comparisons with recent baselines (e.g., CGL, PCL, L-MCL). While the authors provided additional experiments in their response, these were not part of the initial submission. Top-tier conferences require strong, self-contained empirical results, and the delayed inclusion of critical comparisons undermines the paper’s credibility. Additionally, the improvements over baselines (e.g., 0.05% over PCL on ImageNet) are marginal, and some baselines (e.g., MVCL) still outperform the proposed method in certain metrics (e.g., CIFAR-100 calibration).\n\n3. **Clarity and Reproducibility**:\n   - While the paper is generally well-written, Reviewer 3 noted distracting typos, and Reviewer 1 criticized the lack of baseline explanations. The authors addressed these in their response, but the initial submission’s clarity was suboptimal. The use of importance sampling and PC-Softmax could also benefit from clearer motivation (e.g., why these choices over alternatives?).\n\n4. **Reviewer Concerns**:\n   - Reviewer 2 explicitly questioned the method’s simplicity and reproducibility of results for MVCL, suggesting the paper is not yet ready for a top-tier venue. Reviewer 5’s low correctness score (1) further highlights concerns about the empirical claims’ robustness.\n   - The authors’ response addressed many points, but the reviewers’ scores for technical/empirical significance (mostly 2/4) indicate that the contributions are not transformative enough for a top conference.\n\n5. **Broader Impact**:\n   - The method’s specialization to OKD and peer networks limits its general applicability, as noted by Reviewer 1. While the authors suggest extensions to offline settings, these are speculative and not empirically validated in the submission.\n\n### Conclusion:\nWhile the paper presents a coherent framework and shows incremental improvements over some baselines, the reviewers’ consensus on marginal novelty and insufficient empirical validation (especially in the original submission) makes it challenging to justify acceptance at a top-tier conference. The revised experiments and clarifications strengthen the paper but do not sufficiently address the core concerns about novelty and baseline comparisons. Thus, the paper falls short of the high bar for acceptance."
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about combining Transformers with MCTS for offline RL, specifically on SameGame. The reviewers have some mixed feedback. Let me start by going through each reviewer's comments carefully.\n\nFirst, Reviewer 1 points out that the idea is novel and promising but the execution and clarity are lacking. The experiments are only on SameGame, which is considered a toy problem, and they didn't compare against Decision Transformers or other relevant baselines like AlphaZero. The writing is unclear, figures are hard to read, and there are technical ambiguities. They gave a 2/4 on correctness and significance, suggesting it's not ready yet but has potential.\n\nReviewer 2 is more critical about novelty. They argue that the method isn't new because it's just replacing the neural network in AlphaZero with a Transformer. The experiments are unfair because the MCTS baseline might not be using a learned policy, making the comparison skewed. They also mention that the setup is similar to existing work, so the contributions aren't significant. Their scores are lower, especially on technical and empirical novelty, giving a 1/4 in some areas.\n\nReviewer 3 also questions the novelty, saying it's a combination of existing methods without enough innovation. The experiments are insufficient because they don't use standard benchmarks or compare against prior SameGame works. They also note that the paper lacks comparisons with other offline RL methods and doesn't justify why a Transformer is better than other architectures. Clarity issues are present but manageable, and reproducibility is possible with effort. Their scores are mid-range but still leaning towards rejection.\n\nThe authors didn't respond, which might mean they couldn't address the concerns. Now, considering top-tier conference standards, the key points are novelty, empirical validation, and clarity.\n\nThe main issues here are the lack of novelty. Reviewer 2 and 3 both say that replacing a ResNet with a Transformer in an AlphaZero-like setup isn't novel enough. The problem is that the method doesn't present a new algorithmic contribution beyond that substitution. The experiments are on a single, less standard domain (SameGame) without comparing to strong baselines like AlphaZero or other MCTS+NN methods. This makes it hard to assess real-world impact or superiority.\n\nClarity is another problem. Reviewer 1 mentions the paper is hard to follow, figures are unclear, and there are typos. While Reviewer 3 says it's easy to follow despite issues, the technical explanation isn't thorough enough, especially in how the Transformer interacts with MCTS.\n\nThe empirical evaluation is weak. They didn't use standard benchmarks like Atari or MiniGo, which are common in RL research. Comparing only to a basic MCTS and a Transformer without a learned prior might not be fair, as pointed out. Also, not testing against prior SameGame methods means we don't know if they're improving on existing work there.\n\nThe paper's claims might not be well-supported. If the MCTS baseline isn't using a learned policy, then outperforming it isn't impressive. The authors need to show they beat methods that do use learned components. Without that, the results don't demonstrate the method's strength.\n\nConsidering all this, even though the idea has potential, the execution falls short in novelty, empirical rigor, and clarity. Top conferences require strong contributions and thorough evaluations. Since the reviewers agree the work is incremental and the experiments are insufficient, it's probably not ready for acceptance. The authors would need to address these points significantly, but without an author response, we can't assume they've done so.\n</think>\n\nThe paper proposes MCTransformer, a framework combining Transformers with MCTS for offline reinforcement learning, evaluated on the SameGame domain. While the core idea of integrating Transformers into MCTS shows potential, the paper faces significant shortcomings that undermine its suitability for a top-tier conference:\n\n### Key Issues:\n1. **Novelty and Originality**:\n   - **Weak Novelty**: Reviewers 2 and 3 argue that the method is largely a straightforward substitution of a Transformer for the neural network in existing MCTS frameworks (e.g., AlphaZero/AlphaGo). This lacks sufficient innovation, as the architecture choice (Transformer vs. ResNet) alone does not constitute a novel contribution to RL or MCTS methodology.\n   - **Lack of Distinction**: The approach does not introduce new algorithmic components or address unresolved challenges in MCTS or offline RL. It instead repurposes existing techniques without clear theoretical or empirical justification for why Transformers improve performance in this context.\n\n2. **Empirical Evaluation**:\n   - **Limited Domain**: The evaluation is confined to SameGame, a single, relatively simple and niche domain. This restricts generalizability and makes it difficult to assess the method’s broader impact. Comparisons to standard benchmarks like Atari or MiniGo are absent, weakening credibility.\n   - **Unfair Baselines**: The MCTS baseline appears underpowered (e.g., lacking a learned prior policy or value function), making comparisons to MCTransformer misleading. Failing to benchmark against Decision Transformers or AlphaZero-like methods (which combine MCTS with neural networks) invalidates claims of superiority over prior work.\n   - **Missing Comparisons**: The paper does not compare against prior SameGame-specific methods (e.g., those cited by Reviewer 3) or alternative offline RL approaches (e.g., imitation learning). This undermines the significance of the results.\n   - **Rollout Policy Ambiguity**: The MCTS rollout policy’s design (e.g., random vs. heuristic vs. learned) is unclear, making it impossible to evaluate the Transformer’s role in improving exploration.\n\n3. **Clarity and Presentation**:\n   - **Poor Exposition**: Reviewer 1 highlights significant clarity issues, including an unclear figure (e.g., undefined terms like $rtg_\\pi(s_t)$) and insufficient methodological details. The paper’s writing style and technical explanations require substantial revision.\n   - **Typos and Grammar**: Multiple grammatical errors and typos (e.g., \"MTCS\" misspellings) further hinder readability.\n\n4. **Reproducibility**:\n   - While the method is conceptually reproducible (due to its similarity to AlphaZero), the lack of code and detailed experimental setup (e.g., hyperparameters, training data specifics) complicates direct replication.\n\n### Strengths:\n- The idea of integrating Transformers into MCTS is conceptually interesting and could inspire future work.\n- The paper demonstrates improved performance over a basic MCTS and Transformer baseline in SameGame, suggesting potential for further exploration.\n\n### Conclusion:\nDespite its promising premise, the paper fails to meet the standards of a top-tier conference due to **insufficient novelty**, **weak empirical validation**, and **poor presentation**. The method’s incremental nature, limited experimental scope, and lack of rigorous comparisons to state-of-the-art baselines undermine its significance. While the core idea has potential, the current submission does not provide compelling evidence of its superiority or broader impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new contrastive learning objective to reduce text degeneration in language models. The reviewers have mixed but mostly critical feedback. Let me go through each point step by step.\n\nFirst, the title and abstract mention that their method improves on unlikelihood training (UL) by considering the relationship between label and negative tokens. But Reviewer 1 points out that UL actually does consider both positive and negative tokens, and the authors misrepresented UL's equation. That's a problem because it undermines the novelty claim. If the core idea isn't actually new, that's a big issue for a top conference.\n\nLooking at the experiments, Reviewer 1 and 2 both say the experiments are insufficient. The paper uses GPT2-small and wikitext-103, which might be outdated. Reviewer 2 mentions that larger models like bigger GPT2 versions weren't tested, and the metrics used (like perplexity) might not be reliable indicators. They also note that the paper doesn't report standard metrics like dist-2 or dist-3, which are commonly used. That makes the results less convincing.\n\nReviewer 3 and 4 also highlight the lack of novelty. Reviewer 3 says the idea is similar to UL with minor differences, and Reviewer 4 points out that human evaluations didn't show statistical significance over UL-TS. The authors didn't address these points since there's no response, so we can't assume they fixed anything.\n\nTechnical novelty is a big factor here. If the method is just a tweak on existing work without clear improvement, it's incremental. The reviewers agree that the novelty is thin. Even though the experiments show some gains, the setup is too limited. For a top-tier conference, the bar is high, and the experiments need to be robust and comprehensive.\n\nAlso, the fact that the human evaluation didn't show a significant preference over UL-TS is a red flag. The paper claims state-of-the-art but the evidence isn't strong enough. Plus, the reviewers pointed out flaws in how they compared to UL, which might mean their improvements aren't as substantial as claimed.\n\nConsidering all this, the main issues are insufficient novelty and inadequate experiments. The method doesn't push the field forward enough, and the evaluation doesn't convincingly demonstrate superiority over existing methods, especially in more challenging settings. Without a strong author response to address these points, it's hard to justify acceptance at a top conference like ICLR.\n</think>\n\nThe paper proposes a contrastive token (CT) learning objective to address text degeneration in autoregressive language models by penalizing previously generated tokens. While the idea is intuitive and the paper is well-written, several critical issues undermine its suitability for a top-tier conference:\n\n### Key Concerns:\n1. **Novelty and Originality**:\n   - Reviewers consistently highlight **thin technical novelty**. The core idea of penalizing negative tokens is derivative of unlikelihood training (UL), and the authors misrepresented UL's formulation (e.g., omitting UL's consideration of both positive and negative tokens in Equation 4). This weakens the claim that CT meaningfully improves upon UL.\n   - Reviewer 3 notes that CT’s distinction from UL (e.g., handling irrelevant tokens differently) is minor and insufficient for a novel contribution at this level.\n\n2. **Experimental Rigor**:\n   - **Limited scope**: Experiments focus on small models (GPT2-small) and outdated benchmarks (Wikitext-103). Larger models (e.g., GPT-2-large) and modern tasks (e.g., abstractive summarization, machine translation) are not tested, despite being mentioned in the abstract. This raises questions about generalizability.\n   - **Flawed metrics**: The reliance on perplexity and dist-1/uniq-1 metrics is problematic. Prior work (e.g., Holtzman et al., 2019) shows that low perplexity can correlate with degenerate outputs, and standard diversity metrics (dist-2/3/4) are omitted. Human evaluations lack statistical significance compared to UL-TS.\n   - **Incomplete baselines**: Key decoding methods like typical sampling and ancestral sampling are missing, weakening comparisons to existing techniques.\n\n3. **Technical Soundness**:\n   - The human evaluation (Table 3) does not statistically outperform UL-TS, contradicting claims of superior fluency and reduced repetition. This inconsistency undermines the paper’s empirical significance.\n   - The choice of beam search over sampling methods is poorly justified, given that beam search produces a single output, potentially biasing results.\n\n### Strengths:\n- The method is simple and reproducible, with code provided.\n- The contrastive learning framing offers a fresh perspective on cross-entropy limitations.\n\n### Conclusion:\nWhile the paper addresses an important problem and contributes a clear, implementable idea, its **lack of novelty**, **insufficient experimental validation**, and **failure to convincingly outperform strong baselines** (especially UL) prevent it from meeting the high bar of a top-tier conference. The reviewers’ consensus on marginal technical and empirical significance, combined with unresolved methodological flaws, further supports rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The user wants me to evaluate whether this paper should be accepted or rejected for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Detecting Out-of-Distribution Data with Semi-supervised Graph “Feature” Networks\". The abstract mentions using graph structures and topological properties to detect OOD data, achieving high AUROC scores on LSUN. The keywords are missing, which might be an oversight, but maybe not critical.\n\nLooking at the reviewer comments, there are four reviewers. Let me go through each one's points.\n\nReviewer 1 points out several weaknesses. The paper uses vague terms like \"common sense\" without clear definitions. The connection between the summary and the actual implementation isn't clear. There's a lack of rigorous validation against state-of-the-art methods, just citing another paper's results. Also, the impact of object detection accuracy on the model isn't analyzed. Clarity issues include grammatical errors and unclear figures. The reviewer suggests more detailed explanations, better experiments, and references. The correctness is rated 3 (minor issues), technical novelty 3 (somewhat new), empirical 2 (marginal).\n\nReviewer 2 is harsher. They say the paper is not well-prepared, with confused notations (Near/Far OOD differing between intro and Table 1). The motivation and method aren't clearly described, so novelty can't be judged. They also mention ignoring the literature, like the survey by Yang et al. Empirical results are not convincing because they don't compare to state-of-the-art OOD methods but instead graph embedding techniques. Only LSUN dataset used, which isn't extensive. Clarity is poor, reproducibility is an issue. Correctness is 1 (main claims not supported), technical and empirical novelty both 1 (not novel or significant). This reviewer recommends a strong reject.\n\nReviewer 3 thinks the idea is interesting but lacks novelty since it combines existing methods. They note the paper didn't compare with state-of-the-art, lacks details on the object detection network's architecture, and ablation studies. Reproducibility is okay but analysis is lacking. Correctness 2 (some claims incorrect), novelty 2 (marginal). Summary suggests not novel enough and experiments not persuasive.\n\nReviewer 4 acknowledges the novel idea of using semantic graphs for interpretable OOD detection but points out critical flaws. The method relies on a pre-trained object detector which might not cover all in-distribution data (like medical images), making the semantic graph incomplete. Experiments are insufficient: no comparison with key OOD methods like Lee et al. or Sehwag et al., no tests on other datasets like CIFAR or ImageNet, no ablation studies, and no hyperparameter analysis. Writing issues include grammatical errors, missing explanations, and poor figure descriptions. Related work is poorly discussed, missing important OOD detection approaches. Correctness 3, technical novelty 3, empirical 2. Overall, the paper is \"far from ready\".\n\nAuthor response is missing, so no rebuttal to address these points.\n\nNow, considering top-tier conference standards, which require high novelty, rigorous methodology, clear writing, and thorough experiments. The reviewers' consensus seems to be that the paper has some interesting ideas but major flaws in execution and presentation.\n\nKey issues:\n1. **Lack of comparison with state-of-the-art**: All reviewers except maybe the first note that the paper doesn't compare against current SOTA OOD methods. They compared graph embeddings but not the actual OOD detection methods that are state-of-the-art. This is a big red flag because without knowing how it stacks up against existing methods, the contribution is hard to assess.\n\n2. **Experimental insufficiency**: Only using LSUN dataset, no ablation studies, no analysis of key parameters. The experiments are not extensive enough to validate the claims, especially for a top conference.\n\n3. **Clarity and writing issues**: Multiple reviewers mention poor writing, unclear figures, grammatical errors, and lack of detail in methodology. For reproducibility, the paper doesn't explain the pipeline clearly enough. This is problematic because top conferences require clear exposition.\n\n4. **Novelty concerns**: While the approach of using graphs/semantic structures is noted as somewhat novel by some, others (like Reviewer 2 and 3) argue it's just combining existing methods without enough innovation. The technical novelty is marginal, and the empirical results don't show a significant leap over existing methods.\n\n5. **Critical assumptions**: The reliance on a pre-trained object detector's accuracy is a flaw. If the detector fails on OOD data, the graph might be flawed, but the authors didn't analyze this. This weakens the robustness claims.\n\n6. **Literature review**: Ignoring key prior work in OOD detection (as pointed out by Reviewer 2 referencing the survey) shows a lack of thorough literature engagement, which is expected at top venues.\n\nConsidering all this, even though the idea has potential (as noted by some reviewers), the execution and presentation are too weak. The lack of proper comparisons and insufficient experiments mean the claims aren't well-supported. The writing issues make it hard to assess the method's validity. For a top conference, these shortcomings are likely deal-breakers. The reviewers' scores, especially Reviewer 2's strong reject, and the consensus on needing major revisions that might not be feasible in the rebuttal period, lead me to think rejection is the right call.\n</think>\n\nThe paper proposes a method for OOD detection using graph structures derived from object detection outputs, achieving high AUROC scores on the LSUN dataset. However, multiple critical issues undermine its suitability for a top-tier conference:\n\n1. **Lack of Rigorous Comparisons**: The paper fails to compare with state-of-the-art OOD detection methods (e.g., Lee et al., Sehwag et al.), instead comparing only to graph embedding techniques. This weakens claims of competitiveness and novelty.\n\n2. **Insufficient Experiments**: \n   - Limited to the LSUN dataset, with no results on standard benchmarks like CIFAR or ImageNet.\n   - No ablation studies, hyperparameter analysis, or interpretability analysis of the semantic graphs.\n   - No evaluation of the method’s sensitivity to object detection inaccuracies, a critical assumption.\n\n3. **Clarity and Reproducibility Issues**: \n   - Poor writing, grammatical errors, and unclear descriptions of the feature extraction pipeline and figures (e.g., Figure 1 lacks explanatory details).\n   - Inconsistent notation (e.g., \"Near/Far OOD\" definitions differ between sections).\n   - Missing formal descriptions of the inference pipeline and insufficient details for reproducibility.\n\n4. **Novelty Concerns**: \n   - The method combines existing techniques (object detection, graph embeddings, OOD classifiers) without sufficient innovation. Reviewers note marginal technical and empirical significance.\n   - The \"novel\" idea of semantic graphs for OOD detection is not clearly distinguished from prior work, and the literature review is inadequate.\n\n5. **Unrealistic Assumptions**: The reliance on a pre-trained object detection model that must already capture in-distribution semantics is problematic, especially for domains (e.g., medical imaging) where such models may not exist.\n\nWhile the core idea of leveraging semantic graphs for OOD detection is intriguing, the paper’s execution falls short of top-tier standards. Major flaws in experimental rigor, clarity, and comparison to relevant work, coupled with unresolved methodological weaknesses, render the current submission unpublishable without substantial revisions. Given the reviewers’ consensus (including a strong reject from Reviewer 2) and the lack of an author response to address these issues, the paper does not meet the bar for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nThe paper is about the convergence analysis of gradient descent (GD) for deep linear neural networks. The main contributions seem to be sharp convergence rates that don't depend on initialization type and show depth independence under certain width conditions. The reviewers have mixed opinions, so I need to weigh their points.\n\nFirst, looking at the abstract and title, the paper addresses a significant problem in understanding why GD works well for deep networks. The key points are sharp rates, independence from initialization, and depth not affecting convergence when layers are wide enough. \n\nNow, the reviewer comments. Reviewer 1 mentions concerns about novelty. They think extending the loss function to general L(W) might not be that challenging once a semi-norm is introduced. Also, the comparison with Du & Hu's work on the operator P(t) might not be fair because the X terms are handled differently. The reviewer suggests toning down the novelty claims here. However, they acknowledge the second extension (initiation method) as novel. \n\nReviewer 2 is positive and recommends acceptance after the update. Reviewer 3 points out that the main theorems (B.1, B.2) might just be extensions of existing work with similar techniques, especially for Gaussian and orthogonal initializations. They note that the proof follows prior work closely and question if the contributions are limited. However, they do find Theorem 3.3 (trajectory similarity to convex problems) novel and valuable. They want more details on technical challenges overcome. \n\nReviewer 4 is positive, calling it a solid theoretical work but noting the analysis is local (needs initial loss not too far from minimum) and lacks intuitive explanations. They still recommend acceptance. \n\nReviewer 5 is critical about novelty, saying the extension to general losses is standard in convex settings and the trajectory results follow from strong convexity. They rate technical novelty lower. \n\nReviewer 6 wants more discussion on overparameterization's role, dependencies of constants, and mentions overclaims in the title/abstract. They initially recommend rejection but suggest acceptance if revised. \n\nThe author responses address several points. They added Section 4 to clarify insights, discussed overparameterization's necessity, and addressed reviewer concerns about the title and abstract overclaims. They explain that extending to general losses required handling low-rank X and using semi-norms, which is non-trivial. They also mention the trajectory analysis as novel, showing GD paths align with convex problems. \n\nNow, considering all this: The paper's main contributions are the sharp rates and the trajectory result. The trajectory part (Theorem 3.3) seems novel and valuable, as multiple reviewers note. However, the core convergence theorems (B.1, B.2) might be extensions of prior work, but the authors argue they handle general losses and layer widths, which required new techniques like semi-norms and beta distribution analysis. \n\nThe authors addressed several concerns: clarified overclaims in title/abstract, added discussions on overparameterization, and explained technical challenges. Reviewers 2,4,5 (some reservations) and 6 (after revisions) lean towards acceptance. Reviewer 3's main issue was novelty but the authors provided explanations. \n\nThe key is whether the contributions are sufficient for a top conference. The trajectory result is novel and insightful. The extensions to general losses and initialization methods, even if building on prior work, add value. The authors have strengthened the paper with clarifications and new sections. \n\nThe weaknesses include some overlap with existing techniques and local analysis, but the paper's clarity and the novel insights in Section 4 help. The rebuttal effectively addressed many concerns, especially around novelty and overclaims. \n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper's main contribution is exploring the effect of swapping the order of Batch Normalization (BN) and bounded activation functions like Tanh. They found that putting BN after the activation (Swap model) leads to better performance compared to the conventional order when using bounded activations. They attribute this to asymmetric saturation and increased sparsity. \n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 and 2 point out some weaknesses in the analysis and clarity. Reviewer 3 mentions the findings are interesting but the practical impact is limited since the Swap models with bounded activations don't outperform ReLU. Reviewer 4 thinks the approach isn't novel enough, and Reviewer 5 has concerns about the rigor of claims and experimental setup.\n\nThe authors addressed several points in their response. They clarified some definitions, provided additional experiments (like using a modified VGG16_11 to address layer-wise saturation issues), and explained the gradient behavior. They also added figures and metrics as requested. \n\nKey strengths: The paper presents an interesting observation about the Swap order with bounded activations, which hasn't been thoroughly explored. The analysis linking asymmetric saturation and sparsity is novel and provides new insights. The experiments are detailed, and the authors made efforts to address reviewer concerns.\n\nWeaknesses: The practical significance is limited because the Swap models still underperform ReLU-based networks. The analysis could be more comprehensive, especially regarding deeper layers and residual connections. Some claims about saturation and sparsity need better theoretical grounding. The paper's novelty is questioned, and the writing had clarity issues initially.\n\nThe authors addressed many of the technical concerns, like providing more figures and clarifying terms. They also validated their claims with additional experiments. However, the core issue that their method doesn't surpass ReLU remains, which might limit its impact. But top-tier conferences do accept papers with niche contributions if the insights are strong enough.\n\nConsidering the reviewers' scores: Most gave correctness a 3 (minor issues) and technical novelty around 3 (somewhat new). The empirical significance was lower, but the topic is relevant. The revised version seems to have strengthened the arguments and addressed many concerns. The paper's contribution, while not earth-shattering, adds valuable knowledge about BN and activation interactions, especially for bounded functions. The authors' responses show they've engaged constructively with feedback.\n\nFinal Decision: Accept, because the novel insights and thorough analysis meet the standards of a top conference, even if the practical impact is limited. The reviewers' concerns were adequately addressed, and the work contributes to understanding neural network components.\n</think>\n\nThe paper investigates the impact of swapping the order of batch normalization (BN) and bounded activation functions, such as Tanh, revealing that placing BN after the activation (\"Swap model\") significantly improves performance compared to the conventional order. The authors attribute this improvement to asymmetric saturation and induced sparsity, supported by empirical analysis and theoretical insights. While the paper has several strengths, including novel observations and clear logic, it also faces criticisms regarding practical limitations (e.g., Swap models still underperform ReLU-based networks), incomplete analysis (e.g., layer-wise saturation inconsistencies), and clarity issues. \n\n**Key Strengths:**\n1. **Novel Observation:** The discovery that the Swap order with bounded activations yields better performance is intriguing and fills a gap in understanding BN-activation interactions.\n2. **Empirical Rigor:** The authors conducted extensive experiments across multiple architectures and activation functions, providing detailed metrics (saturation, skewness, sparsity) to support their claims.\n3. **Constructive Revisions:** The authors addressed reviewer concerns by clarifying definitions, adding figures (e.g., skewness for VGG16_11), and refining experiments (e.g., using a CIFAR-optimized VGG variant).\n4. **Theoretical Insights:** The link between asymmetric saturation, sparsity, and generalization, while requiring further validation, offers a new perspective on network optimization.\n\n**Key Weaknesses:**\n1. **Limited Practical Impact:** The Swap model with bounded activations does not surpass ReLU-based networks, reducing its immediate applicability.\n2. **Incomplete Analysis:** The saturation and skewness in deeper layers were initially inconsistent, but this was addressed by revising the model architecture for CIFAR-100.\n3. **Clarity and Rigor:** Some claims (e.g., the relationship between saturation and sparsity) were initially vague but were clarified in the rebuttal. Terms like \"center of the function\" were ambiguously defined but were resolved.\n4. **Scope Limitations:** The analysis excludes residual networks and broader architectures, limiting generalizability.\n\n**Reviewer Feedback and Responses:**\n- **Reviewer 1:** Addressed by adding figures and clarifying the role of asymmetric saturation.\n- **Reviewer 2:** The authors acknowledged the limited scope but defended the contribution’s novelty and provided additional experiments.\n- **Reviewer 3:** The revised analysis of VGG16_11 and discussion of saturation metrics strengthened the argument, though practical limitations remain.\n- **Reviewer 4:** The authors clarified gradients and sparsity metrics, but the lack of ResNet experiments persists.\n- **Reviewer 5:** The revised definitions and experiments (e.g., gradient analysis) improved rigor, though some claims remain contentious.\n\n**Conclusion:** While the paper has limitations, its novel insights into BN-activation interactions and thorough empirical analysis meet the standards of a top-tier conference. The authors’ revisions adequately addressed critical concerns, and the work contributes meaningfully to understanding neural network design choices, even if its practical impact is niche.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about improving adversarial robustness through a new adversarial training algorithm called ARoW. The main idea is applying more regularization to samples that are less robust. The theoretical contribution is a new upper bound on the robust risk, and empirically, they claim state-of-the-art performance.\n\nLooking at the reviewer comments, there's a mix of positive and critical feedback. Reviewers 2, 6, 12, 14, 15 (after rebuttal) lean towards acceptance, while others like Reviewers 3, 5, 9, 10, 11, 13, 16 have significant concerns. \n\nThe theoretical part is a point of contention. Reviewers 5, 9, 10, 11, 16 argue that Theorem 1 isn't a meaningful contribution. They say it's just a rephrasing of TRADES' bound for multi-class, which isn't tight and the math is unclear. The authors tried to clarify in their response, defining z(x) more rigorously, but some reviewers still find the theorem lacking. However, Reviewers 7 and 12 acknowledge the practical value even if the theory isn't groundbreaking.\n\nOn the empirical side, the improvements over existing methods are small but consistent. Reviewers 8, 13, 15 note that the gains are marginal, especially with extra data. The authors provided additional experiments showing slight edges over HAT when settings are matched, but the differences are still narrow. Some baselines might not be optimally tuned, as pointed out by Reviewer 15. However, the rebuttal addressed some of these by adding more results.\n\nClarity and rigor in the math definitions (like z(x)) were problematic. Reviewers 10-12 had issues with the notation and rigor, but the authors revised the definitions, which might resolve some confusion. The experiments cover standard datasets but not larger ones like ImageNet, which is a minor weakness but not fatal for a top-tier conference.\n\nThe paper's strengths include a clear motivation, solid empirical validation across multiple datasets, and the method's simplicity compared to HAT. The theoretical contribution, while debated, provides a new perspective even if not earth-shattering. The authors addressed several concerns in the rebuttal, like clarifying z(x) and adding experiments with extra data.\n\nConsidering top-tier standards, the paper needs to have significant novelty or substantial empirical impact. The theoretical part is incremental but the empirical results, though marginal, are consistent. The method's practical advantages (like ease of implementation and fairness improvements) add value. The clarity issues were addressed, and the experiments, while not on huge datasets, are standard in the field.\n\nWeighing all factors: The paper has enough empirical merit and a novel angle in regularization, even if the theory is not groundbreaking. The improvements, though small, are consistent and the method offers practical benefits. The theoretical contribution, while debated, is a step forward in adversarial training. The authors have sufficiently addressed concerns in the rebuttal. Therefore, it should be accepted.\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about the order of Batch Normalization (BN) and activation functions, specifically when using bounded activations like Tanh. The main claim is that swapping BN to after the activation (Swap model) leads to better performance compared to the conventional order (BN before activation) when using bounded activations. They attribute this to asymmetric saturation and induced sparsity.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. Reviewer 1 points out that some figures are missing, like the skewness for VGG16_11, which is important for supporting their argument about asymmetric saturation. They also question why sigmoid wasn't considered, even though it can be adjusted to fit the center requirement. The authors addressed this by explaining that sigmoid's center isn't at the origin unless shifted, but they didn't include it in their analysis, which might limit the scope.\n\nReviewer 2 mentions that the connection between asymmetric saturation and generalization isn't strong enough. They note that higher layers have lower saturation but still perform well, which the authors addressed by revising the model to VGG16_11, ensuring all layers have high saturation. However, the reviewer still feels the arguments are weak and the analysis limited, especially since the Swap models with Tanh don't outperform ReLU-based networks. The authors tried to clarify some points but didn't resolve the concern about practical significance.\n\nReviewer 3 thinks the findings are interesting but the analysis isn't comprehensive. They point out that the performance gain doesn't surpass ReLU, making the practical impact low. The authors provided additional experiments with ResNet, showing some improvement, but it's still marginal. The reviewer is on the fence but leans towards rejection unless clarified.\n\nReviewer 4 is concerned about whether the improved performance is due to faster training rather than better generalization. The authors added convergence plots showing Swap models don't just train faster but also generalize better. However, the reviewer still feels the approach isn't novel enough for a top-tier conference.\n\nReviewer 5 highlights several issues, including unclear definitions (like \"center of the function\") and contradictions in the saturation-sparsity relationship. The authors clarified these points, but some concerns remain, like the lack of improvement over ReLU. The reviewer is skeptical about the reproducibility and rigor of the claims.\n\nThe authors addressed many points in their response, like revising the model to VGG16_11, providing code details, and explaining the saturation metrics. However, key issues persist: the Swap models don't beat ReLU, the analysis is limited to certain architectures, and some claims are still not fully supported. The reviewers' scores are mostly in the middle (correctness 2-3, novelty 2-3), indicating the paper has merit but isn't groundbreaking.\n\nConsidering the top-tier conference standards, the paper needs strong novel contributions and solid evidence. While the findings are interesting, the limitations (e.g., not surpassing ReLU, limited applicability without residual connections) and unresolved concerns about the mechanisms (asymmetric saturation's role) might make it borderline. However, the authors did address many critiques, and the topic is relevant. But since the reviewers' overall scores are on the lower side and the practical impact is limited, I think it's a close call but likely should be rejected unless the conference has a spot for borderline papers. Given the reviewers' summaries leaning towards reject or borderline, I'll go with reject.\n</think>\n\nFinal Decision: Reject\n\nThe paper presents an interesting observation regarding the interaction between batch normalization and bounded activation functions when their order is swapped. However, several critical issues remain unresolved despite the authors' efforts to address reviewer concerns. Key weaknesses include:\n\n1. **Limited Practical Impact**: The Swap model with bounded activations (e.g., Tanh) does not outperform ReLU-based networks, which are the current standard. This undermines the paper’s practical significance, as the proposed approach does not offer a clear advantage over existing methods.\n\n2. **Incomplete Analysis**: The authors’ explanation of asymmetric saturation’s role in generalization is not fully convincing. Questions about the relationship between saturation, sparsity, and performance remain unresolved, particularly in deeper layers and across different architectures. While revisions addressed some gaps (e.g., VGG16_11), the analysis still lacks rigor in linking these metrics to generalization.\n\n3. **Scope Limitations**: The findings are restricted to non-residual architectures and bounded activations, excluding widely used models like ResNet. The authors’ experiments on ResNet showed minimal improvements, further limiting the contribution’s relevance.\n\n4. **Ambiguities and Clarifications**: Despite revisions, certain claims (e.g., \"center of the function,\" saturation-sparsity relationship) were not fully clarified, and some reviewer concerns about reproducibility and metric definitions persisted.\n\nWhile the paper contributes an intriguing empirical observation, the unresolved methodological and interpretive issues, coupled with the lack of clear practical benefits over existing techniques, make it insufficient for a top-tier conference. The reviewers’ scores (mostly 2-3 on correctness and novelty) and their summaries (borderline reject or weak accept) align with this conclusion."
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about collaborative adversarial training (CAT) where two models are trained together using different adversarial methods. The main idea is that since different methods have different strengths and weaknesses, combining them can improve robustness. The experiments show SOTA on CIFAR-10 under Auto-Attack.\n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 mentions the method is interesting but notes lack of novelty compared to ensemble methods and missing recent baselines. They also point out typos and the need for theoretical analysis. Reviewer 2 is more critical, saying the idea isn't novel and the experiments aren't strong enough. They also mention the method doubles computational cost, which is a big issue. Reviewer 3 is also negative, pointing out that the experiments are on small datasets and lack comparisons with important baselines. Reviewer 4 thinks it's somewhat novel but wants more ablations and motivation.\n\nThe main strengths are the SOTA results and the straightforward approach. But the weaknesses are significant. The method isn't very novel since ensemble methods exist, and they didn't cite or compare with those. The computational cost is doubled, which is a practical downside. The lack of theoretical backing and thorough experiments (like on larger datasets or more baselines) is a problem. Also, the authors didn't respond, so they couldn't address these points.\n\nTop-tier conferences usually require high novelty, thorough experiments, and strong theoretical grounding. Here, the method's incremental nature and the missing comparisons with existing ensemble techniques are major issues. The SOTA result is good, but if it's just combining existing methods without deeper insight, it might not be enough. The computational cost is a practical concern for acceptance. The reviewers' scores are mostly in the middle to low range, especially on novelty and significance. \n\nSo, considering all this, the paper probably doesn't meet the high bar for a top conference. The contributions are marginal, and there are significant shortcomings in both methodology and experimental validation.\n</think>\n\nThe paper proposes a collaborative adversarial training (CAT) framework that trains two robust models simultaneously using different adversarial methods, leveraging their complementary strengths to improve robustness. While the method achieves state-of-the-art (SOTA) results on CIFAR-10 under Auto-Attack, several critical issues undermine its novelty, empirical rigor, and significance for a top-tier conference:\n\n### Key Strengths:\n1. **Empirical Results**: The method achieves SOTA robustness on CIFAR-10 without additional data, which is a notable practical contribution.\n2. **Conceptual Simplicity**: The idea of combining adversarial training methods through collaboration is straightforward and easy to implement.\n3. **Clear Writing**: The paper is well-structured and accessible.\n\n### Major Weaknesses:\n1. **Lack of Novelty**:\n   - The core idea of combining adversarial training methods is not novel. Reviewers highlighted prior work on ensemble adversarial training (e.g., Tramèr et al., 2018; Pang et al., 2019) that was neither cited nor compared. The authors failed to distinguish their approach from these methods, weakening its originality.\n   - The \"collaborative\" training resembles existing knowledge distillation or ensemble techniques, with minimal theoretical or methodological innovation.\n\n2. **Empirical Shortcomings**:\n   - **Missing Baselines**: The paper does not compare against recent SOTA methods like Adversarial Weight Perturbation (AWP) or other ensemble-based approaches, making it difficult to assess true superiority.\n   - **Limited Datasets**: Experiments are confined to small-scale datasets (CIFAR-10/100). Larger datasets (e.g., ImageNet) and diverse architectures (e.g., VGG, MobileNet) are needed to validate generalizability.\n   - **Incomplete Ablations**: The authors do not explore critical variants, such as using the same method for both models (TRADES-TRADES) or integrating additional attacks/defenses. The lack of analysis on computational efficiency (e.g., trade-offs between accuracy/robustness and resource costs) is also problematic.\n\n3. **Theoretical Gaps**:\n   - No theoretical analysis explains *why* collaborative training improves robustness compared to single-method training or ensembles. The symmetric KL-divergence objective is proposed without justification for its superiority over alternatives.\n   - The motivation hinges on the \"distinct robustness\" of different methods, but no ablation (e.g., comparing models trained with the same method but different seeds) confirms this premise.\n\n4. **Practical Drawbacks**:\n   - **Computational Cost**: Training two models doubles the computational and memory requirements, a significant drawback for real-world deployment. The authors do not address this trade-off or propose mitigations.\n   - **Reproducibility**: Key terms like \"ALP\" are undefined, and code is not provided, raising reproducibility concerns.\n\n5. **Reviewers' Consensus**:\n   - Reviewers 2 and 3 rated the technical and empirical novelty as marginal (scores of 2/4). While Reviewer 4 acknowledged some novelty, they emphasized the lack of motivation and ablations.\n   - Multiple reviewers criticized the paper for omitting critical prior work and failing to engage with existing ensemble methods, weakening its contribution to the field.\n\n### Final Assessment:\nWhile the SOTA results on CIFAR-10 are impressive, the method’s incremental nature, lack of theoretical grounding, and failure to address prior work significantly limit its impact. The computational cost and incomplete experimental validation further reduce its appeal for a top-tier conference. The reviewers’ critiques highlight that the paper does not meet the novelty or rigor standards required for acceptance at a leading venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about maintaining diverse playing styles in self-play for reinforcement learning. The main idea is using a bi-objective optimization model to balance skill and diversity. The reviewers have given mixed feedback. \n\nStarting with Reviewer 1. They point out that the paper doesn't cite Lanctot et al. 2017, which is a key work on multiagent RL algorithms like Fictitious Play. That's a problem because those methods are population-based and could be relevant baselines. The experiments also have an issue where the test pool has way more EMOGI and BiO agents, which might bias the results in favor of those methods. That's a big flaw because it makes the comparison unfair. The reviewer also mentions that AlphaStar was designed to have diverse styles, so the paper's contribution here isn't clear. The technical novelty is rated low, which is a red flag for a top conference.\n\nReviewer 2 thinks the paper is mostly clear but has some issues. They note that reducing playing style to a single scalar is a simplification that's not discussed enough. The experiments need more details, like confidence intervals and more examples in figures. However, they do think the approach is novel and the results are promising. The technical correctness is okay, but the novelty is only marginal.\n\nReviewer 3 points out missing baselines like quality-diversity algorithms (MAP-Elites) and domain randomization. They also mention that the experiments are limited to two domains, one of which is simple (Pong). The lack of comparison to these baselines weakens the contribution. The technical novelty is again considered marginal, and the empirical results aren't broad enough. The experiments' setup for RQ3 is problematic because of the imbalanced opponent pool.\n\nThe authors didn't respond, so we can't see if they addressed these points. The main issues are the missing baselines (like Fictitious Play, MAP-Elites), the flawed experimental setup in Table 3, and the marginal novelty. Top-tier conferences require strong novelty and thorough comparisons. The paper's contributions don't seem significant enough given existing work, and the experiments have design flaws that affect validity. While the idea is interesting, the lack of proper baselines and the biased experiments undermine its credibility. The reviewers' scores on novelty and significance are mostly low, which is critical for acceptance. So, I think this should be rejected.\n</think>\n\nThe paper proposes a method to maintain diverse playing styles in self-play through a bi-objective optimization framework, aiming to improve policy generalization. While the idea is conceptually interesting and the experiments show some promise, several critical issues raised by reviewers undermine its suitability for a top-tier conference:\n\n### Key Strengths:\n1. **Conceptual Approach**: The paper addresses an important problem in self-play by explicitly optimizing for both skill and diversity, which is a valid direction in multi-objective RL.\n2. **Empirical Results**: The method shows competitive performance in Pong and Justice Online, particularly in handling different environments (e.g., randomized starting positions).\n3. **Clarity**: The paper is generally well-written and the methodology is reproducible with sufficient details in the appendix.\n\n### Major Weaknesses:\n1. **Missing Baselines**:\n   - **Lanctot et al. (2017)**: The paper fails to engage with foundational multiagent RL frameworks like Fictitious Play, which are population-based and directly relevant to self-play. This omission weakens the novelty and significance of the proposed method.\n   - **Quality-Diversity Algorithms**: Reviewers highlight the lack of comparisons to established quality-diversity methods (e.g., MAP-Elites), which are designed for optimizing both performance and diversity. This omission suggests the contribution overlaps with existing work.\n   - **AlphaStar**: The paper claims novelty in maintaining diverse styles but does not rigorously compare against AlphaStar, which explicitly aims to produce diverse strategies. This undermines the uniqueness of the proposed approach.\n\n2. **Flawed Experimental Design**:\n   - **Biased Opponent Sampling**: In Table 3, the test pool heavily overrepresents EMOGI and BiO agents (30 each), giving them an unfair advantage. This design flaw casts doubt on the validity of the results and the method’s superiority over baselines like PBT.\n   - **Limited Domains**: While Justice Online is complex, Pong is a simple domain with limited action space. Including more benchmark domains (e.g., StarCraft, MuJoCo) would strengthen generalizability claims.\n   - **Lack of Domain Randomization Baseline**: The paper attributes performance gains in \"different environments\" to its method but does not test whether simple domain randomization during training achieves similar results.\n\n3. **Technical and Explanatory Gaps**:\n   - **Simplification of Playing Style**: Reducing playing style to a single scalar (aggressive/defensive) is a critical limitation not adequately discussed. This oversimplification may fail in high-dimensional style spaces, as noted by Reviewer 3.\n   - **Unclear Methodology**: Ambiguities in equations (e.g., Equation 4’s \"relevant state change\") and undefined terms (e.g., \"reward weights\") reduce clarity.\n   - **Marginal Novelty**: The adaptation of NSGA-II to self-play is incremental, and the bi-objective formulation does not fundamentally differ from prior multi-objective RL work.\n\n4. **Reviewer Consensus**:\n   - **Novelty and Significance**: All reviewers rated technical and empirical significance as marginal (scores of 2 or 3 out of 4). This is a critical barrier for a top-tier conference.\n   - **Correctness**: Reviewer 1 explicitly states claims are \"not well-supported,\" citing the biased experiments and missing baselines.\n\n### Conclusion:\nWhile the paper addresses a relevant problem and shows some empirical promise, its failure to engage with key prior work, flawed experimental design, and incremental technical contributions render it insufficient for a top-tier conference. The lack of rigorous comparisons to established baselines and the biased evaluation setup significantly weaken its credibility. Addressing these issues would require substantial revisions beyond the scope of a conference paper.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the information provided.\n\nThe paper is about regression with label differential privacy (DP). The key idea is using a prior distribution of labels to design an optimal randomization mechanism, specifically \"randomized response on bins.\" They claim this is better than existing methods like Laplace or Exponential mechanisms. The experiments show their method works well.\n\nLooking at the reviewers' comments. Reviewer 1 initially had an issue with conflating regression and classification but then retracted that after the authors clarified. That seems resolved. Reviewer 2 was concerned about a fair comparison but agreed to include the results in the appendix. The authors addressed that by adding an appendix with the comparison, which is good.\n\nReviewer 3 thinks the method is interesting and practical, but points out the lack of comparison with [1], Ghazi et al. 2021. The authors responded by adding experiments comparing their method to that paper's algorithm, showing better performance. However, they note it's not a fair comparison because [1] is for classification. The reviewer also mentions the paper is clearly written and the proofs seem okay, but the technical novelty is somewhat there but not entirely new. The empirical results are solid but maybe not groundbreaking.\n\nReviewer 4 gives a high score (8), noting the method is novel and optimal. They point out a weakness about the second term in the loss equation (Equation 1) and the number of bins. The authors addressed the first by acknowledging it's a future direction and the second by noting empirical observations but no formal proof. Since the reviewer still gave a high score, that's positive.\n\nReviewer 5 says the paper is well-written and provides experiments but notes the method isn't technically novel compared to Ghazi et al. They mention the method is a generalization but the technical novelty is low. The authors responded by explaining their LP solution is different and allows a dynamic programming approach, which is a key novelty. However, the reviewer's technical novelty score was low (2), which is a concern for a top conference.\n\nReviewer 6 thinks the problem is good but notes mapping continuous labels to finite bins might be an issue. The authors addressed this by extending their theorem to continuous intervals, which should handle that. The reviewer also mentions some typos but overall finds it interesting.\n\nThe author responses seem thorough. They addressed most concerns, especially the comparison with Ghazi et al. by adding experiments, even if the comparison isn't perfect. The technical contributions, while building on prior work, have some novel elements like the dynamic programming solution and the extension to continuous labels.\n\nNow, considering top-tier standards. The paper's main strengths are the clear writing, empirical validation, and the theoretical proof of optimality under certain conditions. The weaknesses are the technical novelty being somewhat incremental compared to Ghazi et al., and the lack of addressing the second term in the loss equation. However, the authors have acknowledged these points and provided explanations or future directions.\n\nTop conferences usually require significant novelty and impact. While some reviewers (like 5) found the technical novelty marginal, others (like 4) saw it as significant. The empirical results show clear improvements over baselines, which is a plus. The fact that they extended the method to continuous labels and provided an efficient algorithm (dynamic programming) adds to the contribution.\n\nThe main issue is the comparison with Ghazi et al. The authors did compare and showed better performance, even if it's not a perfect comparison. Including that in the appendix with a discussion, as Reviewer 2 suggested, should address that concern.\n\nOverall, the paper seems to meet the standards. The contributions, while building on prior work, offer a novel approach for regression specifically, which is a distinct problem from classification. The experiments support the claims, and the theoretical results are solid. The minor issues are addressed in the responses. Therefore, I think it should be accepted.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\nThe paper proposes a novel approach to achieving label differential privacy (DP) in regression tasks through a \"randomized response on bins\" mechanism. The contributions are well-supported by theoretical analysis (optimality proofs, extension to continuous labels) and empirical validation across multiple datasets. Key strengths include:\n\n1. **Technical Contributions**: \n   - The mechanism is proven optimal under a given regression loss, with an efficient dynamic programming algorithm for bin optimization. This builds on prior work (e.g., Ghazi et al. 2021) but extends it to regression, which requires handling continuous or ordered label spaces and loss functions that penalize errors based on distance (e.g., MSE). \n   - Theoretical results, such as the extension to continuous label domains (Theorem 15 in the appendix), demonstrate non-trivial advancements over classification-focused methods.\n\n2. **Empirical Validation**: \n   - Experiments show clear improvements over baselines (Laplace, Exponential, staircase mechanisms) and even outperform the classification-focused Ghazi et al. method when adapted to regression tasks, despite the latter not being designed for this purpose. Including these results in the appendix (as reviewers suggested) strengthens the paper’s empirical rigor.\n\n3. **Clarity and Reproducibility**: \n   - The paper is well-written and accessible, with thorough appendices addressing reviewer concerns (e.g., comparisons, typos, and extensions).\n\n**Weaknesses Addressed**:\n- **Novelty Concerns**: While some reviewers noted incremental contributions compared to Ghazi et al., the authors clarified that their LP formulation and dynamic programming solution are structurally distinct and tailored to regression. This distinction, combined with the extension to continuous labels, justifies the novelty for a top-tier conference.\n- **Fair Comparison**: The authors added appendix results comparing to Ghazi et al., acknowledging the limitations but demonstrating empirical superiority in regression settings.\n- **Theoretical Gaps**: Reviewer 4’s concern about the second term in Equation (1) was acknowledged as a future direction, and the bin-count issue was empirically addressed. These are acceptable for a conference paper, especially given the space constraints.\n\n**Top-Tier Standards**: \nThe paper meets the bar for a top conference by advancing the state-of-the-art in label DP for regression, offering both theoretical guarantees and practical algorithms. While not groundbreaking, its focused contribution to a less-explored area (regression vs. classification) and rigorous evaluation justify acceptance. The minor criticisms (e.g., incremental novelty, unresolved theoretical questions) are outweighed by the paper’s strengths and the thoroughness of the author responses."
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether to accept or reject this paper for a top-tier conference. Let me start by going through all the provided information carefully.\n\nFirst, the paper's title is \"Functional Risk Minimization,\" and the abstract mentions a novel approach where each data point comes from its own function instead of a single function with noise. The key idea is to model each data point's own function, which they call Functional Generative Models (FGMs). They propose Functional Risk Minimization (FRM) as a framework that subsumes ERM (Empirical Risk Minimization) for many loss functions and claims better performance in small experiments. They also link FRM to memorization in over-parameterized models.\n\nLooking at the reviewer comments, there's a mix of positive and critical points. \n\nReviewer 1 is quite critical. They point out several weaknesses: the algorithm isn't formally derived, scalability issues because of Hessian computations, lack of theoretical guarantees (like why FRM outperforms ERM), and clarity issues in the presentation. They mention that the paper's claims aren't well-supported, giving it a correctness score of 2 (out of 4?), which is low. However, they acknowledge the high novelty and potential significance. But overall, they can't support acceptance due to these issues.\n\nReviewer 2 thinks the idea is interesting and well-written but has confusion about the relation to hierarchical Bayesian models and the test-time use of a single parameter. They also note that the functional generative model wasn't evaluated experimentally. However, they're on the fence, suggesting it's marginally above the line but needs more clarity.\n\nReviewer 3 is more positive, giving a thumbs-up for acceptance, noting high novelty and quality but mentions computational costs and the need to justify why FRM is crucial over ERM modifications. They think it's marginally acceptable.\n\nThe author response addresses some points. They added experiments with CNNs on MNIST variants, showing FRM's scalability and benefits in structured data. They clarify the algorithm's derivation, scalability via Hessian-vector products, and theoretical underpinnings. They also address the Bayesian vs. hierarchical model confusion and test-time use of a single parameter.\n\nNow, considering all this:\n\nStrengths:\n- Novelty is high; the idea of per-data-point functions is original and could impact how noise is modeled.\n- The authors have addressed some scalability concerns with new experiments and clarified the algorithm's derivation.\n- The framework subsumes ERM, which is a strong theoretical point.\n- The experiments now include neural networks, showing practical applicability beyond small cases.\n\nWeaknesses:\n- Reviewer 1's main concerns were about the lack of formal guarantees (theoretical) and clarity in the algorithm's derivation. The authors mention they proved subsumption for some losses, but not all. The theoretical justification for when FRM outperforms ERM is still not fully there.\n- Scalability is addressed but with caveats (slower than ERM, requires Hessian approximations). The new experiments help but might not be sufficient for a top conference.\n- Clarity issues were present, but the authors improved explanations and equations. However, the original paper's clarity was a problem, and even with responses, it's unclear if the revised version fixed all ambiguities.\n- The test-time use of a single parameter was confusing, but authors clarified that it's similar to ERM's approach, which makes sense.\n\nTop-tier conferences usually require strong theoretical backing, clear presentation, and solid empirical validation. The paper's theoretical claims are there but lack formal proofs for key points (like why FRM minimizer has low loss). The experiments are now better but still might be limited in scale (MNIST variants are standard but not as large as ImageNet, etc.). The scalability is addressed but not yet on par with ERM's efficiency.\n\nHowever, the paper's novelty and potential impact are significant. The authors have made efforts to address scalability and clarify points. The new experiments add value. The theoretical gaps are present but might be acceptable for a first work in a new direction, especially given the references to over-parameterized models and connections to modern ML challenges (like generalization despite memorization).\n\nConsidering that top conferences sometimes accept high-risk, high-reward papers that open new avenues, even if not fully polished, this might tip the scale. The reviewers' mixed feedback but the authors' responses addressing major points (like scalability and some theoretical aspects) could mean the paper is now acceptable. However, if the theoretical gaps are too big and the experiments still small, maybe not.\n\nBut the key is whether the contributions are significant enough despite the flaws. The framework's potential to unify or replace ERM is a big deal. The authors have shown it works in multiple settings (supervised, unsupervised, RL), which is a plus. The clarity issues were addressed in the response, so maybe the revised paper is clearer.\n\nFinal call: The paper has high novelty and potential, addresses major concerns with new experiments and clarifications, even if not perfect. For a top conference, it's a borderline case but leans towards acceptance as a first step in a promising direction.\n</think>\n\nFinal Decision: Accept\n\n### Detailed Reasoning:\n1. **Novelty and Potential Impact**:  \n   The paper introduces a fundamentally new framework (FRM) that rethinks foundational assumptions in machine learning. By modeling each data point as arising from its own function, it offers a novel perspective on noise modeling and generalization, particularly in over-parameterized regimes. This has the potential to reshape how we approach risk minimization, especially given its subsumption of ERM and connections to modern deep learning phenomena like double descent. The experiments across supervised, unsupervised, and reinforcement learning domains further highlight its broad applicability.\n\n2. **Addressed Concerns**:  \n   The authors effectively addressed key criticisms from reviewers:  \n   - **Scalability**: New experiments with CNN-based VAEs on MNIST variants demonstrate scalability to neural networks, and the approximation (Equation 9) is justified via Hessian-vector products, aligning with recent advances in large-scale optimization.  \n   - **Clarity**: The revised presentation clarifies the distinction between FGMs (the model) and FRM (the objective), explicitly lays out the FRM objective, and addresses confusion around test-time parameter usage.  \n   - **Theoretical Gaps**: While formal guarantees (e.g., generalization bounds) are lacking, the authors provide intuitive arguments linking FRM to low test loss under their assumptions and show empirical benefits in structured data. The theoretical subsumption of ERM for common losses is a strong foundational contribution.\n\n3. **Empirical Validation**:  \n   The expanded experiments on MNIST variants (colored, translated, etc.) provide compelling evidence that FRM excels in structured data scenarios, a key use case the authors emphasize. While the experiments are not at the scale of industrial benchmarks, they are sufficient to demonstrate the framework’s promise and align with the paper’s exploratory nature.\n\n4. **Trade-offs for a First Work**:  \n   The paper is positioned as a \"first step\" toward a new paradigm. Top-tier conferences often accept such high-potential works even if they lack perfection, especially when they open new research directions. The theoretical and empirical gaps are acknowledged, but the framework’s potential to address critical issues in over-parameterized learning (e.g., generalization despite memorization) justifies its acceptance.\n\n5. **Remaining Weaknesses**:  \n   - **Theoretical Rigor**: Lacks formal guarantees for FRM’s performance (e.g., convergence, generalization).  \n   - **Scalability Limits**: FRM remains slower than ERM, though the authors argue this is manageable with further research.  \n   - **Clarity in Original Submission**: While improved in the response, the initial presentation’s confusion (e.g., ERM terminology) suggests room for refinement in the final version.  \n\nHowever, these weaknesses are outweighed by the paper’s innovative contributions and the authors’ demonstrated commitment to addressing concerns. The work’s significance and potential to inspire future research in foundational ML principles justify its acceptance."
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new validation protocol called PVP, aiming to solve the problem of splitting data into train/validation sets, which can lead to a trade-off between model performance and reliable evaluation. The reviewers have mixed opinions, but there are some common concerns.\n\nFirst, looking at the strengths: The problem is important, especially in real-world ML where data is scarce. The idea of using data augmentation and a sampling method to create a validation set without splitting the training data is novel. Reviewers 1, 2, and 3 acknowledge the novelty and potential impact. The experiments cover three data modalities, which is a plus.\n\nNow the weaknesses. Reviewer 1 points out that the experiments might not properly address model selection. The paper's experiments seem to focus on fixed models, so it's unclear if PVP actually helps in selecting better hyperparameters or architectures. That's a big issue because validation sets are crucial for that. Also, the experiments' clarity is a problem. Tables 3 and 4 are confusing, and the metrics like \"deterioration\" aren't well-defined. The lack of theoretical backing is another point—Reviewer 4 says there's no justification why the proximal set is representative of the true data distribution.\n\nReviewer 2 mentions that the datasets used are too small. Using CIFAR-LT instead of larger datasets like ImageNet might not validate the method's scalability. Without bigger data, the claims might not hold in real-world scenarios. Also, no theoretical analysis is provided, which is a gap for a top-tier conference.\n\nReviewer 3 agrees the problem is significant but flags several major issues. The method's applicability beyond classification isn't clear, and there's a lack of experiments on real-world or industrial setups. They also note missing ablation studies on hyperparameters and statistical significance in results. The suggestion to merge the proximal set with training data isn't addressed, which could be a better approach.\n\nReviewer 4 is harsher, saying the approach lacks theoretical basis and the metrics used don't convincingly show the validation set is representative. They argue that a good validation set should reflect the data distribution, which the paper doesn't prove. The test-validation gap metric might be manipulated without real validity.\n\nThe author response isn't provided, so we can't see if they addressed these points. For a top conference, the experiments need to be thorough and clearly presented. The current issues with experimental design, dataset size, and lack of theory are significant. While the idea is novel, the empirical validation is insufficient, and the theoretical justification is missing. Reviewers 1 and 4 lean towards reject, while 2 and 3 are on the fence but point out major flaws. Since top conferences require strong empirical and theoretical support, and this paper falls short in both, the decision should be to reject.\n</think>\n\nThe paper proposes the Proximal Validation Protocol (PVP) to address the trade-off between training data usage and validation set reliability. While the problem is important and the method is novel, several critical issues undermine its acceptance at a top-tier conference:\n\n### Key Strengths:\n1. **Novelty and Relevance**: The problem of validation set construction is understudied, and PVP offers a creative solution using data augmentation and distributional sampling.\n2. **Empirical Scope**: Experiments span three data modalities (images, text, tabular), demonstrating broad applicability.\n3. **Clarity of Motivation**: The paper highlights a real-world gap between academic research and industrial practice, aligning with practical concerns.\n\n### Major Weaknesses:\n1. **Empirical Rigor**:\n   - **Lack of Model Selection Evaluation**: The experiments focus on fixed models, failing to demonstrate whether PVP improves hyperparameter/model selection, which is the core purpose of validation sets. This undermines the practical relevance of the claims.\n   - **Small Datasets**: Most experiments use small or lightly modified datasets (e.g., CIFAR-LT instead of ImageNet, limited text datasets). This raises questions about scalability and real-world applicability.\n   - **Unclear Metrics and Presentation**: Tables 3 and 4 lack clear definitions (e.g., \"deterioration\"), and the methodology for comparing validation protocols is poorly explained. The test-validation gap metric is insufficient to justify the method’s validity without theoretical grounding.\n   - **Missing Ablations**: The impact of data augmentation choices, sampling strategies, and hyperparameters (e.g., proximal set size) is not rigorously explored. For instance, using simple augmentations like Gaussian noise could test robustness.\n\n2. **Theoretical Gaps**:\n   - No theoretical justification explains why the \"proximal set\" approximates the true data distribution or why it avoids overfitting. Reviewer 4 argues this is critical for validation set validity.\n   - The method’s reliance on feature extractors (e.g., pre-trained ResNet/BERT) introduces dependencies on model architecture, contradicting the claim that PVP is \"independent of models and optimizers.\"\n\n3. **Methodological Concerns**:\n   - **Applicability Beyond Classification**: The method is framed for classification tasks (e.g., angular distribution sampling), but its extension to regression, self-supervised learning, or other modalities is untested and unaddressed.\n   - **Statistical Significance**: Key results (e.g., Tables 1 and 2) lack statistical significance tests, making it hard to trust the reported improvements.\n\n4. **Writing and Presentation**:\n   - Clarity issues persist (e.g., awkward phrasing, undefined terms like \"F1 score\" in Definition 3.1). While the core idea is accessible, the experiments section is poorly structured and confusing.\n\n### Conclusion:\nWhile the problem is significant and the proposed method is innovative, the paper falls short in empirical rigor, theoretical justification, and clarity. The lack of demonstration that PVP improves model selection, reliance on small datasets, and absence of critical ablation studies weaken its credibility. Top-tier conferences demand stronger validation of claims and deeper exploration of limitations. The reviewers’ concerns—particularly about empirical and theoretical gaps—are not adequately addressed, and the current submission does not meet the high standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about improving protein-protein interaction (PPI) prediction using pretrained structure embeddings. The main idea is to use structural information from OmegaFold along with GNNs. \n\nLooking at the reviewers' comments, there's a common theme of limited novelty and technical contribution. Reviewer 1 points out that the method mainly uses existing models like OmegaFold and standard GNNs like SAGE and GAT, so the innovation is low. They also mention issues with the protein-level embeddings possibly missing local structures and inconsistencies in the results tables. \n\nReviewer 2 agrees, saying the approach is just combining existing methods without significant changes. They note a 5% improvement, which might not be enough for a top conference. Reviewer 3 mentions unclear presentation, lack of confidence intervals, and not comparing to state-of-the-art methods like dMaSIF. They also question the clarity of the AUC metric here. \n\nReviewer 4 is more critical, pointing out weak baselines (like using simple ResNet instead of stronger PPI models) and the absence of comparisons to AlphaFold-Multimer, which is a big deal in the field. They also question the necessity of the GNN layer since the task could be pairwise without needing a network. The negative sampling part is unclear too. \n\nThe authors didn't respond, which might mean they didn't address these valid concerns. \n\nThe key issues here are: \n1. **Novelty**: The method isn't novel enough. Using existing pretrained embeddings and standard GNNs without significant modifications doesn't add much new. \n2. **Baseline Comparisons**: The paper doesn't compare against state-of-the-art methods like dMaSIF or AlphaFold-Multimer, making it hard to gauge actual improvement. \n3. **Technical Flaws**: The protein embeddings might miss local structures crucial for interactions. The GNN's role is questionable, and the model design isn't well-supported by results. \n4. **Empirical Results**: The improvements are marginal (5%), and there are inconsistencies in the tables. Lack of confidence intervals and proper statistical validation weakens the claims. \n5. **Clarity and Presentation**: Several reviewers noted unclear explanations, especially about the setup and metrics like AUC. \n\nTop-tier conferences require significant contributions, clear methodology, and thorough evaluations. This paper falls short in all these areas. The lack of innovation, weak baselines, and technical issues outweigh the minor improvements reported. Without strong empirical evidence and better comparisons, it's unlikely to meet the standards of a top conference like ICLR.\n</think>\n\nThe paper proposes using pretrained structural embeddings from OmegaFold combined with graph neural networks (GNNs) for protein-protein interaction (PPI) prediction. While the motivation to incorporate structural information is valid, the reviewers consistently highlight critical shortcomings that undermine its suitability for a top-tier conference:\n\n### Key Issues:\n1. **Limited Novelty and Technical Contribution**:\n   - The method primarily repurposes existing components (OmegaFold embeddings and standard GNN architectures like GraphSAGE/GAT) without introducing novel modeling ideas. Reviewers note that the core innovation is minimal, as the approach amounts to \"using known encodings with no significant architectural modifications.\"\n   - The GNN’s role is questioned, as the task could be framed as pairwise interaction prediction without requiring a network structure. The lack of ablation studies (e.g., comparing to a baseline without GNNs) weakens the justification for its inclusion.\n\n2. **Weak Empirical Evaluation**:\n   - **Baseline Comparisons**: The paper fails to benchmark against state-of-the-art PPI methods like dMaSIF or AlphaFold-Multimer, which are critical for establishing significance. Comparisons to simple models (e.g., ResNet) are insufficient for a competitive evaluation.\n   - **Marginal Improvements**: Reported gains (e.g., 5% improvement) are modest and inconsistent across datasets (e.g., discrepancies in Table 2 vs. Table 3). The absence of confidence intervals and statistical validation further undermines the results.\n   - **Unclear Experimental Setup**: Questions remain about negative sampling strategies, the handling of variable-length embeddings (mean pooling may discard critical local structural information), and the lack of clarity on how AUC is computed for PPI classification.\n\n3. **Clarity and Presentation**:\n   - The paper suffers from poor writing quality (typos, grammatical errors) and unclear explanations (e.g., defining how AUC is measured for PPI). Tables are poorly formatted, and figures could better visualize results.\n   - Overemphasis on OmegaFold’s details distracts from the paper’s core contributions, which are already derivative.\n\n4. **Reproducibility Concerns**:\n   - While hyperparameters are provided, key experimental details (e.g., MSA input type for OmegaFold, negative sampling strategy) are inadequately described, complicating reproducibility.\n\n### Conclusion:\nThe paper’s contributions are incremental and lack the technical or empirical rigor expected of a top-tier conference. The method’s reliance on existing components without meaningful innovation, coupled with weak baselines and insufficient validation, fails to demonstrate the significance required for acceptance. Addressing these issues would require substantial revisions, but given the current state, the work does not meet the bar for ICLR.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "Okay, let's start by going through all the information provided. The paper is about protein sequence and structure co-design using an equivariant translation approach. The main claims are that their method, ProtSeed, outperforms existing methods in terms of speed and accuracy, using a one-shot update instead of autoregressive or diffusion models. \n\nLooking at the reviewers' comments, there's a mix of positive and critical feedback. Reviewers 1, 2, 3, 4, 5, 7, and 8 seem to have some positive points. Reviewer 1 is satisfied after the author's response and even increased their score. Reviewer 3 mentions clarity improvements but still wants a clearer calculation for SeqIPA. Reviewer 4 had some concerns about novelty and claims of being the first co-design method but the authors addressed that by revising their claims. Reviewer 5 points out weaknesses like lack of hyperparameters and training details, discrepancies in prior work results, and functional evaluation. They also mention low AAR scores. However, they still recommend acceptance with improvements. Reviewer 7 also recommends acceptance but notes the need for comparisons with more recent models and ablation studies. \n\nOn the other hand, Reviewer 6 and 8 have more critical takes. Reviewer 6 gives lower scores, stating the method isn't novel enough and has technical issues like unclear SeqIPA calculations and lack of ablation studies. They also question the necessity of context features. Reviewer 8 is positive but mentions the term \"co-design\" might be misleading since it requires context features, not generating from scratch. \n\nThe authors addressed many points: they added ablation studies, clarified SeqIPA, provided more details on training costs, and promised code release. They also revised claims about being the first co-design method after Reviewer 4's feedback. The experiments show improvements over baselines, and they added comparisons with ProtMPNN as suggested by Reviewer 7. \n\nKey strengths: Strong empirical results, faster inference, clear improvements over baselines, and addressing reviewer concerns through revisions. The method's technical approach, while building on AlphaFold2 components, introduces novel elements like the iterative translation and cross-conditioning. \n\nWeaknesses: Some reviewers question novelty, lack of functional evaluations beyond structural metrics, and dependency on context features. The authors acknowledged these and provided explanations or plans to address them in future work. \n\nConsidering it's a top-tier conference, the paper needs to have significant novelty and robust evaluation. The authors have made substantial revisions, added necessary details, and clarified claims. While there are valid criticisms about context dependency and functional metrics, the improvements and empirical results seem strong enough. The ablation studies and additional experiments with ProtMPNN strengthen their case. The technical novelty, even if incremental, combined with clear performance gains, likely meets the bar for acceptance. \n\nFinal Decision: Accept\n</think>\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's start by going through all the information provided. The paper is about data leakage in federated learning (FL) specifically for tabular data. The authors propose TabLeak, which addresses challenges like mixed discrete-continuous features, variance in reconstructions, and assessing quality. The reviewers have mixed opinions.\n\nFirst, looking at the strengths mentioned by Reviewer 3, they highlight the novelty of applying gradient attacks to tabular data, which is a new area. The method uses softmax for continuous relaxation and pooling to reduce variance, which are straightforward but effective. The experiments are thorough, showing a 10% improvement over baselines, which is significant. The entropy-based uncertainty measure is also a good contribution.\n\nHowever, Reviewers 1 and 2 point out issues. Reviewer 1 mentions poor writing and grammar, which could hinder understanding. They also say the methods are standard, so the novelty is low. Reviewer 2 agrees the technical contributions are weak, noting that softmax and ensembling are common techniques. They suggest more sophisticated methods like Gumbel-Softmax or mixed-integer optimization could be better, but the authors addressed some of these in their response.\n\nThe author responses address several points. They provided additional experiments showing the effectiveness of pooling and softmax over alternatives like Gumbel-Softmax, which actually performed worse. They also clarified the variance issue with mixed data through synthetic experiments. The rebuttal seems to strengthen their case by adding more empirical evidence and addressing reviewer concerns about baselines and methodology.\n\nThe key here is balancing the novelty and significance against the criticism. While the methods might use standard components, combining them effectively for a new domain (tabular FL) with strong empirical results is valuable. The problem is important because tabular data is common in sensitive applications like finance and healthcare. The authors' rebuttal effectively counters some criticisms by showing their approach's superiority over alternatives and clarifying their methodology.\n\nDespite the initial concerns about writing, Reviewer 3 found it well-written, and the authors addressed some of the clarity issues. The technical contributions, though built from existing components, solve specific challenges in a new context, which is a valid contribution for a top-tier conference. The experiments are extensive and show clear improvements, which is a strong point.\n\nConsidering all this, the paper addresses an important problem, provides a novel solution in a new domain, and backs it with solid experiments. The criticisms about method novelty are mitigated by the context-specific adaptations and the problem's significance. Therefore, it should be accepted.\n</think>\n\nThe paper addresses the critical issue of data leakage in federated learning (FL) for tabular data, a domain with high practical relevance (e.g., finance, healthcare) but limited prior work on privacy risks. The proposed method, TabLeak, tackles unique challenges in tabular data (mixed discrete-continuous features, reconstruction variance, quality assessment) through three components: a softmax structural prior, pooled ensembling, and entropy-based uncertainty estimation. Key points from the evaluation include:\n\n### Strengths:\n1. **Novelty and Problem Relevance**: \n   - The work is the first to systematically study data leakage attacks on tabular FL, an understudied but critical area. Reviewer 3 explicitly acknowledges the novelty of applying gradient inversion to tabular data and the importance of identifying its unique challenges (e.g., mixed data types).\n   - The problem is timely and impactful, as tabular data is widely used in privacy-sensitive applications.\n\n2. **Technical Contributions**:\n   - The softmax relaxation for discrete features and pooled ensembling are pragmatic solutions tailored to tabular data’s mixed structure. While components like softmax and ensembling are not novel in isolation, their combination and adaptation to tabular FL is a meaningful contribution.\n   - The rebuttal addresses criticism by demonstrating that alternatives like Gumbel-Softmax underperform (e.g., 42% vs. 72% accuracy on Adult dataset) and that pooling effectively reduces variance (e.g., 2.6→1.7 standard deviation in mixed datasets).\n\n3. **Empirical Rigor**:\n   - The experiments are extensive, with ablation studies, comparisons to strong baselines (e.g., cosine similarity-based attacks), and analysis across four datasets. The 10% improvement over baselines on Adult (batch size 32) is compelling.\n   - The authors addressed reviewer concerns by adding experiments (e.g., synthetic datasets to validate variance claims, ensemble size analysis) and clarifying methodology (e.g., network initialization impact).\n\n### Weaknesses:\n1. **Methodological Criticisms**:\n   - Reviewers 1 and 2 noted that the technical contributions rely on \"standard\" techniques (e.g., softmax, ensembling). However, the rebuttal strengthens the case by showing these choices are optimal for tabular data (e.g., Gumbel-Softmax’s failure due to non-stochastic nature of discrete features).\n   - The baseline comparison (e.g., linear models) was addressed in the rebuttal, and the authors clarified that the random baseline’s performance reflects real-world data dependencies.\n\n2. **Writing and Clarity**:\n   - Reviewer 1 criticized the writing quality, but Reviewers 2 and 3 found it clear. The authors addressed notational inconsistencies and typos in their response, suggesting the issues are resolvable.\n\n### Conclusion:\nWhile the technical contributions are incremental in isolation, the problem of tabular FL leakage is novel and significant. The authors demonstrate strong empirical results and address reviewer concerns effectively. The work fills a critical gap in understanding privacy risks for tabular FL, which is essential for high-stakes applications. The method’s simplicity and reproducibility (acknowledged by Reviewer 3) are strengths, not weaknesses, for practical impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let me start by going through all the information provided. The paper is about online black-box adaptation to label-shift in the presence of conditional-shift. The authors are trying to see how existing label-shift methods perform when there's also conditional shift, which is a common real-world scenario. They tested this on synthetic and real datasets, and also proposed some heuristics like using OOD validation sets and handling non-invertible confusion matrices.\n\nLooking at the reviewer comments, there's a mix of positive and negative points. Reviewer 1 thinks the problem is well-motivated and the paper is honest about its inconclusive results, but points out several weaknesses like lack of systematic approach, insufficient experiments, unclear definitions, and not addressing related work properly. They also mention that the experiments are modest and don't provide clear generalizable insights. The technical and empirical novelty is rated as marginal.\n\nReviewer 2 says the work is novel but the empirical results don't show significant improvements over existing methods. The heuristics are common practices and lack theoretical support. They also note that the proposed methods don't outperform baselines in some cases, like on the iWildCam dataset. Again, the novelty is considered limited.\n\nReviewer 3 is more critical. They point out unclear writing, especially in the algorithm design and notation. They question the necessity of the validation set and the reliance on label-shift assumptions even when they're broken. They also mention missing comparisons with baselines that use validation data directly. The reproducibility is an issue since codes aren't provided. They recommend rejection.\n\nReviewer 4 finds the paper not self-contained, with unclear methodology and lack of theoretical justifications. They think the contributions are minimal and the paper is in a preliminary stage, so they lean towards rejection.\n\nThe author responses address some points, like adding more background, clarifying notation, and providing more experimental results in appendices. They acknowledge the lack of theoretical grounding and the modest improvements but argue that the paper starts a discussion and shows practical utility despite assumptions being violated.\n\nNow, considering the standards of a top-tier conference, which usually require significant contributions, clear presentation, and thorough evaluation. The main issues here are the marginal novelty, insufficient experiments, and lack of theoretical backing. The authors' responses help but don't fully resolve the major concerns. The empirical results don't show strong improvements, and the heuristics are seen as ad-hoc. The writing and clarity issues, though addressed, might still leave the paper below the bar for top venues. Since the reviewers, especially 3 and 4, are quite negative, and the improvements are incremental, I think the paper should be rejected unless there's a strong argument for acceptance. But given the consensus on the weaknesses, rejection seems appropriate.\n</think>\n\nThe paper addresses an important problem of adapting models to label-shift in the presence of concurrent conditional-shift, which is relevant for real-world deployments. However, the reviewers consistently highlight critical shortcomings that undermine its suitability for a top-tier conference. Key issues include:\n\n1. **Marginal Contributions**: The proposed heuristics (e.g., OOD validation, confusion matrix adjustments) are seen as incremental or ad-hoc, with limited empirical improvements over existing methods. Reviewers note that some heuristics are already common practices (e.g., adding scalars to matrices) or lack theoretical justification. The experiments do not conclusively demonstrate significant advantages over baselines, particularly in key datasets like iWildCam.\n\n2. **Empirical Weaknesses**: The experiments are described as insufficiently comprehensive. Reviewers point out a lack of comparisons with strong baselines (e.g., optimal fixed classifiers, domain adaptation methods like CORAL), limited dataset diversity, and unclear validation protocols. The results are inconsistent (e.g., no improvement on some datasets, minor or noisy gains on others), making it hard to draw generalizable insights.\n\n3. **Clarity and Presentation**: The paper suffers from poor self-containedness, ambiguous notation, and undefined terms (e.g., \"conditional shift,\" \"pseudo-count hyper-parameter\"). Sections like the Bayesian discussion and regression extensions are confusingly presented. While the authors addressed some issues in their response, the core clarity problems and lack of theoretical grounding remain unresolved.\n\n4. **Novelty and Significance**: The work is framed as an extension of Wu et al. (2021), but reviewers judge its contributions as \"marginally significant\" or \"not novel.\" The exploration of regression label-shift and non-invertible confusion matrices is embryonic and underdeveloped, with no clear path to broader impact.\n\n5. **Reproducibility**: Despite some clarifications, the lack of code and detailed experimental protocols (e.g., validation set collection, hyperparameter tuning) raises reproducibility concerns. Reviewers 3 and 4 explicitly note this as a barrier.\n\n**Author Responses**: The authors addressed minor issues (e.g., adding appendices, clarifying notation) but did not resolve fundamental flaws. They acknowledge the lack of theoretical grounding and modest empirical gains, framing the work as an exploratory study. However, top-tier conferences typically require stronger evidence of novelty, rigor, and impact.\n\n**Conclusion**: While the problem is well-motivated and the paper’s honesty is commendable, the combination of incremental contributions, insufficient empirical validation, unclear presentation, and lack of theoretical depth places it below the bar for acceptance at a top-tier conference. The reviewers’ consensus on its marginal significance and unresolved weaknesses strongly supports rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about using binary labels as an auxiliary task in multi-task learning for image classification. The main idea is that representing class labels as binary vectors (instead of one-hot) and training on both the original and these binary labels can improve model performance. \n\nLooking at the reviewers' comments, there's a mix of positive and negative points. Reviewer 1 thinks the idea is interesting but points out several weaknesses like unclear explanations, insufficient experiments, and lack of analysis on when the method works best. They also mention reproducibility issues because some terms like \"Metabalance\" aren't explained. \n\nReviewer 2 is more critical. They say the claims aren't well-supported, the notation is poor, and the experiments are insufficient. They even gave a 1/4 for correctness, which is a big red flag. But they do acknowledge some novelty. \n\nReviewer 3 is more positive, calling the method simple and effective, especially for difficult tasks. However, they note the improvements are small and the contribution might not be significant enough. They also mention missing related work on label coding. \n\nReviewer 4 points out the limited technical contribution, lack of explanation on why binary labels help, and the absence of experiments on large datasets like ImageNet. They also note reproducibility issues due to missing implementation details. \n\nThe authors didn't respond, which might mean they didn't address these concerns. \n\nSo, the key issues here are: \n1. Clarity and reproducibility problems. The paper isn't clear enough, with terms undefined and missing details. \n2. Weak experiments: not enough datasets, no comparison to strong baselines, and lack of analysis on when the method works. \n3. Technical novelty is marginal. The idea isn't entirely new, and related work isn't thoroughly discussed. \n4. Some reviewers think the improvements are too small, especially on standard datasets. \n\nTop-tier conferences require strong contributions, clear writing, and thorough experiments. While the core idea has merit, the execution and presentation fall short. The reproducibility issues and lack of detailed experiments (like on ImageNet) are significant drawbacks. The authors didn't address these in their response, so there's no indication they'll be fixed. \n\nHmm, but Reviewer 3 thought it's acceptable with more work. However, the majority of the reviews, especially the more critical ones (like Reviewer 2's low correctness score), suggest the paper isn't up to the standard yet. The experiments aren't robust enough, and the method's benefits aren't clearly demonstrated across the board. \n\nFinal call: The paper has potential but needs more work on experiments, clarity, and addressing related work. Since it's a top-tier conference, which usually expects more polished and impactful work, I think it should be rejected unless major revisions are done. But since the authors can't revise before the decision here, the answer is to reject.\n</think>\n\nThe paper proposes an interesting idea of using binary label representations as an auxiliary task in multi-task learning to improve classification performance. However, several critical issues undermine its readiness for a top-tier conference:\n\n1. **Clarity and Reproducibility**: Multiple reviewers highlighted significant clarity issues, including undefined terms (e.g., \"Metabalance\"), ambiguous descriptions of the binary label generation process, and missing implementation details. The lack of an algorithm box or pseudocode further complicates reproducibility. Without these, the method cannot be reliably implemented or validated.\n\n2. **Weak Experimental Rigor**: \n   - The experimental section is underdeveloped. Key baselines (e.g., dense labels, label smoothing, or state-of-the-art methods) are missing, making it difficult to assess the method’s superiority. \n   - The choice of datasets (e.g., absence of ImageNet) limits generalizability. \n   - No analysis of hyperparameters (e.g., task weighting) or sensitivity to random seeds is provided, casting doubt on the robustness of results. \n   - The paper fails to explore scenarios like distribution shifts or few-shot learning, which are critical for demonstrating broader utility.\n\n3. **Technical Novelty and Significance**: While the core idea is novel, reviewers noted that related work (e.g., label smoothing, error-correcting output codes, and unsupervised meta-learning) was inadequately addressed. The method’s marginal improvements over baselines on some datasets further weaken its significance. The lack of deeper analysis (e.g., why binary labels help, or when they do not) reduces its theoretical contribution.\n\n4. **Reviewer Consensus**: Despite some positive remarks (e.g., simplicity and potential utility for niche tasks), the majority of reviewers (especially Reviewers 1 and 2) concluded that the paper’s flaws—particularly in clarity, reproducibility, and experimental depth—are too severe for acceptance. Reviewer 2’s low correctness score (1/4) and Reviewer 4’s criticism of the limited technical contribution further underscore these concerns.\n\n5. **Author Response**: The absence of an author response suggests that these issues were not addressed, leaving unresolved questions about the method’s validity and implementation.\n\n**Final Decision: Reject**  \nWhile the paper introduces an intriguing concept, its current form lacks the rigor, clarity, and thorough evaluation required for a top-tier conference. Significant revisions would be necessary to address these shortcomings, but without an author response or evidence of such improvements, it does not meet the bar for acceptance."
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about identifying phase transition thresholds in permuted linear regression using message passing. The reviewers have mixed opinions, but there are some common themes.\n\nFirst, looking at the strengths: Reviewer 4 mentions that the paper provides a novel contribution by precisely characterizing phase transitions, which previous works only addressed in terms of convergence rates. The authors also connect their work to a branching random walk process, which is a solid technical approach. The numerical experiments align with theoretical predictions, which is a plus. Reviewer 3 also thinks the paper is interesting and attempts to predict phase transitions, even if they aren't an expert.\n\nNow the weaknesses. Reviewer 1 and 2 point out major issues with clarity and organization. The paper is described as poorly written, with heavy notation and lack of self-containment. They mention that key sections are copied from other works without proper explanation, making it hard for unfamiliar readers to follow. Reviewer 5 even says the writing is so bad that it's inaccessible to a general ICLR audience. \n\nTechnical novelty is another point. Reviewer 1 and 2 note that the work is incremental, building on existing methods from Semerjian and Zhang et al. The main contribution in the non-oracle case uses an approximation via Taylor expansion, which might not be highly novel. Reviewer 4 also mentions that parts of the analysis are inspired by prior work, limiting originality.\n\nReproducibility is an issue too. Reviewers 2 and 4 point out that the code isn't provided, which is a problem for verifying results. The experiments are mentioned but not clearly linked in the text, making it hard to trace their connection to the claims.\n\nThe authors' response addresses some points but doesn't fully alleviate concerns. They acknowledge the difficulty in writing about two complex areas but don't provide concrete plans to reorganize or clarify the paper sufficiently. The defense about the approximation in Section 4.2 and the framework being the first in their problem area is weak, as reviewers noted existing methods.\n\nConsidering top-tier conference standards, clarity and reproducibility are critical. The paper's current state, with poor organization and lack of self-contained explanations, would hinder its accessibility. While the technical contributions are novel to some extent, the incremental nature and reliance on prior frameworks might not meet the high bar for significance. The reproducibility issues also weaken the case for acceptance.\n\nFinal Decision: Reject\n</think>\n\nFinal Decision: Reject\n\n**Detailed Reasoning:**\n\n1. **Clarity and Organization:** Multiple reviewers (1, 2, 5) emphasize that the paper is poorly written, notation-heavy, and lacks self-containment. Key sections (e.g., message passing derivations) are copied verbatim from prior works (e.g., Semerjian et al.) without sufficient explanation, making it inaccessible to readers unfamiliar with those references. Reviewer 5’s critique is particularly damning, stating the writing is \"highly sub-optimal\" and \"inaccessible to an ordinary ICLR reader.\" These issues are critical for a top-tier conference, where clarity and self-contained exposition are essential.\n\n2. **Technical Novelty and Significance:** While the paper claims to precisely identify phase transition thresholds, reviewers note that the contributions are incremental. The oracle case analysis is framed as a corollary of existing results (Theorem 1 in prior work), and the non-oracle case relies on an approximation (Taylor expansion) that reviewers argue is not sufficiently novel. Reviewer 2 and 4 highlight that aspects of the work (e.g., message passing frameworks, BRW mappings) are borrowed from prior literature, limiting originality. The partial permutation recovery section is also criticized for lacking practical motivation and unclear connections to existing work.\n\n3. **Reproducibility:** Reviewers 2 and 4 note that the code is not provided, and experimental results are poorly linked to the text (e.g., figures and tables are referenced ambiguously). The empirical claims about alignment with theoretical predictions are weakened by the asymptotic vs. finite-sample gap highlighted by Reviewer 5. Without code or clearer experimental descriptions, reproducibility is compromised.\n\n4. **Author Response:** The authors acknowledge challenges in writing about complex topics but do not address structural reorganization or clarity improvements adequately. Their defense of the approximation in Section 4.2 and claims of being the \"first framework\" for message passing in this context are unconvincing given prior work. The response does not resolve concerns about rigor or accessibility.\n\n5. **Conference Standards:** Top-tier conferences like ICLR require strong technical novelty, clear exposition, and reproducibility. While the paper’s core idea (phase transitions in permuted regression) is interesting, the current submission falls short in these areas. The incremental nature of the work and lack of rigorous justification for approximations further weaken its case for acceptance.\n\n**Conclusion:** Despite potential technical merit in the analysis of phase transitions, the paper’s poor organization, lack of clarity, and reproducibility issues make it unsuitable for a top-tier conference in its current form. The authors should consider substantial revisions and resubmission to a journal or a more specialized venue."
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about Scrunch, a method for privacy-preserving representation learning in MLaaS. The goal is to prevent sensitive info leakage when data is sent to the cloud. The main idea is splitting the neural network into client and server parts, with some loss functions to anonymize the data before sending.\n\nLooking at the reviewers' comments. Reviewer 1 says the idea isn't new, the experiments are limited (only two datasets, not enough baselines like FHE), and the paper is incremental. They mention grammatical issues and low novelty. The technical and empirical significance are rated low.\n\nReviewer 2 is more critical. They point out major flaws in the threat model. The server could potentially reconstruct the encoder or manipulate the loss function, undermining privacy. Also, during training, the server has access to labels and centers, enabling model inversion attacks. The experiments are simplistic, using shallow models and not challenging tasks. They also note that the method is similar to split learning but not cited. The technical correctness is rated lower here.\n\nReviewer 3 mentions lack of a clear security model and potential info leakage through features. They give an example with time-series data from wearables, where encoded features might still leak trends. Also, concerns about applicability to NLP, where frequent words could be reverse-engineered. They think encryption is still needed, so the method isn't sufficient.\n\nAll reviewers agree on low novelty and technical significance. The experiments are insufficient, with only two datasets and limited baselines. The core idea isn't new, and there are fundamental flaws in the threat model. Reviewer 2's points about the server's ability to subvert the method are particularly damning. The authors didn't respond, so they couldn't address these issues. \n\nConsidering top-tier standards, the paper needs strong novelty, thorough experiments, and solid technical soundness. Here, the method has critical vulnerabilities, incremental contributions, and insufficient validation. The flaws in the threat model are especially concerning because they invalidate the privacy claims. Without fixing these, the paper can't meet the conference's standards. \n\nSo, the decision should be to reject.\n</think>\n\nThe paper proposes a privacy-preserving representation learning approach called Scrunch for MLaaS, aiming to anonymize data while maintaining task accuracy. However, the reviewers' critiques highlight significant issues that undermine its viability for a top-tier conference:\n\n1. **Technical Flaws in Threat Model**:  \n   Reviewer 2 identifies critical vulnerabilities:  \n   - The server can reconstruct the encoder via surrogate models or manipulate the loss function (e.g., disabling center loss), rendering privacy guarantees ineffective.  \n   - The training phase exposes sensitive information (e.g., labels, class centers) to the server, enabling model inversion attacks.  \n   - The threat model is poorly defined, and the approach assumes server honesty, which contradicts standard adversarial assumptions.  \n\n2. **Novelty and Incremental Contribution**:  \n   - Reviewers 1 and 2 note that splitting models into client/server components (e.g., split learning) is not novel and lacks foundational innovation.  \n   - The core idea of using center loss to control information flow is incremental and does not meaningfully advance the state of the art.  \n\n3. **Empirical Weaknesses**:  \n   - Limited experiments: Only two shallow models (LeNet-5, VGG-16) and datasets (MNIST, CelebA) are tested. More complex architectures (e.g., ResNet, Vision Transformers) and real-world tasks (e.g., identity inference) are absent.  \n   - Inadequate baselines: Comparisons with strong privacy-preserving methods (e.g., federated learning, homomorphic encryption) are missing.  \n   - Ambiguities in evaluation: The private task for MNIST is undefined, and the choice of hyperparameters (e.g., λ) is not thoroughly explored.  \n\n4. **Clarity and Reproducibility**:  \n   - While the paper is generally clear, grammatical errors and ambiguous statements (e.g., confusing intra/inter-class distances) weaken its presentation.  \n   - Key details (e.g., optimizers, training epochs) are omitted, hindering reproducibility.  \n\n5. **Author Response**:  \n   The absence of an author response prevents addressing these critiques, leaving unresolved concerns about the method’s validity and rigor.  \n\n**Conclusion**: The paper’s technical flaws, lack of novelty, and insufficient empirical validation render it unsuitable for a top-tier conference. The fundamental vulnerabilities in the threat model and privacy guarantees are particularly damning, as they invalidate the core contribution. Without substantial revisions to address these issues, the work does not meet the standards of a leading venue like ICLR.  \n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper is about the average-case performance of no-regret learning dynamics, specifically q-replicator dynamics (QRD), in potential games. The main contributions seem to be analyzing how these dynamics converge to Nash equilibria and evaluating their average performance by considering both the regions of attraction and the quality of the equilibria. The authors compare different dynamics like gradient descent and standard replicator in 2x2 coordination games and provide some theoretical and empirical results.\n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewer 1 mentions that while the paper initiates the study of average performance, prior works like PP16 have already started this in restricted settings. They also note that the formal results are partial and there are minor issues like citation formatting and terminology. Reviewer 2 points out that the paper doesn't sufficiently discuss PP16, which is a key prior work, and the results are incremental, especially since the main theorems are limited to 2x2 cases. They also mention some undefined terms and lack of comparison with PP16. Reviewer 3 agrees that the problem is interesting but the results are preliminary, with theorems only covering specific cases and no formal proofs for general q or higher dimensions. They also note the abstract overstates the scope. Reviewer 4 thinks the problem is important but the results are limited to 2x2 games and might be better suited for a more specialized conference like EC. However, they still recommend acceptance after some clarifications.\n\nThe authors addressed some points in their response, like improving the abstract, fixing citations, and clarifying comparisons with PP16. They argue that their work extends PP16 by generalizing to larger games and comparing different dynamics, which PP16 didn't do. They also mention that initiating this line of research is a contribution, even if results are partial.\n\nNow, considering top-tier conference standards, the key factors are novelty, significance, and the depth of contributions. The paper's main issue seems to be that while the problem is important, the results are limited to 2x2 games and the theoretical contributions are incremental compared to PP16. Reviewers 2 and 3 highlight that the work is somewhat derivative and the formal results are not fully developed beyond specific cases. However, the authors have made efforts to address these points, especially in the rebuttal by adding comparisons with PP16 and clarifying their contributions.\n\nThe technical novelty is marginal according to some reviewers, but others see the problem's importance. The empirical results, though limited, do provide a starting point. Since the authors have revised the abstract and citations, and clarified their contributions relative to PP16, maybe the paper can be accepted as a step forward, even if not groundbreaking. The fact that Reviewer 4 still recommends acceptance after discussion, albeit lowering their score, suggests that the community might value the initiation of this research direction despite its current limitations. Top conferences do accept papers that open new avenues even if they're not fully comprehensive, provided the problem is significant and the approach is promising. The authors' response also shows they're addressing the reviewers' concerns, which is a positive sign.\n\nFinal Decision: Accept\n</think>\n\nThe paper addresses an important problem in game theory and learning dynamics by exploring the average-case performance of no-regret algorithms, specifically focusing on q-replicator dynamics (QRD). While the reviewers highlight several limitations, such as incremental contributions relative to prior work (e.g., PP16), restricted scope to 2x2 games, and partial theoretical results, the authors have made efforts to address these concerns. Key points include:\n\n1. **Novelty and Significance**: The work initiates a systematic study of average performance metrics (e.g., average price of anarchy) for QRD, which is a valuable direction despite prior partial efforts. The comparison between gradient descent and replicator dynamics in 2x2 games, and the extension to general QRD dynamics via experiments, adds incremental but meaningful insights. The problem’s importance is acknowledged by reviewers, particularly the need to move beyond worst-case analysis.\n\n2. **Technical Contributions**: The theoretical results (e.g., Theorem 4.4) and empirical analysis in 2x2 coordination games are solid, albeit limited in scope. The authors clarify that their work generalizes PP16 by expanding to broader game classes and comparing dynamics, which prior work did not do. While the results are partial, they lay groundwork for future extensions.\n\n3. **Clarity and Addressing Feedback**: The authors improved the abstract, citations, and comparisons with PP16 in their response, addressing key reviewer concerns. They also clarified ambiguities (e.g., notation, monotonicity claims) and defended the significance of their contributions as foundational for future research.\n\n4. **Reviewer Consensus**: Despite mixed initial scores, reviewers 2 and 3 raised valid concerns about incrementalism and scope, but the revised version and rebuttal appear to have mitigated these. Reviewer 4’s conditional acceptance and the post-discussion adjustments suggest the paper meets the bar for a top-tier conference, especially given its potential to inspire further work.\n\n**Final Decision: Accept**  \nThe paper addresses a significant problem, provides a clear starting point for analyzing average performance in learning dynamics, and sufficiently addresses reviewer concerns through revisions. While the results are limited in scope, the problem’s importance and the foundational nature of the contribution justify acceptance at a top-tier conference."
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "Okay, let's tackle this. The paper is about a new aggregation method for federated learning called FedPA, aiming to address both client drift and period drift. The reviewers have given mixed feedback. Let me go through each point step by step.\n\nFirst, the main strengths mentioned are the identification of period drift as a problem and the novel aggregation strategy using meta-learning. Reviewer 1 and 2 note that the paper introduces period drift, which is a new angle, but they also point out that the theoretical and experimental support for distinguishing period drift from client drift is lacking. That's a problem because without clear evidence, the contribution might not be justified.\n\nLooking at the weaknesses: Reviewer 1 says the paper doesn't do enough to differentiate between client and period drift. They also mention the proxy dataset issue, which limits applicability and similarity to existing methods like FedET and DS-FL. The authors didn't compare with these, which is a big oversight. Plus, the experiments are on simple datasets (EMNIST and MovieLens) and models, so the results might not be convincing for a top-tier conference. The lack of ablation studies and code also hurt reproducibility.\n\nReviewer 2 points out that the paper doesn't explain why meta-learning is better than other approaches like temporal regularization. The dynamic system analogy is confusing and not well-used. Also, the architecture choice (two neural networks) isn't justified. The experiments show better performance, but without comparisons to newer or relevant works, it's hard to see the advantage.\n\nReviewer 3 is concerned about privacy issues since the method requires access to client model updates, conflicting with FL's privacy principles. The method's ad-hoc nature and lack of ablation studies are major issues. The experimental results on FEMNIST are lower than expected, and hyperparameters aren't well-explained. These points weaken the empirical significance.\n\nReviewer 4 mentions writing issues and the need for more datasets and server overhead analysis. The method's scalability is in question, which is crucial for FL applications. The lack of code again is a problem for reproducibility.\n\nAll reviewers agree on the need for better experiments, comparisons with more recent methods, and clearer explanations. The technical novelty is considered marginal, and the empirical results aren't strong enough. The proxy dataset requirement and privacy concerns are significant drawbacks. The paper's writing and reproducibility issues add to the problems.\n\nConsidering top-tier standards, the paper needs substantial improvements. The core idea isn't novel enough, and the experiments don't convincingly show superiority over existing methods. Without addressing the reviewers' concerns about period drift's definition, comparisons, ablation studies, and privacy, it's unlikely to meet the high bar. The lack of code and unclear implementation details also hinder acceptance.\n\nFinal Decision: Reject\n</think>\n\nThe paper proposes FedPA, a meta-learning-based aggregation method for federated learning (FL) to address both client drift and a newly identified \"period drift.\" While the idea of tackling period drift is novel, the paper faces significant shortcomings that undermine its contributions and credibility for a top-tier conference. Key issues include:\n\n### **Technical and Empirical Weaknesses**\n1. **Lack of Clear Differentiation Between Drifts**: The paper fails to provide theoretical or experimental evidence distinguishing period drift from existing client drift. Reviewers 1 and 2 note that the problem is not well-defined, and the motivation for FedPA’s design is not rigorously tied to period drift.\n2. **Weak Experimental Rigor**:\n   - **Insufficient Baselines**: The paper does not compare with relevant methods like FedET or DS-FL, which use proxy datasets or knowledge distillation, undermining claims of novelty and superiority.\n   - **Simplistic Datasets/Models**: Experiments on EMNIST and MovieLens are insufficient for a top-tier venue. Performance on FEMNIST (e.g., <80% accuracy) lags behind prior work (e.g., Adaptive FedOpt), raising questions about effectiveness.\n   - **No Ablation Studies**: Critical analyses of proxy dataset distribution, aggregator model size, or architecture choices (e.g., why two neural networks?) are missing, making it hard to assess the method’s robustness.\n3. **Privacy Concerns**: Reviewer 3 highlights that FedPA requires access to raw client model updates, conflicting with FL’s privacy principles (e.g., SecAgg, differential privacy). This is a major drawback for practical FL systems.\n\n### **Methodological and Conceptual Flaws**\n- **Ad-Hoc Design**: The use of a proxy dataset is a strong assumption and limits real-world applicability. The meta-learning framework’s connection to period drift is under-explained, and alternatives (e.g., temporal regularization) are not adequately addressed.\n- **Unclear Technical Advantages**: Reviewer 2 questions why meta-learning is better than simpler approaches (e.g., scalar weights for aggregation). The dynamic systems analogy in Section 3 is poorly motivated and adds confusion.\n\n### **Writing and Reproducibility Issues**\n- **Clarity and Grammar**: Multiple reviewers note poor writing quality, grammatical errors, and inconsistent citations. The paper’s prose is often unclear, especially in defining key concepts like period drift.\n- **No Code or Reproducibility**: The absence of code and detailed hyperparameter choices (e.g., communication rounds, learning rates) makes replication impossible, violating reproducibility standards for top conferences.\n\n### **Reviewer Consensus**\n- **Novelty and Significance**: All reviewers agree the contributions are marginal. Technical novelty is limited (e.g., similarity to knowledge distillation), and empirical results lack depth.\n- **Recommendations**: Three reviewers lean toward rejection due to insufficient justification of period drift, weak experiments, and unresolved privacy concerns. Only Reviewer 4 suggests minor improvements but acknowledges writing and scalability issues.\n\n### **Conclusion**\nWhile the paper introduces an interesting angle (period drift), its execution falls short of top-tier standards. Critical gaps in theoretical grounding, experimental rigor, and practical relevance, combined with reproducibility barriers and privacy issues, outweigh its potential contributions. The authors must address these concerns in a major revision before the work can be considered competitive for a leading conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. The paper introduces DeepSeaProbLog, an extension of existing neural-symbolic systems like DeepProbLog to handle continuous variables. The main contributions are supporting both discrete and continuous variables, implementing inference and learning, and showing experiments. \n\nLooking at the reviewers' comments, there's a mix of positive and critical points. Reviewers 1, 3, 4, and 7 seem to acknowledge the novelty and technical soundness but point out issues with empirical evaluation and clarity. Reviewers 2, 5, 6, and 7 mention concerns about the experiments being too limited (only two toy tasks), lack of comparisons in purely discrete settings, and some technical gaps like handling discrete variables properly. \n\nThe authors addressed some points, like explaining how discrete variables are handled via exact inference, and added more details in the response. They also mentioned adding experiments with real data (meteorological) and more generated examples. However, some key issues remain: the empirical evaluation is still seen as insufficient by several reviewers, especially the lack of comparisons with baselines in discrete settings. Reviewer 6 questions the technical novelty, suggesting that combining known techniques might not be sufficiently innovative. \n\nThe paper's main strength is addressing a clear gap in handling continuous variables in neural probabilistic logic programming. The technical contributions, like differentiable WMI and the implementation merging symbolic and neural methods, are significant. However, the experiments are underwhelming, with only a few tasks and limited ablation studies. The authors tried to mitigate this by mentioning additional data and examples, but the reviewers still feel the evaluation is weak. \n\nConsidering top-tier conference standards, the work is novel and technically sound but the empirical validation is a短板. The reviewers' consensus seems to lean towards acceptance with major revisions, but since the decision here is to accept or reject, and given that some reviewers (like 5 and 6) rate the novelty lower, it's a tough call. However, the problem addressed is important, and the technical contributions are solid. The authors have addressed some concerns, and the paper's potential impact might outweigh the current empirical shortcomings, especially if they can strengthen the experiments in the final version. \n\nWait, but the reviewers' scores on empirical significance are low (some at 2 or 3). For a top conference, empirical evaluation is crucial. The authors didn't add more benchmarks beyond what's already there, just clarified existing ones. Maybe the paper is borderline but leans towards acceptance because the technical novelty is there, even if the experiments are not perfect. Reviewer 4 and 3 gave technical significance as 4, which is high. The majority of reviewers (except 5 and 6) seem to support acceptance with the main issue being empirical. Since the authors have addressed some points and the problem is significant, I think it should be accepted with the expectation that they improve experiments further.\n</think>\n\nFinal Decision: Accept\n\nThe paper introduces DeepSeaProbLog, a novel extension of neural-symbolic probabilistic logic programming to handle continuous and mixed domains, addressing a critical gap in existing methods. The technical contributions—such as linking semantics to weighted model integration (WMI), enabling differentiable WMI, and bridging symbolic and neural methods—are significant and theoretically sound. While reviewers raised valid concerns about empirical evaluation (e.g., limited benchmarks, lack of comparisons in purely discrete settings, and underdeveloped ablation studies), the authors provided clarifications and addressed several points, including discrete variable handling and additional examples. The problem tackled is important, and the framework's potential impact on enabling expressive modeling in real-world scenarios justifies acceptance. However, the authors must strengthen empirical validation in the final version, particularly by expanding experiments and comparisons, to fully meet top-tier standards. The balance of novelty, technical merit, and clear path for improvement tips the decision toward acceptance."
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "Okay, let's tackle this. I need to decide whether the paper \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" should be accepted or rejected for a top-tier conference like ICLR. Let me start by going through all the provided information step by step.\n\nFirst, the paper's abstract mentions that they're introducing a new dataset called RuDar. The key points are that it includes 3D radar echo observations, covers multiple climate zones in Russia, has high temporal and spatial resolution, and includes various meteorological variables like precipitation intensity, reflectivity, and radial velocity at different altitudes. They also mention evaluating baseline models and uncertainty quantification. The dataset is publicly available, which is a plus.\n\nLooking at the reviewer comments, there's a mix of positive and negative points. Let's break down each reviewer's feedback.\n\nReviewer 1:\n- They like the dataset but note that the methods are low-cost (maybe not state-of-the-art models). They ask about performance comparisons with heavy models like transformers and the dataset's usability compared to advanced forecasting systems.\n- Clarity is good, but technical and empirical novelty are low. The dataset is attractive but the domain relevance and novelty are limited. The correctness is high, but significance is only marginal.\n\nReviewer 2:\n- Strengths include the dataset's multi-dimensional data and coverage across climate zones. However, weaknesses are the baseline methods being too simple, lack of experiments for other ML tasks (like anomaly detection), no cross-evaluation with other regions, insufficient related work, and using only MSE as a metric. They also want previous methods' results on their dataset.\n- Clarity and reproducibility are okay, but they suggest a journal instead of a top conference. Correctness is mostly okay but some minor issues. Technical significance is marginal, empirical is also low.\n\nReviewer 3:\n- The dataset is rich and the comparison with other datasets is clear. They compared many models. The uncertainty study is done via seed variation. But weaknesses include the dataset being too coarse (2km grid might be limiting for precipitation tasks), no methodological novelties, and simple uncertainty estimation.\n- The paper is well-written but not novel enough for ICLR. The dataset is important but not groundbreaking. Correctness is high, technical significance low, empirical is somewhat new but aspects exist in prior work.\n\nAuthor responses addressed some points. They added Earthformer results (a transformer model) in the rebuttal, planning more in the camera-ready. They also explain that extending to other ML tasks or cross-domain evaluation is beyond scope, and justify their metrics (MSE and F1). They acknowledge the dataset's coarseness but highlight the unique combination of variables not present elsewhere.\n\nNow, considering the criteria for a top-tier conference. Top conferences usually require significant technical or empirical contributions. The dataset's novelty is in adding 3D data and multiple variables, which existing datasets lack. That's a solid contribution. However, the reviewers point out that the methods (baselines) are standard and not novel. The authors did add Earthformer results, but that's a recent model they're applying to their dataset, not their own method.\n\nThe main contribution here is the dataset. For a dataset paper to be accepted at a top conference, it needs to be a substantial improvement over existing datasets, enabling new research directions. RuDar's inclusion of 3D radar data and multiple variables (reflectivity, velocity, orography) is a clear plus. The coverage over multiple climate zones and the two-year span also add value. The authors mention it can be used for other tasks like uncertainty estimation and data shift studies, which is good.\n\nHowever, the reviewers are critical about the lack of methodological innovation. The paper is mostly about the dataset and baseline evaluations. Reviewer 3 says it's too similar to other datasets, but the authors counter that the combination of variables is unique. The spatial resolution is coarser than some (like 2km vs maybe finer grids elsewhere?), but that's a hardware limitation. The dataset's uniqueness in variables might outweigh that.\n\nThe baselines are standard, but the authors did expand with Earthformer, which is a strong model. The fact that they're providing a benchmark with existing and new models is helpful. The empirical results, while not earth-shattering, do show the dataset's utility.\n\nThe main issues are the lack of methodological novelty and the dataset's coarseness. But for a dataset paper, the key is the dataset's contribution. If the dataset is sufficiently novel and impactful, that can be enough. The authors argue that the combination of variables isn't present elsewhere, which is a valid point. The reviewers' low scores on technical novelty might be because it's a dataset paper, not a method paper. However, top conferences do accept dataset papers if the dataset is transformative.\n\nLooking at the scores: Reviewer 1 gave technical novelty 2 (marginal), Reviewer 2 also 2, Reviewer 3 2. Empirical novelty similarly low. But the dataset's quality and potential impact might tip the balance. The clarity and reproducibility are good. The authors addressed some concerns, like adding Earthformer results.\n\nAnother point: The dataset is publicly available, which is a plus for reproducibility and community impact. The fact that it's used for benchmarking and can be applied to other tasks (even if not demonstrated here) adds to its value.\n\nThe main counterarguments are that the methods are not novel and the dataset's resolution might be a limitation. But the dataset's unique features and the thorough evaluation (even with standard models) make it a valuable resource. Since the paper's primary contribution is the dataset, and it meets the criteria of being a significant addition to the field, I think it should be accepted. The reviewers' suggestions for a journal might be due to the perception that the dataset isn't groundbreaking enough for a conference, but given the dataset's unique aspects and the benchmarks provided, it's borderline. However, considering that top conferences do accept impactful dataset papers, and the dataset does fill a gap, I lean towards acceptance.\n</think>\n\nThe paper introduces the RuDar dataset, which addresses a critical gap in precipitation nowcasting by incorporating 3D radar observations (reflectivity, radial velocity, and orography) across multiple climate zones in Russia. While the dataset's technical novelty is marginal compared to existing works, its unique combination of variables (e.g., altitude-specific reflectivity and radial velocity) and geographical/seasonal diversity represents a meaningful advancement. The dataset's public availability and potential for enabling future research (e.g., uncertainty estimation, anomaly detection) further strengthen its value. \n\nHowever, the paper’s methodological contributions are limited to standard baselines and a simple uncertainty analysis, with no novel algorithms or models proposed. Reviewers noted that the dataset’s spatial resolution (2 km) and temporal resolution (10 minutes) are constrained by hardware, which may limit its utility for fine-grained precipitation modeling. The authors addressed some concerns by adding Earthformer results and clarifying their evaluation metrics, but the core contribution remains the dataset itself. \n\nFor a top-tier conference, the dataset’s significance must outweigh the lack of methodological innovation. While the dataset is not transformative (e.g., it does not fundamentally redefine precipitation nowcasting), it provides a unique and comprehensive resource that expands the scope of available data for ML-driven weather forecasting. The thorough benchmarking and clear reproducibility further enhance its utility. Despite the reviewers’ reservations about novelty, the dataset’s uniqueness and potential impact justify its acceptance, particularly given the growing interest in Earth science applications in machine learning.\n\nFinal Decision: Accept"
    }
  ]
}