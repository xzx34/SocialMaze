{
  "summary": {
    "model": "gpt-4o",
    "total_papers": 100,
    "correct_predictions": 90,
    "accuracy": 90.0,
    "confusion_matrix": [
      [
        71,
        5
      ],
      [
        5,
        19
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 93.42105263157895,
        "precision": 93.42105263157895
      },
      "Accept": {
        "total": 24,
        "recall": 79.16666666666666,
        "precision": 79.16666666666666
      }
    }
  },
  "results": [
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to hierarchical reinforcement learning (HRL) for multi-agent systems, focusing on skill coordination. The authors propose a modular framework that first trains sub-skills for each end-effector and then coordinates these skills using a meta-policy. The paper addresses an important problem in multi-agent reinforcement learning (MARL) and demonstrates promising results in complex manipulation tasks.\n\n**Strengths:**\n\n1. **Novelty and Impact:** The paper introduces a method for coordinating pre-learned skills in a multi-agent setting, which is a significant contribution to the field. The approach is innovative in leveraging skill behavior diversification to enhance coordination.\n\n2. **Empirical Results:** The authors provide empirical evidence showing that their method outperforms existing baselines in complex tasks. The inclusion of videos and code enhances the reproducibility and transparency of the results.\n\n3. **Addressing Reviewer Concerns:** The authors have addressed most of the reviewers' concerns, such as providing additional baselines, clarifying the role of temporal abstraction, and discussing the importance of expert knowledge in skill discovery.\n\n4. **Comprehensive Author Response:** The authors have provided detailed responses to the reviewers' questions, demonstrating a thorough understanding of the limitations and potential improvements of their work.\n\n**Weaknesses:**\n\n1. **High Variance in Results:** Reviewer 1 noted high variance in the performance of the method, which the authors addressed by running more seeds and applying a moving average. However, this issue might still indicate some instability in the method.\n\n2. **Dependence on Predefined Subtasks:** The method relies on predefined subtasks, which may limit its applicability to tasks that can be easily decomposed. This was a concern raised by Reviewer 3, and while the authors provided additional baselines, the reliance on expert knowledge remains a limitation.\n\n3. **Clarity and Notation:** Reviewer 1 found the notations inconsistent and confusing, which the authors have attempted to address. However, clarity in presentation is crucial for a top-tier conference, and any remaining issues in this area could detract from the paper's impact.\n\n4. **Lack of Strong Baselines:** While the authors added baselines to address reviewer concerns, the absence of a strong baseline using DIAYN only (without predefined subtasks) limits the ability to fully assess the importance of expert knowledge.\n\n**Conclusion:**\n\nThe paper presents a significant contribution to the field of hierarchical reinforcement learning and multi-agent systems. The authors have addressed most of the reviewers' concerns and provided a comprehensive response. Despite some limitations, such as reliance on predefined subtasks and potential issues with variance, the novelty and empirical results make a compelling case for acceptance. The paper is likely to stimulate further research in skill coordination and multi-agent learning.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"SGD Learns One-Layer Networks in WGANs\" presents an analysis of the convergence of stochastic gradient descent-ascent (SGD) in the context of Wasserstein GANs (WGANs) with a one-layer generator network. The authors claim to provide the first result showing convergence to global equilibrium points for non-linear generators, focusing on simplified discriminator classes, specifically linear and quadratic discriminators.\n\n### Strengths:\n1. **Novelty in Analysis**: The authors tackle the challenging problem of analyzing the convergence of gradient descent-ascent in GAN settings, which is a non-trivial task due to the non-convex nature of the generator's minimization problem.\n2. **Theoretical Contributions**: The paper provides theoretical results on the convergence of SGD to global solutions and discusses sample complexity, which are important contributions to the understanding of GAN training dynamics.\n3. **Clarity and Structure**: The paper is well-written, and the theoretical analysis appears to be valid and complete, as noted by the reviewers.\n\n### Weaknesses:\n1. **Simplified Discriminator**: The primary criticism from reviewers is the use of overly simplified discriminators (linear and quadratic), which do not align well with practical WGAN implementations that typically use more complex neural network discriminators. This simplification limits the applicability and impact of the results.\n2. **Over-Claiming Contributions**: Reviewers noted that the paper's title and claims might be misleading, as the results are specific to very simplified settings that do not generalize well to standard WGAN scenarios.\n3. **Lack of Practical Relevance**: The choice of simple discriminators is seen as a drawback because it does not capture the true nature of WGAN training, which involves more complex interactions between generator and discriminator.\n4. **Insufficient Addressing of Concerns**: Despite the authors' responses, reviewers felt that key concerns, such as the applicability of the quadratic discriminator to WGANs and the lack of exploration of more complex discriminator architectures, were not adequately addressed.\n\n### Author Responses:\nThe authors defend their choice of discriminator by arguing that it simplifies the analysis and maintains sufficient distinguishing power for the generator class considered. They emphasize that their goal is to understand the dynamics of SGD in a simplified setting, which they argue is a strength rather than a weakness. However, the reviewers remain unconvinced, suggesting that the simplifications undermine the paper's relevance to real-world WGAN training.\n\n### Conclusion:\nWhile the paper makes a theoretical contribution to the understanding of SGD dynamics in a simplified GAN setting, the significant gap between the theoretical model and practical WGAN implementations limits its impact and relevance for a top-tier conference. The reviewers' concerns about the oversimplification of the discriminator and the misleading nature of the paper's claims are substantial and have not been sufficiently addressed in the author responses.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper presents a novel method for out-of-distribution (OOD) image detection using the Normalized Compression Distance (NCD), which is claimed to work without requiring retraining of the model or using OOD samples for validation. This is a significant contribution given the constraints, as it aims to make OOD detection more practical for real-world applications.\n\n**Strengths:**\n1. **Novelty and Practicality:** The proposed method, MALCOM, introduces a new approach to OOD detection that does not rely on retraining or OOD samples for validation, addressing a practical limitation in existing methods.\n2. **Comprehensive Evaluation:** The authors have conducted extensive experiments and comparisons with state-of-the-art methods, demonstrating the effectiveness of their approach under the specified constraints.\n3. **Response to Feedback:** The authors have addressed reviewer concerns by conducting additional experiments, including comparisons with methods that use adversarial samples for validation, and have shown that their method performs competitively.\n\n**Weaknesses:**\n1. **Justification of Constraints:** Reviewers have raised concerns about the necessity and frequency of the constraints imposed by the paper. While the authors have provided some justification, it remains unclear how often these constraints are encountered in practice.\n2. **Comparison with Broader Methods:** The paper primarily compares MALCOM with methods under similar constraints. Reviewer 2 suggests that comparisons with less constrained methods could provide a better understanding of the method's relative standing.\n3. **Complexity of Experiments:** Reviewer 3 notes that the experiments could have been conducted in more complex settings, such as safety-critical applications, to better demonstrate the method's significance and efficiency.\n4. **Writing and Presentation:** There are issues with the clarity and grammatical correctness of the writing, which could hinder understanding. The authors have made efforts to improve this, but some concerns remain.\n\n**Reviewer Consensus:**\n- Reviewers generally find the approach interesting and novel but have concerns about the justification of constraints and the fairness of comparisons.\n- There is a consensus that the paper requires further revision and improvement before it is ready for publication in a top-tier conference.\n\n**Author Response:**\n- The authors have made efforts to address the reviewers' concerns by conducting additional experiments and improving the paper's presentation. They have also provided a detailed explanation of their method's effectiveness and the rationale behind their constraints.\n\n**Conclusion:**\nWhile the paper presents a novel and potentially impactful method for OOD detection, the concerns about the justification of constraints, the need for broader comparisons, and the clarity of presentation suggest that the paper is not yet ready for acceptance at a top-tier conference. The authors have made commendable efforts to address feedback, but further revisions are necessary to fully address the reviewers' concerns and strengthen the paper's contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" presents a novel approach to Mahalanobis metric learning by formulating it as an optimization problem aimed at minimizing the number of violated similarity/dissimilarity constraints. The authors propose a fully polynomial time approximation scheme (FPTAS) for this problem, leveraging tools from linear programming in low dimensions.\n\n### Strengths:\n1. **Novelty and Contribution**: The authors claim that their algorithm is the first with a provable guarantee on the number of violated constraints for arbitrary inputs, which is a significant contribution to the field. This approach differs from previous methods that focus on minimizing an error function rather than the number of violations directly.\n   \n2. **Theoretical Foundation**: The use of LP-type problems and the application of geometric approximation algorithms provide a solid theoretical foundation for the proposed method.\n\n3. **Experimental Results**: The paper includes experimental results on both synthetic and real-world datasets, demonstrating the algorithm's performance, especially in adversarial settings.\n\n4. **Parallelizability**: The algorithm is fully parallelizable, which is a practical advantage for handling large datasets.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Several reviewers noted issues with clarity, particularly in the proofs and definitions. The authors acknowledged these and promised revisions, but the current state of the paper may hinder understanding.\n\n2. **State-of-the-Art Comparison**: While the authors argue that ITML and LMNN are the state-of-the-art, reviewers suggested that more recent works should be discussed and compared. The authors provided some clarification in their response, but the paper could benefit from a more comprehensive comparison.\n\n3. **Experimental Evaluation**: Reviewer 3 pointed out that the experimental results do not show a clear improvement in accuracy over existing methods. The authors argue that their method is robust to adversarial inputs, but this advantage should be more clearly demonstrated.\n\n4. **Theoretical Guarantees**: While the authors claim a provable guarantee on the number of violated constraints, the exact computational hardness of the problem is not addressed, which could be a critical aspect for a top-tier conference.\n\n5. **Conclusion Missing**: The absence of a conclusion section is a significant oversight, as it leaves the paper without a clear summary of findings and implications.\n\n### Conclusion:\nThe paper presents a potentially impactful contribution to the field of metric learning by introducing a novel approach with theoretical guarantees. However, the issues with clarity, lack of comprehensive state-of-the-art comparison, and the absence of a conclusion section are significant drawbacks. While the authors have addressed some concerns in their response, the paper in its current form may not meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" presents a method for defending against adversarial attacks using a modified autoencoder and GAN framework. The authors propose a novel approach to reduce the computational cost of existing methods like Defense-GAN while maintaining or improving performance against adversarial attacks.\n\n**Strengths:**\n1. **Novelty and Contribution**: The paper introduces a method that reduces the computational cost of Defense-GAN by using an autoencoder to provide a better initialization for the latent space optimization. This is a meaningful contribution as it addresses a significant limitation of Defense-GAN.\n2. **Experimental Results**: The authors provide experimental results on MNIST and Fashion-MNIST datasets, showing that their method is computationally cheaper and performs comparably or better than Defense-GAN.\n3. **Response to Reviewers**: The authors have addressed some of the reviewers' concerns by including additional experiments with PGD adversarial training and results on the CelebA dataset, demonstrating the method's applicability to more complex datasets.\n\n**Weaknesses:**\n1. **Incremental Contribution**: Reviewer 2 points out that the contribution is incremental, as it mainly refines an existing method (Defense-GAN). The paper lacks a comparison with other state-of-the-art adversarial defenses like TRADES, which could provide a more comprehensive evaluation.\n2. **Writing and Clarity**: Multiple reviewers noted that the paper's writing is difficult to follow, with unclear explanations and numerous typos. This affects the paper's readability and the clarity of the proposed method.\n3. **Limited Dataset Evaluation**: The initial evaluation was limited to simple datasets (MNIST and Fashion-MNIST), which may not fully demonstrate the method's effectiveness. Although the authors added results on CelebA, the lack of CIFAR-10 results is a notable omission.\n4. **Technical Concerns**: Reviewers raised concerns about the reliance on the autoencoder's ability to detect adversarial samples and the potential vulnerability to white-box attacks. The authors' response acknowledges these limitations but does not fully resolve them.\n5. **Page Limit and Presentation**: The paper exceeds the recommended page length and contains presentation issues, which detract from its overall quality.\n\n**Conclusion:**\nWhile the paper presents an interesting approach to reducing the computational cost of adversarial defenses, the contribution is considered incremental, and the paper suffers from significant clarity and presentation issues. The authors have made efforts to address some reviewer concerns, but the paper still lacks comprehensive evaluations against state-of-the-art methods and on more complex datasets. Given the standards of a top-tier conference, these weaknesses outweigh the strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"NORML: Nodal Optimization for Recurrent Meta-Learning\" presents a novel meta-learning framework that utilizes an LSTM-based meta-learner for neuron-wise optimization. The paper aims to address scalability issues in meta-learning by proposing a method where the number of meta-learner parameters increases linearly with the learner parameters, potentially allowing it to scale to larger networks.\n\n**Strengths:**\n1. **Scalability Focus:** The paper addresses an important issue in meta-learning, which is the scalability of meta-learners when dealing with large networks.\n2. **Experimental Evaluation:** The paper includes experiments on few-shot learning benchmarks such as Mini-ImageNet and Omniglot, which are standard datasets for evaluating meta-learning methods.\n\n**Weaknesses:**\n1. **Limited Novelty:** As pointed out by all reviewers, the proposed method is an incremental variation of existing work, particularly the Meta-Learner LSTM by Ravi & Larochelle. The novelty of the approach is not sufficiently demonstrated.\n2. **Lack of Direct Comparisons:** The paper fails to provide direct comparisons with the most relevant baseline, the Meta-Learner LSTM, under the same experimental conditions. This makes it difficult to assess the true benefit of the proposed method.\n3. **Missing Baselines:** Reviewer 3 highlights the absence of comparisons with more appropriate versions of MAML and MAML++, which would provide a more comprehensive evaluation of the method's performance.\n4. **Limited Applicability:** The method is specifically designed for fully connected networks, which limits its applicability given that most few-shot learning models use convolutional architectures.\n5. **Inaccurate Descriptions:** There are inaccuracies in the description of prior work, which undermines the credibility of the paper's claims.\n6. **Writing and Presentation:** Reviewer 2 notes significant issues with the writing quality, including grammatical errors and poor flow, which detract from the clarity and professionalism expected at a top-tier conference.\n\n**Conclusion:**\nThe paper presents an interesting approach to addressing scalability in meta-learning, but it falls short in several critical areas. The novelty is limited, and the lack of direct comparisons with the most relevant baselines makes it difficult to assess the true contribution of the work. Additionally, the writing quality and inaccuracies in the description of related work further weaken the submission. Given these issues, the paper does not meet the standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper presents a novel approach to multi-task learning by modeling multi-level task dependencies using an attention mechanism. The authors argue that their method can capture both general and data-specific task dependencies, particularly in hierarchical data like text and graphs. They claim improvements over existing methods and propose a decomposition method to reduce parameter complexity.\n\n### Strengths:\n1. **Novelty**: The idea of modeling task dependencies at multiple levels, particularly for hierarchical data, is a novel contribution. The authors propose a method to capture both general and data-specific dependencies, which could potentially improve performance in multi-task learning scenarios.\n2. **Complexity Reduction**: The decomposition method to reduce parameter complexity from \\(O(T^2)\\) to \\(O(T)\\) is a significant contribution, addressing scalability issues in multi-task learning.\n3. **Potential Impact**: If successful, the approach could lead to better performance in multi-task learning applications, particularly those involving text and graph data.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Multiple reviewers noted that the paper is difficult to follow due to grammatical errors and unclear explanations. This significantly affects the readability and comprehension of the paper.\n2. **Lack of Strong Baselines**: The reviewers pointed out the need for comparisons with relevant baselines, particularly those involving attention mechanisms in multi-task learning. The absence of such comparisons weakens the experimental validation of the proposed method.\n3. **Missing Related Work**: Reviewer 4 highlighted the lack of discussion and comparison with critically related prior work, such as the \"Taskonomy\" paper. This omission raises concerns about the novelty and positioning of the work within the existing literature.\n4. **Experimental Setup**: There are concerns about the experimental setup, such as the potential overlap of training and test data across tasks and the need for additional evaluations to isolate the contribution of the attention mechanism.\n5. **Technical Details**: Several technical aspects, such as the definition and extraction of the task dependency matrix and the implementation of the \"Transfer Block,\" are not clearly explained, making it difficult to assess the technical soundness of the approach.\n\n### Author Response:\nThe authors have attempted to address the reviewers' concerns by clarifying distinctions from prior work, explaining technical details, and promising additional experiments. However, the responses indicate that significant revisions are needed to address the major issues raised by the reviewers.\n\n### Conclusion:\nWhile the paper presents an interesting approach with potential contributions to the field of multi-task learning, the current version suffers from significant issues in clarity, experimental validation, and positioning within the existing literature. The paper requires substantial revisions to address these concerns, including a thorough proof-reading, stronger experimental comparisons, and a clearer discussion of related work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning disentangled representations using a progressive learning strategy in a hierarchical fashion. The authors propose an extension to the Variational Ladder Autoencoder (VLAE) that incrementally learns representations from high- to low-levels of abstraction, aiming to improve disentanglement.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a fresh idea of combining progressive learning with hierarchical representation learning, which is not extensively explored in the literature. The approach is innovative and could potentially open new directions in disentangled representation learning.\n\n2. **Experimental Validation:** The authors provide both qualitative and quantitative evidence supporting their claims. They conduct experiments on benchmark datasets and introduce a new metric (MIG-sup) to evaluate disentanglement, which complements existing metrics.\n\n3. **Rebuttal and Improvements:** The authors have addressed many concerns raised by the reviewers through additional experiments and clarifications. They provided ablation studies, improved explanations, and quantitative comparisons that strengthened the paper.\n\n4. **Clarity and Presentation:** The paper is generally well-written and easy to follow. The authors have made efforts to clarify their methodology and results in response to reviewer feedback.\n\n**Weaknesses:**\n\n1. **Initial Reviewer Concerns:** Some reviewers initially expressed concerns about the clarity of the paper's purpose, the relationship between hierarchical and disentangled representations, and the effectiveness of the proposed method compared to existing models like VLAE.\n\n2. **Metric Limitations:** There were concerns about the practicality of the proposed metric, MIG-sup, which requires ground truth for generative factors, potentially limiting its applicability.\n\n3. **Comparison with Related Work:** The paper could benefit from a more thorough comparison with related work, particularly in discussing similarities and differences with existing methods like those in [3].\n\n4. **Robustness and Hyperparameter Sensitivity:** The method's sensitivity to hyperparameters, such as beta, was noted, and while the authors addressed this to some extent, it remains a point of consideration.\n\n**Conclusion:**\n\nThe authors have made significant efforts to address the reviewers' concerns, and the additional experiments and clarifications provided in the rebuttal have strengthened the paper. The novelty of the approach, combined with the promising results and the authors' responsiveness to feedback, make this paper a valuable contribution to the field. While there are areas for improvement, particularly in terms of metric applicability and robustness, the paper meets the standards of a top-tier conference in terms of innovation and potential impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" presents a novel approach to reduce training time for CNNs by employing a multi-precision strategy. The authors propose a precision-switching mechanism based on gradient diversity to optimize the training process. While the paper addresses an important problem in deep learning, there are several critical issues that need to be considered before making a decision on its acceptance.\n\n### Strengths:\n1. **Novelty**: The paper introduces a new approach to quantized training by using multiple precision levels, including low-precision fixed-point representations, which is a step forward from existing mixed-precision methods.\n2. **Potential Impact**: The proposed method could significantly reduce training time and energy consumption, which is valuable for large-scale CNNs.\n3. **Generalisability**: The authors claim that the method is agnostic to network and dataset, which, if true, would make it widely applicable.\n\n### Weaknesses:\n1. **Clarity and Explanation**: Multiple reviewers pointed out that the explanation of the precision-switching mechanism and the choice of parameters is unclear. The authors have attempted to address this in their response, but the core understanding of why and how the switching mechanism works remains somewhat heuristic and not fully justified.\n2. **Experimental Results**: The results do not consistently show significant improvements over state-of-the-art methods. Reviewer 2 noted that the proposed strategy does not perform well in some scenarios, and the improvements in accuracy and speedup are not always substantial.\n3. **Reproducibility**: There are concerns about the clarity of the quantization scheme and the lack of detailed explanation for reproducing the results. The authors have added some details in their response, but it remains to be seen if these are sufficient.\n4. **Presentation**: There are issues with the presentation of figures and tables, which were noted by reviewers. The authors have addressed this by adding larger figures in the appendix, but this does not fully resolve the issue of clarity in the main text.\n5. **Justification of Choices**: The choice of precision levels and the switching mechanism lacks a strong theoretical foundation. The authors have provided some empirical justifications, but these are not entirely convincing.\n\n### Author Response:\nThe authors have made efforts to address the reviewers' concerns by adding more explanations and revising sections of the paper. They have clarified some points about the switching mechanism and provided additional experimental results. However, the fundamental issues regarding the clarity of the method and the significance of the results remain.\n\n### Conclusion:\nWhile the paper presents an interesting approach with potential benefits, the lack of clarity in the methodology, insufficiently convincing experimental results, and issues with reproducibility and presentation make it unsuitable for acceptance at a top-tier conference at this stage. The authors should focus on providing a clearer theoretical foundation for their method, improving the presentation of their results, and ensuring that their approach can be easily reproduced by others in the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel approach to understanding the expressivity of deep neural networks (DNNs) by leveraging Sharkovsky's Theorem from dynamical systems. The authors aim to characterize the depth-width trade-offs of ReLU networks for representing functions with periodic points, addressing an open question posed by Telgarsky regarding the limitations of shallow networks.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a new connection between DNN expressivity and Sharkovsky's Theorem, providing a theoretical framework to understand depth-width trade-offs. This is a significant contribution to the field of deep learning and approximation theory.\n\n2. **Theoretical Rigor:** The paper is well-grounded in theory, with cleanly written proofs and a solid mathematical foundation. The use of dynamical systems to analyze neural networks is innovative and adds depth to the understanding of neural network expressivity.\n\n3. **Addressing Open Questions:** The work addresses open questions in the literature, particularly those raised by Telgarsky, and provides a natural property (periodic points) to derive depth-width trade-offs.\n\n4. **Positive Reviewer Feedback:** All reviewers have provided positive feedback, appreciating the paper's clarity, theoretical contributions, and the authors' responsiveness to suggestions. Reviewer 2, in particular, strongly suggests acceptance.\n\n5. **Author Responsiveness:** The authors have been responsive to reviewer comments, restructuring the paper to improve clarity and adding examples and empirical results to support their theoretical findings.\n\n**Weaknesses:**\n\n1. **Practical Applicability:** While the theoretical contributions are significant, the practical applicability of the results is less clear. Reviewer 3 points out that the period-dependent depth lower bound is not very useful for practical tasks, as assessing or approximating the period for a given task is challenging.\n\n2. **Complexity of Application:** The paper's results, while theoretically interesting, may be difficult to apply in real-world scenarios. The authors acknowledge this limitation in their response, noting that finding the certificate for arbitrary continuous functions is not straightforward.\n\n3. **Structure and Presentation:** Although the authors have restructured the paper following reviewer feedback, there is still a concern about the initial overload of general theory before reaching the main contributions. This could potentially hinder accessibility for readers less familiar with the mathematical background.\n\n**Conclusion:**\n\nThe paper makes a substantial theoretical contribution to the understanding of neural network expressivity and addresses important open questions in the field. Despite some concerns about practical applicability and initial presentation, the novelty, theoretical rigor, and positive reviewer feedback strongly support acceptance. The authors have demonstrated a willingness to improve the paper based on feedback, further strengthening the case for acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of how different activation functions impact the training of overparametrized neural networks. The authors focus on the minimum eigenvalue of a Gram matrix, which is crucial for understanding the convergence rate of training in the lazy regime. The paper distinguishes between non-smooth and smooth activation functions, providing insights into their effects on the eigenvalues of the Gram matrix and, consequently, on the training dynamics.\n\n**Strengths:**\n\n1. **Theoretical Contribution:** The paper makes a significant theoretical contribution by analyzing the impact of activation functions on the training of overparametrized neural networks. It extends existing work by providing quantitative bounds on the minimum eigenvalue of the Gram matrix, which is a crucial aspect of understanding training dynamics.\n\n2. **Comprehensive Analysis:** The paper covers a wide range of activation functions, including both non-smooth (e.g., ReLU) and smooth (e.g., tanh, swish) functions. This comprehensive analysis is valuable for the deep learning community.\n\n3. **Empirical Validation:** The authors provide empirical experiments that support their theoretical findings. Although there is a gap between theory and experiment in some cases, the experiments on synthetic data and CIFAR10 with ReLU align with the theoretical predictions.\n\n4. **Well-Written and Self-Contained:** The paper is well-written, self-contained, and well-referenced. The authors have made efforts to improve the readability and organization of the paper, especially the lengthy appendix.\n\n**Weaknesses:**\n\n1. **Length and Complexity:** The paper is long and complex, which might make it challenging for a general audience to parse. The appendix, although necessary for the completeness of the proofs, adds to the paper's length and complexity.\n\n2. **Gap Between Theory and Experiment:** While the authors acknowledge the gap between theory and experiment on CIFAR10, it would be beneficial to have a more detailed discussion on the reasons for this gap and potential ways to bridge it.\n\n3. **Clarity of Extensions:** Although the authors have addressed the concern about the clarity of extensions, the paper could benefit from a more concise presentation of the main results and their extensions.\n\n**Author Responses:**\n\nThe authors have addressed the reviewers' concerns by revising the paper to improve clarity and organization. They have added a table of contents to the appendix, reorganized sections, and provided informal statements of theorems in the main text. They have also clarified assumptions and corrected minor errors pointed out by the reviewers.\n\n**Conclusion:**\n\nThe paper presents a valuable theoretical contribution to the understanding of activation functions in overparametrized neural networks. Despite its length and complexity, the paper is well-organized and provides significant insights into the training dynamics of neural networks. The authors have made substantial efforts to address the reviewers' concerns and improve the paper's readability. Given the importance of the topic and the quality of the theoretical analysis, the paper is suitable for publication at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Extreme Classification via Adversarial Softmax Approximation\" presents a novel approach to negative sampling in the context of extreme classification, where the number of classes is very large. The authors propose an adversarial sampling mechanism that aims to improve the signal-to-noise ratio during training, thereby enhancing convergence speed and reducing computational costs.\n\n**Strengths:**\n\n1. **Novelty and Theoretical Contribution:** The paper introduces a new adversarial sampling method that is theoretically grounded. The authors provide a formal proof (Theorem 2) that supports the intuition that \"hard\" negative samples are beneficial, which is a significant theoretical contribution to the field.\n\n2. **Practical Relevance:** The proposed method is computationally efficient, with a cost that scales logarithmically with the number of classes, making it suitable for large-scale applications.\n\n3. **Experimental Results:** The paper demonstrates the effectiveness of the proposed method on two large-scale datasets, showing a reduction in training time compared to several baselines.\n\n4. **Clarity and Presentation:** The paper is well-written and presents the technical details in a clear and didactic manner, making it accessible even to non-experts.\n\n**Weaknesses:**\n\n1. **Related Work and Contextualization:** Reviewers pointed out that the paper does not sufficiently discuss related work, particularly recent advancements in non-uniform negative sampling and sampled softmax methods. The authors have acknowledged this and promised to address it in the final version.\n\n2. **Experimental Evaluation:** The experimental evaluation lacks comparisons with some state-of-the-art methods, such as Slice and DiSMEC, which are relevant to the problem domain. The authors argue that their focus is on negative sampling, but comparisons with broader methods could provide a more comprehensive evaluation.\n\n3. **Scope of Experiments:** The paper primarily focuses on single-label classification and does not explore the potential extension to multi-label settings, which could limit its applicability.\n\n4. **Additional Experiments:** The authors have agreed to conduct additional experiments on smaller datasets to compare with full softmax classification, but these results were not available at the time of review.\n\n**Author Response:**\n\nThe authors have provided a detailed response, addressing the concerns raised by the reviewers. They have committed to expanding the related work section and conducting additional experiments. They also clarified the distinction between their approach and related methods, emphasizing the theoretical contributions and practical implications of their work.\n\n**Conclusion:**\n\nThe paper presents a novel and theoretically sound approach to negative sampling in extreme classification, with promising experimental results. While there are some gaps in related work discussion and experimental comparisons, the authors have acknowledged these issues and plan to address them. Given the paper's contributions and the authors' commitment to improving the manuscript, it has the potential to make a significant impact in the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" presents a method for segmenting trajectories into sub-skills using a weakly-supervised approach inspired by multiple instance learning (MIL). The authors aim to extract reusable skills from human demonstrations without requiring detailed supervision, such as the order of skill primitives. The paper is evaluated across various environments and demonstrates preliminary evidence of zero-shot transfer to new skill combinations.\n\n**Strengths:**\n1. **Problem Relevance:** The problem of learning reusable skills from demonstrations is significant in the field of robot learning and manipulation. The approach of using weak supervision is appealing due to its potential to reduce the annotation burden.\n2. **Application of MIL:** The adaptation of MIL to trajectory segmentation is a novel application, and the authors have explored different pooling techniques to improve performance.\n3. **Diverse Evaluation:** The method is tested across multiple environments, including both simulated and real-world settings, which adds robustness to the evaluation.\n\n**Weaknesses:**\n1. **Lack of Novelty:** Both reviewers 2 and 3 highlight that the novelty of the approach is limited. The use of log-sum-exp pooling, while effective, is not new and has been applied in other domains.\n2. **Experimental Limitations:** The results are underwhelming, with validation accuracy ranging from 35-60%. The success rate in behavior cloning tasks is also low, and there is a lack of comparison with fully-supervised baselines, which would provide a clearer benchmark for the method's effectiveness.\n3. **Scope and Clarity:** Reviewer 1 points out that the framing and motivation of the paper do not align well with the results. The paper lacks detailed experimental setup descriptions, making it difficult to assess the validity of the results.\n4. **Theoretical and Empirical Support:** The paper does not provide sufficient theoretical analysis or empirical evidence to support the claims made. The authors acknowledge the ill-posed nature of the problem but do not provide bounds or conditions under which the method would be successful.\n5. **Baseline Comparisons:** The paper lacks comparisons with unsupervised clustering methods and other relevant baselines, which would strengthen the evaluation.\n\n**Author Responses:**\nThe authors provide some clarifications and additional context in their response, but they do not fully address the core concerns raised by the reviewers. They acknowledge the limitations of their approach and the challenges posed by the weak supervision setting. However, the responses do not offer substantial new insights or improvements to the paper's contributions or experimental results.\n\n**Conclusion:**\nWhile the paper addresses an important problem and proposes an interesting application of MIL, the lack of novelty, limited experimental results, and insufficient theoretical grounding make it unsuitable for a top-tier conference. The authors need to provide stronger empirical evidence, clearer experimental descriptions, and more comprehensive baseline comparisons to demonstrate the significance and utility of their approach.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" presents an attempt to improve Monte Carlo Tree Search (MCTS) methods in reinforcement learning, specifically in the context of games like tic-tac-toe. However, there are several critical issues with the paper that make it unsuitable for publication in a top-tier conference.\n\n1. **Writing Quality and Clarity**: All three reviewers have highlighted significant issues with the writing quality. The paper is described as poorly written, with grammatical errors and incomprehensible passages throughout. Reviewer 2 explicitly mentions that they could not understand the abstract or the main content of the paper, which is a fundamental problem for any academic publication. Clear communication of ideas is essential, and the current state of the paper fails to meet this basic requirement.\n\n2. **Lack of References**: The absence of references is a major flaw. Academic papers must situate their contributions within the existing body of knowledge, and references are crucial for this purpose. The author admits to not knowing how to use a .bib file, which led to the omission of references. This indicates a lack of familiarity with standard academic practices and tools, which is concerning for a submission to a top-tier conference.\n\n3. **Scientific Rigor and Contribution**: Reviewers have pointed out that the paper does not demonstrate the preliminary elements of scientific writing and lacks justification for its claims. There is no clear explanation of the methodology, and the mathematical symbols are not defined. The paper appears to be more of a set of notes rather than a polished academic work. Without a clear and rigorous presentation of the research, it is impossible to assess the validity and significance of the proposed method.\n\n4. **Author Response**: The author acknowledges the issues raised by the reviewers and expresses a desire to improve their writing skills. However, the response does not address the fundamental issues with the paper's content and structure. The author also mentions discovering similar ideas in their country but does not provide any concrete steps they plan to take to improve the paper.\n\nGiven these significant issues, the paper does not meet the standards required for publication in a top-tier conference. The lack of clarity, absence of references, and insufficient scientific rigor are critical shortcomings that cannot be overlooked.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Differentiable Hebbian Consolidation for Continual Learning\" presents a novel approach to addressing the problem of catastrophic forgetting in neural networks through a Differentiable Hebbian Consolidation model. The authors propose a Differentiable Hebbian Plasticity (DHP) Softmax layer that incorporates a rapid learning plastic component to the fixed parameters of the softmax output layer, aiming to retain learned representations over longer timescales. The paper is evaluated on several benchmarks, including Permuted MNIST, Split MNIST, and a Vision Datasets Mixture, and introduces an imbalanced variant of Permuted MNIST.\n\n**Strengths:**\n\n1. **Novelty and Relevance:** The paper addresses a significant problem in the field of continual learning, proposing a novel approach inspired by Hebbian learning and complementary learning systems (CLS). The idea of integrating fast and slow weights is innovative and provides a fresh perspective on tackling catastrophic forgetting.\n\n2. **Theoretical Motivation:** The method is well-motivated by cognitive principles and neurobiological substrates, providing a strong theoretical foundation for the proposed approach.\n\n3. **Experimental Evaluation:** The authors conduct extensive experiments on multiple benchmarks, demonstrating the effectiveness of their approach. They also provide additional experiments on Split CIFAR-10/100 in response to reviewer feedback, showcasing the scalability of their method.\n\n4. **Clarity and Writing:** The paper is generally well-written, with clear explanations of the methodology and results. The authors have also addressed reviewer concerns by adding clarifications and additional experiments.\n\n**Weaknesses:**\n\n1. **Empirical Results:** While the proposed method shows improvements over naive baselines, its performance is not consistently superior to existing methods, particularly on more challenging datasets. The method performs best when combined with other task-specific synaptic consolidation methods, which raises questions about its standalone effectiveness.\n\n2. **Comparison with CLS-based Approaches:** The paper lacks direct comparisons with other CLS-based approaches, which could provide a more comprehensive evaluation of the proposed method's relative strengths and weaknesses.\n\n3. **Complexity and Understanding:** Some reviewers found the details of the DHP Softmax challenging to follow, indicating that the paper could benefit from additional clarifications and visual aids to enhance understanding.\n\n4. **Scope of Experiments:** Although the authors added experiments on Split CIFAR-10/100, the scope of the experiments remains somewhat limited compared to other works in the literature that explore more sophisticated contexts, such as reinforcement learning scenarios.\n\n**Author Response:**\n\nThe authors have made a commendable effort to address reviewer concerns, providing additional experiments, clarifications, and theoretical justifications. They have also acknowledged the limitations of their work and provided a rationale for their experimental choices.\n\n**Conclusion:**\n\nWhile the paper presents a novel and theoretically sound approach to continual learning, the empirical results are not sufficiently compelling to warrant acceptance at a top-tier conference. The method's reliance on combination with other techniques and the lack of direct comparisons with CLS-based approaches are significant drawbacks. Additionally, the complexity of the method and the limited scope of experiments further weaken the paper's impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents an interesting approach to reducing the number of supervised synaptic updates required to model the primate ventral stream using deep neural networks. The authors propose three strategies to achieve this reduction and claim that their methods can achieve a significant match to the adult ventral stream with substantially fewer updates.\n\n**Strengths:**\n1. **Novelty and Relevance:** The paper addresses a significant issue in computational neuroscience, which is the biological plausibility of training deep neural networks with a large number of supervised updates. This is a relevant problem as it challenges the current understanding of how the brain might develop its visual processing capabilities.\n2. **Methodological Innovation:** The introduction of weight compression and critical training as methods to reduce the number of updates is innovative and could have implications for both neuroscience and machine learning.\n3. **Empirical Validation:** The authors provide empirical evidence that their methods can reduce the number of updates while maintaining a high Brain-Score, which is a benchmark for similarity to the primate ventral stream.\n4. **Potential for Broader Impact:** The methods proposed could be useful in developing more efficient neural networks that require less data and computational resources, which is a significant concern in the field.\n\n**Weaknesses:**\n1. **Interpretation of Results:** Several reviewers, particularly Reviewer 1 and Reviewer 2, express concerns about the interpretation of the Brain-Score metric. The authors' claims about the biological significance of their results are seen as potentially misleading because the Brain-Score is not a direct measure of biological similarity.\n2. **Biological Plausibility:** Reviewer 3 questions the biological plausibility of the methods, particularly the weight compression strategy, and the lack of a theoretical foundation for the assumptions made in the paper.\n3. **Clarity and Communication:** There is a need for clearer communication regarding the limitations of the Brain-Score and the biological relevance of the results. The authors have acknowledged this and proposed changes to address these concerns.\n4. **Lack of Theoretical Justification:** The paper lacks a strong theoretical foundation for why the proposed methods should be considered biologically plausible, which is a critical aspect for a top-tier conference in computational neuroscience.\n\n**Author Responses:**\nThe authors have provided detailed responses to the reviewers' concerns, acknowledging the limitations of the Brain-Score and clarifying the intent behind their methods. They have proposed changes to the manuscript to address issues of clarity and interpretation. However, some concerns, particularly about the biological plausibility and theoretical justification, remain only partially addressed.\n\n**Conclusion:**\nWhile the paper presents innovative methods and addresses an important problem, the concerns about the interpretation of results and the biological plausibility of the methods are significant. For a top-tier conference, it is crucial that the claims made are well-supported by both empirical evidence and theoretical justification. The authors' responses indicate a willingness to improve the manuscript, but the current version still lacks the necessary rigor in these areas.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" presents a novel approach to unsupervised object-wise 3D scene decomposition and rendering using a generative model called ROOTS. The paper claims to extend the Generative Query Networks (GQN) framework by providing object-oriented representation decomposition, which is hierarchical and viewpoint invariant.\n\n**Strengths:**\n1. **Novelty and Contribution**: The paper introduces a new model, ROOTS, which aims to provide object-oriented 3D scene decomposition. This is a significant contribution to the field of unsupervised learning and representation learning.\n2. **Qualitative Results**: The qualitative results presented in the paper are impressive, showcasing the model's ability to swap objects in and out of scenes and render 2D images from different viewpoints.\n3. **Potential Impact**: Learning 3D representations from 2D images is an important problem, and the proposed methodology could lead to more interpretable and compositional representations.\n\n**Weaknesses:**\n1. **Writing and Clarity**: The paper suffers from poor writing and clarity issues, which were highlighted by multiple reviewers. The authors have attempted to address these issues in their revision, but the initial submission was difficult to follow.\n2. **Unsupported Claims**: Some claims in the paper are not well-supported by the experiments. For example, the claim that ROOTS has clearer generations than GQN is not evident from the figures provided.\n3. **Lack of Comparisons and Ablations**: The paper lacks proper comparisons with the latest works and does not include ablation studies to isolate the contributions of different components of the model.\n4. **Related Work**: The paper initially ignored a significant amount of related work in the vision community, which was a major concern for the reviewers. The authors have attempted to address this in their revision.\n5. **Experimental Weaknesses**: The experiments are considered weak by the reviewers, with concerns about the fairness of comparisons and the lack of significant improvements over GQN.\n\n**Author Response:**\nThe authors have made significant revisions to the paper, addressing many of the reviewers' concerns. They have improved the clarity of the paper, added more related work, and provided additional experiments. However, some reviewers remain unconvinced about the strength of the experimental results and the lack of ablation studies.\n\n**Conclusion:**\nWhile the paper presents an interesting idea with potential contributions to the field, the weaknesses in writing, unsupported claims, lack of comparisons, and experimental weaknesses are significant concerns for a top-tier conference. The authors have made efforts to address these issues, but the paper still lacks the maturity and rigor expected at this level. The novelty and potential impact are acknowledged, but the execution and presentation need further refinement.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improved Training of Certifiably Robust Models\" presents a method to enhance the training of neural networks for certified robustness against adversarial attacks by introducing two regularizers. The paper aims to address the gap between certifiable and empirical robustness by proposing regularizers that yield tighter convex relaxation bounds.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper introduces two new regularizers that aim to improve the tightness of convex relaxation bounds, which is a significant contribution to the field of adversarial robustness.\n2. **Experimental Results:** The authors provide empirical evidence that their method results in tighter certification bounds than non-regularized baselines. They also claim state-of-the-art certified accuracies on MNIST and CIFAR10 datasets.\n3. **Revisions and Clarifications:** The authors have made substantial revisions to address reviewer concerns, including restructuring the paper, adding illustrative examples, and expanding experimental results.\n\n**Weaknesses:**\n1. **Clarity and Presentation:** Multiple reviewers noted that the theoretical sections, particularly Section 4, are confusing and convoluted. There is a lack of clarity in distinguishing between the original non-convex problem and its convex relaxation.\n2. **Experimental Limitations:** The experiments primarily focus on MNIST and CIFAR10, which are considered basic datasets. Reviewer 3 pointed out the need for more diverse datasets to validate the robustness claims.\n3. **Theoretical Justification:** Reviewers expressed concerns about the theoretical justification of the regularizers. There is skepticism about whether the proposed method truly leads to tighter bounds compared to existing methods like Fast-Lin.\n4. **Baseline Comparisons:** Reviewer 1 and 2 highlighted the lack of certain baseline comparisons, which weakens the experimental validation of the proposed method.\n\n**Author Response:**\nThe authors have addressed several concerns raised by the reviewers. They clarified the purpose of their regularizers, provided additional experimental results, and corrected some of the theoretical misunderstandings. They also added discussions and examples to improve the clarity of their arguments.\n\n**Evaluation:**\nWhile the paper presents an interesting approach to improving certified robustness, the concerns about clarity, theoretical justification, and experimental validation are significant. The authors have made efforts to address these issues, but the fundamental confusion in the theoretical sections and the limited experimental scope remain problematic for a top-tier conference. The paper's contributions are promising, but they require further refinement and validation to meet the high standards expected at such conferences.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents a novel approach to integrating linguistic and visual processing in multimodal tasks. The authors propose using language to modulate both top-down and bottom-up visual processing, which they argue is more effective than the traditional top-down approach alone.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a new perspective on integrating language with visual processing by modulating both top-down and bottom-up pathways. This is a significant departure from the conventional methods that primarily focus on top-down modulation.\n\n2. **Experimental Results:** The authors demonstrate that their approach achieves state-of-the-art or competitive results on several datasets. The ablation studies further support the claim that both top-down and bottom-up modulation are essential for improved performance.\n\n3. **Clarity and Writing:** The paper is generally well-written and easy to follow. It provides a comprehensive literature review and clearly describes the proposed model and experiments.\n\n**Weaknesses:**\n\n1. **Empirical Results and Significance:** Reviewer 2 and Reviewer 4 raise concerns about the empirical results. The model does not consistently outperform existing methods across all metrics and datasets, which questions the robustness and generalizability of the approach. The lack of statistical significance tests further weakens the empirical claims.\n\n2. **Conceptual Motivation:** Reviewer 4 points out the lack of a clear conceptual motivation for the model. The paper does not sufficiently explain why language should modulate early-stage visual processing or what specific benefits this integration provides.\n\n3. **Error Analysis and Model Interpretation:** There is a lack of detailed error analysis and interpretation of how linguistic representations affect visual processing. This makes it difficult to understand the model's behavior and the underlying reasons for its performance.\n\n4. **Response to Reviewers:** The authors' responses to some critical points raised by the reviewers, particularly regarding empirical results and conceptual motivation, are not entirely convincing. The absence of a critical baseline reference (Mei et al. 2018) and discrepancies in reported numbers further undermine the paper's credibility.\n\n**Conclusion:**\n\nWhile the paper presents an interesting and potentially impactful idea, the concerns about empirical robustness, lack of conceptual clarity, and insufficient error analysis are significant drawbacks. The paper does not fully meet the high standards expected at a top-tier conference, particularly in terms of empirical validation and theoretical grounding.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Cortico-cerebellar networks as decoupled neural interfaces\" presents an intriguing hypothesis that the cerebellum functions similarly to decoupled neural interfaces (DNI) in deep learning, addressing the credit assignment problem in the brain. The authors propose that the cerebellum helps the cerebral cortex solve the locking problem, akin to the forward and backward locking problems in deep learning. They demonstrate this through a cortico-cerebellar-DNI (CC-DNI) model tested on various tasks.\n\n**Strengths:**\n\n1. **Novel Hypothesis:** The paper introduces a novel perspective by linking cerebellar function with DNI, which could stimulate new research directions in both neuroscience and deep learning.\n\n2. **Experimental Rigor:** The experiments are well-conducted, demonstrating the model's applicability across different tasks, including motor and cognitive domains.\n\n3. **Clarity and Structure:** The paper is generally well-written, with clear explanations of the model and its implications.\n\n4. **Addressing Reviewer Feedback:** The authors have made significant revisions in response to reviewer comments, adding new figures, a new section, and extending the discussion to include specific predictions and comparisons with experimental findings.\n\n**Weaknesses:**\n\n1. **Lack of Novel Insights:** As pointed out by Reviewer 2, the paper primarily juxtaposes existing knowledge of the cerebellum and DNI without offering substantial new insights into either domain. The authors' response attempts to address this by adding predictions and comparisons, but the novelty of these insights remains questionable.\n\n2. **Biological Plausibility:** Reviewer 3 highlights concerns about the biological plausibility of the model, particularly regarding the computation and transmission of gradients in the brain. The authors acknowledge these limitations but do not fully resolve them.\n\n3. **Comparison with Existing Models:** The paper lacks a thorough comparison between the proposed CC-DNI model and existing DNI architectures, which would strengthen claims about its advantages.\n\n4. **Predictions and Experimental Validation:** While the authors have added predictions and comparisons with experimental data, these additions may not be sufficiently robust or novel to warrant publication in a top-tier conference.\n\n**Conclusion:**\n\nThe paper presents an interesting hypothesis and makes a commendable effort to address reviewer feedback. However, the lack of substantial new insights, unresolved questions about biological plausibility, and insufficient comparison with existing models limit its impact. While the authors have improved the manuscript, the contributions may not meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Interaction Processes for Time-Evolving Graphs\" presents a method for modeling continuous time-evolving graphs using a temporal point process framework. The authors propose a deep neural approach that incorporates LSTM with time gates, a selection mechanism, and a fusion mechanism to handle multiple time resolutions and dynamic interactions. The paper is evaluated based on its novelty, technical soundness, experimental validation, and clarity.\n\n**Strengths:**\n\n1. **Relevance and Importance:** The problem of modeling time-evolving graphs is significant, with applications in various domains such as social networks and fraud detection.\n\n2. **Experimental Results:** The paper demonstrates improved empirical performance over several datasets and provides ablation studies to analyze the contributions of different components of the model.\n\n3. **Response to Reviewers:** The authors have addressed some of the reviewers' concerns by adding an overview diagram and providing additional experimental results, including comparisons with the JODIE baseline.\n\n**Weaknesses:**\n\n1. **Novelty:** Both reviewers 1 and 2 express concerns about the limited novelty of the contributions. The use of LSTM with time gates and the fusion mechanism is considered incremental, and similar approaches have been introduced in prior work. The main novelty seems to lie in the combination of existing techniques rather than introducing fundamentally new concepts.\n\n2. **Justification and Analysis:** There is a lack of detailed justification and analysis for the proposed mechanisms. Reviewers note the absence of insights into why certain components, such as the attention mechanism, improve performance. The authors' response provides some clarification but does not fully address these concerns.\n\n3. **Clarity and Presentation:** Reviewer 3 highlights that the paper is difficult to understand in detail, suggesting that the presentation could be improved. The authors have attempted to address this by adding a toy example, but the overall clarity remains a concern.\n\n4. **Comparison with Baselines:** The initial submission lacked comparison with an important baseline (JODIE), which was later added in the response. However, the lack of initial comparison raises questions about the thoroughness of the experimental evaluation.\n\n5. **Technical Concerns:** Reviewer 2 points out potential issues with the log-likelihood function and the number of parameters in the model compared to baselines. These technical concerns are critical for assessing the validity of the experimental results.\n\n**Conclusion:**\n\nWhile the paper addresses an important problem and shows promising results, the concerns about novelty, justification, and clarity are significant. The combination of existing techniques without substantial innovation, along with the lack of detailed analysis and initial oversight in baseline comparison, weakens the paper's contribution to the field. Given the standards of a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to computational biology, specifically in predicting DNA folding patterns using a bidirectional LSTM RNN model. The authors claim that their model outperforms traditional machine learning models and emphasize the importance of memory in sequential DNA states for chromatin folding prediction.\n\n**Strengths:**\n1. **Novel Application**: The application of bidirectional LSTM RNNs to predict DNA folding patterns is a novel approach in the context of computational biology.\n2. **Performance**: The authors report that their model outperforms other classical approaches, which is a positive aspect if the results are robust and reproducible.\n3. **Revisions and Clarifications**: The authors have made several revisions to address reviewer comments, including expanding the literature review, clarifying the methodology, and adding additional evaluation metrics.\n\n**Weaknesses:**\n1. **Methodological Novelty**: Reviewer 3 points out that the paper lacks methodological novelty and is similar to previously published works. The authors' response does not convincingly address this concern, as they primarily focus on the application rather than introducing new methodologies.\n2. **Clarity and Detail**: There are still concerns about the clarity and detail of the data and methods description. For instance, the representation of input features and the rationale for using a bidirectional LSTM are not fully convincing.\n3. **Evaluation and Baselines**: The evaluation needs strengthening with additional baseline models and metrics. Although the authors added more metrics, the comparison to other models remains limited.\n4. **Generalizability**: The paper lacks a demonstration of generalizability to other datasets or organisms, which is crucial for a top-tier conference.\n5. **Literature Review**: The literature review was expanded, but the authors initially omitted relevant works, which raises concerns about the comprehensiveness of their research context.\n\n**Conclusion:**\nWhile the paper presents an interesting application of machine learning to a biological problem, it falls short in several key areas expected of a top-tier conference paper. The lack of methodological novelty, insufficient clarity in the description of methods, limited evaluation, and concerns about generalizability are significant drawbacks. Despite the authors' efforts to address reviewer comments, these issues remain substantial enough to warrant rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Domain Adaptive Multibranch Networks\" presents a novel approach to unsupervised domain adaptation by introducing a deep learning framework that allows different domains to undergo different sequences of operations. This approach contrasts with existing methods that process all domains with the same series of operations. The paper claims that this flexibility leads to higher accuracy and can handle multiple domains simultaneously.\n\n**Strengths:**\n\n1. **Novelty and Contribution**: The paper introduces a novel concept of adaptive computation graphs applied to domain adaptation, which is a fresh perspective in the field. The idea of allowing different domains to have different computational paths is innovative and potentially impactful.\n\n2. **Technical Soundness**: The reviewers generally agree that the method is technically sound and sensible. The authors have provided a clear motivation and a well-structured framework.\n\n3. **Clarity and Writing**: The paper is well-written and easy to follow, which is crucial for understanding complex methodologies.\n\n4. **Potential for Extension**: The framework can be extended to multi-task settings and multi-source/multi-target domain adaptation, which adds value to the proposed method.\n\n**Weaknesses:**\n\n1. **Experimental Validation**: The primary concern across the reviews is the lack of comprehensive experimental validation. The paper does not compare its results with state-of-the-art methods like CDAN, which is crucial for establishing the effectiveness of the proposed method. The authors have acknowledged this and provided additional experiments in their response, but the initial submission lacked this critical comparison.\n\n2. **Ablation Studies and Sensitivity Analysis**: The paper lacks detailed ablation studies and hyperparameter sensitivity analyses, which are essential to understand the robustness and limitations of the proposed method. The authors have addressed some of these concerns in their response, but the initial submission was incomplete in this regard.\n\n3. **Parameter Sharing Explanation**: There is some confusion regarding parameter sharing, as highlighted by Reviewer 2. The authors have attempted to clarify this in their response, but the initial presentation was not entirely clear.\n\n4. **Baseline Comparisons**: The baseline results presented in the paper are not comparable to those reported in other UDA papers, which raises concerns about the experimental setup and the validity of the results.\n\n**Author Response:**\n\nThe authors have provided a detailed response to the reviewers' concerns, including additional experiments and clarifications. They have acknowledged the need for comparisons with state-of-the-art methods and have conducted further experiments to address this. They have also provided more insights into the behavior of their method under different settings.\n\n**Conclusion:**\n\nWhile the paper presents a novel and interesting approach to domain adaptation, the initial submission lacked critical experimental validation and comparisons with state-of-the-art methods. The authors have addressed these issues in their response, but the extent to which these additions improve the paper is not entirely clear from the provided information. Given the importance of experimental validation in a top-tier conference, the initial shortcomings are significant.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"GG-GAN: A Geometric Graph Generative Adversarial Network\" presents a novel approach to graph generation using a geometric perspective and a Wasserstein GAN framework. The authors claim that their model addresses key challenges in graph generation, such as modeling complex relations, isomorphic graphs, and exploiting latent distributions. However, the paper has received mixed reviews from the reviewers, highlighting both strengths and weaknesses.\n\n**Strengths:**\n1. **Novelty and Theoretical Support:** The paper introduces a new approach to graph generation, which is supported by theoretical insights. The use of a geometric perspective and permutation equivariance are innovative aspects.\n2. **Clarity and Organization:** The paper is generally well-written and organized, making it accessible to readers.\n3. **Scalability:** The authors claim that GG-GAN can scale to generate large graphs, which is a significant advantage over some existing methods.\n\n**Weaknesses:**\n1. **Experimental Limitations:** The experiments are conducted on small datasets with graphs having only 9 to 20 vertices. This raises concerns about the generalizability and scalability of the method to larger, more complex graphs. The choice of datasets and the lack of comparison with more competitive baselines (e.g., NetGAN, TagGen) are significant drawbacks.\n2. **Over-claimed Contributions:** Several reviewers noted that the paper might overstate its contributions, particularly in terms of novelty and performance improvements over existing methods.\n3. **Mode Collapse and Diversity:** Reviewer 2 pointed out the lack of discussion on mode collapse and generation diversity, which are critical issues in generative models.\n4. **Theoretical and Methodological Clarity:** There are concerns about the clarity and justification of some theoretical claims and methodological choices, such as the role of the similarity function φ and the sampling procedures.\n5. **Literature Review:** The paper lacks a comprehensive literature review, which would help position the work within the broader context of graph generation research.\n\n**Conclusion:**\nWhile the paper presents an interesting approach to graph generation, the experimental evaluation is insufficient to demonstrate the claimed advantages over existing methods. The lack of comparison with state-of-the-art baselines, limited dataset sizes, and over-claimed contributions are significant issues. Additionally, the absence of an author response means that the concerns raised by the reviewers remain unaddressed. Given these factors, the paper does not meet the standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Ecological Inference\" presents an approach to ecological inference using deep learning techniques, specifically focusing on the Maryland 2018 midterm elections. The paper aims to address the problem of inferring individual labels from aggregate data, which is a significant challenge in political science and other fields.\n\n### Strengths:\n1. **Relevance and Importance**: The paper addresses an important problem in ecological inference, which has practical applications in political science, particularly in understanding voting behaviors.\n2. **Novelty in Approach**: The integration of deep learning techniques, such as Bayesian neural networks, into ecological inference is a novel approach that could potentially offer new insights and improvements over traditional methods.\n3. **Real-world Application**: The use of real-world data from the Maryland 2018 midterm elections adds practical value and relevance to the study.\n4. **Potential Impact**: The approach could have significant implications for future redistricting cycles and understanding racially polarized voting.\n\n### Weaknesses:\n1. **Clarity and Structure**: Multiple reviewers noted that the paper is difficult to follow due to structural issues and lack of clarity in the presentation of methods and results. This includes unclear descriptions of input data, evaluation tasks, and experimental details.\n2. **Incomplete Experiments and Analysis**: The experiments lack key baselines, such as a comparison with a standard multi-level model (MLM), which makes it difficult to assess the true impact of the proposed deep learning approach. The analysis does not sufficiently explain or interpret the results, leaving many claims unsubstantiated.\n3. **Reproducibility Concerns**: The paper omits crucial experimental details, such as train/test splits, hyperparameters, and specific settings for few-shot learning experiments, which hampers reproducibility.\n4. **Lack of Technical Novelty**: Reviewer 4 pointed out that the proposed approach is not technically new, and the paper lacks adequate citations of related work, which raises concerns about the originality and contribution to the field.\n5. **Writing and Presentation**: There are numerous typos and minor writing issues throughout the paper, which detract from its professionalism and readability.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes an interesting approach by integrating deep learning into ecological inference, it falls short in several critical areas. The lack of clarity, incomplete experiments, insufficient analysis, and reproducibility issues are significant drawbacks. Additionally, the absence of a response from the authors to address these concerns further weakens the case for acceptance. Given the standards of a top-tier conference, these issues are substantial enough to warrant rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights using task-specific updates of model parameters. The authors claim that their method, $\\alpha$VIL, outperforms existing multitask learning approaches in various settings. However, the reviewers have raised several concerns that need to be addressed to determine the paper's suitability for a top-tier conference.\n\n### Strengths:\n1. **Clarity and Writing**: The paper is well-written and easy to follow, which is a positive aspect noted by all reviewers.\n2. **Novelty**: The approach of using task-specific updates for dynamic task weighting is novel, and the authors argue that it is different from existing methods like Discriminative Importance Weighting (DIW) and meta-learning approaches.\n3. **Empirical Evaluation**: The paper includes experiments on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, providing a broad evaluation of the proposed method.\n\n### Weaknesses:\n1. **Theoretical Justification**: A significant concern is the lack of theoretical justification or formal analysis of the proposed methodology. This is crucial for a top-tier conference, as it provides a deeper understanding of why the method works and under what conditions it is expected to perform well.\n   \n2. **Experimental Results**: The experimental results are not consistently convincing. The proposed method does not consistently outperform the baselines, and the improvements are often within the range of statistical insignificance. This is particularly problematic given the high standards of a top-tier conference.\n\n3. **Baseline Comparisons**: The choice of baselines is somewhat limited, and the results do not clearly demonstrate the superiority of the proposed method over existing approaches. The reviewers suggest that the method's performance is not significantly better than DIW, a strong baseline.\n\n4. **Algorithmic Clarity**: There are concerns about the clarity and intuition behind certain algorithmic choices, such as the subtraction of 1 from weights in Algorithm 1. This lack of clarity can hinder the understanding and reproducibility of the method.\n\n5. **Experimental Setup**: The choice of datasets and tasks for evaluation is questioned, particularly the use of MultiMNIST with only one auxiliary task, which does not fully exploit the multitask learning setup.\n\n6. **Statistical Significance**: The lack of statistical significance testing for the results, especially on the NLU tasks, is a notable omission that weakens the empirical claims.\n\n### Author Responses:\nThe authors acknowledge the lack of theoretical justification and plan to address this in future versions. They also provide explanations for some of the algorithmic choices and experimental setups. However, the responses do not fully resolve the concerns about the empirical results and the novelty of the method compared to existing approaches.\n\n### Conclusion:\nWhile the paper presents an interesting idea and is well-written, the lack of theoretical justification, the unconvincing empirical results, and the limited novelty compared to existing methods are significant drawbacks. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance. The authors are encouraged to strengthen the theoretical foundation, improve the empirical evaluation, and provide clearer distinctions from existing methods in future submissions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning\" presents a novel approach to combining meta-reinforcement learning (meta-RL) with imitation learning (IL) to address the challenges of sparse rewards and task adaptation. The proposed method, PERIL, aims to leverage demonstrations to improve adaptation rates and explore beyond the provided demonstrations.\n\n### Strengths:\n1. **Novelty and Relevance**: The paper addresses a significant problem in reinforcement learning, particularly the challenge of adapting to new tasks with sparse rewards. The combination of meta-RL and IL is a promising direction that could lead to more efficient learning algorithms.\n2. **Empirical Results**: The results indicate that PERIL outperforms existing baselines like PEARL and MetaIL, which suggests that the method has potential.\n3. **Visualization and Analysis**: The paper provides visualizations of the latent space, which help in understanding what the method has learned.\n\n### Weaknesses:\n1. **Complexity and Clarity**: The method is quite complex, involving multiple components and loss terms. Reviewers noted that the paper lacks clarity in explaining how these components fit together and why each is necessary. The notation and mathematical formulation are inconsistent, which adds to the confusion.\n2. **Insufficient Baselines and Ablations**: The paper does not compare PERIL against several relevant baselines that combine meta-RL and IL. Additionally, there is a lack of ablation studies to determine the necessity of each component of the method.\n3. **Clarity in Methodology**: There are several points of confusion in the methodology, such as the role of certain loss terms and the assumptions made about the task and environment. The problem formulation is mixed with the approach, making it difficult to discern what is a constraint of the setting versus a design choice.\n4. **Related Work and Citations**: The paper lacks citations in key areas and does not adequately position itself with respect to prior work. This is crucial for understanding the novelty and significance of the contributions.\n5. **Experimental Design**: The experiments lack statistical significance tests and multiple seeds, which are necessary for a robust evaluation. The claims of zero-shot learning are not well-substantiated, and the tasks used do not convincingly demonstrate the method's ability to explore beyond demonstrations.\n\n### Author Responses:\nThe authors have not adequately addressed the concerns raised by the reviewers. The complexity and clarity issues remain, and the lack of sufficient baselines and ablations is a significant oversight. The responses do not provide enough detail to resolve the confusion around the methodology or the assumptions made.\n\n### Conclusion:\nWhile the paper addresses an important problem and shows promising results, the issues with clarity, complexity, and insufficient evaluation are significant. For a top-tier conference, it is crucial that the paper is clear, well-positioned with respect to prior work, and thoroughly evaluated. The current submission does not meet these standards.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Score and Lyrics-Free Singing Voice Generation\" presents a novel approach to generating singing voices without pre-assigned scores and lyrics, which is an interesting and challenging problem. The authors propose three different schemes for this task and provide a detailed description of their methodology, including the use of generative adversarial networks (GANs) and the development of customized evaluation metrics.\n\nStrengths:\n1. Novelty: The paper addresses a new problem in the field of singing voice generation, which is a significant contribution.\n2. Methodology: The authors have developed a comprehensive pipeline involving source separation, transcription models, and adversarial networks.\n3. Open-source Code: The authors have open-sourced their code, which enhances the reproducibility of their work.\n4. Baseline Comparisons: In response to reviewer feedback, the authors have included comparisons with baseline methods (Sinsy and Synthesizer V), which provide context for evaluating their results.\n\nWeaknesses:\n1. Evaluation: The evaluation metrics used (Vocalness, Average Pitch, Matchness) have been criticized for not fully capturing the quality of singing voice generation. The subjective nature of the task makes evaluation challenging, but the authors have attempted to address this with MOS user studies.\n2. Motivation and Clarity: Some reviewers noted a lack of clear motivation for the task settings and insufficient discussion on the differences between them. The authors have attempted to address this in their revisions.\n3. Literature Review: The literature review could be improved, particularly regarding neural network-based singing voice synthesis.\n4. Complexity and Focus: The paper attempts to tackle multiple problems simultaneously, which may dilute the focus and clarity of the main contributions.\n\nThe authors have made significant efforts to address the reviewers' concerns, particularly by expanding the evaluation section and providing baseline comparisons. However, some fundamental issues remain, such as the adequacy of the evaluation metrics and the clarity of the motivation for the task settings.\n\nGiven the novelty of the problem and the efforts made to address reviewer feedback, the paper has potential. However, the evaluation and clarity issues are critical for a top-tier conference, where the standards for rigor and clarity are exceptionally high. The paper would benefit from further refinement, particularly in justifying the chosen evaluation metrics and providing clearer motivation and context for the task settings.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" presents a novel algorithm for pruning neurons in deep ReLU networks. The authors claim that their approach is theoretically grounded and can significantly reduce the number of neurons without affecting the learned function. However, several critical issues have been identified by the reviewers, which I will address in detail.\n\n1. **Clarity and Presentation**: The paper suffers from significant clarity and presentation issues. Important concepts are explained in footnotes, and many definitions are missing or unclear. The main theoretical contributions are not presented rigorously, and the connection between the theory and the algorithm is not well articulated. The authors acknowledge these issues in their response and agree that the paper is hard to read and understand.\n\n2. **Dependence on Unpublished Work**: A significant portion of the paper relies on results from an unpublished work by the authors. This makes it difficult for reviewers and readers to verify the correctness and relevance of the theoretical claims. The authors themselves suggest that it might be better to publish the other paper first to provide a clearer foundation for their current work.\n\n3. **Experimental Validation**: The experimental validation is insufficient. The experiments are limited to toy datasets and MNIST, which are not challenging enough to demonstrate the effectiveness of the pruning technique. There is a lack of comparison with state-of-the-art pruning methods, and the authors admit that more experiments are necessary. The reviewers consistently point out the need for experiments on more complex datasets like CIFAR and comparisons with existing techniques.\n\n4. **Novelty and Contribution**: While the authors propose a novel approach to neuron pruning, the novelty is questioned by the reviewers. Some steps of the algorithm are considered trivial or well-known in the literature. The authors claim that their main contribution is the theoretical foundation, but this is not well communicated or substantiated in the paper.\n\n5. **Author Response**: The authors acknowledge many of the issues raised by the reviewers and express a willingness to improve the paper. They provide additional explanations and express confidence in their theoretical contributions. However, they also admit that they were not able to communicate their main messages effectively in the current version of the paper.\n\nIn conclusion, while the paper presents an interesting approach to neuron pruning, it is not ready for publication in its current form. The reliance on unpublished work, lack of clarity, insufficient experimental validation, and inadequate comparison with existing methods are significant drawbacks. The authors need to address these issues comprehensively before the paper can be considered for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to training deep neural networks using a new floating-point representation called Block Minifloat (BM). The paper aims to improve computational efficiency and reduce power consumption by using low-precision formats (4-8 bits) for weights, activations, and gradients. The authors provide both theoretical insights and empirical evaluations, including hardware synthesis results, to support their claims.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a new representation that combines the benefits of minifloats and shared exponent bias, which is a significant contribution to the field of efficient DNN training. The use of Kulisch accumulators for BM is a novel idea that addresses the challenges of using narrow exponent fields.\n\n2. **Comprehensive Evaluation:** The authors provide a thorough evaluation of their proposed method, including software simulations on various models and datasets, as well as hardware synthesis results. This comprehensive approach strengthens the validity of their claims.\n\n3. **Hardware Implementation:** The paper goes beyond theoretical proposals by implementing the BM representation in hardware and providing area and power consumption metrics. This practical aspect is crucial for assessing the real-world applicability of the proposed method.\n\n4. **Clarity and Revisions:** The authors have responded to reviewer comments by clarifying key aspects of their work, such as the alignment of minifloat distributions and the handling of denormal numbers. They have also improved the presentation of their results and addressed specific concerns raised by the reviewers.\n\n**Weaknesses:**\n\n1. **Limited Novelty in Some Aspects:** Some reviewers noted that the shared-exponent bias concept is not entirely new, and its application to FP8 training is straightforward. However, the authors have demonstrated how this approach can be leveraged to achieve significant hardware efficiency gains.\n\n2. **Complexity and Overhead:** The use of multiple BM formats in training raises concerns about potential hardware overhead. The authors have addressed this issue by explaining how their hardware design accommodates these formats, but the complexity might still be a concern for some applications.\n\n3. **Presentation Issues:** Despite revisions, some parts of the paper, such as the explanation of certain equations and the presentation of results, could still benefit from further clarification and simplification.\n\n4. **Scope of Experiments:** The experiments are primarily conducted on smaller networks, and while the authors justify this choice, it limits the generalizability of the results to larger, more complex networks.\n\n**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of efficient DNN training with a novel representation that has been thoroughly evaluated both in software and hardware. The authors have addressed most of the reviewers' concerns and provided additional clarity in their revised draft. While there are some limitations and areas for improvement, the strengths of the paper, particularly its practical hardware implementation and comprehensive evaluation, make it a strong candidate for acceptance at a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper presents a novel approach to training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces, which is a relevant and potentially impactful topic. However, several critical issues have been identified by the reviewers, and the authors' responses, while clarifying some points, do not fully address these concerns.\n\n### Strengths:\n1. **Relevance and Novelty**: The paper addresses the challenge of training GMMs in high-dimensional spaces using SGD, which is a relevant problem given the increasing size of datasets in machine learning.\n2. **Implementation**: The authors provide a publicly available TensorFlow implementation, which is a positive contribution to the community.\n3. **Potential for Online Learning**: The authors emphasize the potential of their method for online learning scenarios, which is an important application area.\n\n### Weaknesses:\n1. **Incremental Contribution**: Reviewers noted that the work appears largely incremental and does not sufficiently differentiate itself from existing methods, particularly in terms of the max-component approximation and regularization techniques.\n2. **Experimental Evaluation**: The experimental results are not convincing. The datasets used are not sufficiently high-dimensional to demonstrate the claimed advantages, and there is a lack of comprehensive comparison with existing methods, particularly in terms of convergence guarantees and performance metrics.\n3. **Theoretical Justification**: There is a lack of theoretical guarantees for convergence or performance, which is a significant drawback for a top-tier conference. The authors' response regarding convergence is not entirely satisfactory, as it relies on general properties of gradient descent without addressing specific challenges in the context of GMMs.\n4. **Clarity and Justification**: Several sections of the paper, such as the motivation for the max-component approximation and the regularization technique, are not clearly justified or explained. The authors' responses indicate an intention to clarify these points, but the current version of the paper lacks this clarity.\n5. **Comparison with Baselines**: The paper does not convincingly demonstrate the advantages of the proposed method over standard EM with K-means initialization, which is a well-established baseline.\n\n### Author Responses:\nThe authors have provided responses to the reviewers' comments, acknowledging some of the issues and promising to incorporate changes. However, the responses do not fully resolve the concerns, particularly regarding the experimental evaluation and theoretical justification.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes a potentially useful method, the current version lacks the necessary theoretical and empirical rigor expected at a top-tier conference. The contributions are not sufficiently novel or well-validated to warrant acceptance. The authors are encouraged to address these issues in future work, particularly by providing stronger theoretical guarantees, more comprehensive experimental evaluations, and clearer explanations of their methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" presents an attempt to improve Monte Carlo Tree Search (MCTS) methods in reinforcement learning, specifically in the context of games like tic-tac-toe. However, there are several critical issues that must be addressed when evaluating its suitability for a top-tier conference.\n\n1. **Writing Quality and Clarity**: All three reviewers unanimously agree that the paper is poorly written, with significant grammatical issues and a lack of clarity. Reviewer 2 explicitly mentions that they could not understand the paper despite spending time on it. This lack of clarity makes it difficult for readers to grasp the contributions and significance of the work, which is a fundamental requirement for any academic paper.\n\n2. **Lack of References**: The paper does not include any references, which is a major flaw. Academic papers must situate their contributions within the context of existing literature. The absence of references suggests a lack of engagement with prior work and raises questions about the novelty and originality of the research.\n\n3. **Scientific Rigor and Completeness**: Reviewers note that the paper lacks the preliminary elements of scientific writing, with undefined mathematical symbols and algorithm boxes that are difficult to follow. Reviewer 3 mentions that much of the paper consists of blank space, indicating a lack of substantive content. This suggests that the paper is not yet a complete or rigorous academic work.\n\n4. **Author Response**: The author acknowledges the issues raised by the reviewers, including the lack of references and the difficulty in understanding the proposed method. While the author expresses a willingness to improve, the response does not provide any concrete solutions or revisions that address the reviewers' concerns.\n\n5. **Novelty and Contribution**: The abstract and the reviewers' comments suggest that the paper attempts to refine MCTS methods, but the lack of clarity and references makes it difficult to assess the novelty and contribution of the work. The author mentions discovering a similar idea in their country, which further complicates the assessment of originality.\n\nGiven these considerations, the paper does not meet the standards of a top-tier conference. The issues with writing, lack of references, and incomplete scientific presentation are significant barriers to acceptance. The author needs to substantially revise the paper, improve the writing, include relevant literature, and clearly articulate the contributions and novelty of the work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to the field of computational biology, specifically in predicting DNA folding patterns using a bidirectional LSTM RNN model. However, there are several critical issues that need to be addressed before it can be considered for publication in a top-tier conference.\n\n1. **Novelty and Contribution**: \n   - Reviewer 3 points out that the paper lacks methodological novelty and is similar to previously published works. The authors have attempted to address this by expanding the literature review and clarifying the differences between their work and existing studies. However, the response does not convincingly establish the novelty of the approach, especially since similar applications of machine learning to genomic data have been explored in the past.\n   - The authors claim that no other full published work predicts Topologically Associated Domain characteristics from ChIP-seq data, but this assertion is not strongly supported with evidence of novelty in methodology or results.\n\n2. **Clarity and Completeness**:\n   - There are significant issues with the clarity of the paper, particularly in the description of the data and methods. Reviewer 3 highlights that the feature representation and the rationale for using a bidirectional LSTM are not clearly explained. The authors have attempted to clarify these points in their response, but the explanations remain somewhat vague and do not fully resolve the reviewers' concerns.\n   - The paper lacks a clear explanation of why the chosen model architecture is particularly suited to the problem, beyond the general properties of DNA.\n\n3. **Evaluation and Baselines**:\n   - The evaluation of the model is a major concern. Reviewer 3 notes that the evaluation needs to be strengthened with additional baseline models and metrics. The authors have added more evaluation metrics and baseline comparisons in their response, but it is unclear if these additions sufficiently address the reviewers' concerns about the robustness and generalizability of the results.\n   - The use of a custom weighted MSE as a loss function is not adequately justified, and its impact on the results is not clearly distinguished from the model architecture itself.\n\n4. **Generalizability**:\n   - Reviewer 1 raises concerns about the generalizability of the approach to other datasets or types of data (e.g., ATAC-seq). The authors acknowledge that their model is specific to the Drosophila dataset and do not provide evidence of its applicability to other organisms or data types, which limits the broader impact of the work.\n\n5. **Response to Reviewers**:\n   - While the authors have made efforts to address the reviewers' comments, many of the responses are either insufficiently detailed or do not fully resolve the issues raised. For instance, the explanation of the bidirectional LSTM's suitability remains superficial, and the novelty claim is not convincingly substantiated.\n\nIn conclusion, while the paper presents an interesting application of machine learning to a biological problem, it falls short in terms of novelty, clarity, and evaluation rigor required for a top-tier conference. The authors need to provide a more compelling demonstration of the novelty and significance of their work, improve the clarity of their methodology, and strengthen the evaluation with more robust comparisons and generalizability assessments.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to hierarchical reinforcement learning (HRL) for multi-agent systems, focusing on skill coordination. The authors propose a modular framework that first trains sub-skills of each end-effector independently and then coordinates these skills using diverse behaviors. The paper addresses a significant problem in multi-agent reinforcement learning (MARL) and demonstrates empirical success in complex tasks involving coordination between multiple agents.\n\n### Strengths:\n1. **Novelty and Impact**: The paper introduces a novel approach to skill coordination in multi-agent systems, which is a critical problem in HRL. The proposed method could have a significant impact on the field, especially in applications requiring complex coordination.\n   \n2. **Empirical Results**: The authors provide empirical evidence showing that their method outperforms existing baselines in challenging tasks. The inclusion of videos and code enhances the reproducibility and transparency of the results.\n\n3. **Comprehensive Author Response**: The authors have addressed the reviewers' concerns comprehensively. They provided additional experiments, clarified the role of temporal abstraction, and discussed the limitations and applicability of their method.\n\n4. **Clarity and Presentation**: Despite some initial concerns about notation and presentation, the authors have revised the paper to improve clarity. The inclusion of additional baselines and analysis strengthens the paper's contributions.\n\n### Weaknesses:\n1. **High Variance in Results**: Reviewer 1 noted high variance in performance, which the authors addressed by running more seeds and applying a moving average. However, high variance can still be a concern in evaluating the robustness of the method.\n\n2. **Dependence on Predefined Subtasks**: The method relies on predefined subtasks, which may limit its applicability to tasks that can be decomposed a priori. This was a concern for Reviewer 3, who suggested exploring DIAYN without predefined subtasks.\n\n3. **Limited Discussion on Alternative Methods**: While the authors provided some discussion on alternative multi-agent methods, the paper could benefit from a more thorough comparison with recent advancements in MARL.\n\n4. **Complexity of Implementation**: The approach involves multiple components (e.g., skill discovery, meta-policy), which may complicate implementation and integration into existing systems.\n\n### Conclusion:\nThe paper presents a well-motivated and empirically validated approach to skill coordination in multi-agent systems. The authors have addressed the reviewers' concerns effectively, providing additional experiments and clarifications. While there are some limitations, such as reliance on predefined subtasks and high variance, the paper's contributions to the field of HRL and MARL are significant. The method's potential impact and the authors' responsiveness to feedback make a strong case for acceptance.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Automatic Music Production Using Generative Adversarial Networks\" presents a novel approach to automatic music accompaniment using CycleGANs and Mel-spectrograms. While the topic is intriguing and relevant to the field of music information retrieval and AI-driven music production, there are several critical issues that need to be addressed before it can be considered for publication in a top-tier conference.\n\n### Strengths:\n1. **Novelty and Relevance**: The paper tackles an interesting problem of automatic music accompaniment, which is a less explored area compared to other music generation tasks. The use of CycleGANs for this purpose is a novel application.\n2. **Potential Impact**: If successful, the approach could significantly impact music production by providing tools for automatic accompaniment generation, which could be useful for artists and producers.\n3. **Combination of Evaluation Methods**: The paper combines subjective human evaluations with objective metrics, which is a comprehensive approach to evaluating generative models in music.\n\n### Weaknesses:\n1. **Title and Scope**: The title is misleading as it suggests a broader scope of \"Automatic Music Production\" rather than the more specific task of \"Automatic Music Accompaniment.\" This could mislead readers about the paper's contributions.\n2. **Literature Review**: The paper lacks citations and discussions on relevant literature, particularly in the symbolic domain of music generation. This oversight limits the contextual understanding of the work within the broader field.\n3. **Methodological Details**: There are significant omissions in the methodological details, particularly concerning the CycleGAN architecture, the source separation process using DEMUCS, and the phase retrieval method. These details are crucial for reproducibility and understanding the technical contributions.\n4. **Experimental Design**: The experiments lack depth and variety. The paper does not explore alternative models or configurations, which limits the understanding of the proposed method's strengths and weaknesses. The choice of tasks and datasets is also narrow.\n5. **Evaluation Metrics**: The proposed evaluation metric, while innovative, lacks sufficient justification and clarity. The binarization of scores and the subjective nature of the evaluation raise concerns about the robustness and reliability of the results.\n6. **Reproducibility**: The lack of shared code and detailed dataset descriptions hinders the reproducibility of the experiments, which is a critical aspect of scientific research.\n\n### Author Responses:\nThe authors have addressed some of the reviewers' concerns, such as providing more details on the phase retrieval process and clarifying certain methodological aspects. However, many responses indicate that the issues raised are either acknowledged but not resolved (e.g., the necessity of the FMA dataset) or deferred to future work (e.g., data quality requirements). The responses also reveal a lack of comprehensive experimental validation and a reliance on subjective evaluations without sufficient objective grounding.\n\n### Conclusion:\nWhile the paper presents a novel application of CycleGANs to music accompaniment, the numerous methodological, experimental, and evaluative shortcomings prevent it from meeting the standards of a top-tier conference. The authors need to address these issues comprehensively, particularly in terms of methodological transparency, experimental rigor, and evaluation robustness, before the work can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"Training independent subnetworks for robust prediction\" presents a novel approach to improving robustness and uncertainty estimation in neural networks by training multiple independent subnetworks within a single model using a multi-input multi-output (MIMO) configuration. The authors claim that this method achieves the benefits of ensemble predictions without the computational overhead typically associated with multiple forward passes.\n\n**Strengths:**\n\n1. **Novelty and Simplicity:** The paper introduces a clever and simple method to leverage the capacity of a single model to train multiple subnetworks, which is a novel approach to achieving ensemble-like benefits without significant computational costs.\n\n2. **Empirical Results:** The experimental results are thorough and demonstrate significant improvements in robustness, accuracy, and calibration error across standard benchmarks like CIFAR-10, CIFAR-100, and ImageNet. The results are compelling and show the potential of the proposed method.\n\n3. **Clarity and Presentation:** The paper is well-written and easy to follow, with clear explanations of the method and detailed experimental analysis. The authors have addressed concerns about clarity in their response, improving the paper's presentation.\n\n4. **Addressing Reviewers' Concerns:** The authors have made efforts to address the reviewers' concerns, such as clarifying the source of the 1% increase in parameters and providing additional details on the computational costs and independence of subnetworks.\n\n**Weaknesses:**\n\n1. **Theoretical Analysis:** Several reviewers noted the lack of a theoretical foundation to explain the emergence of independence among subnetworks. While the authors provide empirical evidence, a theoretical understanding would strengthen the paper.\n\n2. **Originality and Related Work:** Some reviewers expressed concerns about the originality of the work, noting similarities with existing multi-branch architectures. The authors have expanded on the differences between MIMO and related methods, but the originality could still be perceived as limited.\n\n3. **Practical Limitations:** There are concerns about the practical limitations of the method, particularly regarding the capacity of a single network to effectively contain multiple subnetworks and the potential trade-offs in batch processing.\n\n4. **Computational Costs:** Although the authors claim that the computational costs are minimal, some reviewers remain skeptical about the practical implications, especially in terms of batch size and inference delay.\n\n**Conclusion:**\n\nThe paper presents a novel and interesting approach with strong empirical results, addressing a significant problem in neural network robustness and uncertainty estimation. While there are some concerns about the theoretical foundation and originality, the authors have made efforts to address these issues in their response. The method's simplicity and effectiveness, combined with the thorough experimental validation, make it a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\" introduces a novel exploration strategy for continuous reinforcement learning, leveraging both epistemic and aleatoric uncertainties. The proposed method, MQES, is evaluated on Mujoco environments and claims to outperform state-of-the-art algorithms.\n\n**Strengths:**\n1. **Novelty and Contribution**: The paper presents a novel approach to exploration in continuous RL by integrating information-theoretic principles with distributional RL. This is a significant contribution as it addresses the exploration-exploitation dilemma in a new way.\n2. **Theoretical Foundation**: The authors provide a theoretical basis for their approach, attempting to rigorously define the mutual information between the optimal Q function and the exploration policy.\n3. **Empirical Results**: The empirical results show that MQES can outperform existing methods in certain environments, particularly in sparse-reward settings.\n\n**Weaknesses:**\n1. **Clarity and Presentation**: Multiple reviewers noted that the paper is difficult to follow due to unclear explanations, notation issues, and a lack of detailed derivations. This significantly hampers the reader's ability to understand and evaluate the contributions.\n2. **Experimental Evaluation**: The experiments are criticized for being preliminary and lacking in detail. Important aspects such as hyperparameter sensitivity and ablation studies are either missing or insufficiently detailed. The choice of baselines is also questioned, with suggestions to include more relevant comparisons like VIME and OAC.\n3. **Theoretical Justification**: Some theoretical claims, particularly regarding the mutual information and its practical implications, are not well justified or clearly explained. This raises concerns about the validity of the proposed method.\n4. **Writing Quality**: The paper suffers from poor writing quality, with vague language and numerous typographical errors, which detracts from the overall presentation.\n\n**Author Responses:**\nThe authors have addressed some concerns by revising equations, adding explanations, and conducting additional experiments. However, many responses indicate that improvements are planned for future work, which suggests that the current submission is not yet polished or complete.\n\n**Conclusion:**\nWhile the paper presents an interesting idea with potential, the current submission falls short of the standards expected at a top-tier conference. The issues with clarity, experimental rigor, and theoretical justification are significant and need to be addressed before the work can be considered for publication. The authors are encouraged to refine their presentation, conduct more comprehensive experiments, and provide clearer theoretical insights in a future submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training using task-specific updates of model parameters. The authors claim that their method, $\\alpha$VIL, outperforms existing multitask learning approaches in various settings.\n\n**Strengths:**\n1. **Clarity and Writing:** The paper is well-written and easy to follow, as noted by all reviewers. The authors provide a clear summary of related work and articulate their methodology intuitively.\n2. **Novelty:** The approach of using direct meta-optimization for task weight estimation is novel, distinguishing it from existing methods like Discriminative Importance Weighting (DIW).\n3. **Empirical Evaluation:** The authors conduct experiments in both computer vision (MultiMNIST) and natural language understanding (NLU) domains, providing a broad evaluation of their method.\n\n**Weaknesses:**\n1. **Theoretical Justification:** A significant concern raised by all reviewers is the lack of theoretical justification or formal analysis of the proposed methodology. This is crucial for a top-tier conference, where a strong theoretical foundation is often expected.\n2. **Experimental Results:** The empirical results are not consistently superior to the baselines. The improvements are marginal and often fall within the range of statistical insignificance. This is particularly concerning given the competitive nature of top-tier conferences.\n3. **Experimental Setup:** The choice of datasets and experimental setup is questioned, especially the use of MultiMNIST with only one auxiliary task. This does not fully exploit the potential of multitask learning with multiple auxiliary tasks.\n4. **Novelty and Comparison:** While the method is novel, reviewers express concerns about its similarity to existing meta-learning approaches. The distinction between $\\alpha$VIL and other methods like MAML is not convincingly established.\n5. **Lack of Ablation Studies:** The paper lacks sufficient ablation studies and additional experiments to thoroughly demonstrate the efficacy and robustness of the proposed method.\n\n**Author Response:**\nThe authors acknowledge the lack of theoretical justification and plan to address this in a future version. They also provide explanations for some of the experimental choices and clarify aspects of their methodology. However, the responses do not fully alleviate the concerns about the empirical results and the need for more comprehensive experiments and analyses.\n\n**Conclusion:**\nWhile the paper presents an interesting approach to multitask learning, the lack of theoretical grounding, marginal empirical improvements, and insufficient experimental validation are significant drawbacks. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance. The authors are encouraged to strengthen the theoretical foundation, expand the experimental evaluation, and provide more detailed analyses in a future submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Global Node Attentions via Adaptive Spectral Filters\" presents a novel approach to graph neural networks (GNNs) by introducing a global self-attention mechanism using learnable spectral filters. The authors aim to address the limitations of existing GNNs, particularly in handling disassortative graphs where local homophily assumptions do not hold.\n\n### Strengths:\n1. **Novelty and Motivation**: The paper tackles an important problem in GNNs, specifically their limitations on disassortative graphs. The introduction of global attention and adaptive spectral filters is a promising direction to enhance GNN capabilities.\n2. **Empirical Evaluation**: The authors conduct extensive experiments on multiple datasets, demonstrating the proposed model's effectiveness, especially on disassortative graphs. The results show that the model outperforms state-of-the-art baselines in these scenarios.\n3. **Clarity and Exposition**: The paper is well-written and accessible, even to those not deeply familiar with the specific area, as noted by Reviewer 2.\n\n### Weaknesses:\n1. **Complexity Concerns**: Multiple reviewers, particularly Reviewer 1 and Reviewer 4, express concerns about the computational complexity of the proposed method. The authors claim that their method is efficient, but there is skepticism about the scalability due to the global attention mechanism.\n2. **Novelty and Contribution**: Reviewers 1 and 4 question the novelty of the approach, arguing that similar ideas have been explored in existing works. The authors' response attempts to clarify the differences, but the reviewers remain unconvinced.\n3. **Limited Theoretical Results**: Reviewer 2 notes the absence of theoretical results, which is a significant drawback for a top-tier conference. The paper relies heavily on empirical results without providing a strong theoretical foundation.\n4. **Dataset Limitations**: The datasets used for evaluation, particularly the disassortative ones, are relatively small. Reviewers suggest that larger datasets or synthetic graphs with controllable parameters could provide more robust validation of the method's scalability and effectiveness.\n\n### Author Responses:\nThe authors provide detailed responses to the reviewers' concerns, particularly regarding complexity and novelty. They clarify the use of Chebyshev polynomials for complexity reduction and emphasize the adaptive nature of their spectral filters. They also add new experimental results to address concerns about dataset size and runtime efficiency.\n\n### Conclusion:\nWhile the paper addresses an important problem and presents a potentially impactful solution, the concerns about computational complexity, scalability, and limited theoretical contributions are significant. The novelty of the approach is also questioned, and the authors' responses, while detailed, do not fully alleviate these concerns. For a top-tier conference, these issues are critical, and the paper may benefit from further refinement and additional experiments to address scalability and theoretical underpinnings.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform for multi-objective optimization using Bayesian optimization techniques. The paper is evaluated based on several criteria, including novelty, technical contribution, empirical validation, and clarity.\n\n### Detailed Reasoning:\n\n1. **Novelty and Technical Contribution:**\n   - The paper introduces a new platform, AutoOED, which is designed to be user-friendly and supports asynchronous multi-objective Bayesian optimization (MOBO). The authors claim that this is the first platform to introduce asynchronous MOBO.\n   - The main technical contribution is the Believer-Penalizer (BP) strategy for asynchronous optimization. However, reviewers have noted that this strategy is presented in a straightforward manner and lacks theoretical backing. The novelty of BP is questioned, as it seems to be a simple combination of existing methods (Kriging Believer and Local Penalization).\n   - The platform's modular design is highlighted as a strength, allowing for flexibility in algorithm development. However, the novelty of the platform itself is questioned, as it is seen as \"yet another Bayesian Optimization package\" with minor contributions.\n\n2. **Empirical Validation:**\n   - The empirical results are mixed. While the authors claim competitive performance, reviewers point out that AutoOED does not consistently outperform existing methods across benchmarks. The performance claims are seen as overstated, and the empirical results do not convincingly demonstrate the superiority of the proposed methods.\n   - The authors have conducted extensive evaluations on benchmark problems, but the results do not show a clear advantage over existing platforms. The real-world benchmark is also criticized for lacking a strong connection to practical applications.\n\n3. **Clarity and Presentation:**\n   - The paper is generally well-written and organized, making it accessible to readers. The inclusion of a graphical user interface (GUI) is a positive aspect, enhancing usability for non-experts.\n   - However, some claims in the paper are seen as misleading or overstated, particularly regarding the platform's capabilities and performance.\n\n4. **Reviewer and Author Interactions:**\n   - The authors have responded to reviewer concerns by updating the paper and addressing specific points. They have clarified the platform's capabilities and made some adjustments to the claims about BP's performance.\n   - Despite these efforts, the core concerns about the lack of significant scientific contribution and the overstated empirical results remain.\n\n5. **Overall Assessment:**\n   - While the platform may be useful from an engineering perspective, the paper lacks the level of scientific innovation and empirical rigor expected at a top-tier conference. The contributions are primarily engineering-oriented, with limited methodological advancements.\n   - The paper's focus on usability and modularity is commendable, but it does not compensate for the lack of novel scientific insights or strong empirical evidence.\n\n### Conclusion:\nGiven the limited novelty, overstated empirical claims, and the focus on engineering contributions over scientific advancements, the paper does not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents a novel approach to 3D snapshot microscopy using Fourier convolutional neural networks. The authors propose a method that combines deep learning decoders with a differentiable simulation of an optical encoder for end-to-end optimization. The paper claims to outperform existing UNet-based architectures in both simulation and real-world applications, such as lensless imaging.\n\n### Strengths:\n1. **Innovative Approach**: The use of Fourier convolutional networks to handle global PSFs in 3D snapshot microscopy is an interesting and potentially impactful idea.\n2. **Well-Written and Structured**: The paper is well-organized, making it accessible and understandable.\n3. **Promising Results**: The experimental results, although primarily in simulation, show promise and suggest that the proposed method could outperform existing techniques.\n4. **Thorough Limitations Section**: The authors have provided a detailed discussion of the limitations, which is appreciated.\n\n### Weaknesses:\n1. **Limited Novelty**: Several reviewers pointed out that the technical novelty is limited, as the main ideas have been explored in prior work, particularly in the Rippel et al. paper.\n2. **Lack of Real-World Validation**: The paper primarily relies on simulated data, and there is a significant gap between simulation and real-world application, which is a critical issue for a top-tier conference.\n3. **Insufficient Evaluation**: The evaluation is limited to a single type of volume and lacks diversity in testing scenarios. The lensless imaging results, while interesting, do not directly support the main focus of the paper on 3D snapshot microscopy.\n4. **Concerns About Fairness of Comparisons**: There are concerns about the fairness of comparisons, particularly regarding computational resources and parameter counts.\n\n### Author Responses:\nThe authors have addressed several concerns raised by the reviewers, including clarifying the novelty of their contributions, providing additional implementation details, and explaining the limitations of their work. They have also emphasized the potential impact of their work in the field of computational imaging.\n\n### Conclusion:\nWhile the paper presents an interesting application of Fourier convolutional networks to 3D snapshot microscopy, the limited technical novelty, lack of real-world validation, and insufficient evaluation are significant drawbacks. The paper may be more suitable for an application-specific conference or journal rather than a top-tier conference focused on machine learning innovations. The authors have made a strong case for the potential impact of their work, but the current form of the paper does not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" addresses an important issue in multi-modal learning, specifically the tendency of models to rely on a single modality, which can hinder generalization. The authors propose a novel metric, conditional learning speed, as a proxy for conditional utilization rate, and introduce a balanced multi-modal learning algorithm to mitigate this issue. The paper is evaluated based on several criteria, including correctness, technical and empirical novelty, and the strength of the experimental validation.\n\n**Strengths:**\n1. **Relevance and Novelty:** The topic is relevant and addresses a well-known issue in multi-modal learning. The introduction of conditional learning speed as a proxy for conditional utilization rate is a novel approach.\n2. **Clarity and Structure:** The paper is generally well-structured and easy to follow, making it accessible to readers.\n3. **Empirical Validation:** The authors conduct experiments on multiple datasets, demonstrating the effectiveness of their proposed method in improving generalization.\n\n**Weaknesses:**\n1. **Experimental Support:** Several reviewers noted that the experimental results do not strongly support the main claims. The differences in performance between the proposed method and baselines are not statistically significant in some cases, particularly on the ModelNet40 and NVGesture datasets.\n2. **Theoretical Foundation:** The paper lacks a solid theoretical foundation for the proposed hypothesis and method. The authors rely heavily on empirical observations without providing a rigorous theoretical analysis.\n3. **Comparison with State-of-the-Art:** The paper does not adequately compare the proposed method with existing state-of-the-art methods addressing similar issues. Reviewers pointed out the lack of comparison with methods like RUBi and others that tackle modality imbalance.\n4. **Dataset Limitations:** The experiments are limited to datasets with visual modalities, which may not fully demonstrate the generalizability of the method to other types of modalities, such as audio-visual datasets.\n5. **Parameter Sensitivity:** There is insufficient analysis of the sensitivity of the proposed method to hyperparameters, which could impact its applicability to different datasets.\n\n**Author Responses:**\nThe authors have addressed some of the reviewers' concerns by providing additional experiments, clarifying certain aspects of their method, and improving the manuscript's readability. However, the responses do not fully resolve the issues related to the strength of experimental validation, theoretical grounding, and comparison with existing methods.\n\n**Conclusion:**\nWhile the paper addresses an interesting and relevant problem in multi-modal learning, the current version lacks sufficient experimental and theoretical support to meet the standards of a top-tier conference. The empirical results, although promising, are not robust enough to convincingly demonstrate the superiority of the proposed method over existing approaches. Additionally, the lack of a strong theoretical foundation and comprehensive comparisons with state-of-the-art methods further weaken the paper's contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"The weighted mean trick – optimization strategies for robustness\" presents an interesting approach to optimizing higher-order moments of the loss distribution through a weighted mean trick. The authors claim that this method can improve robustness against outliers and provide a stronger theoretical background compared to other robust loss functions. However, the paper has received mixed reviews from the reviewers, with several significant concerns raised.\n\n**Strengths:**\n1. **Interesting Problem:** The paper addresses an interesting problem of optimizing higher-order moments to improve robustness, which could have potential applications in robust learning algorithms.\n2. **Theoretical Insights:** The paper provides theoretical insights into how weighted means can be used to penalize variance and higher-order moments.\n3. **Empirical Validation:** The authors have conducted experiments to validate their approach, showing similar performance to other robust loss functions.\n\n**Weaknesses:**\n1. **Lack of Theoretical Rigor:** Several reviewers pointed out that the theoretical contributions are limited and not well-supported. The paper lacks a clear theoretical justification for the choice of weights and the impact of negative weights.\n2. **Algorithmic Concerns:** There are concerns about the proposed algorithm not being a true gradient descent method, as it does not include all necessary terms in the gradient update.\n3. **Clarity and Coherence:** The paper suffers from clarity issues, with several reviewers finding the notation and explanations confusing. The connection between different sections of the paper is not well-established.\n4. **Novelty and Significance:** The contributions are considered marginally significant or novel by several reviewers. The results are seen as straightforward and not extending significantly beyond existing work.\n5. **Practical Concerns:** The practical aspects of the approach, such as the choice of weights and the impact of clipping negative weights, are not well-addressed. There is also a lack of convergence guarantees for the proposed algorithm.\n\n**Author Responses:**\nThe authors have attempted to address some of the concerns raised by the reviewers, such as updating notations, providing new experimental results, and clarifying certain points. However, the responses do not fully resolve the fundamental issues related to theoretical rigor, algorithmic correctness, and clarity.\n\n**Conclusion:**\nWhile the paper presents an interesting idea, the significant concerns regarding theoretical rigor, algorithmic correctness, and clarity cannot be overlooked. The contributions are not sufficiently novel or significant for a top-tier conference, and the paper does not meet the high standards required for acceptance. The authors should consider addressing these issues in a future submission, potentially with a more focused and rigorous theoretical framework and clearer presentation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical analysis of the Feedback Alignment (FA) algorithm, focusing on its convergence properties and implicit regularization effects in deep linear networks. The paper has been reviewed by four reviewers, each providing detailed feedback on various aspects of the work.\n\n**Strengths:**\n\n1. **Theoretical Contribution:** The paper provides a rigorous theoretical framework for understanding the convergence of the FA algorithm in deep linear networks. This is a significant contribution, as FA is an alternative to backpropagation that has not been thoroughly analyzed in the literature.\n\n2. **Novel Observations:** The paper introduces the concept of implicit anti-regularization, which is a novel finding that could have implications beyond the specific setting of FA. This observation is considered interesting by multiple reviewers.\n\n3. **Clarity and Structure:** The paper is well-written and structured, making it accessible to readers. The authors have clearly stated their assumptions and provided detailed proofs for their claims.\n\n4. **Numerical Experiments:** Although limited, the numerical experiments conducted support the theoretical findings and provide additional insights into the behavior of FA.\n\n**Weaknesses:**\n\n1. **Limited Scope:** The analysis is restricted to linear networks with specific initialization schemes (spectral initialization), which limits the generalizability of the results. This is a significant concern, as practical neural networks often use non-linear activations and random initializations.\n\n2. **Empirical Validation:** The paper lacks extensive empirical validation, particularly under non-spectral initialization conditions. This raises questions about the practical relevance of the theoretical findings.\n\n3. **Assumptions and Simplifications:** The assumptions made for the analysis, such as the diagonalizability of matrices and specific initialization schemes, may oversimplify the problem and limit the applicability of the results to real-world scenarios.\n\n4. **Comparison with Backpropagation:** While the paper provides insights into the FA algorithm, it does not thoroughly compare FA with backpropagation, especially in terms of practical performance and advantages.\n\n**Author Responses:**\n\nThe authors have addressed several concerns raised by the reviewers, including clarifying the limitations of spectral initialization and adding visual tools to aid understanding. They have also provided additional experiments and theoretical insights in the revised version. However, some concerns, particularly regarding the generalizability of the results and the lack of extensive empirical validation, remain only partially addressed.\n\n**Conclusion:**\n\nWhile the paper makes a valuable theoretical contribution to the understanding of the FA algorithm, the limitations in scope and empirical validation are significant drawbacks for a top-tier conference. The novelty and significance of the contributions are somewhat diminished by the restrictive assumptions and lack of practical applicability. Given the high standards of a top-tier conference, these issues are critical.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning Sample Reweighting for Adversarial Robustness\" presents a novel approach to adversarial training by introducing a bi-level optimization framework that learns to reweight training samples based on a multi-class margin. The authors claim that this method improves both clean and robust accuracy compared to existing techniques.\n\n### Strengths:\n1. **Novelty in Approach**: The paper introduces a learning-based reweighting strategy for adversarial robustness, which is a departure from heuristic methods. The use of a parametric function to map multi-class margins to importance weights is a novel contribution.\n2. **Methodological Soundness**: The approach is methodologically sound, leveraging MAML-based techniques for bi-level optimization.\n3. **Comprehensive Evaluation**: The paper includes extensive experiments against strong adversaries like AutoAttack, demonstrating the efficacy of the proposed method.\n4. **Clarity and Organization**: The paper is well-written and organized, making it accessible to readers.\n\n### Weaknesses:\n1. **Limited Technical Novelty**: Several reviewers pointed out that the adaptation of MAML to adversarial training, while novel in this context, is not a groundbreaking technical contribution. The novelty is somewhat incremental.\n2. **Evaluation Concerns**: There are significant concerns about the evaluation, particularly the lack of adaptive attack evaluations, which are crucial for a fair assessment of adversarial robustness.\n3. **Performance Marginality**: The improvements in performance are marginal, especially when compared to state-of-the-art baselines like AWP. The reliance on TRADES for competitive results further diminishes the standalone impact of BiLAW.\n4. **Theoretical Justification**: The paper lacks a strong theoretical justification for the use of multi-class margins and the specific design choices in the reweighting mechanism.\n5. **Incomplete Experiments**: Some experiments, such as those on CIFAR-100, are missing, and there is a lack of ablation studies to isolate the contributions of different components of the method.\n\n### Author Responses:\nThe authors provided detailed responses to the reviewers' concerns, including preliminary experiments on adaptive attacks and clarifications on the novelty and performance of their method. However, many of the responses indicate that further work is needed to address the reviewers' concerns fully, such as additional experiments and theoretical justifications.\n\n### Conclusion:\nWhile the paper presents an interesting approach and shows some promise, the concerns raised by the reviewers, particularly regarding the novelty, evaluation completeness, and marginal performance improvements, are significant. The paper does not fully meet the high standards of a top-tier conference, which typically requires both strong theoretical contributions and comprehensive empirical validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Target Propagation via Regularized Inversion\" presents a novel approach to target propagation (TP) by introducing a regularized inversion method. The authors aim to provide an alternative to backpropagation (BP) that is easily implementable in differentiable programming frameworks and potentially advantageous for training recurrent neural networks (RNNs) on long sequences.\n\n**Strengths:**\n1. **Novelty and Implementation:** The paper introduces a new variant of TP that is straightforward to implement using autograd systems, which is a practical contribution.\n2. **Theoretical Insights:** The paper provides some theoretical analysis, including a bound on the difference between TP and BP.\n3. **Potential for RNNs:** The approach is particularly targeted at RNNs, which are known to suffer from vanishing and exploding gradient problems, suggesting a potential niche application.\n\n**Weaknesses:**\n1. **Lack of Strong Theoretical Guarantees:** Reviewers noted the absence of strong theoretical guarantees that the proposed method leads to faster or more reliable optimization compared to BP. The theoretical analysis provided is considered insufficiently explored and lacks depth.\n2. **Experimental Validation:** The experimental results are not convincing. The paper lacks extensive empirical validation, with experiments showing low accuracy on standard datasets like CIFAR-10. This raises concerns about the method's generalizability and practical utility.\n3. **Comparison with Existing Methods:** The paper does not adequately compare the proposed method with existing alternatives, such as feedback alignment or Hessian-free optimization, which limits the understanding of its relative advantages.\n4. **Scope and Applicability:** The method's reliance on weight transport and its higher computational cost compared to BP limit its applicability, especially in settings where these factors are critical, such as neuromorphic hardware or large-scale distributed systems.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting idea, it lacks the necessary theoretical and empirical support to justify its claims. The novelty is considered marginal, and the experimental results do not convincingly demonstrate the method's superiority over existing approaches. The paper also fails to address key questions about the method's applicability and performance in realistic scenarios.\n\n**Conclusion:**\nGiven the paper's weaknesses in theoretical justification, empirical validation, and comparison with existing methods, it does not meet the high standards required for a top-tier conference. The lack of an author response further suggests that the authors may not have addressed these concerns adequately.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"On the Implicit Biases of Architecture & Gradient Descent\" explores the generalization capabilities of neural networks, focusing on the roles of architecture and gradient descent. The authors present theoretical results, including new PAC-Bayes bounds, and conduct experiments to understand the implicit biases in neural networks.\n\n### Strengths:\n1. **Theoretical Contributions**: The paper introduces new PAC-Bayes bounds that are non-vacuous and analytically tractable, which is a significant achievement in the context of neural networks. This is acknowledged by multiple reviewers as a strong theoretical contribution.\n2. **Novel Insights**: The paper provides insights into the role of margin in generalization, suggesting that gradient descent can target large margin solutions, which may lead to better generalization.\n3. **Clarity and Writing**: The paper is generally well-written and structured, making complex theoretical concepts accessible to the reader.\n\n### Weaknesses:\n1. **Experimental Validation**: Several reviewers point out that the experimental validation is limited and not comprehensive. The experiments are primarily conducted with the \"Nero\" optimizer, which is not standard, and there is a lack of comparison with other optimizers like SGD or Adam. This raises concerns about the generality of the results.\n2. **Incremental Contributions**: Some reviewers feel that the contributions, particularly the PAC-Bayes bounds, are incremental and build heavily on prior work (e.g., Valle-Pérez et al. (2019)). The lack of direct comparison with previous bounds is also noted.\n3. **Lack of Rigor in Margin Analysis**: The analysis of the implicit bias of gradient descent related to margin is seen as lacking rigor and clarity. Reviewers express confusion about the claims and their implications, suggesting that the discussion could be more formally presented.\n4. **Limited Applicability**: The theoretical results are primarily applicable to MLPs and do not extend to more complex architectures like CNNs or ResNets, which limits the practical significance of the findings.\n\n### Author Responses:\nThe authors have provided clarifications and additional context in their responses, addressing some misunderstandings and emphasizing the novelty of their findings regarding margin and generalization. However, the responses do not fully alleviate concerns about the experimental setup and the generality of the results.\n\n### Conclusion:\nWhile the paper presents interesting theoretical results and insights into the implicit biases of neural networks, the experimental validation is insufficient for a top-tier conference. The reliance on a non-standard optimizer and the lack of comprehensive comparisons with existing methods weaken the empirical contributions. Additionally, the theoretical contributions, though significant, are seen as incremental by some reviewers. Given these considerations, the paper does not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical insights in meta-learning and practical applications, specifically through the introduction of regularization terms inspired by theoretical assumptions. The paper has received mixed reviews from the four reviewers, with a general leaning towards rejection. Here is a detailed analysis of the paper based on the provided information:\n\n1. **Novelty and Contribution**:\n   - The paper attempts to apply theoretical insights into practical meta-learning algorithms by introducing regularization terms. However, several reviewers have pointed out that the proposed regularizations are not novel and resemble existing techniques like spectral normalization and weight decay. The authors argue that their approach is fundamentally different, but this distinction does not seem to be convincingly demonstrated in the paper.\n\n2. **Theoretical and Practical Consistency**:\n   - Reviewer 2 and Reviewer 3 raised concerns about the consistency of the theoretical setting with the practical few-shot learning setting. The authors have attempted to address these concerns in their response, but it remains unclear if the theoretical assumptions can be effectively enforced or respected in practice, especially when the assumptions may not hold for the learning problem at hand.\n\n3. **Experimental Validation**:\n   - The experimental results show some improvement over baseline meta-learning algorithms. However, the improvements are often within the range of standard deviation, casting doubt on their significance. The authors have added more baselines and tuned hyperparameters in response to reviewer feedback, but the overall impact of these changes on the paper's contribution is not clear.\n\n4. **Clarity and Presentation**:\n   - The paper is generally well-organized and clearly written, as noted by Reviewer 4. However, there are concerns about the clarity of the technical details, particularly regarding the calculation of subgradients and the implementation of the proposed regularizations.\n\n5. **Reviewer Consensus**:\n   - The reviewers generally appreciate the motivation and the attempt to connect theory with practice. However, they express skepticism about the novelty, significance, and practical applicability of the proposed methods. The authors' responses address some concerns but do not fully resolve the issues raised by the reviewers.\n\n6. **Author Response**:\n   - The authors have made efforts to address the reviewers' concerns by adding more experiments, clarifying their approach, and adjusting the narrative of the paper. Despite these efforts, the fundamental concerns about novelty and practical significance remain.\n\nIn conclusion, while the paper presents an interesting attempt to apply theoretical insights to practical meta-learning, the lack of novelty in the proposed methods, the questionable significance of the experimental results, and the unresolved concerns about theoretical consistency suggest that the paper does not meet the standards of a top-tier conference at this time.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper presents a novel approach to improving the efficiency of diffusion models, specifically focusing on reducing the number of inference steps required to generate high-quality samples. The proposed method, Differentiable Diffusion Sampler Search (DDSS), introduces a new family of samplers called Generalized Gaussian Diffusion Models (GGDM) and optimizes them using a perceptual loss that is closely related to the Kernel Inception Distance (KID).\n\n**Strengths:**\n\n1. **Problem Significance:** The paper addresses a critical issue in diffusion models, which is the high computational cost associated with generating samples. This is a relevant and timely problem in the field of generative models.\n\n2. **Empirical Results:** The paper demonstrates significant improvements in sample quality with fewer inference steps compared to existing methods like DDPM and DDIM. The results on datasets like CIFAR-10 and ImageNet 64x64 are compelling, showing that the method can achieve better sample quality in fewer steps.\n\n3. **Theoretical Grounding:** The authors have addressed concerns about the theoretical motivation of their perceptual loss by connecting it to KID, providing a clearer understanding of the distribution matching happening in their optimization process.\n\n4. **Clarity Improvements:** The authors have made efforts to improve the clarity of the paper, addressing specific concerns raised by reviewers regarding notation and the explanation of their method.\n\n**Weaknesses:**\n\n1. **Theoretical Guarantees:** While the authors have improved the theoretical motivation of their perceptual loss, there are still concerns about the lack of theoretical guarantees regarding the distribution that the samples are optimized to match. The method's reliance on empirical results without strong theoretical backing could be a limitation.\n\n2. **Generality and Robustness:** The performance of the proposed method on larger and more diverse datasets is still somewhat unclear. The authors have promised additional results on challenging datasets for the camera-ready version, but these are not currently available.\n\n3. **Clarity and Complexity:** Despite improvements, some reviewers still found the paper's clarity lacking, particularly regarding the explanation of the GGDM and the optimization process. The complexity of the method and its reliance on specific computational tools (e.g., gradient rematerialization) may limit its accessibility and practicality.\n\n4. **Novelty Concerns:** Some aspects of the contributions, such as the use of non-Markovian samplers, are seen as extensions of existing work (e.g., DDIM), which may limit the perceived novelty of the approach.\n\n**Conclusion:**\n\nThe paper presents a promising approach to improving the efficiency of diffusion models, with strong empirical results and a reasonable theoretical foundation. However, the concerns about theoretical guarantees, generality, and clarity are significant for a top-tier conference. While the authors have made substantial revisions to address these issues, the paper may still benefit from further refinement and additional empirical validation on larger datasets.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform aimed at facilitating optimal experimental design using multi-objective Bayesian optimization (MOBO). The paper introduces a novel strategy called Believer-Penalizer (BP) for asynchronous optimization and emphasizes the platform's user-friendly graphical user interface (GUI).\n\n### Detailed Reasoning:\n\n1. **Novelty and Contribution:**\n   - The paper's primary contribution is the development of a software platform, AutoOED, which integrates existing MOBO techniques and introduces the BP strategy for asynchronous optimization.\n   - The novelty of the BP strategy is questioned by multiple reviewers, who find it to be a straightforward combination of existing methods without substantial theoretical backing.\n   - The platform's novelty is further diminished by the fact that it primarily implements existing techniques with minor contributions, as noted by several reviewers.\n\n2. **Technical and Empirical Evaluation:**\n   - The empirical results presented in the paper are not compelling. Reviewers note that the performance of AutoOED is mixed, with the BP strategy not consistently outperforming existing methods.\n   - The paper's claims about the platform's performance are seen as overstated, with figures showing that AutoOED underperforms on several benchmarks.\n   - The authors have attempted to address these concerns in their response by updating the paper with additional comparisons and clarifications, but the fundamental issues with the empirical evaluation remain.\n\n3. **Engineering and Usability:**\n   - The platform is praised for its engineering aspects, particularly its user-friendly GUI and modular design, which could benefit non-expert users and facilitate the development of new MOBO algorithms.\n   - However, the focus on engineering contributions over scientific innovation is a point of contention, as top-tier conferences typically prioritize novel methodological contributions.\n\n4. **Scope and Limitations:**\n   - The platform initially supported only 2 or 3 objectives, which is a significant limitation for a tool claiming to handle multi-objective optimization. The authors have since addressed this by extending support to single and many-objective scenarios.\n   - The paper's scope is more aligned with system and software engineering rather than groundbreaking scientific research, which may not meet the expectations of a top-tier conference.\n\n5. **Reviewer Consensus:**\n   - The majority of reviewers express concerns about the lack of significant scientific contributions and the overstated claims regarding the platform's performance.\n   - While the platform is well-engineered, the consensus is that it does not offer enough methodological innovation to warrant acceptance at a top-tier conference.\n\n### Conclusion:\nGiven the emphasis on engineering over scientific novelty, the mixed empirical results, and the lack of substantial methodological contributions, the paper does not meet the standards expected for acceptance at a top-tier conference. The authors are encouraged to further develop the theoretical aspects of their contributions and provide more robust empirical evidence to support their claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\" presents a novel approach to improving the efficiency of diffusion models, which are known for their high sample quality but require numerous inference steps. The authors propose Differentiable Diffusion Sampler Search (DDSS) and Generalized Gaussian Diffusion Models (GGDM) to address this issue. The paper has undergone revisions based on reviewer feedback, and the authors have made efforts to clarify and theoretically ground their methodology.\n\n### Strengths:\n1. **Problem Importance**: The paper addresses a significant problem in the field of generative models, specifically the inefficiency of diffusion models during inference.\n2. **Novel Approach**: The introduction of DDSS and GGDM provides a new method for optimizing samplers, which is compatible with pre-trained diffusion models without requiring fine-tuning.\n3. **Empirical Results**: The paper demonstrates strong empirical results, showing improved sample quality with fewer inference steps compared to existing methods like DDPM and DDIM.\n4. **Theoretical Clarifications**: The authors have addressed concerns about the theoretical grounding of their approach by connecting their perceptual loss to the Kernel Inception Distance (KID), providing a clearer understanding of the distribution matching process.\n5. **Revisions and Improvements**: The authors have made significant revisions to improve clarity and address reviewer concerns, including providing more detailed explanations and additional empirical evidence.\n\n### Weaknesses:\n1. **Theoretical Guarantees**: While the authors have improved the theoretical motivation, some reviewers still express concerns about the lack of theoretical guarantees regarding the optimality of the proposed models and the choice of loss function.\n2. **Clarity and Presentation**: Despite improvements, some reviewers found the paper's clarity lacking, particularly in the explanation of the GGDM and the use of notations.\n3. **Limited Dataset Evaluation**: The experiments are primarily conducted on smaller datasets like CIFAR-10 and ImageNet 64x64. There is a desire for results on larger, more challenging datasets to better demonstrate the method's scalability and robustness.\n4. **Novelty Concerns**: Some reviewers feel that aspects of the contributions exist in prior work, and the novelty of the GGDM without DDSS is questioned.\n\n### Conclusion:\nThe paper presents a promising approach to improving the efficiency of diffusion models, with strong empirical results and a novel optimization method. The authors have made substantial efforts to address reviewer concerns, particularly regarding theoretical grounding and clarity. However, some concerns remain about the theoretical guarantees and the breadth of empirical evaluation.\n\nGiven the importance of the problem, the novelty of the approach, and the improvements made in response to feedback, the paper is a valuable contribution to the field. The remaining concerns, while valid, do not outweigh the potential impact and significance of the work.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Boosting Approach to Reinforcement Learning\" presents an interesting and potentially impactful approach to reinforcement learning (RL) by leveraging boosting techniques from supervised learning. The authors propose an algorithm that improves the accuracy of weak learners iteratively, with sample complexity and running time that do not explicitly depend on the number of states. This is a significant contribution, especially for large-scale RL problems.\n\n**Strengths:**\n1. **Theoretical Contribution:** The paper provides a solid theoretical foundation for the proposed algorithm, with rigorous analysis and proofs. The approach of using a non-convex variant of the Frank-Wolfe method to handle non-convexity in the value function is innovative.\n2. **Novelty:** While some reviewers noted similarities to prior work (Hazan and Singh 2021), the authors argue that their approach does not require convex cost assumptions, which is a critical difference. This distinction adds novelty to their work.\n3. **Potential Impact:** The independence of the sample complexity from the number of states is a compelling result that could have significant implications for solving large-scale RL problems.\n\n**Weaknesses:**\n1. **Clarity and Presentation:** Multiple reviewers highlighted issues with the paper's clarity, notation overload, and lack of intuitive explanations. The paper is described as dense and difficult to follow, which could hinder its accessibility and impact.\n2. **Empirical Validation:** The absence of empirical results is a notable gap. While the authors focus on theoretical contributions, initial experiments could provide valuable insights into the practical applicability and performance of the proposed algorithm.\n3. **Incremental Novelty:** Some reviewers felt that the technical novelty was somewhat incremental, given the similarities to existing work. The authors' response clarifies some differences, but the perception of limited novelty remains a concern.\n4. **Practicality Concerns:** There are questions about the practicality of the proposed algorithm, particularly regarding the assumptions made and the applicability of the weak learner model.\n\n**Author Responses:**\nThe authors have acknowledged the feedback and committed to addressing the clarity and notation issues in the final version. They also provided additional context and justification for their assumptions and the novelty of their approach. However, the lack of empirical validation remains unaddressed, and the paper's clarity issues are significant enough to warrant concern.\n\n**Conclusion:**\nWhile the paper presents a theoretically sound and potentially impactful approach to RL, the issues with clarity, presentation, and lack of empirical validation are significant drawbacks. For a top-tier conference, these aspects are crucial for ensuring that the work is accessible and its contributions are clearly communicated to the broader community. The authors' willingness to address these issues is commendable, but without concrete revisions and empirical results, the paper may not meet the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Natural Language Descriptions of Deep Visual Features\" presents a novel approach, MILAN, for interpreting deep learning models in computer vision by generating natural language descriptions of neuron activations. The paper is well-received by the reviewers, with several strengths and some areas for improvement noted.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a novel method for generating natural language descriptions of neuron activations, which is a significant contribution to the field of model interpretability. The approach is technically sound and well-motivated, addressing a gap in the current methods that only label a small subset of neurons.\n\n2. **Technical Soundness**: The method is described in detail, and the authors provide sufficient technical details for reproducibility. The use of mutual information to guide the generation of descriptions is a clever approach that aligns well with the goal of capturing the richness of neuron activations.\n\n3. **Empirical Validation**: The paper includes extensive experiments demonstrating the effectiveness of MILAN across different model architectures, datasets, and tasks. The results show that MILAN achieves higher agreement with human annotations compared to baseline methods.\n\n4. **Applications**: The paper highlights three applications of MILAN, showcasing its utility in model analysis, auditing, and editing. These applications demonstrate the practical relevance of the method.\n\n5. **Open Science**: The authors commit to open-sourcing the data, code, and trained models, which is a positive aspect for the research community.\n\n### Areas for Improvement:\n1. **Inter-Annotator Agreement**: Reviewer 2 points out that the inter-annotator agreement in the MILANNOTATIONS dataset is not very high. The authors address this by explaining that MILAN is designed to match at least one of the human descriptions, which justifies the higher BERTScore for MILAN.\n\n2. **Scope and Generalization**: Reviewer 4 suggests that the paper's scope should be clarified, as the experiments are focused on vision models. The authors have addressed this by changing the title to reflect the focus on visual features.\n\n3. **Baseline Comparisons**: Reviewer 4 suggests comparing MILAN with off-the-shelf image captioning models. The authors conducted additional experiments showing that these models perform poorly on the task, supporting the novelty and effectiveness of MILAN.\n\n4. **Limitations and Future Work**: Reviewers suggest discussing the limitations of the approach and exploring its application to more diverse datasets and tasks. The authors acknowledge these points and indicate plans for future work.\n\n5. **Bias and Ethical Considerations**: The authors acknowledge potential biases in the MILANNOTATIONS dataset and suggest that addressing these biases would be an area for future research.\n\n### Conclusion:\nThe paper presents a significant advancement in the interpretability of deep learning models in computer vision. The method is novel, technically sound, and empirically validated with extensive experiments. The authors have addressed most of the reviewers' concerns and have shown a commitment to improving the paper based on feedback. The potential applications of MILAN in model analysis, auditing, and editing further enhance its significance.\n\nGiven the strengths of the paper and the authors' responsiveness to feedback, the paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" presents a novel approach to generating synthetic time-series data using Variational Auto-Encoders (VAEs). The authors claim that their method offers interpretability, the ability to encode domain knowledge, and reduced training times compared to existing methods. However, the paper has received mixed reviews from the reviewers, with several significant concerns raised.\n\n### Strengths:\n1. **Clarity and Writing**: The paper is generally well-written and clearly conveys the main points of the proposed models.\n2. **Interesting Approach**: The integration of interpretability and domain knowledge into the VAE framework is an interesting idea that could potentially address some limitations of existing generative models for time-series data.\n\n### Weaknesses:\n1. **Lack of Novelty**: Several reviewers pointed out that the proposed models do not present substantial originality. The base model appears to be a standard VAE applied to time-series data without significant modifications or innovations.\n2. **Insufficient Empirical Evaluation**: The empirical evaluation is limited and does not convincingly demonstrate the superiority of TimeVAE over existing methods. The experiments are conducted on novel datasets, making it difficult to compare with other works in the literature.\n3. **Interpretability Claims**: The paper claims interpretability as a key contribution, but there is no empirical evidence or discussion provided to support this claim. The lack of ablation studies or interpretability evaluations weakens this aspect of the contribution.\n4. **Hyper-parameter Selection**: The introduction of new decoder building blocks brings additional hyper-parameters, but the paper does not provide guidance on how to select these hyper-parameters or perform model selection.\n5. **Literature Context**: The paper lacks a comprehensive review of related work in areas such as state-space models and deep generative models for sequential prediction tasks. This omission makes it difficult to assess the technical contribution of the work.\n6. **Experimental Methodology**: The choice of evaluation metrics and the experimental setup have been questioned. The use of synthetic datasets and the lack of comparison with probabilistic autoregressive models are notable limitations.\n\n### Conclusion:\nThe paper introduces an interesting idea of incorporating interpretability into VAEs for time-series generation, but it falls short in several critical areas. The lack of novelty, insufficient empirical evaluation, and unsubstantiated claims of interpretability are significant drawbacks. The paper does not meet the high standards required for a top-tier conference, and substantial revisions are needed to address the concerns raised by the reviewers.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" presents a novel approach to synthesizing data using deep generative models with differential privacy guarantees. The paper introduces a one-shot privacy-preserving method that avoids the iterative privacy degradation seen in other approaches like DP-SGD. The authors provide theoretical guarantees and empirical evaluations, demonstrating the method's effectiveness.\n\n**Strengths:**\n\n1. **Novelty and Motivation:** The paper introduces a novel framework that addresses the limitations of existing methods, such as gradient sanitization approaches. The motivation for the work is well-articulated, and the proposed method is innovative in its use of characteristic functions and adversarial re-weighting objectives.\n\n2. **Theoretical and Empirical Support:** The paper provides theoretical guarantees for the proposed method and supports its claims with empirical evaluations on multiple datasets. The results show that PEARL outperforms existing methods at reasonable privacy levels.\n\n3. **Clarity and Writing:** The paper is generally well-written, with clear mathematical formulations and explanations of the proposed approach.\n\n**Weaknesses:**\n\n1. **Benchmarking and Baselines:** The reviewers noted the need for more thorough benchmarking against additional datasets and baselines. The authors addressed this by adding another dataset and baseline during the rebuttal period, but the initial submission lacked these comparisons.\n\n2. **Complexity Analysis and Related Work:** Reviewer 3 pointed out the lack of complexity analysis and missing references to important related work. The authors addressed these concerns in their response, providing a complexity analysis and adding the missing references.\n\n3. **Clarity on Privacy-Preserving Optimization:** There were initial concerns about the clarity of the privacy-preserving optimization method. The authors provided additional explanations and clarified the one-shot sampling strategy in their response.\n\n4. **Empirical Evaluation on Complex Datasets:** The paper does not evaluate on more complex image datasets like CIFAR-10, citing the difficulty of generating such images under differential privacy constraints. While this is a valid point, it limits the empirical scope of the paper.\n\n**Author Responses:**\n\nThe authors have been responsive to the reviewers' comments, addressing most of the concerns raised. They added additional datasets and baselines, clarified the privacy-preserving optimization method, and provided a complexity analysis. However, some limitations remain, such as the lack of evaluation on more complex datasets and the initial lack of comprehensive benchmarking.\n\n**Conclusion:**\n\nThe paper presents a significant contribution to the field of privacy-preserving data synthesis with a novel approach that is theoretically sound and empirically validated. The authors have addressed most of the reviewers' concerns, and the remaining limitations do not outweigh the paper's contributions. Given the novelty, theoretical grounding, and empirical performance of the proposed method, the paper is suitable for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A Frequency Perspective of Adversarial Robustness\" presents an analysis of adversarial examples through the lens of frequency components, challenging the common misconception that adversarial examples are primarily high-frequency noise. The authors claim that adversarial examples are dataset-dependent and propose a frequency-based explanation for the accuracy vs. robustness trade-off.\n\n### Strengths:\n1. **Novel Perspective**: The paper provides a fresh perspective by analyzing adversarial examples in the frequency domain, which is a valuable contribution to the adversarial robustness community.\n2. **Comprehensive Experiments**: The authors conducted extensive experiments on CIFAR-10 and ImageNet-derived datasets, providing empirical support for their claims.\n3. **Insightful Observations**: The paper offers interesting insights into the dataset dependency of adversarial examples and the impact of frequency components on adversarial training.\n\n### Weaknesses:\n1. **Lack of Novelty**: Several reviewers pointed out that the main claims of the paper are not entirely novel and have been discussed in prior works. The dataset dependency of adversarial examples is a known concept, and the paper does not sufficiently differentiate itself from existing literature.\n2. **Insufficient Justification**: The motivation for the study is not well-justified, as the misconception about high-frequency noise has already been questioned in the literature. The paper's contributions are more of additional evidence rather than groundbreaking insights.\n3. **Limited Scope**: The study is limited to PGD attacks and does not explore other types of attacks like C&W or auto-attack, which could provide a more comprehensive understanding.\n4. **Clarity and Presentation**: Some reviewers found the paper confusing, with unclear contributions and a lack of emphasis on the authors' unique ideas. The presentation of figures and grammatical errors also detract from the overall quality.\n\n### Author Responses:\nThe authors provided detailed responses to the reviewers' comments, clarifying their contributions and addressing some of the concerns. They acknowledged the overlap with existing works but emphasized the differences in their approach, particularly focusing on the frequency properties of adversarial noise rather than the images themselves. They also provided additional experimental results and discussed potential improvements in the presentation.\n\n### Conclusion:\nWhile the paper presents an interesting perspective and conducts thorough experiments, the lack of significant novelty and the overlap with existing literature are major concerns. The contributions, although insightful, do not appear to be sufficiently groundbreaking for a top-tier conference. The authors' responses address some issues but do not fully resolve the concerns about novelty and clarity.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Sample and Computation Redistribution for Efficient Face Detection\" presents a method for improving face detection performance, particularly for small faces in low-resolution images, by introducing Computation Redistribution (CR) and Sample Redistribution (SR). The paper claims state-of-the-art performance on the WIDER FACE dataset and provides a detailed ablation study to support its claims.\n\n### Strengths:\n1. **Performance**: The paper demonstrates significant improvements in face detection performance, particularly for small faces, which is a challenging problem in computer vision. The results on the WIDER FACE dataset are impressive, showing a substantial improvement over existing methods like TinaFace.\n   \n2. **Methodology**: The proposed methods, CR and SR, are simple yet effective. The authors provide a detailed explanation of their approach, including a random search strategy in a designed search space, which is a practical contribution to the field.\n\n3. **Empirical Validation**: The paper includes extensive experiments and ablation studies that validate the effectiveness of the proposed methods. The authors also provide comparisons with state-of-the-art methods, showing the superiority of their approach.\n\n4. **Revisions and Responses**: The authors have addressed the reviewers' concerns by adding figures, algorithms, and additional comparisons in the revised version. They have also clarified the advantages of their approach over evolutionary methods and other NAS techniques.\n\n### Weaknesses:\n1. **Novelty**: Some reviewers pointed out that the search strategies implemented are straightforward and not particularly novel. The paper's contributions are seen as incremental rather than groundbreaking.\n\n2. **Comparison with NAS Methods**: While the authors have added comparisons with evolutionary methods and other NAS techniques, some reviewers felt that the comparison was still inadequate. The paper could benefit from a more comprehensive evaluation against a broader range of NAS methods.\n\n3. **Clarity and Detail**: Initial versions of the paper lacked clarity in some areas, such as the overview of the framework and details of the sample distribution method. The authors have addressed these issues in the revised version, but the initial lack of clarity may have affected the reviewers' perception.\n\n4. **Limited Application Scope**: One reviewer noted that the proposed methods might be limited to object detection tasks with a wide range of object scales, which could restrict the broader applicability of the approach.\n\n### Conclusion:\nThe paper presents a well-executed study with significant empirical results that demonstrate improvements in face detection performance. While the novelty of the approach may be questioned, the practical impact and the thoroughness of the empirical validation make it a valuable contribution to the field. The authors have adequately addressed the reviewers' concerns, and the revised version of the paper is more comprehensive and clear.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes\" presents a novel approach to neural network compression using tropical geometry. The authors propose a theoretical framework that connects the approximation error between neural networks with the Hausdorff distance of their tropical zonotopes. They also introduce a compression algorithm based on this theory and evaluate it empirically.\n\n**Strengths:**\n1. **Theoretical Contribution:** The paper provides a novel theoretical perspective on neural network approximation using tropical geometry. This is a significant contribution as it opens up new avenues for research in neural network theory.\n2. **Novelty:** The approach of using tropical geometry for neural network compression is relatively unexplored, making the paper's contributions novel.\n3. **Clarity in Theoretical Explanation:** The background on tropical geometry is well-explained, making the paper accessible to non-experts in the field.\n\n**Weaknesses:**\n1. **Empirical Evaluation:** The empirical evaluation is limited to small-scale datasets and outdated architectures. This raises concerns about the practical applicability and scalability of the proposed methods to modern, large-scale networks.\n2. **Comparison with State-of-the-Art:** The paper lacks a comprehensive comparison with state-of-the-art methods. While it compares with some non-tropical methods, the choice of baselines is not entirely convincing, and the results do not clearly demonstrate superiority or competitiveness.\n3. **Practical Utility of Theoretical Bounds:** The bounds provided in the paper are noted to be loose and not practically useful, as indicated by the reviewers. This diminishes the immediate impact of the theoretical contributions.\n4. **Reproducibility and Clarity:** The lack of detailed experimental setup and pseudo-code makes it difficult to reproduce the results. Although the authors have provided code as supplementary material, the paper itself should be more self-contained.\n\n**Author Responses:**\nThe authors have addressed some of the reviewers' concerns by clarifying the scope of their work and providing additional theoretical insights. They have also acknowledged the limitations of their empirical evaluation and positioned their work as a proof-of-concept rather than a state-of-the-art method.\n\n**Conclusion:**\nWhile the paper presents an interesting theoretical framework and a novel approach to neural network compression, the empirical evaluation is not sufficiently robust to meet the standards of a top-tier conference. The limitations in scalability, lack of comprehensive comparisons with modern methods, and the practical utility of the theoretical bounds are significant drawbacks. The paper's contributions, while novel, do not currently demonstrate enough empirical significance or practical applicability to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Behavior Proximal Policy Optimization\" presents an approach to offline reinforcement learning (RL) by leveraging the inherent conservatism of on-policy algorithms like PPO. The authors propose a method called Behavior Proximal Policy Optimization (BPPO) and claim it outperforms state-of-the-art offline RL algorithms on the D4RL benchmark.\n\n**Strengths:**\n1. **Theoretical Contribution:** The paper provides a theoretical framework for offline monotonic policy improvement, which is a significant contribution to the field of offline RL.\n2. **Empirical Results:** The experimental results on the D4RL benchmark show that BPPO performs competitively, especially on more challenging tasks like Adroit, Antmaze, and Kitchen, where it significantly outperforms existing methods.\n3. **Simplicity:** The proposed method is a simple modification of PPO, making it easy to implement and understand.\n\n**Weaknesses:**\n1. **Theoretical Assumptions:** Assumption 1 was initially considered too strong, but the authors have addressed this by replacing it with a more data-driven approach. However, the replacement of $A_{\\pi_k}$ with $A_{\\pi_\\beta}$ is acknowledged to loosen the theoretical bounds.\n2. **Novelty Concerns:** Some reviewers noted that the theoretical results overlap with existing work like GePPO, and the novelty of the method is questioned. The authors argue that the specific application to offline RL and the advantage replacement are novel contributions.\n3. **Experimental Limitations:** While the results are promising, the improvements on the Gym environments are marginal. The authors have supplemented additional experiments and comparisons, but some reviewers remain unconvinced about the practical significance of the results.\n\n**Reviewer Feedback:**\n- The reviewers have mixed opinions, with some appreciating the theoretical insights and empirical results, while others are concerned about the novelty and practical impact.\n- The authors have been responsive to reviewer concerns, providing additional experiments and clarifications.\n\n**Final Assessment:**\nThe paper presents a theoretically grounded and empirically validated approach to offline RL. While there are concerns about the novelty and theoretical assumptions, the authors have addressed these to a reasonable extent. The empirical results, particularly on challenging tasks, demonstrate the potential of BPPO. Given the theoretical contributions, empirical validation, and the authors' responsiveness to feedback, the paper meets the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" presents a novel algorithm for pruning neurons in deep ReLU networks. The authors claim that their method is theoretically grounded and can significantly reduce the number of neurons without affecting the learned function. However, several critical issues have been identified by the reviewers, which need to be addressed to meet the standards of a top-tier conference.\n\n### Detailed Reasoning:\n\n1. **Clarity and Presentation:**\n   - The paper suffers from significant clarity and presentation issues. Important concepts are explained in footnotes, and many definitions are missing or poorly integrated into the main text. This makes the paper difficult to follow and understand.\n   - The authors acknowledge these issues and agree that the presentation needs improvement. They also recognize that their theoretical contributions were not communicated effectively.\n\n2. **Theoretical Foundation:**\n   - The paper heavily relies on unpublished work, which is not accessible to reviewers or readers. This makes it impossible to verify the correctness and relevance of the theoretical claims.\n   - The authors themselves suggest that it might be better to publish the foundational theoretical work first, which indicates that the current paper is premature.\n\n3. **Experimental Validation:**\n   - The experiments are limited to toy datasets and MNIST, which are insufficient to demonstrate the effectiveness of the proposed method. There is a lack of comparison with state-of-the-art pruning techniques on more complex datasets.\n   - The authors acknowledge the need for more extensive experiments and comparisons, which are crucial for validating the practical utility of their method.\n\n4. **Novelty and Contribution:**\n   - While the authors claim that their method is novel, reviewers have pointed out that some of the pruning steps are not particularly new. The novelty of the third step, which involves clustering neurons, is not well-explained or justified.\n   - The authors argue that their method is theoretically driven, but without access to the unpublished work, this claim cannot be substantiated.\n\n5. **Reviewer Consensus:**\n   - All reviewers have expressed concerns about the paper's readiness for publication. The issues range from clarity and presentation to insufficient experimental validation and reliance on unpublished work.\n\n6. **Author Response:**\n   - The authors have acknowledged the feedback and agree with many of the criticisms. They express a willingness to improve the paper but also indicate that significant changes are needed, including potentially publishing the theoretical work separately.\n\nGiven these considerations, the paper is not ready for publication at a top-tier conference. The issues with clarity, reliance on unpublished work, and insufficient experimental validation are significant barriers that need to be addressed before the paper can be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs\" presents an interesting approach to improving recurrent neural networks (RNNs) by incorporating insights from neuroscience, specifically the thalamocortical architecture, to enhance the learning and chaining of motor motifs. The authors aim to address the challenges of continual learning and out-of-distribution generalization in RNNs, proposing a biologically inspired architecture that purportedly improves robustness and accuracy.\n\n**Strengths:**\n1. **Biological Inspiration:** The paper attempts to bridge neuroscience and machine learning by using insights from the thalamocortical circuit to improve RNN performance. This interdisciplinary approach is innovative and could potentially lead to new insights in both fields.\n2. **Continual Learning:** The focus on continual learning is relevant and timely, as it addresses the problem of catastrophic forgetting, which is a significant challenge in machine learning.\n3. **Zero-shot Learning:** The proposed architecture reportedly allows for zero-shot transfer to new motif chains, which is a valuable capability in dynamic environments.\n\n**Weaknesses:**\n1. **Lack of Convincing Evidence:** Reviewers have pointed out that the paper does not convincingly demonstrate the necessity of the proposed inductive bias. Alternative hypotheses and simpler solutions, such as resetting mechanisms, are not thoroughly tested or ruled out.\n2. **Scale and Relevance:** The experiments are conducted on a small scale (300 units, 10 motifs), which raises concerns about the generalizability and relevance of the findings to larger, more complex systems.\n3. **Writing and Clarity:** Several reviewers noted issues with the clarity and organization of the paper. Important details are scattered, and the presentation is confusing, which hinders the communication of the scientific ideas.\n4. **Technical Novelty:** The technical contributions are considered marginally significant by multiple reviewers. The novelty and significance of the empirical results are also questioned.\n5. **Trivial Solutions:** There are concerns about the existence of trivial solutions to the problem posed, which the authors have not adequately addressed.\n\n**Author Response:**\nThe authors have made efforts to address the reviewers' concerns by clarifying their approach and providing additional evidence. However, the responses do not fully resolve the fundamental issues raised, particularly regarding the necessity of the proposed architecture and the scale of the experiments. The authors' explanations, while detailed, do not convincingly demonstrate that their approach is superior to simpler alternatives or that it scales to more complex tasks.\n\n**Conclusion:**\nWhile the paper presents an interesting interdisciplinary approach, it falls short of the standards expected at a top-tier conference. The lack of convincing evidence for the necessity and superiority of the proposed architecture, combined with the small scale of the experiments and issues with clarity and presentation, suggest that the paper is not ready for acceptance. The authors are encouraged to address these issues, particularly by testing alternative hypotheses and scaling up their experiments, before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Meta-Referential Games to Learn Compositional Learning Behaviours\" presents a novel benchmark for evaluating compositional learning behaviors in artificial agents through meta-referential games. The authors propose a Symbolic Continuous Stimulus (SCS) representation and compare it with traditional one-hot encodings (OHE) using state-of-the-art reinforcement learning (RL) agents.\n\n**Strengths:**\n1. **Novelty of the Problem:** The paper addresses an interesting and relevant problem in AI, focusing on compositional learning behaviors, which are crucial for developing intelligent agents capable of generalization.\n2. **Benchmark Proposal:** Introducing a new benchmark for compositional learning is a valuable contribution that could spur further research in this area.\n\n**Weaknesses:**\n1. **Clarity and Readability:** All reviewers noted that the paper is difficult to follow, with missing details and complex explanations. This significantly hampers the ability to understand and evaluate the contributions.\n2. **Insufficient Experimental Validation:** The experiments lack depth, with insufficient comparisons and ablation studies. The results are not compelling, as they show only marginal improvements over baselines, and the performance is barely above chance.\n3. **Limited Novelty and Significance:** The technical and empirical contributions are considered marginal by all reviewers. The SCS representation's utility outside the paper's specific context is unclear.\n4. **Lack of Engagement with Existing Literature:** The paper does not adequately situate its contributions within the broader literature on compositionality and representation learning, missing opportunities to connect with established theories and findings.\n5. **Reviewer Concerns:** Reviewers raised specific concerns about the applicability of the SCS representation, the lack of continuous representation comparisons, and the need for more detailed experimental setups and results.\n\n**Author Response:**\nThe authors acknowledged the feedback and expressed intent to improve the paper's clarity and buildability. However, they did not provide concrete plans or evidence of addressing the fundamental issues raised by the reviewers.\n\n**Conclusion:**\nWhile the paper tackles an interesting problem, the current submission falls short of the standards required for a top-tier conference. The lack of clarity, insufficient experimental validation, and limited novelty and significance are critical issues that need to be addressed. The authors need to provide a more comprehensive evaluation, clearer explanations, and stronger connections to existing work to make a compelling case for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Uncertainty for deep image classifiers on out of distribution data\" addresses an important problem in machine learning: the calibration of model predictions on out-of-distribution (OOD) data. The authors propose a post-hoc calibration method that uses outlier exposure to improve model calibration on corrupted data. The paper has received mixed reviews from the four reviewers, with some appreciating the simplicity and effectiveness of the method, while others express concerns about the assumptions and the novelty of the approach.\n\n**Strengths:**\n1. **Important Problem:** The paper tackles the significant issue of model overconfidence on OOD data, which is crucial for deploying models in real-world applications.\n2. **Simplicity and Efficiency:** The proposed method is simple, intuitive, and computationally efficient, as it does not require retraining the model.\n3. **Empirical Results:** The method shows consistent improvements in calibration across different types of corruptions, as evidenced by the experiments on CIFAR-10 and ImageNet datasets.\n4. **Post-hoc Applicability:** The method can be applied post-hoc to any model, making it versatile and easy to integrate into existing systems.\n\n**Weaknesses:**\n1. **Assumptions on Corruptions:** Reviewer 1 points out that the method assumes the novel distribution is from a small set of known possibilities, which simplifies the problem and may not align with the assumptions in existing work.\n2. **Limited Evaluation:** The paper primarily evaluates the method on artificially corrupted datasets. While the authors have added some experiments on completely OOD datasets like SVHN, the evaluation could be more comprehensive.\n3. **Clarity and Presentation:** Several reviewers noted issues with the clarity of the paper, including undefined terms and unclear explanations of the methods.\n4. **Novelty and Related Work:** The paper builds on prior work, and some reviewers feel that it lacks sufficient novelty. Additionally, there are missing citations to relevant recent work in the field.\n\n**Author Responses:**\nThe authors have made efforts to address the reviewers' concerns by adding new experiments, clarifying the methodology, and providing additional explanations in the appendix. They emphasize that their method does not rely on extra information and that it is evaluated on unseen corruptions at test time. They also clarify the distinction between calibration under distribution shift and unsupervised domain adaptation.\n\n**Conclusion:**\nWhile the paper addresses an important problem and presents a simple and effective method, the concerns about the assumptions, limited evaluation, and clarity cannot be overlooked. The authors have made commendable efforts to address these issues, but the fundamental concern about the assumptions and the novelty of the approach remains. For a top-tier conference, the paper needs to demonstrate a more comprehensive evaluation and clearer differentiation from existing work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" presents an interesting approach to learning layout representations using a Transformer-based model, leveraging a large-scale dataset of over one million slides. The authors propose a novel pre-training method for layout representation and demonstrate its effectiveness on two downstream tasks: element role labeling and image captioning.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a Transformer-based approach to layout representation, which is a relatively unexplored area. The creation of a large-scale dataset of slide layouts is a significant contribution to the field.\n2. **Potential Impact**: The approach has potential applications in various graphic design tasks, such as layout auto-completion and retrieval, which are relevant and impactful.\n3. **Writing and Clarity**: The paper is generally well-written and easy to follow, with a clear explanation of the proposed method.\n\n### Weaknesses:\n1. **Evaluation and Baselines**: The evaluation is a major weakness. The paper lacks comparisons with strong baselines, such as other state-of-the-art methods in layout representation and related tasks. The reviewers pointed out the absence of comparisons with relevant works like Cao et al., SIGGRAPH 2019, and others.\n2. **Motivation for Transformer Use**: The motivation for using a Transformer-based model over other architectures, such as Graph Neural Networks, is not well-justified. The reviewers suggest that GNNs could be more suitable for capturing the structural properties of layouts.\n3. **Dataset Details**: Important details about the dataset, such as the number of elements per slide and the parsing method, are missing. This information is crucial for understanding the dataset's richness and the model's applicability.\n4. **Task Complexity**: The tasks used for evaluation are relatively simple, and the paper does not explore more complex tasks that could better demonstrate the model's capabilities.\n5. **Lack of Visualization**: The paper lacks qualitative results and visualizations, which are important for understanding the model's performance in layout-related tasks.\n\n### Author Response:\nThe authors acknowledge the weaknesses pointed out by the reviewers and propose to address them in future versions. They plan to include more detailed evaluations, comparisons with additional baselines, and more systematic evaluation methods. However, these improvements are not present in the current submission.\n\n### Conclusion:\nWhile the paper presents a novel approach and makes a significant contribution with the dataset, the weaknesses in evaluation, lack of strong baselines, and insufficient justification for the chosen model architecture are critical issues. These shortcomings prevent the paper from meeting the standards of a top-tier conference at this stage. The authors' willingness to address these issues in future work is promising, but the current submission does not provide enough evidence of the model's superiority or applicability.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Batch Normalization and Bounded Activation Functions\" explores an interesting and potentially impactful topic: the interaction between batch normalization (BN) and bounded activation functions, specifically when the order of these components is swapped. The authors claim that placing BN after bounded activation functions like Tanh can lead to better performance due to asymmetric saturation and increased sparsity.\n\n**Strengths:**\n1. **Novelty and Interest:** The paper presents a novel observation that swapping the order of BN and bounded activation functions can lead to improved performance. This is an interesting finding that could have implications for neural network design.\n2. **Empirical Analysis:** The authors conduct extensive experiments to support their claims, providing quantitative metrics for saturation, skewness, and sparsity.\n3. **Clarity and Reproducibility:** The paper is generally well-written, and the authors have provided code to facilitate reproducibility.\n\n**Weaknesses:**\n1. **Lack of Theoretical Justification:** The paper lacks a strong theoretical foundation to explain why the observed phenomena occur. The arguments connecting asymmetric saturation and improved performance are not entirely convincing.\n2. **Limited Practical Impact:** While the findings are interesting, the practical impact is limited because the Swap model with bounded activations does not outperform the conventional model with ReLU, which is widely used in practice.\n3. **Incomplete Analysis:** The analysis is somewhat limited, as it does not fully explore the implications of the findings across different architectures and datasets. The authors acknowledge that the Swap model is not tested on architectures with residual connections, which are prevalent in modern deep learning models.\n4. **Reviewer Concerns:** Multiple reviewers have raised concerns about the clarity and rigor of the claims, particularly regarding the relationship between saturation, sparsity, and generalization performance. The authors' responses address some of these concerns but do not fully resolve them.\n\n**Author Responses:**\nThe authors have provided detailed responses to the reviewers' comments, clarifying some points and adding additional experimental results. However, the responses do not fully address the fundamental concerns about the theoretical justification and practical significance of the findings.\n\n**Conclusion:**\nWhile the paper presents an interesting observation, the lack of a strong theoretical explanation and limited practical impact make it difficult to justify acceptance at a top-tier conference. The paper would benefit from a more comprehensive analysis and stronger theoretical grounding to support its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper titled \"Multi-scale Feature Learning Dynamics: Insights for Double Descent\" presents a theoretical analysis of epoch-wise double descent in neural networks using a linear teacher-student model. The authors derive closed-form expressions for the generalization error and validate their findings with numerical experiments. The paper addresses an important phenomenon in deep learning, providing insights into the dynamics of feature learning and generalization error.\n\n**Strengths:**\n\n1. **Theoretical Contribution:** The paper provides a novel theoretical framework for understanding epoch-wise double descent using a linear model. This is significant as it extends the understanding of double descent beyond model complexity to training dynamics.\n\n2. **Analytical Rigor:** The use of replica theory to derive closed-form expressions for the generalization error is a strong point. The authors have made efforts to connect their theoretical findings with empirical observations in deep neural networks.\n\n3. **Empirical Validation:** The paper includes numerical experiments that demonstrate the applicability of the theoretical insights to real-world scenarios, such as training ResNet18 on CIFAR10.\n\n4. **Revisions and Clarifications:** The authors have addressed many of the reviewers' concerns in their response, providing additional experiments, clarifications, and connections to existing literature. They have also corrected typos and improved the presentation of their results.\n\n**Weaknesses:**\n\n1. **Novelty and Significance:** Some reviewers questioned the novelty of the contributions, suggesting that similar insights might have been explored in prior works. The authors have attempted to clarify these points, but the novelty remains a concern for some reviewers.\n\n2. **Technical Issues:** Reviewer 3 pointed out numerous typos and errors in the initial submission, which could have affected the clarity and perceived correctness of the paper. Although the authors have addressed these issues, it raises concerns about the initial quality of the submission.\n\n3. **Connections to Prior Work:** While the authors have made efforts to connect their work to existing literature, some reviewers felt that these connections could be stronger. The authors have added discussions to address this, but it remains a point of contention.\n\n4. **Empirical Significance:** The empirical contributions were considered marginally significant by some reviewers, as the primary focus of the paper is theoretical.\n\n**Conclusion:**\n\nThe paper addresses an important and timely topic in deep learning, providing valuable theoretical insights into the phenomenon of epoch-wise double descent. The authors have made significant efforts to address the reviewers' concerns, improving the clarity and depth of their analysis. While there are some concerns about novelty and initial presentation, the overall contribution of the paper is substantial, particularly in its theoretical advancements and empirical validation.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a new contrastive learning objective aimed at reducing text degeneration in language models, specifically targeting issues like repetition. The reviewers generally agree that the paper is well-written and the idea is intuitive. However, there are several critical concerns raised across the reviews that need to be addressed when considering the paper for acceptance at a top-tier conference.\n\n### Strengths:\n1. **Clarity and Simplicity**: The paper is noted for its clarity and the simplicity of the proposed method, which is easy to implement and integrate into existing systems.\n2. **Motivation**: The motivation for the work is sound, addressing a known issue in language model training.\n3. **Reproducibility**: The authors have provided code and resources to facilitate reproducibility.\n\n### Weaknesses:\n1. **Novelty**: The primary concern across all reviews is the lack of significant novelty. The proposed method is seen as an incremental improvement over existing techniques like unlikelihood training (UL). The reviewers argue that the paper does not sufficiently differentiate itself from UL, and some claims about UL are contested.\n   \n2. **Experimental Setup**: The experiments are considered insufficient and not convincing. The paper primarily uses GPT2-small and does not explore larger models or more diverse tasks, which limits the generalizability of the results. The choice of metrics and the interpretation of results are also questioned, with suggestions for more comprehensive evaluations using established metrics in related tasks.\n\n3. **Significance and Impact**: The reviewers express doubts about the significance and impact of the proposed method, especially given the lack of statistical significance in human evaluations and the limited scope of experiments. The utility of the method for larger, more advanced models is also questioned.\n\n4. **Lack of Author Response**: The absence of an author response means that the authors did not address the reviewers' concerns, which could have provided additional insights or clarifications.\n\n### Conclusion:\nThe paper presents an interesting idea with potential applications, but it falls short in terms of novelty, experimental rigor, and demonstrated impact. The concerns about the experimental setup and the lack of significant differentiation from existing methods are particularly critical for a top-tier conference. Without additional experiments or a stronger demonstration of the method's utility, the paper does not meet the high standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" presents an approach to out-of-distribution (OOD) detection using graph structures derived from image features. The paper claims to achieve competitive results using this novel method. However, several critical issues have been identified by the reviewers, which need to be addressed to meet the standards of a top-tier conference.\n\n### Strengths:\n1. **Novelty in Approach**: The idea of using graph structures to represent image features for OOD detection is interesting and potentially novel. It suggests a new direction for interpretable OOD detection.\n2. **Comprehensive Graph Embedding Comparisons**: The paper includes a comparison of different graph embedding algorithms, which could be valuable for future research.\n\n### Weaknesses:\n1. **Clarity and Writing Quality**: All reviewers noted significant issues with the clarity and quality of writing. The paper contains grammatical errors, poorly defined terms, and lacks detailed explanations of key concepts and methods. This makes it difficult for readers to fully understand and evaluate the work.\n   \n2. **Lack of Rigorous Experimental Validation**: The empirical results are not convincing due to the lack of comparison with state-of-the-art methods. The paper only reports results on the LSUN dataset without broader validation on other datasets like CIFAR-10/100 or ImageNet. This limits the generalizability and impact of the findings.\n\n3. **Insufficient Detail for Reproducibility**: The paper does not provide enough detail on the methodology, particularly the feature extraction and graph creation processes, making it difficult for others to reproduce the results.\n\n4. **Assumptions and Limitations**: The reliance on a pre-trained object detection network is a significant limitation, especially for domains where such networks may not be applicable (e.g., medical imaging). The paper does not adequately address how inaccuracies in object detection could impact the OOD detection performance.\n\n5. **Lack of Author Response**: The absence of an author response means that the authors did not address the reviewers' concerns, leaving critical issues unresolved.\n\n### Conclusion:\nGiven the significant issues related to clarity, experimental validation, and reproducibility, the paper does not meet the standards required for acceptance at a top-tier conference. The novel idea is overshadowed by the lack of rigorous empirical support and poor presentation. The authors need to address these issues comprehensively, including improving the writing, providing detailed methodological explanations, and conducting more extensive experiments with comparisons to state-of-the-art methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" presents a novel approach by integrating Transformers with Monte-Carlo Tree Search (MCTS) in the context of offline reinforcement learning. The idea is to leverage the strengths of both methods to improve exploration and evaluation in reinforcement learning tasks.\n\n**Strengths:**\n1. **Novelty in Approach:** The combination of Transformers with MCTS in an actor-critic setup is a novel idea. This integration aims to address the limitations of Decision Transformers in adapting and exploring during test time.\n2. **Potential for Future Work:** The approach could inspire further research and development in combining sequence modeling with traditional search methods in reinforcement learning.\n\n**Weaknesses:**\n1. **Clarity and Presentation:** All reviewers noted significant issues with the clarity and presentation of the paper. The exposition is unclear, and the figures and explanations are not self-explanatory, which hinders understanding.\n2. **Limited and Non-Standard Evaluation:** The experiments are conducted solely on the SameGame domain, which is not a standard benchmark for evaluating reinforcement learning algorithms. This choice limits the generalizability and impact of the results.\n3. **Lack of Comprehensive Comparisons:** The paper does not compare its method against a wide range of relevant baselines, such as AlphaZero or other MCTS-based methods with neural networks. This omission makes it difficult to assess the true performance and novelty of the proposed approach.\n4. **Questionable Novelty:** Reviewers pointed out that the method is similar to existing frameworks like AlphaGo/AlphaZero, with the primary difference being the use of Transformers instead of ResNet. This raises concerns about the originality and significance of the contribution.\n5. **Empirical Support:** The empirical results are not convincing due to the limited scope of the experiments and the lack of comparisons with established methods in the field.\n\n**Overall Assessment:**\nWhile the idea of combining Transformers with MCTS is intriguing and potentially impactful, the paper falls short in several critical areas. The lack of clarity, limited experimental validation, and insufficient comparisons with existing methods significantly undermine the paper's contributions. The novelty is also questioned due to similarities with existing frameworks. For a top-tier conference, the paper needs substantial improvements in clarity, experimental rigor, and demonstration of novelty and significance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper titled \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" presents a convergence analysis of gradient descent (GD) for deep linear neural networks, focusing on different initialization schemes and general strongly convex and smooth objectives. The authors claim to provide novel insights into the convergence behavior of GD, particularly in overparameterized settings.\n\n### Strengths:\n1. **Clarity and Presentation**: The paper is well-written and the results are clearly presented. The authors have made efforts to address reviewer concerns by adding a new section discussing insights and combining theorems for clarity.\n   \n2. **Novelty in Theorem 3.2**: The authors claim that Theorem 3.2, which shows that the trajectory of overparameterized deep linear networks is close to the original convex problem with an appropriately rescaled learning rate, is novel. This result could provide valuable insights into the behavior of GD in non-convex settings.\n\n3. **Technical Rigor**: The paper appears to be technically sound, with rigorous proofs supporting the claims. The authors have addressed concerns about the novelty of their techniques by highlighting the challenges in extending existing results to more general settings.\n\n4. **Revisions and Responses**: The authors have been responsive to reviewer feedback, making significant revisions to address concerns about overclaims, novelty, and the need for additional insights.\n\n### Weaknesses:\n1. **Limited Novelty in Some Results**: Several reviewers noted that some of the main results, particularly Theorems B.1 and B.2, are extensions of existing work. The novelty of these results is questioned, as they seem to rely heavily on techniques from prior work.\n\n2. **Local Convergence Analysis**: Reviewer 4 pointed out that the convergence analysis is local, which may limit the applicability of the results. The authors have attempted to address this concern, but it remains a point of contention.\n\n3. **Lack of Intuitive Insights**: Some reviewers felt that the paper lacks intuitive understanding and discussion of the results, particularly regarding the convergence region and the role of overparameterization. The authors have added a new section to address this, but it may not fully satisfy all concerns.\n\n4. **Overclaims**: Initially, the paper made some overclaims regarding the generality of the results. The authors have revised the title and abstract to be more specific, but this issue was a significant concern for some reviewers.\n\n5. **Concerns About Novelty**: Reviewer 5 expressed major concerns about the novelty of the paper, suggesting that the main contributions are marginally significant or novel.\n\n### Conclusion:\nThe paper presents a solid theoretical contribution to the understanding of GD in deep linear neural networks, particularly in overparameterized settings. While there are concerns about the novelty of some results, the authors have made substantial efforts to address these issues and provide additional insights. The novel aspects, particularly Theorem 3.2, and the rigorous analysis make a compelling case for acceptance. However, the concerns about limited novelty and the local nature of the analysis cannot be entirely dismissed.\n\nGiven the mixed reviews and the efforts made by the authors to address the concerns, the paper could be considered for acceptance, especially if the novel insights are deemed valuable by the community. However, the decision is borderline, and the paper may benefit from further revisions to strengthen its contributions and address remaining concerns.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Variational Neural Cellular Automata\" presents a novel approach to generative modeling by integrating Neural Cellular Automata (NCA) into a variational autoencoder framework. The authors propose the Variational Neural Cellular Automata (VNCA) model, which is inspired by biological processes of cellular growth and differentiation. The paper is evaluated based on its novelty, technical soundness, empirical results, and overall contribution to the field.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a novel approach by combining NCAs with variational autoencoders, which is a relatively unexplored area. The idea of using NCAs for generative modeling is intriguing and could open new research directions.\n2. **Clarity and Writing**: The paper is well-written and clear, making it accessible to readers who may not be familiar with NCAs. The authors have made efforts to address reviewer comments and improve the paper's clarity and content.\n3. **Robustness and Self-Organization**: The VNCA demonstrates robustness to perturbations, which is an interesting property that could have practical applications.\n4. **Rebuttal and Improvements**: The authors have addressed many of the reviewers' concerns, added new experiments, and clarified the distinctions between their approach and existing methods.\n\n### Weaknesses:\n1. **Experimental Results**: The empirical results are underwhelming compared to state-of-the-art methods, particularly on datasets like CelebA. The VNCA performs well on MNIST but struggles with more complex datasets.\n2. **Technical Novelty**: Some reviewers pointed out that the technical novelty is limited, as the VNCA can be seen as a deep convolutional network with weight sharing. The main novelty lies in the NCA interpretation rather than the architecture itself.\n3. **Motivation and Practical Impact**: The motivation for using NCAs in this context is not fully convincing, and the practical impact of the work is limited by the performance gap with state-of-the-art models.\n4. **Comparisons and Baselines**: The paper could benefit from more comprehensive comparisons with existing methods, particularly in terms of quantitative metrics.\n\n### Reviewer Opinions:\n- The reviewers are divided, with some leaning towards acceptance due to the novelty and potential of the approach, while others are concerned about the empirical performance and limited novelty.\n- The authors have made significant efforts to address reviewer concerns, which has led to some reviewers increasing their scores.\n\n### Conclusion:\nWhile the paper presents an interesting and novel approach to generative modeling, the empirical results and technical novelty are not strong enough to meet the standards of a top-tier conference. The performance gap with state-of-the-art models and the limited practical impact are significant drawbacks. However, the paper could be a valuable contribution to a more specialized venue or workshop focused on cellular automata or novel generative models.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Regression with Label Differential Privacy\" presents a novel approach to training regression models with label differential privacy (DP). The authors propose a mechanism called \"randomized response on bins\" and demonstrate its optimality under a given regression loss function. The paper is well-received by the reviewers, with several strengths and some weaknesses noted.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper introduces a new mechanism for label differential privacy in regression tasks, which is a significant extension of existing work focused on classification. The proposed method is shown to be optimal with respect to the expected loss between the original and flipped labels.\n2. **Clarity and Writing:** Multiple reviewers commend the paper for its clear writing and organization, making it accessible and easy to follow.\n3. **Empirical Evaluation:** The authors conduct thorough experiments on real-world datasets, demonstrating the efficacy of their proposed method compared to existing DP mechanisms.\n4. **Theoretical Rigor:** The paper provides theoretical proofs of the optimality of the proposed mechanism, which are well-supported and correct according to the reviewers.\n\n**Weaknesses:**\n1. **Comparison with Existing Methods:** Some reviewers note the lack of direct comparison with other label-DP methods, such as those by Ghazi et al. (2021). However, the authors address this in their response by adapting the algorithm from Ghazi et al. to their framework and providing empirical results.\n2. **Technical Novelty:** While the paper is novel in its application to regression, some reviewers feel that aspects of the contributions exist in prior work, particularly in the context of classification.\n3. **Consideration of Bayes Optimal Error:** Reviewers suggest that the paper could explore the impact of the proposed mechanism on the Bayes optimal error, which the authors acknowledge as a potential area for future work.\n\n**Author Responses:**\nThe authors have addressed the reviewers' concerns effectively. They have included additional experiments and discussions in the appendix to clarify the differences between their work and existing methods. They also acknowledge the limitations and potential future directions, such as considering the Bayes optimal error.\n\n**Conclusion:**\nThe paper presents a significant contribution to the field of differential privacy, particularly in extending label DP to regression tasks. The proposed method is theoretically sound, empirically validated, and addresses a relevant problem in privacy-preserving machine learning. The authors have adequately addressed the reviewers' concerns, and the paper meets the standards of a top-tier conference in terms of novelty, clarity, and impact.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a novel approach to understanding the optimization landscape of two-layer ReLU neural networks through convex optimization. The paper claims to provide a complete characterization of the global minima of the nonconvex problem by leveraging a convex optimization framework, which is a significant theoretical contribution.\n\n**Strengths:**\n\n1. **Novelty and Theoretical Contribution:** The paper extends the work of Pilanci and Ergen (2020) by providing a complete characterization of all global optima using a convex optimization framework without relying on duality. This is a significant theoretical advancement in understanding the optimization landscape of neural networks.\n\n2. **Technical Rigor:** The paper is technically sound, with most claims well-supported by theoretical analysis. The reviewers generally agree on the correctness and significance of the contributions.\n\n3. **Potential Impact:** The insights provided by this paper could inspire further research into extending these results to deeper networks and other activation functions, potentially impacting the broader field of neural network optimization.\n\n4. **Reviewer Consensus:** The reviewers generally agree on the paper's strengths, with two reviewers explicitly recommending acceptance and highlighting the paper's theoretical contributions.\n\n**Weaknesses:**\n\n1. **Scope Limitation:** The paper's results are limited to two-layer ReLU networks with L2 regularization. While this is a significant step, the applicability to deeper networks or other activation functions remains an open question.\n\n2. **Minor Errors and Typos:** Reviewers noted several typos and minor errors in the paper, which the authors have acknowledged and promised to correct in the final version.\n\n3. **Empirical Validation:** The paper lacks empirical validation of the theoretical results, which could strengthen the practical relevance of the findings.\n\n4. **Related Work and References:** Some relevant references were initially missing, but the authors have agreed to include them in the revised version.\n\n**Author Response:**\n\nThe authors have addressed most of the reviewers' concerns, including clarifying technical points, correcting typos, and agreeing to include additional references. They have also provided insights into potential extensions of their work to deeper networks and other activation functions, although these remain speculative at this stage.\n\n**Conclusion:**\n\nGiven the paper's significant theoretical contributions, the consensus among reviewers, and the authors' responsiveness to feedback, the paper is a strong candidate for acceptance. The limitations regarding scope and empirical validation are acknowledged, but they do not overshadow the paper's contributions to the theoretical understanding of neural network optimization.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" presents a method for predicting protein-protein interactions (PPIs) by leveraging pretrained structure embeddings from OmegaFold and using them as inputs to a graph neural network (GNN). The reviewers have provided a comprehensive analysis of the paper, highlighting both strengths and weaknesses.\n\n### Strengths:\n1. **Motivation**: The paper addresses a well-motivated problem in the field of bioinformatics, where structural information is crucial for accurate PPI prediction.\n2. **Transferability**: The authors demonstrate the transferability of their model across different species, which is a valuable contribution to the field.\n3. **Reproducibility**: The paper provides detailed hyper-parameter settings, which aids in reproducibility.\n\n### Weaknesses:\n1. **Limited Novelty**: The primary criticism across all reviews is the limited novelty of the approach. The method essentially combines existing techniques (OmegaFold embeddings and GNNs) without significant innovation or modification.\n2. **Empirical Evaluation**: The empirical results show only marginal improvements over existing methods, and the choice of baselines is questioned. The reviewers note the absence of comparisons with state-of-the-art methods like AlphaFold-Multimer.\n3. **Clarity and Presentation**: The paper suffers from clarity issues, with unclear explanations of key concepts such as AUC in the PPI context and the setup of the binary classification problem. The presentation of results could be improved with better formatting and inclusion of confidence intervals.\n4. **Model Design**: The necessity of using a GNN for the task is not convincingly justified, and the advantage of using structural embeddings is not well-supported by the results.\n5. **Lack of Author Response**: The absence of an author response means that the authors did not address the reviewers' concerns, leaving critical questions unanswered.\n\n### Conclusion:\nThe paper tackles an important problem and proposes a method that is straightforward and potentially useful. However, the lack of novelty, insufficient empirical validation, and unclear presentation significantly undermine its suitability for a top-tier conference. The reviewers consistently point out that the contributions are marginal and do not advance the state of the art in a meaningful way. Given these considerations, the paper does not meet the high standards required for acceptance at a prestigious conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper titled \"Data Leakage in Tabular Federated Learning\" addresses an important and emerging issue in federated learning (FL) concerning data leakage, specifically focusing on tabular data. The authors propose a novel attack method, TabLeak, which aims to reconstruct tabular data from gradients, a problem that presents unique challenges due to the mixed discrete-continuous nature of tabular data.\n\n### Strengths:\n1. **Relevance and Importance**: The topic is highly relevant as federated learning is increasingly being used in privacy-sensitive applications involving tabular data, such as finance and healthcare.\n2. **Novelty**: The paper identifies unique challenges in reconstructing tabular data, which is a novel contribution. The proposed method, TabLeak, introduces a softmax structural prior and a pooled ensembling scheme, which are innovative in this context.\n3. **Empirical Evaluation**: The authors provide a comprehensive experimental evaluation across multiple datasets, demonstrating the effectiveness of their method. They show significant improvements over baseline methods.\n4. **Reproducibility**: The authors have made their source code available, which enhances the reproducibility of their results.\n\n### Weaknesses:\n1. **Technical Novelty**: Some reviewers pointed out that the techniques used, such as softmax relaxation and ensembling, are standard in other contexts. However, the authors argue that their application to tabular data in FL is novel.\n2. **Writing and Clarity**: Reviewer 1 noted issues with the writing quality, which could hinder understanding. The authors have addressed these concerns in their response, indicating improvements in the revised version.\n3. **Baseline Comparisons**: Reviewer 2 criticized the choice of baselines as weak. The authors defended their choices, explaining the rationale behind using these baselines and providing additional experimental results to support their claims.\n\n### Author Responses:\nThe authors provided detailed responses to all reviewer comments, addressing concerns about the novelty, baseline comparisons, and writing clarity. They conducted additional experiments to support their claims about the variance introduced by mixed-type data and the effectiveness of their pooling strategy. They also clarified the limitations of alternative methods like the Gumbel-Softmax in their context.\n\n### Conclusion:\nThe paper presents a novel and significant contribution to the field of federated learning by addressing the specific challenges of data leakage in tabular data. While some techniques used are standard, their application to this problem is innovative. The authors have provided a thorough empirical evaluation and addressed reviewer concerns effectively. The topic's relevance, combined with the paper's contributions, makes it a suitable candidate for a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Protein Sequence and Structure Co-Design with Equivariant Translation\" presents a novel approach to protein design, focusing on the simultaneous generation of protein sequences and structures. The proposed method, ProtSeed, introduces a trigonometry-aware encoder and a roto-translation equivariant decoder, which together aim to improve the efficiency and accuracy of protein design compared to existing methods.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a novel framework for protein sequence and structure co-design, which is a significant contribution to the field of bioengineering. The use of a trigonometry-aware encoder and a roto-translation equivariant decoder is innovative and addresses the limitations of existing autoregressive and diffusion models.\n   \n2. **Empirical Results**: The experimental results demonstrate that ProtSeed outperforms state-of-the-art methods in terms of both accuracy and computational efficiency. The paper provides strong empirical evidence supporting the effectiveness of the proposed approach.\n\n3. **Clarity and Presentation**: The paper is generally well-written, with a clear presentation of the methodology and results. The authors have made efforts to address reviewer concerns and improve the clarity of their claims and descriptions.\n\n4. **Rebuttal and Responsiveness**: The authors have been responsive to reviewer feedback, addressing concerns about clarity, novelty, and empirical validation. They have provided additional ablation studies and clarified the methodology in the appendix.\n\n### Weaknesses:\n1. **Technical Novelty**: Some reviewers have pointed out that aspects of the contributions are not entirely novel, as they build upon existing methods like AlphaFold2. However, the authors argue that their approach is distinct in its ability to cross-condition on sequence and structure, which is not possible with AlphaFold2.\n\n2. **Reproducibility**: There are concerns about the lack of hyperparameter and training details, as well as the absence of code and model release, which could hinder reproducibility. The authors have promised to release these upon acceptance.\n\n3. **Evaluation Metrics**: Some reviewers have questioned the evaluation metrics used, suggesting that they may not fully capture the functional performance of the designed proteins. The authors have acknowledged this and provided additional discussions in the appendix.\n\n4. **Claims of Novelty**: There were initial concerns about over-claiming novelty, particularly regarding the \"first protein sequence-structure co-design\" claim. The authors have agreed to revise these claims to avoid misleading statements.\n\n### Conclusion:\nThe paper presents a significant advancement in the field of protein design, with a novel approach that addresses key limitations of existing methods. While there are some concerns about the novelty and reproducibility, the authors have been responsive and have provided additional clarifications and studies to support their claims. The strong empirical results and the potential impact of the proposed method on protein engineering make this paper a valuable contribution to the conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Batch Normalization and Bounded Activation Functions\" presents an investigation into the effects of swapping the order of batch normalization (BN) and bounded activation functions, such as Tanh, in neural networks. The authors claim that placing BN after the activation function (Swap model) results in better performance compared to the conventional order (BN before activation) when using bounded activation functions. They attribute this improvement to asymmetric saturation and increased sparsity.\n\n### Strengths:\n1. **Novel Observation**: The paper presents an interesting observation that the order of BN and activation functions can significantly impact performance when using bounded activations.\n2. **Empirical Analysis**: The authors conduct extensive experiments to support their claims, providing quantitative metrics for saturation, skewness, and sparsity.\n3. **Potential Impact**: Understanding the interaction between BN and activation functions is crucial for designing more efficient neural network architectures.\n\n### Weaknesses:\n1. **Lack of Theoretical Justification**: The paper lacks a strong theoretical foundation to explain why the Swap model performs better. The claims about asymmetric saturation and sparsity enhancing generalization are not convincingly supported.\n2. **Limited Practical Significance**: The Swap model with bounded activations does not outperform the conventional model with ReLU, which limits the practical impact of the findings.\n3. **Inconsistent Observations**: The saturation and skewness metrics do not consistently support the claims across all layers of the networks tested, raising questions about the generality of the findings.\n4. **Clarity and Rigor**: Several claims in the paper are not clearly articulated or rigorously justified, leading to confusion and skepticism among reviewers.\n5. **Scope of Analysis**: The analysis is limited to specific architectures and does not consider widely used architectures with residual connections, which reduces the general applicability of the findings.\n\n### Author Responses:\nThe authors provided responses to the reviewers' concerns, clarifying some points and adding additional experimental results. However, the responses did not fully address the fundamental issues related to the theoretical justification and practical significance of the findings. The authors acknowledged the limitations in the scope of their analysis and provided additional data, but these efforts were insufficient to overcome the paper's weaknesses.\n\n### Conclusion:\nWhile the paper presents an interesting empirical observation, it falls short in providing a comprehensive and theoretically grounded explanation for the observed phenomena. The practical significance of the findings is limited, as the Swap model with bounded activations does not outperform conventional models with ReLU. The lack of clarity and rigor in the presentation further detracts from the paper's overall quality. Given these considerations, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" presents DeepSeaProbLog, an extension of DeepProbLog that supports both discrete and continuous random variables. The paper aims to address the limitations of existing neural probabilistic logic programming systems by introducing a language that can handle infinite and uncountable domains.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper introduces a novel approach to integrating discrete and continuous domains in neural probabilistic logic programming, which is a significant step forward in the field of neural-symbolic AI. The theoretical contributions, particularly the link between the semantics of hybrid probabilistic programming and WMI inference, are noteworthy.\n2. **Implementation and Experiments:** The authors provide an implementation of DeepSeaProbLog and conduct experiments demonstrating its capabilities. The experiments, although limited, show the potential of the approach in handling complex tasks that involve both discrete and continuous variables.\n3. **Theoretical Rigor:** The paper includes rigorous theoretical proofs supporting the proposed methods, which adds to the credibility of the work.\n\n**Weaknesses:**\n1. **Empirical Evaluation:** Several reviewers pointed out that the empirical evaluation is not extensive enough. The experiments are limited to a few toy applications, and there is a lack of comparison with existing benchmarks and baselines, especially in the discrete setting.\n2. **Incrementality Concerns:** Some reviewers feel that the work is an incremental improvement over DeepProbLog. While the authors argue that their contributions are significant, the perception of incrementality could affect the perceived impact of the work.\n3. **Clarity and Presentation:** There are concerns about the clarity of the paper, particularly regarding the handling of discrete random variables and the semantics of the proposed language. Some key details are relegated to the appendix, which could hinder understanding.\n4. **Scalability and Practicality:** Questions about the scalability and practical applicability of the approach remain, as the paper does not thoroughly address these aspects.\n\n**Author Responses:**\nThe authors have provided detailed responses to the reviewers' concerns, clarifying several points and making commitments to improve the paper. They have addressed the incrementality concerns by explicitly stating their contributions and have promised to release their implementation upon acceptance.\n\n**Conclusion:**\nThe paper presents a novel and theoretically sound approach to extending neural probabilistic logic programming to discrete-continuous domains. While there are concerns about the empirical evaluation and clarity, the theoretical contributions and potential impact on the field are significant. The authors' responses indicate a willingness to address the reviewers' concerns, which is promising.\n\nGiven the novelty and potential impact of the work, along with the authors' commitment to improving the paper, I lean towards acceptance. However, the authors should be encouraged to enhance the empirical evaluation and clarity in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi Task Learning of Different Class Label Representations for Stronger Models\" introduces a novel approach to image classification by using binary label representations as an auxiliary task in a multi-task learning framework. The paper presents an interesting idea that explores an understudied aspect of classification, namely label representation. However, there are several critical issues that need to be addressed before the paper can be considered for acceptance at a top-tier conference.\n\n### Strengths:\n1. **Novelty**: The idea of using binary labels as an auxiliary task is novel and could potentially lead to improvements in classification performance.\n2. **Potential Impact**: If successful, this approach could provide a cheaper alternative to improving performance compared to data augmentation or network architecture improvements.\n3. **Empirical Results**: The paper reports improvements in accuracy across several datasets, especially in scenarios with limited training data.\n\n### Weaknesses:\n1. **Clarity and Explanation**: The paper suffers from clarity issues, particularly in explaining the generation and role of binary labels. The lack of a clear explanation of the methodology, including the use of Metabalance, hinders understanding and reproducibility.\n2. **Experimental Evaluation**: The experimental section is weak, lacking comparisons with state-of-the-art methods and additional baselines. The results are not sufficiently robust or comprehensive to support the claims made.\n3. **Theoretical Insight**: There is a lack of theoretical insight or analysis into why binary labels improve performance. The paper does not adequately address the intuition or motivation behind the approach.\n4. **Reproducibility**: The paper lacks sufficient details for reproducibility, such as implementation details and hyperparameter settings. The absence of an appendix or supplementary material further exacerbates this issue.\n5. **Related Work**: The discussion of related work is insufficient. The paper does not adequately position itself within the existing literature on label representations and multi-task learning.\n6. **Significance of Contribution**: While the idea is novel, the contribution is not significant enough given the small empirical improvements and the lack of a deeper understanding of the problem.\n\n### Review Summary:\nThe reviewers generally agree that while the paper presents an interesting idea, it is not yet ready for publication at a top-tier conference. The main concerns revolve around the lack of clarity, insufficient experimental validation, and the need for a more thorough exploration of the approach's theoretical underpinnings. The paper would benefit from addressing these issues, expanding the experimental section, and providing a more detailed discussion of related work.\n\n### Conclusion:\nGiven the current state of the paper, it does not meet the standards required for acceptance at a top-tier conference. The authors are encouraged to address the identified weaknesses, particularly in terms of clarity, experimental rigor, and theoretical insight, before resubmitting.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" presents a new dataset for precipitation nowcasting, which includes 3D radar echo observations and covers multiple climate zones in Russia. The dataset is comprehensive, with a high temporal resolution and a significant amount of data collected over two years. The authors also evaluate several baseline models on this dataset and provide uncertainty quantification.\n\n**Strengths:**\n1. **Dataset Richness:** The dataset is rich in terms of geographical and meteorological data, offering a new dimension of precipitation measurements that are not commonly available in existing datasets.\n2. **Clarity and Reproducibility:** The paper is well-written, clear, and the methods are reproducible. The dataset and code are publicly available, which is a significant contribution to the community.\n3. **Baseline Evaluation:** The authors provide a thorough evaluation of several baseline models, including a recent transformer-based model, Earthformer, which adds value to the dataset's utility.\n\n**Weaknesses:**\n1. **Limited Novelty:** The paper lacks methodological novelty. The primary contribution is the dataset itself, which, while valuable, may not meet the novelty standards of a top-tier conference.\n2. **Baseline Models:** The baseline models used are relatively simple, and the evaluation could be more comprehensive with more advanced models.\n3. **Evaluation Metrics:** The use of MSE and F1 as evaluation metrics is somewhat limited. More sophisticated metrics could provide a better understanding of model performance.\n4. **Lack of Additional Experiments:** The authors claim the dataset's applicability to other ML tasks but do not provide experiments to support this claim.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the dataset is valuable, the paper lacks the novelty and methodological contributions expected at a top-tier conference. The dataset's release is important, but it may be more suitable for a journal focused on datasets or a conference with a specific track for dataset contributions.\n\n**Author Response:**\nThe authors have addressed some concerns by adding results from a transformer-based model and clarifying the dataset's limitations. However, the core issues of limited novelty and the need for more comprehensive evaluations remain.\n\n**Conclusion:**\nWhile the dataset is a valuable contribution to the field of precipitation nowcasting, the paper does not meet the novelty and methodological innovation criteria typically required for acceptance at a top-tier conference. The focus on the dataset without significant methodological advancements or novel insights limits its suitability for such venues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Enriching Online Knowledge Distillation with Specialist Ensemble\" presents a framework for online knowledge distillation that aims to enhance teacher diversity through a label prior shift and importance sampling. The authors propose a novel aggregation strategy using post-compensated Softmax and demonstrate the efficacy of their approach through extensive experiments.\n\n**Strengths:**\n1. **Clarity and Structure:** The paper is generally well-written and structured, making it easy to follow. The authors provide a clear motivation for their work and describe their methodology in detail.\n2. **Novelty in Framework:** While the individual components like label prior shift and PC-Softmax have been used before, their integration into a framework for online distillation is novel.\n3. **Empirical Analysis:** The authors conduct thorough experiments and provide ablation studies to support their claims. They demonstrate improvements in accuracy and calibration over prior methods.\n\n**Weaknesses:**\n1. **Limited Novelty:** Several reviewers noted that the novelty of the contributions is marginal, as the paper combines existing techniques without substantial innovation.\n2. **Experimental Comparisons:** The paper lacks comparisons with some recent and relevant methods, which could provide a more comprehensive evaluation of the proposed approach's effectiveness.\n3. **Specialization and Applicability:** The method seems specialized for online distillation and peer networks, and the authors do not convincingly argue why it cannot be applied to offline distillation or non-peer networks.\n4. **Reproducibility Concerns:** Some reviewers pointed out inconsistencies in reproduced results from other works, raising concerns about the reproducibility of the proposed method's results.\n\n**Author Responses:**\nThe authors provided detailed responses to the reviewers' concerns, including additional experiments and clarifications. They addressed the novelty of their approach and provided further empirical evidence to support their claims. However, some concerns about the novelty and broader applicability of the method remain.\n\n**Conclusion:**\nWhile the paper presents a well-structured and empirically supported framework for online knowledge distillation, the novelty of the contributions is limited, and the experimental comparisons are not comprehensive. The authors' responses and additional experiments help address some concerns, but the paper still falls short of the high novelty and significance standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics\" presents an investigation into the average-case performance of q-replicator dynamics in games, focusing on both convergence and the quality of equilibria reached. The paper is evaluated based on the following criteria: correctness, technical novelty and significance, empirical novelty and significance, clarity, and relevance to a top-tier conference.\n\n### Correctness\nThe reviewers generally agree that the claims and statements in the paper are well-supported and correct. The authors have addressed specific concerns raised by the reviewers, such as clarifying the statement of Theorem 4.4 and improving the formatting of citations. The authors have also provided a detailed explanation of the monotonicity of stable manifolds and regions of attraction, which was a point of confusion.\n\n### Technical Novelty and Significance\nThe paper attempts to extend the study of average-case performance metrics to a broader class of games, building on prior work (PP16). However, the reviewers express concerns about the novelty and significance of the contributions. Reviewer 2 and Reviewer 3 highlight that the results seem incremental and preliminary, particularly since the main results are limited to symmetric 2x2 coordination games. The authors argue that initiating this line of research is a contribution in itself, but the reviewers are not fully convinced of the paper's impact in terms of technical novelty.\n\n### Empirical Novelty and Significance\nThe empirical contributions are considered marginally significant. The paper provides experimental results to support the theoretical findings, but these are primarily limited to 2x2 games. The reviewers express a desire for a more comprehensive treatment of larger and more complex game settings.\n\n### Clarity\nThe paper is generally well-written and easy to read, with clear motivation and explanation of proof ideas. The authors have made efforts to address clarity issues raised by the reviewers, such as overloading notation and explaining factorization better.\n\n### Relevance to a Top-Tier Conference\nThe problem addressed by the paper is relevant and interesting, but the scope and depth of the results may not meet the standards of a top-tier conference. Reviewer 4 suggests that the paper might be better suited for a conference with a more focused audience, such as EC, given the specific nature of the results.\n\n### Conclusion\nWhile the paper addresses an important problem and provides some interesting insights, the contributions are perceived as incremental and preliminary. The main results are limited to a specific class of games, and the paper does not provide a comprehensive treatment of the general case. The novelty and significance of the contributions are questioned, and the paper may not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" presents a method for privacy-preserving representation learning in the context of Machine Learning as a Service (MLaaS). The proposed approach involves splitting a neural network into two parts, with the client using the first part to encode data before sending it to the server for inference. The paper claims to maintain high accuracy while minimizing information leakage.\n\nUpon reviewing the comments from the three reviewers, several critical issues are identified:\n\n1. **Novelty and Incremental Contribution**: \n   - Reviewer 1 and Reviewer 2 both highlight that the idea of splitting a network for privacy is not novel and has been explored in prior work, such as split learning. The paper does not sufficiently differentiate itself from existing methods or provide a significant advancement over the state of the art.\n   - The paper does not mention related work on split learning, which is a significant oversight.\n\n2. **Experimental Evaluation**:\n   - The experimental evaluation is limited, as noted by Reviewer 1 and Reviewer 2. The paper only uses two datasets (MNIST and CelebA) and compares against a single baseline. This is insufficient for a top-tier conference, where a more comprehensive evaluation with multiple datasets and baselines is expected.\n   - Reviewer 2 also points out that the models used (LeNet-5 and VGG-16) are relatively shallow and not representative of more complex architectures used in practice.\n\n3. **Technical Flaws and Threat Model**:\n   - Reviewer 2 provides a detailed critique of the technical flaws in the proposed approach, particularly concerning the threat model. The server's ability to recreate the encoder and the lack of defense during the training phase are significant issues that undermine the privacy claims.\n   - The paper does not adequately address how the server can be prevented from reconstructing the input data, which is a critical aspect of privacy-preserving methods.\n\n4. **Clarity and Quality**:\n   - While the paper is generally clear, there are several grammatical errors and ambiguous statements that detract from its quality, as noted by Reviewer 1 and Reviewer 2.\n   - Reviewer 3 expresses confusion about how the model guarantees privacy, indicating that the explanation of the method's effectiveness is not sufficiently clear.\n\n5. **Lack of Author Response**:\n   - The absence of an author response means that the authors did not address the reviewers' concerns or provide clarifications, which is a missed opportunity to strengthen their submission.\n\nGiven these considerations, the paper does not meet the standards of a top-tier conference due to its limited novelty, insufficient experimental validation, significant technical flaws, and lack of clarity in addressing privacy concerns.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning\" presents a novel approach to address the challenges of client drift and introduces the concept of period drift in federated learning (FL). The proposed method, FedPA, utilizes a meta-learning framework to learn a parameterized aggregator that aims to mitigate these drifts.\n\n**Strengths:**\n1. **Identification of Period Drift:** The paper introduces the concept of period drift, which is a novel contribution to the FL literature. This could potentially open new avenues for research in FL.\n2. **Meta-Learning Approach:** The use of a meta-learning framework to address aggregation bias is an interesting approach that could offer insights into improving FL performance.\n3. **Experimental Results:** The paper provides experimental results showing that FedPA can outperform some existing FL baselines on certain datasets.\n\n**Weaknesses:**\n1. **Lack of Theoretical Justification:** The paper does not provide sufficient theoretical analysis to distinguish between client drift and period drift. This weakens the motivation for the proposed method.\n2. **Experimental Limitations:** The experiments are conducted on relatively simple datasets (EMNIST and MovieLens), and the results are not compared against more complex datasets like CIFAR-100 or Stack Overflow, which are more representative of real-world FL scenarios.\n3. **Proxy Dataset Assumption:** The reliance on a proxy dataset for training the aggregator is a significant limitation, as it may not always be feasible or privacy-compliant in real-world applications.\n4. **Reproducibility Concerns:** The paper lacks open-source code, which raises concerns about the reproducibility of the results.\n5. **Writing and Clarity Issues:** Multiple reviewers noted grammatical errors and inconsistencies in the writing, which detracts from the overall clarity and quality of the paper.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting idea, it lacks sufficient theoretical and experimental support to justify its claims. The novelty is limited, and the practical applicability of the method is questionable due to the reliance on a proxy dataset. The paper also needs significant improvements in writing and clarity.\n\n**Conclusion:**\nGiven the weaknesses identified, particularly the lack of theoretical justification, limited experimental validation, and practical limitations, the paper does not meet the standards of a top-tier conference. The contributions, while potentially interesting, are not sufficiently novel or significant to warrant acceptance without further development and validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Identifying Phase Transition Thresholds of Permuted Linear Regression via Message Passing\" presents an investigation into the phase transition thresholds in permuted linear regression problems using a message-passing framework. The paper is evaluated based on the comments from five reviewers, each providing insights into the strengths and weaknesses of the work.\n\n### Strengths:\n1. **Interesting Problem**: The paper addresses an interesting problem in the field of permuted linear regression, which is relevant for understanding phase transitions in high-dimensional statistical models.\n2. **Novel Contribution**: The identification of phase transition thresholds is a novel contribution, as previous works focused only on statistical order without precise thresholds.\n3. **Technical Depth**: The paper employs advanced techniques such as message passing and recursive distributional equations, which are technically sophisticated.\n\n### Weaknesses:\n1. **Clarity and Organization**: Multiple reviewers noted that the paper is poorly written, with significant issues in clarity and organization. The notation is heavy, and the exposition is difficult to follow without prior knowledge of related works.\n2. **Incremental Contribution**: The novelty of the contribution is questioned, as much of the work builds on existing frameworks and results from prior literature. The paper is seen as incremental rather than groundbreaking.\n3. **Lack of Self-Containment**: The paper heavily relies on prior works for understanding, making it inaccessible to readers unfamiliar with those works. It lacks a self-contained exposition.\n4. **Empirical Validation**: There is a lack of clear empirical validation and reproducibility. The theoretical results are asymptotic, and the empirical results are for finite samples, leading to a disconnect between theory and practice.\n5. **Writing Quality**: Numerous grammatical errors, typos, and unclear explanations detract from the paper's readability and professionalism. This affects the overall presentation and understanding of the work.\n6. **Assumptions and Justifications**: Some assumptions, such as the known signal matrix in the oracle case, are seen as unrealistic or not well-justified. The paper lacks rigorous justification for certain claims and assumptions.\n\n### Conclusion:\nThe paper presents an interesting study on phase transitions in permuted linear regression, but it falls short of the standards expected at a top-tier conference due to significant issues in clarity, organization, and novelty. The writing quality and lack of self-containment make it difficult for readers to fully grasp the contributions without extensive background knowledge. Additionally, the empirical validation is not sufficiently robust to support the theoretical claims.\n\nGiven these considerations, the paper requires substantial revision and improvement before it can be considered for publication at a top-tier conference. The authors should focus on improving the clarity and organization of the paper, providing a more self-contained exposition, and strengthening the empirical validation of their results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Functional Risk Minimization\" presents a novel framework that challenges the traditional assumptions of Empirical Risk Minimization (ERM) by introducing a model where each data point is associated with its own function. This approach is intriguing and potentially impactful, as it could provide new insights into handling noise and generalization in machine learning, especially in the over-parameterized regime.\n\n**Strengths:**\n\n1. **Novelty and Potential Impact:** The paper introduces a new perspective on modeling noise and learning objectives, which could significantly impact the understanding and development of machine learning models, particularly in the context of over-parameterized models.\n\n2. **Theoretical Contributions:** The authors provide a theoretical framework that subsumes ERM for several common loss functions, offering a new avenue for understanding generalization.\n\n3. **Empirical Results:** The paper includes experiments demonstrating the potential advantages of Functional Risk Minimization (FRM) over ERM in specific scenarios, particularly when noise is structured.\n\n4. **Author Response:** The authors have addressed several concerns raised by the reviewers, including scalability issues and the relationship between FRM and Bayesian models. They have also provided additional experiments to support their claims.\n\n**Weaknesses:**\n\n1. **Clarity and Presentation:** The paper suffers from clarity issues, particularly in distinguishing between the theoretical framework and the implementable algorithm. The presentation of key concepts and equations is not always clear, which could hinder understanding.\n\n2. **Scalability Concerns:** While the authors have addressed scalability to some extent, the proposed method still appears computationally intensive, and the experiments are relatively small-scale. The scalability to large datasets and models remains a concern.\n\n3. **Lack of Formal Guarantees:** The paper lacks formal arguments or proofs regarding when and why FRM will outperform ERM. The theoretical underpinnings of the proposed method need to be more robustly established.\n\n4. **Comparison with Existing Methods:** The paper could benefit from a more thorough comparison with existing methods, particularly in terms of computational efficiency and performance on large-scale tasks.\n\n**Conclusion:**\n\nThe paper presents a novel and potentially impactful idea that challenges existing paradigms in machine learning. However, the clarity of presentation, scalability, and lack of formal guarantees are significant concerns. While the authors have made efforts to address these issues in their response, the paper still requires substantial improvements to meet the standards of a top-tier conference. Given the current state of the paper, it may be more suitable for a workshop or a journal where the authors can further refine their ideas and address the concerns raised.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" presents a novel approach to reinforcement learning by proposing a bi-objective optimization model that aims to maintain a population of agents with diverse playing styles and high skill levels. The paper is evaluated on two games, Pong and Justice Online, and claims to improve the generalization of policies by ensuring diversity in playing styles.\n\n**Strengths:**\n1. **Novelty and Approach:** The paper introduces a bi-objective optimization model that is relatively novel in the context of reinforcement learning. The approach of maintaining a diverse set of playing styles while optimizing for skill level is an interesting contribution.\n2. **Clarity and Reproducibility:** The paper is generally well-written and provides sufficient details for reproducibility, as noted by the reviewers.\n3. **Empirical Results:** The results in the two games show promise, indicating that the proposed method can learn well-generalized policies.\n\n**Weaknesses:**\n1. **Literature Gaps:** The paper does not adequately address related work, particularly missing comparisons with established self-play algorithms like those discussed by Lanctot et al. (2017). This omission is significant as it could provide a more comprehensive baseline for comparison.\n2. **Experimental Design Issues:** The experimental setup, particularly the sampling of opponents, seems to favor the proposed method, potentially skewing results. This raises concerns about the validity of the empirical findings.\n3. **Limited Evaluation Domains:** The evaluation is limited to only two games, one of which is relatively simple (Pong). This limits the generalizability of the results and does not convincingly demonstrate the method's applicability to a wide range of RL tasks.\n4. **Simplification of Playing Style:** Reducing playing style to a single scalar is a significant simplification that may not capture the complexity of diverse strategies in more complex games.\n5. **Lack of Comparison with Quality-Diversity Algorithms:** The paper does not compare its approach with quality-diversity algorithms, which are relevant to the problem being addressed. This omission limits the understanding of the method's relative performance.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper addresses an important problem and presents an interesting approach, it suffers from significant weaknesses in terms of literature coverage, experimental design, and evaluation breadth. The lack of a response from the authors to address these concerns further weakens the case for acceptance.\n\n**Conclusion:**\nGiven the paper's novelty and potential, it could be a valuable contribution to the field. However, the significant gaps in literature coverage, experimental design issues, and limited evaluation domains make it unsuitable for a top-tier conference at this stage. The paper would benefit from addressing these weaknesses, particularly by including more comprehensive baselines and expanding the evaluation to more complex and varied domains.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Online black-box adaptation to label-shift in the presence of conditional-shift\" presents an empirical study on adapting machine learning models to distribution shifts that involve both label-shift and conditional-shift. The authors propose several heuristics to improve existing methods for online adaptation, particularly in scenarios where the label-shift assumption is violated.\n\n**Strengths:**\n1. **Relevance and Motivation:** The problem of distribution shifts is highly relevant in real-world applications, and the paper addresses a gap in the literature by considering both label-shift and conditional-shift.\n2. **Empirical Exploration:** The paper provides empirical results on synthetic and realistic datasets, which is valuable for understanding the practical implications of the proposed methods.\n3. **Honesty in Results:** The authors are transparent about the inconclusive nature of some results and do not oversell their contributions.\n\n**Weaknesses:**\n1. **Lack of Theoretical Foundation:** The proposed heuristics lack theoretical justification, which is a significant drawback for a top-tier conference. The paper is primarily empirical without a strong theoretical underpinning.\n2. **Limited Novelty and Impact:** The contributions are marginally novel, as the heuristics are extensions of existing methods without substantial innovation. The empirical improvements are not significant enough to demonstrate a clear advantage over existing methods.\n3. **Clarity and Self-Containment:** The paper is not self-contained, requiring readers to refer to previous work for a complete understanding. Key concepts and notations are not clearly defined, leading to confusion.\n4. **Empirical Scope:** The empirical study is limited in scope, with modest experiments that do not provide comprehensive insights into the effectiveness of the proposed methods across various types of distribution shifts.\n5. **Reviewer Consensus:** All reviewers express concerns about the lack of significant improvements, theoretical support, and clarity. The consensus is that the paper is not ready for publication in its current form.\n\n**Author Responses:**\nThe authors acknowledge the weaknesses pointed out by the reviewers and provide clarifications and additional context. However, the responses do not address the fundamental issues of theoretical justification and significant empirical improvements. The authors' revisions and explanations, while helpful, do not sufficiently elevate the paper to meet the standards of a top-tier conference.\n\n**Conclusion:**\nWhile the paper addresses an important problem and provides some empirical insights, the lack of theoretical foundation, limited novelty, and insufficient empirical impact make it unsuitable for acceptance at a top-tier conference. The paper would benefit from a more rigorous theoretical analysis and a broader empirical study to substantiate its claims.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Complex Query Answering with Neural Link Predictors\" presents a novel approach to answering complex queries over incomplete knowledge graphs using neural link predictors. The proposed method, Continuous Query Decomposition (CQD), is evaluated against state-of-the-art methods and demonstrates significant improvements in accuracy while using less training data.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new framework for complex query answering that leverages neural link predictors without the need for extensive training on complex queries. This is a significant contribution to the field, as it addresses the challenge of answering complex queries with missing edges in knowledge graphs.\n\n2. **Performance**: The proposed method achieves state-of-the-art results, with improvements ranging from 8% to 40% in Hits@3 across different datasets. This demonstrates the effectiveness of the approach.\n\n3. **Explainability**: The method provides a degree of explainability by showing intermediate solutions for each query atom, which is a valuable feature for understanding the reasoning process.\n\n4. **Efficiency**: The approach requires orders of magnitude less training data compared to existing methods, which is a practical advantage.\n\n5. **Comprehensive Evaluation**: The paper includes extensive experiments and comparisons with existing methods, providing a strong empirical foundation for the claims made.\n\n### Weaknesses:\n1. **Complexity and Clarity**: Some reviewers noted that the mathematical formalism is excessive and could be simplified for better clarity. While the authors have attempted to address this, it remains a point of concern.\n\n2. **Timing and Scalability**: There are concerns about the scalability of the combinatorial optimization method, especially for longer query chains. The authors have provided some timing results, but further analysis and optimization might be necessary for practical applications.\n\n3. **Comparison with Baselines**: While the authors have conducted additional experiments with different neural link predictors, there is still a need for a more detailed comparison with baselines using the same predictors to isolate the source of improvements.\n\n4. **Artificial Examples**: Some examples used in the paper were considered artificial, which could affect the perceived applicability of the method to real-world scenarios.\n\n### Author Responses:\nThe authors have been responsive to reviewer comments, providing additional experiments, clarifications, and updates to the paper. They have addressed concerns about the use of different neural link predictors and provided more details on the experimental setup and results.\n\n### Conclusion:\nThe paper presents a significant advancement in the field of complex query answering over knowledge graphs. Despite some concerns about clarity and scalability, the strengths of the paper, particularly its novelty, performance improvements, and practical efficiency, make it a strong candidate for acceptance. The authors have shown a willingness to address reviewer concerns and have provided additional data to support their claims.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training using task-specific updates of model parameters. The authors claim that their method, $\\alpha$VIL, outperforms existing multitask learning approaches in various settings.\n\n**Strengths:**\n1. **Clarity and Writing:** The paper is well-written and easy to follow, as noted by all reviewers. The authors provide a clear summary of related work and motivate their methodology well.\n2. **Novelty:** The approach of using task-specific updates for dynamic task weighting is novel, and the authors argue that it is different from existing methods like Discriminative Importance Weighting (DIW) and meta-learning approaches.\n3. **Empirical Evaluation:** The paper includes experiments on both computer vision (MultiMNIST) and natural language understanding (NLU) tasks, providing a broad evaluation of the proposed method.\n\n**Weaknesses:**\n1. **Theoretical Justification:** A significant concern raised by all reviewers is the lack of theoretical justification or formal analysis of the proposed methodology. This is crucial for a top-tier conference, as it provides a deeper understanding of why the method works and under what conditions it is expected to perform well.\n2. **Experimental Results:** The experimental results are not consistently superior to the baselines. On the NLU tasks, the improvements are marginal and often fall within the range of statistical insignificance. The results on MultiMNIST, while better, are not convincingly significant given the standard deviations.\n3. **Baseline Comparisons:** The choice of baselines is somewhat limited, and the results do not consistently show that $\\alpha$VIL outperforms existing methods. The reviewers suggest that the improvements are minor and not statistically significant.\n4. **Algorithmic Clarity:** There are concerns about the clarity and intuition behind certain algorithmic choices, such as the use of alpha parameters and their distinction from task-specific weights.\n5. **Experimental Design:** The choice of datasets and tasks for evaluation is questioned, particularly the use of MultiMNIST with only one auxiliary task, which does not fully demonstrate the potential of multitask learning with multiple auxiliary tasks.\n\n**Author Responses:**\nThe authors acknowledge the lack of theoretical justification and promise to address this in a future version. They also provide explanations for some of the algorithmic choices and experimental design decisions. However, these responses do not fully resolve the concerns about the significance of the results and the theoretical underpinnings of the method.\n\n**Conclusion:**\nWhile the paper presents an interesting and novel approach to multitask learning, the lack of theoretical justification and the marginal improvements in experimental results are significant drawbacks. For a top-tier conference, it is essential that the proposed method not only demonstrates clear empirical benefits but also provides a solid theoretical foundation. The authors' responses indicate potential improvements, but these are not present in the current submission.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" presents an investigation into the behavior of stochastic gradient descent with momentum (SGDm) under non-iid sampling conditions, specifically focusing on covariate shift. The authors identify a resonance phenomenon that can lead to divergence in SGDm, which is a novel insight into the optimization process under non-standard conditions.\n\n**Strengths:**\n\n1. **Novelty and Importance:** The paper addresses an important problem in optimization, particularly relevant for settings like continual learning and reinforcement learning where non-iid sampling is common. The identification of resonance as a potential cause of divergence in SGDm is a novel contribution.\n\n2. **Theoretical and Empirical Analysis:** The paper combines theoretical analysis with empirical validation. The authors use existing mathematical tools to analyze the problem and supplement their findings with experiments that extend beyond the theoretical assumptions.\n\n3. **Clarity and Structure:** The paper is well-written and structured, following a clear scientific format from hypothesis to analysis and empirical validation.\n\n4. **Reviewer Support:** Several reviewers acknowledge the importance of the problem and the quality of the writing. Reviewer 4 and Reviewer 6, in particular, express strong support for the paper, highlighting its potential impact and thoroughness.\n\n**Weaknesses:**\n\n1. **Technical Novelty Concerns:** Reviewers 2 and 3 raise concerns about the novelty of the mathematical techniques used. They argue that the paper relies heavily on existing results and does not provide significant new theoretical contributions.\n\n2. **Practical Relevance:** There is skepticism about the practical relevance of the findings. Reviewer 5 questions whether the resonance phenomenon occurs frequently in real-world datasets, and Reviewer 6 suggests that the paper lacks empirical examples demonstrating the problem's relevance.\n\n3. **Presentation and Intuition:** Reviewers note that the paper could improve in explaining the intuition behind the resonance phenomenon and its implications for practical scenarios. The authors' response attempts to address this with additional explanations and analogies.\n\n4. **Stochasticity in SGDm:** Reviewer 3 questions the validity of the analysis given that the theoretical results are based on expected gradients, which may not fully capture the stochastic nature of SGDm.\n\n**Author Responses:**\n\nThe authors have provided detailed responses to the reviewers' concerns, acknowledging the need to soften claims of technical novelty and offering additional explanations and experimental results to address concerns about stochasticity and practical relevance. They express willingness to make changes to the paper based on feedback and propose further empirical work to explore the phenomenon in real-world settings.\n\n**Conclusion:**\n\nThe paper presents a novel and interesting insight into the behavior of SGDm under non-iid conditions, which is relevant for certain machine learning applications. While there are concerns about the technical novelty and practical implications, the authors have addressed these concerns to some extent in their responses. The paper's strengths in identifying a new phenomenon and providing a thorough analysis make it a valuable contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents an analysis of recursive value estimation using overparameterized linear representations in reinforcement learning (RL). The authors explore the convergence properties of classical value estimation algorithms (TD, FVI, and RM) in this setting and propose new regularizers to improve stability and generalization.\n\n### Strengths:\n1. **Novelty and Theoretical Contribution**: The paper addresses an important and timely topic in RL, focusing on the overparameterized regime, which is relevant given the current trends in deep learning. The theoretical analysis provides insights into the implicit biases of classical algorithms in this regime, which is a novel contribution.\n2. **Unified Interpretation**: The paper offers a unified interpretation of overparameterized linear value estimation, which is a valuable theoretical contribution.\n3. **Algorithmic Development**: The introduction of regularizers based on the theoretical findings is a practical contribution that could potentially improve the performance of RL algorithms in practice.\n\n### Weaknesses:\n1. **Lack of Empirical Rigor**: The empirical evaluation is limited to relatively simple environments, which raises concerns about the generalizability of the proposed regularizers to more complex, real-world scenarios.\n2. **Connection to Deep RL**: Several reviewers pointed out that the connection to deep RL is not well-established. The overparameterized linear setting may not directly translate to the typical settings in deep RL, where the feature space is not necessarily extremely large.\n3. **Technical Issues and Clarity**: There are several technical issues and clarity problems noted by the reviewers, such as the lack of proper credit to existing results, errors in the proofs, and unclear explanations of certain assumptions and results.\n4. **Reviewer Concerns**: Some reviewers expressed concerns about the correctness and significance of certain claims, particularly regarding the matrix decomposition and the implications of the results in the context of deep RL.\n\n### Author Responses:\nThe authors have addressed some of the reviewers' concerns, particularly regarding the novelty of certain results and the connection to neural tangent kernels (NTK). However, some responses were not entirely satisfactory, especially concerning the empirical evaluation and the broader applicability of the results.\n\n### Conclusion:\nWhile the paper makes a respectable theoretical contribution to the understanding of overparameterization in RL, the empirical evaluation is not robust enough for a top-tier conference. Additionally, the connection to deep RL, which is crucial for the paper's relevance, is not convincingly established. The technical issues and lack of clarity in some parts further weaken the paper's overall impact.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" presents a new adversarial training algorithm aimed at enhancing robustness against adversarial attacks. The proposed method involves applying more regularization to data vulnerable to adversarial attacks, and it claims to achieve state-of-the-art performance in terms of both generalization and robustness.\n\n**Strengths:**\n1. **Theoretical Motivation:** The paper provides a theoretical basis for the proposed method, claiming to derive an upper bound of the robust risk. This is a significant aspect as it attempts to provide a mathematical foundation for the approach.\n2. **Empirical Results:** The paper includes extensive experiments on multiple datasets (CIFAR10, F-MNIST, SVHN) and demonstrates improvements in both clean and robust accuracy over existing methods.\n3. **Clarity and Reproducibility:** The paper is generally well-written, and the authors have provided code and implementation details, which supports reproducibility.\n\n**Weaknesses:**\n1. **Theoretical Contribution:** Several reviewers, including Reviewer 5, 9, and 16, have raised concerns about the theoretical contribution of Theorem 1. They argue that the theorem is essentially a re-expression of existing results (TRADES) and that the bound for multi-class problems is loose and lacks significant novelty.\n2. **Marginal Improvements:** The empirical improvements over existing methods like TRADES and HAT are reported to be marginal, especially when extra data is used. This raises questions about the practical significance of the proposed method.\n3. **Baseline Comparisons:** There are concerns about the baseline results, particularly with the use of extra data, where the improvements are not as pronounced as expected. This suggests that the baselines may not have been optimally tuned.\n4. **Complexity and Implementation:** While the authors argue that ARoW is easier to implement than HAT, the marginal improvements may not justify the adoption of a new method unless it provides substantial benefits.\n\n**Author Responses:**\nThe authors have addressed some of the concerns by clarifying the theoretical aspects and providing additional experimental results. They acknowledge that the theoretical bound is not better than TRADES for binary classification and have focused on multi-class problems. They also provided additional results with extra data and clarified the implementation advantages of ARoW.\n\n**Conclusion:**\nWhile the paper presents a theoretically motivated approach and shows empirical improvements, the concerns about the theoretical novelty and the marginal nature of the improvements are significant. For a top-tier conference, the contributions should be both novel and impactful. The theoretical contributions are not sufficiently distinguished from existing work, and the empirical gains, although consistent, are not substantial enough to warrant acceptance at a top-tier venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Proximal Validation Protocol\" introduces a novel approach to constructing validation sets in machine learning, aiming to address the trade-off between training data usage and reliable model evaluation. The proposed method, Proximal Validation Protocol (PVP), uses data augmentation and a distributional-consistent sampling algorithm to create a validation set without splitting the training data.\n\n### Strengths:\n1. **Novelty and Practical Relevance**: The paper addresses a significant and practical problem in machine learning, which is the trade-off between training data usage and validation set construction. The approach of using data augmentation to create a validation set is novel and could potentially be impactful in real-world applications.\n2. **Empirical Evaluation**: The paper provides empirical results across different data modalities (images, text, and tabular data), demonstrating the potential benefits of PVP over traditional validation protocols.\n\n### Weaknesses:\n1. **Empirical Evaluation Concerns**: Several reviewers pointed out that the datasets used for evaluation are small and may not fully support the claims. The choice of datasets like CIFAR10-LT and CIFAR-100-LT instead of larger and more standard datasets like ImageNet raises concerns about the generalizability of the results.\n2. **Lack of Theoretical Justification**: The paper lacks a theoretical basis for why the proximal validation set is representative of the data generating distribution. This is a significant concern as it questions the validity of the proposed method.\n3. **Clarity and Presentation**: The paper's presentation, particularly in the experiments section, is unclear. Important metrics and definitions are not well-explained, making it difficult to evaluate the contributions accurately.\n4. **Limited Scope of Evaluation**: The experiments focus on fixed models and do not adequately demonstrate the method's effectiveness in model selection or hyperparameter tuning, which are critical aspects of validation set usage.\n5. **Reproducibility and Robustness**: There is a lack of thorough ablation studies and robustness checks, such as the impact of different data augmentation techniques or the choice of hyperparameters.\n\n### Reviewer Consensus:\nThe reviewers generally agree that while the problem addressed is important and the proposed solution is novel, the paper suffers from significant weaknesses in empirical evaluation, theoretical justification, and clarity. The lack of a theoretical foundation and the use of small datasets undermine the paper's contributions. Additionally, the absence of an author response suggests that these concerns were not addressed.\n\n### Conclusion:\nGiven the significant concerns raised by the reviewers, particularly regarding the empirical evaluation's robustness and the lack of theoretical support, the paper does not meet the standards of a top-tier conference. The novelty of the approach is acknowledged, but the execution and validation of the proposed method are insufficient to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Accept",
      "correct": false,
      "response": "The paper \"Deep Coherent Exploration For Continuous Control\" presents a novel approach to exploration in deep reinforcement learning (RL) by introducing a framework that unifies step-based and trajectory-based exploration strategies. The authors propose modeling the last layer parameters of the policy network as latent variables and using a recursive inference step to handle these variables, which is claimed to improve the speed and stability of learning in various RL algorithms.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper extends the work of van Hoof et al. (2017) to deep RL, which is a significant contribution given the scalability challenges in deep RL. The recursive and analytical update for non-Markov policy gradients and the focus on the last layer are valuable additions.\n   \n2. **Clarity and Presentation**: The paper is generally well-written, with a clear introduction and related work sections. The authors have provided detailed mathematical derivations and an appendix to aid reproducibility.\n\n3. **Empirical Results**: The proposed method shows strong empirical results for on-policy methods (A2C and PPO) in Mujoco tasks, outperforming baseline methods. The ablation studies are insightful and add to the understanding of the method's effectiveness.\n\n4. **Scalability**: The method is scalable to deep RL algorithms, which is a significant advantage over previous methods that were limited to linear policies.\n\n### Weaknesses:\n1. **Off-Policy Methods**: The integration with SAC is less effective and more heuristic compared to on-policy methods. The authors acknowledge this and propose a new formulation, but the results for this new approach are not yet available.\n\n2. **Limited Domains**: The experiments are limited to Mujoco tasks, which, while varied, do not cover more complex or diverse environments. This limits the generalizability of the results.\n\n3. **Complexity and Practical Significance**: While the method improves on-policy methods, its complexity and the lack of significant improvement in SAC raise questions about its practical significance, especially given SAC's status as a state-of-the-art method.\n\n4. **Presentation Issues**: Some sections, particularly those involving technical details, could be clearer. The figures also need improvement in terms of readability.\n\n### Author Responses:\nThe authors have addressed several concerns raised by the reviewers, particularly regarding the integration with SAC. They propose a new formulation that is more consistent with their approach for on-policy methods and promise to provide updated results. They also acknowledge the need for better presentation and clarity in certain sections and have committed to making these improvements.\n\n### Conclusion:\nThe paper presents a promising approach to exploration in deep RL, with strong theoretical foundations and empirical results for on-policy methods. However, the issues with off-policy methods and the limited experimental domains are significant drawbacks. The authors' commitment to addressing these issues is promising, but the lack of updated results for the new SAC formulation is a concern.\n\nGiven the strengths in novelty, scalability, and empirical results for on-policy methods, and considering the authors' responses and commitments to address the weaknesses, the paper has the potential to make a valuable contribution to the field. However, the decision hinges on the updated results for the new SAC formulation and improvements in presentation.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Disentangling Representations of Text by Masking Transformers\" presents a novel approach to extracting disentangled representations from pre-trained BERT models by learning binary masks over transformer weights or hidden units. The authors aim to identify subnetworks that encode distinct aspects of the representation, which could enhance model robustness by reducing reliance on spurious correlations.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The approach of using binary masks to uncover disentangled representations from pre-trained models is innovative. It sidesteps the need for training a disentangled model from scratch, which is computationally advantageous.\n\n2. **Clarity and Writing:** The paper is well-written and the methodology is clearly explained, making it accessible to readers.\n\n3. **Experimental Results:** The authors provide evidence that their method performs comparably or better than existing methods, such as variational autoencoders and adversarial training, in certain tasks. They also demonstrate the method's potential for improving robustness against spurious correlations.\n\n4. **Computational Efficiency:** The method does not require fine-tuning the entire model, which can be beneficial in terms of computational resources.\n\n**Weaknesses:**\n\n1. **Generality and Scope:** Several reviewers raised concerns about the generalizability of the method. The experiments are limited to specific datasets and tasks, primarily focusing on sentiment and genre in movie reviews. The authors' response provides some justification, but the lack of broader domain experiments remains a limitation.\n\n2. **Benchmark Performance:** The paper does not achieve state-of-the-art results on standard benchmarks, which is often a criterion for top-tier conferences. While the authors argue that the focus is on robustness and disentanglement, the lack of benchmark performance might limit the perceived impact.\n\n3. **Experimental Setup:** The choice of datasets and the limited number of genres in the experiments were questioned. The authors provided some rationale, but the concerns about the experimental setup's representativeness persist.\n\n4. **Technical Details:** Some technical aspects, such as the specifics of which layers are masked and the intuition behind the learned masks, were not fully detailed in the original submission, though the authors addressed these in their response.\n\n5. **Comparison with Baselines:** While the authors added comparisons with VAE baselines in their response, the initial lack of such comparisons was a significant oversight. The results still do not clearly outperform all baselines across all tasks, which could affect the paper's impact.\n\n**Conclusion:**\n\nThe paper presents an interesting and novel approach to disentangling representations in pre-trained models, which is a valuable contribution to the field. However, the concerns about generalizability, the limited scope of experiments, and the lack of state-of-the-art benchmark performance are significant drawbacks for a top-tier conference. While the authors have addressed some of these issues in their response, the paper still falls short of the high standards typically required for acceptance at such venues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A generalized probability kernel on discrete distributions and its application in two-sample test\" presents a novel approach to generalizing the maximum mean discrepancy (MMD) for discrete distributions. The authors propose a generalized probability kernel (GPK) and introduce power-MMD as an extension within this framework. However, the paper has several significant issues that need to be addressed before it can be considered for publication in a top-tier conference.\n\n### Strengths:\n1. **Novelty**: The idea of generalizing MMD for discrete distributions and introducing a new framework like GPK is interesting and could potentially contribute to the field of kernel-based hypothesis testing.\n2. **Potential Impact**: If properly developed, the proposed methods could offer new tools for two-sample testing, which is a fundamental problem in statistics and machine learning.\n\n### Weaknesses:\n1. **Theoretical Incompleteness**: The reviewers pointed out several theoretical gaps and inconsistencies. For instance, the definition of the kernel and its properties are not clearly established. The authors have attempted to address some of these issues in their revision, but the responses suggest that the theoretical foundation is still not robust.\n   \n2. **Lack of Experimental Evaluation**: The absence of any experimental results is a major drawback. For a paper proposing new statistical methods, empirical validation is crucial to demonstrate the utility and effectiveness of the proposed techniques.\n\n3. **Presentation and Clarity**: The paper suffers from numerous grammatical errors, unclear notation, and a lack of rigorous presentation. This makes it difficult to follow the arguments and understand the contributions. Although the authors have revised the paper to address some of these issues, the extent of the initial problems suggests that significant work is still needed.\n\n4. **Incomplete Responses to Reviewers**: While the authors have made some revisions in response to the reviewers' comments, many concerns remain unaddressed. For example, the removal of the KSD-related content indicates a lack of confidence in the original results, and the new focus on polynomial GPK still lacks sufficient theoretical and empirical support.\n\n5. **Relevance and Motivation**: The motivation for the proposed methods is not convincingly presented. The authors need to provide clearer examples and applications where their methods would be particularly beneficial.\n\n### Conclusion:\nGiven the current state of the paper, it does not meet the standards of a top-tier conference. The theoretical contributions are not sufficiently developed, the lack of empirical results is a critical omission, and the presentation issues significantly hinder the paper's readability and impact. The authors need to address these fundamental issues, provide a more rigorous theoretical foundation, and include comprehensive experimental validation before the paper can be reconsidered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"CAT: Collaborative Adversarial Training\" proposes a novel framework for adversarial training by leveraging the strengths of different adversarial training methods collaboratively. The authors claim that this approach improves the robustness of neural networks, as demonstrated through experiments on CIFAR-10 and CIFAR-100 datasets.\n\n### Strengths:\n1. **Novelty and Simplicity**: The idea of using collaborative adversarial training is straightforward and easy to implement. The paper is well-written and easy to follow, which is a positive aspect for reproducibility.\n2. **Experimental Results**: The authors report improved robustness and accuracy, achieving state-of-the-art results on CIFAR-10 under the Auto-Attack benchmark.\n3. **Clarity**: The paper is generally clear, and the proposed method is intuitive.\n\n### Weaknesses:\n1. **Lack of Theoretical Analysis**: The paper lacks a theoretical foundation explaining why the proposed collaborative approach leads to improved robustness. This is a significant gap, as understanding the underlying mechanisms is crucial for a top-tier conference.\n2. **Computational Cost**: The proposed method doubles the computational resources required, which is a substantial drawback given the already high cost of adversarial training.\n3. **Limited Novelty**: While the approach is somewhat novel, similar ideas have been explored in the context of ensemble methods and knowledge distillation. The paper does not adequately differentiate itself from these existing methods.\n4. **Insufficient Baseline Comparisons**: The paper does not compare its results against some of the more recent and stronger baselines, such as those involving adversarial weight perturbation (AWP) or other ensemble methods.\n5. **Limited Scope of Experiments**: The experiments are limited to small-scale datasets (CIFAR-10 and CIFAR-100) and do not include larger datasets like ImageNet, which would provide stronger evidence of the method's effectiveness.\n6. **Missing Ablations and Analysis**: Important ablations and analyses, such as the effect of using more than two defenses or the impact of different adversarial attacks, are missing. This limits the understanding of the method's robustness and generalizability.\n\n### Conclusion:\nThe paper presents an interesting approach to adversarial training, but it falls short in several critical areas required for acceptance at a top-tier conference. The lack of theoretical analysis, limited novelty, high computational cost, and insufficient experimental validation on larger datasets and stronger baselines are significant drawbacks. Additionally, the absence of an author response to address these concerns further weakens the case for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Structure and randomness in planning and reinforcement learning\" introduces a novel planning algorithm called Shoot Tree Search (STS), which aims to improve upon traditional Monte Carlo Tree Search (MCTS) by employing a multi-step expansion strategy. The paper presents empirical results showing that STS outperforms several baseline methods in challenging domains such as Sokoban and Google Research Football.\n\n**Strengths:**\n1. **Novelty and Simplicity:** The idea of multi-step expansion in tree search is simple yet potentially impactful, offering a new perspective on balancing depth and breadth in search strategies.\n2. **Empirical Evaluation:** The paper provides a thorough empirical evaluation across multiple domains, demonstrating the effectiveness of STS over traditional MCTS and other baselines.\n3. **Relevance:** The problem addressed is significant in the field of reinforcement learning, particularly in planning within large state spaces.\n\n**Weaknesses:**\n1. **Technical Novelty:** Several reviewers question the novelty of the approach, noting that similar ideas have been explored in past works. The authors argue that their approach is distinct due to the use of neural network value function approximators and the specific implementation of multi-step expansion.\n2. **Presentation and Clarity:** Reviewers have pointed out issues with the clarity and organization of the paper, including missing definitions and unclear explanations of algorithms.\n3. **Comparative Analysis:** The comparison with existing methods, particularly MCTS with large numbers of simulations, is not thoroughly addressed. The authors have provided additional experimental results in their response, but some concerns remain about the scalability and generalizability of STS.\n4. **Theoretical Insights:** The paper lacks a deep theoretical analysis of the proposed method, particularly concerning the bias-variance trade-off and the conditions under which STS provides significant advantages.\n\n**Author Responses:**\nThe authors have provided detailed responses to the reviewers' concerns, including additional experimental results and clarifications. They have acknowledged the need for better presentation and have committed to revising the paper to address these issues. The authors also highlight the practical importance of their method in scenarios with limited computational budgets.\n\n**Conclusion:**\nWhile the paper presents an interesting idea with promising empirical results, the concerns about technical novelty, clarity, and the depth of analysis are significant for a top-tier conference. The authors' responses and additional experiments partially address these concerns, but the paper still requires substantial revisions to meet the high standards expected at such a venue. Given the current state of the paper, it may benefit from further development and refinement before being considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper \"New Bounds For Distributed Mean Estimation and Variance Reduction\" presents a novel approach to distributed mean estimation (DME) by leveraging lattice-based quantization methods. The authors aim to address the limitations of previous methods that depend on the input norm, which can be problematic when the mean itself has a large norm. The paper proposes a method that focuses on the distance between inputs rather than their norm, providing both theoretical and practical insights.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a new approach to DME using lattice-based quantization, which is a significant contribution to the field. The method provides a solution quality dependent only on the distance between inputs, which is a novel perspective compared to traditional methods.\n\n2. **Theoretical Rigor:** The authors provide matching upper and lower bounds for their algorithms, demonstrating the asymptotic optimality of their communication-to-error trade-off. This theoretical grounding is crucial for a top-tier conference.\n\n3. **Practical Relevance:** The paper addresses a fundamental problem in distributed machine learning and provides a practical implementation of the proposed algorithms. The experiments, although limited, show improvements over existing methods.\n\n4. **Clarity and Structure:** The paper is generally well-written, with a clear description of the problem, motivation, and prior work. The main ideas are explained in a comprehensible manner.\n\n**Weaknesses:**\n\n1. **Experimental Limitations:** The experiments are primarily conducted with a small number of machines (n=2), which limits the expressiveness of the results. Although the authors mention additional experiments in the appendix, more extensive experimentation in the main text would strengthen the paper.\n\n2. **Algorithmic Complexity:** The practical version of the algorithm does not achieve the optimal theoretical bounds, and the authors acknowledge the gap between theory and practice. This discrepancy could be a concern for some reviewers, although the authors provide a reasonable justification.\n\n3. **Literature Review and Comparisons:** Reviewer 1 suggests that the paper could benefit from a more extensive discussion of related work, particularly in non-distributed settings. Incorporating these comparisons could provide a more comprehensive view of the contributions.\n\n4. **Minor Errors and Clarifications:** Several minor errors and points of clarification were noted by the reviewers, which the authors have addressed in their response. These include corrections to definitions, notations, and explanations of certain algorithmic steps.\n\n**Author Response:**\n\nThe authors have provided detailed responses to the reviewers' comments, addressing both major and minor concerns. They have acknowledged the limitations of their experiments and plan to include additional results in the revision. They have also clarified the theoretical aspects and plan to improve the presentation of their algorithms.\n\n**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of distributed machine learning with its novel approach to mean estimation. While there are some limitations, particularly in the experimental section, the theoretical contributions and the authors' willingness to address the reviewers' concerns suggest that the paper is suitable for a top-tier conference. The strengths of the paper, particularly its theoretical rigor and practical relevance, outweigh the weaknesses.\n\nFinal Decision: Accept"
    }
  ]
}