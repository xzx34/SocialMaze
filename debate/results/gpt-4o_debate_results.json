{
  "summary": {
    "model": "gpt-4o",
    "total_papers": 100,
    "correct_predictions": 86,
    "accuracy": 86.0,
    "confusion_matrix": [
      [
        76,
        0
      ],
      [
        14,
        10
      ]
    ],
    "stats": {
      "Reject": {
        "total": 76,
        "recall": 100.0,
        "precision": 84.44444444444444
      },
      "Accept": {
        "total": 24,
        "recall": 41.66666666666667,
        "precision": 100.0
      }
    }
  },
  "results": [
    {
      "paper_id": "rJePwgSYwB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"SGD Learns One-Layer Networks in WGANs\" presents an analysis of the convergence of stochastic gradient descent-ascent (SGD) for training Wasserstein GANs (WGANs) with a one-layer generator network. The authors claim that their work demonstrates convergence to a global solution in polynomial time and sample complexity. However, the reviewers have raised several critical concerns that need to be addressed to determine the suitability of this paper for a top-tier conference.\n\n1. **Simplified Discriminator Models**: The primary concern across all reviewers is the use of overly simplified discriminator models, specifically linear and quadratic discriminators. These models do not align with the standard practice in WGANs, which typically involve more complex neural network discriminators. The simplification undermines the applicability and relevance of the results to real-world WGAN training scenarios. Reviewer 1 and Reviewer 2 both emphasize that the choice of discriminator limits the generalizability of the findings and does not capture the essence of WGANs, which aim to approximate the Wasserstein distance using complex neural networks.\n\n2. **Misleading Claims**: Reviewer 1 and Reviewer 2 point out that the paper's title and claims may be misleading. The term \"one-layer network\" is ambiguous and could be interpreted in different ways. The paper's focus on simplified models does not support the broader claims about SGD learning in WGANs, as the results are not easily extendable to more complex and practical settings.\n\n3. **Lack of Extension to Complex Models**: Reviewer 3 highlights that the analysis provided in the paper cannot be easily extended to more complex discriminator models due to potential issues with error propagation in the gradient ascent step. This limitation further questions the practical impact of the theoretical results presented.\n\n4. **Insufficient Addressing of Reviewer Concerns**: Reviewer 1 notes that the authors' responses did not adequately address the concerns raised, particularly regarding the treatment of quadratic discriminators as WGANs and the lack of revisions to clarify these points.\n\n5. **Theoretical Contribution vs. Practical Relevance**: While Reviewer 2 acknowledges the theoretical contribution of analyzing gradient descent-ascent, the practical relevance is questioned due to the simplified setting. The theoretical insights, although valid, may not translate to practical improvements in WGAN training with standard neural network discriminators.\n\nIn summary, while the paper provides a theoretical analysis of SGD convergence in a simplified WGAN setting, the limitations in model complexity and the potential for misleading claims significantly detract from its suitability for a top-tier conference. The paper's contributions, although theoretically interesting, do not sufficiently advance the understanding or practice of WGAN training in realistic scenarios.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJlA6eBtvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Differentiable Hebbian Consolidation for Continual Learning\" presents a novel approach to addressing the problem of catastrophic forgetting in neural networks through the use of Differentiable Hebbian Plasticity (DHP) Softmax layers. The paper is well-motivated and grounded in cognitive principles, and it introduces a method that is compatible with differentiable programming frameworks. However, there are several critical points to consider when evaluating the paper for a top-tier conference.\n\n1. **Novelty and Contribution**: The paper introduces a new framework that combines Hebbian learning principles with differentiable programming. This is a novel approach and contributes to the field of continual learning by proposing a method that can potentially reduce forgetting. However, the novelty is somewhat diminished by the fact that the method primarily shows improvements when combined with existing synaptic consolidation methods, rather than standing out on its own.\n\n2. **Clarity and Presentation**: While the paper is generally well-written, there are areas where clarity is lacking, particularly in the technical details of the DHP Softmax. Reviewer 2 highlights confusion regarding the indexing in equations and the motivation for applying the Hebbian strategy only to the final softmax layer. These issues suggest that the paper could benefit from additional explanations and possibly illustrative figures to enhance understanding.\n\n3. **Empirical Evaluation**: The empirical evaluation is a significant concern. The paper primarily evaluates the method on MNIST variants and a vision dataset mixture, which are relatively simple benchmarks. Reviewers 1, 3, and 4 express concerns about the lack of evaluation on more challenging datasets like CIFAR-100 or CORe50, which are standard in the continual learning literature. The results on the simpler datasets are mixed, with the proposed method not consistently outperforming baselines or other methods.\n\n4. **Comparison with Existing Methods**: Reviewer 3 points out the lack of comparison with other CLS-based approaches, which is a gap in the experimental validation. Additionally, the method's performance relative to synaptic consolidation methods is not convincingly superior, as noted by Reviewer 4.\n\n5. **Potential for Improvement**: The paper has potential, particularly if the authors address the clarity issues and expand the empirical evaluation to include more challenging and diverse datasets. Demonstrating scalability and effectiveness in more complex settings would strengthen the paper significantly.\n\nIn conclusion, while the paper presents an interesting idea and has potential, the current version lacks the empirical rigor and clarity expected at a top-tier conference. The method's novelty is somewhat overshadowed by its reliance on existing techniques for strong performance, and the evaluation does not convincingly demonstrate its superiority or scalability.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BJe55gBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" presents a novel approach to understanding the expressivity of deep neural networks (DNNs) by leveraging Sharkovsky's Theorem from dynamical systems. The authors aim to address an open question posed by Telgarsky regarding the characterization of functions that cannot be well-approximated by shallow networks. The paper introduces a connection between DNN expressivity and periodic points, providing lower bounds for the width needed to represent periodic functions as a function of depth.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a novel application of Sharkovsky's Theorem to the field of deep learning, providing a new perspective on depth-width trade-offs in ReLU networks. This is a significant theoretical contribution that addresses an open question in the literature.\n\n2. **Theoretical Rigor**: The proofs and theoretical results are well-structured and appear to be correct, as noted by Reviewer 2. The use of dynamical systems and eigenvalue analysis to derive the results is both innovative and technically sound.\n\n3. **Clarity and Exposition**: Reviewers have praised the paper for its clear exposition, particularly the explanation of Sharkovsky's Theorem and its implications for neural networks. The paper is described as a delight to read, indicating that it is well-written and accessible to its intended audience.\n\n4. **Reviewer Enthusiasm**: All reviewers express strong support for the paper, with Reviewer 1 giving it the maximum score and Reviewer 2 strongly suggesting acceptance. This indicates a consensus on the paper's quality and significance.\n\n### Weaknesses:\n1. **Practical Implications**: Reviewer 3 raises a valid concern about the practical applicability of the theoretical results. While the paper broadens theoretical understanding, it lacks guidance on how to apply these results to real-world tasks, such as assessing or approximating the period for a given classification task.\n\n2. **Structure and Presentation**: Reviewer 3 suggests that the paper's structure could be improved by presenting the main theorem earlier and providing more intuition for some definitions. This could enhance the paper's readability and accessibility, particularly for readers less familiar with the mathematical background.\n\n3. **Additional Analysis**: Reviewer 2 requests additional analysis on the impact of a bias term and the sharpness of the lower bound. While these are not necessary for publication, addressing them could strengthen the paper's contribution and provide deeper insights.\n\n### Conclusion:\nThe paper makes a significant theoretical contribution to the understanding of depth-width trade-offs in ReLU networks by applying Sharkovsky's Theorem. Despite some concerns about practical applicability and presentation, the paper's novelty, theoretical rigor, and positive reception by reviewers make it a strong candidate for acceptance. The suggestions for improvement, such as restructuring and providing additional intuition, can be addressed in the final version.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "HyxWteSFwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Interaction Processes for Time-Evolving Graphs\" presents a method for modeling continuous time-evolving graphs using a temporal point process framework combined with a recurrent neural network architecture. The paper aims to address the challenges of modeling dynamic graphs at multiple time resolutions and introduces several mechanisms, such as time gates, attention mechanisms, and a selection mechanism for important nodes.\n\n**Strengths:**\n1. **Relevance and Importance:** The problem of modeling time-evolving graphs is significant, with applications in various domains such as social networks and fraud detection.\n2. **Empirical Performance:** The paper reports improved empirical performance on multiple datasets, which suggests that the proposed method is effective in practice.\n3. **Novelty in Combination:** While individual components like LSTM, attention mechanisms, and temporal point processes are not novel, their combination in this context is somewhat innovative.\n\n**Weaknesses:**\n1. **Lack of Novelty:** Both reviewers highlight that the individual components of the proposed method are not novel. The main novelty lies in the combination of these components, which is considered of limited novelty for a top-tier conference.\n2. **Insufficient Justification and Analysis:** There is a lack of detailed analysis and justification for the proposed mechanisms. For instance, the paper does not adequately demonstrate how the stacked LSTM captures multiple time resolutions or how the attention mechanism effectively selects relevant nodes.\n3. **Missing Comparisons:** The paper does not compare its method with JODIE, a recent and relevant baseline, which is a significant oversight. Such comparisons are crucial for establishing the method's relative performance.\n4. **Experimental Concerns:** The experimental setup raises concerns, such as the potential for the proposed method to have more parameters than the baselines, which could explain the performance improvements. Additionally, the paper does not use commonly accepted benchmarks, limiting the comparability of results.\n5. **Clarity and Readability:** Reviewer 3 points out that the paper is difficult to understand, lacking clarity in how the components work together and how the method handles dynamic graph changes. An overview diagram or toy example could improve readability.\n6. **Computational Cost:** There is no discussion of the computational cost or scalability of the proposed method, which is important for practical applications.\n\n**Conclusion:**\nThe paper addresses an important problem and shows promising empirical results. However, the lack of novelty, insufficient analysis, missing comparisons with relevant baselines, and issues with clarity and experimental setup are significant drawbacks. For a top-tier conference, these weaknesses outweigh the strengths, as the contribution does not meet the high standards of novelty, rigor, and clarity expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HyeKcgHFvS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Gradient-based training of Gaussian Mixture Models in High-Dimensional Spaces\" presents a novel approach to training Gaussian Mixture Models (GMMs) using Stochastic Gradient Descent (SGD) in high-dimensional spaces. The authors propose several innovations, including a max-component approximation for numerical stability, a new regularizer to avoid pathological local minima, and a method for enforcing GMM constraints in SGD. They also introduce an SGD-compatible simplification of the GMM model to manage memory usage.\n\n**Strengths:**\n1. **Relevance and Novelty:** The paper addresses a relevant problem in machine learning, particularly in the context of high-dimensional data. The use of SGD for GMMs is a novel approach that could offer computational advantages over traditional methods like EM.\n2. **Implementation:** The authors provide a publicly available TensorFlow implementation, which is a valuable resource for the community.\n3. **Clarity:** The paper is generally well-written and clear, making it accessible to readers.\n\n**Weaknesses:**\n1. **Incremental Contribution:** Reviewers 1 and 3 note that the contributions, particularly the max-component approximation, are incremental and do not significantly advance the state of the art. The max-component trick is a known technique, and the paper does not convincingly demonstrate its superiority over existing methods.\n2. **Experimental Evaluation:** The experimental results are not compelling. The datasets used (e.g., MNIST and SVHN) are not truly high-dimensional, and the results do not convincingly demonstrate the advantages of the proposed method over EM with K-means initialization. Reviewer 2 highlights the lack of convergence guarantees and the need for stronger empirical results.\n3. **Theoretical Justification:** There is a lack of theoretical guarantees for convergence or performance, which is a significant drawback for a top-tier conference. The paper relies heavily on empirical results, which are not sufficiently strong to compensate for this lack.\n4. **Regularization and Smoothing:** The regularization approach and the smoothing technique are not well-justified theoretically. Reviewers express confusion about the motivation and implementation details, indicating a need for clearer explanations and justifications.\n5. **Comparison with Baselines:** The paper does not provide a thorough comparison with existing methods, particularly EM with K-means initialization. The practical advantages of the proposed method are not clearly demonstrated.\n\n**Conclusion:**\nWhile the paper presents an interesting approach to training GMMs with SGD, it falls short in several critical areas. The contributions are incremental, the experimental results are not compelling, and there is a lack of theoretical justification and comprehensive comparison with existing methods. For a top-tier conference, the paper needs to provide either a significant theoretical advancement or exceptionally strong empirical results, neither of which is present here.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BklhsgSFvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Learning to Transfer via Modelling Multi-level Task Dependency\" presents a novel approach to multi-task learning by incorporating attention mechanisms to model task dependencies at multiple levels. While the idea of using attention to dynamically model task relationships is intriguing, several critical issues have been identified by the reviewers that need to be addressed before considering this paper for acceptance at a top-tier conference.\n\n1. **Clarity and Presentation**: Multiple reviewers have pointed out that the paper suffers from poor grammar and presentation issues, making it difficult to follow. Reviewer 1 and Reviewer 2 specifically mention the need for thorough proofreading and clearer explanations of key concepts. Reviewer 4 also highlights missing critical specifics and confusing notations, which hinder the understanding of the proposed method.\n\n2. **Novelty and Prior Work**: Reviewer 3 and Reviewer 4 raise concerns about the novelty of the work, noting that the claim regarding the assumption of task correlation in existing multi-task learning frameworks is not entirely accurate. Reviewer 4 points out the lack of discussion and comparison with critically related prior work, such as the \"Taskonomy\" paper, which already addresses task relationships in a similar manner. This oversight questions the novelty and contribution of the proposed method.\n\n3. **Experimental Evaluation**: The experimental section is deemed insufficient by the reviewers. Reviewer 2 suggests adding relevant baselines and comparisons to demonstrate the effectiveness of the attention component. Reviewer 4 also emphasizes the need for strong experimental comparisons with recent works to validate the proposed method's superiority.\n\n4. **Technical Details and Justification**: There are several technical ambiguities and missing details in the paper. Reviewer 2 and Reviewer 4 highlight issues with the explanation of the attention mechanism, task dependency matrices, and the overall framework. The justification for the hierarchical task dependency is also not convincingly presented, as noted by Reviewer 4.\n\n5. **Rebuttal and Revisions**: Although the authors have attempted to address some of the reviewers' concerns in their rebuttal, the revisions do not sufficiently resolve the major issues identified. The lack of a clear distinction from existing methods and the absence of comprehensive experimental validation remain significant drawbacks.\n\nIn conclusion, while the paper presents an interesting approach to multi-task learning, the issues related to clarity, novelty, experimental validation, and technical details are too significant to overlook. These shortcomings prevent the paper from meeting the high standards expected at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SkluFgrFwH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning Mahalanobis Metric Spaces via Geometric Approximation Algorithms\" presents an approach to Mahalanobis metric learning by formulating it as an optimization problem and proposing a fully polynomial time approximation scheme (FPTAS) for it. The paper claims to achieve nearly-linear running time and discusses improvements in practice, with experimental results on synthetic and real-world datasets.\n\n**Strengths:**\n1. **Theoretical Contribution:** The paper introduces an FPTAS for the Mahalanobis metric learning problem, which is a significant theoretical contribution, especially if the problem is computationally hard.\n2. **Parallelizability:** The algorithm is fully parallelizable, which is a practical advantage in handling large datasets.\n3. **Experimental Results:** The paper provides experimental results comparing the proposed method with classical methods like ITML and LMNN.\n\n**Weaknesses:**\n1. **Novelty and State-of-the-Art:** Reviewers express concerns about the novelty of the approach and its position relative to the state-of-the-art. The paper does not clearly articulate what makes the proposed method novel compared to existing methods.\n2. **Lack of Clarity:** Several points in the paper lack clarity, such as the definition of \"accuracy\" in Theorem 1.1, the computational hardness of the problem, and the combinatorial dimension in Lemma 2.1.\n3. **Missing Conclusion:** The absence of a conclusion section is a significant oversight, as it leaves the reader without a summary of the findings and implications.\n4. **Experimental Performance:** Reviewer 3 notes that the experimental results do not show a clear improvement in accuracy over existing methods, which raises questions about the practical benefits of the proposed approach.\n5. **Comparison with Recent Work:** The paper does not adequately compare its results with recent work in the field, such as the papers by Verma and Branson (NIPS 2015) and Ye et al. (IJCAI).\n\n**Reviewer Concerns:**\n- Reviewer 1 is not an expert in the subarea but raises concerns about the novelty and the lack of discussion on recent work.\n- Reviewer 2 appreciates the porting of LP-type problems into distance learning but highlights the need for clarity on computational hardness and existing results.\n- Reviewer 3 questions the novelty and practical advantages of the approach, given the lack of improvement in experimental accuracy.\n\n**Conclusion:**\nThe paper presents an interesting theoretical approach to Mahalanobis metric learning, but it falls short in several critical areas. The lack of clarity, insufficient comparison with recent work, and the absence of a conclusion section are significant drawbacks. Moreover, the experimental results do not convincingly demonstrate the practical benefits of the proposed method over existing approaches. Given these issues, the paper does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "DigrnXQNMTe",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A generalized probability kernel on discrete distributions and its application in two-sample test\" proposes a new kernel-based method for comparing discrete probability distributions. The reviewers have provided extensive feedback, highlighting several critical issues with the paper.\n\n1. **Theoretical Foundation and Novelty:**\n   - The paper aims to generalize existing discrepancy measures like MMD and KSD. However, the reviewers note that the theoretical contributions are not well-founded. Reviewer 1 points out that key definitions are unclear and potentially misnamed, questioning the fundamental understanding of the kernel concept. Reviewer 3 also questions the relevance of the proposed setting, especially in discrete contexts where labels do not naturally relate to distances.\n   - There is a lack of rigorous proofs and missing sections, such as the proof of Theorem 5 and the connection with Bernstein polynomials. Reviewer 2 mentions that the theoretical manipulations are elementary and mostly follow existing literature, indicating a lack of significant innovation.\n\n2. **Experimental Evaluation:**\n   - A major shortcoming is the complete absence of experimental evaluation. Reviewer 2 and Reviewer 3 both emphasize the need for experiments to validate the proposed methods, especially in a field where empirical results are crucial for demonstrating the utility of new statistical methods.\n\n3. **Presentation and Clarity:**\n   - The paper suffers from numerous grammatical errors, unclear notation, and a lack of coherent structure, as noted by all reviewers. This makes the paper difficult to follow and undermines the communication of the proposed ideas.\n   - Reviewer 4 highlights inconsistencies in definitions and notations, which further complicates understanding the paper's contributions.\n\n4. **Practical Relevance and Application:**\n   - The practical application of the proposed methods is not well-justified. Reviewer 4 questions the applicability of the proposed kernels in two-sample tests, especially given the need to know the distributions p and q, which is not typically feasible in such tests.\n\n5. **Literature Review and Context:**\n   - The paper lacks a thorough literature review and fails to adequately position its contributions within the existing body of work. Reviewer 4 suggests additional references that could provide context and support for the proposed methods.\n\nOverall, the paper presents an interesting idea of generalizing probability kernels, but it falls short in execution across multiple dimensions: theoretical rigor, empirical validation, clarity of presentation, and practical applicability. The issues highlighted by the reviewers are significant and suggest that the paper is not ready for publication in a top-tier conference without substantial revisions and additional work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns using a bidirectional LSTM RNN. However, there are several critical issues that need to be addressed before it can be considered for publication in a top-tier conference.\n\n1. **Novelty and Contribution**: Reviewer 3 points out that the paper lacks methodological novelty and is similar to previously published works, which are not cited. This is a significant concern for a top-tier conference, where originality and contribution to the field are paramount. The authors need to clearly articulate what new insights or advancements their work provides over existing studies.\n\n2. **Clarity and Detail**: There are several comments regarding the lack of clarity in the description of data and methods. Reviewer 3 mentions that the features used as inputs are not clearly described, and there is confusion about the representation of these features. Additionally, the motivation for using a bidirectional LSTM is not well justified, and the connection to the biological context is weak.\n\n3. **Evaluation and Baselines**: The evaluation of the model is not robust. Reviewers 1 and 3 both highlight the need for a fair comparison with other models using the same loss function. The paper should include additional baseline models and evaluation metrics to strengthen the claims of improved performance. The use of a modified weighted MSE as a loss function should be justified and compared with other common loss functions.\n\n4. **Generalizability**: Reviewer 1 raises concerns about the generalizability of the approach to other datasets and epigenetic features. The paper should address how the proposed method can be applied to different organisms or datasets beyond the specific case of Drosophila melanogaster.\n\n5. **Technical Issues and Presentation**: There are several technical issues and presentation errors noted by the reviewers, such as unclear equations, incorrect labels in figures, and lack of formalization in certain sections. These issues detract from the overall quality and readability of the paper.\n\n6. **Feature Importance and Model Interpretation**: The paper lacks a thorough analysis of feature importance and model interpretation, particularly for the LSTM model. Reviewer 3 suggests methods for quantifying feature importance, which would enhance the understanding of the model's predictions.\n\nIn summary, while the paper addresses an interesting problem and applies a relevant machine learning technique, it falls short in terms of novelty, clarity, evaluation, and generalizability. These are critical aspects for acceptance in a top-tier conference. The authors need to address these issues comprehensively to make a strong case for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxycxHKDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Domain Adaptive Multibranch Networks\" presents a novel approach to unsupervised domain adaptation by introducing a multiflow network that allows different domains to undergo different sequences of operations. This approach is innovative in addressing the information asymmetry between domains and offers flexibility in handling multiple domains simultaneously.\n\n**Strengths:**\n\n1. **Novelty and Innovation:** The idea of using adaptive computation graphs for domain adaptation is novel, particularly in the context of allowing different domains to have different computational paths. This approach is sensible and technically sound, as noted by Reviewer 2.\n\n2. **Potential for Extension:** The proposed framework can be easily extended to multi-task settings or multi-source/multi-target domain adaptation, which adds to its versatility and potential impact.\n\n3. **Clarity and Writing:** The paper is well-written and easy to follow, making the proposed method accessible to readers.\n\n4. **Motivation and Theoretical Soundness:** The motivation for the approach is clear, and the theoretical underpinnings are well-explained, as highlighted by Reviewer 1.\n\n**Weaknesses:**\n\n1. **Experimental Validation:** The most significant drawback is the insufficient experimental validation. The paper only compares its method with RPT and DANN, which are not state-of-the-art methods in domain adaptation. As pointed out by Reviewers 2 and 3, it is crucial to compare the proposed method with more recent and competitive methods like CDAN to establish its effectiveness.\n\n2. **Ablation Studies and Hyperparameter Analysis:** The paper lacks detailed ablation studies and hyperparameter sensitivity analyses. Reviewer 3 emphasizes the need to explore different configurations, such as varying the number of parallel flows and investigating the potential of the method in multi-source or multi-target domain adaptation.\n\n3. **Explanation of Results:** Reviewer 2 notes that the behavior of the network, particularly the lack of parameter sharing at the end of training, is counter-intuitive and requires further explanation.\n\n4. **Baseline Comparisons:** The baseline results presented in the paper are not comparable to those reported in other UDA papers, which raises concerns about the validity and competitiveness of the experimental results.\n\n**Conclusion:**\n\nWhile the paper presents a novel and interesting approach to domain adaptation, the lack of comprehensive experimental validation and comparison with state-of-the-art methods is a critical issue. The absence of detailed ablation studies and hyperparameter analyses further weakens the paper's contribution. To be considered for a top-tier conference, the paper needs to address these experimental shortcomings and provide a more thorough evaluation of the proposed method's effectiveness.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1xzdlStvB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs\" presents a novel approach to reduce training time for large-scale CNNs by employing a multi-precision strategy. The proposed method, MuPPET, aims to dynamically switch between different precision levels during training to optimize performance and energy efficiency.\n\n### Strengths:\n1. **Novelty**: The paper introduces a new training strategy that extends existing mixed-precision approaches by incorporating multiple precision levels, including low-precision fixed-point representations. This is a novel contribution to the field of quantized training.\n2. **Potential Impact**: The approach could significantly reduce training times and energy consumption, which are critical factors in large-scale deep learning applications.\n3. **Complementary Nature**: The proposed method is orthogonal and complementary to existing low-precision training techniques, potentially broadening its applicability.\n\n### Weaknesses:\n1. **Lack of Clarity and Justification**: Several reviewers pointed out that the paper lacks a clear explanation of the core algorithm, particularly the precision-switching mechanism. The motivation behind key decisions, such as the choice of precision levels and the switching criteria, is not well justified.\n2. **Inconsistent Results**: The results do not consistently outperform state-of-the-art methods. In some cases, the accuracy achieved by MuPPET is lower than the FP32 baseline, which raises concerns about the reliability and effectiveness of the proposed method.\n3. **Presentation Issues**: The paper suffers from presentation issues, including unclear notations, small and unreadable figures, and insufficient details about the quantization scheme. These issues hinder the reproducibility of the results.\n4. **Lack of Comprehensive Evaluation**: The paper does not provide training and validation curves, which are essential for understanding the training dynamics and the impact of the proposed method. Additionally, the isolated impact of various parameters on performance is not explored.\n5. **Heuristic Nature**: The approach appears heuristic, and the necessity of the dynamic switching mechanism for achieving good results is not convincingly demonstrated.\n\n### Conclusion:\nWhile the paper presents an interesting idea with potential benefits, it falls short in several critical areas. The lack of clarity in the algorithm's explanation, inconsistent results, and presentation issues are significant drawbacks. Moreover, the paper does not convincingly demonstrate the necessity and effectiveness of the dynamic precision-switching mechanism. Given these concerns, the paper does not meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rkgfdeBYvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\" presents a theoretical analysis of how different activation functions impact the training of overparametrized neural networks. The authors focus on the minimum eigenvalue of a Gram matrix, which is crucial for understanding the convergence rate of training in the lazy regime. The paper distinguishes between non-smooth and smooth activation functions, providing insights into their respective impacts on training dynamics.\n\n**Strengths:**\n\n1. **Theoretical Contribution:** The paper makes a significant theoretical contribution by analyzing the behavior of the minimum eigenvalue of the Gram matrix for different activation functions. This is a crucial aspect of understanding the training dynamics of overparametrized neural networks.\n\n2. **Comprehensive Analysis:** The authors provide a thorough analysis that includes both non-smooth and smooth activation functions. They offer insights into why non-smooth activations like ReLU tend to have large eigenvalues, while smooth activations can have small or zero eigenvalues under certain conditions.\n\n3. **Empirical Validation:** The paper includes empirical experiments that validate the theoretical findings, particularly on synthetic data. This strengthens the credibility of the theoretical results.\n\n4. **Well-Written and Referenced:** The paper is well-written, self-contained, and well-referenced, making it accessible to readers familiar with the topic.\n\n**Weaknesses:**\n\n1. **Length and Complexity:** Both reviewers noted that the paper is long and potentially difficult for a general audience to parse. The extensive appendix, while thorough, may overwhelm readers and detract from the main contributions.\n\n2. **Appendix Organization:** The organization of the appendix could be improved to emphasize the main techniques and ideas. Some theorems referenced in the appendix are not mentioned in the main text, which could confuse readers.\n\n3. **Gap in CIFAR10 Experiment:** There is a noted gap between theory and experiment in the CIFAR10 dataset, although this is partially mitigated by the agreement with theory in the ReLU experiment.\n\n4. **Clarity in Presentation:** Some parts of the paper, such as the explanation of the DZXP setting and certain assumptions, could be clarified to improve understanding.\n\n**Reviewer Consensus:**\n\nBoth reviewers acknowledge the paper's significant theoretical contributions and the importance of the topic. Reviewer 1 appreciates the theoretical analysis and empirical validation but suggests improvements in clarity and organization. Reviewer 2 strongly supports acceptance, highlighting the paper's thoroughness and the interesting use of polynomial approximations.\n\n**Conclusion:**\n\nGiven the paper's substantial theoretical contributions, empirical validation, and relevance to the field of deep learning, it meets the standards of a top-tier conference. The weaknesses identified, primarily related to presentation and organization, do not outweigh the paper's strengths. The authors are encouraged to address these issues in the final version to enhance clarity and accessibility.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Dmpi13JiqcX",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Disentangling Representations of Text by Masking Transformers\" presents a novel approach to extracting disentangled representations from pre-trained BERT models by learning binary masks over weights or activations. The paper is well-written and the idea is clearly explained, which is a positive aspect noted by multiple reviewers. The approach is computationally efficient as it does not require fine-tuning the entire model, which is an attractive feature for practical applications.\n\n**Strengths:**\n1. **Novelty and Simplicity:** The idea of using binary masks to identify subnetworks responsible for different text features is novel and straightforward. It avoids the need for training a disentangled model from scratch.\n2. **Computational Efficiency:** The method is computationally efficient as it does not require fine-tuning the entire model, which is beneficial for large-scale applications.\n3. **Clarity and Writing:** The paper is well-written and easy to understand, which is crucial for conveying complex ideas effectively.\n4. **Potential Impact:** The approach has the potential to impact the field by providing a new way to achieve disentangled representations without extensive computational resources.\n\n**Weaknesses:**\n1. **Generality and Scope:** Several reviewers raised concerns about the generality of the approach. The experiments are limited to a specific domain (movie reviews) and only two genres, which questions the broader applicability of the method.\n2. **Experimental Results:** The experimental results do not convincingly demonstrate superiority over existing methods. Some reviewers noted that the proposed method does not outperform fine-tuned baselines in certain tasks.\n3. **Lack of Benchmarking:** The paper lacks benchmarking against standard datasets and tasks, which is essential for assessing the method's effectiveness in a broader context.\n4. **Attribute Leakage:** There is still a fair amount of attribute leakage, and the probe used to measure this is relatively weak, which might not fully capture the extent of leakage.\n5. **Scalability Concerns:** The method increases the number of parameters significantly, which could be a concern for scalability, especially when dealing with multiple attributes.\n\n**Reviewer Concerns:**\n- The paper does not adequately address the generality of the approach across different domains and tasks.\n- There is a lack of discussion on the performance of the method in comparison to fine-tuned baselines, especially in semantic tasks.\n- The paper does not explore the potential of the disentangled representations in downstream tasks, which could demonstrate the practical utility of the approach.\n\n**Conclusion:**\nWhile the paper presents an interesting and novel approach, the concerns about the generality, experimental validation, and lack of benchmarking against standard datasets are significant. The paper does not convincingly demonstrate that the proposed method outperforms existing approaches or that it is applicable to a wide range of tasks and domains. For a top-tier conference, it is crucial that the paper not only presents a novel idea but also provides strong empirical evidence of its effectiveness and generality.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9_J4DrgC_db",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Deep Coherent Exploration For Continuous Control\" presents a method for exploration in deep reinforcement learning (RL) that aims to unify step-based and trajectory-based exploration strategies. The paper builds on the work of van Hoof et al. (2017) and extends it to deep RL methods, specifically targeting continuous control tasks.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces a scalable exploration framework for deep RL, which is a significant contribution given the challenges in exploration for deep RL methods. The approach of modeling the last layer parameters as latent variables and using recursive inference is innovative and addresses the scalability issue of previous methods.\n\n2. **Empirical Results:** The method shows strong empirical results for on-policy methods (A2C and PPO), with significant performance improvements over baselines. The detailed ablation studies and the comprehensive appendix enhance the reproducibility and understanding of the method.\n\n3. **Clarity and Presentation:** The paper is generally well-written, with clear explanations of the proposed method and its theoretical underpinnings. The introduction and related work sections are particularly well-done, situating the work within the broader RL literature.\n\n**Weaknesses:**\n\n1. **Off-Policy Performance:** The method does not perform as well with off-policy methods like SAC, and the results are not as strong as those for on-policy methods. This is a significant limitation, as SAC is a state-of-the-art method for continuous control tasks.\n\n2. **Limited Experimental Scope:** The experiments are limited to Mujoco tasks, which may not fully demonstrate the method's effectiveness in more complex or diverse environments. Additional experiments on tasks with sparse rewards or pure exploration challenges would strengthen the paper.\n\n3. **Complexity and Practicality:** The proposed method introduces additional complexity, particularly for off-policy methods, without a clear performance benefit. This raises concerns about the practical significance and applicability of the method in real-world scenarios.\n\n4. **Connection to Prior Work:** While the paper builds on van Hoof et al. (2017), some reviewers feel that the contribution is not sufficiently distinct from the prior work, particularly for the off-policy case.\n\n**Reviewer Consensus:**\n\nThe reviewers generally appreciate the novelty and potential impact of the method, particularly for on-policy methods. However, there are concerns about the off-policy performance, the limited experimental scope, and the practical significance of the method. Reviewer 1 suggests reframing the method as primarily an on-policy approach, which could address some of these concerns.\n\n**Final Decision:**\n\nGiven the strengths in novelty and on-policy performance, but considering the significant weaknesses in off-policy performance and experimental scope, the paper presents a mixed case. For a top-tier conference, the expectation is for a method to demonstrate strong performance across a range of settings and to have clear practical significance. While the paper has potential, the limitations, particularly in off-policy settings, are substantial.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "t86MwoUCCNe",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"New Bounds For Distributed Mean Estimation and Variance Reduction\" presents a novel approach to distributed mean estimation (DME) by leveraging lattice-based quantization methods. The authors aim to address the limitations of previous methods that depend on the norm of input vectors, which can be problematic when the mean itself has a large norm. Instead, the proposed method focuses on the distance between inputs, providing a more robust solution in such scenarios.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new quantization method using lattice theory, which is a significant contribution to the field of distributed machine learning. This approach allows for error bounds that are independent of the input norm, addressing a critical limitation in existing methods.\n\n2. **Theoretical Rigor**: The authors provide both upper and lower bounds for the communication-to-error trade-off, demonstrating the asymptotic optimality of their algorithms. This theoretical foundation is crucial for a top-tier conference.\n\n3. **Practical Implementation**: The paper not only presents a theoretical framework but also discusses practical implementations using cubic lattices, which are computationally feasible and only incur a logarithmic factor loss in accuracy.\n\n4. **Experimental Validation**: Although limited, the experiments show practical improvements over prior approaches, supporting the theoretical claims.\n\n5. **Clarity and Motivation**: Reviewers generally agree that the paper is well-written, with a clear problem statement and motivation. The overview of prior work is comprehensive, situating the contribution within the broader research landscape.\n\n### Weaknesses:\n1. **Literature Review and Discussion**: Reviewer 1 suggests that the paper could benefit from a more extensive discussion of related work, particularly in non-distributed settings. This could strengthen the paper by situating it more firmly within the existing body of research.\n\n2. **Algorithmic Details**: Reviewer 2 requests a more straightforward description of the computationally simple algorithm via reduction to $\\ell_\\infty$, which could enhance the paper's accessibility.\n\n3. **Experimental Limitations**: The experiments are limited to $n=2$, which is not very expressive for a distributed setting. Larger-scale experiments would provide stronger evidence of the method's practical applicability.\n\n4. **Minor Errors and Typos**: Several minor errors and typos were noted by the reviewers, which need to be addressed to improve the paper's polish.\n\n5. **Communication Cost**: Reviewer 3 raises a valid point about the communication cost of setting up the network (e.g., electing a leader or forming a binary tree), which is not accounted for in the analysis.\n\n### Conclusion:\nThe paper presents a significant advancement in the field of distributed mean estimation by addressing a fundamental limitation of previous methods. The theoretical contributions are robust, and the practical implications are promising. While there are areas for improvement, particularly in terms of experimental validation and minor clarifications, the overall contribution is substantial and aligns with the standards of a top-tier conference.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "Bkel6ertwS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning DNA folding patterns with Recurrent Neural Networks\" presents an interesting application of machine learning to predict DNA folding patterns using a bidirectional LSTM model. However, there are several critical issues that need to be addressed before considering it for publication in a top-tier conference.\n\n1. **Novelty and Contribution**: Reviewer 3 points out that the paper lacks methodological novelty and is similar to previously published works, which are not cited. This is a significant concern for a top-tier conference, where originality and contribution to the field are crucial. The authors need to clearly articulate what new insights or advancements their work provides over existing studies.\n\n2. **Clarity and Detail**: There are several comments regarding the lack of clarity in the paper. Reviewer 2 mentions that the equation is not clearly presented, and Reviewer 3 highlights that the description of data and methods is not clear enough. The paper needs to provide a detailed and clear explanation of the methods, including how features are represented and the rationale behind the choice of models.\n\n3. **Evaluation and Baselines**: The evaluation of the proposed model is not robust. Reviewer 1 and Reviewer 3 both suggest that the comparison with other models is not fair due to the use of different loss functions. The authors should ensure that all models are evaluated using the same metrics and provide additional baseline models for comparison. Moreover, the use of additional evaluation metrics like MSE and R^2 score is recommended to strengthen the evaluation.\n\n4. **Generalizability**: Reviewer 1 raises concerns about the generalizability of the approach, particularly the specificity of the loss function and its applicability to other datasets or epigenetic features. The authors should address these concerns by demonstrating the model's performance on diverse datasets and discussing its potential generalizability.\n\n5. **Feature Importance and Model Interpretation**: Reviewer 3 suggests that the paper lacks an analysis of feature importance for the biLSTM model. Understanding which features are important and how the model makes predictions is crucial for biological applications. The authors should provide an analysis of feature importance and discuss the biological significance of the findings.\n\n6. **Technical Issues and Improvements**: There are several technical issues and suggestions for improvement, such as clarifying the use of the biLSTM, addressing the choice of the center LSTM hidden state, and correcting errors in figures and text. These need to be addressed to improve the overall quality of the paper.\n\nIn summary, while the paper presents an interesting application of machine learning to a biological problem, it currently lacks the novelty, clarity, and robust evaluation required for acceptance in a top-tier conference. The authors need to address the concerns raised by the reviewers, particularly regarding the novelty, clarity of methods, evaluation, and generalizability, before the paper can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygcdeBFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Score and Lyrics-Free Singing Voice Generation\" presents a novel approach to generating singing voices without relying on pre-assigned scores and lyrics. The authors propose three different schemes for this task and develop a pipeline involving source separation, transcription models, and adversarial networks. While the paper tackles an interesting and challenging problem, several critical issues have been identified by the reviewers that need to be addressed before it can be considered for acceptance at a top-tier conference.\n\n1. **Evaluation and Metrics**: The primary concern across multiple reviews is the weak evaluation of the proposed models. The metrics used, such as \"Vocalness\" and \"Average Pitch,\" are criticized for not adequately capturing the quality of singing voice generation. The lack of a baseline comparison further exacerbates this issue, making it difficult to assess the success of the proposed methods.\n\n2. **Motivation and Justification**: The motivation for the chosen task settings is not clearly articulated. Reviewers have pointed out that the paper does not sufficiently explain why these specific tasks are important or what insights can be gained from their performance differences. Additionally, the choice of neural network architecture and the adaptation of BEGAN for sequence generation are not well justified with experimental results or literature support.\n\n3. **Dataset and Training Details**: There is a lack of clarity regarding the datasets used for training and evaluation. Reviewers have noted confusion about the genres and types of data included, as well as concerns about the quality of the training data due to the use of source separation. This lack of transparency hinders the ability to evaluate the generalizability and robustness of the models.\n\n4. **Comparison with Existing Methods**: The paper does not provide a comparison with existing methods, such as those using MIDI representations or state-of-the-art singing voice synthesis models. This omission makes it difficult to position the work within the broader context of singing voice generation research.\n\n5. **Literature Review and Writing**: While the paper is generally well-structured, there are areas where the literature review could be expanded, particularly regarding neural network-based singing voice synthesis. Additionally, some writing issues and unclear explanations detract from the overall clarity of the paper.\n\n6. **Novelty and Contribution**: The paper introduces a new problem and provides a baseline for future research. However, the novelty of the approach is somewhat overshadowed by the lack of strong experimental validation and theoretical justification.\n\nIn summary, while the paper addresses an interesting and novel problem, the issues related to evaluation, motivation, dataset clarity, and comparison with existing methods are significant. These concerns need to be addressed to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "H1livgrFvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Out-of-Distribution Image Detection Using the Normalized Compression Distance\" presents a novel approach to out-of-distribution (OOD) detection that does not require retraining the model or using OOD samples for validation. The method, MALCOM, leverages a similarity metric inspired by data compression to identify OOD samples based on global average pooling and spatial patterns in feature maps.\n\n**Strengths:**\n1. **Novelty and Cleverness:** The approach is innovative in its use of compression-based distance measures for OOD detection, which is a fresh perspective in this domain.\n2. **Constraints Addressed:** The paper addresses a practical problem by proposing a method that works without retraining or using OOD samples for validation, which can be beneficial in real-world applications where such data is unavailable.\n3. **Evaluation:** The authors have conducted experiments across several benchmarks and neural network architectures, supporting their claims about the method's effectiveness.\n\n**Weaknesses:**\n1. **Comparison and Fairness:** Reviewers noted that the comparison with existing methods might not be entirely fair. The replication of prior methods showed significantly lower performance than reported, raising concerns about the validity of the comparisons.\n2. **Justification of Constraints:** Reviewer 2 pointed out that the paper does not adequately justify the necessity of the strict constraints imposed by the method. It is unclear how often these constraints are encountered in practice, which affects the perceived importance of the work.\n3. **Significance and Complexity:** Reviewer 3 mentioned that the experiments could have been conducted in more complex settings, such as safety-critical applications, to better demonstrate the method's significance and efficiency.\n4. **Theoretical Guarantees:** There is a lack of theoretical guarantees or proofs regarding the exhaustiveness of the proposed method in detecting all possible OOD examples.\n5. **Presentation and Clarity:** The paper requires additional proofreading for grammatical errors, and the presentation of the problem's importance needs to be clearer.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting and novel approach, it lacks in certain areas such as fair comparison, justification of constraints, and demonstration of significance in complex settings. The concerns about the fairness of comparisons and the necessity of the constraints are significant, as they impact the paper's contribution to the field.\n\n**Final Decision: Reject**\n\nThe paper should be rejected due to the concerns about the fairness of comparisons, lack of justification for the constraints, and insufficient demonstration of significance in complex settings. These issues need to be addressed to meet the standards of a top-tier conference."
    },
    {
      "paper_id": "BJg8_xHtPr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"OBJECT-ORIENTED REPRESENTATION OF 3D SCENES\" presents a novel approach to unsupervised object-wise 3D scene decomposition and rendering using a generative model called ROOTS. The paper aims to extend the Generative Query Networks (GQN) framework by providing object-oriented representation decomposition and hierarchical object-oriented representation at both 3D global-scene and 2D local-image levels.\n\n**Strengths:**\n1. **Novelty and Contribution**: The paper introduces a new model, ROOTS, which aims to improve upon GQN by providing object-oriented and hierarchical representations. This is a significant contribution to the field of 3D scene understanding and representation learning.\n2. **Qualitative Results**: The qualitative results are impressive, demonstrating the model's ability to swap objects in and out of scenes and render 2D images from different viewpoints.\n3. **Potential Impact**: Learning 3D representations from 2D images is an important problem, and the proposed methodology could lead to more interpretable and compositional representations.\n\n**Weaknesses:**\n1. **Writing and Clarity**: The paper is poorly written, with numerous grammatical errors and unclear explanations. This significantly hinders the reader's ability to understand the methodology and results.\n2. **Unsupported Claims**: Some claims in the paper are not supported by the experiments, such as the assertion that ROOTS has clearer generations than GQN, which is not evident in the provided figures.\n3. **Lack of Comparisons and Ablations**: The paper lacks proper comparisons with the latest works and does not include ablation studies to isolate the contributions of different components of the model.\n4. **Experimental Weaknesses**: The experiments are not robust enough to substantiate the claims made. There is a lack of detailed analysis on why ROOTS performs better or worse than GQN in certain scenarios.\n5. **Related Work and Citations**: The paper does not adequately position itself within the existing body of work. It lacks citations to relevant literature in 3D representation and rendering, which is crucial for a top-tier conference paper.\n6. **Complexity and Explanation**: The method is complex and not well-explained, making it difficult for readers to follow the proposed approach. The appendix and figures do not sufficiently clarify the methodology.\n\n**Author Responses and Revisions:**\nThe authors have made some revisions to address clarity issues and added more experiments. However, the revisions appear to be substantial, almost like a full rewrite, which raises concerns about the initial submission's quality. The new experiments still seem weak, and the clarity issues, although improved, were significant in the original submission.\n\n**Conclusion:**\nWhile the paper presents an interesting idea with potential contributions to the field, the current submission does not meet the standards of a top-tier conference due to its poor writing, unsupported claims, lack of thorough experimental validation, and insufficient positioning within the existing literature. The revisions, although helpful, do not fully address these critical issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" presents a method aimed at improving the exploration capabilities of Monte Carlo Tree Search (MCTS) in reinforcement learning contexts, specifically in games like tic-tac-toe. However, several critical issues have been identified by the reviewers, which I will address in detail:\n\n1. **Lack of References and Contextualization**: The paper does not cite any previous research or related work, which is a fundamental flaw in academic writing. References are crucial for situating the research within the existing body of knowledge, demonstrating awareness of prior work, and justifying the novelty and significance of the proposed method. The absence of references suggests a lack of engagement with the broader research community and undermines the credibility of the work.\n\n2. **Clarity and Comprehensibility**: All reviewers have highlighted severe issues with the writing quality. The paper is described as poorly written, with numerous grammatical errors and unclear sentences. Reviewer 2 explicitly mentions the difficulty in understanding the abstract and the paper as a whole. Clear and precise communication is essential in academic writing to convey complex ideas effectively. The inability of the reviewers to comprehend the paper indicates a significant barrier to understanding the research contributions.\n\n3. **Scientific Rigor and Justification**: Reviewer 2 points out that the paper is full of inaccurate and unjustified statements. Scientific writing requires that claims be supported by evidence, logical reasoning, and, where applicable, mathematical rigor. The lack of justification for the statements made in the paper raises concerns about the validity and reliability of the research findings.\n\n4. **Presentation and Structure**: Reviewer 3 notes that the paper contains many undefined mathematical symbols, excessive blank space, and difficult-to-follow algorithm boxes. These issues suggest a lack of attention to detail in the presentation and structure of the paper. A well-structured paper should guide the reader through the research process, clearly defining terms and symbols, and presenting algorithms in a comprehensible manner.\n\n5. **Overall Impression**: The reviewers collectively express that the paper does not meet the standards of a serious academic endeavor. It is described as a set of notes rather than a polished research paper. The significant amount of work required to bring the paper to an acceptable standard for publication indicates that it is not ready for a top-tier conference.\n\nGiven these substantial issues, the paper does not meet the standards required for publication at a top-tier conference. The lack of references, poor writing quality, insufficient scientific rigor, and inadequate presentation are critical flaws that cannot be overlooked.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Hyg53gSYPB",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space\" presents a method to defend against adversarial attacks by using a modified autoencoder combined with a GAN. The proposed method, AE-GAN+sr, aims to reduce the computational cost of DefenseGAN by using the encoder to provide a better initialization for the latent space optimization.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper proposes a novel approach by combining an autoencoder with a GAN to improve the efficiency of adversarial defense mechanisms. The idea of using the encoder to initialize the search process is interesting and potentially reduces computational costs.\n2. **Empirical Results:** The method shows reasonable performance against adversarial attacks on MNIST and Fashion-MNIST datasets, with a significant reduction in computational cost compared to DefenseGAN.\n\n**Weaknesses:**\n1. **Incremental Contribution:** As noted by Reviewer 2, the contribution is considered too incremental. The paper primarily refines an existing method (DefenseGAN) without introducing substantial new insights or techniques.\n2. **Limited Evaluation:** The experiments are limited to simple datasets (MNIST and Fashion-MNIST), which do not convincingly demonstrate the method's effectiveness on more complex datasets. This limitation is highlighted by multiple reviewers.\n3. **Writing and Clarity:** The paper suffers from poor writing quality, with numerous typos and unclear explanations. This makes it difficult to follow the methodology and understand the implementation details.\n4. **Lack of Comprehensive Comparisons:** The paper does not compare its method against state-of-the-art adversarial training defenses like PGD Adversarial Training and TRADES, which are crucial benchmarks in the field.\n5. **Theoretical Justification:** There is a lack of theoretical assurance regarding the similarity between the latent distribution and the prior distribution, which raises concerns about the robustness of the method.\n6. **Unclear Methodology:** Several reviewers found the explanation of the encoder-assisted search process unclear, particularly regarding the practical implementation and the number of gradient steps required.\n\n**Conclusion:**\nWhile the paper presents an interesting approach to reducing the computational cost of adversarial defenses, it falls short in several critical areas. The contribution is incremental, the evaluation is limited to simple datasets, and the writing quality is poor. Additionally, the lack of comparisons with state-of-the-art methods and unclear methodology further weaken the paper. For a top-tier conference, these issues are significant enough to warrant rejection.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HkgU3xBtDS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"REFINING MONTE CARLO TREE SEARCH AGENTS BY MONTE CARLO TREE SEARCH\" presents an attempt to improve exploration in reinforcement learning using Monte Carlo Tree Search (MCTS). However, based on the reviewer comments and the abstract provided, there are several critical issues that need to be addressed.\n\n1. **Writing Quality and Clarity**: All three reviewers have highlighted significant issues with the writing quality. The paper is described as poorly written, with grammatical errors and incomprehensible passages. Reviewer 2 explicitly mentions that they could not understand the abstract or the main content of the paper, which is a fundamental issue for any academic publication. Clear communication of ideas is essential, and the inability to convey the research effectively is a major drawback.\n\n2. **Lack of References**: The absence of references is a serious concern. Academic papers must situate their contributions within the existing body of knowledge. By not citing any previous research, the paper fails to acknowledge prior work and does not provide a basis for comparison or validation of its claims. This omission suggests a lack of engagement with the field and undermines the credibility of the research.\n\n3. **Scientific Rigor and Completeness**: Reviewers have noted that the paper does not represent the preliminary elements of scientific writing. There are unjustified statements, undefined mathematical symbols, and a lack of coherence in the presentation of the algorithm. Reviewer 3 mentions that the paper appears more like a set of notes rather than a complete academic paper. This indicates a lack of rigor and thoroughness in the research and its presentation.\n\n4. **Contribution and Novelty**: While the paper claims to improve MCTS by handling the trade-off between exploitation and exploration, the lack of clarity and detail makes it difficult to assess the novelty and significance of the contribution. Without a clear explanation and supporting evidence, it is challenging to determine the impact of the proposed method.\n\n5. **Experimental Validation**: The abstract mentions experiments with a small problem, but without further details, it is impossible to evaluate the robustness and generalizability of the results. A top-tier conference expects comprehensive experimental validation to support any claims of improvement over existing methods.\n\nGiven these substantial issues, the paper does not meet the standards required for publication at a top-tier conference. The authors need to significantly revise the paper, improve the writing, provide references, and ensure that the research is presented with scientific rigor and clarity.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygkpxStvr",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Weakly-Supervised Trajectory Segmentation for Learning Reusable Skills\" presents a method for segmenting trajectories into sub-skills using a weakly-supervised approach inspired by multiple instance learning (MIL). The authors aim to extract reusable skills from human demonstrations without requiring explicit segmentation or ordering of primitives. The paper is evaluated across various environments, and preliminary evidence of zero-shot transfer to new skill combinations is demonstrated.\n\n### Strengths:\n1. **Problem Relevance**: The problem of learning reusable skills from demonstrations is significant in the field of robot learning and manipulation. The approach of using weak supervision to segment trajectories is innovative and could potentially reduce the need for extensive manual labeling.\n2. **Application of MIL**: The use of MIL for trajectory segmentation is a novel application, and the exploration of different pooling techniques is a valuable contribution.\n3. **Diverse Evaluation**: The method is tested across multiple environments, including both simulated and real-world scenarios, which demonstrates the potential applicability of the approach.\n\n### Weaknesses:\n1. **Lack of Novelty**: Both Reviewer 2 and Reviewer 3 highlight that the novelty of the approach is limited. The use of log-sum-exp pooling is not new and has been applied in other domains. The paper does not introduce a fundamentally new algorithmic contribution.\n2. **Experimental Limitations**: The results are underwhelming, with validation accuracy ranging from 35-60%. The success rate of 50% in behavioral cloning tasks is low, and there is no comparison to fully-supervised baselines, which would provide a clearer understanding of the method's effectiveness.\n3. **Insufficient Theoretical Analysis**: Reviewer 3 points out the lack of theoretical analysis regarding the dataset requirements for successful skill learning. This is a critical omission, as it leaves the feasibility of the approach in more complex scenarios unaddressed.\n4. **Clarity and Detail**: Reviewers note that the experimental setup lacks clarity, with insufficient details on task setup, architectural choices, and hyper-parameters. This makes it difficult to assess the robustness and reproducibility of the results.\n5. **Baseline Comparisons**: The paper lacks comparisons with unsupervised clustering methods and other relevant baselines, such as Policy Sketches, which could provide a more comprehensive evaluation of the proposed method.\n\n### Author Responses:\nThe authors have not provided responses to address the reviewers' concerns, which leaves the issues raised unmitigated. Without addressing the lack of novelty, experimental clarity, and theoretical analysis, the paper does not meet the standards of a top-tier conference.\n\n### Conclusion:\nWhile the paper addresses an important problem and applies an interesting approach, the lack of novelty, insufficient experimental results, and absence of theoretical analysis significantly weaken its contribution. The paper does not provide enough evidence to support its claims, and the results do not demonstrate a substantial advancement over existing methods. Therefore, the paper does not meet the criteria for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "25OSRH9H0Gi",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Putting Theory to Work: From Learning Bounds to Meta-Learning Algorithms\" aims to bridge the gap between theoretical insights and practical applications in meta-learning by introducing regularization terms based on theoretical assumptions. The reviewers have provided a mix of positive and negative feedback, which I will analyze to determine the paper's suitability for a top-tier conference.\n\n### Strengths:\n1. **Motivation and Clarity**: The paper is well-motivated, aiming to connect theory with practice in meta-learning, which is a valuable contribution. The writing is clear and well-organized, making the paper accessible.\n2. **Experimental Design**: The experiments are well-designed, and the results show improvements over baseline meta-learning algorithms, indicating the potential practical utility of the proposed methods.\n\n### Weaknesses:\n1. **Novelty Concerns**: Several reviewers, particularly Reviewer 1 and Reviewer 4, express concerns about the novelty of the proposed regularization methods. They argue that the methods are similar to existing techniques like spectral normalization and weight decay, which diminishes the originality of the contribution.\n2. **Theoretical Consistency**: Reviewer 2 points out inconsistencies between the theoretical setting and the practical few-shot learning setting, such as the absence of a validation set in the theoretical framework. This raises questions about the applicability of the theoretical insights to real-world scenarios.\n3. **Empirical Validation**: Reviewer 3 and Reviewer 4 suggest that the improvements observed in the experiments might not be statistically significant, as they are within the range of standard deviations and differences in reproduced results. This casts doubt on the robustness of the findings.\n4. **Assumption Enforcement**: There is skepticism about the ability to enforce theoretical assumptions through regularization, as these assumptions are inherent to the learning problem rather than the algorithm. This could limit the generalizability of the proposed methods.\n5. **Lack of Competitors and Technical Details**: Reviewer 4 notes the lack of comparisons with recent methods and missing details on the computation of subgradients, which are crucial for understanding the implementation and effectiveness of the proposed regularization.\n\n### Conclusion:\nWhile the paper addresses an important problem and presents some promising results, the concerns about novelty, theoretical consistency, empirical validation, and technical details are significant. The proposed methods appear to lack sufficient originality and rigorous validation to meet the standards of a top-tier conference. The paper would benefit from addressing these issues, particularly by providing more robust empirical evidence, clarifying theoretical assumptions, and ensuring the novelty of the contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5i4vRgoZauw",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream\" presents an intriguing approach to reducing the number of supervised synaptic updates required to model the primate ventral stream using deep neural networks. The authors propose three strategies to achieve this reduction and demonstrate that their methods can maintain a significant portion of the BrainScore, a metric used to evaluate the similarity between artificial neural networks and the primate brain.\n\n**Strengths:**\n1. **Novelty and Relevance:** The paper addresses a critical issue in computational neuroscience, namely the biological plausibility of training deep neural networks with vast amounts of labeled data. This is a significant step towards more biologically plausible models of the visual system.\n2. **Methodological Innovation:** The introduction of weight compression and critical training methods are innovative and provide a fresh perspective on how to initialize and train neural networks with fewer updates.\n3. **Comprehensive Experiments:** The authors conduct thorough experiments, including transfer experiments to other models, which add depth to their findings.\n4. **Clarity and Writing:** The paper is well-written, and the context is clearly described, making it accessible to readers.\n\n**Weaknesses:**\n1. **Over-reliance on BrainScore:** The primary critique from reviewers is the reliance on BrainScore as a metric for biological similarity. The paper does not adequately address the limitations of this metric, which could lead to overestimating the biological significance of their results.\n2. **Assumptions and Biological Plausibility:** Some assumptions, such as the initialization method being a valid model of the infant visual system, lack a strong theoretical foundation. The biological relevance of these methods is questioned by reviewers.\n3. **Interpretation of Results:** The interpretation of achieving 80% of the BrainScore as biologically meaningful is challenged. The paper could benefit from a more cautious presentation of these results.\n4. **Lack of Theoretical Basis:** Reviewer 3 highlights the lack of a theoretical foundation for the claims and experiments, which weakens the scientific rigor of the study.\n\n**Reviewer Consensus:**\nThe reviewers are divided in their opinions. Reviewer 4 is strongly in favor of acceptance, highlighting the importance and novelty of the work. Reviewer 5 acknowledges the potential impact but raises concerns about the biological assumptions and novelty. Reviewers 1, 2, and 3 express significant concerns about the interpretation of results and the reliance on BrainScore, suggesting that the paper may not meet the high standards of a top-tier conference without addressing these issues.\n\n**Final Decision:**\nGiven the mixed reviews and the substantial concerns raised about the interpretation of results and the reliance on BrainScore, the paper does not currently meet the standards of a top-tier conference. The authors need to address these concerns, particularly by providing a more robust theoretical foundation and a clearer interpretation of the biological significance of their results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "OGg9XnKxFAH",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Training independent subnetworks for robust prediction\" presents an innovative approach to ensemble learning by using a multi-input multi-output (MIMO) configuration to train independent subnetworks within a single neural network. This method aims to improve robustness and uncertainty estimation without significantly increasing computational costs.\n\n**Strengths:**\n\n1. **Novelty and Simplicity:** The paper introduces a novel approach to ensemble learning by leveraging MIMO configurations, which allows for training multiple subnetworks within a single model. This approach is simple yet effective, as it enables robust predictions with minimal parameter increase.\n\n2. **Empirical Results:** The experimental results are compelling, showing improvements in negative log-likelihood, accuracy, and calibration error on standard datasets like CIFAR10, CIFAR100, and ImageNet. The results demonstrate that the subnetworks behave as independent networks, leading to a robust ensemble.\n\n3. **Clarity and Presentation:** The paper is well-written and easy to follow, with clear explanations of the method and detailed experimental results. The use of figures and plots enhances the understanding of the approach and its benefits.\n\n**Weaknesses:**\n\n1. **Theoretical Underpinning:** Several reviewers noted the lack of theoretical analysis or explanation for why the subnetworks behave independently. While the empirical results are strong, a theoretical understanding would strengthen the paper's contributions.\n\n2. **Originality Concerns:** There are concerns about the originality of the approach, as similar multi-branch architectures have been used in related fields. The paper could benefit from a more thorough discussion of related work to highlight its unique contributions.\n\n3. **Computational Cost Analysis:** The paper claims that the benefits of using multiple predictions can be achieved 'for free,' but there is a lack of detailed analysis on computational costs, such as MACs or FLOPS. Reviewer 4 points out that the method reduces inference delay at the cost of batch size, which could be misleading.\n\n4. **Dataset Diversity:** While the paper evaluates the method on standard datasets, it lacks experiments on more diverse datasets, such as OpenImages, which could demonstrate the method's generalizability.\n\n**Conclusion:**\n\nThe paper presents a promising approach to ensemble learning with strong empirical results and a clear presentation. However, the lack of theoretical analysis, concerns about originality, and insufficient computational cost analysis are significant drawbacks. While the method shows potential, these issues need to be addressed to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HygqFlBtPS",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improved Training of Certifiably Robust Models\" presents a novel approach to improving the training of neural networks for certified robustness against adversarial attacks. The authors propose two regularizers aimed at achieving tighter bounds in convex relaxations, which are crucial for certifying robustness.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces two new regularizers that are claimed to improve the tightness of certification bounds, which is a significant contribution to the field of adversarial robustness.\n2. **Empirical Results**: The experiments demonstrate that the proposed regularizers result in tighter certification bounds compared to non-regularized baselines, suggesting practical utility.\n3. **Relevance**: The problem addressed is important for the field of adversarial robustness, and the proposed solution could potentially benefit various applications, including NLP.\n\n### Weaknesses:\n1. **Clarity and Presentation**: Both Reviewer 1 and Reviewer 2 highlight issues with the clarity of the theoretical sections. There is confusion regarding the comparison between the relaxed and unrelaxed problems, and the assumptions made in the analysis are not clearly justified.\n2. **Theoretical Justification**: Reviewer 2 points out that the theoretical justification for why the proposed regularizers should work is not convincingly presented. The paper lacks a clear explanation of why the proposed method should lead to tighter bounds.\n3. **Experimental Limitations**: Reviewer 3 notes that the experiments are conducted on the MNIST dataset, which is not sufficient for a top-tier conference. More diverse datasets would strengthen the empirical validation.\n4. **Comparison with Baselines**: There is a lack of comparison with certain relevant baselines, as noted by Reviewer 1. Including results from other methods like those of Gowal et al. would provide better context for evaluating the improvements.\n\n### Author Responses:\nThe authors have not provided responses to address the reviewers' concerns, which leaves the issues unmitigated. This lack of response is particularly problematic for a top-tier conference where clarity, thoroughness, and rigorous validation are paramount.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes a potentially valuable solution, the issues with clarity, theoretical justification, and experimental validation are significant. For a top-tier conference, the paper needs to provide a clearer theoretical foundation, address the presentation issues, and include more comprehensive experiments on diverse datasets. Without these improvements, the paper does not meet the high standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to hierarchical reinforcement learning (HRL) for multi-agent systems, focusing on skill coordination for complex manipulation tasks. The authors propose a modular framework that first trains sub-skills for each end-effector and then coordinates these skills using a meta-policy. The paper is evaluated on tasks involving bimanual manipulation and multi-agent coordination, demonstrating superior performance over existing baselines.\n\n**Strengths:**\n\n1. **Novelty and Impact:** The paper addresses an important problem in HRL and multi-agent systems by proposing a method that leverages skill behavior diversification for coordination. This approach could have significant implications for robotics and AI, particularly in tasks requiring complex coordination.\n\n2. **Empirical Results:** The proposed method outperforms baselines in various challenging tasks, indicating its effectiveness. The inclusion of videos and code enhances the reproducibility and transparency of the research.\n\n3. **Clarity and Presentation:** Despite some noted issues, the paper is generally well-presented, making the methodology understandable to readers familiar with the field.\n\n**Weaknesses:**\n\n1. **Notation and Clarity:** Reviewer 1 points out inconsistencies in notation and a lack of clarity in the diagrams, which could hinder understanding. This is a critical issue for a top-tier conference, where clarity is paramount.\n\n2. **Variance in Performance:** The high variance in training performance, as noted by Reviewer 1, raises concerns about the robustness and reliability of the method. More seeds or additional analysis could help address this issue.\n\n3. **Discussion on Multi-Agent Methods:** Reviewer 1 suggests a deeper discussion on alternative multi-agent methods, which could strengthen the paper by situating it within the broader context of existing research.\n\n4. **Scientific Insight:** Reviewer 2 expresses a lack of scientific insight, questioning the novelty of applying temporal abstraction in a multi-agent setting. This suggests that the paper may not sufficiently differentiate itself from existing work.\n\n5. **Pre-Specified Subtasks:** Reviewer 3 criticizes the reliance on pre-specified subtasks, which limits the generalizability of the approach. This is a significant limitation for a top-tier conference, where broad applicability is often valued.\n\n6. **Technical Details:** Reviewer 2 raises concerns about the technical implementation, such as the learning of continuous distributions and the fixed execution length of skills. These are important considerations that need to be addressed for a comprehensive understanding of the method.\n\n**Conclusion:**\n\nWhile the paper presents a promising approach with strong empirical results, several critical issues need to be addressed. The lack of clarity in notation, high variance in performance, limited discussion on alternative methods, and reliance on pre-specified subtasks are significant drawbacks. Additionally, the paper does not provide sufficient scientific insight or technical clarity to meet the standards of a top-tier conference. Addressing these concerns could significantly improve the paper, but in its current form, it falls short of the required standard.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rvosiWfMoMR",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Automatic Music Production Using Generative Adversarial Networks\" presents a novel approach to automatic music accompaniment using CycleGANs and Mel-spectrograms. While the topic is intriguing and relevant, especially in the context of leveraging GANs for creative tasks, the paper has several significant shortcomings that need to be addressed before it can be considered for publication in a top-tier conference.\n\n### Strengths:\n1. **Novelty and Relevance**: The application of CycleGANs to music accompaniment is a novel approach that could potentially contribute to the field of music production and AI-generated music.\n2. **Combination of Evaluation Methods**: The use of both subjective human evaluations and objective metrics is a strength, as it acknowledges the inherent subjectivity in music evaluation.\n\n### Weaknesses:\n1. **Title and Scope**: The title is misleading as it suggests a broader scope of \"Automatic Music Production\" rather than the more specific task of \"Automatic Music Accompaniment.\" This discrepancy needs to be addressed to accurately reflect the paper's content.\n\n2. **Literature Review and Context**: The paper lacks a comprehensive review of related work, particularly in the symbolic domain of music generation. This omission limits the contextual understanding of the contribution and its novelty.\n\n3. **Methodological Details**: There is a lack of detailed explanation regarding the methodologies used, such as the specifics of the CycleGAN architecture, training procedures, and the Demucs algorithm for source separation. This lack of detail hinders reproducibility and understanding of the approach.\n\n4. **Evaluation and Results**: The evaluation lacks depth, particularly in discussing the impact of source separation errors and the choice of evaluation metrics. The paper does not provide sufficient justification for the chosen attributes and metrics, nor does it adequately address the potential biases in human evaluations.\n\n5. **Experimental Design**: The experiments are limited in scope, focusing only on specific instrument combinations. There is a need for more diverse experimental settings and comparisons with other models to strengthen the claims of the paper.\n\n6. **Reproducibility**: The paper does not provide enough information for reproducibility, such as the specific datasets used, the filtering process for selecting tracks, and the code for experiments.\n\n7. **Technical Clarity**: Several technical aspects, such as the process of converting Mel-spectrograms back to audio and the quantization of spectrograms, are not clearly explained, leading to potential misunderstandings about the implementation.\n\n### Reviewer Consensus:\nAll reviewers agree that while the paper presents an interesting idea, it requires significant revisions. The reviewers highlight the need for more comprehensive experiments, clearer methodological details, and a more accurate representation of the paper's scope and contributions.\n\n### Conclusion:\nGiven the current state of the paper, it does not meet the standards of a top-tier conference due to its lack of clarity, insufficient experimental validation, and incomplete literature review. The authors are encouraged to address these issues, expand their experiments, and provide a more detailed and accurate presentation of their work for future submissions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "6zaTwpNSsQ2",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"A Block Minifloat Representation for Training Deep Neural Networks\" presents a novel approach to training deep neural networks using a new floating-point format called Block Minifloat (BM). The paper aims to improve computational efficiency and energy consumption by using low-precision representations, specifically 4-8 bit formats, while maintaining accuracy comparable to standard floating-point representations.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new floating-point format that extends existing block floating-point concepts by sharing an exponent bias across blocks. This approach allows for reduced exponent bit usage, which is a novel contribution in the context of hardware efficiency.\n   \n2. **Hardware Implementation**: The authors provide a detailed hardware evaluation, including area and power consumption metrics, which is more comprehensive than many similar papers. The use of a Kulisch accumulator for minifloat dot products is a notable innovation that addresses practical implementation challenges.\n\n3. **Experimental Validation**: The paper includes extensive experiments on various models and datasets, demonstrating the effectiveness of the proposed BM format. The results show significant improvements in hardware efficiency without compromising accuracy.\n\n4. **Comprehensive Exploration**: The authors explore a wide range of configurations and provide insights into the trade-offs between precision, computational density, and energy consumption.\n\n### Weaknesses:\n1. **Clarity and Explanation**: Several reviewers noted that some key contributions, such as the alignment of minifloat distribution and the derivation of certain equations, are not explained in sufficient detail. This lack of clarity can hinder the reader's understanding of the paper's contributions.\n\n2. **Complexity and Overhead**: The paper introduces multiple BM formats for different stages of training, which could lead to significant hardware overhead. While the authors addressed this concern in their response, the complexity of supporting multiple formats could still be a practical limitation.\n\n3. **Presentation Issues**: There are issues with the presentation of results, such as unclear table annotations and abrupt introductions of concepts like log-BM. These issues detract from the overall readability and coherence of the paper.\n\n4. **Limited Novelty in Some Aspects**: Some reviewers pointed out that certain aspects of the paper, such as the shared-exponent bias, are not entirely novel. The paper's novelty primarily lies in its hardware implementation and efficiency improvements.\n\n### Conclusion:\nThe paper presents a significant contribution to the field of efficient neural network training through its novel floating-point format and comprehensive hardware evaluation. While there are some issues with clarity and presentation, the authors have addressed major concerns raised by the reviewers, particularly regarding hardware overhead. The paper's strengths in innovation, experimental validation, and practical impact outweigh its weaknesses.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights during training using task-specific updates of model parameters. The reviewers have provided a comprehensive analysis of the paper, highlighting both strengths and weaknesses.\n\n### Strengths:\n1. **Clarity and Presentation**: The paper is well-written and easy to follow, with a clear summary of related work. This is a positive aspect as it ensures that the methodology is accessible to readers.\n2. **Novelty**: The approach of using task-specific updates for dynamic task weighting is novel and potentially impactful in the field of multitask learning.\n3. **Empirical Evaluation**: The paper includes experiments in both computer vision and natural language understanding domains, which demonstrates the applicability of the method across different types of tasks.\n\n### Weaknesses:\n1. **Lack of Theoretical Justification**: All reviewers pointed out the absence of a theoretical foundation or formal analysis to support the proposed methodology. This is a significant drawback for a top-tier conference, where theoretical rigor is often expected.\n2. **Experimental Results**: The empirical results are not consistently superior to existing methods. The improvements are minor and often fall within the range of statistical insignificance. This raises concerns about the practical effectiveness of the proposed method.\n3. **Comparison with Baselines**: The choice of baselines is limited, and the results do not convincingly demonstrate the superiority of the proposed method over existing approaches. The reviewers suggest that the method is not sufficiently different from existing dynamic task weighting methods.\n4. **Algorithmic Clarity**: There is some confusion regarding the distinction between the proposed alpha weights and task-specific weights, which suggests that the algorithm's novelty and implementation might not be as robust as claimed.\n5. **Experimental Setup**: The choice of datasets and tasks, particularly the use of a single auxiliary task in the vision domain, does not fully exploit the potential of multitask learning setups with multiple auxiliary tasks.\n\n### Additional Concerns:\n- The reviewers have requested more statistical significance testing, ablations, and a deeper discussion of the algorithm's properties.\n- There are stylistic and presentation issues that need addressing, such as the clarity of tables and consistency in reporting results.\n\n### Conclusion:\nWhile the paper presents an interesting idea with potential, the lack of theoretical justification, insufficient empirical evidence of superiority, and limited novelty compared to existing methods are significant drawbacks. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance. The authors are encouraged to address these concerns, particularly by providing a stronger theoretical foundation, expanding the experimental setup, and clarifying the algorithmic contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" presents a novel algorithm for pruning neurons in deep ReLU networks. However, several critical issues have been identified by the reviewers, which collectively suggest that the paper is not ready for publication in a top-tier conference. Here is a detailed analysis of the paper's strengths and weaknesses:\n\n1. **Novelty and Contribution**:\n   - The paper proposes a new pruning technique specific to ReLU networks, leveraging activation patterns and hyperplane arrangements. However, the novelty of the approach is questioned by multiple reviewers, as similar ideas have been explored in existing literature on network pruning.\n   - The paper heavily relies on unpublished work, which is not adequately detailed or accessible, making it difficult to assess the validity and originality of the proposed method.\n\n2. **Clarity and Presentation**:\n   - The paper suffers from significant clarity and presentation issues. Important concepts are explained in footnotes rather than the main text, and many definitions are missing or unclear.\n   - The paper contains numerous typos and grammatical errors, which detract from its readability and professionalism.\n\n3. **Experimental Validation**:\n   - The experimental evaluation is insufficient. The authors only conduct experiments on toy datasets and MNIST, which are not complex enough to demonstrate the effectiveness of the pruning technique.\n   - There is a lack of comparison with existing state-of-the-art pruning methods, both theoretically and experimentally. This makes it difficult to assess the relative performance and advantages of the proposed method.\n\n4. **Theoretical Foundation**:\n   - The theoretical foundation of the paper is weak. The connection to the unpublished work is not well-explained, and the derivation of key equations is either missing or unclear.\n   - The paper does not provide rigorous proofs or theorems for its main claims, which is a significant omission for a top-tier conference.\n\n5. **Reviewers' Consensus**:\n   - All reviewers have expressed concerns about the paper's readiness for publication. The issues range from lack of clarity and insufficient experimental validation to reliance on unpublished work and lack of novelty.\n\nIn conclusion, while the paper introduces an interesting idea for neuron pruning in ReLU networks, the numerous issues identified by the reviewers indicate that it is not yet suitable for publication in a top-tier conference. The authors need to address the clarity and presentation issues, provide a more rigorous theoretical foundation, and conduct more comprehensive experiments with comparisons to existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights using task-specific updates of model parameters. The reviewers have provided a comprehensive analysis of the paper, highlighting both strengths and weaknesses.\n\n### Strengths:\n1. **Clarity and Writing**: The paper is well-written and easy to follow, with a clear summary of related work. This is crucial for understanding the proposed methodology and its context within the existing literature.\n2. **Novelty**: The approach of using model updates for task weight estimation is novel, as noted by the authors and reviewers. This could potentially offer a new perspective in the field of multitask learning.\n3. **Empirical Evaluation**: The paper includes experiments in both computer vision and natural language understanding domains, which demonstrates the applicability of the method across different types of tasks.\n\n### Weaknesses:\n1. **Lack of Theoretical Justification**: Reviewers have pointed out the absence of a theoretical foundation or formal analysis for the proposed method. This is a significant drawback, especially for a top-tier conference, where theoretical rigor is often expected.\n2. **Experimental Results**: The experimental results are not consistently superior to existing methods. The improvements are minor and often fall within the range of statistical insignificance. This raises questions about the practical effectiveness of the proposed method.\n3. **Comparison with Baselines**: The choice of baselines is somewhat limited, and the results do not convincingly demonstrate the superiority of the proposed method over existing approaches. The reviewers suggest that the method is not significantly different from existing dynamic task weighting methods.\n4. **Algorithmic Clarity**: There are concerns about the clarity and necessity of certain components of the algorithm, such as the distinction between alpha and task-specific weights. This indicates a need for further refinement and explanation of the methodology.\n5. **Experimental Setup**: The choice of datasets and tasks for evaluation is questioned, particularly the use of a single auxiliary task in the vision domain, which does not fully exploit the potential of multitask learning.\n\n### Additional Considerations:\n- The reviewers have suggested several improvements, including more thorough discussions, additional experiments, and ablations, which could strengthen the paper.\n- The lack of statistical significance in the results and the absence of a clear advantage over existing methods are critical issues that need to be addressed.\n\n### Conclusion:\nWhile the paper presents an interesting idea with potential, the lack of theoretical justification, the inconclusive experimental results, and the limited novelty compared to existing methods are significant drawbacks. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance. The authors are encouraged to address these concerns, particularly by providing a stronger theoretical foundation and more convincing empirical evidence.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JzG0n48hRf",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper addresses an important problem in machine learning: the calibration of model predictions on out-of-distribution (OOD) data. The authors propose a post-hoc calibration method that uses outlier exposure to improve calibration on corrupted datasets. The reviewers have provided a mix of positive and negative feedback, which I will analyze in detail.\n\n### Strengths:\n1. **Importance of Problem**: The problem of overconfident predictions on OOD data is significant, especially in safety-critical applications.\n2. **Novelty**: The proposed method of using corrupted data for calibration is a novel approach compared to traditional methods that assume i.i.d. validation data.\n3. **Simplicity and Applicability**: The method is simple and can be applied post-hoc to various models, making it versatile.\n4. **Empirical Results**: The method shows improvements over existing techniques on benchmark datasets like CIFAR-10 and ImageNet, as measured by metrics like ECE.\n\n### Weaknesses:\n1. **Assumptions on Known Distributions**: The method assumes that the possible novel distributions are known, which is a significant limitation. This assumption simplifies the problem and reduces its applicability to real-world scenarios where distribution shifts are unknown.\n2. **Lack of Comprehensive Evaluation**: The paper primarily evaluates the method on artificially corrupted datasets. There is a lack of evaluation on more realistic OOD scenarios, such as using completely different datasets (e.g., SVHN for a model trained on CIFAR-10).\n3. **Missing Related Work**: The paper does not adequately cite recent related work, which is a critical oversight in situating the contribution within the existing literature.\n4. **Clarity and Presentation**: Several reviewers noted issues with the clarity of the paper, including undefined terms and unclear methodology descriptions. This affects the paper's accessibility and reproducibility.\n5. **Limited Theoretical Insight**: The paper lacks a theoretical explanation for why the method works well across different types of corruptions, which would strengthen the contribution.\n\n### Reviewer Consensus:\n- Reviewers acknowledge the importance of the problem and the potential of the proposed method.\n- There is a consensus that the paper lacks comprehensive evaluation and theoretical grounding.\n- The clarity and presentation of the paper need significant improvement.\n\n### Author Responses:\nThe authors provided additional experiments and clarifications in response to reviewer comments. However, the fundamental concerns about assumptions, evaluation breadth, and clarity remain largely unaddressed.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes a novel approach, the limitations in assumptions, evaluation, and clarity are significant. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for acceptance. The authors are encouraged to address these concerns, particularly by expanding the evaluation to more realistic OOD scenarios, improving the clarity of the presentation, and providing a more comprehensive theoretical analysis.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "w6Vm1Vob0-X",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Global Node Attentions via Adaptive Spectral Filters\" presents a novel approach to improving Graph Neural Networks (GNNs) by introducing a global self-attention mechanism using learnable spectral filters. The authors aim to address the limitations of traditional GNNs, which often assume local homophily, by allowing the model to attend to nodes regardless of their distance.\n\n**Strengths:**\n\n1. **Problem Relevance:** The paper addresses an important problem in the field of GNNs, specifically their limitations on disassortative graphs. This is a well-recognized issue and tackling it could have significant implications for the field.\n\n2. **Theoretical Motivation:** The use of graph wavelets and spectral filters provides a solid theoretical foundation for the proposed method. This is acknowledged by Reviewer 3, who appreciates the theoretical motivation.\n\n3. **Empirical Evaluation:** The authors conduct experiments on multiple datasets, comparing their method against several state-of-the-art baselines. The results show that the proposed model performs well on both assortative and disassortative graphs.\n\n**Weaknesses:**\n\n1. **Novelty Concerns:** Both Reviewer 1 and Reviewer 4 express concerns about the novelty of the approach. They argue that the idea of using attention to capture features from distant nodes is not new and that similar concepts have been explored in previous works like ChevNet and Non-Local Graph Neural Networks.\n\n2. **Complexity Issues:** A significant concern raised by multiple reviewers (1 and 4) is the computational complexity of the proposed method. The need to compute a full matrix eigen decomposition and the global attention mechanism's scalability are questioned. The authors' claims about complexity reduction are not convincingly supported, and this could limit the method's applicability to larger graphs.\n\n3. **Limited Experimental Scope:** The datasets used for evaluating disassortative graphs are relatively small, which raises questions about the method's scalability and effectiveness on larger, real-world graphs. Reviewer 3 suggests additional experiments on synthetic graphs to better understand the method's performance across different graph types.\n\n4. **Lack of Theoretical Results:** Reviewer 2 notes the absence of theoretical results, which could strengthen the paper's contributions. While the empirical results are promising, theoretical guarantees or insights would enhance the paper's impact.\n\n**Conclusion:**\n\nWhile the paper addresses a relevant problem and presents a method with a solid theoretical basis, the concerns about novelty, computational complexity, and limited experimental validation on larger datasets are significant. The reviewers' feedback indicates that the paper does not meet the high standards required for a top-tier conference, particularly given the unresolved issues around scalability and the lack of clear differentiation from existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rklj3gBYvH",
      "source": "ICLR 2020",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"NORML: Nodal Optimization for Recurrent Meta-Learning\" presents a novel approach to meta-learning using an LSTM-based meta-learner for neuron-wise optimization. While the topic is relevant and addresses scalability issues in meta-learning, several critical issues have been identified by the reviewers that need to be considered in the evaluation.\n\n1. **Novelty and Contribution**: \n   - The paper proposes a variation of the Meta-Learner LSTM by Ravi & Larochelle, but the novelty is considered limited. The proposed method is seen as an incremental improvement rather than a groundbreaking innovation.\n   - The paper claims to address scalability issues, but the reviewers point out that similar parameter-sharing techniques have been used in previous works, questioning the novelty of the approach.\n\n2. **Experimental Validation**:\n   - The experiments lack a direct comparison with the Meta-Learner LSTM under the same conditions, which is crucial to validate the claimed improvements.\n   - Reviewer 3 highlights the absence of appropriate baselines, such as a version of MAML with learned learning rates per hidden unit and per inner loop step, and MAML++.\n   - The experiments are limited to fully connected networks, which restricts the applicability of the method, as most few-shot learning tasks involve convolutional networks.\n\n3. **Writing and Presentation**:\n   - Reviewer 2 criticizes the writing quality, noting numerous grammatical errors and poor flow of ideas. This detracts from the clarity and professionalism expected at a top-tier conference.\n   - There are inaccuracies in the description of prior work, which undermines the credibility of the paper's claims.\n\n4. **Theoretical and Technical Depth**:\n   - The theoretical contribution is considered minimal, and the paper lacks a rigorous theoretical analysis to support its claims.\n   - Reviewer 1 questions the validity of certain claims, such as the ability of the LSTM meta-learner to handle a large number of inner-loop updates without issues.\n\n5. **Overall Impact**:\n   - The paper does not convincingly demonstrate a significant advancement over existing methods. The lack of comprehensive experimental validation and the limited novelty reduce its potential impact.\n\nGiven these considerations, the paper does not meet the high standards required for acceptance at a top-tier conference. The issues with novelty, experimental validation, writing quality, and theoretical depth are significant and would require substantial revisions to address.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SJxpsxrYPS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS\" presents a novel approach to learning hierarchical representations using a progressive learning strategy in the context of Variational Autoencoders (VAEs). The authors propose a method that incrementally activates layers in a ladder architecture, aiming to improve disentanglement of representations.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a fresh idea of combining progressive learning with hierarchical representation learning in VAEs, which is not extensively explored in the literature. This approach is innovative and has the potential to open new avenues for research in disentangled representation learning.\n\n2. **Experimental Validation**: The authors provide both qualitative and quantitative evidence supporting their claims. They conduct experiments on benchmark datasets and introduce a new metric (MIG-sup) to evaluate disentanglement, which adds value to the paper.\n\n3. **Rebuttal Improvements**: The authors have addressed several concerns raised by the reviewers in their rebuttal. They provided additional experiments that demonstrate the effectiveness of their method compared to the baseline VLAE, particularly in terms of disentanglement and information flow.\n\n4. **Clarity and Presentation**: The paper is generally well-written and easy to follow. The motivation for the work is clear, and the results are presented in a convincing manner.\n\n### Weaknesses:\n1. **Lack of Clarity in Purpose**: Reviewer 2 pointed out that the paper lacks clarity in distinguishing between the goals of learning hierarchical versus disentangled representations. This confusion could be addressed by better articulating the relationship between these two concepts.\n\n2. **Comparative Analysis**: While the authors compare their method to VLAE, some reviewers suggest that a more thorough comparison, including ablation studies, would strengthen the paper. Specifically, understanding the individual contributions of progressive learning and other implementation strategies would be beneficial.\n\n3. **Metric Limitations**: The proposed MIG-sup metric, while innovative, requires ground truth for generative factors, which limits its applicability in real-world scenarios where such ground truth is not available.\n\n4. **Reproducibility Concerns**: Some details, such as the definition of certain variables in equations, are missing, which could hinder reproducibility. This needs to be addressed to ensure that other researchers can replicate the results.\n\n### Conclusion:\nThe paper presents a promising approach to disentangled representation learning using progressive learning strategies. Despite some weaknesses, particularly in clarity and comparative analysis, the authors have made significant improvements in response to reviewer feedback. The novelty of the approach, combined with the additional experiments provided in the rebuttal, suggests that the paper has the potential to make a meaningful contribution to the field.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "9GUTgHZgKCH",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Reducing the number of neurons of Deep ReLU Networks based on the current theory of Regularization\" proposes a novel algorithm for pruning neurons in deep ReLU networks. However, several critical issues have been identified by the reviewers, which collectively suggest that the paper is not ready for publication in a top-tier conference. Here is a detailed analysis of the paper's strengths and weaknesses:\n\n1. **Novelty and Contribution**:\n   - The paper introduces a new reduction algorithm for ReLU networks, which is a relevant and timely topic given the ongoing interest in model compression and efficiency.\n   - However, the novelty of the approach is questioned by multiple reviewers, as the techniques of removing inactive neurons and combining always-active neurons are not particularly novel. These ideas have been explored in existing literature on network pruning.\n\n2. **Clarity and Presentation**:\n   - The paper suffers from significant clarity and presentation issues. Important concepts are explained in footnotes rather than the main text, and many definitions are missing or unclear.\n   - The paper is poorly organized, making it difficult to follow. There are numerous typos and grammatical errors, which further detract from the readability and professionalism expected at a top-tier conference.\n\n3. **Theoretical Foundation**:\n   - The paper heavily relies on unpublished work by the authors, which is not adequately detailed or accessible for verification. This reliance on unpublished work undermines the credibility and verifiability of the proposed method.\n   - The theoretical contributions are not presented rigorously, and key results are not framed as theorems or supported by proofs.\n\n4. **Experimental Validation**:\n   - The experimental evaluation is insufficient. The experiments are limited to toy datasets and MNIST, which are not challenging enough to demonstrate the effectiveness of the pruning technique.\n   - There is a lack of comparison with existing state-of-the-art pruning methods, which is crucial to establish the competitiveness of the proposed approach.\n   - The experiments do not provide statistical significance measures, such as standard errors, to support the claims of performance preservation.\n\n5. **Reviewers' Consensus**:\n   - All reviewers express concerns about the paper's readiness for publication. They highlight issues with clarity, insufficient experimental validation, and reliance on unpublished work.\n   - The reviewers recommend rejection due to the inability to verify the correctness of the technique and the lack of convincing experimental results.\n\nIn conclusion, while the paper addresses an important topic, the combination of unclear presentation, insufficient theoretical and experimental validation, and reliance on unpublished work makes it unsuitable for acceptance at a top-tier conference at this time. The authors are encouraged to address these issues, improve the clarity and rigor of their presentation, and provide more comprehensive experimental validation in future submissions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Qpik5XBv_1-",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions\" presents an approach to integrate linguistic and visual features in both top-down and bottom-up modulation of visual input. The authors argue that using language throughout the bottom-up visual pathway, from pixels to high-level features, is necessary for improved performance in multimodal tasks.\n\n**Strengths:**\n1. **Novelty and Approach:** The paper proposes a novel approach by integrating language throughout the visual hierarchy, which is a departure from the traditional top-down approach. This is an important contribution to the field of language-vision problems.\n2. **Experimental Results:** The paper demonstrates that the proposed model achieves state-of-the-art or competitive results on several datasets. The qualitative examples provided are impressive and demonstrate the success of the approach.\n3. **Writing and Clarity:** The paper is generally well-written and easy to follow, with a good literature review.\n\n**Weaknesses:**\n1. **Empirical Results:** There are concerns about the empirical results, particularly the model's performance on certain metrics and datasets. The results on the test set are not as strong as on the validation set, indicating potential overfitting.\n2. **Conceptual Motivation:** The paper lacks a clear conceptual motivation for the model. There is insufficient discussion on why language should be integrated into early-stage visual representations and what specific benefits this integration provides.\n3. **Error Analysis and Understanding:** The paper does not provide a satisfactory analysis of the model's success. There is a lack of understanding of the actual effect of linguistic representations within the model, and no statistical significance tests are provided.\n4. **Reviewer Concerns:** Reviewer 2 and 4 have raised significant concerns about the novelty, empirical results, and conceptual contribution of the paper. Reviewer 4, in particular, emphasizes the need for a clear conceptual motivation and better model analysis.\n\n**Post-Rebuttal Considerations:**\nThe authors' rebuttals did not adequately address the concerns raised by the reviewers. The lack of a critical baseline reference (Mei et al., 2018) and the absence of significance tests further weaken the paper's contributions. The authors' responses did not provide sufficient clarity or additional insights into the model's novelty or empirical results.\n\n**Conclusion:**\nWhile the paper presents an interesting approach and shows some promising results, the concerns about empirical validation, conceptual motivation, and the lack of thorough analysis are significant. The paper does not meet the high standards required for a top-tier conference, particularly in terms of providing a strong conceptual contribution and robust empirical validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "eOdSD0B5TE",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"On the Implicit Biases of Architecture & Gradient Descent\" explores the interplay between neural network architecture and gradient descent in the context of generalization. It presents theoretical results and empirical findings, focusing on the implicit biases introduced by both architecture and optimization.\n\n**Strengths:**\n\n1. **Theoretical Contributions:** The paper provides new theoretical insights into the average test error of neural network-Gaussian process (NNGP) posteriors, which is a novel contribution. The development of new technical tools to analytically bound and estimate this error is a significant theoretical advancement.\n\n2. **Interesting Topic:** The paper addresses an important question in deep learning regarding the roles of architecture and gradient descent in generalization, which is a topic of active research and interest.\n\n3. **Clarity and Writing:** The paper is generally well-written and presents its ideas clearly, making it accessible to readers.\n\n**Weaknesses:**\n\n1. **Experimental Limitations:** Multiple reviewers pointed out that the experimental validation is lacking. The experiments are not comprehensive enough to support the claims, particularly regarding the implicit bias of gradient descent. The use of the \"Nero\" optimizer instead of standard GD or SGD raises concerns about the generality of the findings.\n\n2. **Incremental Contributions:** Some reviewers noted that the contributions, particularly the PAC-Bayes bound, are incremental and build heavily on previous work (e.g., Valle-Prez et al. (2019)). The improvements over existing results are not substantial enough for a top-tier conference.\n\n3. **Lack of Rigorous Margin Analysis:** The discussion on margin and its role in generalization is not rigorous enough. Reviewers found the claims about gradient descent's implicit bias related to margin unconvincing and lacking theoretical support.\n\n4. **Limited Applicability:** The theoretical results are primarily applicable to MLPs and do not extend to more complex architectures like CNNs or ResNets, which limits the practical significance of the findings.\n\n5. **Overpromise and Under-delivery:** The paper promises a comprehensive analysis of both architectural and optimization biases but falls short in delivering a balanced and thorough investigation of both aspects.\n\n**Reviewer Consensus:**\n\nThe majority of reviewers lean towards rejection, citing the paper's premature nature, lack of comprehensive experiments, and incremental contributions. While some reviewers acknowledge the potential of the theoretical results, they agree that the paper requires significant improvements in framing, experimental validation, and clarity of claims.\n\n**Author Responses:**\n\nThe authors provided clarifications and addressed some misunderstandings, but the core concerns about experimental validation and the rigor of the margin analysis remain unresolved. The responses did not sufficiently convince the reviewers to change their overall assessment.\n\n**Conclusion:**\n\nGiven the paper's current state, it does not meet the high standards required for a top-tier conference. The theoretical contributions, while interesting, are not groundbreaking enough, and the experimental evidence is insufficient to support the claims. The paper would benefit from a more focused approach, improved experiments, and a clearer presentation of its contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "SgEhFeRyzEZ",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks\" presents a theoretical analysis of the Feedback Alignment (FA) algorithm, focusing on its convergence properties and implicit regularization effects in deep linear networks. The paper is well-written and provides a clear structure, with all claims and statements being well-supported and correct. However, the decision to accept or reject this paper for a top-tier conference requires a careful evaluation of its novelty, significance, and potential impact.\n\n### Strengths:\n1. **Theoretical Contributions**: The paper provides a rigorous theoretical analysis of the FA algorithm, offering convergence guarantees for both continuous and discrete dynamics in deep linear networks. This is a valuable contribution to the understanding of FA, which is an alternative to backpropagation.\n   \n2. **Novel Observations**: The paper identifies an interesting phenomenon of implicit anti-regularization, where certain initializations lead to learning less important components before more important ones. This is contrasted with implicit regularization, where components are learned in decreasing order of importance.\n\n3. **Clarity and Structure**: The paper is well-structured and clearly written, making it accessible to readers interested in the theoretical aspects of neural network training algorithms.\n\n4. **Numerical Experiments**: Although limited, the numerical experiments provided support the theoretical findings and illustrate the behavior of FA compared to gradient descent (GD).\n\n### Weaknesses:\n1. **Limited Scope and Generalizability**: The analysis is primarily limited to linear networks with diagonalizable matrices and specific initializations (spectral initialization). This restricts the applicability of the results to more general settings, particularly non-linear networks, which are more common in practice.\n\n2. **Practical Relevance**: The conditions under which the theoretical results hold (e.g., spectral initialization) are not commonly used in practice. This raises concerns about the practical significance of the findings, as the results may not translate to real-world scenarios where random initialization is prevalent.\n\n3. **Lack of Empirical Validation**: The paper lacks extensive empirical validation under more realistic settings, such as non-spectral initialization. This limits the ability to assess whether the observed phenomena (e.g., implicit anti-regularization) occur in practical applications.\n\n4. **Novelty and Impact**: While the theoretical contributions are interesting, the novelty and impact are somewhat limited by the restrictive assumptions and the focus on linear networks. The gap between the theoretical analysis and practical applications remains significant.\n\n### Reviewer Consensus:\nThe reviewers generally acknowledge the correctness and clarity of the paper but express concerns about the significance and novelty of the contributions. The restrictive assumptions and limited empirical validation are common points of criticism. Reviewer 3 is more positive, highlighting the potential interest to the community, but the other reviewers are more cautious, emphasizing the need for broader applicability and practical relevance.\n\n### Conclusion:\nGiven the strengths and weaknesses outlined above, the paper presents a solid theoretical contribution but falls short in terms of practical relevance and generalizability. The restrictive assumptions and limited empirical validation under realistic settings are significant drawbacks for a top-tier conference. While the paper is well-executed in its theoretical scope, the novelty and impact are not sufficient to warrant acceptance at a top-tier conference without addressing these limitations.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "CES-KyrKcTM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"The weighted mean trick  optimization strategies for robustness\" presents an interesting approach to optimizing higher-order moments of the loss distribution by minimizing a weighted mean. The authors claim that this method can improve robustness against outliers and provide a stronger theoretical foundation compared to other robust loss functions.\n\n**Strengths:**\n1. **Theoretical Insight:** The paper provides a theoretical framework for using weighted means to optimize higher-order moments, which could be valuable for robust learning algorithms.\n2. **Potential Applications:** The approach could have implications for various robust learning tasks, as noted by Reviewer 7.\n3. **Empirical Validation:** The paper includes experimental results showing that the proposed method performs similarly to other robust loss functions on noisy datasets.\n\n**Weaknesses:**\n1. **Lack of Theoretical Rigor:** Several reviewers, particularly Reviewers 1, 2, 4, and 5, point out that the theoretical contributions are limited and not well-supported. The choice of weights and their effects are not clearly justified, and the paper lacks a rigorous theoretical foundation for some claims.\n2. **Algorithmic Concerns:** Reviewers 2, 3, and 5 highlight issues with the proposed algorithm, particularly that it does not seem to minimize the intended objective due to missing terms in the gradient update.\n3. **Clarity and Coherence:** The paper suffers from clarity issues, as noted by multiple reviewers. The notation is inconsistent, and the connection between different sections is not well-explained.\n4. **Novelty and Significance:** The technical novelty is questioned by several reviewers, who feel that the contributions are marginal and that similar ideas have been explored in prior work.\n5. **Practical Concerns:** The paper does not adequately address practical aspects such as the choice of weights, convergence guarantees, and the impact of clipping negative weights.\n\n**Reviewer Consensus:**\nThe majority of reviewers express concerns about the theoretical contributions, clarity, and practical implementation of the proposed method. While Reviewer 7 is more positive, they also acknowledge that the theorems are straightforward and that the paper lacks applications to existing robust estimation problems.\n\n**Conclusion:**\nGiven the significant concerns raised by the majority of reviewers regarding the theoretical rigor, clarity, and practical implementation of the proposed method, the paper does not meet the standards of a top-tier conference. The contributions, while potentially interesting, are not sufficiently novel or well-supported to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HNA0kUAFdbv",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CANVASEMB: Learning Layout Representation with Large-scale Pre-training for Graphic Design\" presents an interesting approach to layout representation learning using a Transformer-based model, CanvasEmb, and introduces a large-scale dataset of over one million slides. The paper aims to leverage self-supervised pre-training to improve performance on downstream tasks such as element role labeling and image captioning.\n\n**Strengths:**\n\n1. **Novelty and Contribution**: The paper introduces a novel application of Transformer-based models to layout representation, which is a relatively unexplored area. The creation of a large-scale dataset is a significant contribution to the field, addressing the scarcity of data in layout learning.\n\n2. **Potential Impact**: The approach has the potential to advance the field of graphic design intelligence by providing a pre-trained model that can be fine-tuned for various tasks with limited data.\n\n3. **Writing and Clarity**: The paper is generally well-written and easy to follow, with a clear explanation of the proposed method.\n\n**Weaknesses:**\n\n1. **Evaluation and Baselines**: The evaluation is a major weakness. The paper lacks a thorough comparison with existing methods, particularly with relevant works such as Cao et al., SIGGRAPH 2019, and other state-of-the-art models like Graph Neural Networks. The choice of baselines is limited and does not convincingly demonstrate the superiority of CanvasEmb.\n\n2. **Methodological Details**: There is insufficient detail on the Transformer model used, the dataset construction, and the pre-training process. This lack of detail hinders reproducibility and understanding of the model's capabilities.\n\n3. **Motivation and Justification**: The motivation for using a Transformer-based model over other architectures, such as Graph Neural Networks, is not well justified. The paper does not convincingly argue why Transformers are the best choice for this task.\n\n4. **Dataset and Task Details**: Important details about the dataset, such as the number of elements per slide and the parsing process, are missing. This information is crucial for understanding the dataset's richness and the model's applicability.\n\n5. **Task Complexity**: The tasks used for evaluation are relatively simple, and the paper does not explore more complex tasks that could better demonstrate the model's capabilities.\n\n6. **Related Work**: The related work section is lacking in depth and does not adequately differentiate the proposed method from existing approaches.\n\n**Conclusion:**\n\nWhile the paper presents an interesting approach and makes a valuable contribution with the dataset, the weaknesses in evaluation, methodological details, and justification for the chosen approach are significant. The lack of thorough comparison with existing methods and the insufficient exploration of complex tasks limit the paper's impact and significance. For a top-tier conference, these issues are critical and need to be addressed to meet the high standards expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "5ECQL05ub0J",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum\" presents an investigation into the behavior of stochastic gradient descent with momentum (SGDm) under non-iid sampling conditions, specifically focusing on covariate shift. The paper identifies a resonance phenomenon that can lead to divergence in SGDm, which is an interesting and potentially impactful observation for fields like continual learning and reinforcement learning.\n\n**Strengths:**\n\n1. **Relevance and Importance:** The paper addresses an important problem in optimization under non-iid conditions, which is relevant for many practical machine learning scenarios.\n\n2. **Theoretical and Empirical Analysis:** The paper provides both theoretical insights and empirical validation. It uses a combination of mathematical modeling and experiments to explore the resonance phenomenon.\n\n3. **Clarity and Structure:** The paper is generally well-written and structured, making it accessible to readers.\n\n4. **Novelty in Application:** While the mathematical techniques may not be novel, the application of these techniques to the problem of SGDm under covariate shift is a novel contribution.\n\n**Weaknesses:**\n\n1. **Technical Novelty:** Several reviewers, particularly Reviewer 2 and Reviewer 3, question the technical novelty of the paper. They argue that the paper relies heavily on existing mathematical results and does not provide significant new theoretical contributions.\n\n2. **Practical Implications:** Reviewer 5 and Reviewer 6 express concerns about the practical implications of the findings. The resonance phenomenon might occur under very specific conditions that are not commonly encountered in practice, limiting the broader applicability of the results.\n\n3. **Presentation and Explanation:** Reviewer 3 points out that the presentation of the theoretical results, particularly Theorem 1, could be improved to better explain the resonance behavior and its implications.\n\n4. **Empirical Validation:** While the paper includes empirical results, Reviewer 6 suggests that the paper lacks an empirical dataset that clearly demonstrates the practical relevance of the resonance phenomenon.\n\n5. **Analysis of the \"Right\" Algorithm:** Reviewer 3 questions whether the paper analyzes the correct version of SGDm, as the theoretical results are based on expected gradients, which may not fully capture the stochastic nature of SGDm.\n\n**Conclusion:**\n\nThe paper presents an interesting and potentially impactful observation about the behavior of SGDm under covariate shift. However, the concerns about the technical novelty, practical relevance, and presentation of the results are significant. While the paper is well-written and the empirical results are valuable, the lack of strong theoretical contributions and clear practical implications make it difficult to justify acceptance at a top-tier conference. The paper would benefit from addressing these concerns, particularly by providing more insights into the practical scenarios where the resonance phenomenon is likely to occur and by enhancing the theoretical contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mxfRhLgLg_",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Deep Ecological Inference\" presents an approach to ecological inference using deep learning techniques, specifically focusing on the Maryland 2018 midterm elections. The paper attempts to bridge the gap between ecological inference and deep learning, which is a valuable endeavor given the importance of ecological inference in political science and its potential applications in real-world scenarios.\n\n**Strengths:**\n1. **Relevance and Importance:** The paper addresses an important problem in political science, which is inferring individual-level voting behavior from aggregate data. This is particularly relevant for understanding voting patterns and has implications for gerrymandering and redistricting.\n2. **Novelty in Application:** While the approach itself may not be entirely novel, applying deep learning techniques to ecological inference is a relatively new direction that could provide valuable insights and improvements over traditional methods.\n3. **Real-world Data:** The use of real-world data from the Maryland 2018 midterm elections adds practical value to the research, demonstrating the applicability of the proposed methods.\n\n**Weaknesses:**\n1. **Clarity and Organization:** Multiple reviewers noted that the paper is difficult to follow due to its structure and numerous typos. Key concepts and methodologies are not introduced clearly, making it challenging for readers to grasp the contributions and significance of the work.\n2. **Experimental Rigor:** The experiments lack key details and comparisons to standard baselines, which undermines the credibility of the results. The absence of a comparison with a standard multi-level model (MLM) is a significant oversight, as it prevents a clear understanding of the improvements offered by the proposed deep MLM.\n3. **Incomplete Analysis:** The analysis of the results is insufficient, with several claims lacking quantitative backing. The paper would benefit from a deeper exploration of why certain models perform better and how the proposed methods improve upon existing techniques.\n4. **Reproducibility:** The lack of detailed experimental settings and hyperparameter choices makes it difficult to reproduce the results, which is a critical aspect of research publication.\n5. **Literature Review:** The paper does not adequately situate itself within the existing body of work. Important related work in learning with label proportions and other related fields is not cited or discussed, which limits the understanding of the paper's contributions relative to existing methods.\n\n**Conclusion:**\nWhile the paper addresses an important problem and attempts to apply deep learning in a novel context, the execution falls short of the standards required for a top-tier conference. The issues with clarity, experimental rigor, and literature review are significant and need to be addressed before the paper can be considered for publication. The paper requires substantial revisions to improve its clarity, provide a more comprehensive experimental analysis, and situate itself within the existing literature.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "fuYtttFI-By",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Programmable 3D snapshot microscopy with Fourier convolutional networks\" presents an interesting application of Fourier convolutional neural networks (CNNs) to the field of computational microscopy. The authors propose a novel approach to optimize both the optical encoder and the deep learning decoder for 3D snapshot microscopy, leveraging the global convolution capabilities of Fourier CNNs to handle the non-local point spread functions (PSFs) required for this task.\n\n**Strengths:**\n1. **Application Relevance:** The paper addresses a significant challenge in 3D snapshot microscopy, which has important biological applications.\n2. **Methodological Approach:** The use of Fourier CNNs to achieve global context in a computationally efficient manner is well-motivated and clearly explained.\n3. **Experimental Results:** The paper demonstrates promising results in simulations, showing that the proposed method outperforms existing UNet architectures and other state-of-the-art methods in certain tasks.\n4. **Clarity and Writing:** The paper is well-written, with clear explanations of the methodology and results.\n\n**Weaknesses:**\n1. **Technical Novelty:** Several reviewers noted that the technical novelty is limited, as the main ideas of using Fourier domain operations have been explored in prior work, such as the Rippel et al. paper. The paper does not sufficiently differentiate its contributions from these existing works.\n2. **Empirical Validation:** The evaluation is primarily based on simulated data, with no real-world experiments. This raises concerns about the applicability and robustness of the method in practical scenarios, especially given the potential domain gap between simulations and real-world data.\n3. **Comparative Analysis:** There is a lack of detailed comparisons with closely related works, particularly in terms of demonstrating specific improvements or innovations over previous approaches.\n4. **Resource and Performance Details:** Some reviewers pointed out the need for more detailed information on resource requirements and performance metrics, such as memory usage and training times, to ensure fair comparisons with other methods.\n\n**Reviewer Consensus:**\nThe reviewers generally acknowledge the quality of the work and its potential impact on computational imaging. However, they express concerns about the limited technical novelty and the lack of real-world validation. Some reviewers suggest that the paper might be better suited for an application-specific conference or journal rather than a top-tier machine learning conference, given its focus on application rather than methodological innovation.\n\n**Conclusion:**\nWhile the paper presents a well-executed study with promising results, the concerns about technical novelty and the absence of real-world validation are significant drawbacks for acceptance at a top-tier conference. The paper might benefit from further development, including real-world experiments and a more thorough comparison with existing methods, to strengthen its contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "BIIwfP55pp",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning\" presents a novel approach to combining meta-reinforcement learning (meta-RL) and imitation learning (IL) to improve adaptation rates in sparse reward environments. The reviewers have provided a comprehensive analysis of the paper, highlighting both strengths and weaknesses.\n\n### Strengths:\n1. **Novelty and Relevance**: The paper addresses a significant problem in reinforcement learning, particularly the challenge of adapting to new tasks with sparse rewards by leveraging demonstrations. This is a relevant and timely topic in the field.\n2. **Empirical Results**: The proposed method, PERIL, shows promising empirical results, outperforming existing baselines like PEARL and MetaIL in certain benchmarks.\n3. **Visualization and Analysis**: The paper provides a detailed analysis of the latent space, which helps illustrate the learning process and the effectiveness of the proposed method.\n\n### Weaknesses:\n1. **Complexity and Clarity**: The method is complex, involving multiple components and loss terms, which are not clearly explained or justified. This complexity makes it difficult for readers to understand the necessity and function of each part.\n2. **Notation and Mathematical Formulation**: There are inconsistencies and unclear explanations in the mathematical formulation, which add to the confusion. Key terms and equations are not well-defined or explained.\n3. **Lack of Baselines and Ablations**: The paper lacks comparisons with other methods that combine meta-RL and IL, and does not provide sufficient ablation studies to justify the necessity of each component of the method.\n4. **Assumptions and Limitations**: The paper makes strong assumptions, such as the availability of an expert distribution conditioned on the learned latent variable, which may not be realistic in practical scenarios.\n5. **Related Work and Citations**: The paper does not adequately position itself within the existing literature, missing citations of closely related works and failing to clearly differentiate itself from similar methods like PEARL.\n6. **Experimental Design**: The experiments lack statistical rigor, such as multiple seeds and significance tests, and do not convincingly demonstrate the claimed capabilities, such as zero-shot learning and exploration beyond demonstrations.\n\n### Conclusion:\nWhile the paper presents an interesting approach to combining meta-RL and IL, the significant issues with clarity, complexity, and experimental validation prevent it from meeting the standards of a top-tier conference. The lack of clear justification for the method's components, insufficient comparisons with related work, and the absence of rigorous experimental validation are critical shortcomings. These issues need to be addressed to make a compelling case for the paper's contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Mos9F9kDwkz",
      "source": "ICLR 2021",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Complex Query Answering with Neural Link Predictors\" presents a novel approach to answering complex queries on incomplete Knowledge Graphs (KGs) using neural link predictors. The proposed method, Continuous Query Decomposition (CQD), is designed to handle queries involving logical conjunctions, disjunctions, and existential quantifiers. The paper claims significant improvements over state-of-the-art methods in terms of accuracy and efficiency, using orders of magnitude less training data.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper introduces a new framework for complex query answering that leverages neural link predictors without requiring extensive training on complex queries. This is a significant contribution to the field, as it addresses the challenge of answering complex queries on incomplete KGs, which is a critical problem in data mining and knowledge representation.\n\n2. **Performance**: The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of Hits@3 across different datasets. The paper provides evidence of relative improvements ranging from 8% to 40%, which is substantial.\n\n3. **Explainability**: The method offers a degree of explainability by showing intermediate solutions for complex query atoms, which is a valuable feature for understanding and trust in AI systems.\n\n4. **Efficiency**: The approach requires significantly less training data compared to previous methods, which is a practical advantage in real-world applications where labeled data is often scarce.\n\n5. **Clarity and Presentation**: The paper is generally well-written and easy to follow, with a clear explanation of the methodology and results.\n\n### Weaknesses and Concerns:\n1. **Technical Details and Comparisons**: Some reviewers raised concerns about the lack of detailed comparisons with existing methods, particularly regarding the differences in optimization techniques and the impact of using different neural link predictors. There is a need for more ablation studies to isolate the contributions of different components of the proposed method.\n\n2. **Complexity and Scalability**: The combinatorial optimization method's time complexity is potentially exponential with respect to the number of hops, which could be a limitation for very large queries. The paper could benefit from a more detailed analysis of the scalability of the proposed methods.\n\n3. **Mathematical Formalism**: One reviewer noted that the paper contains excessive mathematical formalism, which might not be necessary given the simplicity of the core idea. Simplifying the presentation could make the paper more accessible.\n\n4. **Clarifications Needed**: There are some areas where the paper could provide more clarity, such as the calibration of the output of ComplEx, the choice of embedding sizes, and the handling of KG incompleteness in evaluations.\n\n5. **Inference Time**: The paper lacks a detailed comparison of inference times between the proposed methods and existing baselines, which is crucial for understanding the practical applicability of the approach.\n\n### Conclusion:\nOverall, the paper presents a significant advancement in the field of complex query answering on knowledge graphs. Despite some areas needing further clarification and additional experiments, the strengths of the paper, particularly its novel approach, strong empirical results, and practical implications, outweigh the weaknesses. The paper addresses a critical problem with a novel solution that has the potential to impact the field positively.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform for optimizing multi-objective problems using Bayesian optimization. The paper introduces a novel strategy called Believer-Penalizer (BP) for asynchronous batch experiments and provides a graphical user interface for ease of use.\n\n**Strengths:**\n1. **Motivation and Relevance:** The problem of multi-objective optimization in experimental design is well-motivated, and the platform addresses a relevant need in the field.\n2. **Usability:** The inclusion of a GUI makes the platform accessible to users without extensive coding or machine learning expertise.\n3. **Open Source Contribution:** The authors plan to open-source the platform, which could benefit the community by providing a modular and user-friendly tool for multi-objective optimization.\n\n**Weaknesses:**\n1. **Limited Novelty:** Several reviewers noted that the methodological contributions are limited. The Believer-Penalizer strategy, while novel, is described as straightforward and lacks theoretical depth or rigorous justification.\n2. **Empirical Evaluation:** The empirical results do not convincingly demonstrate the superiority of AutoOED over existing methods. The performance is mixed, and the claims of superiority are not consistently supported by the results.\n3. **Scope and Limitations:** The platform is limited to problems with 2 or 3 objectives, which restricts its applicability. Additionally, the paper does not address single-objective optimization, which is a natural extension of multi-objective optimization.\n4. **Comparison with Existing Work:** The paper lacks a comprehensive comparison with existing Bayesian optimization platforms, and some relevant works are not cited.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the platform is well-engineered and potentially useful, the paper lacks significant methodological contributions and empirical evidence to justify publication in a top-tier conference. The novelty and impact of the work are considered marginal, and the empirical results do not convincingly support the claims made by the authors.\n\n**Conclusion:**\nGiven the limited methodological novelty, mixed empirical results, and the scope of the platform, the paper does not meet the standards of a top-tier conference. While the platform may be a valuable tool for practitioners, the scientific contributions are not substantial enough for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper presents a novel approach to improving the efficiency of diffusion models by introducing Differentiable Diffusion Sampler Search (DDSS) and Generalized Gaussian Diffusion Models (GGDM). The primary goal is to reduce the number of inference steps required to generate high-quality samples, which is a significant challenge in the field of generative models.\n\n**Strengths:**\n\n1. **Problem Importance:** The paper addresses a critical issue in diffusion models, which is the high computational cost associated with generating samples. This is a relevant and timely problem given the increasing interest in diffusion models.\n\n2. **Novel Approach:** The introduction of a non-Markovian sampler family (GGDM) and the optimization procedure (DDSS) are innovative. The approach of optimizing perceptual losses instead of traditional likelihood-based objectives is a fresh perspective that has shown empirical success.\n\n3. **Empirical Results:** The paper demonstrates significant improvements in sample quality with fewer steps compared to existing methods like DDPM and DDIM. The results on datasets like CIFAR-10 and LSUN are compelling, showing that the method can achieve better sample quality in fewer timesteps.\n\n4. **Technical Contributions:** The paper discusses practical techniques such as gradient rematerialization to handle the computational challenges, which is a valuable contribution to the field.\n\n**Weaknesses:**\n\n1. **Theoretical Justification:** Several reviewers pointed out the lack of theoretical guarantees and clarity regarding the statistical properties of the proposed models. The paper does not provide a strong theoretical foundation for why the proposed method works, which is a significant drawback for a top-tier conference.\n\n2. **Clarity and Presentation:** The paper suffers from clarity issues, particularly in the explanation of the GGDM framework and the optimization process. The notation and presentation could be improved to make the paper more accessible.\n\n3. **Limited Dataset Evaluation:** The experiments are primarily conducted on smaller datasets, and there is a lack of results on more challenging, high-resolution datasets. This raises questions about the generalizability and robustness of the proposed method.\n\n4. **Novelty Concerns:** Some reviewers expressed concerns about the novelty of the contributions, particularly in relation to existing methods like DDIM. The paper needs to better differentiate its contributions from prior work.\n\n5. **Empirical Overfitting:** There is a concern that optimizing perceptual losses might lead to overfitting to evaluation metrics like FID/IS, which could limit the broader applicability of the method.\n\n**Conclusion:**\n\nWhile the paper presents an interesting approach with promising empirical results, the lack of theoretical justification, clarity issues, and limited evaluation on diverse datasets are significant concerns. For a top-tier conference, these weaknesses are critical and suggest that the paper is not yet ready for acceptance. The authors should focus on addressing these issues, particularly providing a stronger theoretical foundation and improving the clarity of the presentation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VDdDvnwFoyM",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation\" presents a novel approach to generating synthetic time-series data using Variational Auto-Encoders (VAEs). The authors claim that their method offers interpretability, the ability to encode domain knowledge, and reduced training times. The paper is evaluated on four multivariate datasets, and the results are compared to state-of-the-art methods.\n\n### Strengths:\n1. **Clarity and Writing**: The paper is well-written and clearly conveys the main points of the proposed models and related work.\n2. **Interesting Approach**: The integration of classical time-series methods into the VAE framework is an interesting approach.\n3. **Potential for Interpretability**: The proposal to incorporate interpretable components into the VAE architecture is a promising direction, especially in domains where transparency is crucial.\n\n### Weaknesses:\n1. **Lack of Novelty**: Several reviewers pointed out that the proposed models do not present substantial originality. The base model appears to be a standard VAE applied to time-series data without significant modifications or innovations.\n2. **Insufficient Empirical Evaluation**: The empirical evaluation lacks depth. There are no ablation studies to assess the impact of the newly introduced components, and the interpretability aspect is not evaluated.\n3. **Limited Benchmarking**: The experiments are conducted on synthetic or novel datasets, making it difficult to compare the proposed method to existing work. The paper does not include well-known benchmarks or a comprehensive set of comparisons with state-of-the-art methods.\n4. **Hyper-parameter Selection**: The paper introduces new hyper-parameters without providing guidance on their selection or conducting a thorough evaluation of their impact.\n5. **Interpretability Claims**: Although interpretability is claimed as a key contribution, there is no discussion or evaluation of this aspect in the results section.\n6. **Technical and Empirical Significance**: The contributions are considered marginally significant or novel by most reviewers. The paper lacks a strong technical contribution that would warrant acceptance at a top-tier conference.\n\n### Reviewer Consensus:\nThe reviewers generally agree that while the paper is well-written and presents an interesting idea, it lacks the necessary novelty, empirical rigor, and comprehensive evaluation to be considered for a top-tier conference. The interpretability aspect, which is a claimed contribution, is not substantiated with evidence or discussion. Additionally, the lack of benchmarking against well-known datasets and methods limits the paper's impact and significance.\n\n### Conclusion:\nGiven the lack of substantial novelty, insufficient empirical evaluation, and unsubstantiated claims of interpretability, the paper does not meet the standards required for acceptance at a top-tier conference. The authors are encouraged to address these issues, particularly by providing a more thorough empirical evaluation, including ablation studies, benchmarking against established datasets, and a detailed discussion of the interpretability aspect.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Z7Lk2cQEG8a",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions\" presents a novel approach to understanding the optimization landscape of two-layer ReLU neural networks through convex optimization. The paper claims to provide a complete characterization of the global optima of such networks by solving a convex optimization problem, which is a significant theoretical advancement.\n\n**Strengths:**\n\n1. **Novelty and Theoretical Contribution:** The paper extends the work of Pilanci and Ergen (2020) by not only finding a single solution but characterizing all global optima of the two-layer ReLU network training problem. This is a significant theoretical contribution that advances the understanding of neural network optimization landscapes.\n\n2. **Convex Optimization Approach:** The use of convex optimization to characterize the non-convex landscape of neural networks is innovative. The paper provides a framework that could potentially be extended to other neural network architectures or activation functions.\n\n3. **Algorithmic Contributions:** The paper introduces a polynomial-time algorithm to test if a neural network is a global minimum, which is a practical tool for researchers and practitioners.\n\n4. **Clarity and Organization:** The paper is generally well-written and organized, making complex theoretical concepts accessible.\n\n5. **Reviewer Support:** The reviewers generally support the paper, highlighting its theoretical significance and potential impact on the field. Reviewer 2 even mentions it as the best submission they reviewed.\n\n**Weaknesses:**\n\n1. **Scope Limitation:** The primary limitation is that the results are restricted to two-layer networks with ReLU activation and weight decay. While this is acknowledged by the authors and reviewers, it limits the immediate applicability of the results to more complex, multi-layer networks.\n\n2. **Technical Novelty Concerns:** Some reviewers noted that while the contributions are significant, aspects of the work build on existing literature, particularly the work by Pilanci and Ergen. However, the paper does provide substantial extensions and new insights.\n\n3. **Empirical Validation:** The paper lacks empirical validation, which could have strengthened the practical significance of the theoretical results. However, given the theoretical nature of the work, this is not a critical flaw.\n\n4. **Minor Errors and Typos:** Reviewers pointed out several minor errors and typos, which the authors should address in the final version.\n\n**Conclusion:**\n\nThe paper makes a significant theoretical contribution to the understanding of neural network optimization landscapes through convex optimization. Despite its limitations in scope and the need for empirical validation, the novelty and depth of the theoretical insights justify its acceptance. The paper has the potential to inspire further research and extensions to more complex neural network architectures.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "xspalMXAB0M",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"A Boosting Approach to Reinforcement Learning\" presents an interesting approach to reinforcement learning (RL) by leveraging boosting techniques from supervised learning. The primary contribution is an algorithm that improves the accuracy of weak learners iteratively, with sample complexity and running time that do not explicitly depend on the number of states. This is a significant result, especially for large-scale Markov Decision Processes (MDPs).\n\n**Strengths:**\n1. **Theoretical Contribution:** The paper provides a solid theoretical foundation for the proposed algorithm, with rigorous analysis and proofs. The independence of sample complexity from the number of states is a notable achievement.\n2. **Novelty:** While some reviewers noted similarities to prior work (Hazan & Singh, 2021), the application of boosting techniques to RL, particularly using a non-convex variant of the Frank-Wolfe method, is relatively novel and could be valuable to the RL and optimization communities.\n3. **Potential Impact:** The approach could be impactful for solving RL problems with large state spaces, which is a significant challenge in the field.\n\n**Weaknesses:**\n1. **Clarity and Presentation:** Multiple reviewers highlighted issues with clarity, notation overload, and the paper's dense presentation. These issues make it difficult for readers to follow the main ideas and understand the contributions fully.\n2. **Lack of Empirical Validation:** The paper lacks empirical results or experiments to validate the theoretical claims. This is a significant drawback, as empirical evidence is crucial for demonstrating the practical applicability and effectiveness of the proposed algorithm.\n3. **Incremental Novelty:** Some reviewers felt that the novelty of the work is limited, as it builds on existing techniques without sufficiently differentiating itself from prior work.\n4. **Undefined Notations and Assumptions:** There are several instances of undefined notations and assumptions that are not well-justified, which further complicates the understanding of the paper.\n\n**Reviewer Consensus:**\nThe reviewers are divided, with some recommending weak acceptance due to the theoretical contributions and potential impact, while others suggest rejection due to clarity issues and lack of empirical validation. The paper's theoretical contributions are acknowledged, but the presentation and lack of experiments are significant concerns.\n\n**Conclusion:**\nWhile the paper presents an interesting theoretical approach with potential impact, the issues with clarity, presentation, and lack of empirical validation are significant drawbacks for a top-tier conference. The paper would benefit from a substantial rewrite to improve clarity, reduce notation overload, and include empirical results to support the theoretical claims. Given the current state of the paper, it does not meet the standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qfLJBJf_DnH",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Brain insights improve RNNs' accuracy and robustness for hierarchical control of continually learned autonomous motor motifs\" presents an interesting approach to improving RNNs for hierarchical motor control by incorporating insights from neuroscience, specifically the thalamocortical circuit. The paper aims to address the challenges of continual learning and out-of-distribution generalization in the context of chaining motor motifs.\n\n**Strengths:**\n1. **Biological Inspiration:** The paper proposes a novel architecture inspired by the thalamocortical circuit, which is a fresh perspective in the realm of RNNs and could potentially offer new insights into both machine learning and neuroscience.\n2. **Continual Learning Relevance:** The focus on continual learning and the problem of catastrophic forgetting is timely and relevant, as these are significant challenges in the field of machine learning.\n3. **Experimental Detail:** Reviewer 4 notes that the level of experimental detail is sufficient for replication, which is a positive aspect for scientific rigor.\n\n**Weaknesses:**\n1. **Lack of Alternative Hypothesis Testing:** Reviewers 2 and 3 highlight that the authors did not adequately test alternative hypotheses or approaches that could solve the problem without the need for the proposed inductive bias. This is a significant oversight, as it weakens the argument for the necessity of the proposed method.\n2. **Scale and Relevance:** Reviewer 3 points out that the experiments are conducted on a very small scale (300 units, 10 motifs), which raises concerns about the generalizability and relevance of the findings to larger, more complex systems.\n3. **Writing and Clarity:** Reviewer 5 criticizes the paper for its confusing presentation and lack of clarity in describing the task and optimization problem. This could hinder the reader's understanding and the paper's impact.\n4. **Trivial Solution Concern:** Reviewer 5 also raises a concern about the existence of a trivial solution (hard resetting to an initial state), which the authors did not empirically address. This could undermine the novelty and significance of the proposed approach.\n5. **Technical and Empirical Novelty:** Multiple reviewers (2, 3, and 5) rate the technical and empirical novelty as marginal, suggesting that the contributions may not be significant enough for a top-tier conference.\n\n**Conclusion:**\nWhile the paper presents an interesting biologically inspired approach, the lack of thorough testing of alternative methods, the small scale of experiments, and the issues with clarity and presentation significantly detract from its potential impact. The concerns about a trivial solution and the marginal novelty further weaken the case for acceptance. Given these considerations, the paper does not meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "VFBjuF8HEp",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\" presents a novel approach to improving the efficiency of diffusion models, which are known for their high sample quality but also for their computational inefficiency due to the large number of steps required for sampling. The authors propose Differentiable Diffusion Sampler Search (DDSS) and Generalized Gaussian Diffusion Models (GGDM) to address this issue.\n\n### Strengths:\n1. **Problem Relevance**: The paper addresses a significant problem in the field of generative models, specifically the inefficiency of diffusion models in terms of sampling speed.\n2. **Novel Approach**: The introduction of a non-Markovian sampler and the optimization of sample quality through perceptual loss is a novel approach that shows promise in reducing the number of steps required for high-quality sample generation.\n3. **Empirical Results**: The empirical results demonstrate substantial improvements in sample quality with fewer steps compared to existing methods, particularly on datasets like CIFAR-10 and LSUN.\n4. **Compatibility**: The method is compatible with any pre-trained diffusion model, which adds to its practical utility.\n\n### Weaknesses:\n1. **Theoretical Justification**: Several reviewers pointed out the lack of theoretical guarantees and clarity regarding the statistical properties of the proposed models. The paper does not provide a strong theoretical foundation for why the proposed method works, particularly concerning the optimization of perceptual loss instead of ELBO.\n2. **Clarity and Presentation**: The paper suffers from clarity issues, with complex notations and insufficient explanations of key concepts, which could hinder understanding and reproducibility.\n3. **Limited Dataset Evaluation**: The experiments are primarily conducted on smaller datasets, and there is a lack of evaluation on larger, more challenging datasets, which would better demonstrate the scalability and robustness of the method.\n4. **Overfitting to Evaluation Metrics**: There is a concern that optimizing perceptual loss might lead to overfitting to specific evaluation metrics like FID, without necessarily improving the underlying model's generative capabilities.\n\n### Reviewer Consensus:\nThe reviewers generally acknowledge the novelty and potential impact of the work but express concerns about the theoretical underpinnings and clarity of the presentation. While some reviewers are inclined to accept the paper due to its empirical success and relevance, others are hesitant due to the aforementioned weaknesses.\n\n### Author Responses:\nThe authors have addressed some concerns, particularly regarding the motivation for using perceptual loss and the compatibility with pre-trained models. However, the responses do not fully resolve the issues related to theoretical justification and clarity.\n\n### Conclusion:\nWhile the paper presents an interesting and potentially impactful approach to improving diffusion models, the lack of theoretical grounding and clarity, combined with limited evaluation on larger datasets, are significant drawbacks for a top-tier conference. The paper would benefit from further refinement and additional experiments to address these issues.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0p-aRvcVs-U",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning\" presents a novel approach to multitask learning by dynamically adjusting task weights using task-specific updates of model parameters. While the paper introduces an interesting concept, there are several critical issues that need to be addressed before it can be considered for acceptance at a top-tier conference.\n\n1. **Theoretical Justification**: One of the major concerns raised by the reviewers is the lack of theoretical justification for the proposed method. Reviewer 1 specifically mentions the need for a formal analysis to support the intuitive approach. Without a solid theoretical foundation, the method's reliability and generalizability remain questionable.\n\n2. **Experimental Results**: The experimental results do not convincingly demonstrate the superiority of the proposed method. Reviewers note that the improvements over baselines are minor and often fall within the range of statistical insignificance. Reviewer 2 points out that the results on the MNIST and NLU tasks are mixed, with no clear best algorithm emerging. Additionally, the choice of baselines is limited, and the experimental setup, particularly for the vision domain, is not fully convincing.\n\n3. **Novelty and Differentiation**: Reviewer 2 raises concerns about the novelty of the approach, suggesting that it resembles existing methods like meta-learning of task weights. The paper needs to clearly articulate how $\\alpha$VIL differs from and improves upon existing methods.\n\n4. **Algorithm Clarity and Implementation**: Reviewer 3 questions the clarity of the algorithm, particularly the distinction between alpha and task-specific weights. This confusion indicates that the algorithm's presentation may need refinement to ensure that its contributions are clearly understood.\n\n5. **Statistical Significance and Ablations**: The paper lacks statistical significance scores for the results, which are crucial for evaluating the robustness of the findings. Additionally, more ablation studies and discussions on the learned importance of auxiliary tasks would strengthen the empirical evaluation.\n\n6. **Presentation and Writing**: While the paper is generally well-written, there are areas for improvement, such as expanding the introduction and addressing minor style issues noted by the reviewers.\n\nIn summary, while the paper presents an intriguing approach to multitask learning, it falls short in several critical areas, including theoretical justification, empirical validation, and clarity of the proposed method. These issues need to be addressed to meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "morSrUyWG26",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"AutoOED: Automated Optimal Experimental Design Platform with Data- and Time-Efficient Multi-Objective Optimization\" presents a software platform for optimizing multi-objective problems using Bayesian optimization. The platform is designed to be user-friendly, with a graphical user interface, and introduces a novel strategy called Believer-Penalizer (BP) for asynchronous batch optimization.\n\n**Strengths:**\n1. **Motivation and Relevance:** The problem of multi-objective optimization in experimental design is well-motivated, and the platform addresses a relevant need in the field.\n2. **Usability:** The inclusion of a GUI makes the platform accessible to users without extensive coding or machine learning expertise, which is a significant advantage.\n3. **Engineering Effort:** The platform is well-engineered, providing a modular framework for implementing and testing multi-objective Bayesian optimization algorithms.\n\n**Weaknesses:**\n1. **Novelty and Contribution:** The primary concern across multiple reviews is the lack of significant methodological novelty. The Believer-Penalizer strategy, while a novel contribution, is described as straightforward and lacks theoretical depth. The paper is perceived more as an engineering effort rather than a scientific advancement.\n2. **Empirical Validation:** The empirical results do not convincingly demonstrate the superiority of the proposed methods. The performance of AutoOED is mixed across benchmarks, and the claims of its effectiveness are overstated according to the reviewers.\n3. **Scope and Limitations:** The platform is limited to problems with 2 or 3 objectives, which restricts its applicability. This limitation is not adequately addressed in the paper.\n4. **Comparison with Existing Work:** The paper lacks a comprehensive comparison with existing Bayesian optimization platforms, and some relevant literature is not cited.\n\n**Reviewer Consensus:**\nThe reviewers generally appreciate the engineering effort and the potential utility of the platform but express concerns about the lack of significant scientific contributions. The paper is seen as more suitable for a software demonstration or a workshop rather than a top-tier conference focused on novel scientific contributions.\n\n**Conclusion:**\nGiven the emphasis on novelty and scientific contribution at top-tier conferences, the paper does not meet the required standards. While the platform may be valuable for practitioners, the lack of methodological innovation and the overstated empirical claims are significant drawbacks.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "RhB1AdoFfGE",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Sample and Computation Redistribution for Efficient Face Detection\" presents a method for improving face detection by focusing on computation and sample redistribution. The authors propose two main techniques: Computation Redistribution (CR) and Sample Redistribution (SR), which aim to optimize the detection of small faces in low-resolution images. The paper claims state-of-the-art performance on the WIDER FACE dataset, outperforming existing methods like TinaFace.\n\n**Strengths:**\n1. **Performance:** The paper demonstrates significant improvements in face detection performance, particularly for small faces, which is a challenging aspect of face detection. The results on the WIDER FACE dataset are compelling, showing a notable improvement over existing methods.\n2. **Efficiency:** The proposed methods achieve a better accuracy-efficiency trade-off, which is crucial for practical applications where computational resources are limited.\n3. **Generalizability:** The computation redistribution method is described as general and potentially applicable to other tasks beyond face detection.\n\n**Weaknesses:**\n1. **Novelty and Significance:** Several reviewers noted that the methods, while effective, are relatively straightforward and not particularly novel. The techniques used, such as neural architecture search, are well-established, and the paper does not provide a strong comparison with other state-of-the-art network search methods.\n2. **Lack of Detail:** Some reviewers pointed out that the paper lacks sufficient detail in describing the search strategies and the overall framework. This includes missing figures and algorithms that could help clarify the methodology.\n3. **Comparison with SOTA:** The paper does not adequately compare its results with other state-of-the-art methods across multiple datasets, which raises questions about the generalizability and robustness of the proposed approach.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that the paper presents a method that achieves good empirical results, particularly on the WIDER FACE dataset. However, there are concerns about the novelty and the lack of detailed comparisons with other methods. Some reviewers are satisfied with the paper's contributions, while others feel that the methods are not sufficiently innovative for a top-tier conference.\n\n**Final Decision:**\nGiven the strengths in performance and efficiency, the paper has merit. However, the concerns about novelty, lack of detailed methodology, and insufficient comparisons with other state-of-the-art methods are significant drawbacks. For a top-tier conference, where both novelty and thoroughness are critical, these issues cannot be overlooked.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "shbAgEsk3qM",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Understanding and Leveraging Overparameterization in Recursive Value Estimation\" presents an analysis of recursive value estimation using overparameterized linear representations in reinforcement learning (RL). The paper aims to provide insights into the utility of deep models in RL by examining classical updates like temporal difference (TD) learning, fitted-value-iteration (FVI), and residual minimization (RM) in the overparameterized regime.\n\n### Strengths:\n1. **Novel Perspective**: The paper offers a novel perspective on the implicit biases of classical value estimation algorithms in the overparameterized setting, which is a significant departure from the traditional underparameterized analysis.\n2. **Unified Interpretation**: It provides a unified interpretation of overparameterized linear value estimation, which could be valuable for understanding the behavior of these algorithms in modern deep RL settings.\n3. **Regularizers**: The introduction of regularizers to improve stability and generalization performance is a practical contribution that could have implications for deep RL applications.\n4. **Theoretical Contributions**: The paper makes theoretical contributions by analyzing the convergence properties and generalization error bounds of the algorithms in the overparameterized regime.\n\n### Weaknesses:\n1. **Lack of Novelty and Credit**: Several reviewers pointed out that some results are not novel and do not adequately credit prior work. For instance, Theorem 4 is criticized for being a well-known result in convex optimization.\n2. **Empirical Evaluation**: The empirical evaluation is considered weak, with experiments conducted in simple environments that do not convincingly demonstrate the practical utility of the proposed methods.\n3. **Connection to Deep RL**: The paper's connection to deep RL is questioned, as the overparameterization in deep RL often involves non-linear models, whereas the paper focuses on linear representations.\n4. **Technical Issues**: There are technical issues and errors in the proofs and derivations, such as incorrect assumptions about matrix decompositions and errors in the application of inequalities.\n5. **Clarity and Writing**: Some reviewers noted that the paper's writing could be improved for clarity, and there are grammatical issues that need addressing.\n\n### Reviewer Consensus:\nThe reviewers are divided in their opinions. Some appreciate the theoretical contributions and the novel perspective on overparameterization, while others are concerned about the lack of novelty, insufficient empirical validation, and technical inaccuracies. The paper's potential impact is limited by its focus on linear models, which may not fully capture the complexities of deep RL.\n\n### Conclusion:\nWhile the paper presents interesting theoretical insights into overparameterization in value estimation, the concerns about novelty, empirical validation, and technical correctness are significant. For a top-tier conference, the paper needs to demonstrate both strong theoretical contributions and practical relevance, which this paper currently lacks. The issues raised by the reviewers, particularly regarding the novelty and empirical significance, suggest that the paper is not yet ready for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "JmU7lyDxTpc",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Multi-scale Feature Learning Dynamics: Insights for Double Descent\" explores the phenomenon of epoch-wise double descent in neural networks using a linear teacher-student model. The authors derive closed-form analytical expressions for the generalization error and validate their findings through numerical experiments. The paper aims to provide insights into the training dynamics of neural networks, particularly focusing on the less-studied epoch-wise double descent.\n\n**Strengths:**\n\n1. **Theoretical Contribution:** The paper provides a theoretical analysis of epoch-wise double descent using a simple linear model. This is valuable as it offers insights into a complex phenomenon observed in deep neural networks.\n\n2. **Analytical Expressions:** The derivation of closed-form expressions for the generalization error is a significant contribution, as it allows for a deeper understanding of the dynamics involved.\n\n3. **Empirical Validation:** The authors validate their theoretical findings with numerical experiments, showing a qualitative match with the behavior of a ResNet18 on CIFAR10. This strengthens the paper's claims about the generality of the identified mechanisms.\n\n4. **Clarity and Presentation:** Reviewer 1 commends the paper for being clearly written and well-presented, which is crucial for conveying complex theoretical ideas effectively.\n\n**Weaknesses:**\n\n1. **Connections to Existing Work:** Reviewers 2 and 3 highlight the need for a more thorough discussion of the connections to existing literature, particularly regarding implicit regularization effects and prior work on double descent in linear models.\n\n2. **Technical Novelty:** Reviewer 3 questions the novelty of the insights provided, suggesting that similar ideas might have appeared in earlier works. This raises concerns about the paper's contribution to advancing the state of the art.\n\n3. **Typos and Errors:** Reviewer 3 notes numerous typos and errors in the manuscript, which can hinder the reader's ability to follow the calculations and assess the correctness of the results.\n\n4. **Depth of Analysis:** Reviewer 1 suggests that the paper could benefit from a deeper exploration of the implications of the analytical theory, such as the potential for triple descent and the differences between epoch-wise and model-size double descent.\n\n5. **Empirical Novelty:** The empirical contributions are considered marginally significant by Reviewer 3, as the observation of epoch-wise double descent in realistic networks is not new.\n\n**Author Responses:**\n\nThe authors have not provided responses to the reviewers' comments, which limits the ability to assess whether the identified weaknesses have been addressed.\n\n**Conclusion:**\n\nWhile the paper makes a valuable theoretical contribution by analyzing epoch-wise double descent in a simple model, the concerns about its novelty, connections to existing work, and presentation issues are significant. The lack of author responses further complicates the decision, as it is unclear whether the authors have addressed these concerns. Given the standards of a top-tier conference, the paper's contributions, while insightful, may not be sufficiently novel or impactful to warrant acceptance without substantial revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "NudBMY-tzDr",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Natural Language Descriptions of Deep Visual Features\" introduces a novel method, MILAN, for generating natural language descriptions of neuron activations in deep learning models, specifically in the context of computer vision. The paper presents a compelling approach to interpretability in deep learning, which is a critical area of research given the increasing complexity and opacity of these models.\n\n### Strengths:\n1. **Novelty and Contribution**: The paper presents a novel approach to neuron interpretation by using mutual information to generate natural language descriptions. This is a significant contribution to the field of model interpretability and explainability.\n   \n2. **Technical Soundness**: The methodology is well-explained, and the authors provide sufficient technical details to understand and reproduce the work. The use of mutual information to guide the generation of descriptions is a clever approach that leverages statistical relationships between neuron activations and image regions.\n\n3. **Empirical Validation**: The paper demonstrates the effectiveness of MILAN across various model architectures, datasets, and tasks. The results show high agreement with human annotations, indicating the method's robustness and generalizability.\n\n4. **Applications**: The paper highlights practical applications of MILAN, such as model auditing and editing, which are valuable for improving model robustness and understanding model behavior.\n\n5. **Open Science**: The authors commit to open-sourcing their data, code, and models, which is commendable and facilitates further research and validation by the community.\n\n### Weaknesses:\n1. **Inter-Annotator Agreement**: Reviewer 2 points out concerns about the inter-annotator agreement in the MILANNOTATIONS dataset. While the authors claim high agreement with human-generated descriptions, the lower inter-annotator agreement raises questions about the consistency and reliability of the dataset used for training.\n\n2. **Scope and Generalization**: Reviewer 4 raises concerns about the generalizability of the method beyond image models and suggests that the paper might overstate the portability of the technique. The experiments are limited to image-based tasks, and it is unclear how well the method would perform in other domains or with different types of data.\n\n3. **Baseline Comparisons**: There is a suggestion to compare MILAN with existing image captioning models to empirically validate its superiority in generating neuron-level descriptions. This comparison could strengthen the claims about the method's effectiveness.\n\n4. **Limitations and Future Work**: While the paper is strong in its contributions, it could benefit from a more thorough discussion of the method's limitations and potential areas for future research, as noted by Reviewer 3.\n\n### Conclusion:\nThe paper presents a significant advancement in the field of model interpretability with a novel method that is both technically sound and empirically validated. Despite some concerns about dataset consistency and the scope of generalization, the strengths of the paper, including its novelty, practical applications, and commitment to open science, make it a strong candidate for acceptance. The reviewers generally support the paper, with suggestions for minor improvements and clarifications.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "lmumJ2pC0JB",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks\" presents a detailed theoretical analysis of the convergence rates of gradient descent (GD) on deep linear neural networks. The paper claims to extend existing results by considering more general loss functions and different initialization schemes, and it provides insights into the behavior of GD in overparameterized settings.\n\n**Strengths:**\n\n1. **Clarity and Presentation:** The paper is well-written and the results are clearly presented. Reviewers generally found the paper enjoyable to read and appreciated the clarity of the exposition.\n\n2. **Novel Insights:** Theorem 3.3, which shows that the trajectory of GD on deep linear networks closely follows that of the corresponding convex problem, is highlighted as a novel and intriguing result. This insight could provide valuable understanding for the broader machine learning community.\n\n3. **Rigorous Analysis:** The paper provides a rigorous convergence analysis, showing that overparameterization leads to depth-independent convergence rates, which is a significant theoretical contribution.\n\n**Weaknesses:**\n\n1. **Limited Novelty:** Several reviewers, including Reviewer 1, 3, and 5, express concerns about the novelty of the contributions. They note that many of the results, particularly regarding convergence rates, are extensions of existing work by Du and Hu (2019) and others. The extension to general strongly convex and smooth objectives is seen as a straightforward application of known techniques in convex optimization.\n\n2. **Overclaims and Lack of Discussion:** Reviewer 6 points out that the paper makes some overclaims, particularly in the title and abstract, by not specifying that the results are limited to overparameterized models. Additionally, there is a lack of discussion on the implications of overparameterization and other important aspects such as the impact of regularization or minibatch settings.\n\n3. **Technical Challenges Not Clearly Highlighted:** Reviewers 3 and 5 request more elaboration on the technical challenges overcome in extending existing results. The paper does not sufficiently highlight the difficulties and innovations involved in achieving the stated results.\n\n4. **Minor Issues and Suggestions:** Several minor issues and suggestions for improvement are noted, such as the need for better discussion of constants in theorems, addressing the impact of regularization, and improving figure readability.\n\n**Overall Assessment:**\n\nThe paper provides a solid theoretical contribution to the understanding of gradient descent in deep linear networks, particularly with the novel insight of trajectory similarity. However, the concerns about the novelty of the main results, the overclaims, and the lack of in-depth discussion on key aspects are significant drawbacks. While the paper is technically sound, the contributions may not be sufficiently groundbreaking for a top-tier conference, given the reliance on existing techniques and results.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "UOOmHiXetC",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Structure and randomness in planning and reinforcement learning\" introduces a novel algorithm called Shoot Tree Search (STS), which aims to improve planning in large state spaces by modifying the expansion phase of Monte Carlo Tree Search (MCTS). The paper claims that STS can control the trade-off between depth and breadth of the search more explicitly and achieve better performance than existing methods.\n\n**Strengths:**\n1. **Novelty and Simplicity:** The idea of multi-step expansion in tree search is simple and potentially impactful, especially in environments with large action spaces.\n2. **Empirical Evaluation:** The paper provides a thorough empirical evaluation, demonstrating improved performance over standard MCTS and other baseline methods in challenging domains like Sokoban and Google Research Football.\n3. **Writing and Clarity:** The paper is generally well-written and easy to understand, with clear explanations of the proposed method and its implementation.\n\n**Weaknesses:**\n1. **Technical Novelty:** Several reviewers express concerns about the novelty of the approach, noting that similar ideas have been explored in previous works. The paper lacks a thorough discussion of related work and does not convincingly differentiate STS from existing methods like AlphaGo's MCTS with rollouts.\n2. **Experimental Design:** The experiments lack critical ablation studies to isolate the benefits of multi-step expansion from other factors, such as the bias of the value function approximator. The choice of hyperparameters and the setup of experiments are not well-justified, leading to questions about the validity of the results.\n3. **Theoretical Justification:** The paper does not provide a strong theoretical foundation for why STS should outperform MCTS, especially in sparse reward environments. The lack of an intuitive explanation for the advantages of STS over MCTS is a significant gap.\n4. **Presentation and Completeness:** Several reviewers point out issues with the presentation, such as missing definitions, unclear algorithm descriptions, and incomplete explanations of experimental results. The paper seems premature for publication without addressing these issues.\n\n**Reviewer Consensus:**\nThe reviewers generally appreciate the simplicity and potential of the idea but express significant concerns about the novelty, experimental design, and presentation quality. There is a consensus that the paper requires substantial revisions to address these issues before it can be considered for publication in a top-tier conference.\n\n**Author Responses:**\nThe authors have not provided sufficient responses to address the reviewers' concerns. Key issues such as the lack of novelty, inadequate experimental design, and missing theoretical justification remain unresolved.\n\n**Conclusion:**\nGiven the concerns about the novelty, experimental design, and presentation quality, the paper does not meet the standards of a top-tier conference. The authors need to conduct more thorough experiments, provide a stronger theoretical foundation, and improve the presentation to make a compelling case for the acceptance of their work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "3c13LptpIph",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Behavior Proximal Policy Optimization\" presents an interesting approach to offline reinforcement learning (RL) by leveraging the inherent conservatism of on-policy algorithms like PPO to address the challenges of offline RL. The authors propose a method called Behavior Proximal Policy Optimization (BPPO) and claim it outperforms state-of-the-art offline RL algorithms on the D4RL benchmark.\n\n**Strengths:**\n\n1. **Theoretical Insight:** The paper provides a novel theoretical insight that on-policy algorithms can naturally address the overestimation problem in offline RL due to their conservative nature. This is an interesting perspective that could potentially simplify offline RL approaches.\n\n2. **Simplicity and Implementation:** The proposed BPPO algorithm is a simple modification of the existing PPO algorithm, making it easy to implement and understand.\n\n3. **Empirical Results:** The experiments conducted on the D4RL benchmark show that BPPO performs competitively with existing state-of-the-art methods, which is a positive indication of its practical applicability.\n\n**Weaknesses:**\n\n1. **Clarity and Writing:** Several reviewers noted that parts of the paper are unclear or written in a colloquial manner, which detracts from the overall presentation. This could hinder the reader's understanding of the theoretical contributions and the algorithm.\n\n2. **Theoretical Soundness:** There are concerns about the soundness of the theoretical claims, particularly regarding the assumptions made (e.g., Assumption 1) and the approximation methods used. Some reviewers pointed out that the theoretical results might not be as novel as claimed, with overlaps with existing works like GePPO.\n\n3. **Experimental Validation:** The paper lacks certain experimental validations, such as demonstrating that BPPO does not suffer from overestimation issues or providing a thorough analysis of hyperparameters. Additionally, the empirical improvements over baselines are not always significant, raising questions about the practical advantages of BPPO.\n\n4. **Novelty and Contribution:** While the theoretical insight is interesting, the novelty of the approach is questioned due to similarities with existing methods. The empirical contributions are also seen as marginal, with BPPO performing similarly to other simple methods like TD3+BC.\n\n5. **Reproducibility and Completeness:** The lack of source code and incomplete experimental comparisons (e.g., missing baselines) are noted as issues that could affect reproducibility and the perceived thoroughness of the work.\n\n**Conclusion:**\n\nWhile the paper presents an intriguing theoretical insight and a simple algorithmic approach, the concerns about clarity, theoretical soundness, and the significance of empirical results are substantial. The novelty of the contribution is also questioned, with some reviewers suggesting that the theoretical results are not as groundbreaking as presented. Given these issues, the paper does not meet the high standards required for a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ffS_Y258dZs",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Meta-Referential Games to Learn Compositional Learning Behaviours\" presents an interesting approach to investigating compositional learning behaviors through a novel benchmark and a symbolic continuous stimulus (SCS) representation. However, there are several critical issues highlighted by the reviewers that need to be addressed before considering this paper for acceptance at a top-tier conference.\n\n1. **Clarity and Readability**: All reviewers noted that the paper is difficult to follow, with complex and unclear explanations of the proposed methods and experiments. Key concepts such as the SCS representation, the benchmark, and the experimental setup are not adequately explained. This lack of clarity makes it challenging for readers to understand the contributions and significance of the work.\n\n2. **Novelty and Significance**: The technical and empirical novelty of the paper is questioned by all reviewers. The proposed SCS representation and benchmark are not convincingly shown to be significant improvements over existing methods. The reviewers suggest that the contributions are marginal and not well-supported by the results presented.\n\n3. **Experimental Evaluation**: The experiments are insufficient to support the claims made in the paper. There is a lack of comparative analysis with existing methods, and the results do not convincingly demonstrate the effectiveness of the proposed approach. The reviewers also point out the need for more seeds in experiments, ablation studies, and a clearer explanation of the experimental setup.\n\n4. **Benchmark and Representation**: The proposed benchmark and SCS representation are not adequately justified or demonstrated to be useful beyond the specific tasks presented in the paper. The reviewers express concerns about the applicability and utility of these contributions in broader contexts.\n\n5. **Engagement with Existing Literature**: The paper does not adequately situate its contributions within the broader literature on compositional learning and related fields. Reviewer 4, in particular, highlights the need for a more thorough engagement with existing research to clarify the paper's contributions.\n\n6. **Correctness and Support for Claims**: The reviewers note that some of the paper's claims are not well-supported by theory or empirical results. This undermines the credibility of the contributions and suggests that further development and validation are needed.\n\nGiven these significant concerns, the paper requires substantial revisions to improve clarity, strengthen the experimental evaluation, and better situate its contributions within the existing literature. The current version does not meet the standards of a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7fFO4cMBx_9",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Variational Neural Cellular Automata\" presents a novel approach by integrating Neural Cellular Automata (NCA) into the decoding process of a variational autoencoder (VAE). The authors aim to explore the potential of NCAs in generative modeling, inspired by biological processes of cellular growth and differentiation. The paper is well-written and provides a clear explanation of the proposed model and its experimental setup.\n\n**Strengths:**\n1. **Clarity and Writing:** The paper is well-structured and clearly explains the concepts and experimental procedures, making it accessible to readers.\n2. **Novelty in Approach:** The integration of NCA into a VAE framework is an interesting idea that could potentially bridge the gap between self-organizing systems and deep learning models.\n3. **Robustness Experiments:** The experiments demonstrating the model's robustness to perturbations are intriguing and highlight a potential advantage of the NCA approach.\n4. **Community Interest:** There is a growing interest in NCAs, and this work contributes to that field by exploring their application in generative models.\n\n**Weaknesses:**\n1. **Underwhelming Experimental Results:** The experimental results are not competitive with state-of-the-art generative models, particularly on datasets like CelebA. This is a significant drawback for a top-tier conference where empirical performance is crucial.\n2. **Lack of Clear Motivation and Impact:** Several reviewers noted that the motivation for using NCAs in this context is not fully clear, and the practical benefits over existing methods are not well-demonstrated.\n3. **Novelty Concerns:** Some reviewers pointed out that the architectural contributions might not be as novel as claimed, with similar ideas existing in the literature, particularly in the context of neural ODEs and resnets.\n4. **Limited Scope of Experiments:** The experiments are primarily conducted on toy datasets, and the paper lacks a comprehensive comparison with other methods in terms of performance metrics like log-likelihood or image quality.\n5. **Technical and Empirical Significance:** The technical novelty and empirical significance are questioned by several reviewers, with some suggesting that the contributions are incremental rather than groundbreaking.\n\n**Reviewer Consensus:**\nThe reviewers are divided, with some appreciating the novelty and potential of the approach, while others are concerned about the lack of competitive results and clear motivation. The paper has received mixed scores, with some reviewers leaning towards acceptance due to the interesting approach and others leaning towards rejection due to the underwhelming results and limited novelty.\n\n**Author Responses:**\nThe authors have addressed several reviewer concerns, improving the paper's clarity and addressing specific technical questions. However, the fundamental issues regarding experimental results and the novelty of the contributions remain.\n\n**Conclusion:**\nWhile the paper presents an interesting idea and contributes to the growing field of NCAs, the lack of competitive experimental results and clear demonstration of practical benefits over existing methods are significant drawbacks. For a top-tier conference, where empirical performance and clear impact are critical, these issues are likely to outweigh the paper's strengths.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-SBZ8c356Oc",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples\" presents a new adversarial training algorithm aimed at enhancing the robustness of deep neural networks against adversarial attacks. The proposed method involves applying more regularization to data that is vulnerable to adversarial attacks, and it claims to achieve state-of-the-art performance in terms of both generalization and robustness.\n\n### Detailed Reasoning:\n\n1. **Theoretical Contribution:**\n   - The paper introduces a new theoretical framework that extends the results from TRADES to multi-classification problems. However, several reviewers have raised concerns about the novelty and significance of this theoretical contribution. Specifically, the extension is considered straightforward and not sufficiently novel to be deemed a significant theoretical advancement. The use of inequalities in Theorem 1 has been criticized as being loose and not providing substantial theoretical insight.\n   - There is a lack of consensus among reviewers about the meaningfulness of the theoretical results. Some reviewers find the theoretical part to be a simple extension or even a restatement of existing results, while others see value in its simplicity and practical implications.\n\n2. **Empirical Evaluation:**\n   - The empirical results show that the proposed method, ARoW, achieves improvements over existing methods like TRADES and MART. However, the improvements are often marginal, and there are concerns about the robustness of the baselines used for comparison. Some reviewers pointed out discrepancies in the reported results for baselines when compared to other sources, suggesting that the baselines might not be optimally tuned.\n   - The experiments are conducted on relatively small datasets (CIFAR-10, F-MNIST, SVHN), and there is a lack of evaluation on larger datasets, which raises questions about the scalability and general applicability of the method.\n\n3. **Clarity and Presentation:**\n   - The paper is generally well-written and clearly presents the proposed method and its differences from existing algorithms. However, some reviewers have pointed out areas where the presentation could be improved, particularly in clarifying the theoretical contributions and the relationship to TRADES.\n\n4. **Reproducibility:**\n   - The authors have provided code and implementation details, which supports the reproducibility of the results. This is a positive aspect of the paper.\n\n5. **Reviewers' Consensus:**\n   - The reviewers are divided in their opinions. Some reviewers are on the fence or lean towards acceptance due to the empirical results and practical implications, while others strongly criticize the theoretical contributions and the marginal empirical improvements, leading them to recommend rejection.\n\n### Conclusion:\n\nGiven the mixed reviews, the primary concerns about the theoretical contributions, and the marginal empirical improvements, the paper does not meet the high standards required for a top-tier conference. The theoretical novelty is not sufficiently compelling, and the empirical results, while promising, do not demonstrate a significant advancement over existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "hE3JWimujG",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Cortico-cerebellar networks as decoupled neural interfaces\" presents an intriguing hypothesis that the cerebellum functions similarly to decoupled neural interfaces (DNI) in deep learning, potentially solving the credit assignment problem in the brain. The authors propose that the cerebellum helps the cerebral cortex solve the locking problem, akin to the computations and architecture of DNI, and they test this hypothesis through various tasks.\n\n**Strengths:**\n1. **Novelty and Interdisciplinary Approach:** The paper introduces a novel perspective by linking neuroscience and deep learning, suggesting that the cerebellum acts as a decoupling mechanism similar to DNIs. This interdisciplinary approach could stimulate new research directions.\n2. **Clarity and Experimental Rigor:** Reviewer 2 commends the paper for its clarity and well-done experimental investigations. The authors demonstrate their model's applicability across different tasks, from motor to cognitive, which showcases the model's versatility.\n3. **Potential Impact:** Reviewer 1 believes the paper could benefit the community and stimulate discussion, indicating its potential impact on both neuroscience and deep learning fields.\n\n**Weaknesses:**\n1. **Lack of New Insights:** Reviewer 2 argues that the paper does not offer new insights into either the cerebellum or DNIs, merely juxtaposing existing knowledge. The novelty of the hypothesis does not translate into substantial new findings or insights.\n2. **Weak Biological Validation:** Reviewer 3 points out the lack of comparison with experimental findings from the cerebellum, which limits the scientific impact. The paper makes predictions but does not provide concrete testable hypotheses or empirical validation.\n3. **Theoretical Gaps:** Reviewer 3 highlights critical unanswered questions about how real gradients are computed and transmitted in the brain, which are crucial for the feasibility of the proposed mechanism. The paper circumvents these issues, which undermines the biological plausibility of the model.\n4. **Clarity and Consistency Issues:** Reviewer 1 notes confusion in the presentation of the model, particularly the distinction between feedforward and recurrent settings. There are also issues with references and figure labeling, as noted by Reviewer 3.\n\n**Conclusion:**\nWhile the paper presents an interesting hypothesis and is well-executed in terms of experiments, it falls short in providing new insights or substantial contributions to the understanding of the cerebellum or DNIs. The lack of biological validation and theoretical gaps further weaken its case for acceptance at a top-tier conference. The paper's novelty and interdisciplinary approach are commendable, but they do not compensate for the lack of depth and rigor in addressing critical scientific questions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "qiAxL3Xqx1o",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"GG-GAN: A Geometric Graph Generative Adversarial Network\" presents a novel approach to graph generation using a geometric perspective and a Wasserstein GAN framework. The authors claim several advantages of their method, including isomorphism consistency, scalability, and novelty. However, the paper has received mixed reviews from the reviewers, highlighting both strengths and weaknesses.\n\n### Strengths:\n1. **Novelty and Theoretical Support**: The paper introduces a new approach to graph generation by associating nodes with positions in space and using a similarity function to connect edges. The theoretical support provided is appreciated by some reviewers.\n2. **Permutation Equivariance**: The model's ability to handle isomorphic graphs consistently is a significant contribution, as noted by Reviewer 1.\n3. **Scalability**: The authors claim that GG-GAN can scale to generate large graphs, which is a desirable property for practical applications.\n4. **Clarity and Organization**: The paper is generally well-written and organized, making it accessible to readers.\n\n### Weaknesses:\n1. **Experimental Validation**: The experimental results are not convincing enough to demonstrate the superiority of GG-GAN over existing methods. The datasets used are small, and the comparisons with baselines are limited. Reviewers 1 and 4 express concerns about the lack of comparison with state-of-the-art methods like NetGAN and TagGen.\n2. **Mode Collapse and Diversity**: Reviewer 2 points out the lack of discussion on mode collapse and generation diversity, which are critical issues in generative models.\n3. **Edge Dependencies**: There is skepticism about the model's ability to capture complex edge dependencies, as highlighted by Reviewer 2.\n4. **Over-claimed Contributions**: Reviewer 4 suggests that the paper might overstate its contributions, particularly in terms of novelty and scalability.\n5. **Theoretical Clarity**: Some theoretical aspects, such as Proposition 1 and the role of certain parameters, are not well-explained, leading to confusion among reviewers.\n\n### Additional Concerns:\n- The literature review is not comprehensive, and the connection to existing work is not well-established.\n- The paper lacks a thorough discussion on the implications of the proposed method and its limitations.\n\n### Conclusion:\nWhile the paper presents an interesting approach to graph generation, the experimental validation is insufficient to support the claims made by the authors. The lack of comprehensive comparisons with state-of-the-art methods and the limited discussion on critical issues like mode collapse and edge dependencies weaken the paper's contributions. Additionally, the theoretical aspects need further clarification to enhance the paper's rigor.\n\nGiven these considerations, the paper does not meet the high standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Batch Normalization and Bounded Activation Functions\" explores an interesting and potentially impactful area of research: the interaction between batch normalization (BN) and bounded activation functions. The authors propose that swapping the conventional order of BN and activation functions can lead to improved performance when using bounded activation functions like Tanh. They attribute this improvement to asymmetric saturation and increased sparsity.\n\n**Strengths:**\n1. **Novelty and Interest:** The paper presents a novel observation that swapping the order of BN and activation functions can lead to better performance with bounded activations. This is an interesting finding that could have implications for neural network design.\n2. **Empirical Analysis:** The authors conduct extensive experiments to support their claims, providing quantitative metrics for saturation, skewness, and sparsity.\n3. **Clarity and Structure:** The paper is generally well-structured, making it relatively easy to follow the logic and experimental setup.\n\n**Weaknesses:**\n1. **Lack of Theoretical Justification:** Several reviewers noted the lack of a strong theoretical foundation for the claims made. The paper relies heavily on empirical observations without providing a rigorous theoretical explanation for why the observed phenomena occur.\n2. **Inconsistencies and Unclear Claims:** There are inconsistencies in the results, particularly regarding saturation and sparsity across different layers. Some claims, such as the relationship between saturation and generalization, are not well-supported or are contradictory.\n3. **Limited Practical Impact:** While the findings are interesting, the practical impact is limited because the Swap model with bounded activations does not outperform the conventional model with ReLU, which is widely used in practice.\n4. **Scope of Analysis:** The analysis is limited to specific architectures without residual connections, which restricts the generalizability of the findings to more commonly used architectures like ResNets.\n5. **Reviewer Concerns:** Multiple reviewers expressed concerns about the clarity and rigor of the claims, as well as the novelty and significance of the contributions. These concerns were not fully addressed in the author responses.\n\n**Conclusion:**\nWhile the paper presents an interesting observation, the lack of a strong theoretical foundation, inconsistencies in the results, and limited practical impact make it unsuitable for acceptance at a top-tier conference. The paper would benefit from a more comprehensive theoretical analysis and a broader scope of experiments to strengthen its claims and demonstrate practical significance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-94tJCOo7OM",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning\" presents a novel approach by integrating Transformers with Monte-Carlo Tree Search (MCTS) in the context of offline reinforcement learning. The authors claim that this combination allows for more efficient sampling and improved performance over existing methods. However, the paper has several critical issues that need to be addressed before it can be considered for acceptance at a top-tier conference.\n\n### Detailed Reasoning:\n\n1. **Novelty and Contribution:**\n   - The idea of combining Transformers with MCTS is interesting and potentially impactful. However, as pointed out by the reviewers, the novelty is limited because the approach closely resembles existing frameworks like AlphaGo/AlphaZero, where the neural network architecture is simply replaced with a Transformer. This diminishes the originality of the contribution.\n\n2. **Empirical Evaluation:**\n   - The empirical evaluation is a significant weakness. The experiments are conducted solely on the SameGame, which is not a standard benchmark for evaluating reinforcement learning algorithms. This choice limits the generalizability and impact of the results.\n   - The comparisons are inadequate. The paper does not compare MCTransformer against a comprehensive set of baselines, including other MCTS-based methods with neural networks, or against other offline RL methods. This makes it difficult to assess the true effectiveness of the proposed method.\n\n3. **Clarity and Presentation:**\n   - The paper suffers from clarity issues, as noted by the reviewers. The exposition is unclear, and the figures and tables are not well-explained. This hinders the reader's ability to fully understand and evaluate the proposed method.\n   - There are several typographical and grammatical errors, which further detract from the paper's professionalism and readability.\n\n4. **Technical Correctness:**\n   - While the method appears to be technically sound, some claims are not well-supported by the empirical evidence provided. The lack of rigorous comparisons and the choice of a non-standard domain raise questions about the robustness and applicability of the results.\n\n5. **Reproducibility:**\n   - The paper does not provide source code, which is a significant drawback for reproducibility. Although the method is similar to existing frameworks, providing code would facilitate verification and further research.\n\n### Conclusion:\n\nGiven the limited novelty, inadequate empirical evaluation, and issues with clarity and presentation, the paper does not meet the standards of a top-tier conference. The idea has potential, but substantial revisions and additional experiments are necessary to strengthen the contribution and demonstrate its significance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "M6M8BEmd6dq",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning\" presents a novel approach to generating synthetic data with differential privacy guarantees. The authors propose a framework that avoids the privacy degradation issues associated with iterative training methods by using a one-shot approach to generate privatized embeddings. The paper is well-written, and the theoretical foundations are sound, as noted by the reviewers.\n\n**Strengths:**\n1. **Novelty and Motivation:** The paper introduces a novel approach to privacy-preserving data synthesis, which is well-motivated and theoretically grounded. The use of characteristic functions and adversarial re-weighting objectives is innovative.\n2. **Theoretical Guarantees:** The authors provide theoretical guarantees for their method, which is crucial for a paper in this domain.\n3. **Empirical Results:** The approach shows promising results in empirical evaluations, outperforming existing methods like DP-MERF and DP-GAN.\n\n**Weaknesses:**\n1. **Benchmarking and Comparisons:** The paper lacks comprehensive benchmarking against a broader set of baselines, such as PATE-GAN, CTGAN, PrivBayes, and others. This is a significant gap, as top-tier conferences expect thorough empirical validation against state-of-the-art methods.\n2. **Dataset Variety:** The empirical evaluations are limited to a few datasets. Including more diverse and complex datasets, such as CIFAR-10 for image data and additional tabular datasets, would strengthen the paper.\n3. **Clarity and Completeness:** Some aspects of the method, such as the privacy-preserving optimization and one-shot sampling strategy, are not clearly explained. Reviewer 2 and 3 have pointed out specific areas where the paper lacks clarity.\n4. **Complexity Analysis:** The paper does not provide a computational complexity analysis, which is important for understanding the feasibility and scalability of the proposed method.\n5. **Missing Related Work:** The paper omits several important references that could provide context and contrast for the proposed method.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that the paper addresses an important problem and presents a novel approach. However, they also highlight significant areas for improvement, particularly in empirical validation and clarity of the method. Reviewer 3 suggests that addressing these issues could elevate the paper to a higher standard.\n\n**Author Responses:**\nThe authors have not provided responses to the reviewers' comments, which leaves the concerns unaddressed. This lack of engagement with the feedback is a missed opportunity to clarify and strengthen the paper.\n\n**Conclusion:**\nWhile the paper presents a promising approach with strong theoretical underpinnings, the lack of comprehensive empirical validation, clarity in the methodology, and engagement with existing literature are significant drawbacks. For a top-tier conference, these issues are critical and need to be addressed before the paper can be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "FLr9RRqbwB-",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Batch Normalization and Bounded Activation Functions\" explores the impact of swapping the order of batch normalization and activation functions, specifically bounded activation functions like Tanh, on neural network performance. The authors claim that this swap leads to improved performance due to asymmetric saturation and increased sparsity.\n\n### Strengths:\n1. **Interesting Observation**: The paper presents an intriguing finding that swapping the order of batch normalization and bounded activation functions can lead to better performance, which is a non-trivial insight.\n2. **Empirical Analysis**: The authors conduct extensive experiments to support their claims, providing quantitative metrics for saturation, skewness, and sparsity.\n3. **Relevance**: The interaction between normalization and activation functions is a fundamental topic in deep learning, making the investigation relevant.\n\n### Weaknesses:\n1. **Lack of Theoretical Justification**: The paper lacks a strong theoretical foundation to explain why the observed phenomena occur. The claims about asymmetric saturation and sparsity enhancing generalization are not convincingly supported.\n2. **Limited Practical Impact**: The Swap model with bounded activations does not outperform the conventional model with ReLU, which limits the practical significance of the findings.\n3. **Inconsistent Observations**: The results are not consistent across all layers, and the paper does not adequately address why saturation and skewness differ in deeper layers.\n4. **Clarity and Rigor**: Several claims are not clearly articulated or rigorously justified. Reviewers have pointed out ambiguities and contradictions in the paper's arguments.\n5. **Generalization**: The analysis is limited to specific architectures without residual connections, which restricts the applicability of the findings to a broader range of models.\n\n### Reviewer Concerns:\n- Reviewers have expressed skepticism about the connection between asymmetric saturation and generalization performance.\n- There are questions about the applicability of the findings to architectures with residual connections and whether the observed benefits are due to accelerated training rather than improved generalization.\n- The paper's novelty is questioned, with some reviewers considering the contributions marginally significant or not novel.\n\n### Author Responses:\nThe authors have provided some clarifications in their responses, but the fundamental issues regarding theoretical justification and practical significance remain unaddressed.\n\n### Conclusion:\nWhile the paper presents an interesting observation, the lack of theoretical backing, limited practical impact, and inconsistencies in the results make it unsuitable for a top-tier conference. The paper would benefit from a more rigorous theoretical analysis and broader applicability to be considered for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "0OlEBibFa_g",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Detecting Out-of-Distribution Data with Semi-supervised Graph 'Feature' Networks\" presents an approach to out-of-distribution (OOD) detection using graph structures derived from object detection outputs. The authors claim that this method is competitive with state-of-the-art techniques, achieving high AUROC scores on the LSUN dataset. However, the paper has several critical issues that need to be addressed before it can be considered for publication in a top-tier conference.\n\n### Strengths:\n1. **Novelty in Approach**: The idea of using graph structures to represent images for OOD detection is interesting and potentially novel. It suggests a new direction for interpretable OOD detection.\n2. **Comprehensive Graph Embedding Comparisons**: The paper compares different graph embedding algorithms, which could be valuable for future research.\n\n### Weaknesses:\n1. **Clarity and Writing Quality**: All reviewers noted that the paper is poorly written, with grammatical errors, unclear notations, and missing details. This significantly affects the paper's readability and the ability to understand the proposed method.\n   \n2. **Lack of Rigorous Experimental Validation**: The paper does not compare its method against existing state-of-the-art OOD detection techniques. The experiments are limited to the LSUN dataset, and there is no evaluation on more commonly used datasets like CIFAR-10/100 or ImageNet. This lack of comprehensive evaluation makes it difficult to assess the true effectiveness of the proposed method.\n\n3. **Insufficient Detail for Reproducibility**: The paper lacks detailed descriptions of the methodology, making it challenging for others to reproduce the results. Critical components such as the feature extraction process and the complete inference pipeline are not adequately explained.\n\n4. **Assumptions and Limitations**: The method relies heavily on a pre-trained object detection network, which may not be suitable for all types of data, particularly those not covered by the training data of the object detector. This assumption limits the generalizability of the approach.\n\n5. **Lack of Theoretical and Empirical Support**: Some claims in the paper are not well-supported by theory or empirical results. The paper does not provide a thorough analysis of how the accuracy of the object detection model impacts the OOD detection performance.\n\n6. **Poorly Defined Concepts**: Terms like \"common sense\" and \"child cognition\" are used without rigorous definitions or references, which undermines the scientific rigor of the paper.\n\n### Conclusion:\nThe paper presents an interesting idea, but it is not ready for publication in its current form. The issues with clarity, experimental validation, and reproducibility are significant and need to be addressed. The paper would benefit from a thorough revision, including clearer writing, more comprehensive experiments, and a detailed description of the methodology. Given these considerations, the paper does not meet the standards of a top-tier conference at this time.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "MTsBazXmX00",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Target Propagation via Regularized Inversion\" presents a novel approach to target propagation (TP) by introducing a regularized inversion method. The authors aim to provide an alternative to backpropagation (BP) that is easily implementable in differentiable programming frameworks and potentially advantageous for training recurrent neural networks (RNNs) on long sequences.\n\n**Strengths:**\n1. **Novelty and Implementation:** The paper introduces a new variant of TP that is straightforward to implement using autograd systems, which is a practical contribution.\n2. **Theoretical Insights:** The paper provides some theoretical analysis, such as bounding the difference between TP and BP, which adds to the understanding of the proposed method.\n3. **Potential for RNNs:** The approach is particularly targeted at RNNs, which are known to suffer from vanishing and exploding gradient problems, suggesting a potential niche application.\n\n**Weaknesses:**\n1. **Lack of Strong Theoretical Guarantees:** Several reviewers noted the absence of strong theoretical guarantees that the proposed method offers significant advantages over BP. The theoretical analysis provided is not fully convincing or comprehensive.\n2. **Experimental Limitations:** The experimental results are not compelling. The performance on realistic datasets like CIFAR-10 is weak, and the experiments do not convincingly demonstrate the superiority of the proposed method over existing techniques.\n3. **Scope and Applicability:** The method requires weight transport, which limits its applicability, especially in contexts where optimization without weight transport is desirable.\n4. **Comparison with Existing Work:** The paper does not sufficiently compare the proposed method with existing TP variants or other optimization techniques for RNNs, such as Hessian-Free Optimization or feedback alignment.\n5. **Clarity and Misleading Claims:** Some parts of the paper are unclear or potentially misleading, such as the claims about the general applicability of the method to all neural networks and the relationship to Gauss-Newton methods.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting idea, it lacks the necessary theoretical and empirical support to justify its claims. The novelty is considered marginal, and the experimental results do not convincingly demonstrate the method's advantages. There are also concerns about the clarity and accuracy of some claims.\n\n**Author Responses:**\nThe authors have not provided responses that sufficiently address the reviewers' concerns, particularly regarding the theoretical guarantees and empirical validation.\n\n**Conclusion:**\nGiven the weaknesses identified, particularly the lack of strong theoretical and empirical support, the paper does not meet the standards of a top-tier conference. The contributions, while interesting, are not sufficiently significant or well-supported to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "h9O0wsmL-cT",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Regression with Label Differential Privacy\" presents a novel approach to ensuring label differential privacy in regression tasks. The authors propose a mechanism called \"randomized response on bins\" and claim its optimality under a given regression loss function. The paper is well-received by the reviewers, with several strengths and some weaknesses noted.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper introduces a new mechanism for label differential privacy in regression, which is a significant extension of existing work primarily focused on classification. The proposed method is theoretically grounded, with proofs of optimality provided.\n2. **Clarity and Writing:** Multiple reviewers commend the paper for its clear writing and organization, making it accessible and easy to follow.\n3. **Empirical Evaluation:** The paper includes extensive experiments on real-world datasets, demonstrating the efficacy of the proposed method compared to existing differential privacy mechanisms like Laplace and Exponential.\n4. **Theoretical Rigor:** The paper provides a theoretical analysis of the proposed mechanism, including its optimality under certain conditions.\n\n**Weaknesses:**\n1. **Comparison with Existing Work:** Some reviewers noted the lack of comparison with other label-DP methods, such as those by Ghazi et al. (2021). While the authors addressed this in their response, including such comparisons in the paper would strengthen the empirical evaluation.\n2. **Technical Novelty:** While the paper extends existing methods to regression, some reviewers feel that the contributions are not entirely novel, as aspects of the work exist in prior literature.\n3. **Practical Considerations:** Reviewers pointed out practical limitations, such as the assumption of a known prior distribution and the focus on label loss rather than overall generalization error.\n\n**Reviewer Scores and Comments:**\n- Reviewers generally rate the paper positively, with scores indicating that the paper is correct and makes a significant contribution. However, there are suggestions for improvement, particularly in empirical comparisons and addressing practical concerns.\n- Reviewer 4 gives a high score (8), emphasizing the novelty and correctness of the work.\n- Reviewer 5 acknowledges the paper's strengths but notes that the technical novelty is marginal compared to prior work.\n\n**Author Responses:**\nThe authors have addressed some reviewer concerns, particularly regarding empirical comparisons, and have been open to including additional discussions in the appendix.\n\n**Conclusion:**\nThe paper presents a novel and theoretically sound approach to label differential privacy in regression tasks. While there are some concerns about the novelty and practical applicability, the overall contribution is significant, and the paper is well-executed. The authors have shown willingness to address reviewer concerns, which adds to the paper's merit.\n\nFinal Decision: Accept"
    },
    {
      "paper_id": "QN_VgTeOYGl",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Data Leakage in Tabular Federated Learning\" addresses an important and timely issue in the field of federated learning (FL), specifically focusing on the vulnerability of tabular data to data leakage attacks. The authors propose a novel attack method, TabLeak, which is designed to overcome the unique challenges posed by tabular data, such as the presence of categorical features and the mixed discrete-continuous nature of the data.\n\n**Strengths:**\n\n1. **Relevance and Importance:** The topic is highly relevant as federated learning is increasingly being applied to sensitive domains like finance and healthcare, where tabular data is prevalent. Understanding and mitigating data leakage risks in these settings is crucial.\n\n2. **Novelty in Application:** While the techniques used (softmax relaxation, ensembling) are not novel in themselves, their application to the problem of data leakage in tabular federated learning is new. The paper identifies specific challenges in this domain and proposes a tailored solution.\n\n3. **Experimental Validation:** The paper provides a comprehensive experimental evaluation across multiple datasets, demonstrating the effectiveness of the proposed method. The results show a significant improvement over baseline methods, which supports the claims made by the authors.\n\n4. **Reproducibility:** The authors have made their source code available, which enhances the reproducibility of their results.\n\n**Weaknesses:**\n\n1. **Technical Novelty:** As noted by the reviewers, the underlying techniques (softmax relaxation, ensembling) are well-known and not particularly innovative. The paper does not introduce fundamentally new methods but rather adapts existing ones to a new context.\n\n2. **Writing and Clarity:** Reviewer 1 points out issues with the writing quality, which could hinder the reader's understanding. This includes grammatical errors and inconsistent use of tense, which detracts from the overall presentation.\n\n3. **Benchmarking and Baselines:** Reviewer 2 criticizes the choice of baselines, suggesting that more sophisticated methods could have been used for comparison. This raises questions about the robustness of the claimed improvements.\n\n4. **Claims and Justifications:** Reviewer 3 highlights that some claims about the challenges specific to tabular data are not fully justified with empirical evidence. This includes the assertion that the mix of continuous and discrete features leads to higher variance in reconstructions.\n\n**Overall Assessment:**\n\nThe paper addresses a significant problem in federated learning and proposes a method that shows promising results in experimental evaluations. However, the lack of technical novelty and the issues with writing and clarity are notable drawbacks. The paper could benefit from a more thorough justification of its claims and a stronger set of baseline comparisons.\n\nGiven the importance of the topic and the potential impact of the findings, the paper has merit. However, for a top-tier conference, the expectations for technical novelty and clarity are high. The paper falls short in these areas, which makes it difficult to justify acceptance without substantial revisions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "jZfksUBb3Zz",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Multi Task Learning of Different Class Label Representations for Stronger Models\" presents an interesting approach to improving image classification performance by introducing a novel auxiliary task using Binary Labels. The idea is to train a network on both the traditional one-hot labels and these new Binary Labels, which are large binary vectors assigned to each class. The authors claim that this multi-task learning approach can lead to higher accuracy, especially in challenging scenarios or when training data is limited.\n\n**Strengths:**\n1. **Novelty:** The concept of using Binary Labels as an auxiliary task is novel and adds a new dimension to label representation, which is an underexplored area.\n2. **Potential Impact:** The approach could offer a cost-effective way to improve model performance without requiring additional data or complex model architectures.\n3. **Empirical Results:** The paper reports improvements in accuracy across several datasets, suggesting that the method has practical benefits.\n\n**Weaknesses:**\n1. **Clarity and Explanation:** There are significant clarity issues, particularly regarding the generation and use of Binary Labels. The paper lacks a detailed explanation of the intuition behind why Binary Labels improve performance.\n2. **Experimental Rigor:** The experimental section is weak, with insufficient baselines and lack of exploration of different scenarios. The results are not robustly supported by a comprehensive set of experiments.\n3. **Reproducibility:** The paper lacks sufficient detail for reproducibility, including missing implementation details and unclear explanations of key components like Metabalance.\n4. **Limited Analysis:** There is a lack of analysis on when and why the proposed method works, and the paper does not sufficiently explore the impact of different hyperparameters or the randomness in Binary Label generation.\n5. **Related Work:** The discussion of related work is inadequate, missing connections to relevant literature on label transformations and multi-task learning.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the idea is interesting and potentially impactful, the paper falls short in terms of clarity, experimental rigor, and depth of analysis. The technical contributions are seen as limited, and the empirical improvements, though present, are not substantial enough to meet the standards of a top-tier conference without further exploration and validation.\n\n**Conclusion:**\nGiven the significant clarity issues, lack of experimental rigor, and insufficient analysis, the paper does not meet the high standards required for acceptance at a top-tier conference. The authors are encouraged to address these weaknesses, particularly by providing more detailed explanations, conducting more comprehensive experiments, and improving the discussion of related work.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "pRCMXcfdihq",
      "source": "ICLR 2023",
      "true_decision": "Accept",
      "prediction": "Accept",
      "correct": true,
      "response": "The paper titled \"Protein Sequence and Structure Co-Design with Equivariant Translation\" presents a novel approach to protein design, focusing on the co-design of protein sequences and structures. The proposed method, ProtSeed, introduces a trigonometry-aware encoder and a roto-translation equivariant decoder, which iteratively updates protein sequences and structures, purportedly offering significant improvements in inference speed and design fidelity over existing methods.\n\n**Strengths:**\n1. **Novelty and Approach:** The paper introduces a novel approach to protein design by integrating sequence and structure co-design, which is a significant step forward from existing autoregressive and diffusion models. The use of a trigonometry-aware encoder and a roto-translation equivariant decoder is innovative.\n2. **Empirical Results:** The paper demonstrates strong empirical results, outperforming state-of-the-art methods across multiple tasks. The reduction in inference time is particularly noteworthy.\n3. **Clarity and Presentation:** The paper is generally well-written, with a clear presentation of the methodology and results. The authors have addressed reviewer comments effectively, improving the clarity of their claims and methodology.\n\n**Weaknesses:**\n1. **Technical Novelty:** Several reviewers noted that while the approach is novel, it builds significantly on existing methods, such as those used in AlphaFold2. The novelty is somewhat incremental, with some reviewers questioning the extent of the innovation.\n2. **Reproducibility Concerns:** There is a lack of detailed hyperparameter and training information, and the authors have not committed to releasing code and models, which could hinder reproducibility.\n3. **Evaluation Metrics:** Some reviewers raised concerns about the evaluation metrics used, suggesting that they may not fully capture the functional performance of the designed proteins. The lack of functional-oriented evaluations is a notable gap.\n4. **Discrepancies and Comparisons:** There are discrepancies in the reported results compared to prior work, and the comparisons to some recent models are lacking. The absence of ablation studies makes it difficult to assess the contribution of individual components of the model.\n\n**Reviewer Consensus:**\nThe reviewers generally acknowledge the paper's contributions and empirical performance but express concerns about the novelty, reproducibility, and some methodological claims. The paper's strengths in empirical results and methodological clarity are balanced by weaknesses in technical novelty and evaluation comprehensiveness.\n\n**Final Decision: Accept**\n\nDespite the noted weaknesses, the paper presents a significant advancement in the field of protein design, with strong empirical results and a novel approach that addresses a critical challenge in bioengineering. The authors have shown responsiveness to reviewer feedback, improving the clarity and robustness of their claims. The paper's contributions, while building on existing methods, offer valuable insights and advancements that merit acceptance at a top-tier conference."
    },
    {
      "paper_id": "oiZJwC_fyS",
      "source": "ICLR 2022",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper titled \"Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes\" presents a novel approach to neural network compression using tropical geometry. The paper introduces a theoretical framework and proposes a method for compressing neural networks, specifically focusing on networks with ReLU activations. The reviewers have provided a range of feedback, highlighting both strengths and weaknesses of the paper.\n\n### Strengths:\n1. **Theoretical Contribution**: The paper provides a mathematically solid analysis of neural networks from a tropical geometry perspective. The introduction of approximation bounds based on Hausdorff distance is novel and could inspire further theoretical research.\n2. **Novelty**: The approach of using tropical geometry for neural network compression is relatively new and unexplored, which adds to the paper's novelty.\n3. **Clarity in Theoretical Presentation**: The background on tropical geometry is well-explained, making the theoretical aspects accessible to readers who may not be familiar with the domain.\n\n### Weaknesses:\n1. **Empirical Evaluation**: The empirical evaluation is limited to small-scale datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and outdated architectures (LeNet, VGG). This raises concerns about the scalability and practical applicability of the proposed method to modern, large-scale networks.\n2. **Comparison with State-of-the-Art**: The paper lacks a comprehensive comparison with state-of-the-art pruning techniques. The methods compared are not the most recent, which weakens the empirical claims.\n3. **Practical Utility of Theoretical Bounds**: The usefulness of the theoretical bounds in practical scenarios is questioned, as the bounds do not seem to align with expected outcomes (e.g., error not decreasing with more weights).\n4. **Reproducibility**: The lack of pseudo-code or detailed experimental setup makes it difficult to reproduce the results, which is a significant drawback for a top-tier conference submission.\n5. **Overclaiming in Abstract**: Some claims in the abstract are not well-supported by the experimental results, particularly regarding the competitiveness against modern pruning techniques.\n\n### Author Responses:\nThe authors have provided responses to some of the concerns, but the responses do not fully address the empirical limitations or the lack of comparison with modern techniques. The authors acknowledge that the experiments are proof-of-concept, which suggests that the method may not yet be ready for practical application.\n\n### Conclusion:\nWhile the paper presents an interesting theoretical contribution and a novel approach to neural network compression, the empirical evaluation is not sufficiently robust for a top-tier conference. The lack of experiments on modern architectures and datasets, combined with insufficient comparison to state-of-the-art methods, limits the paper's impact and applicability. The theoretical insights are valuable, but the practical significance needs further development and validation.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "mNk7mgWZcJa",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Scrunch: Preventing sensitive property inference through privacy-preserving representation learning\" presents a method for privacy-preserving representation learning in the context of Machine Learning as a Service (MLaaS). The authors propose splitting a neural network into two parts, with the client using the first part to encode data before sending it to the server for inference. The paper aims to maintain predictive performance while minimizing information leakage.\n\nUpon reviewing the comments from the three reviewers, several critical issues emerge:\n\n1. **Novelty and Incremental Contribution**: \n   - Reviewer 1 and Reviewer 2 both highlight that the idea of splitting a network for privacy is not novel and has been explored in prior work, such as split learning. The paper does not sufficiently differentiate itself from existing methods or provide a significant advancement over the state of the art.\n   - The paper's novelty is further questioned due to its lack of mention of related work in split learning, which is a significant oversight.\n\n2. **Technical Flaws and Threat Model**:\n   - Reviewer 2 points out severe flaws in the proposed approach, particularly regarding the threat model. The server's ability to reconstruct the encoder and the lack of defense against privacy leakage during training are significant issues. The assumption that the server will honestly implement the center loss is unrealistic in a curious server scenario.\n   - Reviewer 3 also questions the security model and the potential for information leakage through encoded features, especially in time-series data or NLP applications.\n\n3. **Experimental Evaluation**:\n   - The experimental evaluation is limited, as noted by Reviewer 1 and Reviewer 2. The paper only uses two datasets and compares against a minimal set of baselines. The choice of datasets and models is simplistic, and the evaluation lacks depth and breadth.\n   - Reviewer 2 suggests that more sophisticated architectures and challenging settings should be tested to demonstrate the method's effectiveness.\n\n4. **Clarity and Quality**:\n   - While the paper is generally clear, there are grammatical errors and ambiguous statements that affect its readability and the clarity of its contributions, as noted by Reviewer 1 and Reviewer 2.\n\n5. **Reproducibility**:\n   - Although the paper provides some hyperparameter settings, it lacks details on optimizers and training epochs, which are necessary for full reproducibility.\n\nOverall, the paper presents an idea that is not sufficiently novel or technically robust for a top-tier conference. The experimental evaluation is inadequate, and the paper does not convincingly address the privacy concerns it aims to solve. The technical flaws, particularly in the threat model, further undermine the paper's contributions.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "IQM-3_Tzldw",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning\" presents a novel approach to address the challenges of client drift and introduces the concept of period drift in federated learning (FL). The proposed method, FedPA, utilizes a parameterized aggregator within a meta-learning framework to improve model aggregation.\n\n**Strengths:**\n1. The paper identifies a new source of drift, termed period drift, which is an important contribution to the understanding of challenges in FL.\n2. The proposed method, FedPA, is innovative in its approach to use meta-learning for aggregation, which could potentially lead to better handling of non-iid data in FL.\n3. The paper includes experimental results that demonstrate the potential of FedPA to outperform conventional baselines in certain settings.\n\n**Weaknesses:**\n1. **Lack of Theoretical and Experimental Justification:** The distinction between client drift and period drift is not well-supported theoretically or experimentally. This is a critical gap, as the concept of period drift is central to the paper's motivation.\n2. **Comparison with Related Work:** The paper does not adequately compare FedPA with similar methods, such as those based on knowledge distillation (e.g., FedET and DS-FL). This limits the ability to assess the novelty and effectiveness of the proposed method.\n3. **Experimental Limitations:** The experiments are conducted on relatively simple datasets and models, which may not convincingly demonstrate the method's effectiveness in more complex, real-world scenarios. The lack of results on more challenging datasets like CIFAR-100 and Stack Overflow is a significant limitation.\n4. **Proxy Dataset Assumption:** The reliance on a proxy dataset for training the aggregator is a strong assumption that may not be feasible in many practical FL scenarios, potentially limiting the applicability of the method.\n5. **Reproducibility and Clarity:** The paper lacks open-source code, which hinders reproducibility. Additionally, there are issues with clarity and writing quality, including grammatical errors and inconsistent citation formats.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting idea, it lacks sufficient theoretical grounding, experimental validation, and clarity. The novelty of the contributions is also questioned, with some aspects overlapping with existing work. The paper's assumptions and experimental setup raise concerns about its applicability and robustness.\n\n**Conclusion:**\nGiven the significant weaknesses identified, particularly the lack of strong theoretical and experimental support for the proposed method, the reliance on a proxy dataset, and the insufficient comparison with related work, the paper does not meet the standards of a top-tier conference. The contributions, while potentially interesting, are not sufficiently novel or well-validated to warrant acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "L6CKiPH3hI",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Enriching Online Knowledge Distillation with Specialist Ensemble\" presents a framework for online knowledge distillation that aims to enhance teacher diversity through a label prior shift, and proposes a new aggregation strategy. The paper has received mixed reviews from the reviewers, with several strengths and weaknesses highlighted.\n\n### Strengths:\n1. **Clarity and Structure**: The paper is generally well-written and structured, making it easy to follow. The motivation for the work is clear, and the organization is logical.\n2. **Novelty in Framework**: While many component ideas are not new, the integration of these ideas into a cohesive framework for online distillation is considered novel by some reviewers.\n3. **Empirical Analysis**: The paper includes a thorough empirical analysis, demonstrating improvements in student performance in terms of accuracy and calibration.\n4. **Ablation Studies**: The inclusion of ablation studies is appreciated, as they help in understanding the impact of different components of the proposed method.\n\n### Weaknesses:\n1. **Limited Novelty**: Several reviewers point out that the novelty of the contributions is limited, as the paper combines existing methods without a strong justification for why these specific methods are the best choice.\n2. **Experimental Comparisons**: The paper lacks comparisons with some recent and relevant methods, which raises concerns about the significance of the reported improvements. The choice of baselines is not well-justified, and some results are inconsistent with previous works.\n3. **Specialization and Aggregation**: There are concerns about the specialization of teachers and the aggregation strategy. The reviewers suggest that the averaging of teacher predictions might be naive and that more sophisticated weighting could be beneficial.\n4. **Reproducibility and Support**: Some claims in the paper are not well-supported by theory or empirical results, and there are concerns about the reproducibility of the results due to inconsistencies with existing literature.\n\n### Additional Concerns:\n- The paper does not sufficiently address why the proposed method is particularly suited for online distillation as opposed to offline distillation.\n- There are minor issues with typos and clarity in some sections, which could be distracting.\n\n### Conclusion:\nWhile the paper presents an interesting approach to online knowledge distillation, the limited novelty, lack of comprehensive experimental comparisons, and insufficient support for some claims make it unsuitable for a top-tier conference at this stage. The authors are encouraged to address these issues, particularly by providing more robust experimental comparisons and justifying the choice of methods used in their framework.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "ryxB2lBtvH",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Learning to Coordinate Manipulation Skills via Skill Behavior Diversification\" presents a novel approach to hierarchical reinforcement learning (HRL) in multi-agent settings, focusing on skill coordination for complex manipulation tasks. The authors propose a modular framework that first trains sub-skills for each end-effector and then coordinates these skills using a meta-policy. The paper is evaluated on tasks involving bimanual manipulation and multi-agent coordination, demonstrating superior performance over existing baselines.\n\n**Strengths:**\n\n1. **Novelty and Impact:** The paper addresses an important problem in HRL and multi-agent systems by proposing a method that leverages skill behavior diversification for coordination. This approach could have significant implications for robotic manipulation and multi-agent coordination tasks.\n\n2. **Empirical Results:** The proposed method outperforms baselines in several challenging tasks, indicating its effectiveness. The inclusion of videos and code enhances the reproducibility and transparency of the research.\n\n3. **Clarity and Presentation:** Despite some noted issues, the paper is generally well-presented, making the methodology understandable to readers familiar with the field.\n\n**Weaknesses:**\n\n1. **Notation and Clarity:** Reviewer 1 points out inconsistencies in notation and a lack of clarity in the diagrams, which could hinder understanding. This is a critical issue for a top-tier conference, where clarity is paramount.\n\n2. **High Variance in Results:** The high variance in performance, as noted by Reviewer 1, raises concerns about the robustness of the method. More seeds or additional analysis could strengthen the empirical claims.\n\n3. **Discussion of Alternatives:** The paper lacks a thorough discussion of alternative multi-agent methods, as highlighted by Reviewer 1. This limits the contextual understanding of the proposed method's advantages and limitations.\n\n4. **Scientific Insight:** Reviewer 2 expresses a lack of scientific insight, questioning the novelty of applying temporal abstraction in a multi-agent setting. This suggests that the paper might not sufficiently differentiate its contributions from existing work.\n\n5. **Predefined Subtasks:** Reviewer 3 criticizes the reliance on predefined subtasks, which limits the general applicability of the approach. This is a significant limitation for a method intended to be broadly applicable.\n\n6. **Technical Details:** There are missing technical details, such as the size of the latent skill embedding, which are crucial for understanding and reproducing the results.\n\n**Author Responses:**\n\nThe authors have the opportunity to address these concerns, particularly regarding notation, variance in results, and the discussion of alternative methods. However, the responses provided are not available for evaluation here.\n\n**Conclusion:**\n\nWhile the paper presents an interesting approach with promising results, the concerns raised by the reviewers are significant, particularly regarding clarity, robustness, and the general applicability of the method. For a top-tier conference, these issues need to be thoroughly addressed to ensure the work meets the high standards expected. Without evidence of substantial revisions and clarifications, the paper may not yet be ready for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "rJxe3xSYDS",
      "source": "ICLR 2020",
      "true_decision": "Accept",
      "prediction": "Reject",
      "correct": false,
      "response": "The paper \"Extreme Classification via Adversarial Softmax Approximation\" presents an innovative approach to address the computational challenges associated with extreme classification by proposing an adversarial sampling mechanism for negative samples. The paper claims significant improvements in training efficiency and provides theoretical backing for the proposed method.\n\n**Strengths:**\n\n1. **Novelty and Contribution:** The paper introduces an adversarial sampling mechanism that is both novel and potentially impactful in the field of extreme classification. The approach is theoretically sound, with a proof that adversarial sampling minimizes gradient variance.\n\n2. **Clarity and Presentation:** According to Reviewer 1, the paper is well-written and accessible even to non-experts, which is a significant strength for a technical paper.\n\n3. **Experimental Results:** The paper demonstrates a reduction in training time by an order of magnitude on large-scale datasets, which is a substantial improvement over existing methods.\n\n**Weaknesses:**\n\n1. **Lack of Contextualization:** Reviewer 2 points out that the paper does not adequately discuss its contributions in the context of existing literature. There are several relevant works on non-uniform negative sampling that the authors have not sufficiently addressed. This omission weakens the paper's positioning within the broader research landscape.\n\n2. **Incomplete Experimental Evaluation:** Reviewer 3 highlights that the experimental evaluation is not comprehensive. The paper does not compare its method against some state-of-the-art methods like Slice and DiSMEC, which are relevant benchmarks in this domain. Additionally, the paper does not evaluate performance on smaller datasets or consider the impact on tail-labels, which are important aspects in extreme classification.\n\n3. **Typographical and Presentation Issues:** There are minor typographical errors and inconsistencies in the paper, as noted by Reviewer 2, which detract from the overall quality of the presentation.\n\n4. **Scope and Generalization:** The paper does not explore the potential extension of the proposed method to multi-label settings, which limits its applicability and relevance to a broader range of problems.\n\n**Conclusion:**\n\nWhile the paper presents a novel approach with promising results, the lack of comprehensive experimental evaluation and insufficient contextualization within existing literature are significant drawbacks. The paper does not convincingly demonstrate its superiority over state-of-the-art methods, nor does it explore the broader applicability of its approach. These issues are critical for a top-tier conference, where the standards for novelty, thoroughness, and impact are exceptionally high.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Jdj0fZhswJC",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics\" addresses an important problem in game theory and learning dynamics, focusing on the average-case performance of q-replicator dynamics (QRD) in games. The paper aims to go beyond convergence to Nash equilibria by evaluating the quality of equilibria reached through these dynamics.\n\n**Strengths:**\n1. **Relevance and Importance:** The problem tackled is significant in the field of game theory and learning dynamics, as it addresses both convergence and the quality of equilibria, which are crucial for practical applications.\n2. **Clarity and Writing:** The paper is generally well-written and easy to follow, with clear explanations of the algorithms, techniques, and results.\n3. **Correctness:** All reviewers agree that the claims and statements are well-supported and correct.\n4. **Motivation:** The paper provides a strong motivation for studying the average-case performance of learning dynamics, which is a challenging and underexplored area.\n\n**Weaknesses:**\n1. **Incremental Contribution:** Several reviewers, particularly Reviewer 2 and Reviewer 3, express concerns about the novelty and significance of the contributions. The results are seen as incremental, particularly in relation to prior work (PP16), and the paper does not sufficiently differentiate itself from this existing research.\n2. **Scope and Generalization:** The results are limited to symmetric 2x2 coordination games, which is a very restricted class. There is a lack of discussion on how the approach could be generalized to more complex or higher-dimensional games.\n3. **Partial Results:** The results are described as partial and preliminary, with some aspects, such as the monotonicity of regions of attraction, lacking formal proofs.\n4. **Audience and Fit:** Reviewer 4 notes that the paper might be better suited for a conference with a more focused audience, such as EC, given the specific nature of the results.\n\n**Reviewer Consensus:**\n- There is a split in the reviewers' recommendations. Reviewer 4 recommends acceptance, albeit with a reduced score, while Reviewer 2 and Reviewer 3 lean towards rejection due to the incremental nature of the work and limited scope.\n- Reviewer 1 acknowledges the importance of the question but notes that the formal results are partial.\n\n**Author Responses:**\n- The authors have addressed some concerns, particularly regarding the comparison with PP16, which led to an improved score from Reviewer 2. However, the fundamental issues of scope and generalization remain.\n\n**Conclusion:**\nWhile the paper addresses an important problem and is well-written, the contributions are seen as incremental and limited in scope. The results are restricted to a narrow class of games, and there is insufficient evidence of generalization to broader contexts. Given the standards of a top-tier conference, where significant novelty and broad applicability are expected, the paper does not meet the necessary criteria for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "kL67fyKb6A",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Online black-box adaptation to label-shift in the presence of conditional-shift\" addresses an important problem in machine learning, particularly in the context of real-world applications where distribution shifts are common. The authors propose heuristics to improve the performance of existing methods under more complex distribution shifts that include both label and conditional shifts. However, several critical issues have been identified by the reviewers, which need to be considered in the decision-making process.\n\n### Strengths:\n1. **Relevance and Motivation**: The problem of distribution shifts is well-motivated and relevant to real-world applications. The paper attempts to extend existing methods to handle more complex scenarios, which is a valuable direction.\n2. **Novelty**: The paper explores the adaptation of label shift methods to scenarios involving additional conditional shifts, which is a novel contribution.\n3. **Clarity in Empirical Results**: The empirical results are presented clearly, and the paper is honest about the inconclusive nature of some findings.\n\n### Weaknesses:\n1. **Lack of Theoretical Foundation**: The proposed heuristics lack theoretical justification, which is a significant drawback for a top-tier conference. Theoretical insights are crucial for understanding the conditions under which the methods will work.\n2. **Empirical Evaluation**: The empirical evaluation is limited in scope. The experiments do not convincingly demonstrate significant improvements over existing methods, and the choice of baselines is not comprehensive.\n3. **Clarity and Completeness**: Several sections of the paper are not self-contained, requiring readers to refer to previous work for understanding. Key concepts and methods are not clearly defined, which affects the paper's clarity.\n4. **Reproducibility**: Due to the lack of detailed explanations and the absence of code, reproducibility is a concern.\n5. **Reviewer Consensus**: All reviewers have pointed out significant issues with the paper, particularly regarding the lack of theoretical support and the limited empirical contributions.\n\n### Conclusion:\nWhile the paper addresses an important problem and proposes novel ideas, the lack of theoretical grounding, limited empirical validation, and issues with clarity and reproducibility make it unsuitable for acceptance at a top-tier conference. The paper appears to be in a preliminary form and would benefit from further development, including a more comprehensive empirical study and theoretical analysis.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "WVZQa2QYJN",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability\" presents a new dataset for precipitation nowcasting, which includes 3D radar echo observations and covers multiple climate zones in the European part of Russia. The dataset is evaluated using several baseline models, and the authors claim it could serve as a benchmark for future research in this area.\n\n**Strengths:**\n\n1. **Dataset Richness and Coverage:** The dataset is comprehensive, covering multiple climate zones and including a variety of meteorological measurements at different altitudes. This could potentially provide a more detailed understanding of precipitation patterns compared to existing datasets.\n\n2. **Public Availability:** The dataset and code are made publicly available, which enhances reproducibility and encourages further research in the community.\n\n3. **Baseline Evaluation:** The paper evaluates several baseline models on the dataset, providing a starting point for future research.\n\n**Weaknesses:**\n\n1. **Lack of Methodological Novelty:** The paper does not introduce any novel methods or significant advancements in the field of precipitation nowcasting. The focus is primarily on the dataset, which, while valuable, may not meet the novelty standards of a top-tier conference.\n\n2. **Baseline Models:** The baseline models used for evaluation are considered relatively simple, and there is a lack of exploration of more advanced models, such as transformer architectures, which could provide deeper insights into the dataset's potential.\n\n3. **Limited Empirical Validation:** While the dataset is claimed to be useful for various machine learning tasks, the paper does not provide empirical validation for these claims beyond precipitation nowcasting.\n\n4. **Evaluation Metrics:** The use of MSE as the sole evaluation metric is insufficient for capturing the full accuracy and reliability of nowcasting models. More comprehensive metrics should be considered.\n\n5. **Comparison with Existing Datasets:** Although the dataset is rich, reviewers noted that it might not be significantly different from existing datasets to warrant publication in a top-tier conference without additional methodological contributions.\n\n**Conclusion:**\n\nWhile the dataset presented in the paper is a valuable contribution to the field of precipitation nowcasting, the lack of methodological novelty and limited empirical validation of claims make it insufficient for acceptance at a top-tier conference. The paper would be better suited for a journal or conference focused on dataset contributions or applied meteorology, where the dataset's impact on the community could be more appropriately recognized.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "HlRfoQDDj-V",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Proximal Validation Protocol\" proposes a novel approach to constructing validation sets in machine learning by using data augmentation and a distributional-consistent sampling algorithm. The authors aim to address the trade-off between using data for training versus validation, which is a significant issue in machine learning model evaluation.\n\n**Strengths:**\n1. **Novelty and Importance:** The paper addresses a relevant and underexplored problem in machine learning, which is the construction of validation sets without compromising the training data. This is a practical issue, especially in real-world applications where data is limited.\n2. **Proposed Method:** The idea of using data augmentation and a novel sampling strategy to create a validation set is innovative. It allows for the full use of the training data while still providing a mechanism for model evaluation.\n3. **Empirical Evaluation:** The paper presents empirical results across multiple data modalities (images, text, and tabular data), suggesting that the proposed method can improve model evaluation.\n\n**Weaknesses:**\n1. **Empirical Evaluation Concerns:** Several reviewers noted that the datasets used for evaluation are small and may not fully support the claims. The lack of experiments on larger, more diverse datasets like ImageNet is a significant limitation.\n2. **Clarity and Presentation:** The paper's presentation, particularly in the experiments section, is unclear. Important metrics and results are not well-defined, making it difficult to assess the validity of the claims.\n3. **Theoretical Justification:** The paper lacks a theoretical foundation to support the proposed method. Without theoretical guarantees, it is challenging to trust that the proximal validation set is representative of the data distribution.\n4. **Generalization and Robustness:** The method seems tailored for classification tasks and may not generalize well to other settings like regression or unsupervised learning. Additionally, the robustness of the method to different data augmentation techniques and hyperparameter choices is not thoroughly evaluated.\n5. **Evaluation Metrics:** The metrics used to evaluate the method (test-validation gap and variance) are not entirely convincing. As one reviewer pointed out, these metrics could be manipulated without truly representing the data distribution.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper addresses an important problem and proposes a novel solution, the empirical evaluation is insufficient, and the lack of theoretical support is a significant drawback. The clarity of the paper is also a concern, as it hinders the understanding and assessment of the contributions.\n\n**Conclusion:**\nGiven the weaknesses identified, particularly the lack of robust empirical evidence and theoretical justification, the paper does not meet the standards of a top-tier conference. The novelty and importance of the problem are acknowledged, but the execution and support for the claims are lacking.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Dy8gq-LuckD",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Recognizing and overcoming the greedy nature of learning in multi-modal deep neural networks\" addresses an important issue in multi-modal learning, specifically the tendency of models to rely on a single modality, which can hinder generalization. The authors propose a novel metric, conditional learning speed, as a proxy for conditional utilization rate, and introduce a balanced multi-modal learning algorithm to mitigate this issue.\n\n**Strengths:**\n1. **Relevance and Interest:** The topic is highly relevant and interesting, as the imbalance in modality utilization is a known challenge in multi-modal learning.\n2. **Novelty:** The introduction of conditional learning speed as a proxy for conditional utilization rate is a novel contribution.\n3. **Clarity:** The paper is generally well-written and easy to follow, making the problem and proposed solution clear.\n\n**Weaknesses:**\n1. **Experimental Validation:** Several reviewers, including Reviewer 2 and Reviewer 4, express concerns about the robustness and clarity of the experimental validation. The results are not statistically significant across all datasets, and there are inconsistencies in the reported results (e.g., changes in Table 1 without explanation).\n2. **Theoretical Support:** The paper lacks strong theoretical backing for the proposed hypothesis and method. Reviewers 3 and 6 highlight the absence of theoretical analysis to support the empirical observations.\n3. **Comparative Analysis:** The paper does not adequately compare its method with state-of-the-art approaches addressing similar issues, as noted by Reviewers 6 and 7. This lack of comparison makes it difficult to assess the significance of the contributions.\n4. **Dataset Limitations:** The experiments are limited to visual-only datasets, which may not fully capture the challenges of multi-modal learning involving more diverse modalities, such as audio-visual datasets.\n5. **Parameter Analysis:** There is insufficient analysis of the parameters involved in the proposed method, such as the imbalance parameter, which could significantly impact performance.\n\n**Reviewer Consensus:**\nThe majority of reviewers recommend rejection, citing insufficient experimental validation, lack of theoretical support, and inadequate comparison with existing methods. While the paper addresses an interesting problem and proposes a novel approach, the current version does not meet the standards of a top-tier conference due to these significant weaknesses.\n\n**Final Decision: Reject**\n\nThe paper requires substantial improvements in experimental validation, theoretical analysis, and comparative evaluation to be considered for acceptance in a top-tier conference."
    },
    {
      "paper_id": "u6KhE9fapjX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"CAT: Collaborative Adversarial Training\" proposes a novel framework for adversarial training by collaboratively training two models using different adversarial training methods. The authors claim that this approach improves robustness and accuracy, achieving state-of-the-art results on CIFAR-10 under the Auto-Attack benchmark.\n\n**Strengths:**\n1. **Intuitive Approach:** The idea of leveraging the differences in robustness between models trained with different adversarial methods is straightforward and easy to understand.\n2. **Experimental Results:** The paper reports improved robustness on CIFAR-10 and CIFAR-100, which are standard benchmarks in the field.\n3. **Clarity:** The paper is generally well-written and easy to follow.\n\n**Weaknesses:**\n1. **Lack of Theoretical Justification:** The paper lacks a theoretical analysis explaining why the collaborative approach leads to improved robustness. This is a significant gap, especially for a top-tier conference where theoretical insights are highly valued.\n2. **Computational Cost:** The proposed method doubles the computational resources required, which is a critical drawback given the already high cost of adversarial training.\n3. **Novelty Concerns:** The idea of combining multiple defenses is not entirely new, and the paper does not adequately differentiate itself from existing ensemble methods. Several reviewers noted the lack of comparison with related work and missing citations.\n4. **Limited Evaluation:** The experiments are limited to small-scale datasets (CIFAR-10 and CIFAR-100) and do not include larger datasets like ImageNet, which would provide stronger evidence of the method's effectiveness.\n5. **Missing Baselines and Ablations:** The paper does not compare against some strong baselines, such as training a single model with multiple attacks, and lacks important ablation studies to understand the contribution of each component of the proposed method.\n\n**Reviewer Consensus:**\nThe reviewers generally agree that while the paper presents an interesting idea, it lacks novelty and theoretical backing. The empirical results, although promising, are not sufficiently comprehensive or compared against a wide range of baselines. The increased computational cost without a clear justification of the benefits further weakens the paper's contribution.\n\n**Conclusion:**\nGiven the lack of theoretical analysis, limited novelty, and insufficient empirical evaluation, the paper does not meet the high standards required for a top-tier conference. The authors are encouraged to address these issues, particularly by providing a theoretical foundation for their method, expanding their experiments to larger datasets, and including more comprehensive comparisons with existing methods.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "-4DiyBMgv9m",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Identifying Phase Transition Thresholds of Permuted Linear Regression via Message Passing\" presents an investigation into the phase transition thresholds in permuted linear regression problems using a message-passing framework. The paper is evaluated based on several criteria, including clarity, novelty, technical correctness, and empirical validation.\n\n### Detailed Reasoning:\n\n1. **Clarity and Presentation:**\n   - Multiple reviewers have pointed out that the paper is poorly written, with significant issues in clarity and organization. The notation is heavy and not self-contained, requiring readers to be familiar with prior works to understand the content fully. This lack of clarity makes the paper difficult to follow, especially for those not already familiar with the specific literature.\n   - There are numerous grammatical errors and typos throughout the paper, which further detract from its readability and professionalism.\n\n2. **Technical Novelty and Significance:**\n   - The paper's contributions are seen as incremental rather than groundbreaking. The techniques used are heavily inspired by prior work, particularly the work of Semerjian et al. (2020) and Zhang et al. (2020). The novelty lies in the approximation used to compute phase transitions in the non-oracle setting, but this is not sufficiently highlighted or justified.\n   - The assumption of a known signal matrix in the oracle case is considered unrealistic, limiting the practical significance of the results.\n\n3. **Correctness and Support for Claims:**\n   - There are concerns about the correctness and support for some of the paper's claims. For instance, the claim of identifying precise phase transition thresholds is contradicted by the approximation methods used.\n   - The theoretical results are asymptotic, while the empirical results are for finite samples, leading to a mismatch between theory and practice.\n\n4. **Empirical Validation:**\n   - The empirical results are not well-aligned with the theoretical predictions, and the paper lacks sufficient numerical experiments to support its claims. The presentation of empirical results is also criticized for being unclear and not rigorous.\n\n5. **Reproducibility:**\n   - The paper does not provide code or sufficient details to ensure the reproducibility of its results, which is a significant drawback for a top-tier conference publication.\n\n6. **Overall Assessment:**\n   - While the paper addresses an interesting problem and provides some novel insights, the issues with clarity, technical rigor, and empirical validation are too significant to overlook. The paper requires substantial revision and reorganization to meet the standards of a top-tier conference.\n\nGiven these considerations, the paper is not ready for publication in its current form.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "98ntbCuqf4i",
      "source": "ICLR 2021",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"MQES: Max-Q Entropy Search for Efficient Exploration in Continuous Reinforcement Learning\" introduces a novel exploration strategy for continuous reinforcement learning, leveraging the concept of Max-Q Entropy Search (MQES). The paper aims to address the exploration-exploitation dilemma by formulating an exploration policy that maximizes information about the globally optimal distribution of the Q function, considering both epistemic and aleatoric uncertainties.\n\n**Strengths:**\n\n1. **Novelty and Contribution**: The paper presents a novel approach by integrating information-theoretic principles with distributional reinforcement learning to handle continuous action spaces. This is a significant contribution as most existing methods focus on discrete action spaces.\n\n2. **Theoretical Foundation**: The paper is grounded in a solid theoretical framework, attempting to maximize mutual information to guide exploration.\n\n3. **Empirical Results**: The empirical evaluations on Mujoco environments show that MQES outperforms some state-of-the-art algorithms, indicating the potential effectiveness of the proposed method.\n\n**Weaknesses:**\n\n1. **Clarity and Presentation**: Multiple reviewers have pointed out that the paper is difficult to follow due to unclear explanations, especially regarding the parameterization of policies and the derivation of key equations. This lack of clarity hinders the reader's ability to fully understand and evaluate the proposed method.\n\n2. **Experimental Evaluation**: The experiments are criticized for being preliminary and lacking in detail. Important aspects such as hyperparameter settings and ablation studies are missing, which are crucial for assessing the robustness and generalizability of the method. Additionally, the choice of baselines is limited, and comparisons with more relevant methods like VIME are absent.\n\n3. **Impact of Hyperparameters**: The impact of hyperparameters on the performance of MQES is not thoroughly discussed or analyzed, which is a significant oversight given their potential influence on the results.\n\n4. **Significance of Results**: While the paper claims that MQES outperforms other methods, the improvements over DSAC are not substantial in many cases. The significance of these results is further questioned due to the limited number of seeds used in evaluations.\n\n5. **Conceptual Concerns**: There are concerns about the applicability of the method in sparse-reward settings, which are often the primary motivation for advanced exploration strategies. The reliance on Q* to guide exploration might limit its effectiveness in such scenarios.\n\n6. **Writing Quality**: The paper suffers from poor writing quality, with vague statements and inconsistent notation, making it challenging to discern the main contributions and understand the methodology.\n\n**Conclusion:**\n\nWhile the paper presents an interesting and potentially impactful idea, the current presentation and evaluation are insufficient for a top-tier conference. The lack of clarity, incomplete experimental evaluation, and limited comparison with relevant baselines significantly weaken the paper's contribution. Additionally, the concerns about the method's applicability in sparse-reward settings and the unclear impact of hyperparameters further detract from its overall quality. Therefore, the paper requires substantial revisions and improvements before it can be considered for publication.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "Gkbxt7ThQxU",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Explicitly Maintaining Diverse Playing Styles in Self-Play\" presents a novel approach to reinforcement learning by introducing a bi-objective optimization model that aims to maintain a population of agents with diverse playing styles and high skill levels. The paper is evaluated in two games, Pong and Justice Online, and claims to improve the generalization of policies by ensuring diversity in playing styles.\n\n**Strengths:**\n1. **Novelty and Approach:** The paper introduces a bi-objective optimization model that is relatively novel in the context of reinforcement learning, particularly in maintaining diverse playing styles.\n2. **Clarity and Reproducibility:** The paper is generally well-written and provides sufficient details for reproducibility, as noted by the reviewers.\n3. **Empirical Results:** The results in the two games show that the proposed method can be competitive and, in some cases, outperform existing methods.\n\n**Weaknesses:**\n1. **Literature and Baselines:** A significant weakness is the omission of relevant literature, particularly the work by Lanctot et al. (2017), which could provide a more comprehensive baseline for comparison. The lack of comparison with quality-diversity algorithms like MAP-Elites is also a notable gap.\n2. **Experimental Design:** The experimental setup, particularly the sampling of opponents, is criticized for potentially biasing results in favor of the proposed method. This raises concerns about the validity of the empirical findings.\n3. **Simplification of Playing Styles:** The reduction of playing styles to a single scalar is a simplification that may not capture the complexity of diverse strategies, as pointed out by Reviewer 2.\n4. **Limited Domains:** The evaluation is limited to only two games, one of which is relatively simple (Pong). This limits the generalizability of the findings to other complex domains.\n5. **Technical Novelty:** While the approach is novel, the technical contribution is considered incremental by some reviewers, particularly in the context of existing multi-objective optimization and quality-diversity literature.\n\n**Overall Assessment:**\nThe paper presents an interesting approach to enhancing diversity in reinforcement learning through a bi-objective optimization model. However, the lack of comprehensive literature review, potential biases in experimental design, and limited evaluation domains weaken the paper's contributions. The simplification of playing styles and the absence of comparisons with established quality-diversity algorithms further detract from the paper's impact.\n\nGiven the standards of a top-tier conference, the paper falls short in addressing critical aspects of related work and providing robust empirical validation. While the idea is promising, the execution and presentation need significant improvements to meet the high standards expected.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7gRvcAulxa",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"A Frequency Perspective of Adversarial Robustness\" presents an analysis of adversarial examples through the lens of frequency analysis, challenging the common misconception that adversarial examples are primarily high-frequency noise. The authors claim that adversarial examples are dataset dependent and propose a frequency-based explanation for the accuracy vs. robustness trade-off.\n\n**Strengths:**\n1. **Novel Perspective:** The paper provides a frequency-based perspective on adversarial examples, which is a valuable addition to the ongoing debate in the adversarial machine learning community.\n2. **Experimental Design:** The experiments are well-designed and provide interesting insights, particularly in sections 6.1 and 6.2, which are appreciated by multiple reviewers.\n3. **Clarity and Supplementary Material:** The paper is generally well-written and includes extensive supplementary material to support its claims.\n\n**Weaknesses:**\n1. **Lack of Novelty:** Several reviewers point out that the main claimthat adversarial examples are dataset dependentis already known in the literature. This reduces the novelty and significance of the contribution.\n2. **Insufficient Justification and Validation:** The paper's claims are not sufficiently justified or validated across a diverse set of datasets and attack methods. The focus on CIFAR-10 and ImageNet-derived datasets is seen as limiting.\n3. **Overlapping Contributions:** The paper's findings overlap with existing literature, such as the work by Bernhard et al. (2021), which diminishes the paper's originality.\n4. **Clarity of Contributions:** Some reviewers find the contributions unclear and not well-highlighted, particularly regarding the practical implications of the proposed methods.\n5. **Limited Scope:** The paper primarily examines the frequency properties of adversarial perturbations for specific network architectures and does not consider other factors like different architectures or optimization properties.\n\n**Reviewer Consensus:**\nThe reviewers are divided, with some recommending acceptance due to the interesting insights and experimental design, while others recommend rejection due to the lack of novelty and insufficient justification of claims. The paper's contributions are seen as incremental rather than groundbreaking, which is a critical factor for acceptance in a top-tier conference.\n\n**Author Responses:**\nThe authors have attempted to address the reviewers' concerns, but several reviewers remain unconvinced about the novelty and significance of the contributions. The responses do not sufficiently clarify the unique contributions or address the overlap with existing work.\n\n**Conclusion:**\nWhile the paper provides interesting insights and a novel perspective, the lack of significant novelty, insufficient validation, and overlap with existing literature are substantial drawbacks. For a top-tier conference, the contributions need to be more groundbreaking and well-validated across diverse scenarios. The paper would benefit from a major revision to address these issues and provide a clearer, more substantial contribution to the field.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "9D5FH6LFbRu",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Functional Risk Minimization\" presents a novel framework for supervised learning by associating each data point with its own function, which is a departure from the traditional assumption of a single function with additive noise. The paper introduces Functional Risk Minimization (FRM) as a new learning objective and claims to provide better performance in small-scale experiments.\n\n### Strengths:\n1. **Novelty**: The idea of associating each data point with its own function is innovative and could potentially lead to new insights in handling noise and generalization in machine learning models.\n2. **Potential Impact**: If successful, this approach could extend the classical Empirical Risk Minimization (ERM) framework and offer new ways to model data generation processes.\n3. **Empirical Results**: The paper provides empirical evidence that FRM can outperform ERM under certain noise conditions, which is promising.\n\n### Weaknesses:\n1. **Clarity and Presentation**: The paper suffers from clarity issues, particularly in distinguishing between the proposed FRM framework and existing concepts like hierarchical Bayesian models. The presentation of the FRM learning objective is not clear, and the connection between the theoretical framework and the implementable algorithm is not well-established.\n2. **Scalability**: The proposed algorithm appears to have scalability issues, as it relies on approximations and Hessian information that may not be feasible for large models. This limits the applicability of the approach to small-scale experiments.\n3. **Theoretical Justification**: There is a lack of formal theoretical arguments supporting the claims that FRM will outperform ERM. The paper does not provide sufficient evidence that the FRM minimizer will achieve low loss or that the finite-sample version approximates the population variant effectively.\n4. **Conceptual Confusion**: The paper's use of terminology, such as ERM and FRM, is non-standard and confusing. This could hinder the understanding and adoption of the proposed framework by the broader research community.\n\n### Reviewer Consensus:\n- **Reviewer 1** highlights significant issues with the clarity and theoretical grounding of the paper, suggesting that the paper is not ready for acceptance.\n- **Reviewer 2** acknowledges the novelty but points out conceptual confusions and the need for further explanation, particularly regarding the relationship with Bayesian models.\n- **Reviewer 3** is more favorable, noting the novelty and potential significance, but also mentions the high computational cost and the need for further discussion on the necessity of FRM over modified ERM.\n\n### Conclusion:\nWhile the paper presents an interesting and potentially impactful idea, the significant issues with clarity, scalability, and theoretical justification make it unsuitable for acceptance at a top-tier conference at this stage. The authors need to address these issues, particularly by providing a clearer presentation of the FRM framework, establishing stronger theoretical foundations, and demonstrating scalability to larger datasets.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "j8s-BRxXST",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper proposes a contrastive learning objective to address text degeneration issues in language models, specifically focusing on reducing repetition. While the idea is intuitive and the paper is well-written, several critical issues have been raised by the reviewers that need to be addressed to meet the standards of a top-tier conference.\n\n1. **Novelty and Contribution**: \n   - The novelty of the paper is questioned by all reviewers. The proposed method is seen as an incremental improvement over existing unlikelihood training methods. The core idea of penalizing previously generated tokens is not new, and the distinction made by the authors between their method and unlikelihood training is not convincingly demonstrated.\n   - Reviewer 1 points out that the authors misrepresent the unlikelihood training method, which already considers both positive and negative tokens. This misrepresentation undermines the claimed novelty of the proposed method.\n\n2. **Experimental Setup and Evaluation**:\n   - The experimental setup is considered insufficient by multiple reviewers. The experiments are conducted on a small-scale model (GPT2-small) and a limited dataset (wikitext-103), which does not convincingly demonstrate the method's effectiveness.\n   - The choice of metrics and the interpretation of results are also criticized. Perplexity is not a reliable metric for evaluating text generation quality, and the use of dist-1 instead of more comprehensive diversity metrics (dist-2, dist-3) is seen as a limitation.\n   - Reviewer 2 highlights the lack of comparison with crucial baselines and the need for experiments on larger models to establish the method's utility.\n\n3. **Significance and Impact**:\n   - The significance of the proposed method is questioned, especially in the context of larger and more advanced language models that may not suffer from the same degree of text degeneration.\n   - The human evaluation results do not show statistically significant improvements over existing methods, which further questions the practical impact of the proposed approach.\n\n4. **Reproducibility**:\n   - While the paper provides code and resources for reproducibility, the lack of novelty and insufficient experimental validation overshadow this positive aspect.\n\nIn conclusion, while the paper presents an interesting idea and is well-written, the lack of significant novelty, insufficient experimental validation, and questionable impact on state-of-the-art models make it unsuitable for acceptance at a top-tier conference. The authors need to address these issues, particularly by providing more comprehensive experiments and clearer differentiation from existing methods, to strengthen their contribution.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "7zc05Ua_HOK",
      "source": "ICLR 2022",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper \"Learning Sample Reweighting for Adversarial Robustness\" presents a novel approach to adversarial training by introducing a meta-learning framework that reweights training samples to improve robustness. The paper is well-written and the methodology is sound, leveraging a bilevel optimization inspired by MAML to learn a parametric function for sample reweighting. The authors provide extensive experimental results demonstrating improvements in both clean and robust accuracy over several baselines.\n\n**Strengths:**\n1. **Novelty in Approach:** The paper introduces a learning-based reweighting strategy for adversarial robustness, which is a departure from heuristic methods. This is a novel application of meta-learning techniques in the context of adversarial training.\n2. **Methodological Soundness:** The approach is methodologically sound, with a clear explanation of the bilevel optimization framework and its implementation.\n3. **Experimental Validation:** The paper includes comprehensive experiments on benchmark datasets, using strong adversaries like AutoAttack, which support the claims of improved robustness.\n\n**Weaknesses:**\n1. **Limited Technical Novelty:** Several reviewers pointed out that the technical novelty is limited, as the approach is a direct adaptation of existing meta-learning techniques (e.g., MAML) to adversarial training. This adaptation may not meet the high novelty bar of a top-tier conference.\n2. **Evaluation Concerns:** There are significant concerns about the evaluation, particularly the lack of adaptive attack evaluations, which are crucial for a fair assessment of adversarial robustness. The authors' response that such evaluations are beyond the scope of the paper is not convincing.\n3. **Performance Marginality:** The improvements in robust accuracy are described as marginal, and the method relies on combining with other techniques like TRADES to achieve competitive results. The lack of results on CIFAR-100 and the use of outdated datasets like MNIST are also noted.\n4. **Theoretical Justification:** The paper lacks a strong theoretical justification for the proposed multi-class margin and its superiority over other input encodings. This is a critical gap that several reviewers highlighted.\n\n**Reviewer Consensus:**\nThe reviewers are divided, with some recommending acceptance due to the empirical results and methodological soundness, while others recommend rejection due to the limited novelty, lack of adaptive attack evaluation, and marginal performance improvements. The concerns about the lack of theoretical justification and the need for more comprehensive ablation studies are also significant.\n\n**Conclusion:**\nWhile the paper presents an interesting application of meta-learning to adversarial training and shows empirical improvements, the concerns about novelty, evaluation completeness, and theoretical justification are substantial. For a top-tier conference, these issues are critical and suggest that the paper is not yet ready for publication. The authors are encouraged to address these concerns, particularly by including adaptive attack evaluations and providing stronger theoretical insights, before resubmitting to a future venue.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "KNSRDB-clPX",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Improving Protein Interaction Prediction using Pretrained Structure Embedding\" presents a method for predicting protein-protein interactions (PPIs) by leveraging pretrained structural embeddings from OmegaFold and using them as inputs to graph neural networks (GNNs). The authors claim that this approach leads to significant improvements in PPI prediction compared to sequence and network-based methods.\n\nUpon reviewing the comments from the four reviewers, several key points emerge:\n\n1. **Novelty and Contribution**: The primary concern across all reviews is the limited novelty and contribution of the paper. The method essentially combines existing techniques (OmegaFold embeddings and GNNs) without significant innovation or modification. The reviewers expected more original contributions, such as a novel pretraining method for protein structures, rather than the straightforward application of existing tools.\n\n2. **Empirical Evaluation**: The empirical results are criticized for being marginally better than existing methods and lacking robustness, as they do not include confidence intervals. Additionally, the choice of baselines is questioned, with reviewers noting the absence of comparisons to state-of-the-art methods like AlphaFold-Multimer or other advanced PPI prediction techniques.\n\n3. **Clarity and Presentation**: The paper's clarity is questioned, with specific issues in explaining the setup of the binary classification problem and the interpretation of results like AUC. The presentation of results in tables is also noted to be suboptimal, lacking confidence intervals and clear highlights of best results.\n\n4. **Technical and Empirical Significance**: The reviewers consistently rate the technical and empirical contributions as marginally significant or not significant at all. The paper does not convincingly demonstrate the advantage of using structural embeddings and GNN aggregation over simpler or more direct methods.\n\n5. **Reproducibility**: While the paper provides detailed hyper-parameter settings, the overall lack of clarity and the questionable choice of baselines may hinder reproducibility and the ability to assess the true impact of the proposed method.\n\nIn summary, while the paper addresses an important problem in bioinformatics, the approach lacks the novelty and empirical rigor expected at a top-tier conference. The combination of existing methods without significant innovation, coupled with unclear empirical advantages and presentation issues, suggests that the paper does not meet the standards required for acceptance.\n\nFinal Decision: Reject"
    },
    {
      "paper_id": "dyifcA9UuRo",
      "source": "ICLR 2023",
      "true_decision": "Reject",
      "prediction": "Reject",
      "correct": true,
      "response": "The paper titled \"Neural Probabilistic Logic Programming in Discrete-Continuous Domains\" introduces DeepSeaProbLog, a neural-symbolic AI approach that extends existing neural probabilistic logic programming languages to handle both discrete and continuous variables. This is a significant advancement in the field, as it addresses a major limitation of current systems like DeepProbLog, which are restricted to discrete domains.\n\n**Strengths:**\n1. **Novelty and Contribution:** The paper presents a novel extension to existing frameworks by incorporating continuous variables, which is a meaningful contribution to the field of neural-symbolic AI. This extension allows for more expressive modeling of real-world problems.\n2. **Technical Soundness:** The paper is technically sound, with well-supported claims and a solid theoretical foundation. The use of weighted model integration and differentiation through a weighted model integral is innovative.\n3. **Clarity and Quality:** The paper is generally well-written, with clear explanations of the proposed methods and their significance.\n\n**Weaknesses:**\n1. **Empirical Evaluation:** A recurring concern among reviewers is the limited empirical evaluation. The experiments are primarily conducted on toy applications, and there is a lack of extensive benchmarking against other methods. This raises questions about the generalizability and practical applicability of the proposed approach.\n2. **Incremental Nature:** Some reviewers perceive the work as an incremental improvement over DeepProbLog, rather than a groundbreaking innovation. This perception is partly due to the limited scope of experiments and the reliance on existing techniques.\n3. **Clarity on Discrete Variables:** There is some ambiguity regarding the handling of discrete variables, particularly in terms of reparameterization and integration. This could have been addressed more thoroughly in the paper.\n4. **Scalability and Efficiency:** Questions about the scalability and efficiency of the implementation remain unanswered. The trade-offs between performance and efficiency, especially when approximations are used, need further exploration.\n\n**Reviewer Consensus:**\nThe reviewers are divided in their opinions. While some appreciate the novelty and potential impact of the work, others are concerned about the limited empirical validation and the incremental nature of the contribution. The paper's strengths in theoretical innovation are somewhat overshadowed by the weaknesses in empirical demonstration and clarity.\n\n**Author Responses:**\nThe authors have provided clarifications and additional information in response to reviewer comments, but some concerns, particularly regarding empirical evaluation and clarity, persist.\n\n**Conclusion:**\nWhile the paper presents a novel and technically sound contribution to the field of neural-symbolic AI, the limited empirical evaluation and perceived incremental nature of the work are significant drawbacks. For a top-tier conference, the expectations for empirical validation and demonstration of practical impact are high. Given these considerations, the paper does not meet the standards required for acceptance at a top-tier conference.\n\nFinal Decision: Reject"
    }
  ]
}